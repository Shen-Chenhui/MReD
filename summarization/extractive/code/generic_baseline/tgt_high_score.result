There is a general consensus that that the paper has good quality and clarity, is well motivated, is sufficiently novel, makes clear contributions for training deep neural networks, and provides convincing experimental results to show the advantages of the proposed SN. Overall, it is clear the paper should be accepted, based on reviewer comments, which unanimously agreed on the quality of the work.
There is a general consensus that that the paper has good quality and clarity, is well motivated, is sufficiently novel, makes clear contributions for training deep neural networks, and provides convincing experimental results to show the advantages of the proposed SN. The reviewers agree that the paper is well written and the proposed bandit optimization-based algorithm provides a nice framework in which to integrate priors, resulting in impressive empirical improvements.
This paper introduces an approach for reducing the dimensionality of training data examples in a way that preserves information about soft target probabilistic representations provided by a teacher model, with applications such as zero-shot learning and distillation. There is a general consensus that that the paper has good quality and clarity, is well motivated, is sufficiently novel, makes clear contributions for training deep neural networks, and provides convincing experimental results to show the advantages of the proposed SN. The reviewers appreciated the author's comments and the revision, which addressed all of the concerns by including (1) additional experiments to performance with and without self-adversarial sampling, and comparisons to TorusE, (2) improved presentation. After discussion of the author rebuttal, the reviewers all agreed on the significant contribution of the paper and that concerns about clarity were sufficiently addressed. The reviewers are all quite positive, and are in agreement that the author responses and revisions have improved the paper quality and addressed initial concerns. In the initial evaluation, the reviewers have raised several concerns such as performance metrics, ablation study, training time comparison, empirical evaluation of the baseline methods on Kinetics, that were addressed by the authors in the rebuttal. The reviewers are all unanimous in their recommendation that the paper be accepted for publication. Overall, it is clear the paper should be accepted, based on reviewer comments, which unanimously agreed on the quality of the work.
This paper introduces an approach for reducing the dimensionality of training data examples in a way that preserves information about soft target probabilistic representations provided by a teacher model, with applications such as zero-shot learning and distillation. There is a general consensus that that the paper has good quality and clarity, is well motivated, is sufficiently novel, makes clear contributions for training deep neural networks, and provides convincing experimental results to show the advantages of the proposed SN. The reviewers agree that the paper is well written and the proposed bandit optimization-based algorithm provides a nice framework in which to integrate priors, resulting in impressive empirical improvements. The reviewers and the AC identified a number of potential weaknesses in the initial paper: (1) the evaluation only showed the final performance of the approach, and thus it was not clear how much benefit was obtained from adversarial sampling vs the scoring model, or further, how good the results would be for the baselines if the same sampling was used, (2) citation and comparison to a closely related approach (TorusE), and (3) a number of presentation issues early on in the paper. The only downside of the paper is its limited empirical evaluation: there is evidence suggesting that defenses against adversarial examples that work well on MNIST/CIFAR do not necessarily transfer well to much higher-dimensional datasets, for instance, ImageNet. The main concern (that I share with one of the reviewers) is about the fact that the paper proposes both a new dataset/environment *and* a solution for the problem. The reviewers appreciated the author's comments and the revision, which addressed all of the concerns by including (1) additional experiments to performance with and without self-adversarial sampling, and comparisons to TorusE, (2) improved presentation. Overall, it is clear the paper should be accepted, based on reviewer comments, which unanimously agreed on the quality of the work.
While I agree that such a comparison is a useful addition to the manuscript, I do not think it is reasonable to request anything beyond an acknowledgement and citation of the work from the authors as a condition for acceptance. There is a general consensus that that the paper has good quality and clarity, is well motivated, is sufficiently novel, makes clear contributions for training deep neural networks, and provides convincing experimental results to show the advantages of the proposed SN. The reviewers appreciated the author's comments and the revision, which addressed all of the concerns by including (1) additional experiments to performance with and without self-adversarial sampling, and comparisons to TorusE, (2) improved presentation. The reviewers are all unanimous in their recommendation that the paper be accepted for publication. Overall, it is clear the paper should be accepted, based on reviewer comments, which unanimously agreed on the quality of the work.
This paper introduces an approach for reducing the dimensionality of training data examples in a way that preserves information about soft target probabilistic representations provided by a teacher model, with applications such as zero-shot learning and distillation. The reviewers are all unanimous in their recommendation that the paper be accepted for publication.
This paper introduces an approach for reducing the dimensionality of training data examples in a way that preserves information about soft target probabilistic representations provided by a teacher model, with applications such as zero-shot learning and distillation. The reviewers are all unanimous in their recommendation that the paper be accepted for publication. I concur with Reviewer 3 that it would be nice to provide a constructive solution to this issue: if something like K-FAC does indeed work well, it would be a great addition to a final version of this paper.
There is a general consensus that that the paper has good quality and clarity, is well motivated, is sufficiently novel, makes clear contributions for training deep neural networks, and provides convincing experimental results to show the advantages of the proposed SN. The reviewers and the AC identified a number of potential weaknesses in the initial paper: (1) the evaluation only showed the final performance of the approach, and thus it was not clear how much benefit was obtained from adversarial sampling vs the scoring model, or further, how good the results would be for the baselines if the same sampling was used, (2) citation and comparison to a closely related approach (TorusE), and (3) a number of presentation issues early on in the paper. The only downside of the paper is its limited empirical evaluation: there is evidence suggesting that defenses against adversarial examples that work well on MNIST/CIFAR do not necessarily transfer well to much higher-dimensional datasets, for instance, ImageNet.
There is a general consensus that that the paper has good quality and clarity, is well motivated, is sufficiently novel, makes clear contributions for training deep neural networks, and provides convincing experimental results to show the advantages of the proposed SN. The reviewers agree that the paper is well written and the proposed bandit optimization-based algorithm provides a nice framework in which to integrate priors, resulting in impressive empirical improvements.
I'd like to thank the reviewers for engaging in a spirited discussion about this paper, both with each other and with the authors. There is a general consensus that that the paper has good quality and clarity, is well motivated, is sufficiently novel, makes clear contributions for training deep neural networks, and provides convincing experimental results to show the advantages of the proposed SN.
This paper introduces an approach for reducing the dimensionality of training data examples in a way that preserves information about soft target probabilistic representations provided by a teacher model, with applications such as zero-shot learning and distillation. The paper proposes a neural net based method for active localization in a known map using a learnt perception model (convnet) and a learnt control policy combined with a set belief state representation. There is a general consensus that that the paper has good quality and clarity, is well motivated, is sufficiently novel, makes clear contributions for training deep neural networks, and provides convincing experimental results to show the advantages of the proposed SN. I'd like to thank the reviewers for engaging in a spirited discussion about this paper, both with each other and with the authors.
This paper introduces an approach for reducing the dimensionality of training data examples in a way that preserves information about soft target probabilistic representations provided by a teacher model, with applications such as zero-shot learning and distillation. The paper proposes a neural net based method for active localization in a known map using a learnt perception model (convnet) and a learnt control policy combined with a set belief state representation. This work uses a variational autoencoder-based approach to combine the benefits of recent methods that learn policies with behavioral diversity with the advantages of successor representations, addressing the generalization and slow inference problems of competing methods such as DIAYN. The reviewers and the AC identified a number of potential weaknesses in the initial paper: (1) the evaluation only showed the final performance of the approach, and thus it was not clear how much benefit was obtained from adversarial sampling vs the scoring model, or further, how good the results would be for the baselines if the same sampling was used, (2) citation and comparison to a closely related approach (TorusE), and (3) a number of presentation issues early on in the paper. I concur with Reviewer 3 that it would be nice to provide a constructive solution to this issue: if something like K-FAC does indeed work well, it would be a great addition to a final version of this paper. There is a general consensus that that the paper has good quality and clarity, is well motivated, is sufficiently novel, makes clear contributions for training deep neural networks, and provides convincing experimental results to show the advantages of the proposed SN. The reviewers agree that the paper is well written and the proposed bandit optimization-based algorithm provides a nice framework in which to integrate priors, resulting in impressive empirical improvements. Overall, it is clear the paper should be accepted, based on reviewer comments, which unanimously agreed on the quality of the work.
This paper introduces an approach for reducing the dimensionality of training data examples in a way that preserves information about soft target probabilistic representations provided by a teacher model, with applications such as zero-shot learning and distillation. The paper proposes a neural net based method for active localization in a known map using a learnt perception model (convnet) and a learnt control policy combined with a set belief state representation. The reviewers are all unanimous in their recommendation that the paper be accepted for publication. There is a general consensus that that the paper has good quality and clarity, is well motivated, is sufficiently novel, makes clear contributions for training deep neural networks, and provides convincing experimental results to show the advantages of the proposed SN. The reviewers and the AC identified a number of potential weaknesses in the initial paper: (1) the evaluation only showed the final performance of the approach, and thus it was not clear how much benefit was obtained from adversarial sampling vs the scoring model, or further, how good the results would be for the baselines if the same sampling was used, (2) citation and comparison to a closely related approach (TorusE), and (3) a number of presentation issues early on in the paper. Overall, it is clear the paper should be accepted, based on reviewer comments, which unanimously agreed on the quality of the work.
This paper introduces an approach for reducing the dimensionality of training data examples in a way that preserves information about soft target probabilistic representations provided by a teacher model, with applications such as zero-shot learning and distillation. There is a general consensus that that the paper has good quality and clarity, is well motivated, is sufficiently novel, makes clear contributions for training deep neural networks, and provides convincing experimental results to show the advantages of the proposed SN. The reviewers and the AC identified a number of potential weaknesses in the initial paper: (1) the evaluation only showed the final performance of the approach, and thus it was not clear how much benefit was obtained from adversarial sampling vs the scoring model, or further, how good the results would be for the baselines if the same sampling was used, (2) citation and comparison to a closely related approach (TorusE), and (3) a number of presentation issues early on in the paper. The only downside of the paper is its limited empirical evaluation: there is evidence suggesting that defenses against adversarial examples that work well on MNIST/CIFAR do not necessarily transfer well to much higher-dimensional datasets, for instance, ImageNet. The reviewers are all unanimous in their recommendation that the paper be accepted for publication. I concur with Reviewer 3 that it would be nice to provide a constructive solution to this issue: if something like K-FAC does indeed work well, it would be a great addition to a final version of this paper. I'd like to thank the reviewers for engaging in a spirited discussion about this paper, both with each other and with the authors. All reviewers agree that the paper will be a great contribution to the *CONF* community. I thank the authors for engaging with the reviewers, addressing their comments and updating the paper accordingly. There is a consensus among reviewers and AC that the paper would make an excellent *CONF* contribution. If the authors provide a rebuttal to a potential reviewer concern, it's a good idea to acknowledge this and note whether it influenced the final decision or not. If the authors provide a rebuttal to a potential reviewer concern, it's a good idea to acknowledge this and note whether it influenced the final decision or not.
There is a general consensus that that the paper has good quality and clarity, is well motivated, is sufficiently novel, makes clear contributions for training deep neural networks, and provides convincing experimental results to show the advantages of the proposed SN. The reviewers agree that the paper is well written and the proposed bandit optimization-based algorithm provides a nice framework in which to integrate priors, resulting in impressive empirical improvements. Overall, it is clear the paper should be accepted, based on reviewer comments, which unanimously agreed on the quality of the work.
There is a general consensus that that the paper has good quality and clarity, is well motivated, is sufficiently novel, makes clear contributions for training deep neural networks, and provides convincing experimental results to show the advantages of the proposed SN. The reviewers agree that the paper is well written and the proposed bandit optimization-based algorithm provides a nice framework in which to integrate priors, resulting in impressive empirical improvements. Strengths: While combining deep learning methods with bundle adjustment is not new, reviewers generally agree that the particular way in which that is achieved in this paper is novel and interesting. The reviewers and the AC identified a number of potential weaknesses in the initial paper: (1) the evaluation only showed the final performance of the approach, and thus it was not clear how much benefit was obtained from adversarial sampling vs the scoring model, or further, how good the results would be for the baselines if the same sampling was used, (2) citation and comparison to a closely related approach (TorusE), and (3) a number of presentation issues early on in the paper. Overall, it is clear the paper should be accepted, based on reviewer comments, which unanimously agreed on the quality of the work.
There is a general consensus that that the paper has good quality and clarity, is well motivated, is sufficiently novel, makes clear contributions for training deep neural networks, and provides convincing experimental results to show the advantages of the proposed SN. This paper introduces an approach for reducing the dimensionality of training data examples in a way that preserves information about soft target probabilistic representations provided by a teacher model, with applications such as zero-shot learning and distillation. The paper proposes a neural net based method for active localization in a known map using a learnt perception model (convnet) and a learnt control policy combined with a set belief state representation. This work uses a variational autoencoder-based approach to combine the benefits of recent methods that learn policies with behavioral diversity with the advantages of successor representations, addressing the generalization and slow inference problems of competing methods such as DIAYN. The authors have taken inspiration from recent publications that demonstrate transfer learning over sequential RL tasks and have proposed a method that trains individual learners from experts using layerwise connections, gradually forcing the features to distill into the student with a hard-coded annealing of coeffiecients. This work introduces a new neural network model that can represent hyperedges of variable size, which is experimentally shown to improve or match the state of the art on several problems.
There is a general consensus that that the paper has good quality and clarity, is well motivated, is sufficiently novel, makes clear contributions for training deep neural networks, and provides convincing experimental results to show the advantages of the proposed SN.
This paper introduces an approach for reducing the dimensionality of training data examples in a way that preserves information about soft target probabilistic representations provided by a teacher model, with applications such as zero-shot learning and distillation. The paper proposes a neural net based method for active localization in a known map using a learnt perception model (convnet) and a learnt control policy combined with a set belief state representation. This work uses a variational autoencoder-based approach to combine the benefits of recent methods that learn policies with behavioral diversity with the advantages of successor representations, addressing the generalization and slow inference problems of competing methods such as DIAYN. The reviewers and the AC identified a number of potential weaknesses in the initial paper: (1) the evaluation only showed the final performance of the approach, and thus it was not clear how much benefit was obtained from adversarial sampling vs the scoring model, or further, how good the results would be for the baselines if the same sampling was used, (2) citation and comparison to a closely related approach (TorusE), and (3) a number of presentation issues early on in the paper. The reviewers appreciated the author's comments and the revision, which addressed all of the concerns by including (1) additional experiments to performance with and without self-adversarial sampling, and comparisons to TorusE, (2) improved presentation. There is a general consensus that that the paper has good quality and clarity, is well motivated, is sufficiently novel, makes clear contributions for training deep neural networks, and provides convincing experimental results to show the advantages of the proposed SN.
This paper introduces an approach for reducing the dimensionality of training data examples in a way that preserves information about soft target probabilistic representations provided by a teacher model, with applications such as zero-shot learning and distillation. There is a general consensus that that the paper has good quality and clarity, is well motivated, is sufficiently novel, makes clear contributions for training deep neural networks, and provides convincing experimental results to show the advantages of the proposed SN. The reviewers agree that the paper is well written and the proposed bandit optimization-based algorithm provides a nice framework in which to integrate priors, resulting in impressive empirical improvements. Strengths: While combining deep learning methods with bundle adjustment is not new, reviewers generally agree that the particular way in which that is achieved in this paper is novel and interesting. The reviewers and the AC identified a number of potential weaknesses in the initial paper: (1) the evaluation only showed the final performance of the approach, and thus it was not clear how much benefit was obtained from adversarial sampling vs the scoring model, or further, how good the results would be for the baselines if the same sampling was used, (2) citation and comparison to a closely related approach (TorusE), and (3) a number of presentation issues early on in the paper. I concur with Reviewer 3 that it would be nice to provide a constructive solution to this issue: if something like K-FAC does indeed work well, it would be a great addition to a final version of this paper. The only downside of the paper is its limited empirical evaluation: there is evidence suggesting that defenses against adversarial examples that work well on MNIST/CIFAR do not necessarily transfer well to much higher-dimensional datasets, for instance, ImageNet. Overall, it is clear the paper should be accepted, based on reviewer comments, which unanimously agreed on the quality of the work. The reviewers and I all agree that the paper should be accepted, and I think it should be considered for an oral presentation.
This paper introduces an approach for reducing the dimensionality of training data examples in a way that preserves information about soft target probabilistic representations provided by a teacher model, with applications such as zero-shot learning and distillation. There is a general consensus that that the paper has good quality and clarity, is well motivated, is sufficiently novel, makes clear contributions for training deep neural networks, and provides convincing experimental results to show the advantages of the proposed SN.
This paper introduces an approach for reducing the dimensionality of training data examples in a way that preserves information about soft target probabilistic representations provided by a teacher model, with applications such as zero-shot learning and distillation. There is a general consensus that that the paper has good quality and clarity, is well motivated, is sufficiently novel, makes clear contributions for training deep neural networks, and provides convincing experimental results to show the advantages of the proposed SN. The reviewers appreciated the author's comments and the revision, which addressed all of the concerns by including (1) additional experiments to performance with and without self-adversarial sampling, and comparisons to TorusE, (2) improved presentation. Overall, it is clear the paper should be accepted, based on reviewer comments, which unanimously agreed on the quality of the work.
This paper introduces an approach for reducing the dimensionality of training data examples in a way that preserves information about soft target probabilistic representations provided by a teacher model, with applications such as zero-shot learning and distillation. The paper proposes a neural net based method for active localization in a known map using a learnt perception model (convnet) and a learnt control policy combined with a set belief state representation. The reviewers are all unanimous in their recommendation that the paper be accepted for publication. I concur with Reviewer 3 that it would be nice to provide a constructive solution to this issue: if something like K-FAC does indeed work well, it would be a great addition to a final version of this paper.
This paper introduces an approach for reducing the dimensionality of training data examples in a way that preserves information about soft target probabilistic representations provided by a teacher model, with applications such as zero-shot learning and distillation. The reviewers appreciated the author's comments and the revision, which addressed all of the concerns by including (1) additional experiments to performance with and without self-adversarial sampling, and comparisons to TorusE, (2) improved presentation.
This paper introduces an approach for reducing the dimensionality of training data examples in a way that preserves information about soft target probabilistic representations provided by a teacher model, with applications such as zero-shot learning and distillation. The paper proposes a neural net based method for active localization in a known map using a learnt perception model (convnet) and a learnt control policy combined with a set belief state representation. This work uses a variational autoencoder-based approach to combine the benefits of recent methods that learn policies with behavioral diversity with the advantages of successor representations, addressing the generalization and slow inference problems of competing methods such as DIAYN. There is a general consensus that that the paper has good quality and clarity, is well motivated, is sufficiently novel, makes clear contributions for training deep neural networks, and provides convincing experimental results to show the advantages of the proposed SN. The reviewers agree that the paper is well written and the proposed bandit optimization-based algorithm provides a nice framework in which to integrate priors, resulting in impressive empirical improvements. The reviewers and the AC identified a number of potential weaknesses in the initial paper: (1) the evaluation only showed the final performance of the approach, and thus it was not clear how much benefit was obtained from adversarial sampling vs the scoring model, or further, how good the results would be for the baselines if the same sampling was used, (2) citation and comparison to a closely related approach (TorusE), and (3) a number of presentation issues early on in the paper. I concur with Reviewer 3 that it would be nice to provide a constructive solution to this issue: if something like K-FAC does indeed work well, it would be a great addition to a final version of this paper. The only downside of the paper is its limited empirical evaluation: there is evidence suggesting that defenses against adversarial examples that work well on MNIST/CIFAR do not necessarily transfer well to much higher-dimensional datasets, for instance, ImageNet. The authors are encouraged to take heed of these comments in preparing their final revision, but I will not object to the acceptance of the paper on these grounds, as the methods proposed therein are truly interesting and exciting.
This paper introduces an approach for reducing the dimensionality of training data examples in a way that preserves information about soft target probabilistic representations provided by a teacher model, with applications such as zero-shot learning and distillation. The paper proposes a neural net based method for active localization in a known map using a learnt perception model (convnet) and a learnt control policy combined with a set belief state representation. This work uses a variational autoencoder-based approach to combine the benefits of recent methods that learn policies with behavioral diversity with the advantages of successor representations, addressing the generalization and slow inference problems of competing methods such as DIAYN. The authors have taken inspiration from recent publications that demonstrate transfer learning over sequential RL tasks and have proposed a method that trains individual learners from experts using layerwise connections, gradually forcing the features to distill into the student with a hard-coded annealing of coeffiecients. This work introduces a new neural network model that can represent hyperedges of variable size, which is experimentally shown to improve or match the state of the art on several problems. The paper presents a variational Bayesian approach for quantising neural network weights and makes interesting and useful steps in this increasingly popular area of deep learning. The reviewers appreciated the author's comments and the revision, which addressed all of the concerns by including (1) additional experiments to performance with and without self-adversarial sampling, and comparisons to TorusE, (2) improved presentation. The reviewers are all unanimous in their recommendation that the paper be accepted for publication.
This paper introduces an approach for reducing the dimensionality of training data examples in a way that preserves information about soft target probabilistic representations provided by a teacher model, with applications such as zero-shot learning and distillation. The paper proposes a neural net based method for active localization in a known map using a learnt perception model (convnet) and a learnt control policy combined with a set belief state representation. This work uses a variational autoencoder-based approach to combine the benefits of recent methods that learn policies with behavioral diversity with the advantages of successor representations, addressing the generalization and slow inference problems of competing methods such as DIAYN. The authors have taken inspiration from recent publications that demonstrate transfer learning over sequential RL tasks and have proposed a method that trains individual learners from experts using layerwise connections, gradually forcing the features to distill into the student with a hard-coded annealing of coeffiecients. The reviewers and the AC identified a number of potential weaknesses in the initial paper: (1) the evaluation only showed the final performance of the approach, and thus it was not clear how much benefit was obtained from adversarial sampling vs the scoring model, or further, how good the results would be for the baselines if the same sampling was used, (2) citation and comparison to a closely related approach (TorusE), and (3) a number of presentation issues early on in the paper. The reviewers appreciated the author's comments and the revision, which addressed all of the concerns by including (1) additional experiments to performance with and without self-adversarial sampling, and comparisons to TorusE, (2) improved presentation. The reviewers are all unanimous in their recommendation that the paper be accepted for publication.
This paper introduces an approach for reducing the dimensionality of training data examples in a way that preserves information about soft target probabilistic representations provided by a teacher model, with applications such as zero-shot learning and distillation. There is a general consensus that that the paper has good quality and clarity, is well motivated, is sufficiently novel, makes clear contributions for training deep neural networks, and provides convincing experimental results to show the advantages of the proposed SN. Overall, it is clear the paper should be accepted, based on reviewer comments, which unanimously agreed on the quality of the work.
This paper introduces an approach for reducing the dimensionality of training data examples in a way that preserves information about soft target probabilistic representations provided by a teacher model, with applications such as zero-shot learning and distillation. The paper proposes a neural net based method for active localization in a known map using a learnt perception model (convnet) and a learnt control policy combined with a set belief state representation. This work uses a variational autoencoder-based approach to combine the benefits of recent methods that learn policies with behavioral diversity with the advantages of successor representations, addressing the generalization and slow inference problems of competing methods such as DIAYN. There is a general consensus that that the paper has good quality and clarity, is well motivated, is sufficiently novel, makes clear contributions for training deep neural networks, and provides convincing experimental results to show the advantages of the proposed SN. The reviewers appreciated the author's comments and the revision, which addressed all of the concerns by including (1) additional experiments to performance with and without self-adversarial sampling, and comparisons to TorusE, (2) improved presentation. Overall, it is clear the paper should be accepted, based on reviewer comments, which unanimously agreed on the quality of the work.
The reviewers are all unanimous in their recommendation that the paper be accepted for publication. I concur with Reviewer 3 that it would be nice to provide a constructive solution to this issue: if something like K-FAC does indeed work well, it would be a great addition to a final version of this paper. The authors are encouraged to take heed of these comments in preparing their final revision, but I will not object to the acceptance of the paper on these grounds, as the methods proposed therein are truly interesting and exciting. They have made a number of suggestion for improving the paper, which the authors should consider incorporating in their final version.
This paper introduces an approach for reducing the dimensionality of training data examples in a way that preserves information about soft target probabilistic representations provided by a teacher model, with applications such as zero-shot learning and distillation. The paper proposes a neural net based method for active localization in a known map using a learnt perception model (convnet) and a learnt control policy combined with a set belief state representation. This work uses a variational autoencoder-based approach to combine the benefits of recent methods that learn policies with behavioral diversity with the advantages of successor representations, addressing the generalization and slow inference problems of competing methods such as DIAYN. The authors have taken inspiration from recent publications that demonstrate transfer learning over sequential RL tasks and have proposed a method that trains individual learners from experts using layerwise connections, gradually forcing the features to distill into the student with a hard-coded annealing of coeffiecients. This work introduces a new neural network model that can represent hyperedges of variable size, which is experimentally shown to improve or match the state of the art on several problems. The paper presents a variational Bayesian approach for quantising neural network weights and makes interesting and useful steps in this increasingly popular area of deep learning. The authors provide an extensive theoretical and empirical analysis, showing performance improvements in zero shot learning and finite sample error upper bounds. The reviewers appreciated the author's comments and the revision, which addressed all of the concerns by including (1) additional experiments to performance with and without self-adversarial sampling, and comparisons to TorusE, (2) improved presentation. After discussion of the author rebuttal, the reviewers all agreed on the significant contribution of the paper and that concerns about clarity were sufficiently addressed. The reviewers are all quite positive, and are in agreement that the author responses and revisions have improved the paper quality and addressed initial concerns. There is a general consensus that that the paper has good quality and clarity, is well motivated, is sufficiently novel, makes clear contributions for training deep neural networks, and provides convincing experimental results to show the advantages of the proposed SN. The reviewers agree that the paper is well written and the proposed bandit optimization-based algorithm provides a nice framework in which to integrate priors, resulting in impressive empirical improvements. Overall, it is clear the paper should be accepted, based on reviewer comments, which unanimously agreed on the quality of the work.
This paper introduces an approach for reducing the dimensionality of training data examples in a way that preserves information about soft target probabilistic representations provided by a teacher model, with applications such as zero-shot learning and distillation. The paper proposes a neural net based method for active localization in a known map using a learnt perception model (convnet) and a learnt control policy combined with a set belief state representation. There is a general consensus that that the paper has good quality and clarity, is well motivated, is sufficiently novel, makes clear contributions for training deep neural networks, and provides convincing experimental results to show the advantages of the proposed SN. The reviewers agree that the paper is well written and the proposed bandit optimization-based algorithm provides a nice framework in which to integrate priors, resulting in impressive empirical improvements. Overall, it is clear the paper should be accepted, based on reviewer comments, which unanimously agreed on the quality of the work.
The reviewers appreciated the author's comments and the revision, which addressed all of the concerns by including (1) additional experiments to performance with and without self-adversarial sampling, and comparisons to TorusE, (2) improved presentation. I concur with Reviewer 3 that it would be nice to provide a constructive solution to this issue: if something like K-FAC does indeed work well, it would be a great addition to a final version of this paper.
This paper introduces an approach for reducing the dimensionality of training data examples in a way that preserves information about soft target probabilistic representations provided by a teacher model, with applications such as zero-shot learning and distillation. I'd like to thank the reviewers for engaging in a spirited discussion about this paper, both with each other and with the authors.
This paper introduces an approach for reducing the dimensionality of training data examples in a way that preserves information about soft target probabilistic representations provided by a teacher model, with applications such as zero-shot learning and distillation. The paper proposes a neural net based method for active localization in a known map using a learnt perception model (convnet) and a learnt control policy combined with a set belief state representation. This work uses a variational autoencoder-based approach to combine the benefits of recent methods that learn policies with behavioral diversity with the advantages of successor representations, addressing the generalization and slow inference problems of competing methods such as DIAYN. There is a general consensus that that the paper has good quality and clarity, is well motivated, is sufficiently novel, makes clear contributions for training deep neural networks, and provides convincing experimental results to show the advantages of the proposed SN. The reviewers are all unanimous in their recommendation that the paper be accepted for publication. The reviewers appreciated the author's comments and the revision, which addressed all of the concerns by including (1) additional experiments to performance with and without self-adversarial sampling, and comparisons to TorusE, (2) improved presentation. After the revisions and rebuttal, reviewers agreed that the paper was improved and increased their ratings as a result, with all recommending accept.
This paper introduces an approach for reducing the dimensionality of training data examples in a way that preserves information about soft target probabilistic representations provided by a teacher model, with applications such as zero-shot learning and distillation. The paper proposes a neural net based method for active localization in a known map using a learnt perception model (convnet) and a learnt control policy combined with a set belief state representation. There is a general consensus that that the paper has good quality and clarity, is well motivated, is sufficiently novel, makes clear contributions for training deep neural networks, and provides convincing experimental results to show the advantages of the proposed SN. Overall, it is clear the paper should be accepted, based on reviewer comments, which unanimously agreed on the quality of the work.
The reviewers are all unanimous in their recommendation that the paper be accepted for publication. There is a general consensus that that the paper has good quality and clarity, is well motivated, is sufficiently novel, makes clear contributions for training deep neural networks, and provides convincing experimental results to show the advantages of the proposed SN. I concur with Reviewer 3 that it would be nice to provide a constructive solution to this issue: if something like K-FAC does indeed work well, it would be a great addition to a final version of this paper. I'd like to thank the reviewers for engaging in a spirited discussion about this paper, both with each other and with the authors.
There is a general consensus that that the paper has good quality and clarity, is well motivated, is sufficiently novel, makes clear contributions for training deep neural networks, and provides convincing experimental results to show the advantages of the proposed SN. The reviewers agree that the paper is well written and the proposed bandit optimization-based algorithm provides a nice framework in which to integrate priors, resulting in impressive empirical improvements. Overall, it is clear the paper should be accepted, based on reviewer comments, which unanimously agreed on the quality of the work.
The reviewers are all unanimous in their recommendation that the paper be accepted for publication. There is a general consensus that that the paper has good quality and clarity, is well motivated, is sufficiently novel, makes clear contributions for training deep neural networks, and provides convincing experimental results to show the advantages of the proposed SN. The reviewers appreciated the author's comments and the revision, which addressed all of the concerns by including (1) additional experiments to performance with and without self-adversarial sampling, and comparisons to TorusE, (2) improved presentation. After discussion of the author rebuttal, the reviewers all agreed on the significant contribution of the paper and that concerns about clarity were sufficiently addressed. Overall, it is clear the paper should be accepted, based on reviewer comments, which unanimously agreed on the quality of the work.
This paper introduces an approach for reducing the dimensionality of training data examples in a way that preserves information about soft target probabilistic representations provided by a teacher model, with applications such as zero-shot learning and distillation. There is a general consensus that that the paper has good quality and clarity, is well motivated, is sufficiently novel, makes clear contributions for training deep neural networks, and provides convincing experimental results to show the advantages of the proposed SN.
The reviewers are all unanimous in their recommendation that the paper be accepted for publication.
This paper introduces an approach for reducing the dimensionality of training data examples in a way that preserves information about soft target probabilistic representations provided by a teacher model, with applications such as zero-shot learning and distillation. The paper proposes a neural net based method for active localization in a known map using a learnt perception model (convnet) and a learnt control policy combined with a set belief state representation. There is a general consensus that that the paper has good quality and clarity, is well motivated, is sufficiently novel, makes clear contributions for training deep neural networks, and provides convincing experimental results to show the advantages of the proposed SN.
The reviewers are all unanimous in their recommendation that the paper be accepted for publication. Overall, it is clear the paper should be accepted, based on reviewer comments, which unanimously agreed on the quality of the work.
Overall, it is clear the paper should be accepted, based on reviewer comments, which unanimously agreed on the quality of the work.
This paper introduces an approach for reducing the dimensionality of training data examples in a way that preserves information about soft target probabilistic representations provided by a teacher model, with applications such as zero-shot learning and distillation. The paper proposes a neural net based method for active localization in a known map using a learnt perception model (convnet) and a learnt control policy combined with a set belief state representation. This work uses a variational autoencoder-based approach to combine the benefits of recent methods that learn policies with behavioral diversity with the advantages of successor representations, addressing the generalization and slow inference problems of competing methods such as DIAYN. The reviewers and the AC identified a number of potential weaknesses in the initial paper: (1) the evaluation only showed the final performance of the approach, and thus it was not clear how much benefit was obtained from adversarial sampling vs the scoring model, or further, how good the results would be for the baselines if the same sampling was used, (2) citation and comparison to a closely related approach (TorusE), and (3) a number of presentation issues early on in the paper. The reviewers appreciated the author's comments and the revision, which addressed all of the concerns by including (1) additional experiments to performance with and without self-adversarial sampling, and comparisons to TorusE, (2) improved presentation. Overall, it is clear the paper should be accepted, based on reviewer comments, which unanimously agreed on the quality of the work.
The reviewers are all unanimous in their recommendation that the paper be accepted for publication. There is a general consensus that that the paper has good quality and clarity, is well motivated, is sufficiently novel, makes clear contributions for training deep neural networks, and provides convincing experimental results to show the advantages of the proposed SN. The reviewers agree that the paper is well written and the proposed bandit optimization-based algorithm provides a nice framework in which to integrate priors, resulting in impressive empirical improvements. The reviewers and the AC identified a number of potential weaknesses in the initial paper: (1) the evaluation only showed the final performance of the approach, and thus it was not clear how much benefit was obtained from adversarial sampling vs the scoring model, or further, how good the results would be for the baselines if the same sampling was used, (2) citation and comparison to a closely related approach (TorusE), and (3) a number of presentation issues early on in the paper. After the revisions and rebuttal, reviewers agreed that the paper was improved and increased their ratings as a result, with all recommending accept.
This paper introduces an approach for reducing the dimensionality of training data examples in a way that preserves information about soft target probabilistic representations provided by a teacher model, with applications such as zero-shot learning and distillation. The paper proposes a neural net based method for active localization in a known map using a learnt perception model (convnet) and a learnt control policy combined with a set belief state representation. There is a general consensus that that the paper has good quality and clarity, is well motivated, is sufficiently novel, makes clear contributions for training deep neural networks, and provides convincing experimental results to show the advantages of the proposed SN. The reviewers agree that the paper is well written and the proposed bandit optimization-based algorithm provides a nice framework in which to integrate priors, resulting in impressive empirical improvements. Overall, it is clear the paper should be accepted, based on reviewer comments, which unanimously agreed on the quality of the work. The reviewers and the AC identified a number of potential weaknesses in the initial paper: (1) the evaluation only showed the final performance of the approach, and thus it was not clear how much benefit was obtained from adversarial sampling vs the scoring model, or further, how good the results would be for the baselines if the same sampling was used, (2) citation and comparison to a closely related approach (TorusE), and (3) a number of presentation issues early on in the paper. I concur with Reviewer 3 that it would be nice to provide a constructive solution to this issue: if something like K-FAC does indeed work well, it would be a great addition to a final version of this paper.
This paper introduces an approach for reducing the dimensionality of training data examples in a way that preserves information about soft target probabilistic representations provided by a teacher model, with applications such as zero-shot learning and distillation. The paper proposes a neural net based method for active localization in a known map using a learnt perception model (convnet) and a learnt control policy combined with a set belief state representation. This work uses a variational autoencoder-based approach to combine the benefits of recent methods that learn policies with behavioral diversity with the advantages of successor representations, addressing the generalization and slow inference problems of competing methods such as DIAYN. There is a general consensus that that the paper has good quality and clarity, is well motivated, is sufficiently novel, makes clear contributions for training deep neural networks, and provides convincing experimental results to show the advantages of the proposed SN. The reviewers and the AC identified a number of potential weaknesses in the initial paper: (1) the evaluation only showed the final performance of the approach, and thus it was not clear how much benefit was obtained from adversarial sampling vs the scoring model, or further, how good the results would be for the baselines if the same sampling was used, (2) citation and comparison to a closely related approach (TorusE), and (3) a number of presentation issues early on in the paper. I concur with Reviewer 3 that it would be nice to provide a constructive solution to this issue: if something like K-FAC does indeed work well, it would be a great addition to a final version of this paper. The authors are encouraged to take heed of these comments in preparing their final revision, but I will not object to the acceptance of the paper on these grounds, as the methods proposed therein are truly interesting and exciting. Overall, it is clear the paper should be accepted, based on reviewer comments, which unanimously agreed on the quality of the work. They have made a number of suggestion for improving the paper, which the authors should consider incorporating in their final version. The authors are encouraged to consider the reviewer's suggestions to revise the paper.
There is a general consensus that that the paper has good quality and clarity, is well motivated, is sufficiently novel, makes clear contributions for training deep neural networks, and provides convincing experimental results to show the advantages of the proposed SN. I'd like to thank the reviewers for engaging in a spirited discussion about this paper, both with each other and with the authors. The reviewers and the AC identified a number of potential weaknesses in the initial paper: (1) the evaluation only showed the final performance of the approach, and thus it was not clear how much benefit was obtained from adversarial sampling vs the scoring model, or further, how good the results would be for the baselines if the same sampling was used, (2) citation and comparison to a closely related approach (TorusE), and (3) a number of presentation issues early on in the paper. I concur with Reviewer 3 that it would be nice to provide a constructive solution to this issue: if something like K-FAC does indeed work well, it would be a great addition to a final version of this paper. The only downside of the paper is its limited empirical evaluation: there is evidence suggesting that defenses against adversarial examples that work well on MNIST/CIFAR do not necessarily transfer well to much higher-dimensional datasets, for instance, ImageNet. The main concern (that I share with one of the reviewers) is about the fact that the paper proposes both a new dataset/environment *and* a solution for the problem.
This paper introduces an approach for reducing the dimensionality of training data examples in a way that preserves information about soft target probabilistic representations provided by a teacher model, with applications such as zero-shot learning and distillation. The paper proposes a neural net based method for active localization in a known map using a learnt perception model (convnet) and a learnt control policy combined with a set belief state representation. There is a general consensus that that the paper has good quality and clarity, is well motivated, is sufficiently novel, makes clear contributions for training deep neural networks, and provides convincing experimental results to show the advantages of the proposed SN. The reviewers agree that the paper is well written and the proposed bandit optimization-based algorithm provides a nice framework in which to integrate priors, resulting in impressive empirical improvements. The reviewers are all unanimous in their recommendation that the paper be accepted for publication. I concur with Reviewer 3 that it would be nice to provide a constructive solution to this issue: if something like K-FAC does indeed work well, it would be a great addition to a final version of this paper. The authors are encouraged to take heed of these comments in preparing their final revision, but I will not object to the acceptance of the paper on these grounds, as the methods proposed therein are truly interesting and exciting. They have made a number of suggestion for improving the paper, which the authors should consider incorporating in their final version. The authors are encouraged to consider the reviewer's suggestions to revise the paper.
This paper introduces an approach for reducing the dimensionality of training data examples in a way that preserves information about soft target probabilistic representations provided by a teacher model, with applications such as zero-shot learning and distillation. The paper proposes a neural net based method for active localization in a known map using a learnt perception model (convnet) and a learnt control policy combined with a set belief state representation. The reviewers are all unanimous in their recommendation that the paper be accepted for publication. Overall, it is clear the paper should be accepted, based on reviewer comments, which unanimously agreed on the quality of the work. The reviewers and I all agree that the paper should be accepted, and I think it should be considered for an oral presentation.
This paper introduces an approach for reducing the dimensionality of training data examples in a way that preserves information about soft target probabilistic representations provided by a teacher model, with applications such as zero-shot learning and distillation. The paper proposes a neural net based method for active localization in a known map using a learnt perception model (convnet) and a learnt control policy combined with a set belief state representation. This work uses a variational autoencoder-based approach to combine the benefits of recent methods that learn policies with behavioral diversity with the advantages of successor representations, addressing the generalization and slow inference problems of competing methods such as DIAYN. The authors have taken inspiration from recent publications that demonstrate transfer learning over sequential RL tasks and have proposed a method that trains individual learners from experts using layerwise connections, gradually forcing the features to distill into the student with a hard-coded annealing of coeffiecients. The reviewers are all unanimous in their recommendation that the paper be accepted for publication. Overall, it is clear the paper should be accepted, based on reviewer comments, which unanimously agreed on the quality of the work. The reviewers and I all agree that the paper should be accepted, and I think it should be considered for an oral presentation.
This paper introduces an approach for reducing the dimensionality of training data examples in a way that preserves information about soft target probabilistic representations provided by a teacher model, with applications such as zero-shot learning and distillation. The paper proposes a neural net based method for active localization in a known map using a learnt perception model (convnet) and a learnt control policy combined with a set belief state representation. The reviewers and the AC identified a number of potential weaknesses in the initial paper: (1) the evaluation only showed the final performance of the approach, and thus it was not clear how much benefit was obtained from adversarial sampling vs the scoring model, or further, how good the results would be for the baselines if the same sampling was used, (2) citation and comparison to a closely related approach (TorusE), and (3) a number of presentation issues early on in the paper. I'd like to thank the reviewers for engaging in a spirited discussion about this paper, both with each other and with the authors. Overall, it is clear the paper should be accepted, based on reviewer comments, which unanimously agreed on the quality of the work.
I'd like to thank the reviewers for engaging in a spirited discussion about this paper, both with each other and with the authors. All reviewers agree that the paper will be a great contribution to the *CONF* community. The reviewers appreciated the author's comments and the revision, which addressed all of the concerns by including (1) additional experiments to performance with and without self-adversarial sampling, and comparisons to TorusE, (2) improved presentation. Overall, it is clear the paper should be accepted, based on reviewer comments, which unanimously agreed on the quality of the work.
This paper introduces an approach for reducing the dimensionality of training data examples in a way that preserves information about soft target probabilistic representations provided by a teacher model, with applications such as zero-shot learning and distillation. The paper proposes a neural net based method for active localization in a known map using a learnt perception model (convnet) and a learnt control policy combined with a set belief state representation. There is a general consensus that that the paper has good quality and clarity, is well motivated, is sufficiently novel, makes clear contributions for training deep neural networks, and provides convincing experimental results to show the advantages of the proposed SN. I concur with Reviewer 3 that it would be nice to provide a constructive solution to this issue: if something like K-FAC does indeed work well, it would be a great addition to a final version of this paper.
