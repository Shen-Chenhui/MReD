The authors describe a method called WAGE, which quantize all operands and operators in a neural network, specifically, the weights (W), activations (A), gradients (G), and errors (E) . The main motivation of the authors in this work is to reduce the number of bits for representation in a network for all the WAGE operations and operands which influences the power consumption and silicon area in hardware implementations. The authors propose WAGE, which discretized weights, activations, gradients, and errors at both training and testing time.
This paper presents a set of studies on emergent communication protocols in referential games that use either symbolic object representations or pixel-level representations of generated images as input. I realize that it would be impossible to report perfectly comparable analyses, but the authors could at least apply the "topographic" analysis of compositionality in the raw-pixel study as well, either by correlating the CNN-based representational similarities of the Speaker with its message similarities, or computing similarity of the inputs in discretized, symbolic terms (or both?
They apply this approach to multi-modal (several "intentions") imitation learning and demonstrate for a real visual robotics task that the proposed framework works better than deterministic neural networks and stochastic neural networks. The paper begins with the authors stating the motivation and problem of how to program robots to do a task based only on demonstrations rather on explicit modeling or programming. The authors propose a new sampling based approach for inference in latent variable models. on Value Iteration Networks is highly relevant to this work: the authors there learn similar tasks (i.e., similar modalities) using the same network. The authors argue that their contribution is 3-fold: (1) does not require robot  rollouts, (2) does not require label for a task, (3) work within raw image inputs. This paper focuses on imitation learning with intentions sampled from a multi-modal distribution. 2. New latent variable model bound that might work better than classic approaches.
This paper proposes a method to build a CNN in the Winograd domain, where weight pruning and ReLU can be applied in this domain to improve sparsity and reduce the number of multiplication. This paper proposes to combine Winograd transformation with sparsity to reduce the computation for deep convolutional neural network. Rather than strictly keeping the architecture of ordinary CNNs, the proposed method applied ReLU to the transform domain, which is interesting. Because this yields a network, which is not mathematically equivalent to a vanilla or Winograd CNN, the method goes through three stages: dense training, pruning and retraining. Review: The paper shows good results using the proposed method and the description is easy to follow. It provides a new way to combine the Winograd transformation and the threshold-based weight pruning strategy. Lin, Zhouhan, Matthieu Courbariaux, Roland Memisevic, and Yoshua Bengio. References: Courbariaux, Matthieu, Yoshua Bengio, and Jean-Pierre David.
This paper studies problems that can be solved using a dynamic programming approach and proposes a neural network architecture called Divide and Conquer Networks (DCN) to solve such problems. Paper significantly benefited from these changes, however experimental section is still based purely on toy datasets (clustering cifar10 patches is the least toy problem, but if one claims that proposed method is a good clusterer one would have to beat actual clustering techniques to show that), and in both cases simple problem-specific baseline (Lloyd for k-means, greedy knapsack solver) beats proposed method. However, while results on convex hull task are good, k-means ones use a single, artificial problem (and do not test DCN, but rather a part of it), and on TSP DCN performs significantly worse than baselines in-distribution, and is better when tested on bigger problems than it is trained on.
Only whether the design of transformation captures the latent representation of the input data, the pseudo-labelling might improve the performance of the unsupervised learning task. This paper presents a method for clustering based on latent representations learned from the classification of transformed data after pseudo-labellisation corresponding to applied transformation. Given that in many applications such parent-class supervised information is not available, the authors of this paper propose domain specific pseudo parent-class labels (for example transformed images of digits) to adapt ACOL for unsupervised learning. The novelty seems to be in the adaptation to GAR from the semi-supervised to the unsupervised setting with labels indicating if data have been transformed or not.
In particular I would like to see a clear comparison in terms of latent traversals on dSprites between beta-VAE and DIP-VAE models presented in Table 3. Empirically the authors demonstrate that DIP-VAE can effectively learn disentangled features, perform comparably better than beta-VAE and at the same time retain the reconstruction quality close to regular VAE (beta-VAE with beta = 1). - the covariance minimisation proposed in the paper looks like an easy to implement yet impactful change to the VAE objective to encourage disentanglement while preserving reconstruction quality ****** The authors propose a new regularization term modifying the VAE (Kingma et al 2013) objective to encourage learning disentangling representations.
In Section 4 they provide quite varied empirical analysis: they confirm their theoretical results on four architectures; they show its use it to regularise on language models; they apply it on large minibatch settings where high variance is a main problem; and on evolution strategies. They start in the Introduction and Section 2 by explaining the multiple uses of random connection weights in deep learning and how the computational cost often restricts their use to a single randomly sampled set of weights per minibatch, which results to higher-variance gradient estimatos than could be achieved otherwise. In a set of experiments it is demonstrated that a significant reduction in gradient variance is achieved, resulting in speedups for training time. However, the painful part: while I am convinced by the idea and love its detailed exposure, and the gradient variance reduction is made very clear, the experimental impact in terms of accuracy (or perplexity) is, sadly,  not very convincing.
Specifically, they observe that: (1) The angle between continuous vectors sampled from a spherical symmetric distribution and their binarized version is relatively small in high dimensions (proven to be about 37 degrees when the dimension goes to infinity), and this demonstrated empirically to be true for the binarized weight matrices of a convenet. This paper presents three observations to understand binary network in Courbariaux, Hubara et al. Pros: The authors lead a very nice exploration into the binary nets in the paper, from the most basic analysis on the converging angle between original and binarized weight vectors, to how this convergence could affect the weight-activation dot product, to pointing out that binarization affects differently on the first layer. It further explains why binarization is able to preserve the model performance by analyzing the weight-activation dot product with "Dot Product Proportionality Property." It also proposes "Generalized Binarization Transformation" for the first layer of a neural network. b. Related to the previous issue, it is not clear to me if in figure 3 and 5, did the authors binarize the activations of that specific layer or all the layers? This paper tries to analyze the effectiveness of binary nets from a perspective originated from the angular perturbation that binarization process brings to the original weight vector. So, how can the given observations be used to explain more recent works? Furthermore, since it uses non-binary operations, it is not clear if this rotation may have some benefits (in terms of resource efficiency) over simply keeping the input layer non-binarized.
One thing missing is a discussion of how this approach is related to semi-supervised learning approaches using GANS where a generative model produces extra data points for the classifier/discriminator. I have some clarifying questions below: - Figure 4 is unclear: does "Confidence loss with original GAN" refer to the method where the classifier is pretrained and then "Joint confidence loss" is with joint training? The authors propose to train a generator network in combination with the classifier and an adversarial discriminator. Classifier is trained to not only maximize classification accuracy on the real training data but also to output a uniform distribution for the generated samples. - How does this compare with a method whereby instead of pushing the fake sample's softmax distribution to be uniform, the model is simply a trained to classify them as an additional "out of distribution" class? The manuscript proposes a generative approach to detect which samples are within vs. This paper proposes a different approach (with could be combined with these methods) based on a new training procedure. Evaluation on several datasets suggests that accounting for the within-sample distribution in this way can often actually improve evaluation performance, and can help the model detect outliers. The generator is trained to produce images that (1) fools a standard GAN discriminator and (2) has high entropy (as enforced with the pull-away term from the EBGAN).
The main significance of this paper is to propose the task of generating the lead section of Wikipedia articles by viewing it as a multi-document summarization problem. The main strength is in the task setup with the dataset and the proposed input sources for generating Wikipedia articles. The authors at first reduce the input size by using various extractive strategies and then use the selected content as input to the abstractive stage where they leverage the Transformer architecture with interesting modifications like dropping the encoder and proposing alternate self-attention mechanisms like local and memory compressed attention. It would have been nice to see how the proposed methods perform with respect to the existing neural abstractive summarization approaches. Unfortunately it is hard to judge the effectiveness of the abstractive model due to the scale of the experiments, especially with regards to the quality of the generated output in comparison to the output of the extractive stage. Linked articles as well as the results of an external web search query are used as input documents, from which the Wikipedia lead section must be generated. Further preprocessing of the input articles is required, using simple heuristics to extract the most relevant sections to feed to a neural abstractive summarizer. This paper considers the task of generating Wikipedia articles as a combination of extractive and abstractive multi-document summarization task where input is the content of reference articles listed in a Wikipedia page along with the content collected from Web search and output is the generated content for a target Wikipedia page.
The paper devises a sparse kernel for RNNs which is urgently needed because current GPU deep learning libraries (e.g., CuDNN) cannot exploit sparsity when it is presented and because a number of works have proposed to sparsify/prune RNNs so as to be able to run on devices with limited compute power (e.g., smartphones). - Experiments (in main paper) only show speedups and do not show loss of accuracy due to sparsity The paper proposes improving performance of large RNNs by combing techniques of model pruning and persistent kernels.
The paper extends softmax consistency by adding in a relative entropy term to the entropy regularization and applying trust region policy optimization instead of gradient descent. Originality The paper proposes a path consistency learning method with a new combination of entropy regularization and relative entropy. This paper presents a policy gradient method that employs entropy regularization and entropy constraint at the same time.
Pros: + The idea of learning from a compressed representation is a very interesting and beneficial idea for large-scale image understanding tasks. The most interesting part is a joint training for both compression and image classification. Summary: This work explores the use of learned compressed image representation for solving 2 computer vision tasks without employing a decoding step. However, applying a deep representation for the compression and then directly solving a vision task (classification and segmentation) can be considered as a novel idea. This is a well-written and quite clear work about how a previous work on image compression using deep neural networks can be extended to train representations which are also valid for semantic understanding.
On the contrary, white-box means that the adversary knows everything about the classification method, including the transformation implemented to make it more robust to attacks. To increase robustness to adversarial attacks, the paper fundamentally proposes to transform an input image before feeding it to a convolutional network classifier. The authors evaluate a number of simple defences that are based on input transformations such TV minimization and image quilting and compare it against previously proposed ideas of JPEG compression and decompression and random crops. * In a white-box scenario, the adversary knows about the transformation and the classification model. Strong points: * To my knowledge, the proposed defense strategy is novel (even if the idea of transformation has been introduced at https://arxiv.org/abs/1612.01401). * The writing is reasonably clear (up to the terminology issues discussed among the weak points), and introduces properly the adversarial attacks considered in the work. * p1: 'too simple to remove adversarial perturbations from input images sufficiently The paper investigates using input transformation techniques as a defence against adversarial examples.
The paper proposes the Skip RNN model which allows a recurrent network to selectively skip updating its hidden state for some inputs, leading to reduced computation at test-time. [6] Sigurdsson et al, "Asynchronous temporal fields for action recognition", CVPR 2017 The authors proposed a novel RNN model where both the input and the state update of the recurrent cells are skipped adaptively for some time steps.
This paper shows that residual networks can be viewed as doing a sort of iterative inference, where each layer is trained to use its "nonlinear part" to push its values in the negative direction of the loss gradient. I think the author should place more focus to study "real" iterative inference with shared parameters rather than analyzing original resnets.
In experiments, the effectiveness of proposed model is clearly shown in counting questions on both a synthetic toy dataset and the widely used VQA v2 dataset. Summary - This paper mainly focuses on a counting problem in visual question answering (VQA) using attention mechanism. Weaknesses - Although the proposed model is helpful to model counting information in VQA, it fails to show improvement with respect to a couple of important baselines: prediction from image representation only and from the combination of image representation and attention weights. - The proposed method improves the baseline by 5% on counting questions. This paper tackles the object counting problem in visual question answering. It proposes many heuristics to use the object feature and attention weights to find the correct count.
Unfortunately, my understanding is that the theory proposed in section 2 does not correspond to the scheme used in the experiments (contrarily to what the conclusion suggest and contrarily to what the discussion of the end of section 3, which says that using embedding is assumed to have an equivalent effect to using the methodology considered in the theoretical part). The model proposed is a variant of the cycle GAN in which in addition embeddings helping the Generator are learned for all the values of the discrete variables.
The paper proposes a way to speed up the inference time of RNN via Skim mechanism where only a small part of hidden variable is updated once the model has decided a corresponding word token seems irrelevant w.r.t. a given task. Contribution: - The paper proposes to use a small RNN to read unimportant text. - The idea of switching small and standard RNNs for skimming and full reading respectively is quite simple and intuitive.
A key problem here is that processing such sequences with ordinary RNNs requires a reduce operation, where the output of the net at time step t depends on the outputs of *all* its predecessor. The paper is certainly relevant, as it can pave the way towards the application of recurrent architectures to problems that have extremely long term dependencies. Apart from efficiency results, the paper also contributes a comparison of model convergence on a long-term dependency task due to (Hochreiter and Schmidhuber, 1997).
The idea to constraint the dimension reduction to fit a certain model, here a GMM, is relevant, and the paper provides a thorough comparison with recent state-of-the-art methods. This applications paper proposes using a deep neural architecture to do unsupervised anomaly detection by learning the parameters of a GMM end-to-end with reconstruction in a low-dimensional latent space. I'm also not convinced of how well the Gaussian model fits the low-dimensional representation and how well can a neural network compute the GMM mixture memberships.
This is complemented by the SCAN model, which is a beta-VAE trained to reconstruct symbols (y; k-hot encoded concepts like {red, suitcase}) with a slightly modified objective. The main way that their model differs from other multimodal methods is that their latent representation is well-suited to applying symbolic operations, such as AND and IGNORE, to the text. 3) SCAN samples/representations are more accurate (generate images of the right concept) and more diverse (far from a uniform prior in a KL sense) than JMVAE and TELBO baselines. This paper proposed a novel neural net architecture that learns object concepts by combining a beta-VAE and SCAN.
In this paper, the authors studied the problem of semi-supervised few-shot classification, by extending the prototypical networks into the setting of semi-supervised learning with examples from distractor classes. While the proposed method is a natural extension of the existing works (i.e., soft k-means and meta-learning).On top of that, It seems the authors have over-claimed their model capability at the first place as the proposed model cannot properly classify the distractor examples but just only consider them as a single class of outliers. This paper proposes to extend the Prototypical Network (NIPS17) to the semi-supervised setting with three possible strategies.
Authors present complex valued analogues of real-valued convolution, ReLU and batch normalization functions. Empirical results show that the new complex-flavored neural networks achieve generally comparable performance to their real-valued counterparts, on a variety of different tasks. The contribution of the current work does not lie in presenting significantly superior results, compared to the traditional real-valued neural networks, but rather in developing an extensive framework for applying and conducting research with complex-valued neural networks. Indeed, the most standard work nowadays with real-valued neural networks depends on a variety of already well-established techniques for weight initialization, regularization, activation function, convolutions, etc. In this work, the complex equivalent of many of these basics tools are developed, such as a number of complex activation functions, complex batch normalization, complex convolution, discussion of complex differentiability, strategies for complex weight initialization, complex equivalent of a residual neural network. I'm on the fence about this work: I like the ideas and they are explained well, but I'm missing some insight into why and how all of this is actually helping to improve performance (especially w.r.t. how phase information is used). Their "related work section" brings up uses of complex valued computation such as discrete Fourier transforms and Holographic Reduced Representations.
Experiments on the MNIST, CIFAR-10, SHVN and KDEF datasets, shows the proposed wavelet-based method has competitive performance with existing methods while still being able to address the overfitting behavior of max pooling. The paper proposes "wavelet pooling" as an alternative for traditional subsampling methods, e.g. max/average/global pooling, etc., within convolutional neural networks. While the currently tested datasets are a good indication of the performance of the proposed method, an evaluation on a large scale scenario, e.g. ILSVRC'12, could solidify the message sent by this manuscript.
As you mention in the paper that the algorithm uses the same amount of computation and memory as Adam optimizer, but could you please provide the reason why you only compare Neumann Optimizer with Baseline RMSProp but not with Adam? The question is that, with the given architectures and dataset, what algorithm should people consider to use between Neumann optimizer and Adam? I understand that you are trying to improve the existing results with their optimizer, but this paper also introduces new algorithm. (9): m_{k-1} The paper proposes a new algorithm, where they claim to use Hessian implicitly and are using a motivation from power-series. Summary: The paper proposes Neumman optimizer, which makes some adjustments to the idealized Neumman algorithm to improve performance and stability in training.
This and / or the defense of training to adversarial examples is an important experiment to assessing the limitations of the attack. Summary of paper: The authors present a novel attack for generating adversarial examples, deemed OptMargin, in which the authors attack an ensemble of classifiers created by classifying at random L2 small perturbations. - I think the authors should make a claim on whether their proposed attack works only for defenses that are agnostic to the attack (such as PGD or region based), or for defenses that know this is a likely attack (see the following comment as well). 2- I think the authors should provide a more detailed and formal description of the OPTMARGIN method. 3- Authors mention that OPTSTRONG attack does not succeed in finding adversarial examples ("it succeeds on 28% of the samples on MNIST;73% on CIFAR-10"). The authors also miss the most standard defense, training with adversarial examples.
Optimizing this formulation using gradient descent can be proven to yield only one optimal global Nash equilibrium, which the authors claim allows Coulomb GANs to overcome the "mode collapse" issue. Leave the stochastic gradient descent optimization algorithm apart (since most of the neural networks are trained in this way), the parametrization and the richness of discriminator family play a vital role in the model collapsing issue. In fact, even with KL-divergence in which log operation is involved, if one can select reasonable parametrization, e.g., directly handling in function space, the saddle point optimization is convex-concave, which means under the same assumption made in the paper, there is only one global Nash Equilibrium. In sum, this paper provides an interesting perspective modeling GAN from the potential field, however, there are several issues need to be addressed. The authors draw from electrical field dynamics and propose to formulate the GAN learning problem in a way such that generated samples are attracted to training set samples, but repel each other.
Summary of paper: The paper proposes an RNN-based neural network architecture for embedding programs, focusing on the semantics of the program rather than the syntax. This paper considers the task of learning program embeddings with neural networks with the ultimate goal of bug detection program repair in the context of students learning to program. The authors evaluate their architectures on the task of predicting error patterns for programming assignments from Microsoft DEV204.1X (an introduction to C# offered on edx) and problems on the Microsoft CodeHunt platform. The authors present 3 architectures for learning representations of programs from execution traces. The neural network is trained using this program traces with an objective for classifying the student error pattern (e.g. list indexing, branching conditions, looping bounds). - Figure 5 seems to suggest that dependencies are only enforced at points in a program where assignment is performed for a variable, is this correct? The application is to predict errors made by students on programming tasks.
They find complex cells that lead to state-of-the-art performance on benchmark dataset CIFAR-10 and ImageNet. They also claim that their method is reaching a new milestone in evolutionary search strategies performance. As a result, this would allow the evolutionary search algorithm to design modules which might be then reused in different edges of the DAG corresponding to the final architecture, which is located at the top level in the hierarchy. In addition, the authors present results that appear to be on par with the state-of-the-art with architecture search on CIFAR-10 and ImageNet benchmark datasets. The authors present a novel evolution scheme applied to neural network architecture search. The method proposed for an hierarchical representation for optimizing over neural network designs is well thought and sound. Finally, while the main motivation behind neural architecture search is to automatise the design of new models, the approach here presented introduces a non-negligible number of hyperparameters that could potentially have a considerable impact and need to be selected somehow. An important contribution is to show that a well-defined architecture representation could lead to efficient cells with a simple randomized search.
They propose a method called Workflow Guided Exploration (WGE) which is learnt from demonstrations but is environment agnostic. Dagger and related methods like Aggrevate provide sample-efficient ways of exploring the environment near where the initial demonstrations were given. SUMMARY The paper deals with the problem of training RL algorithms from demonstration and applying them to various web interfaces such as booking flights. This paper introduces a new exploration policy for Reinforcement Learning for agents on the web called "Workflow Guided Exploration". - My main concern is that while imitation learning and inverse reinforcement learning are mentioned and discussed in related work section as classes of algorithms for incorporating prior information there is no baseline experiment using either of these methods. In addition, they make general comparison to RL literature such as hierarchy rather than more concrete comparisons with the problem at hand (learning from demonstrations.) Their proposed algorithm DAgger fixes this (the mistakes by the policy are linear in the horizon length) by using an iterative procedure where the learnt policy from the previous iteration is executed and expert demonstrations on the visited states are recorded, the new data thus generated is added to the previous data and a new policy retrained.
Two key ideas in the paper include the use of Gaussian noise for the aggregation mechanism in PATE instead of Laplace noise and selective answering strategy by teacher ensemble. If the final privacy guarantee is data-dependent, then this is very different to the way differential privacy is usually applied. These works also involve a "student" being trained using sensitive data with queries being answered in a differentially private manner. minor comments: Figure 2, legend needs to be outside the Figure, in the current Figure a lot is covered by the legend This paper considers the problem of private learning and uses the PATE framework to achieve differential privacy. The novelty of this work is a refined aggregation process, which is improved in three ways: a) Gaussian instead of Laplace noise is used to achieve differential privacy. The paper proposes novel techniques for private learning with PATE framework. The labels produced by the teachers are aggregated in a differentially private manner and the aggregated labels are then used to train a student classifier, which forms the final output. That is, the privacy guarantee would depend only on public information, rather than the private data. Summary: In this work, PATE, an approach for learning with privacy,  is modified to scale its application to real-world data sets. 2. It would be great to have an intuitive explanation about differential privacy and selective aggregation mechanisms with examples. on the negative side: In the introduction, the authors introduce the problem by the importance of privacy issues in medical and health care data. Another way to resolve this would be to have an output-dependent privacy guarantee. However, I think some clarification is needed with regard to item c above: Theorem 2 gives a data-dependent privacy guarantee. However, since the privacy guarantee now depends on the data, it is itself sensitive information.
The authors use the coreset construction with a CNN to demonstrate an active learning algorithm for multi-class classification. This paper proposes a batch mode active learning algorithm for CNN as a core-set problem.
====================== The paper introduces a system to estimate a floor-level via their mobile device's sensor data using an LSTM to determine when a smartphone enters or exits a building, then using the change in barometric pressure from the entrance of the building to indoor location. Using the entrance point's barometer reading as a reference, the method calculates the relative floor the user has moved to using a well known relationship between heights and barometric readings.
In this paper, the expressive power of neural networks characterized by tensor train (TT) decomposition, a chain-type tensor decomposition, is investigated. The authors of this paper first present a class of networks inspired by various tensor decomposition models. The result of this paper is interesting and also important from a viewpoint on analysis for the tensor train decomposition. In addition, I would like to see the performance of RNNs and MLPs with the same number of units/rank in order to validate the analogy between these networks. Finally the authors show that almost all tensor train networks (exluding a set of measure zero) require exponentially large width to represent in CP networks, which is analogous to shallow networks. The authors compare the complexity of TT-type networks with networks structured by CP decomposition, which corresponds to shallow networks.
The paper proposes an approach to exploration based on initializing a value function to 1 everywhere, then letting the value decay back toward zero as the state space is explored. Actually, to be more precise, the paper shows that the logarithm of E-Values can be thought of as a generalization of visit counters, with propagation of the values along state-action pairs. This paper presents an exploration method for model-free RL that generalizes the counter-based exploration bonus methods and takes into account long term exploratory value of actions rather than a single step look-ahead. The method presented in the paper trains a parallel "E-value" MDP, with initial value of 1 for all state-action pairs. Several of the cited papers use counters to determine which states are "known" and then solve an MDP to direct exploration past immediate outcomes.
The comparison with relative gain of bootstrap wrt ensemble of policies still needs more thorough experimentation, but the approach is novel and as the authors point out, does improve continually with better Contextual Bandit algorithms. This, and also a bigger discussion on prior bandit learning methods like LOLS will help under the context for why we're performing the reduction stated in the paper. The work is fairly novel in its approach, combining a learned reward estimator with a contextual bandit algorithm for exploration/exploitation. The key specificity of this algorithm is its ability to deal with the credit assignment problem by learning automatically a progressive "reward shaping" (the residual losses) from a feedback that is only provided at the end of the epochs. The authors propose a new episodic reinforcement learning algorithm based on contextual bandit oracles.
Original Review: The paper proposes a technique for quantizing the weights of a neural network, with bit-depth/precision varying on a per-parameter basis. In the end, while do like the general idea of utilizing the gradient to identify how sensitive the model might be to quantization of various parameters, there are significant clarity issues in the paper, I am a bit uneasy about some of the compression results claimed without clearer description of the bookkeeping, and I don't believe an approach of this kind has any significant practical relevance for saving runtime memory or compute resources. ----------------- ORIGINAL REVIEW: The paper proposes a method for quantizing neural networks that allows weights to be quantized with different precision depending on their importance, taking into account the loss. This paper addresses a very relevant topic, because in limited resources there is a constrain in memory and computational power, which can be tackled by quantizing the weights of the network. It would be interesting to know an analogy, for instance, saying that this adaptive compression in memory would be equivalent to quantizing all weights with n bits. I now think the paper does just enough to warrant acceptance, although I remain a bit concerned that since the benefits are only achievable with customized hardware, the relevance/applicability of the work is somewhat limited.
A major strength of the result is that it can work for general continuous distributions and does not really rely on the input distribution being Gaussian; the main weakness is that some of the distribution dependent quantities are not very intuitive, and the alignment requirement might be very high. I am still not convinced by the convolution case (which is the main point of this paper), as even though it does not require Gaussian input (a major plus), it still seems very far from "general distribution".
[Reviewed on January 12th] This article applies the notion of "conceptors" -- a form of regulariser introduced by the same author a few years ago, exhibiting appealing boolean logic pseudo-operations -- to prevent forgetting in continual learning,more precisely in the training of neural networks on sequential tasks. What is missing is the following: I think that without any additional effort, a network can learn a new task in parallel to other task, or some other techniques may be used which are not bound to any algebraic methods. For example, the main method this article is compared to (EWC) had a very strong section on Reinforcement learning examples in the Atari framework, not only as an illustration but also as a motivation. This paper introduces a method for learning new tasks, without interfering previous tasks, using conceptors.
The paper by Arjovsky et al (2017) provided a framework based on the Wasserstein distance, a distance measure between probability distributions belonging to the class of so-called Integral Probability Metrics (IPMs).
Then, the author proposed a topic-specific question generation model by encoding the extracted topic using LSTM and a pre-decode technique that the second decoding is conditioned on the hidden representation of the first decoding result. In the proposed method, the authors first extract the topic based on the similarity of the target question token and answer token using word embedding. This paper proposed a topic-based question generation method, which requires the input of target topic in addition to the descriptive text. Add Comment The authors propose a scheme to generate questions based on some answer sentences, topics and question types. When the ground truth topic is provided, it's not fair to compare with the previous method, since knowing the similar word present in the answer will have great benefits to question generation. Authors claim to generate topic-specific questions, however, the dataset choice, experiments, and examples show that the generated questions are essentially keyword/key phrase-based. The authors proposed heuristic method to extract the topic and question type without further annotation. The authors performed the experiment on AQAD dataset, and show their performance achieve state-of-the-art result when using automatically generated topic, and perform better when using the ground truth topic. This paper presents a neural network-based approach to generate topic-specific questions with the motivation that topical questions are more meaningful in practical applications like real-world conversations. The proposed model can generate question with respect to different topic and pre-decode seems a useful trick.
These are crucial for understanding the contribution of the paper; while reading the paper, I assumed that the authors consider the case of a single hidden unit with K = 2 RELU activations (however, that complicated my understanding on how it compares with state of the art). Summary: The paper considers the problem of a single hidden layer neural network, with 2 RELU units (this is what I got from the paper In this paper the authors studied the theoretical properties of manifold descent approaches in a standard regression problem, whose regressor is a simple neural network. Leveraged by two recent results in global optimization, they showed that with a simple two-layer ReLU network with two hidden units, the problem with a standard MSE population loss function does not have spurious local minimum points. Contribution: As discussed in the literature review section, apart from previous results that studied the theoretical convergence properties for problems that involves a single hidden unit NN, this paper extends the convergence results to problems that involves NN with two hidden units. Based on the results by Lee et al, which shows that first order methods converge to local minimum solution (instead of saddle points), it can be concluded that the global minima of this problem can be found by any manifold descent techniques, including standard gradient descent methods. While I appreciate the technical contributions, in order to improve the readability of this paper, it would be great to see more motivations of the problem studied in this paper (even with simple examples).
Pros: Authors ask the question of convergence of optimization (ignoring generalization error): how "likely" is that an over-parameterized (d1d0 > N) single hidden layer binary classifier "find" a good (possibly over-fitted) local minimum. This paper studies the question: Why does SGD on deep network is often successful, despite the fact that the objective induces bad local minima? Secondly, as the authors aptly pointed out in the discussion section, this results doesn't mean neural networks will converge to good local minima because these bad local minimas can have a large basins of attraction. This result seems interesting, although it is clearly not sufficient to explain even the success on the setting studied in this paper, since the number of minima of a certain type does not correspond to the probability of the SGD ending in one: to estimate the latter, the size of each basin of attraction should be taken into account.
Summary: This paper studies the geometry of linear and neural networks and provides conditions under which the local minima of the loss are global minima for these non-convex problems. Summary: The paper focuses on the characterization of the landscape of deep neural networks; i.e., when and why local minima are global, what are the conditions for saddle critical points, etc. For example, Lemma 4 is not correct as written — an invertible mapping \\sigma is not necessarily locally open. For the linear, deep case, the paper corrects imprecisions in the previous work (Lu + Kawaguchi).
The model is based on both source/target syntax trees and performs an attentional encoder-decoder style network over the tree structure. This paper aims to translate source code from one programming language to another using a neural network architecture that maps trees to trees. Statements like "We are the first to consider employing neural network approaches towards tackling the problem [of translating between programming languages]" are obviously not true (surely many people have *considered* it), and they're particularly grating when the treatment of related  work is poor, as it is in this paper.
This paper is utilizing reinforcement learning to search new activation function. The author uses reinforcement learning to find new potential activation functions from a rich set of possible candidates. Authors propose a reinforcement learning based approach for finding a non-linearity by searching through combinations from a set of unary and binary operators. The search result is a new activation function named Swish function.
This paper presents a study of reinforcement learning methods applied to Erdos-Selfridge-Spencer games, a particular type of two-agent, zero-sum game. The authors describe the game and some of its properties, notably that there exists a tractable potential function that indicates optimal play for each player for every state of the board. This paper presents an adversarial combinatorial game: Erdos-Selfridge-Spencer attacker-defender game, with the goal to use it as a benchmark for reinforcement learning. In particular, I have the following concerns: • these games have optimal policies that are expressible as a linear model, meaning that if the architecture or updating of the learning algorithm is such that there is a bias towards exploring these parts of policy space, then they will perform better than more general algorithms. The paper presents Erdos-Selfridge-Spencer games as environments for investigating deep reinforcement learning algorithms. ◦ It is unclear whether 'incorrect actions' in the supervised learning evaluations, refer to non-optimal actions, or simply actions that do not preserve the dominance of the defender, e.g. both partitions may have potential >0.5 This relates to the "surprising" fact that "Reinforcement learning is better at playing the game, but does worse at predicting optimal moves.". [p7 Fig 6 and text] Here the authors are comparing how well agents select the optimal actions as compared to how close they are to the end of the game. I am also not aware of trying to use games with known potential functions/optimal moves as a way to study the performance of RL algorithms. In fact, there is a level of non-determinism in how the attacker policies are encoded which means that an optimal policy cannot be (even up to soft-max) expressed by the agent (as I read things the number of pieces chosen in level l is always chosen uniformly randomly). Reinforcement learning >> is better at playing the game, but does worse at predicting optimal moves. The defender network has to do this to the features of A and of B, and compare the values; the attacker (with the action space following theorem 3) has to do this for (at most) K progressive partitions. • As the authors state, this paper is an empirical evaluation, and the theorems presented are derived from earlier work.
The authors' Main Claim appears to be: "While common wisdom might suggest that prior knowledge about game semantics such as ladders are to be climbed, jumping on spikes is dangerous or the agent must fetch the key before reaching the door are crucial to human performance, we find that instead more general and high-level priors such as the world is composed of objects, object like entities are used as subgoals for exploration, and things that look the same, act the same are more critical." The authors interpret the fact that performance falls so much between conditions b and c to mean that human priors about "objects are special" are very important. The authors have to include RL agent in all their experiments to be able to dissociate what is due to human priors and what is due to the noise introduced in the game. The authors present a study of priors employed by humans in playing video games -- with a view to providing some direction for RL agents to be more human-like in their behaviour. It's also important to take into account how much work general priors about video game playing (games have goals, up jumps, there is basic physics) are doing here (the authors do this when they discuss versions of the game with different physics). This paper investigates human priors for playing video games. Here without semantic priors I would hypothesize that human performance would fall quite far (whereas with semantics people would be able to figure it out quite well). Thus, we could consider a weaker version of the claim: semantic priors are important but even in the absence of explicit semantic cues (note, this is different from having the wrong semantic cues as above) people can do a good job on the game. The authors study, using experiments, what aspects of human priors are the important parts. There are two ways to interpret the authors' main claim: the strong version would maintain that semantic priors aren't important at all. The problem here comes from an unclear definition of what the authors mean by an "object" so in revision I would like authors to clarify what precisely they mean by a prior about "the world is composed of objects" and how this particular experiment differentiates "object" from a more general prior about "video games have clearly defined goals, there are 4 clearly defined boxes here, let me try touching them." This would give at least some proxy to study the importance of priors about "how video games are generally constructed" rather than priors like "objects are special". They conduct a series of experiments that systematically elides visual cues that humans can use in order to reason about actions and goals in a platformer game that they have a high degree of control over.
Summary: In this paper the authors offer a new algorithm to detect cancer mutations from sequencing cell free DNA (cfDNA). In this paper the author propose a CNN based solution for somatic mutation calling at ultra low allele frequencies. The algorithm for learning the context of sequencing reads compared to true mutations is based on a multi layered CNN, with 2/3bp long filters to capture di and trinucleotide frequencies, and a fully connected layer to a softmax function at the top. his paper proposes a deep learning framework to predict somatic mutations at extremely low frequencies which occurs in detecting tumor from cell-free DNA. The data is based on mutations in 4 patients with lung cancer for which they have a sample both directly from the tumor and from a healthy region. The authors suggest to overcome this problem by training an algorithm that will identify the sequence context that characterize sequencing errors from true mutations. The authors claim the reduced performance show they are learning lung cancer-specific context. We also liked the thoughtful construction of the network and way the reference, the read, the CIGAR and the base quality were all combined as multi channels to make the network learn the discriminative features of from the context. The idea is that in the sample being sequenced there would also be circulating tumor DNA (ctDNA) so such mutations could be captured in the sequencing reads. Using matched samples of tumor and normal from the patients is also a nice idea to mimic cfDNA data. Finally, performance itself did not seem to improve significantly compared to previous methods/simple filters, and the novelty in terms of ML and insights about learning representations seemed limited. If the entire point is to classify mutations versus errors it would make sense to combine their read based calls from multiple reads per mutations (if more than a single read for that mutation is available) - but the authors do not discuss/try that. The tackled problem is a hard task in computational biology, and the proposed solution Kittyhawk, although designed with very standard ingredients (several layers of CNN inspired to the VGG structure), seems to be very effective on both the shown datasets. Since we know nothing about all these samples it may very well be that that are learning technical artifacts related to their specific batch of 4 patients.
Experiments on artificial bars data and natural image patch datasets compare several variants of the proposed method, while varying a few EA method substeps such as selecting parents by fitness or randomly, including crossover or not, or using generic or specialized mutation rates. The paper presents a combination of evolutionary computation (EC) and variational EM for models with binary latent variables represented via a particle-based approximation. I don't think the paper succeeded very well at this: there were no comparisons to non-EA algorithms, or to approaches that use EA in the "outer loop" as above.
Although the author claim the novelty as adding noise to the discriminator, it seems to me that at least for the RBF case it just does the following: 1. This manuscript explores the idea of adding noise to the adversary's play in GAN dynamics over an RKHS. ==== original review === The paper describes a generative model that replaces the GAN loss in the adversarial auto-encoder with MMD loss.
This paper introduces a comparison between several approaches for evaluating GANs. The authors consider the setting of a pre-trained image models as generic representations of generated and real images to be compared. I think this paper tackles an interesting and important problem, what metrics are preferred for evaluating GANs. In particular, the authors showed that Inception Score, which is one of the most popular metric, is actually not preferred for several reasons. Appendix G in https://arxiv.org/pdf/1706.04987.pdf) Do you think it would be useful to compare other generative models (e.g. VAEs) using these evaluation metrics? The paper describes an empirical evaluation of some of the most common metrics to evaluate GANs (inception score, mode score, kernel MMD, Wasserstein distance and LOO accuracy). In the paper, the authors discuss several GAN evaluation metrics. Overall, I think this paper is worthy for acceptance as several GAN methods are proposed and good evaluation metrics are needed for further improvements of the research field. The paper evaluates popular GAN evaluation metrics to better understand their properties. Specifically, the authors pointed out some desirable properties that GANS evaluation metrics should satisfy. Given that the underlying application is image generation, the authors move from a pixel representation of images to using the feature representation given by a pre-trained ResNet, which is key in their results and further comparisons. - The authors implicitly contradict the argument of Theis et al against monolithic evaluation metrics for generative models, but this is not strongly supported. Cons -It is not clear why GANs are the only generative model considered The result, comparing data distributions and the distribution of the generator would be the preferred choice (that can be attained by Kernel MMD and 1-NN classifier), seems to be reasonable. This paper has some interesting insights and a few ideas of how to validate an evaluation method. -The evaluations rely on using a pre-trained imagenet model as a representation.
The authors propose a method for graph classification by combining graph kernels and CNNs. In a first step patches are extracted via community detection algorithms. The paper is well written, proposes an interesting and original idea, provides experiments with real graph datasets from two domains, bioinformatics and social sciences, and a comparison with SoA algorithms both graph kernels and other deep learning architectures. Moreover, although the authors claim that typical graph kernel methods are two-stage approached decoupling representation from learning, their proposal also folds into that respect, as representation is achieved in the preprocessing step of patching extractions and normalization, while learning is achieved by the CNN. * The originality is not high as the application of neural networks for graph classification has already been studied elsewhere and the proposed method is a direct combination of three existing methods, community detection, graph kernels, and CNNs. This paper proposes a graph classification method by integrating three techniques, community detection, graph kernels, and CNNs.
The proposed architecture could identify the 'key' states through assigning higher weights for important states, and applied reservoir sampling to control write and read on memory. This paper considers a new way to incorporate episodic memory with shallow-neural-nets RL using reservoir sampling. For the latter question, the authors propose using a "query network" that based on the current state, pulls out one state from the memory according to certain probability distribution. The comparisons between this episodic approach and recurrent neural net with basic GRU memory show the advantage of proposed algorithm.
This paper proposes an activation function, called displaced ReLU (DReLU), to improve the performance of CNNs that use batch normalization. The key argument authors present against ReLU+BN is the fact that using ReLU after BN skews the values resulting in non-normalized activations. Overall, I don't think this paper meet *CONF*'s novelty standard, although the authors present some good numbers, but they are not convincing. Although the BN paper suggests using BN before non-linearity many articles have been using BN after non-linearity which then gives normalized activations (https://github.com/ducha-aiki/caffenet-benchmark/blob/master/batchnorm.md) and also better overall performance. As a result, the activations outputted by DReLU can have a mean closer to 0 and a variance closer to 1 than the standard ReLU. I encourage the authors to validate their claims against simple approach of using BN after non-linearity. The proposed method shows encouraging results in a controlled setting (i.e., all other units, like dropout, are removed). This paper describes DReLU, a shift version of ReLU.
This paper presents a methodology to allow us to be able to measure uncertainty of the deep neural network predictions, and then apply explore-exploit algorithms such as UCB to obtain better performance in online content recommendation systems. Moreover the given strategy would achieve a linear regret if used as described in the paper which is not desirable for bandits algorithms (smallest counter example with two arms following a Bernouilli with different parameters if the best arms generates two zero in a row at the beginning, it is now stuck with a zero mean and zero variance estimate). In the paper "DEEP DENSITY NETWORKS AND UNCERTAINTY IN RECOMMENDER SYSTEMS", the authors propose a novel neural architecture for online recommendation. The experiments only concern two slightly different versions of the proposed algorithm in order to show the importance of the deconvolution of both considered noises, but nothing indicates that the model performs fairly well compared to existing approaches. My main doubt comes from Section 4.2.1, as I am not sure how exactly the two subnets fed into MDN to produce both mean and variance, through another gaussian mixture model. The only positioning argument that is given in that section is the final sentence "In this paper we model measurement noise using a Gaussian model and combine it with a MDN". The state of the art section is very confusing, with works given in a random order, without any clear explanation about the limits of the existing works in the context of the task addressed in the paper. The defended idea is to use the context to fit a mixture of Gaussian with a NN and to assume that the noise could be additively split into two terms. The idea is interesting but maybe not pushed far enough in the paper: *At fixed context x, assuming that the error is a function of the average reward u and of the number of displays r of the context could be a constant could be a little bit more supported (this is a variance explanation that could be tested statistically, or the shape of this 2D function f(u,r) could be plot to exhibit its regularity). Also, it would have been useful to compare ot to other neural models dealing with uncertainty (some of them having been applied to bandit problems- e.g., Blundell et al.
This paper contributes to the growing literature on depth separations in neural network, showing cases where depth is provably needed to express certain functions. Specifically, the paper shows that there are functions on R^d that can be approximated well by a depth-3 sigmoidal network with poly(d) weights, that cannot be approximated by a depth-2 sigmoidal network with poly(d) weights, and with respect to any input distributions with sufficiently large density in some part of the domain. The paper proves the separation by constructing a very specific function that cannot be approximated by 2-layer networks.
There are a few ideas the paper discusses: (1) compared to pruning weight matrices and making them sparse, block diagonal matrices are more efficient since they utilize level 3 BLAS rather than sparse operations which have significant overhead and are not "worth it" until the matrix is extremely sparse. The paper proposes to make the inner layers in a neural network be block diagonal, mainly as an alternative to pruning. This is a mostly experimental paper which evaluates the capabilities of neural networks with weight matrices that are block diagonal.
Adding supervision on execution traces in ∂NCM improves performance over NTM and NRAM which are trained end-to-end from input/output examples only. The observation that adding additional forms of supervision through execution traces improves generalization may be unsurprising, but from what I understand the main contribution of this paper lies in the abstraction of existing neural abstract machines to ∂NCM.
One analysis that would have helped convince me is a comparison to an equivalent non-grounded deep learning model (e.g., a CNN trained to make equivalent classifications), and show how this would not help us understand human behavior. The analyses are motivated by results from cognitive and developmental psychology, exploring questions such as whether agents develop biases for shape/color, the difficulty of learning negation, the impact of curriculum format, and how representations at different levels of abstraction are acquired. They examined a few key phenomena: shape/color bias, learning negation concepts, incremental learning, and how learning affects the representation of objects via attention-like processes. The authors used situated versions of human language learning tasks as simulation environments to test a CNN + LSTM deep learning network. 4.1 Word learning biases This experiment shows that, when an agent is trained on shapes only, it will exhibit a shape bias when tested on new shapes and colors. I was impressed by the range of phenomena they tackled and their analyses were informative in understanding the behavior of deep learning models This paper presents an analysis of an agent trained to follow linguistic commands in a 3D environment. Table 3: indicates is: indicates if This paper presents an analysis of the properties of agents who learn grounded language through reinforcement learning in a simple environment that combines verbal instruction with visual information. The crucial question, here, would be whether, when an agent is trained in a naturalistic environment (i.e., where distributions of colors, shapes and other properties reflect those encountered by biological agents), it would show a human-like shape bias. Developing methods that enable humans to understand how deep learning models solve problems is an important problem for many reasons (e.g., usability of models for science, ethical concerns) that has captured the interest of a wide range of researchers. The behaviour of the agent is analyzed by means of a set of "psycholinguistic" experiments probing what it learned, and by inspection of its visual component through an attentional mechanism.
This submission claims that: [a] "[based on the critiqued paper] one might assume that DRL-based algorithms are able to 'learn to navigate' and are thus ready to replace classical mapping and path-planning algorithms", More worryingly, when observing that the method of (Mirowski et al, 2017) may not generalize to unseen environments in claim [c], the authors of this submission seem to confuse navigation, cartography and SLAM, and attribute to that work claims that were never made in the first place, using a straw man argument. It seems, as the authors also claim in [b], that the work of (Mirowski et al, 2017), which was about navigation in known environments, actually is repeatable. The (Mirowski et al, 2016) paper shows that a neural network-based agent with LSTM-based memory and auxiliary tasks such as depth map prediction can learn to navigate in fixed environments (3D mazes) with a fixed goal position (what they call "static maze"), and in fixed mazes with changing goal environments (what they call "environments with dynamic elements" or "random goal mazes").
Based on experiments using CIFAR10, the authors show that adversarial training is effective in protecting against "shared" adversarial perturbation, in particular against universal perturbation. This paper analyses adversarial training and its effect on universal adversarial examples as well as standard (basic iteration) adversarial examples. Summary: This paper empirically studies adversarial perturbations dx and what the effects are of adversarial training (AT) with respect to shared (dx fools for many x) and singular (only for a single x) perturbations. - Singular perturbations are easily detected by a detector model, as such perturbations don't change much when applying AT.
Using the ground work for each proposes a new competitive super resolution technique using CNNs. Overall I liked authors' endeavors bringing together different fields of research addressing similar issues. The positive aspects of this work is that the use of two neural networks in tandem for this task may be interesting, and the authors attempt to discuss the network's behavior by drawing relations to successful sparsity-based super-resolution. For example different network architecture figures (training/testing for CNNs) could be used to explain in a compact way instead of plain text.
To reduce the memory required for training, the authors also propose a path-wise training procedure based on the independent convolution paths of CrescendoNet. The experimental results on CIFAR-10, CIFAR-100 and SVHN show that CrescendoNet outperforms most of the networks without residual connections. In this paper, the authors propose  a new network architecture, CrescendoNet, which is a simple stack of building blocks without residual connections. The authors claim that "Through our analysis and experiments, we note that the implicit ensemble behavior of CrescendoNet leads to high performance". The paper presents a new CNN architecture: CrescendoNet. It does not have skip connections yet performs quite well.
- While the motivation is that classes have different complexities to learn and hence you might want each base model to focus on different classes, it is not clear why this methods should be better than normal boosting: if a class is more difficult, it's expected that their samples will have higher weights and hence the next base model will focus more on them. -  "to replace the softmax error function (used in deep learning)": I don't think we have softmax error function In conventional boosting methods, one puts a weight on each sample. This paper instead designed a new boosting method which puts large weights on the category with large error in this round.
- While the LSTM baseline matches the results of Le et al., later work such as Recurrent Batch Normalization or Unitary Evolution RNN have demonstrated much better performance with a vanilla LSTM on those tasks (outperforming both IRNN and RIN). Summary: The authors present a simple variation of vanilla recurrent neural networks, which use ReLU hiddens and a fixed identity matrix that is added to the hidden-to-hidden weight matrix.
Review Summary: The primary claim that there is "a strong correlation between small generalization errors and high learnability" is correct and supported by evidence, but it doesn't provide much insight for the questions posed at the beginning of the paper or for a general better understanding of theoretical deep learning. More importantly, this relationship between test accuracy and learnability doesn't answer the original question Q2 posed: "Do larger neural networks learn simpler patterns compared to neural networks when trained on real data". Other results presented in the paper are puzzling and require further experimentation and discussion, such as the trend that the learnability of shallow networks on random data is much higher than 10%, as discussed at the bottom of page 4. -As suggested in the final sentence of the discussion, it would be nice if conclusions drawn from the learnability experiments done in this paper were applied to the design new networks which better generalize Summary: This paper presents very nice experiments comparing the complexity of various different neural networks using the notion of "learnability" Significance:  I find the results in this paper to be quite significant, and to provide a new way of understanding why deep neural networks generalize. The paper suggests that the learnability of a model is a good measure of how simple the function learned by that model is --- furthermore, it shows that this notion of learnability correlates well (across extensive experiments) with the test accuracy of the model.
1. This paper proposes a deep neural network compression method by maintaining the accuracy of deep models using a hyper-parameter. This is unfortunate because I believe this method, which takes as input a large complex network and compresses it so the loss in accuracy is small, would be really appealing to companies who are resource constrained but want to use neural network models.
* https://arxiv.org/pdf/cond-mat/9805073.pdf The paper makes a mathematical analogy between deep neural networks and quantum field theory, and claims that this explains a large number of empirically observed phenomena. * Three important results are stated as "theorem", with a statement like "Deep feedforward networks learn by breaking symmetries" proven in 5 lines, with no formal mathematics.
* results are only partially motivated and analyzed This paper proposed some new energy function in the BEGAN (boundary equilibrium GAN framework), including l_1 score, Gradient magnitude similarity score, and chrominance score, which are motivated and borrowed from the image quality assessment techniques. Quick summary: This paper proposes an energy based formulation to the BEGAN model and modifies it to include an image quality assessment based term. experiments on the using different hyper-parameters of the energy function, as well as visual inspections on the quality of the learned images, are presented. As a result I feel that the title and the claims in the paper are somewhat misleading and premature: that the proposed techniques improves the training and evaluation of energy based gans. Summary: The paper extends the the recently proposed Boundary Equilibrium Generative Adversarial Networks (BEGANs), with the hope of generating images which are more realistic.
This paper presents a set of regularizers which aims for manipulating the statistical properties like sparsity, variance and covariance. The paper does a good job of setting up and comparing empirical performance of various regularizers (penalties on weights and penalties on hidden unit representations) and compares results against a baseline. 1. Summary The authors of the paper compare the learning of representations in DNNs with Shannons channel coding theory, which deals with reliably sending information through channels.
The authors propose reducing the number of parameters learned by a deep network by setting up sparse connection weights in classification layers. Minor Second line of Section 2.1: "lesser" -> less or fewer This paper examines sparse connection patterns in upper layers of convolutional image classification networks. Detailed comments and questions: The distribution of connections in "windows" are first described to correspond to a sort of semi-random spatial downsampling, to get different views distributed over the full image. While it seems clear in general that many of the connections are not needed and can be made sparse (Figures 1 and 2), I found many parts of this paper fairly confusing, both in how it achieves its objectives, as well as much of the notation and method descriptions. It was not clear to me why scatter (the way it is defined in the paper) would be a useful performance proxy anywhere but the first classification layer.
The authors present a solid overview of unsupervised metrics for NLG, and perform a correlation analysis between these metrics and human evaluation scores on two task-oriented dialog generation datasets using three LSTM-based models. The authors here show that the performance of various NN models as measured by automatic metrics like BLEU and METEOR is correlated with human eval. This paper's main thesis is that automatic metrics like BLEU, ROUGE, or METEOR is suitable for task-oriented natural language generation (NLG). 1) This paper conducts an empirical study of different unsupervised metrics' correlations in task-oriented dialogue generation.
Summary: The paper proposes a self-play model for goal oriented dialog generation, aiming to enforce a stronger coupling between the task reward and the language model.
The paper proposes a method for few-shot learning using a new image representation called visual concept embedding. This latter paper showed how to extract from a CNN some clustered representations of the features of the internal layers of the network, working on a large training dataset. I believe this paper needs to focus on the working of the VCs for few-shot experiments, showing the influences of some of the choices (layer, network layout, smoothing, clustering, etc). The paper adds few operations after the pipeline for obtaining visual concepts from CNN as proposed by Wang et al. Using the visual concept embedding, two simple methods are used for few-shot learning: a nearest neighbor method and a probabilistic model with Bernoulli distributions.
either in terms of FLOPS or measured times This paper applies gated convolutional neural networks [1] to speech recognition, using the training criterion ASG [2]. Arguments in section 2.3 are weak because, again, all other grapheme-based end-to-end systems have the same benefit as CTC and ASG. The authors argue that ASG is better than CTC in section 2.3.1 because it does not use the blank symbol and can be faster during decoding. It would be even better to compare CTC and ASG to seq2seq-based models with the same gated convnet. However, all of the other grapheme-based end-to-end systems enjoy the same benefit as CTC and ASG. There is no reason to believe that ASG can be faster than CTC in both training and decoding. The paper can be significantly improved if the authors compare the performance and decoding speed against CTC with the same gated convnet.
The paper proposes a technique for training quantized neural networks, where the precision (number of bits) varies per layer and is learned in an end-to-end fashion. We'd like the number of bits parameter to trade off between accuracy (at least in terms of quantization error, and ideally overall loss as well) and precision. This paper proposes to optimize neural networks considering the three different terms: original loss function, quantization error and the sum of bits.
This paper provides a systematic study of data augmentation in image classification problems with deep neural networks and argues that data augmentation could replace some common explicit regularizers like the weight decay and dropout. The paper proposes data augmentation as an alternative to commonly used regularisation techniques like weight decay and dropout, and shows for a few reference models / tasks that the same generalization performance can be achieved using only data augmentation.
If the current paper really wants to make denotational semantics part of the core claim, I think it would help to talk about the representational implications in more detail---what kinds of things can and can't you model once you've committed to set-like bottlenecks between modules? This paper presents a model for visual question answering that can learn both parameters and structure predictors for a modular neural network, without supervised structures or assistance from a syntactic parser. On a slightly modified set of structured scene representations from the CLEVR dataset, this approach outperforms two LSTM baselines with incomplete information, as well as an implementation of Relation Networks. We know that more traditional semantic parsing approaches with real logical forms are capable of getting excellent accuracy on structured QA tasks with a lot more complexity and less data than this one.
This paper proposes a framework for learning to search, MCTSNet. The paper proposes an idea to integrate simulation-based planning into a neural network. The authors propose to train this using policy gradient in which data of optimal state-action pairs is generated by a standard MCTS with a large number of simulations. If I understand them correctly, the comparison is between a neural network that has been learned on 250,000 trajectories of 60 steps each where each step is decided by a ground truth close-to-optimal algorithm, say MCTS with 1000 rollouts (is this mentioned in the paper). The proposed method incorporates simulation-based search inside a neural network by expanding, evaluating and backing-up a vector-embedding. I suspect that generating the training data and learning the model takes an enormous amount of CPU time, while 25 MCTS rollouts can probably be done in a second or two. This is different from standard planning when a backup is handled with more simulations, the Q value function will have better statistics, and then get smaller regrets (see 4(b) in Algorithm 1). Would it be fair to have a baseline that learns the MCTS coefficient on the training data? This paper designs a deep learning architecture that mimics the structure of the well-known MCTS algorithm. The proposed method allows each component of MCTS to be rich and learnable, and allows the joint training of the evaluation network, backup network, and simulation policy in optimizing the MCTS network. The idea is to represent all operators such as backup, action selection and node initialisation by neural networks. - It looks like after training MCTSnet with a massive amount of data from another MCTS, MTCSnet algorithm as in Algorithm 2 will not do very much more planning yet.
- provides fairly extensive experimental comparison of their method and 3 others (Reluplex, Planet, MIP) on 2 existing benchmarks and a new synthetic one Summary: This paper: - provides a compehensive review of existing techniques for verifying properties of neural networks
Summary: This paper proposes an approach to learn embeddings for structured datasets i.e. datasets which have heterogeneous set of features, as opposed to just words or just pixels. Comments: The paper is well written and addresses an important problem of learning word embeddings when there is inherent structure in the feature space. The paper proposes an approach called Feat2vec that relies on Structured Deep-In Factorization machines-- a paper that is concurrently under review at *CONF*2018, which I haven't read in depth. The paper compares against a Word2vec baseline that pools all the heterogeneous content learns just one set of embeddings. The structured deep-in factorization machines allow higher-level interactions in embedding learning which allows the authors to learn embeddings for heterogeneous set of features. This paper provides a clean way of learning embeddings for structured features that can be discrete -- indicating presence / absence of a certain quality. Finally, the most striking flaw of this paper is the lack of references to previous works on word embeddings and feature representation, I would suggest the author check and compare themselves with previous work on this topic. The authors introduced a new compatibility function between features and, as in the skipgram approach, they propose a variation of negative sampling to deal with structured features.
Two proposals in the paper are: (1) Using a learning rate decay scheme that is fixed relative to the number of epochs used in training, and This paper proposes a fast way to learn convolutional features that later can be used with any classifier. The acceleration of the training comes from a reduced number of training epocs and a specific schedule decay of the learning rate. I believe re-thinking new learning rate schedules is interesting, however I recommend the rejection of this paper. Essentially, if I understand correctly, this paper is proposing to prematurely stop training an use the intermediate feature to train a conventional classifier (which is not that away from the softmax classifier that CNNs usually use).
The paper also presents some visualizations the similarity structure of the learned representations and proposes a window-based method for processing the data. Main issues: No related works (such as those using RNN for time series analysis or clustering of time series data streams etc.) were described by the paper, no baselines were used in the comparison evaluations, and no settings/details were provided in the experiment section. This paper proposes a strategy that is inspired by the recurrent auto-encoder model, such that clustering of multidimensional time series data can be performed based on the context vectors generated by the encoding process. For example, the authors claim that the proposed LSTM-based autoencoder networks can be natively scaled up to data with very high dimensionality. The authors show that for their application, better performance is obtained when the network is only trained to reconstruct a subset of the data measurements.
This paper presents an interesting approach to identify substructural features of molecular graphs contributing to the target task (e.g. predicting toxicity). Both parts are based on conv nets for molecular graphs, and this framework is a kind of 'self-supervised' scheme compared to the standard situations that the environment provides rewards. The algorithm first builds two conv nets for molecular graphs, one is for searching relevant substructures (policy improvement), and another for evaluating the contribution of selected substructures to the target task (policy evaluation). It would be unconvincing that the proposed neural nets approach fits to this hard combinatorial task rather than these existing (mostly exact) methods. The predictive model is an interesting two-step approach where important atoms of the molecule are added one-by-one with a reward given by a second Q-network that learns how well we can solve the prediction problem with the given set of atoms. Cons: - it would be a bit unconvincing that identifying 'hard selection' is better suited for neural nets, rather than many existing exact methods (without using neural networks).
The authors propose a DNN, called subspace network, for nonlinear multi-task censored regression problem. Conclusion: Though with a quite novel idea on solving multi-task censored regression problem, the experiments conducted on synthetic data and real data are not convincing enough to ensure the contribution of the Subspace Network. This paper presents a new multi-task network architecture within which low-rank parameter spaces were found using matrix factorization. They compare the proposed SN with other traditional approaches on a very small data  set with 670 samples and 138 features. This work proposes a multi task learning framework for the modeling of clinical data in neurodegenerative diseases. Differently from previous applications of machine learning in neurodegeneration modeling, the proposed approach models the clinical data accounting for the bounded nature of cognitive tests scores. 3. In synthetic data experiments (Table1), only small margins could be observed between SN, f-MLP and rf-MLP, and only Layer 1 of SN performs better above all others. 8. The performance on One-Layer Subspace Network (with only the input features) could be added. The authors are generating the synthetic data according to the model, and it is thus not surprising that they managed to obtain the best performance.
They propose directly optimizing the time-dependent discrimination index using a siamese survival network. The paper entitled 'Siamese Survival Analysis' reports an application of a deep learning to three cases of competing risk survival analysis. The authors optimize for the c-index by minimizing a loss function driven by the cumulative risk of competing risk m and correct ordering of comparable pairs. While the idea of optimizing directly for the c-index directly is a good one (with an approximation and with useful complementary loss function terms), the paper leaves something to be desired in quality and clarity. It is unclear why the authors solution is able to solve such an issue, specially given the modest reported gains in comparison with several competitive baselines.
The paper presents an approach to do task aware distillation, task-specific pruning and specialized cascades. An ideal story would for a paper like this would be: here are some complementary ideas that we can combine in non-obvious ways for superlinear benefits, e.g. it turns out that by distilling into a cascade in some end-to-end fashion, you can get much better accuracy vs. - Ba & Caruana 2014 This paper presents three different techniques for model specialization, i.e. adapting a pretrained network to a more specific task and reduce its computational cost while maintaining the performance.
When c is an adversarially trained discriminator network, the experts learn to model the different transformations that map altered images back to unaltered ones. This paper describes a setting in which a system learns collections of inverse-mapping functions that transform altered inputs to their unaltered "canonical" counterparts, while only needing unassociated and separate sets of examples of each at training time. 2) The authors only run experiments on the MNIST data, where 1) the mechanisms are simulated and relatively simple, and 2) samples from the canonical distribution are also available. Experiments on MNIST data shows that in the end of training, each expert wins almost all samples from one transformation and no other, which confirms that each expert model a single inverse transformation.
To me this greatly mutes the value of this result, and the contribution of the paper overall, because local minimum are *very* likely to occur on the boundary between activation regions at non-differentiable points (e.g. as in Figure 2). The main result is that every local minimum of the total surface is a global minimum of the region where the ReLU activations corresponding to each sample do not change. For example, if there is a differentiable valley in L_f that terminates on the boundary of an activation region, then this phenomena could occur, since a local-minima-creating boundary in L_f might just lead to a saddle point in L_gA.
On both of the synthetic tasks --- which involve predicting gaussians --- the proposed approach can fit the data reasonably using far fewer parameters than the baselines, although 3BE does achieve better overall performance. On a real world task that involves predicting a distribution of future stock market prices from multiple input stock marked distributions, the proposed approach significantly outperforms both baselines. It's not clear how much value there is adding yet another distribution to distribution regression approach, this time with neural networks, without some pretty strong motivation (which seems to be lacking), as well as experiments. This is an intriguing paper on running regressions on probability distributions: i.e. a target distribution is expressed as a function of input distributions.
A few contributions are claimed in this paper: (1) differentiable decision tree which allows for gradient-based optimization; (2) supervised VAE where class-specific Gaussian prior is used for the probabilistic decoder in the VAE; (3) combination of these two models. The paper tries to build an interpretable and accurate classifier via stacking a supervised VAE (SVAE) and a differentiable decision tree (DTT). Summary This paper proposes a hybrid model (C+VAE)---a variational autoencoder (VAE) composed with a differentiable decision tree (DDT)---and an accompanying training scheme.
The authors propose a particular variance regularizer on activations and connect it to the conditional entropy of the activation given the class label. The data is then partitioned within a batch based on this Z value, and monte carlo sampling is used to estimate the variance of Y conditioned on Z, but it's really unclear as to how this behaves as a regularizer, how the z is sampled for each monte carlo run, and how this influences the gradient. The main one is that the connection between conditional entropy and the proposed variance regularizer seems tenuous.
The primary issues I have with this work are threefold:  (i) The paper is not suitably organized/condensed for an *CONF* submission, (ii) the presentation quality is quite low, to the extent that clarity and proper understanding are jeopardized, and (iii) the novelty is limited. The appropriateness of using additional pages over the recommended length will be judged by reviewers."  In the present submission, the first 8+ pages contain minimal new material, just various background topics and modified VAE update rules to account for learning noise parameters via basic EM algorithm techniques. In contrast, this submission suggests either treating sigma^2 as a trainable parameter, or else introducing a more flexible zero-mean mixture-of-Gaussians (MoG) model for the decoder noise.
b) In Section 2 the authors state "Imposing extra data hypothesis actually violates the ME principle and degrades the model to non-ME model." … Statements like this need to be made much clearer, since imposing feature expectation constraints (such as Eq.
- the evaluation should include a measure of the capacity of the architecture to : a) reconstruct perfectly the input b) denoise perturbations over node labels and additional/missing  edges This paper studies the problem of learning to generate graphs using deep learning methods. A downside to the algorithm is that it has complexity O(k^4) for graphs with k nodes, but the authors argue that this is not a problem when generating small graphs. - some of the main issues with graph generation are acknowledged (e.g. the problem of invariance to node permutation) and a solution is proposed (the binary assignment  matrix) The main issue with training models in this formulation is the alignment of the generated graph to the ground truth graph. To handle this, the paper proposes to use a simple graph  matching algorithm (Max Pooling Matching) to align nodes and edges. - notions for measuring the quality of the output graphs are of interest: here the authors propose some ways to use domain knowledge to check simple properties of molecular graphs First, the motivation for one-shot graph construction is not very strong: - I don't understand why the non-differentiability argued in (a) above is an issue. Even after doing so, the authors need to solve a matching problem to resolve the alignment issue. This work proposed an interesting graph generator using a variational autoencoder. I would have at least liked to see a comparison to a method that generated SMILES format in an autoregressive manner (similar to previous work on chemical graph generation), and would ideally have liked to see an attempt at solving the alignment problem within an autoregressive formulation (e.g., by greedily constructing the alignment as the graph was generated). Based on this motivation, the paper decides to generate a graph in "one shot", directly  outputting node and edge existence probabilities, and node attribute vectors. Strengths: - Generating graphs is an interesting problem, and the proposed approach seems like an easy-to-implement, mostly reasonable way of approaching the problem. The main challenges of generating graphs as opposed to text or images are said to be the following: (a) Graphs are discrete structures, and incrementally constructing them would lead to non-differentiability (I don't agree with this; see below) Even disregarding Johnson (2017), which the authors claim to be unaware of, I would consider approaches that generate SMILES format (like Gomez-Bombarelli et al) to be doing graph generation using deep learning. - many crucial elements  in graph generation are not dealt with: a) the adjacency matrix and the label tensors are not independent of each other, the notion of a graph is in itself a way to represent the 'relational links' between the various components The search space of small graph generation is usually very small, is there any other traditional methods can work on this problem? The authors propose a variational auto encoder architecture to generate graphs.
Summary of the paper: This paper presents a method, called \\alpha-DM (the authors used this name because they are using \\alpha-Divergence to measure the distance between two distributions), that addresses three important problems simultaneously: (a) Objective score discrepancy: i.e., in ML we minimize a cost function but we measure performance using something else, e.g., minimizing cross entropy and then measuring performance using BLEU score in Machine Translation (MT). The objective is based on alpha-divergence between the true input-output distribution q and the model distribution p. The model is trained on an empirical distribution whose points are sampled from the true distribution. Furthermore, in the same Section 2 the paper fails to mention that reinforcement learning training also does not completely correspond to the evaluation approach, at which stage greedy search or beam search is used. (b) Sampling distribution discrepancy: The model is trained using samples from true distribution but evaluated using samples from the learned distribution Then the authors present the results for machine translation task and also analysis of their proposed method. The paper does not argue why alpha divergence is better that the aforementioned combination method and also does not include it in the comparison. First, the model is *not* trained on the true distribution which is unknown. 2. In page 2, the line before the last line, "… resolbing problem" --> "… resolving problem" This paper considers a dichitomy between ML and RL based methods for sequence generation. An alpha-divergence formulation is considered to combine both methods. The new objective generalizes  Reward-Augmented Maximum Likelihood (RAML) and entropy-regularized Reinforcement Learning (RL), to which it presumably degenerates when alpha goes to 1 or to 0 respectively. In summary, I'm not convinced that the fact that ML optimizes a different objective than the blue score is a problem with the ML estimator. The performance of the proposed method is not significantly better than other models in MT task.
The authors then introduce a greedy algorithm that expands the different layers in a neural network until the metric indicates that additional features will end up not being used effectively. The authors propose an approach to dynamically adjust the feature map depth of a fully convolutional neural network.
The paper proposes an interesting alternative to recent approaches to learning from logged bandit feedback, and validates their contribution in a reasonable experimental comparison. 5.3: so that results more comparable In this paper the authors studied the problem of off-policy learning, in the bandit setting when a batch log of data generated by the baseline policy is given. In general, I think the problem studied in this paper is very interesting, and the topic of counterfactual learning, especially policy optimization with the use of offline and off-policy log data, is important.
The improved upper bound given in Theorem 1 appeared in SampTA 2017 - Mathematics of deep learning ``Notes on the number of linear regions of deep neural networks'' by Montufar. The paper also discusses the exact computation of the number of linear regions in small trained networks. Previous work [1], [2], has derived lower and upper bounds for the number of linear regions that a particular neural network architecture can have. Paper Summary: This paper looks at providing better bounds for the number of linear regions in the function represented by a deep neural network. [1] On the number of linear regions of Deep Neural Networks, 2014, Montufar, Pascanu, Cho, Bengio
A composite of transformations coupled with the LAM/RAM networks provides a highly expressive model for modelling arbitrary joint densities but retaining interpretable conditional structure. The authors propose to combine nonlinear bijective transformations and flexible density models for density estimation. Specifically, the authors examine models involving linear maps from past states (LAM) and recurrence relationships (RAM).
Once we train and achieve a network with best performance under this constraint, we take the sign of each weight (and leave them intact), and use the remaining n-1 bits of each weight in order to add some new connections to the network. The experiment is also limited to MNIST and fully connected neural networks.
This paper addresses multiple issues arising from the fact that commonly reported best model performance numbers are a single sample from a performance distribution. The method of choosing the best model under 'internal' cross-validation to take through to 'external' cross-validation against a second hold-out set should be regarded as one possible stochastic solution to the optimisation problem of hyper-parameter selection.
------ The paper presents smoothing probabilistic box embeddings with softplus functions, which make the optimization landscape continuous, while also presenting the theoretical background of the proposed method well. The key contribution of the paper is facilitating optimization of these models by gradient based methods, which eventually leads to improved accuracy on relevant benchmark data (on par or beyond SOTA). That is, transforming the original model constructed from indicator functions (hence difficult to optimize by gradient based method) to a smooth differentiable function by diffusing the landscape. As the authors find, the smoothed function leads to improved performance against SOTA on relevant benchmark data such as WordNet hypernymy, Flick caption entailment and MovieLnes market basket data. - The main thrust of section 5.2 is that smoothed box embeddings retain better performance with increasing numbers of negatives. This paper proposes a soft relaxation of the box lattice (BL) model of Vilnis et al. (Why would we expect the smoothed box model to handle unseen captions better?) The paper assumes some familiarity with the problem domain and existing works (there is not a lot of exposition for an unfamilar reader), but should be of strong interest to anyone working on embeddings or graph prediction.
The author defines a winning lottery ticket as a sparse subnetwork that can reaching the same performance of the original network when trained from scratch with the "original initialization". To explain the difficulty of training pruned networks from scratch or why training needs the overparameterized networks that make pruning necessary,  the authors propose a lottery ticket hypothesis: unpruned, randomly initialized NNs contain subnetworks that can be trained from scratch with similar generalization accuracy. The paper follows by proposing a method to find these winning tickets by pruning methods, which are typically used for compressing networks, and then proceed to test this hypothesis on several architectures and tasks. This paper proposes a conjecture to explain this phenomenon that the authors call "The Lottery Ticket Hypothesis":  large networks that can be trained successfully contain at initialization time small sub-networks — which are defined by both connectivity and the initial weights that the authors call "winning tickets" — that if trained separately for similar number of iterations could reach the same performance as the large network. The paper also conjectures that the reason large networks are more straightforward to train is that when randomly initialized large networks have more combinations for subnetworks which makes have a winning ticket more likely. The paper thoroughly investigates the existence of such "winning-tickets" on MNIST and CIFAR-10 on both, fully connected but also convolutional neural networks.
This work introduces SNIP, a simple way to prune neural network weights before training according to a specific criterion. - It is correct that the weights used to train the pruned model are possibly different from the ones used to compute the connection sensitivity. Given (variance scaled) initial weights, SNIP finds the architecturally important parameters in the network, then the pruned network is established and trained in the standard way. One genuinely perplexing result to me is that the method behaves better than random pruning, yet after selecting the salient neurons the weights can be reinitialized, as per the rebuttal: > # Initialization procedure The fact that it works is very surprising and again suggests that the method identifies constant background pixels rather than important weights. This is important - on MNIST each digit has a constant zero border, all connections to the border are not needed and can be trivially removed (one can crop the images to remove them for similar results). Original review: The paper presents an intriguing result in which a salient, small subset of weights can be selected even in untrained networks given sensible initialization defaults are used. Fundamentally, if you decouple weight pruning from initialization it also means that: - the first layer will be pruned out of connections to constant pixels (which is seen in the visualizations), this remains meaningful even after a reinitialization The experiments done are also extensive, as they cover a broad range of tasks: MNIST / CIFAR 10 classification with various architectures, ablation studies on the effects of different initialisations, visualisations of the pruning patterns and exploration of regularisation effects on a task involving fitting random labels. The sentence "Note that initializing neural networks is a random process, typically done using normal distribution with zero mean and a fixed variance." is wrong and artificially inflates the paper's contribution. My main concern is that paper differentiates between weights and connections (both terms are introduced on page iv to differentiate from earlier work). This initial step that identifies the connections to be pruned works off a mini-batch of data.
Based on the insight from the analysis in the supplementary materials, the authors propose a two-stage VAE which separate learning the a parsimonious representation of the low-dimensional (lower than the ambient dimension of the input space), and the training a second VAE to learn the unknown approximate posterior. Cons: - The title and general tone of the paper is too broad: it is only VAE models with Gaussian approximate posteriors and likelihoods. Also, I think *the reported FID scores alone may be considered as a significant enough contribution*, because to my knowledge this is the first paper significantly closing the gap between generative quality of GAN-based models and non-adversarial AE-based methods.
The key innovation of the article, compared to the aforementioned papers, lies on the idea of learning face/voice embeddings to maximise their ability to predict covariates, rather than by explicitly trying to optimise an objective related to cross-modal matching. Compared with similar work from Nagrani et al (2018) who generate paired inputs of voices and faces and train a network to classify if the pair is matched or not, the proposed method doesn't require paired inputs. Instead, the resulting embeddings are fed to a modality-agnostic, multiclass logistic regression classifier that aims to predict simple covariates such as gender, nationality or identity. Authors aim to reveal relevant dependencies between voice and image data (under a cross-modal matching framework) through common covariates (gender, ID, nationality). Key to the proposed approach, unlike related prior work, these modules are not directly trained on some particular form of the cross-modal matching task.
This paper discusses the effect of weight decay on the training of deep network models with and without batch normalization and when using first/second order optimization methods. The dominant generalization benefit due to weight decay comes from increasing the effective learning rate of parameters on which batch normalization is applied.
The main contribution is the model the problem as a time-dependent context and then use a directed information flow loss instead of the mutual information loss. The paper presents a learning-based method for learning the latent context codes from demonstrations along with a GAIL model. One of the main difference of this work in comparison to unsupervised segmentation models GMM or BP-AR-HMM is the fact that the options learned are composable. The proposed approach uses a pre-training step, based on a variational auto-encoder (VAE), to estimate latent variable sequences. A reasonable confirmation that the model indeed learns composition is to generate a trajectory for a sequence of latent code not seen in data. 2. The paper missed an important line of work which solves nearly the same problem -- option discovery and policy learning.
This paper proposes to use a Lanczos alogrithm, to get approximate decompositions of the graph Laplacian, which would facilitate the computation and learning of spectral features in graph convnets. The authors propose a novel method for learning graph convolutional networks. The paper under review builds useful insights and novel methods for graph convolutional networks, based on the Lanczos algorithm for efficient computations involving the graph Laplacian matrices induced by the neighbor edge structure of graph networks. In terms of presentation quality, the paper is clearly written, the proposed methods are well explained, and the notation is consistent. The main advantage of the proposed method as illustrated in particular by the experimental results in the citation network domain is its ability to generalize well in the presence of a small  amount of training data, which the authors attribute to its efficient capturing of both short- and long-range interactions.
This paper proposes an approach for automatic robot design based on Neural graph evolution. [Strengths]: This paper shows some promise when graph network-based controllers augmented with evolutionary algorithms. Detailed comments: - in the abstract you say that "NGE is the first algorithm that can automatically discover complex robotic graph structures". Given that, the novelty of the paper is fairly incremental as it uses NerveNet to evaluate fitness and ES for the main design search. The overall approach has a flavor of genetical algorithms, as it also performs evolutionary operations on the graph, but it also allows for a better mechanism for policy sharing across the different topologies, which is nice. Please include further description of the ES cost function and algorithm in the main body of the paper. The authors propose a scheme based on a graph representation of the robot structure, and a graph-neural-network as controllers. This paper uses graph network to train each morphology using RL. The cost of evaluating the function is typically more pressing, and as a result it is important to have algorithms that can converge within a small number of iterations/generations. - Sec 4.1:  would argue that computational cost is rarely a concern among evolutionary algorithms. - in the introduction you mention that automatic robot design had limited success. This expedites the score function evaluation improving the time complexity of the evolutionary process. [Summary]: This paper tackles the problem of automatic robot design. Bayesian optimization is used for gait optimization in robotics, and Genetic algorithms have been used for automatic robot design. - "to speed up and trade off between evaluating fitness and evolving new species" Unclear sentence. How would the given graph network compare to this? My main concern about the paper is that, currently, the experiments do not include any strong baseline (the ES currently is not a strong baseline, see comments below).
The authors use their derived formula for VRR to predict the minimum mantissa precision needed for accumulators for three well known networks: AlexNet, ResNet 32 and ResNet 18. The analysis is based on Variance Retention Ratio (VRR), and authors show the theoretical impact of reducing the number of bits in the floating point accumulator. For tightness analysis they present convergence results while perturbing the mantissa bits to less than those predicted by their formula, and show that it leads to more than 0.5% loss in the final test error of the network. The authors address this with  an analytical method to predict the number of mantissa bits needed for partial summations during the forward, delta and gradient computation ops for convolutional and fully connected layers. Quality and clarity: The paper presents a theoretical framework and method to determine the necessary number of bits in a deep learning networks.
Authors propose to augment a conditional GAN model with an unsupervised branch for spanning target manifold and show better performance than the conditional GAN in natural scene generation and face generation. The authors claim the generator of RoC-Gan will span the target manifold, even in the presence of large amounts of noise. 1.Similar idea of using an autoencoder as another branch to help image generation has been proposed in Ma et al.'s work. 2. In the paper authors claim that skip connection makes it harder to train the longer path, which is kind of contradictory to what is commonly done in tasks of image classification, semantic segmentation and depth estimation. My concern for using such data sets(the resolution of images is low and the distribution is simple)  is that: although the noise seems to corrupt most of the image, the distribution of the image is not complex, so the generative model can recover it easily. Although the paper demonstrates the robustness of their model with different experiments, most of them were not performed on deep neural networks and complicated data sets. - The theoretical analysis of the method relates RoC-GAN to the original GAN, rather than CGAN!
## Strengths - The paper is theoretically sound, the statement of the theorems are clear and the authors seem knowledgeable when bounding the generalization error via Rademacher complexity estimation. Then, {\\bf Theorem 1} provides an upper bound for the empirical Rademacher complexity of the class of 1-layer networks with hidden units of bounded \\textit{capacity} and \\textit{impact}. - in the final equation block, we have the Rademacher complexity of F_{W_2}, instead it should be F_{W^prime} It is shown empirically that common algorithms used in supervised learning (SGD) yield networks for which such upper bound decreases as the number of hidden units increases. Finally {\\bf Theorem 3} is presented, which provides a lower bound for the Rademacher complexity of a class of neural networks, and such bound is compared with existing lower bounds. Next, {\\bf Theorem 2} which is the main result, presents a new upper bound for the generalization error of 1-layer networks. An empirical comparison with existing generalization bounds is made and the presented bound is the only one that in practice decreases when the number of hidden units grows.
This paper presents and empirical and theoretical study of the convergence of asynchronous stochastic gradient descent training if there are delays due to the asynchronous part of it. In this work, they study the convergence behaviors of a wide array of ML models and algorithms under delayed updates, and propose a new convergence analysis of asynchronous SGD method for non-convex optimization.
This paper is not a great topic fit for *CONF*: it's primarily about a hand-designed performance metric for sequence labeling and a hierarchical Bayesian model with Gaussian observations and fit with Gibbs sampling in a full-batch setting. 2. the paper also proposes a new sequence labeling method based on inference in a hierarchical Bayesian model, focused on simultaneously labeling multiple sequences that have the same underlying procedure but with varying segment lengths.
Given that the main attraction of the paper is the potential for more performant word embeddings, I do not believe the work will have wide appeal to *CONF* attendees, because no evidence is provided that the features from the learned tensor, say [a, b, T*a*b], are more useful in downstream applications than [a,b] (one experiment in sentiment analysis is tried in the supplementary material with no compelling difference shown). The authors point out that others have observed that this form of compositionality does not leverage any information on the syntax of the pair (a,b), and the propose using a tensor contraction to model an additional multiplicative interaction between a and b, so they propose finding the word whose embedding is closest to a + b + T*a*b, where T is a tensor, and T*a*b denotes the vector obtained by contracting a and b with T. Unlike Arora's original work, the assumptions they make on their subject material are not supported enough, as in their lack of explanation of why linear addition of two word embeddings should be a bad idea for composition of the embedding vectors of two syntactically related words, and why the corrective term produced by their method makes this a good idea. They test this idea specifically on the use-case where (a,b) is an adjective,noun pair, and show that their form of compositionality outperforms weighted versions of additive compositionality in terms of spearman and pearson correlation with human judgements. - the 2017 Gittens, Achlioptas, Drineas ACL paper provides theory on the linear composition of some word embeddings The paper deals with further development of RAND-WALK model of Arora et al. The authors claim that the Arora's RAND-WALK model does not capture any syntactic information.
However, I have concerns about the presented data and the validity of rotation equivariance in modeling visual responses in general (below). In the next step, a rotational equivariant neural network architecture together with a sparse coding read-out layer is trained to predict the neuron responses from the stimuli.
Weaknesses: The dataset created here is entirely synthetic, and the paper only includes one single small real-world case; it seems like it would be easy to generate a larger and more varied real world dataset as well (possibly from the large literature of extant solved problems in workbooks). The dataset is then used to evaluate a number of recurrent models (LSTM, LSTM+attention, transformer); these are very powerful models for general sequence-sequence tasks, but they are not explicitly tailored to math problems. The results and discussions in the main part of the paper are too light in my opinion; the average model accuracy across modules is not an interesting metric at all, although it does show that the Transformer performs better than recurrent networks. This paper develops a framework for evaluating the ability of neural models on answering free-form mathematical problems. I'd like to see more of a discussion of *prior data sets* rather than papers proposing models for problems. This paper presents a new synthetic dataset to evaluate the mathematical reasoning ability of sequence-to-sequence models. The main contribution is a synthetically generated dataset that includes a variety of types and difficulties of math problems; it is both larger and more varied than previous datasets of this type. The insights from the analysis of the failure cases are intriguing, but it also points out that the neural networks models are not really performing mathematical reasoning since the generalization is very limited. But it's not clear to me why testing more sophisticated models that are tailored for math questions would *not* be useful. It would have been useful to compare the general models here with some specific math problem-focused ones as well. The authors try to do this with composition, which is good, but I am not sure whether that captures the real important thing - the ability to generalize, say learning to factorise single-variable polynomials and test it on factorising polynomials with multiple variables? Several models including LSTM, LSTM + Attention, Transformer are evaluated on the proposed dataset. In fact, assuming that such methods outperform general-purpose models, we could investigate why and where this is the case (in fact the proposed dataset is very useful for this).
This paper is positioned in the context of Bayesian GANs (Saatsci & Wilson 2017) which, by placing a posterior distribution over the generative and discriminative parameters, can potentially learn all the modes. ========= The paper extends Bayesian GANs by altering the generator and discriminator parameter likelihood distributions and their respective priors. The Bayesian GAN defines a posterior distribution for the generator that is proportional to the likelihood that the discriminator assigns to generated samples.
For instance, the experiments seem to indicate that generalizing density estimation from CIFAR training set to CIFAR test set is likely challenging and thus the models underfit the true data distribution, resulting in the simpler dataset (SVHN) having higher likelihood. Minor nitpick: There seems to be some space crunching going on via Latex margin and spacing hacks that the authors should ideally avoid :) This paper displays an occurrence of density models assigning higher likelihood to out-of-distribution inputs compared to the training distribution. (Also AREA CHAIR NOTE): Another parallel submission to *CONF* titled "Generative Ensembles for Robust Anomaly Detection" makes similar observations and seemed to suggest that ensembling can help counter the observed CIFAR/SVHN phenomena unlike what we see in Figure 10. Would be interesting to test anomaly detection using deep generative models trained on modalities other than images. - Lack of an extensive exploration of datasets Pros: - The finding that SVHN has larger likelihood than CIFAR according to networks is interesting.
The main contribution is the use of a IHT-based strategy to update the coefficients, with a gradient-based update for the dictionary (NOODL algorithm). The paper shows that there is an alternating optimization-based algorithm for this problem that under standard assumptions provably converges exactly to the true dictionary and the true coefficients x (up to some negligible bias). The paper deals with the problem of recovering an exact solution for both the dictionary and the activation coefficients. In [1], the authors give two algorithms: one with a better sample complexity than the algorithm presented here, but which has some systematic, somewhat large, error floor which it cannot exceed, and another which can obtain similar rates of convergence to the exact solution, but which requires polynomial sample complexity (the explicit bound is not stated in the paper). The contribution improves Arora 2015 in that it converges linearly and recovers both the dictionary and the coefficients with no bias.
The good property of loss surface for networks with skip connections is impressive and the authors present interesting experimental results pointing out that The paper analyzes the loss landscape of a class of deep neural networks with skip connections added to the output layer. Page 8 says "This effect can be directly related to our result of Theorem 3.3 that the loss landscape of skip-networks has no bad local valley and thus it is not difficult to reach a solution with zero training error". Apart from its technicality in the proof, the statement of Lemma 3.2 is just as expected and gives me little surprise, because having more than N hidden nodes connected directly to the output looks morally "equivalent" to having a layer as wide as N, and it is known that in such settings (e.g. Nguyen & Hein 17') it is easy to attain global minima. This paper presents a class of neural networks that does not have bad local valleys. This paper shows that a class of deep neural networks have no spurious local valleys –--implying no strict local-minima.
Original review: Summary: they propose a differentiable learning algorithm that can output a brush stroke that can approximate a pixel image input, such as MNIST or Omniglot. While research from big labs [1] have the advantage of having access to massive compute so that they can run large scale RL experiments to train an agent to "sketch" something that looks like MNIST or Omniglot, the authors probably had limited resources, and had to be more creative to come up with a solution to do the same thing that trains in a couple of hours using a single P40 GPU. I think the value in this method is that it can be converted to a full generative model with latent variables (like a VAE, GAN, sketch-rnn) where you can feed in a random vector (gaussian or uniform), and get a sketch as an output, and do things like interpolate between two sketches. Unlike sketch-pix2seq[3] (which is a pixel input -> sketch output model based on sketch-rnn[2]), their method trains in an unsupervised manner and does not require paired image/stroke data. Original Review: This paper deals with the problem of strokes-based image generation (in contrast to raster-based). If things don't work out this time, I recommend the authors asking some friends who have published (successfully) at good ML conferences to proof read this paper for content and style. Since this emulation model is differentiable, they can easily train an algorithm to output a stroke to approximate the drawing via back propagation, and avoid using RL and costly compute such in earlier works such as [1]. 2) While I like this method and approach, to play devil's advocate, what if I simply use an off the shelf bmp-to-svg converter that is fast and efficient (like [6]), and just build a set of stroke data from a dataset of pixel data, and train a sketch-rnn type model described in [3] to convert from pixel to stroke? I think the model can be easily converted to one though to address this issue, and I strongly encourage the authors to try enforcing a Gaussian prior to an embedding space (that can fit right between the 16x16x128 average pooling op to the fully connected 1024 sized layer), and show results where we can interpolate between two latent variables and see how the vector sketches are interpolated.
The reported experiments cover reasonable ground in terms of questions relevant to compositionality (relationship to representation compression, generalization), and I appreciate the comparison to human judgments, which lends credibility to applicability of the framework. The paper demonstrates the use of this framework to assess the role of compositionality in a hypothetical compression phase of representation learning, compares the correspondence of TRE with human judgments of compositionality of bigrams, provides an explanation of the relationship of the metric to topographic similarity, and uses the framework to draw conclusions about the role of compositionality in model generalization. This paper describes a framework - Tree Reconstruction Error (TRE) - for assessing compositionality of representations by comparing the learned outputs against those of the closest compositional approximation. The authors find that this measure correlates with the mutual information between the input x and z, approximates the human judges of compositionality on a language dataset and finally presents a study on the relation between the proposed measure and generalizalization performance, concluding that their measure correlates with generalization error as well as absolute test accuracy. (2015) is a relevant reference here, as it similarly learns primitive (word) representations to be compatible with a chosen composition function. -- how do we know if the learned representations capture the compositional structure present in the inputs, and tries to come up with a systematic framework to answer that question. I like the idea of learning basis vectors from the representations and constraining to follow the primitive semantics.
The paper proposes a new way to construct adversarial examples: do not change the intensity of the input image directly, but deform the image plane (i.e. compose the image with Id + tau where tau is a small amplitude vector field). - for instance, a study of the impact of the regularization would have been interesting (how does the sigma of the Gaussian smoothing affect the type of adversarial attacks obtained and their performance -- is it possible to fool the network with [very] smooth deformations?); The paper introduces an iterative method to generate deformed images for adversarial attack. - The interpolation scheme (how is defined the intensity I(x,y) for a non-integer location (x,y) within the image I) is rather important (linear interpolation, etc.) and should be at least mentioned in the main paper, and at best studied (it might impact the gradient descent path and the results); In this paper, the authors proposed a new attack using deformation. These results, as acknowledged by the authors, do not well support the effectiveness of deformation adversarial attack and defense. More clearly: the space of small deformations tau comes with an inner product (here L2, but one could choose another one), and the gradient \\nabla g obtained depends on this inner product choice M, even though the derivative Dg is the same (they are related by Dg(tau) = < \\nabla_M g | tau >_M for any tau). The idea is quite simple, generate small displacement and resample (interpolate) image until the label flips. 7), in order to be smoother: this can be interpreted as a prior (gradient descent paths should be made of smooth deformations) and as an associated inner product change (there do exist a metric M such that the gradient for that metric is \\nabla_M g = S \\nabla_L2 g). This remark is important in that it changes my rating of the paper (being more indulgent with papers proposing new ideas, as otherwise the novelty is rather low compared to [Xiao and al.]). However, the intuition behind the proposal does not make strong sense to the reviewer: since the main focus of this work is on model attack, why not directly (iteratively or not) adding random image deformations to fool the system? Pros: - The way of constructing deformation adversarial is interesting and novel Experimental results on several benchmark datasets (MNIST, ImageNet) and commonly used deep nets (CNN, ResNet, Inception) are reported to show the power of adversarial deformations.
Question and minor comments: In the original paper of STL, the author pointed out that by freezing the gradient of variational parameters to drop the score function term, we can utilize the flexible variational families like the mixture of Gaussians. (Same for Figure 4) The paper observes the gradient of multiple objective such as IWAE, RWS, JVI are in the form of some "reward" multiplied with score function which can be calculated with one more reparameterization step to reduce the variance. Clarity: The paper is clearly written in the sense that the motivation of research is clear, the derivation of the proposed method is easy to understand. About the motivation of the paper, I think it might be better to move the Fig.1 about the Bias to the introduction and clearly state that the author found that the STL is biased "experimentally". Author experimentally found that the estimator of the existing work(STL) is biased and proposed to reduce the bias by using the technique like  REINFORCE. Faced on this, I wonder which strategy is better to tighten the lower bound, should we use the STL with the mixture of Gaussians or use the proposed method? I think author want to point that when K=1, STL is unbiased with respect to the 1 ELBO, but when k>1, it is biased with respect to IWAE estimator. The reason I doubt the reason is that as I written in the below, the original STL can handle the mixture of Gaussians as the latent variable but the proposed method cannot. I know that motivation is a bit different for STL and proposed method but some comparisons are needed.
The work is very incremental over Luo et al (2018) "End-to-end Active Object Tracking and Its Real-world Deployment via Reinforcement Learning", as the only two additions are extra observations o_t^{alpha} for the target, and a reward function that has a fudge factor when the target gets too far away. The paper proposes a novel reward function - "partial zero sum", which only encourages the tracker-target competition when they are close and penalizes whey they are too far. For active object tracking in real-world/3D environment, designing the reward function only based on the distance between the expected position and the tracked object position can not well reflect the tracker capacity. I think the contributions of this work is incremental compared with [Luo et al (2018)] in which the major difference is the partial zero sum reward structure is used and the observations and actions information from the tracker are incorporated into the target network, while the network architecture is quite similar to [Luo et al (2018)]. This work aims to address the visual active tracking problem in which the tracker is automatically adjusted to follow the target. The scale changes of the target should also be considered when designing the reward function of the tracker. The ablation study shows that the tracker-aware observations and a target's reward structure that penalizes when it gets too far do help the tracker's performance, and that training the target agent helps the tracker agent achieve higher scores. This paper presents a simple multi-agent Deep RL task where a moving tracker tries to follow a moving target. However, the proposed method does not consider the issue, and the evaluation using the reward function based on the position distance may not be sufficient.
The authors perform a careful study of mixed integer linear programming approaches for verifying robustness of neural networks to adversarial perturbations. This paper studies a Mixed Integer Linear Programming (MILP) approach to verifying the robustness of neural networks with ReLU activations.
This paper proposes a novel method of solving ill-posed inverse problems and specifically focuses on geophysical imaging and remote sensing where high-res samples are rare and expensive. The key insight is that instead of training to directly predict x, the paper proposes to predict different piecewise constant projections of x from x_init , with one CNN trained for each projection, each projection space defined from a random delaunay triangulation, with the hope that learning prediction for each projection is more sample efficient. Pros: - The proposed approach is interesting and novel - I've not previously seen the idea of predicting different picewise constant projections instead of directly predicting the desired output (although using random projections has been explored) To  alleviate these problems, this paper proposes a novel idea: instead of fully reconstructing in the original space, the authors create reconstructions in projected spaces. This paper describes a novel method for solving inverse problems in imaging.
The method of the authors assumes that a goal-conditioned policy is already learned, and they use a Kullback-Leibler-based distance between policies conditioned by these two states as the loss that the representation learning algorithm should minimize. The paper presents a method to learn representations where proximity in euclidean distance represents states that are achieved by similar policies. The first weakness of the approach is that it assumes that a learned goal-conditioned policy is already available, and that the representation extracted from it can only be useful for learning "downstream tasks" in a second step. This is partly suggested when the authors mention that the representation could be learned from only a partial goal-conditioned policy, but this idea definitely needs to be investigated further. But when looking at the framework, this is close to what the authors do in practice: they use a distance between two *goal*-conditioned policies, not *state*-conditioned policies. In that respect, wouldn't it be possible to *simultaneously* learn a goal-conditioned policy and the representation it is based on? The paper suggests a method for generating representations that are linked to goals in reinforcement learning. But learning the goal-conditioned policy from the raw input representation in the first place might be the most difficult task. It is not clear if the end results is the same, meaning you just learn faster, or does the state reach a better final policy. - Still in Section 6.4, the authors insist that ARC outperforms VIME, but from Fig.7, VIME is not among the best performing methods. But there are also several weaknesses: - for all experiments, the way to obtain a goal-conditioned policy in the first place is not described. 2. Use D to estimate a model/goal-directed policies and consequenttly features F. - Main missing details is about how the goal reaching policy is trained. - As the goal-conditional policy is quite similar to the original task of navigation, it is important to know for how long it was trained and taken into account. Major remarks: - The author state they add experimental details and videos via a link to a website. To me, the last paragraph of Section 4 should be a subsection 4.4 with a title such as "state abstraction (or clustering?) from actionable representation".
The paper presents a coupled deep learning approach for generating realistic liquid simulation data that can be useful for real-time decision support applications. Given densely registered 4D implicit surfaces (volumes over time) for a structured scene, a neural-network based model is used to interpolate simulations for novel scene conditions (e.g. position and size of dropped water ball). The experimental results are sufficient for simulating liquids/smoke, except I would like to also see a comparison to using deformation field network only, without its predecessor. Right now the implicit surface deformation model is only tested on liquids examples, which limits the impact to that specialist domain -- it's a bit more of a SIGGRAPH type of paper than *CONF*. The interpolation model composes two components -- given these conditions, it first regresses weights combining a set of precomputed deformation fields, and then a second model regresses dense volumetric deformation corrections -- these are helpful as some events are not easily modeled with a set of basis deformations. I think it would be helpful to add more ablation (deformation-only results for all cases) and experiments with different numbers of bases in the final version. Another useful experiment would be to vary the number of bases and/or the resolution of the deformation correction network and see the effects. This paper introduces a deep learning approach for physical simulation. 1. The primary novelty here is in the problem formulation (e.g., defining cost function etc.) where two networks are used, one for learning appropriate deformation parameters and the other to generate the actual liquid shapes. The first network utilizes a set of precomputed deformations, while the weights can be set to generate different output shapes. The approach combines two networks for synthesizing 4D data that represents 3D physical simulations. So, the authors need to provide accuracy and computation cost/time comparisons with such methods to establish the benefits of using a deep learning based surrogate model. While this is a good applied paper with a large variety of experimental results, there is a significant lack of novelty from a machine learning perspective. For example, when designing the first network, can we also design another neural network that applies the deformation backwards and enforce some consistency to improve the results? The results are impressive from the perspective of the current abilities of deep neural networks. Also, is there a simple setting so that the current network design generates accurate results. This is primarily an application paper on simulating liquids in controlled scenes using nets and appears novel in that narrow domain.
Learning an input-dependent baseline function helps clear out the variance created by such perturbations in a way that does not bias the policy gradient estimate (the authors provide a theoretical proof of that fact). The authors propose different methods to train the input dependent baseline function: o) a multi-value network based approach o) a meta-learning approach The paper introduces and develops the notion of input-dependent baselines for Policy Gradient Methods in RL. Detailed comments: 1) On learning the input-dependent baselines: Generalising over context via a parametric functional approximation, like UVFAs [1] seems like a more natural first choice.
The authors then address the problem of data drift in BMI and describe a number of domain adaptation algorithms from simple (CCA to more complex ADAN) to help ameliorate it. A number of different approaches are described from creating a pre-execution calibration routine whereby trials on the given day are used to calibrate to an already trained BMI (e.g. required for CCA) to putting data into an adversarial network trained on data from earlier days. While I like the paper and studied methods -- using adverserial domain adaptation is interesting to use in this context --, I think that the authors oversell a bit. I.e. your BMI is neural data -> AE -> LSTM -> EMG?
Summary: Train a multilingual NMT system using the technique of Johnson et al (2017), but augment the standard cross-entropy loss with a distillation component based on individual (single-language-pair) teacher models. Although performance relative to individual models is still impressive - and seems to be better than than in previous work - this makes the experiments comparing to the multilingual baseline less meaningful.
The novel part of the approach (using subsplit Bayesian networks as a variational distribution) is intelligently combined with recent ideas from the approximate-inference literature (reweighted wake-sleep, VIMCO, reparameterization gradients, and multiple-sample ELBO estimators) to yield what seems to be an effective approach to a very hard inference problem. Its leverages recently proposed subsplit Bayesian networks (SBNs) as a variational approximation over tree space and combines this with modern gradient estimators for VI. This paper proposes a variational approach to Bayesian posterior inference in phylogenetic trees.
Casting the recently proposed LOLA gradient adjustment into a general matrix form, they diagnose an example where the shaping term in LOLA prevents convergence to SFP. Minor Comment: First paragraph in Section 2.2, "It is highly undesirable to converge to Nash in this game" -> Nash equilibria This paper introduces a new algorithm for differential game, where the goal is to find a optimize several objective functions simultaneously in a game of n players. The main goal of the paper is to show that SOS converges locally to SFP, and to fixed points only, while avoiding strict saddles. The authors first argue why Nash equilibria should not be the right solution concept for multi-agent learning and propose "stable fixed points" (SFP) as a possible solution concept. In Section 2.2, the authors make a broad statement that ''Nash equilibria cannot be the right solution concept for multi-agent learning.'' They provide one example where Nash is undesirable (L^1 = L^2 = xy).
In the submitted manuscript, the authors compare the performance of sign-symmetry and feedback alignment on ImageNet and MS COCO datasets using different network architectures, with the aim of testing biologically-plausible learning algorithms alternative to the more artificial backpropagation. In this work, on the other hand, there seems to be two scientific questions: first, to assess whether BP algorithms can be useful in artificial settings, and second, to determine whether they can say anything about how the brain learns, as in Bartunov (indeed, the author's conclusions highlight precisely these two points). The obtained results are promising and quite different to those in (Bartunov , 2018) and lead to the conclusion that biologically plausible learning algorithms in general and sign- symmetry in particular are effective alternatives for ANN training. and assess sign-symmetry's merit as a BP algorithm for learning in the brain, the work requires the authors the algorithms to be tested under similar conditions before claiming that there is a "direct conflict". They extend results of Bartunov et al 2018 (which found that feedback alignment fails on particular architectures on ImageNet), demonstrating that sign-symmetry performs much better, and that preserving error signal in the final layer (but using FA or SS for the rest) also improves performance. The claims and conclusions need to be more explicit, and the work needs to better seated in the context of both the previous literature, and the important questions at play for assessing biologically plausible learning algorithms. On the other hand, I think the conclusions regarding the first question -- whether sign-symmetry can be useful in artificial settings -- are fine given the experiments. Summary: The authors are interested in whether particular biologically plausible learning algorithms scale to large problems (object recognition and detection using ImageNet and MS COCO, respectively). To this end, though the authors claim that the conditions on which Bartunov et al tested are "somewhat restrictive", this logic can equally be flipped on its head: the conditions under which this paper tests sign-symmetry are not restrictive enough to productively move in the direction of assessing sign-symmetry's usefulness as a description of learning in the brain, and so the conclusion that the algorithm remains a viable option for describing learning in the brain is not sufficiently supported.
This paper proposes learning a latent variable deep generative model over every randomly sampled subset of observed features. The goal of this paper is to use deep generative models for missing data imputation. The method is compared against 1) classical approaches in missing data imputation on UCI benchmarks; 2) image inpainting against recently proposed GANS for the similar task, as well as; 3) against universal marginalizer, which learns conditional densities using a feedforward / autoregressive architecture. The paper presents a model for learning conditional distribution when arbitrary partitioning the input to observed and masked parts. Inference in this latent variable model is achieved through the use of an inference network which conditions on the set of "missing" (to the generative model) features. "Missing Value Imputation Based on Deep Generative Models." arXiv preprint arXiv:1808.01684 (2018). Novelty: Generative models have a long history of being used to impute missing data. The complexity of data considered is simplistic (and may not make use of the expressivity of the deep generative model). This provides a model able to achieve good results on image inpainting and feature imputation tasks. This paper introduces the VAEAC model, inspired by CVAEs, it allows conditioning on any subset of the latent features. My concern about the experimental results on missing data imputation is that strong competition such as Gondra et al'17 and Yoon et al'18 that report better results on UCI than classical approaches are not included.
The authors propose an approximation of the so-called best-response function, that maps the hyperparameters to the corresponding optimal parameters (w.r.t the minimization of the training loss), allowing a formulate as a single-level optimization problem and the use gradient descent algorithm. The approach assumes a distribution on the hyperparameters, governed by a parameter, which is adapted during the course of the training to achieve a compromise between the flexibility of the best-response function and the quality of its local approximation around the current hyperparameters.
While the authors present the method as an alternative to batch normalization, most of the reported results show a better performance for BN. The experimental results show that ENorm performs better than baseline methods on CIFAR-10 and ImageNet datasets. The paper is written well and provides a sound theoretical analysis to show the main idea, but unfortunately, the experimental results do not seem to support the effectiveness of the proposed method. ================== After rebuttal ================== The authors provided new experiments supporting the proposed method. The authors show that the proposed method preserve functionally equivalent property in respect of the output of the functions (Linear, Conv, and Max-Pool) and show also that ENorm converges to the global optimum through the optimization. The authors propose a new weight re-parameterization technique called Equi-normalization (ENorm) inspired by the Sinkhorn-Knopp algorithm. The authors propose a new regularization method for neural networks. This certainly explains the poor performances of your baseline when you increase the number of layers in Figure 4, and probably explains why you need to add a BN layer at the end of your network to help with the training of the baselines. (-) The experimental results cannot consistently show the effectiveness of the proposed method in test accuracy.
After showing theoretical guarantees of these methods (linear convergence) the authors propose to combine them with existing techniques, and show in fact this leads to better results. This paper looks at solving optimization problems that arise in GANs, via a variational inequality perspective (VIP). For strongly-monotone operators (a generalization of strongly-convex functions) extrapolation updates are shown to have linear convergence. Summary: The authors take a variational inequality perspective to the study of the saddle point problem that defines a GAN. For clarity it should be mentioned that GANs parametrized with neural nets lead to non-monotone VIs. A provably convergent algorithm for that setting is still an open problem, no? While for \\eta \\in (0, 1) implicit and extrapolation are similar, adding the remark that implicit method is stable for any \\eta > 0 (and therefore can lead to an arbitrary fast convergence) would give a more balanced view. + The theory for optimization of VIs with stochastic gradients (though only in monotone setting) was very interesting to me and contains some novel results (Theorem 2, Theorem 4) But I understand that producing state-of-the-art inception scores is not the focus of the paper, therefore I would suggest that the authors release an implementation of the proposed new optimizers (ExtraAdam) for a popular DL framework (e.g. pytorch) such that practitioners working with GANs can quickly try them out in a "plug-and-play" fashion.
This paper is about issues that arise when applying Information Bottleneck (IB) concepts to machine learning, more precisely in deterministic supervised learning such as classification (deterministic in the sense that the target function to estimate is deterministic: it associates each example to one true label only, and not to a distribution over labels). SUMMARY: This paper is about potential problems of the information bottleneck principle in cases where the output variable Y is a deterministic function of the inputs X. Such a deterministic relationship between outputs and inputs induces the problem that the the IB "information curve" (i.e. I(T;Y) as a function of I(X;T)) is piece-wise linear and, thus, no longer strictly concave, which is crucial for non-degenerate ("interesting") solutions. This work analyses the information bottleneck (IB) method applied to the supervised learning of a deterministic rule Y=f(X). The authors argue that most real classification problems indeed show such a deterministic relation between the class labels and the inputs X, and they explore several issues that result from such pathologies. 2) They show that in the case of a deterministic rule, the information bottleneck curve has a simple shape, piecewise linear, and is not strictly concave. In the experiments of the present paper, the results seem to suggest that the interesting intermediate representations (separation in 10 compact clusters of the MNIST classes) is actually easier to obtain (large range of \\beta) optimizing the IB Lagrangian rather than the proposed squared IB Lagrangian. The idea as I understood it is as follows: 1) In a first section the authors discuss the relationship between supervised learning through minimization of the empirical cross entropy and the maximization of the empirical mutual information with an intermediate latent variable T. Namely: (1) the "Information Bottleneck curve" cannot be computed with the Information Bottleneck Lagrangian approach (because of optimization landscape issues: optimization of such a piecewise-linear function with a linear penalty will always yield the same optimum whatever the slope of the penalty is [same story as L1 vs. - the two first problems described ((1) and (2)) are original, interesting contributions to the field, of particular interest for people interested in applying information bottleneck concepts to supervised learning; I feel it would be appropriate, either in the general literature section, either for discussing how to compute in practice the mutual informations (exact values vs. 7) They use the IB method to train a neural net on MNIST, using the Kolchinsky estimate of the mutual informations. This is not an image classification task though, as predictions are made for each pixel; still, given an input image X, there is only one correct output Y, so, still in the deterministic supervised classification problem. Analyzing situations in which Y = f(X) (with f being a deterministic function) is certainly interesting from a theoretic point of view, but I am not convinced that this analysis is truly relevant for practical problems.
This paper proposes training multiple generative models that share a common latent variable, which is learned in a weakly supervised fashion, to achieve high level coordination between multiple agents. The paper proposes training generative models that produce multi-agent trajectories using heuristic functions that label variables that would otherwise be latent in training data. The approach extends VRNN to a hierarchical setup with high level coordination via a shared learned latent variable.
The authors conduct experiments on both static and time series data and validate that the method perform better than related methods in terms of clustering results as well as interpretability. The authors also suggest augmenting their setup with a model of cluster transition dynamics for time-series data, which seems to improve the clustering further, as well as providing an interpretable 2D visualisation of the system's dynamics. The authors may want to better discuss the performance of this algorithm, especially compared to its much lower modeling complexity with respect to the proposed method. This paper proposes a deep learning method for representation learning in time series data. This paper deals with an interesting problem as learning an interpretable representation in time series data is important in areas such as health care and business. The enhance consistency over time, the model is further equipped with an additional cost term enforcing transition smoothness across data points and latent embeddings. This construction motivates the formulation of the auto encoder through the definition of several cost terms promoting reconstruction, clustering, and consistency across latent mappings.
Indeed, the method performs better than several competitors plus a single human expert. 4. Compared to the existing models (DenseNet, Multi-scale CNN etc.), the performance improvement of the proposed model is limited. and human experts), the proposed architecture shows a much higher performance.
Summary: This work demonstrates that, although the Boltzmann softmax operator is not a non-expansion, a proposed dynamic Boltzmann operator (DBS) can be used in conjunction with value iteration and Q-learning to achieve convergence to V* and Q*, respectively. In that work, the weighting is state-dependent, so the main algorithmic novelty in this paper is removing the dependence on state visitation for the beta parameter by making it solely dependent on time. (3) For the DBS-DQN algorithm, the authors set beta_t = ct^2 - how is the value of c determined? (2) The proof of Theorem 1 uses the fact that |L(Q) - max(Q)| <= log(|A|) / beta, which is not immediately clear from the result cited in McKay (2003). Another recent paper that actually does prove a regret bound for a Boltzmann policy for RL is 'Variational Bayesian Reinforcement Learning with Regret Bounds', which also anneals the temperature, this should be mentioned. Clarity: In the DBS Q-learning algorithm, it is unclear under which policy actions are selected, e.g. using epsilon-greedy/epsilon-Boltzmann versus using the Boltzmann distribution applied to the Q(s, a) values.
Comparing on the small datasets, the proposed method seems to significantly improve the performance over current best results of NPMT+LM. The authors show that the proposed approach leads to good results on three translation datasets. This paper proposes a seq2seq model which incorporates dependency parse information from the source side by embedding each word's subgraph (according to a predetermined dependency parse) using the Graph2Seq model of Song et al. This paper proposes a method for combining the Graph2Seq and Seq2Seq models into a unified model that captures the benefits of both. The paper thoroughly describes in series of experiments that demonstrate that the authors' proposed method outperforms several of the other NMT methods on a few translation tasks. However, the core idea of the proposed method, that is, combining the word representation, sub-graph state, incoming and outgoing representations seems to be novel.
While one would indeed expect an overhead due to the binary search, it is not clear a priori how large this overhead needs to be to achieve a competitive zero confidence attack with PGD (especially with a tuned step size for PGD, see above). But as pointed out above, it is possible to convert a fixed perturbation attack to a zero confidence attack via a binary search. - In the second paragraph of the introduction, the authors claim that fixed perturbation attacks and zero confidence attacks differ significantly. - In the introduction, the authors emphasize the distinction between fixed perturbation attacks and zero confidence attacks. However, from an optimization point of view, these two notions are clearly related and a fixed perturbation attack can be converted to a small perturbation / zero confidence attack via a binary search over the perturbation size. This paper proposes an efficient zero-confidence attack algorithm, MARGINATTACK, which uses the modified Rosen's algorithm to optimize the same objective as CW attack. The authors make a distinction between "fixed perturbation" attacks and "zero confidence" attacks. The authors claim that zero confidence attacks pose a harder problem and hence mainly compare their experimental results to the CW attack. Method such as projected gradient descent fall into the "fixed perturbation" category, while MarginAttack and CW belong to the "zero confidence" category. As a result, it is not clear whether the running time benchmarks are a fair comparison since MarginAttack does not automatically tune its parameters.
Authors present a novel regularizer to impose graph structure upon hidden layers of a neural Network. Furthermore, by extending the graph Fourier transformation to overcomplete dictionary representation, authors further propose a spectral bottleneck regularizer. Authors highlight the contribution of graph spectral regularizer to the interpretability of neural networks. Experimental results show that when suitable structural information and corresponding regularizers are imposed, the interpretability of the intermediate layers is improved. Authors propose a class of graph spectral regularizers and their performance is different in different tasks. It is not surprising that adding such regularizers to the training process of neural networks can help to get more structural activations. Also, in none of the experiments authors mention how the added regularizer affects the model performance. Regarding the capsule network example, when you write that without regularization each digit responds differently to perturbation of the same dimension, isn't it possibly true only up to a, unknown, permutation of the neurons?
This paper argues that a random orthogonal output vector encoding is more robust to adversarial attacks than the ubiquitous softmax. This paper argues that the vulnerability of classifiers to (black-box) adversarial attacks stems from the use of a final cross-entropy layer trained on one-hot labels. One could train a classification model where the final fully connected layer (C inputs K output logits) were a frozen matrix (updates disabled) of K orthogonal basis vector (ie, the same as the C_{RO}) codebook they propose. The authors argue that cross-entropy with one hot labels causes gradient correlation in the last layer and this propagates all the way through the network (bottom of page 3) but there are no experiments supporting this conjecture.
This paper considers parameterizing Dirichlet, Dirichlet-multinomial, and Beta distributions with the outputs of a neural network. In section 2, the authors consider parameterizing Dirichlet, Dirichlet-multinomial, and Beta distributions with the outputs of a neural network (Section 2). They present the distributions and gradients, discuss appropriate activation functions for the output layer, and evaluate this approach on synthetic and real datasets with mixed results. If I were to rewrite this paper, I would focus on answering the question "What are the unique challenges of parameterizing Dirichlet, Dirichlet-multinomial, and Beta distributions with the outputs of a neural network and how can we address them?", replacing section 2 with an expanded section 3. The authors briefly argue that the proposed methods are superior because they provide uncertainty estimates for the output distributions. 3. In section 4, the authors evaluate the proposed networks on a collection of synthetic and real tasks. As the authors note, parameterizing an exponential family distribution with the outputs of a neural network is not a novel contribution (e.g. Rudolph et al.
The authors propose to estimate and minimize the empirical Wasserstein distance between batches of samples of real and fake data, then calculate a (sub) gradient of it with respect to the generator's parameters and use it to train generative models. [6]: https://www.sciencedirect.com/science/article/pii/0377042794900337 The paper 'Generative model based on minimizing exact empirical Wasserstein distance' proposes a variant of Wasserstein GAN based on a primal version of the Wasserstein loss rather than the relying on the classical Kantorovich-Rubinstein duality as first proposed by Arjovsky in the GAN context. Typos: Eq (1) and (2): when taken over the set of all Lipschitz-1 functions, the max should be a sup The paper proposed to use the exact empirical Wasserstein distance to supervise the training of generative model. The quantitative results-- empirical Wasserstein distance show the superiority of the proposed methods. My concerns come from both theoretical and experimental aspects: The linear-programming problem Eq.(4)-Eq.(7) has been studied in existing literature.
One of the biggest issues of this paper is that they use CTC as an acoustic model, while still many real speech recognition applications and major open source (Kaldi) use hybrid HMM/DNN(TDNN, LSTM, CNN, etc.) systems. This paper discusses applications of variants of RNNs and Gated CNN to acoustic modeling in embedded speech recognition systems, and the main focus of the paper is computational (memory) efficiency when we deploy the system. The findings presented in this paper are interesting and quite useful when one wants to implement a LSTM-based acoustic model on mobile devices. - Section 3.2: I actually don't fully understand the claims of this experiment based on TIMIT, as it is phoneme recognition, and not directly related to the real application, which is the main target of this paper I think. The main issue of this paper is the lack of novelty: the three evaluated approaches (Diag LSTM, QRNN and Gated ConvNet) are not novel, the only novelty is the addition of a 1D convolution, which is not enough for a conference like *CONF*. This analysis is actually valuable, and this suggested change about the position of this TIMIT experiment can avoid some confusion of the main target of this paper.) This paper investigates a number of techniques and neural network architectures for embedded acoustic modeling. My suggestion is to place these TIMIT based experiments as a preliminary experiment to investigate the variants of RNN or gated CNN before the WSJ experiments.
The contributions of this paper allow to use this model on real-word datasets by reducing the time and space complexity of the NTP model. 3) Section 2 on the NTP framework is not very helpful for a reader that has not read the previous paper on NTP (in particular, the part on training and rule learning). This paper propose an extension of the Neural Theorem Provers (NTP) system that addresses the main issues of this method. In particular, it would be useful for the authors to focus on providing more insights on how the proposed techniques improve the results, and in what ways. The increments presented are reasonable and justified, but the experimental results, specifically on the larger datasets, warrant further investigation. [Cons] - Empirical performance on larger datasets needs further investigation. The authors propose several techniques to speed up the previously proposed Neural Theorem Prover approach. The attention mechanism (essentially reducing the model capacity) is also well-known but its effect in this particular framework is not properly elaborated. - No ablation study is performed so the effect of incorporating mentions and attention are unclear. [Summary] This paper scales NTPs by using approximate nearest neighbour search over facts and rules during unification. However, I feel that the paper in its current form is not yet ready for publication in *CONF*, for the following reasons: 1) The authors propose three improvements.
(1) The proposed base open-domain QA method (+DISTANT) itself improves a lot, which I think is the major contribution of the paper. This paper proposes a new open-domain QA system which gives state-of-the-art performance on multiple datasets, with a large improvement on QuasarT and TriviaQA.
The problem of learning cumulative quantities in a neural net is that we need two types of samples: - the positive examples: samples from which we train our network to predict its own value plus the new quantity, By using these networks within a CFR framework, the authors manage to avoid huge memory requirements traditionally needed to save cumulative regret and average strategy values for all information sets across many iterations. Here, maybe a way to alleviate this problem would be to generate negative samples (where the network would be trained to predict low cumulative values) by following a different (possibly more exploratory) policy. As a result, the NN is not providing any compression or generalization, and I would expect that the network can memorize the training set data exactly, i.e. predict the exact mean counterfactual value for each infoset over the data. For example the network predicting the cumulative regret may generalize to large values at newly visited states, instead of predicting a value close to 0.
Contribution: - Using a known parameters crystallography simulator (X-ray beam, structure being analyzed, environment (crystalline or not)) built a dataset (called DiffraNet) of 25,000 512x512 grayscale labeled images of resulting diffraction images of various materials/structures (crystalline or not) . o This is to assess the generalization level of the DiffraNet dataset patterns' fine-tuned classification algorithms to real-life obtained patterns (relates to the previously stated representability of the samples). • Were any real-life setting obtained pattern samples classified using DiffraNet dataset patterns' fine-tuned classification algorithms? o It's stated in Figure 2 and Table 6 that 2 classes are either blank or no-crystal but is that a known fact (purposely chosen) or no pattern images for crystalline structures due to inadequate experimental settings to uncover the crystalline nature of the analyzed structure?
This paper performs an analysis of the cell-state updating scheme of LSTM and realizes that it is mainly controlled by the forget gate.
It is claimed that the natural language instruction following approaches described in the first paragraph "require a large amount of human supervision" in the form of action sequences. This submission proposes a method for learning to follow instructions by splitting the policy into two stages: human instructions to robot-interpretable goals and goals to actions. While this works for the simple environments considered in this paper, it cannot generalize to real-world instruction-following scenarios where the number of distinct goal configurations is too large to tractably enumerate. This paper contains an important core insight---much of what's hard about instruction following is generic planning behavior that doesn't depend on the semantics of instructions, and pre-learning this behavior makes it possible to use natural language supervision more effectively. Even if we set aside the distinctions between human-generated instructions and synthetic command languages like used in Hermann Hill & al., the goal -> policy module is defined by a buffer of cached trajectories and goal representations.
This paper presents a thorough and systematic study of the effect of pre-training over various NLP tasks on the GLUE multi-task learning evaluation suite, including an examination of the effect of language model-based pre-training using ELMo. The paper seems to suggest that it consists of pre-training a model on the same task on which it is later evaluated. The work presented in this paper relates to the impact of the dataset on the performance of contextual embedding (namely ELMO in this paper) on many downstream tasks, including GLUE tasks, but also alternative NLP tasks. The main conclusion is that both single-task and LM-based pre-training helps in most situations, but the gain is often not large, and not consistent across all GLUE tasks. The work is focused on experiments, and draws several conclusions that are interesting, mostly around the amount of gain one can expect and the fact that the choice of the dataset is task-dependent. Contextualized word representations have gained a lot of interest in recent years and the NLP and ML community could benefit from such detailed comparison of such methods. One of the issue is that the authors if seems to believe that ELMO is the best contextual language model. Minor details: Page 1: "can yield very strong performance on NLP tasks" is a very busy way to express the fact that Sentence Encoders work well in practice. Table 2 is very interesting, the results suggesting that we are indeed very far from fully robust sentence representation method. The study and the experimental results will be useful and interesting to the community. I understand that the results of the current papers would hove to be re-run on all these tasks, but I'm afraid the current paper will have a limited impact if it does not use the most effective method at the date of publication...
The authors propose a method to improve sample diversity of GANs. They introduce multiple discriminators, each aims to not only compare real and fake examples but also compare different "micro-batch" of examples. The submission proposes to increase the variety of generated samples from GANs by a) using an ensemble of discriminators, and b) tasking them with distinguishing not only fake from real samples, but also their fake samples from the fake samples given to the respective other discriminators. Specifically, it proposes to split a minibatch of samples into further smaller minibatches (microbatches) and train different discriminators on each. As a sanity check - given a fixed generator - if you continue to train the discriminators on randomly drawn samples from this generator distribution does the microbatch discrimination objective continue to make progress and converge to a minimum? I would have liked to see this measure evaluated on conventionally trained GANs as a baseline, as well on the methods compared to in Section 5.
Pros: (1) The authors propose a sensible approach, which is also novel to be best of our knowledge, using SVM to select support data from old data to be fed to the network along with the new data in the incremental learning framework to avoid catastrophic forgetting. This paper presents a continual learning method that aims to overcome the catastrophic forgetting problem by holding out small number of samples for each task to be used in training for new tasks. The authors consider the last layer and the softmax function as SVM, and obtain support vectors, which are used as important samples of the old dataset. Cons: Major Points: (1) To show that the method proposed in the paper addresses catastrophic forgetting, in addition to the overall accuracy shown in Figure 3, it is also necessary to show the accuracy of different models on old classes when new classes are added to the network. SupportNet is compared to five methods (all data: network is re-trained with new and old data, upper bound for performance, iCaRL: state-of-the-art method for incremental learning, EWC: Only EWC regularizer added, Fine tune: Only new data, Random guessing: Random guess to assign labels) on six datasets (MNIST, CIFAR-10, CIFAR-100, Enzyme Function Prediction, HeLa Subcellular Structure Classification, Breast Tumor Classification). This paper presents a hybrid concept of deep neural network and support vector machine (SVM) for preventing catastrophic forgetting.
Is it to avoid disastrous states during exploration / training, or to inject prior knowledge into the agent to speed up learning, or to balance trade-offs between constraint violation and reward optimization? The more novel part of this work is in augmenting the state of the agent with the state of a DFA that is tracking the action sequence for constraint violations. This work aims to use formal languages to add a reward shaping signal in the form of a penalty on the system when constraints are violated. Pros: - Augmenting agent state with state of DFA tracking action sequence constraints is novel and useful for this problem
This work proposes an ensemble method for convolutional neural networks wherein each convolutional layer is replicated m times and the resulting activations are averaged layerwise. The CIFAR-10 baseline numbers are not ideal, and since IEA is basically "plug and play" in existing architectures, starting from one of these settings instead (such as Wide ResNet https://arxiv.org/abs/1605.07146) and showing a boost would be a stronger indication that this method actually improves results. I would also significantly reduce the claims of novelty, such as "We introduce the usage of such methods, specifically ensemble average inside Convolutional Neural Networks (CNNs) architectures." in the abstract, given that this is the exact idea explored in other work including followups to Maxout. IEA proposes to use multiple "parallel" convolution groups, which are then averaged to improve performance. 3. Under this interpretation of the tables---again, correct me if I'm wrong---the proper comparison would be "IEA (ours)" versus "Ensemble of models using CNL", or  "(m=3, k=1)" versus "(m=1, k=3)" in my notation. My key concerns here are on relation to past work, greater comparison to closely related methods, and improvement of baselines results. Is the performance boost greater than simply using an ensemble of m networks directly (resulting in the equivalent number of parameters overall)? A discussion of this comparison in greater detail, or even derivation of IEA as a special setting or extension of ResNet (coupled with stronger performance on the datasets) would help ground the work in prior publication.
By jointly training all these networks, the authors are now able to compress a given network by mapping it's discrete architecture into the latent space, then performing gradient descent towards higher accuracy and lower parameter count (according to the learned regressors). Specifically, compared to those methods, the search space for the proposed paper is larger because although the number of layers is fixed, the connections between layers give more freedom to the compression algorithm. I am extremely interested to know whether the result of 'compress', ie a new concrete architecture found by gradient descent in the latent space, actually has the number of parameters and the accuracy that the regressors predict. Presumably along the way to 20x compression of parameter count, the optimisation passes through a number of progressively smaller discrete architectures - what do these looks like? This paper deals with Architecture Compression, where the authors seem to learn a mapping from a discrete architecture space which includes various 1D convnets. Two further regressors are trained to map from the continuous latent space to accuracy, and parameter count. This seems to preclude even basic architectural advancement like skip connections / ResNet - the authors even mention this in section 3.1, and point to experiments on resnets in section 4.4, but the words "skip" and "resnet" do not appear anywhere else in the paper.
Limiting source perturbations to character swaps and neighbors in embedding space, then using automatic metrics to measure semantic distance seems both unnecessary and unlikely to succeed. Summary: Proposes a framework for performing adversarial attacks on an NMT system in which perturbations to a source sentence aim to preserve its meaning, on the theory that an existing reference translation will remain valid if this is done. The authors further study a series of automatic metrics for determining whether semantic meaning in the input space has changed, and find that the chrF method produces scores most correlated with human judgement of semantic meaning. Next, standard gradient-based adversarial attacks are carried out, replacing the three tokens that result in the biggest drop in (approximate) reference probability, either 1) with no constraints, 2) constrained to character swaps of the original token, or 3) constrained be among the 10 closest embeddings to the original token. From Table 6 it looks like random sampling is actually slightly better than adversarial training in terms of robustness to CharSwap attacks in the Transformer model.
The proposed method instead constructs a target distribution which places probability mass not only on leaf category nodes but also on their neighbors in a known semantic hierarchy of labels, then penalizes the KL-divergence between a model's predicted distribution and this target distribution. I suspect that the authors chose not to perform this comparison since unlike DeViSE and ConSE their method cannot predict category labels not seen during training; instead it is constrained to predicting a known supercategory when presented with an image of a novel leaf category. Instead of directly optimizing the standard cross-entropy loss, the paper considers some soft probability scores that consider some class graph taxonomy. Instead the authors should compare their method against DeViSE and ConSE for zero-shot learning. This paper proposes a new soft negative log-likelihood loss formulation for multi-class classification problems.
Instead, the paper proposes learning a conditional GAN, which can potentially generate large amounts of realistic synthetic data, and use this data (in addition to original training data) for model compression. Summary: The paper proposes an approach for improving standard techniques for model compression, i.e. compressing a big model (teacher) in a smaller and more computationally efficient one (student), using data generated by a conditional GAN (cGAN). This score evaluates the quality of generated data by using it in model compression: "good" synthetic data results in a smaller gap in performance between student and teacher models. The authors proposed to use a separately trained GAN network to generate synthetic data for the student-teacher training. The experimental results look good, GAN generated data help train a better performed student in knowledge distillation.
For the scale of datasets discussed, where SVM based methods seem to be working well, it is possible that approaches [1,2] which can exploit label correlations can do even better. I concur with the following paper than an additional hidden layer is essential: Liu et al "Deep Learning for Extreme Multi-label Text Classification". The approach offers clever and promising techniques to force the inference process in structured classification to converge, but experiments seem to lack apple-to-apple comparisons. However, I think the authors should rather present this work as structured classification, as labels dependencies not modeled by the hierarchy are exploited, and as other graph structure could be exploited to drive the RL search. The results the authors quote as representative of other approaches seem in fact entirely reproduced on datasets that were not used on the original papers, and the authors do not try an apple-to-apple comparison to determine if this 'reproduction' is fair. This paper presents an end to end rl approach for hierarchical text classification.
To improve the robustness of neural networks under various conditions, this paper proposes a new regularizer defined on the graph of the training examples, which penalizes the large similarities between representations belonging to different classes, thus increase the stability of the transformations defined by each layer of the network. This paper proposes the interesting addition of a graph-based regularisers, in NNs architectures, for improving their robustness to different perturbations or noise. Experiments on CIFAR-10 show that the method improves the robustness of the neural networks to different types of perturbations (perturbations of the input, aka adversarial examples, and quantization of the network weights/dropout0. 1. It is still not clear why would this regularization help robustness especially when considering adversarial examples. Compared to the previous version, this paper made a good improvement in its experimental results, by adding two different robustness settings in section 4.1 and section 4.3, and also include DeepFool as a strong attack method for testing adversarial robustness. 2. On the other hand, while the proposed regularizer can be interpreted in a perspective of the Laplacian of the similarity graph, the third part in Equation (4), that expresses the smoothness as the sum of similarities between different classes, seems more intuitive to me.
The paper investigates the problem of universal replies plaguing the Seq2Seq neural generation models. - it would be interesting to know for the trivial questions if the performance was impacted by the deemphasizing (one that do result in universal replies) This paper presents a framework for understanding why seq2seq neural response generators prefer "universal"/generic replies. In section 3, the authors then introduce the "max-marginal regularization" which is a linear combination of log-likelihood and max-margin (where the score is given by log-likelihood) losses. The paper looks into improving the neural response generation task by deemphasizing the common responses using modification of the loss function and presentation the common/universal responses during the training phase.
The paper provides a concise specification of the query language (a mixture of logical and numeric operators) and asserts a theorem that the given procedure for constructing the query loss produces a function such that anytime the function is 0, the constraints are satisfied. In this paper the authors propose DL2 a system for training and querying neural networks with logical constraints Summary ------- This paper proposes DL2, a framework for turning queries over parameters and input, output pairs to neural networks into differentiable loss functions, and an associated declarative language for specifying these queries. Strengths --------- The framework is expressive enough that many interesting use cases are clear, from specifying background knowledge during training to model inspection. By replacing labeled examples with domain knowledge about the relationships among classes in CIFAR-100, the paper demonstrates a compelling use case for DL2. PSL by construction produces convex loss functions, and so the constraint that all outputs for a group of classes is either high OR low would probably not work well.
This paper considers a resizable mini-batch gradient descent (RMGD) algorithm based on a multi-armed bandit for achieving best performance in grid search by selecting an appropriate batch size at each epoch with a probability defined as a function of its previous success/failure. The idea of viewing the choice of hyperparameters in a learning algorithm as a bandit problem is known and has been explored in different contexts, although the specific application to minibatch size is new as far as I know. The authors consider the problem of determining the minibatch size for SGD by first fixing a set of candidate sizes, and then learning a distribution over those sizes using a MAB algorithm. In other words, their result does *not* show that their algorithm is close to outperforming a fixed choice of batch size (for that to hold, the comparator would need to be \\sum_t y(w(b^*,...,b^*),b^*)). The paper applies multi-armed bandits for choosing the size of the minibatch to be used in each training epoch of a standard CNN. The key point here is that the comparator (the second sum) starts each epoch at the result that was found by their adaptive algorithm, *not* what would have been found if a batch size of b^* had been used from the beginning.
In this paper, the author proposed a better control variate formula for second-order Monte Carlo gradient estimators, based on a special version of DiCE (Foerster et al, 2018). This paper extends the "infinitely differentiable Monte Carlo gradient estimator" (or DiCE) with a better control variate baseline for reducing the variance of the second order gradient estimates. Such issues reduce the value of the contribution in its current form and may contribute to ongoing misunderstandings of the control variate framework and action-dependent baselines in RL, to the detriment of variance reduction techniques in machine learning. Overview: This nicely written paper contributes a useful variance reduction baseline to make the recent formalism of the DiCE estimator more practical in application. The baseline is an extension of the same method used in the original paper, and does not generalize past the second order gradient, making the promising formalism of the DiCE estimator as infinitely differentiable still unrealizable in practice.
The method in the paper is as follows: assuming access to expert demonstration and a resettable simulator, the start state of the agent in the beginning of training is sampled from end of demonstration (close to the rewarding state) where the task of achieving the goal is easy. The paper presents a strategy for solving sparse reward tasks with RL by sampling initial states from demonstrations. The method, Backplay, starts by sampling states near the end of a demonstration trajectory, so that the agent will be initialized to states near the goal. Also, it is unclear to me whether the curves shown is comparable as the starting point of the agent, at least in the beginning of training, is close to the goal with higher success rate for the Backplay method compared to baselines. The  authors present a very elegant strategy of using Backplay, that learns a curriculum around a suboptimal demonstration. The Backplay method outperforms the vanilla baseline, however, from the training curves (~3500 epochs) in the appendix A4, it looks like the Uniform sampling baseline is doing as well or better than the proposed method. This work can also benefit from a more diverse set of tasks to better evaluate the effectiveness of the method, and provide more insight on when such a strategy is beneficial.
This paper develops a reinforcement learning approach for negotiating coalitions in cooperative game theory settings. The authors propose a new setup in which self-interested agents must cooperatively form teams to achieve a reward. The paper shows 4 results: 1) It considers a hand-designed bot similar to models from the game theory literature. It uses simple cooperative weighted voting games 1) to study the efficacy of deep RL in theoretically hard environments and 2) to compare solutions found by deep RL to a fair solution concept known in the literature on cooperative game theory. Details: Section 4.5/Experiment 4: The Shapely value comparison is the most important part of the paper. 3) A popular result in cooperative game theory predicts how effective agents should be.
Summary: the paper proves the convergence of empirical length map (length process) in NN to the length map for a permissible activation functions in a wide-network limit. The importance of proving the main statement under more general conditions on activation functions is doubtful and the authors do not comment on that.
It shows different pooling strategies reach to similar levels of deformation stability after sufficient training. It shows different pooling strategies reach to similar levels of deformation stability after sufficient training. Fair experiments with conclusion of similar stability of different pool layers after training is very evident. Fair experiments with conclusion of similar stability of different pool layers after training is very evident. iii) Although, the results presented on smoother filter initialization are interesting, but these are results are not compared in a one to one setting to different pooling methods, convolutions or residual networks. iii) Although, the results presented on smoother filter initialization are interesting, but these are results are not compared in a one to one setting to different pooling methods, convolutions or residual networks.
This work proposes a hybrid VAE-based model (combined with an adversarial or maximum mean discrepancy (MMD) based loss) to perform timbre transfer on recordings of musical instruments. The method is compared against the UNIT model under a variety of training conditions, and evaluated for within-domain reconstruction and transfer accuracy as measured by maximum mean discrepancy. The proposed method is designed to be many-to-many, and uses a single pair of encoders and decoders with additional conditioning inputs to select the source and target domains (timbres). * MMD losses in the context of GANs have also been studied in the following papers: - "Training generative neural networks via Maximum Mean Discrepancy optimization", Dziugaite et al. By further conditioning our system on several different instruments, the proposed method can generalize to many-to-many transfer within a single variational architecture able to perform multi-domain transfers. The many-to-many results are clearly better than the pairwise results in this regard, but in the context of musical timbre transfer, I don't feel that this model successfully achieves its goal -- the results of Mor et al. (2018) do actually make use of an adversarial training criterion (referred to as a "domain confusion loss"), contrary to what is claimed in the introduction. Since the authors do additional information here for each sample (notes), it would be possible to pair generated and real examples by instrument and note, rather than (in addition to) unsupervised, feature-space pairing by MMD. The authors proposed a Modulated Variational auto-Encoders (MoVE) to perform musical timbre transfer. I think the main point of transfer over a regular generative model that goes from labels to audio is precisely that it can be done without label information. Detailed comments ----------------- At several points in the manuscript, the authors refer to "invertible" representations (e.g., page 4, just after eq.
This paper proposes an unpaired image-to-image translation method which applies the co-segmentation network and adaptive instance normalization techniques to enable the manipulation on the local regions. The paper deals with image to image (of faces) translation solving two main typical issues: 1) the style information comes from the entire region of a given exemplar, collecting information from the background too, without properly isolating the face area; The paper propose to use co-segmentation to find the common areas to for image translation. Since the attribute information has been modeled by the network parameters, will different exemplar image lead to different translation outputs? Some of these works also applied the mask technique or adaptive instance normalization to the image-to-image translation problem. * The local mask-based highway adaptive instance normalization apply the style information to the local region correctly. * As the main experiments are about facial attributes translation, I strongly recommend to the author to compare their work with StarGAN [4]. 1) The paper says that "For example, in a person's facial image translation, if the exemplar image has two attributes,
This paper presents a GAN approach adapted for multi-modal distributions of single class data. Comments 1) the authors wrote that they develop a multi-modal one-class generative adversarial network based detector to distinguish anomalies from normal data (products). To force the generator to produce samples in the low density areas of the data distribution, a Complementary GAN is used. The proposed idea is to train a GAN generator to simulate anomalies in the data in order to provide the one-class classifier with more negative examples.
Summary ------ The authors propose an adaptation of the Adam method, with the AMSGrad correction and an additional parameter to p to exponentiate the diagonal conditioning matrix V (Padam). The authors proposed a generalized adaptive moment estimation method(Game). Compared to the existing methods AMSGrad and PAdam, the new method Game tracks only two parameters in iteration and hence saves memory. 1) adam models keeps in memory x_t (the model parameter), g_t (the model gradient), m_t (momentum) \\hat v_t and v_{t-1} (the monotone and non monotone version of the second order moment estimation.
On many of the datasets, the performance difference between the proposed method and uncertainty sampling is quite small in table 1. The MDP state proposed by this paper is the current performance score on each sample in a hold-out set. In the experiments, there needs to be discussion of how much variety there is in the different datasets in terms of their statistical properties that are relevant to active learning, such as how well the data cluster? A feature of this paper is that the method is relatively simple compared to some prior LAL methods, and also that it learns policies that can transfer successfully across diverse heterogenous datasets.
The paper also presents issues with the standard "learning to learn" optimizers, one being the short-horizon bias and as credited by the authors has been observed before in the literature, and the second one is what is termed the "exponential explosion of gradients" which I think lacks enough justification as currently presented (see below for details). From the text it is also unclear whether the authors have optimized the parameters of the first-order methods with respect to their training or validation performance - I hope this is the latter as that is the only way to fairly compare the two approaches.
Pros: - The proposed method is shown to work with existing methods like weight pruning, low-rank compression and quantization. Specifically, the proposed solution is to repeatedly apply weight compression and fine-tuning over the entire training process. A model compression framework, DeepTwist, was proposed which makes the weights zero if they are small in magnitude. This paper proposed a general framework, DeepTwist, for model compression. The paper does not really propose a new way of compressing the model weights, but rather a way of applying existing weight compression techniques. Then proximal function can be applied directly after Distortion Step to project the solutions. Since SGD is used for training, several minibatches are needed to achieve a relatively stable solution for projection using the proximal function, which is exactly the proposed framework in Fig. 1. Unlike the existing work, weight compression is applied as a form of weight distortion, i.e. the model has the full degree of freedom during fine-tuning (to recover potential compression errors).
The permuted phase component does not admit weight sharing and invariances exploited by convolutional networks, which results in severely hindered clean accuracy -- only 96% on MNIST and 45% on CIFAR-10 for a single model. This paper proposes Permutation Phase Defense (PPD), a novel image hiding method to resist adversarial attacks. This paper explores the idea of utilizing a secret random permutation in the Fourier phase domain to defense against adversarial examples. In summary the authors propose a  simple and intuitive method to improve the defense on adversarial attacks by combining random permutations and using a 2d DFT. While the security of a model against adversarial attacks is important, a defense should not sacrifice clean accuracy to such an extent. The paper demonstrated the method on MNIST and CIFAR10, and evaluates it against a number of adversarial attacks. The experiments with regards to robustness to adversarial attacks I find convincing, however the overall performance is not very good (such as the accuracy on Cifar10). The idea is drawn from cryptography, where the random permutation is treated as a secret key that the adversarial does not have access to. While the permutation is kept as a secret, it is plausible that the adversary may attempt to learn the transformation when given enough input-output pairs. The authors should introduce an appropriate threat model and evaluate this defense against plausible attacks under that threat model.
Strengths: + This paper is interesting in the sense that it empirically shows that using regularization in training deep RL can be helpful when the goal is the generalization from one flavour of an environment to another one but very similar to the original. More importantly, the performance boost is significant when DQN's parameters which are used to initialize the model for the new environment were trained using dropout and L2 regularization in the default flavour/mode. + The experiments show that using REGULARIZED FINE-TUNING and FINE-TUNING for a new flavour /mode can help with sample efficiency compared with the models which are trained from scratch [10M frames vs 50M frames and 50M frames vs 100M frames ](Table 3). During training, if I do not see car accelerating, I think it makes no sense to expect to generalize to a new game that has this property as it is out-of-distribution. Specifically, they showed that when features (parameters of DQN) are trained in one environment (default flavour/mode) and then used as an initialization for the same model but for a slightly different environment ( i.e. still captures key concepts of the original environment ) can boost the performance of the model in the new flavour/environment.
Clarity: I have not been able to fully understand why the proposed (uniform) sampling variant of BN is better than previous effort at making BN less computationally expensive in a GPU-based training environment by reading the paper: 1. The paper proposes a sampling-based method that aims at accelerating Batch Normalization (BN) during training of a neural network. This paper proposes a new technique that can reduce the computational complexity of batch normalization. (-) How to determine the sampling ratio for each normalization method is not provided, and it would be better if the authors can show some studies about sampling ratio versus the speed gain. But the idea of uniform-sampling seems     rather straight-forward, and more important, I do not see its justification from reading the paper; the other technique introduced, Virtual Dataset Normalization, seems to be a direct application of Virtual Batch Normalization (Salimans et al 16). The paper has an interesting idea about sampling some features to speed up the batch normalization.
In section,  -> In this section are it has -> and it has This paper mainly focuses on combining RL tasks with linear temporal logic formulas and proposed a method that helps to construct policy from learned subtasks. The experiments results show that the composition method does better than soft Q-learning on composing learned policies, but how it performed compared to earlier hierarchical reinforcement learning algorithms? This paper presents a way use using FSA-augmented MDPs to perform AND and OR of learned policies.
This paper proposes Deep Overlapping Community detection model (DOC), a graph convolutional network (GCN) based community detection algorithm for network data. AISTATS 2015 The current paper considers the overlapping community detection problem and suggests to use the so-called graph neural networks for its solution. Most of the shallow/deep graph embedding methods can also be used for link prediction task (many of the recent paper report such results).
2. In section 3 — Interpretable Multi-Variable LSTM, by stacking exogenous time series and target series, the authors implicitly formulate their algorithms in a way to consider auto-regression. 1. In the related work the authors state that "In time series analysis, prediction with exogenous variables is formulated as an auto-regressive exogenous model " . Summary: The authors propose IMV-LSTM, which can handle multi-variate time series data in a manner that enables accurate forecasting, and interpretation (importance of variables across time, and importance of each variable).
General questions: - "dependence of observations at each time step on all latent variables": Unfortunately, this means that the complexity of evaluating the model during training is O(n^2), where n is the sequence size, rather than linear in the standard case. This is largely an experimental paper, proposing and evaluating various modifications of variational recurrent models towards obtaining sequence data representations that are effective in downstream tasks. Empirically, when used in conjunction with prior updating and/or hierarchical latent variables, the proposed "stochastic generation" approach improves upon the baselines, but not when used in isolation.
The paper studies a particular task (the XOR detection problem) in a particular setup (see below), and proves mathematically that in that case, the training performs better when the number of features grows. The paper proves that, under this setup, training with a number of features k > 120 will perform better than with k = 2 only (while k = 2 is theoretically sufficient to solve the problem). The paper tries to offer an explanation about why over-parametrization can be helpful in neural networks; in particular, why over-parametrization can help having better generalization errors when we train the network with SGD and the activation functions are RELU. The design of the predictor is: - for each pair, compute 2k features (of the form ReLu(linear combination of the values, without bias));
You state in section 2, Related Work "we train models for emotion detection on conversation data that has been explicitly labeled by annotators" – please describe how this was done. The paper in particular is contributing its collected set of 25k empathetic dialogs, short semi-staged conversations around a particular seeded emotion and the results of various ways of incorporating this training set into a generative chatbot. In section 4, Empathetic dialog generator, you state that the dialog model has access to the situation description given by the speaker (also later called the situational prompt) but not the emotion word prompt. While the results clearly do not solve the problem of automating emapthy, the paper does give insights into which methods perform better than others (Generation vs Retrieval) and explicitly adding emotion predictions vs using an ensemble of encoders.
This paper proposes to use n-ary representations for convolutional neural network model quantization. Summary: This paper proposes a technique for quantizing the weights and activations of a CNN. The main contribution is in replacing the heuristic to find good quantization intervals of (Zhu et al, 2016) with a different heuristic based on a hierarchical clustering algorithm, and empirically validating its effectiveness. - In section 4, it is not made clear whether the activations are quantized according to the same scheme as the weights (apart from the issue of selecting a good clipping interval, which is addressed). I have several questions for this paper: 1) the main algorithm is mainly based on the hypothesis that the weights are with Gaussian distribution. This paper is about CNN model compression and inference acceleration using quantization. - The paper is a bit short on references, considering the many recent works on quantized neural networks. - Last paragraph of section 4: "(Cai et al., 2017) experimentally showed that the pre-activation distribution after batch normalization are all close to a Gaussian with zero mean and unit variance.
This paper proposed a new method for image restoration based a task-discriminator in addition to the GAN network. It shows superior performance than the baseline methods without such task-discriminator on medical image restoration and image super-resolution. In this paper, the authors propose a novel method of Task-GAN of image coupling by coupling GAN and a task-specific network, which alleviates  to  avoid hallucination or mode collapse. For medical image reconstruction and image super-resolution, the proposed method was not compared with any of the state-of-the-art methods, but only with the same method without a task-discriminator as a baseline. Authors propose to augment GAN-based image restoration with another task-specific branch such as classification tasks for further improvement. and Mu Lee, K., Accurate image super-resolution using very deep convolutional networks. and Shi, W., Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network. Adding an task-discriminator in a GAN network seems straightforward to improve the specific task. 1. The idea of adding a task-specific branch has been proposed in Huang et al's work. 4. The proposed method is not compared with other super-resolution methods. In addition, why not acquire real low-dose data and show the quality results using the proposed model?
This paper analyzes the surface of the complete orthogonal dictionary learning problem, and provides converge guarantees for randomly initialized gradient descent to the neighborhood of a global optimizer. The paper presents a convergence analysis for manifold gradient descent in complete dictionary learning.
State of the art results on miniImageNet 5-way, 1-shot, the only experiments here which compare to others, show accuracies better than 53: - Versa: https://arxiv.org/abs/1805.09921. Summary: This work tackles few-shot (or meta) learning, providing an extension of the gradient-based MAML method to using a mixture over global hyperparameters. Stochastic EM is used for end-to-end learning, an algorithm that is L times more expensive than MAML, where L is the number of mixture components. While the idea of task clustering is potentially useful, and may be important in practical use cases, I feel the proposed method is simply just too expensive to run in order to justify mild gains. They further propose a non-parametric approach to dynamically increase the capacity of the meta learner in continual learning problems. This paper presents a mixture of hierarchical Bayesian models for meta-learning to modulate transfer between various tasks to be learned. This type of effort is needed to motivate an extension of MAML which makes everything quite a bit more expensive, and lacks behind the state-of-art, which uses amortized inference networks (Versa, neural processes) rather than gradient-based. Please just be exact in the main paper about what you do, and what main competitors do, in particular about the number of points to use in each task update. 1. The performance of few-shot classification on MiniImageNet is not comparable to the state of the art (Table 2, Table 1). The proposed method is tested in a few-shot learning setup on miniImagenet, on a synthetic continual learning problem, and an evolutionary version of miniImagenet. This paper proposes a mixture of MAMLs (Finn et al., 2017) by exploiting the interpretation of MAML as a hierarchical Bayesian model (Grant et al. If expensive gradient-based meta-learning methods are to be consider in the future, the authors have to provide compelling arguments why the additional computations pay off.
I guess that the reason may be that: the significant computational cost (both in FLOPS and memory consumption) increase due to multiple discriminators destroys the benefit from the small performance improvement. 2. We propose a new method for training multiple-discriminator GANs: Hypervolume maximization, which weighs the gradient contributions of each discriminator by its loss. + On the experiments run, the HVM method appears to be an improvement over the two previous approaches of softmax weighting and straightforward averaging for multiple discriminators. Training multiple discriminators (in this paper up to 24) significantly increases compute cost and effective model size. The authors emphasize that the performance is improved with more discriminators, but it's good to contain comparison of the computational cost (FLOPS and memory consumption) at the same time.
The paper proposes a class of Evolutionary-Neural hybrid agents (Evo-NAS) to take advantage of both evolutionary algorithms and reinforcement learning algorithms for efficient neural architecture search. The proposed hybrid architect is evaluated on both synthetic and text classification tasks and then compared against pure evolutionary and RL-based agents.
This paper addresses an important problem, namely, unsupervised image classification, and may present interesting ideas. Similar question arises in the first paragraph of Section 4.1: if the connector network is constructed as a piecewise linear function (as stated earlier in the paper in abstract and introduction), then how can it be written as a matrix?
The major problem is that this paper does not correctly state the difference between their analysis and Kolher et al who already derived similar results for OLS. The analysis shows that GDBN converges to a stationary point when the learning rate is less than or equal to 1, regardless of the condition number of the problem.
This latent space representation is used as the reinforcement learning signal for the learner (probing) agent similar to the curiosity driven techniques where larger changes in the representation of mind are sought out since they should lead to larger differences in demonstrator agent behavior. Overall, while the agent behaviour modelling focused on a type of inner state (based on past trajectories) provides benefits in the evaluated examples, it is unsure how well the approach scales to more complex domains based on strong similarity and simplicity of the tested toy scenarios (evaluation on sorting problems is an interesting step towards to address this shortcoming). 3. The core premise behind training the learner agent with RL is using a curiosity driven approach to train a probing policy to incite new demonstrator behaviors by maximizing the differences between the latent vectors of the behavior trackers at different time steps. The authors consider the scenario of two agents, a demonstrator acting in an environment to achieve a goal, and a learner, which can also interact with the environment, but whose goal is to learn the demonstrator's policy by carrying out actions eliciting strong changes in the demonstrator's trajectory. The authors test this in several gridworld environments as well as a sorting task and show that their method achieves superior performance and generalizes better to unseen states and task variations compared to several baseline methods. 1) Summary This paper proposes a method for learning an agent by interacting and probing an expert agents behavior. - Same scale for the y-axes across figures This paper presents a method for interactive agent modeling that involves learning to model a demonstrator agent not only through passively viewing the demonstrator agent, but also through interactions from a learner agent that learns to probe the environment of the demonstrator agent so as to maximally change the behavior of the demonstrator agent. When the probing agent is testing the expert, it is essentially showing the imitator many different configurations of the environment.
This paper describes an approach for training conditional future frame prediction models, where the conditioning is with respect to the current frame and additional inputs - specifically actions performed in a reinforcement learning (RL) setting. Moreover, this work is developed in the context of RL applications, and since prior art [4] has shown that better predictive models do not necessarily lead to better RL results, it would be interesting to evaluate the proposed approach against baselines in an RL setting. Rather than using pixel-wise loss for an action-conditioned video prediction model ("Forward Model"), they use an adversarial loss combined with mutual-information loss (from InfoGAN) and content loss (based on difference in convnet features of VGG network, rather than pixels).
It will be useful if the authors can test the performance of the state-of-the-art networks for CIFAR-10/100 and ImageNet, where the weights of the last fully connected layer have been sampled from different distributions. Inspired by these networks, the authors propose weight initialization schemes for finite width networks. Furthermore, the only application of these infinitely wide networks proposed in this paper is for initialization of the weights of finite width networks. Summary: The paper attempts to proposal a weight initialization scheme to enable infinite deep infinite-width networks.
The interesting part of this paper appears in Section 4, where the author makes a connection between the SGD training process and \\alpha-SMLC(strong Markov learning chain). There is no actual connection to SGD left, therefore it is even hard to argue that the predicted shape will be observed, independent of dataset or model(one could think about a model which can not model a bias and the inputs are mean-free thus it is hard to learn the marginal distribution, which might change the trajectory) (the \\alpha SMLC) - through experiments, the paper shows that the trajectories of SGD and \\alpha-SMLC have  similar conditional entropy. It is an interesting observation that the trajectory of \\alpha-SMLC  is similar to that of SGD in these plots, but the authors have not made a sufficient effort to interpret this. In summary, this paper does the following: - The initial problem is to analyze the trajectory of SGD in training ANNs in the space of  P of probability measures on Y \\times Y.
This paper proposes an architecture search technique in which the hyperparameters are modeled as categorical distribution and learned jointly with the NN. Cons: -I speculate that there is a trade-off between the number of different parameters and whether one training is good enough to learn the architecture distribution. The authors propose to formulate the neural network architecture as a collection of multivariate categorical distributions.
As the proposed method is built based on reinforcement learning, it would be better if the authors could include state-of-the-art reinforcement learning algorithms as their baselines. The author proposed an Imagination Reconstruction Network for the recommendation task, which implements an imagination-augmented policy via three components: (1)  the imagination core (IC) that predicts the next time steps conditioned on actions sampled from an imagination policy; The paper proposed a new framework for session-based recommendation system that can optimize for sparse and delayed signal like purchase. Minor comments: (1) It would be better if the authors can test the proposed model on more datasets. Summary: The paper presents a session-based recommendation approach by focusing on user purchases instead of clicks. (2) State-of-the-art reinforcement learning algorithms were not taken into account for baselines in the experiments. Regardless the sketchy description of the algorithm, the empirical results look good, with comprehensive baseline methods for comparison. Even though the author has promised to release their implementation upon acceptance, I still think the paper needs a major change to make the proposed algorithm more accessible and easier for reproduce. The paper aimed at improving the performance of recommendation systems via reinforcement learning. (3) Some details are missing, resulting in the fact that it is hard for other researchers to fully capture the mechanism of the proposed algorithm. Weaknesses of the paper: (1) The motivations of applying reinforcement learning techniques are not convinced to me. With this level of clarity, I don't think it's easy for other people to reproduce the results in this paper, especially in section 4, where I expect more details about the description of the proposed new architecture. This seems one of the most important components of the proposed algorithm, but I found it's very hard to understanding what is done here exactly. Empirically, the authors compare their method to several recent baselines. Comments: The proposed architecture is an interesting inspiration from Neuroscience which fits into the sequential recommendation problem. How do the authors define the loss functions, i.e., \\mathcal{L}_{A3C} and \\mathcal{L}_{IRN}? There are a lot of supervised learning algorithms to the task of recommendations. Why do the authors utilize reinforcement learning to the task but not other supervised learning techniques? The proposed algorithm with an innovative IRN architecture was intriguing. The method is inspired from concepts of cognitive science which adds an imagination reconstruction network to an actor-critic RL framework in order to encourage exploration.
This paper proposes a new approach to enforcing disentanglement in VAEs using a term that penalizes the synergistic mutual information between the latent variables, encouraging representations where any given piece of information about a datapoint can be garnered from a single latent. Unfortunately, the authors do not properly evaluate their newly proposed Non-Syn VAE, only providing a single experiment on a toy dataset and no quantitative metric results. The paper proposes a new objective function for learning disentangled representations in a variational framework, building on the beta-VAE work by Higgins et al, 2017. Also why one should use the authors' suggested penalization term instead of total correlation is not discussed, nor demonstrated as they perform similarly on both disentanglement and synergy loss. I commend the authors for taking a multi-disciplinary perspective and bringing the information synergy ideas to the area of unsupervised disentangled representation learning. If the authors want to continue with the synergy minimisation approach, I would recommend that they attempt to use it as a novel interpretation of the existing disentangling techniques, and maybe try to develop a more robust disentanglement metric by following this line of reasoning. The authors aim at training a VAE that has disentangled latent representations in a "synergistically" maximal way. Though I appreciate this is a somewhat subjective opinion, for me, penalizing the synergistic information is probably actually a bad thing to do when taking a more long-term view on disentanglement.
This paper proposed a new way of learning point-wise alignment of two images, and based on this idea, one-shot classification and open-set recognition can be further improved. The authors propose a deep learning method based on image alignment to perform one-shot classification and open-set recognition. In this work, the authors tackle the problem of few-shot learning and open-set classification using a new type of NNs which they call alignment-based matching networks or ABM-Nets for short. Authors argue that using average (independent) greedy matching of pixel embedding (based on 4-6 layer cnn hypercolumns) is a better metric for one-shot learning than just using final layer embedding of a 4-6 layer cnn for the whole image. The proposed model is an extension of Matching Networks [Vinyals et al., 2016] where a different image embedding is adopted and a pixel-wise alignment step between test and reference image is added to the architecture. On the other hand, authors may argue that the hyper-column matching is not just about performance, whereas it also adds interpretability to why two images are categorized the same. They main idea is to benefit from binary maps between the query image and the support set (for the case of few-shot learning for the sake of discussion here) to guide the similarity measure. The tacit assumption of the authors is that a classifier driven by a point-wise alignment may improve the interpretation.
The authors compare the differentiable function against using prediction error via REINFORCE and DQN, showing that their intrinsic curiosity method results in more interactions with unseen objects than the other two methods. There are several things to like about this paper: - The problem of making "real-world" practical algorithms for exploration is clearly one of the biggest outstanding problems in reinforcement learning. - There is very little *science* in this paper, beyond the experiments pitting "improved algorithm" vs DQN/REINFORCE, which nobody ever claimed would be a good approach to exploration! The paper is based on using the gradient of the forward model to directly optimize the policy to produce higher prediction errors, as in Pathak et al. The authors motivate this work with arguments about the sample-efficiency required by real robot learning, and demonstrate basic results using a real robot.
This paper proposes a new framework for topic modeling, which consists of two main steps: generating bag of words for topics and then using RNN to decode a sequence text. For the first task, classification is not the main purpose of topic models, and while text classification _is_ used in many topic modeling papers, it is almost always accompanied by other evaluation metrics such as held-out perplexity and topic coherence. In addition to the above framework, to make the model work better, several add-ons are also proposed, combining autoencoder, loss clipping, and a generative model to generate text sequences based on the bag-of-words. This paper proposes TopicGAN, a generative adversarial approach to topic modeling and text generation. Table 4 shows slightly better results for "Preference" for TopicGAN with joint training, but "Accuracy" is measured only for the proposed model and not the baseline model. Specifically, the paper adopts the framework of InfoGAN to generates the bag-of-words of a document and the latent codes in InfoGAN correspond to the latent topics in topic modelling. (2) Another major one is why the word sequence generator is introduced in the proposed model. (3) I did not see a major improvement of the proposed model over others, given that the only numerical result reported is classification accuracy and the state-of-the-art conventional topic models are not compared. I did not see the contribution of this part to the whole model as a topic model, although the joint training shows the marginal performance gain on text generation. Thus I feel it is not a fair evaluation to just compare the models using text classification tasks. (3) Some of the experiment settings are not provided, for example, the number of topics, the value of \\alpha and \\lambda in the proposed model, the hyperparameters of LDA, which are crucial for the results. The model basically combines two steps: first to generate words (bag-of-words) for a topic, then second to generate the sequence of the words.
b) While the experimental results suggest the algorithm can recover the similar performance to the optimal policy of the true reward function, whether this observation can generalize outside the current synthetic environment is unclear to me. Particularly, they adopt the gradient-based meta learning algorithm, MAML, and the maximal entropy (MaxEnt) IRL framework, and derive the required meta gradient expression for parameter update. This paper attempts to the solve  data-set coverage issue common with Inverse reinforcement learning based approaches - by introducing a meta-learning framework trained on a smaller number of basic tasks. Pros: + The solution proposed here in novel - combining meta-learning on tasks to alleviate a key problem with IRL based approaches.
It builds on a recent approach by Achiam et al on Constrained Policy Optimization (oft- mentioned "CPO") and an accepted NIPS paper by Chow which introduces Lyapunov constraints as an alternative method.
The paper presents a pool-based active learning method that achieves sub-linear runtime complexity while generating high-entropy samples, as opposed to linear complexity of more traditional uncertainty sampling (i.e., max-entropy) methods. This paper proposed a query-synthesis-based active learning algorithm that uses GAN to generate high entropy sample; instead of annotating the synthesized sample, the paper proposed to find the most similar unlabeled data from the pool via nearest neighbor search, with the latter is the main contribution of the paper. I liked the whole idea of developing a faster algorithm for active learning based on the nearest neighborhood method. The main difference is the added nearest neighbor component, as GAAL is directly using the generated examples, thus achieving constant runtime complexity, rather than sub-linear. For example, the method outperforms random sampling for small datasets, based on number of samples, but what happens when you look at execution time?
This paper introduces an interesting unifying perspective on several sequence generation training algorithms, exposing both MLE, RAML and SPG as special cases of this unified framework. In summary, the contribution over RAML and SPG in combining them is quite incremental, and the practical importance of combining them is questionable, as is the integrity of the presented experiments, given how poorly MIXER is reported perform, and the omission of stronger baselines like SCST and AC methods. ==> Originality and significance The unifying framework is interesting, and helps shed new light on some standard issues in sequence generation. Significance    3/5: RAML and SPG have not been established as important methods in practice, so combining them is less interesting. This enables insightful new interpretations of standard issues in MLE training in terms of exploration for instance. The ideas are clearly presented, which is crucial in a paper trying to unify different approaches, and the new perspective on exploration is well motivated. - interesting unifying perspective on sequence generation algorithms
This work attempts to study the degree to which a layer by layer information bottleneck inspired objective can improve performance, as well as generally attempt to clarify some of the discussion surrounding Shwartz-Ziv & Tishby 2017. By using the method, the authors 1) validate the IB theory of deep nets using weight decay, and 2) provides a layer-wise explicit IB functional training for DNN which is shown to have better prediction accuracy. I believe the authors instead meant to say that the cited works deviate from the information bottleneck theory of learning suggested in (Shwartz-Ziv & Tishby 2017). Technically Achille & Soatto explicitly formed a variational approximation to the posterior over the weights of the neural network and so was not a "single bottleneck layer" as stated in the paper. This paper provides a method to do explicit IB functional estimation for deep neural networks inspired from the recent mutual information estimation method (MINE). This work is about layer-wise training of networks by way of optimizing the IB cost function, which basically measures the compression of the inputs under the constraint that some degree of information with respect to the targets must be preserved. The title, abstract and especially the conclusion ("This provides, for the first time, strong and direct emperical evidence for the validity of the IB theory of deep learning") seem to present the paper as somehow offering some clarity and further support for the assertions of the Shwartz-Ziv & Tishby 2017 paper, but that paper hoped to establish that information bottleneck can explain the workings of ordinary networks. On one hand, I find this paper interesting, because it aims at carefully studying the proposed link between DNN training and IB optimization, thereby showing that layer-wise IB training indeed seems to work very well in practice. Despite a recurring focus of the text that this paper applies and information theoretic objective at each layer of the network, and hence is novel, the final sentence of the paper suggests it might not actually be needed and single layer IB objectives can work as well. There is some discussion of the text suggesting they believe their method acts like an approximate weight decay, but there are no results showing the effect of weight decay just on the baseline classification accuracies they compare against.
It seems to me that (except the minor small section of streaming data), the paper is more like a proper verification of how tree-based learning algorithms work very well in tabular data--which is far from the basis of the paper and does not make the paper novel enough for *CONF*. This paper proposes a hybrid machine learning algorithm using Gradient Boosted Decision Trees (GBDT) and Deep Neural Networks (DNN).
However, here the authors have assumed that the latent space of the generative models are influenced only by the morphological properties of the image - which is wrong. Studying the properties of a generative model on such datasets is very challenging and the authors have not added a discussion around that. * Providing benchmark data for tasks such disentanglement is important but I am not sure generating data is sufficient contribution for a paper. 2. Extracting morphological properties of the image is straight-foward for MNIST kind of objects. This paper discusses the problem of evaluating and diagnosing the representations learnt using a generative model. However, when the entire image is subject to the generative model, it learns multiple properties from the image apart from shape too - such as texture and color. 3. Now assuming that my GAN model has learnt good representation in Morpho-MNIST dataset, is it guaranteed to learn good representations in other datasets as well? I'm not convinced that ability of a model in disentangling thickness correlates to their ability in natural image generation. Since their method is manually designed for MNIST, the manuscript would benefit from a justification or discussion on the  common pitfalls and the correlation between MNIST generation and more complex natural image generation tasks. Since the presented metrics do not show a significant difference between the VAE and Vanilla GAN model, the question remains whether evaluating on MNIST is a good proxy for the performance of the model on colored images with backgrounds or not. They suggest analysing performance of generative models based on these tools. Latent space features could be affected by the color or texture of the image as well. Then that method/data are a benchmark. Additionally, there are lot of low level pixel relations that the model learns to fit the distribution of the given images.
page 4, the concept of stationary point and general position can be introduced before presenting Theorem 1 to improve readability. 3. the notations of subgradient and gradient are used without claim The paper theoretically analyzes the sparsity property of the stationary point of layerwise l1-regularized network trimming. 1. While the paper analyzes the properties of the stationary point of the layerwise objective (5), the experiments seem to be conducted based on the different joint objective (8). The perspective of the proof is interesting: By chain rule, the stationary point satisfies nnz(W^j) linear equations, but the subgradients of the loss function w.r.t. the logits have at most N\\times ks variables.
Experiments: - While the reported compression rates are good, it is not clear to me what they mean in practice, because the proposed algorithm zeroes out individual parameters in the matrix W_l of each layer. The paper proposes a multi-layer pruning method called MLPrune for neural networks, which can automatically decide appropriate compression ratios for all the layers. This paper introduces an approach to pruning the parameters of a trained neural network. Main concerns / comments are: - Part of the novelty relays on computing the Hessian, and the algorithm goes for very large networks (parameter wise), why? Weaknesses: Novelty: - In essence, this method relies on the work of Marten & Grosse to approximate the Hessian matrix used in the Optimal Brain Surgeon strategy. The connections from all layers with the smallest loss increments are pruned and the network is re-trained to the final model.
The paper is definitely addressing an important problem as the authors cite many previous work which uses the setup of set of auxiliary tasks helping a main one. For Breakout experiment, single task actually outperforms all baselines and this means the proposed method results in a harm similar to ImageNet case. The paper is addressing the problem of a specific multi-task learning setup such that there are two tasks namely main task and auxiliary task. Authors suggest to further scale loss functions using the cosine similarity but it only experiments with the simpler case of binary decision of using both gradients or only the main one. The paper proposes a method for using auxiliary tasks to support the optimization with respect to a main task. The simple and sensible approach proposed in the paper is using cosine similarity between the gradients of two loss functions and incorporating the auxiliary one if it is positively aligned with the main gradient. 1) The proposed method is based on the intuition: if the gradients of the target and auxiliary loss are in the same direction, the auxiliary loss will help the main/target task. In ImageNet experiment, auxiliary tasks actually hurt the final performance as the single task is better than all methods including the proposed one. The author(s) also experiment the proposed method on three tasks, one supervised learning image classification task, two reinforcement learning tasks, and show improved results respectively. The paper studies the problem of how to measure the similarity between an auxiliary task and the target tasks, and further decide when to use the auxiliary loss in the training epoches. Especially experiments where auxiliary tasks have been used before would be interesting to test with the only addition being introducing the method proposed. When optimizing for the main loss function, the gradient of the auxiliary loss function is also used to update the shared parameters in cases of high cosine similarity with the main task. For Breakout+MSPacMan experiment, multi task and the proposed method performs almost exactly same.
As far as I understood from the paper, changing the objective function to the upper bound of f-divergence have two merits compared to the existing methods. The paper proposes a method for training generative models with general f-divergences between the model and empirical distribution (the VAE and GAN objectives are captured as specific instantiations). The application of upper bounds to general f-divergences for training generative models is novel as far as I know. This paper proposed a novel variational upper bound for f-divergence, one example of which is the famous evidence lower bound for max-loglikelihood learning. The main contribution of our paper is the introduction of an upper bound on the f-divergence." Specifically, optimizing the reverse KL (and/or Jensen-Shannon divergence) leads to sharper images, perhaps at the loss of some variance in the data distribution.
2. The major novelty of this approach is the use of annotations supporting images and textual (pretrained) embedding spaces, but no related work regarding Wes was neither introduced in the Related Work section nor was it clearly explained in the text. Summary ========= The authors present an extension to the VAE model by exploring the possibility of using the label space to create a new embedding space, which they call Probabilistic Semantic Embedding (PSE).
The paper proposes a novel method called Manifold Mixup, which linearly interpolating (with a careful selected mixing ratio) two feature maps in latent space as well as their labels during training, aiming at regularizing deep neural networks for better generalization and robust to adversarial attacks. The authors experimentally show that networks with Manifold Mixup as regularizer can improve accuracy for both supervised and semi-supervised learning, are robust to adversarial attacks, and obtain promising results on Negative Log-Likelihood on held out samples. The authors suggest that Mixup can suffer from interpolations intersecting with a real sample, but how Manifold Mixup can avoid this issue is not very clear to me. 4. It would be useful to also present the results for SVHN for supervised learning since the Cifar10 and Cifar100 datasets are similar, and the authors have already used SVHN for other task in the paper.
Under several assumptions (input is Gaussian, non-linear activation is strictly increasing, stable system) it is shown that SGD converges linearly to the ground truth system with near-optimal sample complexity. This work considers the problem of learning a non-linear dynamical system in which the output equals the state. The proof idea is to reduce this problem to the problem of learning a single non-linear neuron in the case that the covariance matrix of the data is well-conditioned. However, understanding non-linear dynamical systems is extremely challenging and this paper provides strong convergence guarantees. The paper studies discrete time dynamical systems with a non-linear state equation. This paper studies the ability of SGD to learn dynamics of a linear system + non-linear activation.
Pros: the authors develop a novel GAN-based approach to denoising, demixing, and in the process train generators for the various components (not just inference). As authors mention in the paper, it is indeed surprising that the proposed GANs-based model with two generators is able to produce samples from the distribution of each signal component by observing only additive mixtures of these signals. For denoising, the authors propose to learn the latent variable that generates the clean test image by solving a ridge regularized non-convex inverse problem (Eq. 3). Actually in the appendix, comparisons are provided for a basic compressive sensing problem, but their only comparator is "LASSO" with a "fixed regularization parameter", and vanilla GAN. In this, paper a GANs-based framework for additive (image) denoising and demixing is proposed. This paper proposed two new GAN structures for learning a generative modeling using the superposition of two structured components. The proposed method is evaluated on both tasks (i.e., denoising and demixing) by conducting toy experiments on handwritten digits (MNIST). Regarding demixing, as explained in the comments below, the proposed model appears to be superficial in the sense that neither theoretical analysis nor thorough empirical evaluation is provided.
My problem here is that authors employ convoluted arguments to introduce this geometric mean prediction and the average prediction, without making the connection discussed above. So, I don't find authors provide convincing answers to the raised questions at the beginning of the paper about the use of dropout when making predictions. We consider the dropout rate a hyper-parameter of the model, the standard learning theory tells us to fix the parameters with the training data and evaluate them later when making predictions.
The authors seek to make it practical to use the full-matrix version of Adagrad's adaptive preconditioner (usually one uses the diagonal version), by storing the r most recently-seen gradient vectors in a matrix G, and then showing that (GG^T)^(-½) can be calculated fairly efficiently (at the cost of one r*r matrix inversion, and two matrix multiplications by an r*d matrix). Rather than adapting diagonal elements of the adaptivity matrix, the paper proposes to consider a low-rank approximation to the Gram/correlation matrix. In particular, I think that the proposed algorithm has a more-than-superficial resemblance to stochastic LBFGS: the main difference is that LBFGS approximates the inverse Hessian, instead of (GG^T)^(-½). I'm glad to see that the authors considered adding momentum (to adapt ADAM to this setting), and their experiments show a convincing benefit in terms of performance *per iteration*. The authors also discuss how to adapt the proposed algorithm with Adam style updates that incorporate momentum. However, their algorithm--while much less computationally expensive than true full-matrix adaptive preconditioning---is still far more expensive than the usual diagonal version. Given that rxr is a small constant sized matrix and that matrix-vector multiplication can be efficiently computed on GPUs, this matrix adapted SGD can be made scalable. In this paper, the authors show how to maintain and update this pxp matrix by storing only smaller matrices of size pxr and rxr, and performing 1.
* Significance: The concept of converting a non-differentiable pipeline to a differentiable version is indeed very useful and widely applicable, but the experimental section did not convince me that this particular method indeed works: the results show a very small improvement (0.7-2%) on a single system (Faster R-CNN), that has already been pretrained (so not clear if this method can learn from scratch). The paper proposes a method for converting a non-differentiable machine learning pipeline into a stochastic, differentiable, pipeline that can be trained end-to-end with gradient descent approaches. The submission describes a method for smoothing a non-differentiable machine learning pipeline (such as the Faster-RCNN detector), so that gradient-based methods may be applied to jointly train all the parameters of the pipeline.
Given that the ground truth attribute decomposition for MNIST is not known, even the qualitative results are impossible to evaluate. This paper proposes a generalization of variational auto-encoders to account for meta-data (attributes), learning new ones, in a way that these can be controlled to generate new samples. This paper proposes to automatically discover these attribute and thus work to produce variations even in the absence of known attribute information. The paper proposes a generative network capable of generating variations of a given input, conditioned on an attribute. While the idea is interesting, the paper is lacking an experimental section, so the methodology is impossible to evaluate.
The paper proposes a new algorithm for implicit maximum likelihood estimation based on a fast Nearest Neighbor search. Summary: This paper proposes a nearest-neighbor-based algorithm for implicit maximum likelihood. Evaluation:  This paper presents an interesting contribution: an implicit likelihood estimation algorithm amenable to theoretical analysis. The paper shows that under some conditions the optimal solution of the algorithm corresponds to the MLE solution and provides some experimental evidence that the method leads to a higher likelihood. Algorithm - While significant advancements have indeed been made for nearest neighbor evaluation as the authors highlight, it's hard to believe without any empirical evidence that nearest neighbor evaluation is indeed efficient in comparison to other methods of likelihood evaluation. The prior work trained the same normalizing flow model via maximum likelihood and adversarial training, and observed vastly different results on likelihood and sample quality metrics. The authors are clearly aware of the current research in generative modeling but the current work provides almost no strong evidence to consider this work as an alternative to other approaches.
This paper proposes to use a  stochastically quantized network combined with adversarial training to improve the robustness of models against adversarial examples. The new thing is that the authors found that quantization of activation function improves robustness, and the approach can be naturally combined with FGSM adversarial training.
This paper proposes a method to train a machine translation system using weakly paired bilingual documents from Wikipedia. Summary The authors propose a relatively simple approach to mine noisy parallel sentences which are useful to greatly improve performance of purely unsupervised MT algorithms. c) training the usual unsup MT pipeline with two additional losses, one that encourages good translation of the extracted parallel sentences and another one forcing the distribution of words to match at the document level. - Are the Supervised results in Table 2 actually a fair reflection of a reasonable NMT model trained with sub-word representations and back translated data? (2017) and that performance is heavily dependent on the number of sentences extracted from the weakly aligned documents. It would be useful to know the proportion of sentences you managed to extract to train your models.
The paper proposes a new graph pooling method by learning the node assignments from a CRF based structure prediction formulation. This paper studied to use CRF to define the cluster assignment of the nodes  for graph pooling, which model the dependency of the cluster assignments between the nodes. Different from existing pooling methods, the proposed model explicitly captures high-order structural relationships between nodes via a CRF, and the pairwise energy in the CRF is defined by a l-hop connection instead of the original neighborhoods.
Summary: the authors introduce a method to learn a deep-learning model whose loss function is augmented with a DPP-like regularization term to enforce diversity within the feature embeddings. However, learning diverse features via DPPs with deep learning frameworks is challenging due the instability in computing the gradient which involves a matrix inverse operation. Better marriage between deep learning and DPPs is an important problem as diversity is crucial in many machine learning problems (even beyond computer vision - the experimental domain of this paper - e.g. in machine translation and document summarization).
The paper presents a case study of training a video classifier using convolutional networks, and on how the learned features related to previous, hand-designed ones. the paper uses model interpretation techniques to understand blackbox CNN fit of zebrafish videos. This is evident in this paper with the authors calling CNNs "black box" and the learnings of a neural network "cheating". Perhaps the authors are also not aware that the fallacies that causes CNNs to overfit on some characteristics in the input data are also present in other machine learning tools such as SVMs. Perhaps the authors are not aware that CNNs are hardly black boxes, their inner workings quite transparent in mathematical terms, which the submitted paper itself explores. while the experimental studies rely on our belief that the interpretation technique indeed interprets, the result that removing experimental features and improving predictive performance is convincing and interesting. The hype over deep learning has caused certain disdain among a section of the research community over the workings of deep neural networks. In order to identify which particular features the neural networks are paying attention to, the paper used Deep Taylor Decomposition, which allowed the authors to identify "clever-hans"-type phenomena (network attending to meaningless experimental setup differences that actually gave a way the ground truth classes). Furthermore, the authors conduct one good analysis (DTD) to explain the results of their CNN. the idea of a case study about the usefulness of model interpretation techniques is interesting.
2. The encouraging result in Table 1 in EMPIRICAL EVALUATION shows the consistent outperformance of MaSS over SGD and Nesterov SGD regardless of the changing learning rates. This paper shows the non-acceleration of Nesterov SGD theoretically with a component decoupled model. It would have been interesting to see how the results from fig3 generalize to the non convex problems considered in the paper.
Summary This paper studied the expressive power of graph NNs, specifically, their universality and limitations under the non-anonymous setting, via the theory of distributed computations. In this paper, results in distributed computing are reformulated in a GNN framework mapping the number of rounds required by a local algorithm to the depth of the GNN in order to solve a given graph problem in a worst case scenario. The key idea is to reduce the computation model of graph NNs to LOCAL (for Turing completeness) or CONGEST (for limitations), which are well-studied in the literature of distributed computations and use the known results for these models.
This work proposes to leverage a pre-trained semantic segmentation network to learn semantically adaptive filters for self-supervised monocular depth estimation. The paper proposes a using pixel-adaptive convolutions to leverage semantic labels in self-supervised monocular depth estimation. Similarly, Fig 4 shows there is advantage in the use of the pre-trained semantic network, it is not clear if this difference is significant. to learn content-adaptive filters that are conditioned on the features of the pre-trained semantic segmentation network.
This paper proposed a neural iterated learning algorithm to encourage the dominance of high compositional language in the multi-agent communication game. From a utility perspective, the paper doesn't go into how this might be practically applied in general to train neural agents to learn compositional languages in more complex environments (where they might be simultaneously speakers and listeners), as they stick to a very simple symbolic referential game. The main contribution of this paper is really: "studying how neural networks behave when trained in an iterated learning setting in a simple referential game".
### Summary: This paper propose a new model for learning generic feature representations for visual-linguistic tasks by pretraining on large-scale vision and language datasets like Conceptual Captions and language-only datasets like BookCorpus and English Wikipedia. ### Other questions: - When training on text-only datasets, what is the input on Visual Feature Embedding since there are no associated images. While ViLBERT designs for easier extendable for other modalities, VLBERT is more focus on the representation learning on the vision and language, since the caption input also combines with the visual feature embedding. This paper proposed a pre-trainable generic representation for visual-linguistic tasks call VL-BERT.
The paper proposes a metric for unsupervised model (and hyperparameter) selection for VAE-based models. The UDR score can be used for unsupervised hyperparameter tuning and model selection for variational disentangled method. This method relies on a key observation from this paper [A] viz., disentangled representations by any VAE-based model are likely to be similar (upto permutation and sign). 2019], the authors adopt the assumption that disentangled representations are all alike (up to permutation and sign inverse) while entangled representations are different, and propose UDR method and its variants. -- To validate the fundamental assumption of UDR, the authors might consider to quantitatively validate that, disentangled representations learned by those approaches you used in the paper are almost the same (up to permutation and sign inverse).
This paper proposes two modifications for the MixMatch method [1] and achieves improved accuracy on a range of semi-supervised benchmarks. Mix-Match is already an elaborate method, and ReMixMatch additionally introduces learned data augmentation, an additional loss term for matching label distributions between labeled and unlabeled data, consistency-loss, and a self-supervised loss (section 3.3). Augmentation anchoring instead of computing the guessed probabilities on unlabelled data as the average probabilities on transformed samples (as in MixMatch), it considers as guessed labels the average probabilities obtained from weak transformations (flip+crop) even when using stronger transformations (Autoaugment like).
Overview: This paper proposes to use semi-supervised learning to enforce interpretability on latent variables corresponding to properties like affect and speaking rate for text-to-speech synthesis. Controlling the noise level is quite important for end-to-end TTS, and I think this method can fit this direction because we can easily obtain the noise attribute (supervision) by data simulation or annotate the noise.
Note that most likely the proposed method would give an even bigger boost with "real" minecraft, as there are even more non-informative observations between the interesting ones, and furthermore I think the environment choice made in this paper is fine, as it still demonstrates the point. The primary claim is that simple components which compute elementwise sum/average/max over the activations seen over time are highly robust to noisy observations (as encountered with many RL environments), as detailed with various empirical and theoretical analyses. This paper studies reinforcement learning for settings where the observations contain noise and where observations have long-range dependencies with the past.
3. By combining all the techniques, the proposed method yields promising performance when training deep models with different batch sizes. Equipped with the proposed techniques, the authors obtain promising results when training deep models with various batch sizes. The authors discuss four techniques to improve Batch Normalization, including inference example weighing, medium batch size, weight decay, the combination of batch and group normalization. (3) In Section 3.1, "we need only figure out …" should be "we only need to figure out …" The paper introduces four techniques to improve the deep network model through modifying Batch Normalization (BN). 6. Note that training deep models with non-i.i.d. minibatches is a typical case to evaluate normalization methods, e.g., Batch Renormalization.
This paper introduces Precision Gating, a novel mechanism to quantize neural network activations to reduce the average bitwidth, resulting in networks with fewer bitwise operations. To show that the resulting lower average bitwidth gained with PG leads to increased performance, the authors implement it in Python (running on CPU) and measure the wall clock time to execute the ResNet-18 model. The idea is to have a learnable threshold Delta that determines if an output activation should be computed in high or low precision, determined by the most significant bits of the value. I do think that the work is slightly premature, and would benefit significantly from adding GPU results and additional experiments.
Weaknesses: - The experimental comparison only included old baselines and the authors should compare to some more recent work such as TA-GAN (NIPS18), and Object-GAN (CVPR19). This paper proposed VHE-GAN for the text-to-image generation task. Their proposed VHE-GAN model encodes an image to decode its associated text and feeds the variational posterior as the source of randomness into the GAN image generator. (VHE) randomized generative adversarial network (GAN) that integrates a probabilistic text decoder, probabilistic image encoder, and GAN into an end-to-end multimodal model. This paper proposes a combined architecture for image-text modeling. - while it is difficult to dilute such a complex model to 8 pages, and the included appendix clarifies many questions in the text body, it would be worth further passes through the main paper with a specific focus on clarity and brevity, to aid in the accessibility of this work. Therefore, the authors should claim how the proposed VHE variational posterior can help the task. The proposed method utilizes the off-the-shell modules and feeds the VHE variational posterior into the generator.
Under the general framework of A2C, the core contribution of the paper is to apply a graph attention network on the knowledge graph to help learn better representation of the game state and reduce the action space. This paper proposes a knowledge graph advantage actor critic (KG-A2C) model to allow an agent to do reinforcement learning in the interactive fiction game. The authors propose an agent that builds a dynamic knowledge graph of each state from the textual observation provided by the games, while choosing actions from a template-based action space. One of the main contribution in the paper is to represent the partial observations of the world as a knowledge graph, and so as to efficiently infer appropriate actions. Even though true, I think, this argument doesn't hold in the context of recent progress in NLP for problems like dialog modeling, where the action space of generating responses is even larger.
For example, it is a bit unclear why f_Z should be with low curvature -- does it mean that you wish the control problem in the latent domain is more like a linear dynamical system, so that the LLC algorithm works better? This work proposes a regularization strategy for learning optimal policy for a dynamic control problem in a latent low-dimensional domain.
Next to some experiments on data sets with a manifold structure, the main contribution of the paper is a tandem of theorems that state the conditions under which models can recover the topology---or the number of connected components---of a data set correctly, However, the main theoretical result on the number of connected components only applies to mixtures of Gaussian distributions, but the purported scope of the paper is the analysis of manifold-based defences in general. If the number of connected components does not match, based on theorem 2, Corollary 1 argues that a generative model can generate an adversarial example that does not exist in the data-generating manifold. This paper studies robustness to adversarial examples from the perspective of having 'topology-aware' generative models. I have to state that I am _not_ an expert in adversarial examples, but an expert in topology-based methods; I consider this paper to belong to the latter field given its theoretical contributions about 'recovering'
- Via experiments it is demonstrated that proposed approach provides compression of the model while having close to the baseline quality for image classification and object detection tasks and for small and large datasets. One minor weakness of the experiments on classification (3.1) is that the training procedure for FSNet and the baselines are different, with FSNet using cyclic learning rates and a larger number of epochs, making the results somewhat difficult to interpret. - Proposed method gives better compression factor and better quality than state-ot-the-art models for classification and object detection tasks. "efficiency searching scheme" This paper proposes a method, termed as Filter Summary (FS), for weight sharing across filters of each convolution layer.
The paper suggest the method to train neural networks using 8 bit floating point precision values. The paper focuses on training neural networks using 8-bit floating-point numbers (FP8). In this work the authors propose an 8-bit floating point format (denoted S2FP8) for tensors. On the downside, the first sections give the impression that FP32 is not used at all: "S2FP8 does not require keeping the first and last layer in FP32 precision, which is needed for other approaches (Mellempudi et al., 2019).". The proposed approach S2FP8 eliminates the need for loss scaling and does not require keeping some of the layers in FP32 precision as in Mellempudi et al. This requires an additional step to accumulate the summary statistics of each tensor in order to convert from FP32 to S2FP8 (the mean and the max of the tensor elements in log space). I am by no means not a hardware design expert, but I am not convinced that the gain of using 8 vs 16 bit floating point numbers outweights any extra complexity of hardware implementation.
The authors present a biologically inspired sleep algorithm for artificial neural networks (ANNs) that aims to improve their generalization and robustness in the face of noisy or malicious inputs. They present a detailed comparative study spanning three datasets, four types of adversarial attacks and distortions, and two other baseline defense mechanisms, in which they demonstrate significant improvements (in some cases) of the sleep algorithm over the baselines. Section 1: Introduction "We report positive results for four types of adversarial attacks tested on three different datasets (MNIST, CUB200, and a toy dataset) ..." The paper proposes an ANN training method for improving adversarial robustness and generalization, inspired by biological sleep. Disclosure on reviewer's experience: I am not an expert on adversarial attack methods or defenses, but I am well read in the general literature on robustness and uncertainty in deep neural networks. Although the results are (I would argue) somewhat mixed, they are nonetheless positive enough to encourage more work in applying "sleep" and other relevant ideas from neuroscience to the problem of robustness in deep neural networks.
This paper tries to provide a deeper understanding of the training dynamics of GANs in practice via characterizing and visualizing the rotation and attraction phenomena nearby a locally stable stationary point (LSSP) and questions the necessity to access a differential/local Nash equilibrium (LNE). Along with this, the authors propose to look at the eigenvalues of the game Jacobian and the individual player Hessian's to evaluate convergence in GANs. The paper presents application of the tools on GANs trained with NSGAN and WGAN-GP objectives on a mixture of Gaussians, MNIST, and CIFAR10.
The main result is that the use of upsampling via a fixed interpolation filter provides an inductive bias towards "natural" images. This paper studies the theoretical reasons why a randomly initialized decoder or autoencoder like architecture can prove useful for image denoising, by using early stopping. The paper provides a theoretical study of regularization capabilities of over-parameterized convolutional generators trained via gradient descent, in the context of denoising with an approach similar to the "deep image prior".
Experiments are performed to evaluate the proposed method against baselines on 3D object detection (both in the semi-supervised setting and unsupervised moving object detection), 3D motion estimation, as well as sim-to-real transfer results (training in CARLA and testing on the KITTI dataset). Results demonstrate the strengths of the proposed view-contrastive framework in feature learning, 3D moving object detection, and 3D motion estimation. The results show that the proposed method: 1) has higher 3D object detection mAP than a view regression-based baseline from prior work (Tung 2019) for settings with little available 3D boundign box supervision; (4) As the latent map update module uses an RNN, it would be good to consider consistency beyond 2 frames (given mask is applied to view-contrastive objective). Also, because the view-contrastive loss is applied at feature-level, reviewer would like to know performance on detecting small objects. To facilitate the 3D view-contrast learning, this paper proposed a novel 2D-3D inverse graphics networks with a 2D-to-3D un-projection encoder, a 2D encoder, a 3D bottlenecked RNNs, an ego-motion stabilization module, and a 3D-to-2D projection module. Moreover, the proposed model is evaluated on the downstream 3D object detection tasks to demonstrate the utility of the learned 3D visual representation. 2019), this paper introduced a novel view-contrast objective applied to its internal 2D and 3D feature space. This model is used to learn a 3D visual representation that can be applied for semi-supervised 3D object detection, and for unsupervised 3D moving object detection.
This paper tackles the problem of learning with noisy labels and proposes a novel cost function that integrates the idea of curriculum learning with the robustness of 0/1 loss. When we have noisy labeled data, instead of motivating the use of 0-1 loss by suggesting that Then, it was extended to the multiclass loss by the following paper: [3] Ghosh et al.: Robust loss functions under label noise for deep neural networks.
I think the main contribution of the paper is that it raises some questions over existing methods/trends in solving exploration problems in reinforcement learning by comparing the performance of multiple methods across various games in ATARI suite. ------------------------------------------------------------------------------------------------------------------------------------------------------------------------- Summary: This paper presents a detailed empirical study of the recent bonus based exploration method on the Atari game suite. Throughout the paper, the experiments and results raise questions on the robustness and generalization of existing exploration methods across various ATARI games, but the paper puts absolutely zero effort into investigating if there is a quick fix to the questions it poses. To support their claim, the authors firstly compare bonus exploration methods, noisy networks, and epsilon-greedy on hard exploration games. The authors combine Rainbow with different exploration methods, such as count-based bonus methods, curiosity-driven methods, and noisy networks. Here are a couple of points that I felt conflicted/confused about the paper: - The conclusion of the paper is that 'progress of exploration in ATARI suite is obfuscated by good results in single domain'.
The authors present a quantum algorithm for approximating the forward pass and gradient computation of a classical convolutional neural network layer with pooling and a bounded rectifier activation. - There's a clear separation of background (which is concise and well explained) and contributions, but maybe it would be worth connecting the introduced algorithm more closely to existing work in non-convolutional quantum neural networks? This submission proposed a quantum convolutional neural network (QCNN). The manuscripd describes a creative way to bring bolean operartions-defined non-linearities into quantum neural networks, at the cost of having to force the system back to classical domain at each layer - and then encoding it back to qubits for the next layer operations.
This paper provides a new technique to adapt a source neural network performed well on classification task to image segmentation and objective detection tasks via the author called parameter-remapping trick. The paper proposes a method called FNA (fast network adaptation), which takes a pretrained image classification network, and produces a network for the task of object detection/semantic segmentation. The paper's overall method is a novel one, unifying NAS on det/seg tasks, while prior works mostly only focus on one task. Could the authors clarify that you compared with every recent high-performance NAS method on seg/det tasks? They do this by first expanding the network into a "supernet" and copy weights  in an ad-hoc manner, then, they perform DARTS-style architecture search before fine-tuning for the task at hand. For example, using exactly the same NAS search method and supernet, and comparing the FNA method with that not using a pretrained model (i.e., directly search on det/seg) could be a good experiment to showcase the importance of adaptation. But no one ever pretrain every classification network for searching on det/seg tasks right? If the authors faithfully compared with state-of-the-art methods in search det/seg architectures, but I'm not super familiar with this literature. In this paper, the authors take a MobileNet v2 trained for ImageNet classification, and adapt it either (i) semantic segmentation on Cityscapes, or (ii) object detection on COCO.
# Summary This paper proposes a new way to learn the optimal transport (OT) cost between two datasets that can utilize subset correspondence information. Summary: The paper presents a gradient-based method for learning the cost function for optimal transport, applied to dataset alignment. # Originality - This paper considers a new problem which is to learn OT given "subset correspondence" information. # Significance - This paper seems to present a neat and sensible idea for learning OT with neural networks given subset correspondence information.
The paper shows that in the worst case, the proposed method has the same gradient query complexity as the SpiderBoost variance reduction method. The paper demonstrates the gradient query complexity improvement over the SpiderBoost method on MNIST and CIFAR-10 data set.
In this paper, the authors proposed two methods of Nesterov Iterative Fast Gradient Sign Method (NI-FGSM) and Scale-Invariant attack Method (SIM) to improve the transferability of adversarial examples. In this paper, the authors apply the Nesterov Accelerated Gradient method to the adversarial attack task and achieve better transferability of the adversarial examples. Experiments are carried out to verify the scale-invariant property and the Nesterov Accelerated Gradient method on both single and ensemble of models. Two methods have been proposed,  namely Nesterov Iterative Fast Gradient Sign Method (NI-FGSM) and Scale-Invariant attack Method (SIM).
The authors proposed a new method called "DropEdge", where they randomly drop out the edges of the input graphs and demonstrate in experiments that this technique can indeed boost up the testing accuracy of deep GCN compared to other baselines. I also like the discussion in sec 4.3 where the authors explicitly clarify what are the difference between DropEdge, Dropout and DropNode, as the other two are the methods that will pop up during reading this paper. The authors propose a simple but effective strategy that aims to alleviate not only overfitting, but also feature degradation (oversmoothing) in deep graph convolutional networks (GCNs).
Particularly, the authors propose a new benchmark PG-19 for long-term sequence modeling. 2) The authors mention that this memory compression architecture enables long sequence modeling. This paper proposes a way to compress past hidden states for modeling long sequences. A variety of compression techniques and training strategies have been investigated in the paper and verified using tasks from multiple domains including language modeling, speech synthesis and reinforcement learning. The paper also introduces a new benchmark for long-range dependencies modelling composed of thousands of books. The authors put huge amount of effort into the experiments but only describe the proposed technique in a very rough and abstract way, lacking necessary technical details to formulate the technique. For testing and evaluating the modeling of really long context sequence modeling, the authors introduce PG-19, a new benchmark based on Project Gutenberg narratives. The outcome is a versatile model that enables long-range sequence modeling, achieving strong results on not only language model tasks but also RL and speech. The paper finally presents an analysis of the compressed memory and provide some insights, including the fact that the attention model uses the compressed memory. ## Original review This paper presents a new variation of the Transformer model, named Compressive Transformer. While this paper is more incremental and novelty may be slightly lacking, I think the breadth of experiments and competitive results warrants an acceptance. The key novelty of this model is to preserve long range memory in a compressed form, instead of discarding them as previous models have done. I think this paper should be accepted, mainly because: - The proposed model is novel as far as I can tell.
The authors present an algorithm CHOCO-SGD to make use of communication compression in a decentralized setting. The authors consider CHOCO-SGD for non-convex decentralized optimization and establish the convergence result based on the compression ratio. Extensive empirical results are presented in this paper and the two use cases highlight some potential usage of the algorithm. First, the authors only provide analysis on CHOCO-SGD but the comparison with baselines are based on their momemtum versions. This paper studies non-convex decentralized optimization with arbitrary communication compression. They compare CHOCO-SGD under various compression schemes with the baseline. The momemtum version of CHOCO-SGD is also provided although no theoretical analysis is presented.
The authors consider several of these choices and then run a variety of experiments on small latent-dimension cases for VAEs. These reveal that sometimes non-Euclidean and in particular product spaces improve performance. Summary: This paper devised a framework towards modeling probability distributions in products of spaces with constant curvature and showed how to generalize the VAE to learn latent representations on such product spaces using Gaussian-like priors generalized for this case. I believe this work should be accepted, as while the numerical results are not particularly impressive, it provides some clear foundational work for further exploration of the use of non-euclidean latent spaces in VAEs. --I would have appreciated a more clearly delineated discussion on how the technical details of this work overlap with past papers, both those that have investigated product spaces (Gu et al 2019) and single curvature spaces in VAEs (spherical & hyperbolic)? Strengths, Weakness, Recommendation I like what the authors are trying to do here; embeddings and discriminative models on non-Euclidean spaces have been developed, offer credible benefits, and generative models are the next step.
The authors observed in Section 4.2 that it is very important for the teacher and student to have similar architectures, but did not explain the more important question that why a real-valued network would be able to teach a binary network, since they have quite different information flow. Some specific questions: - Why the real-valued teacher can help train the binary network while they have different information flow? this paper reviews the current literature in Binary Neural Networks and the authors compile the existing methods to build a strong baseline. If the goal is to develop a compute- and memory-efficient architecture, it would be good to also consider real-valued-network baselines that were proposed with computational and/or memory efficiency as a design goal.
https://openreview.net/forum?id=rJl-b3RcF7 This paper studies the tasks of pruning filters, a provable, sampling-based approach for generating compact Convolutional Neural Networks (CNNs). Summary: In this paper, the author propose a provable pruning method, and also provide a bound for the final pruning error. This paper attacks the problem of pruning neural networks to obtain sparser models for deployment. To achieve a more reasonable algorithm, author prune the redundant channel by controlling the deviation of the summation statistically small,  and reusing the filter by important sampling the given channel. But ever since the "lottery" papers, I think it makes sense in locating the sparse and representative pruned structure, which can achieve better performance than full overparameterized model. Unlike the other deterministic method, sampling skill suffer variance propagation problem, the pre-layer variance will affect the sampling probability of next layer, how this pruning work if we change status of the pre-layer,  I didn't find any theoretical guarantee and only find a proof of single layer reconstruction bound. Besides, Author should also consider an experiment in modern lightweight network, vgg and resnet like model are out of fashion and so big that any one can make a sound result on it.
Alike other HRL agents, their method has two types of policies (manager and subpolicies), but different from other works they do not keep parameters fixed in post training for new tasks. This paper is under the research area of "hierarchical reinforcement learning." However, just like temporal abstraction, the HRL is a general idea instead of an existing problem formulation or a particular algorithm. It seems that the author is not aware of this point as the paper claims a particular way of achieving HRL is the HRL itself (in section 4.1 "In the context of HRL, a hierarchical policy with a manager πθh(zt|st) selects every p time-steps one of n sub-policies to execute."). But I don't think most works are limited in this aspect, as claimed by the paper in the abstract. I agree with the author that the agent needs to adapt its skills when faced with new tasks. The authors clearly position their work in the HRL paradigm and explain current limitations/challenges within that field. 2. I think the author didn't justify his key design choices well.
This paper describes an approach to applying attention in equivariant image classification CNNs so that the same transformation (rotation+mirroring) is selected for each kernel. [Original review] The authors propose a self-attention mechanism for rotation-equivariant neural nets. They show that introduction of this attention mechanisms improves classification performance over regular rotation-equivariant nets on a fully rotational dataset (rotated MNIST) and a regular non-rotational dataset (CIFAR-10). I think this paper should be accepted, because it presents a novel and non-trivial concept (rotation-equivariant self attention).
This paper proposes to use the robust subspace recovery layer (RSR) in the autoencoder model for unsupervised anomaly detection. ============================ The paper proposes using Robust Subspace Recovery in combination with an autoencoder (and possibly GANs) for anomaly detection.
[4] Danilo Jimenez Rezende, Shakir Mohamed, Daan Wierstra, Stochastic Backpropagation and Approximate Inference in Deep Generative Models, 2014 Overview: This paper introduces a new method for uncertainty estimation which utilizes randomly initialized networks. They then evaluate their approach on an out-of-distribution detection task, they measure the calibration of their uncertainty estimates and finally perform a small ablation study for their concentration result. Additional comments / questions: (somewhat minor) p1: "While deep ensembles …, where the individual ensembles are trained on different data" - here and related text, it should probably be "individual models" / "individual networks".
[Major Comments] Predicting the middle point between two states for modeling the dynamics via deep neural networks is not new, but I did not know any other works that use this idea for controlling PDEs. In this paper, the authors outline a method for system control utilizing an "agent" formed by two neural networks and utilizing a differentiable grid-based PDE solver (assuming the PDE describing the system is known). [Summary] This paper proposes to combine deep learning and a differentiable PDE solver for understanding and controlling complex nonlinear physical systems over a long time horizon. [Detailed Comments] In Section 3, the authors claim that their model "is conditioned only on these observables" and "does not have access to the full state." However, the model requires a differentiable PDE solver to provide the gradient of how interactions affect the outcome. ## Summary The authors propose a method for training physical systems whose behavior is governed by partial differential equations.
Summary: This paper proposes the use of two intrinsic rewards for exploration in MARL settings. Overall, I would strongly argue for accepting this paper if comparisons to single-agent exploration methods were added. This paper proposes methods for incentivizing exploration in multi-agent RL. Main Comments: Overall, I think this paper would be a good contribution for *CONF* 2020 and I lean towards accepting it.
But the fact that you have this predicate lets the model do these filters and greater-than comparisons inside the network in an opaque way, while also getting interpretable operations for some questions (table 5 is further confirmation of this, and of the fact that you probably are not capturing many of more the complex, compositional questions in DROP). Another question raised by the passage-span predicate: the more you use bare passage-span programs for training, the more the network learns to put all of its compositional reasoning inside, in an opaque way, instead of giving you interpretable compositionality. This paper discusses an extended DSL language for answering complex questions from text and adding data augmentation as well as weak supervision for training an encoder/decoder model where the encoder is a language model and decoder a program synthesis machine generating instructions using the DSL. --the diff sum example in table 2 is confusing; the program appears to sum up the numbers but the result is a subtraction without a sum operation in it. Interpretability: The use of passage-span as a predicate is really interesting, and it raises a lot of questions. Because you're selecting passage spans directly instead of performing some kind of matching operation, you have to have your search select all of the appropriate spans for this to be "interpretable", and not just hiding the logic inside of the network. To me, someone who is intimately familiar with this research area, the key contributions (the things that I learned) are (1) using passage-span and key-value predicates actually works, (2) how much difference hard EM and thresholding make, and (3) the data augmentation in this work is pretty clever. This predicate lets the model shortcut any interpretable reasoning and do operations entirely inside the encoder/parser. Overall, I like the paper and I think it contains simple extensions to previous methods referenced in the paper enabling them to work well on these datasets. This paper presents a semantic parser that operates over passages of text instead of a structured data source.
We encourage the authors to layout in the beginning the derivations form this point of view which will make the paper easier to digest, the expression in Equation 7 seems mysterious and pulled out of a hat, but it is easier to understand by going to perturbation analysis usually done on KL for Fisher Natural gradient and to do it also here starting from the linearization of W2 with ||.||H−1(q)  , and how to approximate it in RKHS as it was already proposed in the literature in Mroueh et al Sobolev Descent. It's important to be able to estimate natural gradient in a practical way, and there have been a few papers looking at this problem but mostly for the case with a Fisher-Rao metric. This paper takes a different and general approach to approximate natural gradient by leveraging the dual formulation for the metric restricted to the Reproducing Kernel Hilbert Space. The authors propose an approximate of the natural gradient under Wasserstein metric when optimizing some cost function over a parametric family of probability distributions.
After highlighting the said properties in context of related work, the authors propose an approach to calculate the context-independent importance of a phrase by computing the difference in scores with and without masking out the phrase marginalized over all possible surrounding word contexts (approximated by sampling surrounding context for a fixed radius under a language model). I like that the authors performed this ablation given that the expectation over surrounding contexts is computed approximately via samples under a language model. This paper proposes a hierarchical decomposition method to encode the natural language as mathematical formulation such that the properties of the words and phrases can encoded properly and their importance be preserved independent of the context.
The authors demonstrate that learnable K-matrices achieve similar metrics compared to hand-crafted features on speech processing and computer vision tasks, can learn from permuted images, achieve performance close to a CNN trained on unpermuted images and demonstrate the improvement of inference speed of a transformer-based architecture for a machine translation task. Even though in the worst-case complexity is not optimal, the authors argue that for matrices that are commonly used in machine learning architectures (e.g. circulant matrix in a convolution layer) the characterization is optimal.
The construction of the dataset focuses on demonstrating that compositional action classification and long-term temporal reasoning for action understanding and localization in videos are largely unsolved problems, and that frame aggregation-based methods on real video data in prior work datasets, have found relative success not because the tasks are easy but because of dataset bias issues. The paper introduces CATER: a synthetically generated dataset for video understanding tasks. The new dataset is the first to account for all of the following fundamental aspect of videos: temporal ordering, short- and long term reasoning, and control for scene biases. This paper proposed a new synthetic dataset (CATER) for video understanding. Due to the inherent biases in available action recognition datasets, models that simply averages video frames do nearly as well as models that take temporal dependencies into account. The compositional action classification task is harder and shows that incorporating LSTMs for temporal reasoning leads to non-trivial performance improvements over frame averaging. In order to address this problem, they design this fully observable synthetic dataset which is built upon CLEVER, along with three tasks that are customized for temporal understanding.
The score network is a deep learning model with transformer and convolution layers, and the post-process network is solving a constrained optimization problem with an T-step unrolled algorithm. The authors decompose this problem into two part: (i) predicting an affinity score between each base pair in the input sequence, using a combination of a transformer sequence encoder network and a convolutional decoder, and (ii) a post-processing step that ensures that structural local and global constraints are enforced. This paper proposes E2Efold, which is an RNA secondary structure prediction algorithm based on an unrolled algorithm. This paper introduces an end-to-end method to predict the secondary structure of RNA, by mapping the nucleotide sequence to a binary affinity matrix.
This paper introduces another new attribution method that measures the amount of information (in bits!) each input region contains, calibrating this score by providing a reference point at 0 bits. (approach - evaluation) The paper uses 3 metrics with differing degrees of novelty: 1) The bbox metric rewards attribution methods that put a lot of mass in ground truth bounding boxes. Summary The paper proposes a novel perturbation-based method for computing attribution/saliency maps for deep neural network based image classifiers. Summary --- (motivation) Lots of methods produce attribution maps (heat maps, saliency maps, visual explantions) that aim to highlight input regions with respect to a given CNN.
This paper proposes a new option discovery method for multi-task RL to reuse the option learned in previous tasks for better generalization. Experiments are conducted on the four rooms environment and Atari 2600 games and demonstrate that the proposed method leads to faster learning on new tasks. Overall, this paper gives a novel option learning framework that results in some improvement in multi-task learning. Some of these approaches might also be better baselines than OptionCritic which doesn't explicitly take into account learning from demonstrations or multi-task transfer. The paper proposes a method for learning options that transfer across multiple learning tasks. Summary: The authors propose to learn reusable options to make use of prior information and claim to do so with minimal information from the user (such as # of options needed to solve the task, which options etc). The authors utilize demonstrations collected beforehand and train an option learning framework offline by minimizing the expected number of terminations while encouraging diverse options by adding a regularization term.
The proposed techniques use both the graph structure, and the current classifier performance/accuracy into account while (actively) selecting the next node to be labeled. The authors present an algorithm for actively learning the nodes in a graph that should be sampled/labeled in order to improve classifier performance the most. To decide which nodes in a graph to label during the active labeling, the paper proposes two approaches. Strength: - One of the ideas of the paper, using region entropy over single node entropy makes sense to me. Second, it proposes to adapt the page rank algorithm (APR) to determine which nodes are far away from labeled nodes. - The paper evaluates on 6 datasets and compares different variants as well to related work on 2 datasets. "We have here shown that the accuracy of AL when uncertainty is computed regionally is much higher than when either local uncertainty or representative nodes are used", this is not the case on CiteSeer in Table 1 Overall it remains unclear *how* to select the right strategy (before seeing the results for a dataset) i.e. which of the proposed approaches or variants should one select for a new dataset. The paper contains several confusing and contradicting statements or claims which are not supported by the experimental results: For example: 1.1. There are experiments to show effectiveness of these techniques, and there are some interesting observations (for example, that the APR technique works better for smaller sample sizes, while the regional uncertainty methods do better for larger sampling fractions.).
The aim of this work is to address this issue by learning how to select representative samples from a dataset for training local linear models to reproduce predictions of black-boxes. 2018 [1], which proposes to meta-learn how to weight samples in a batch so as to maximize performance on a validation set. A key challenge is that in order for local models to be interpretable, they need to be simple in form and therefore lower in capacity (i.e. linear); thus, if they are trained on entire datasets they will underfit. The contribution of this paper is thus the use of RL for meta-learning how to subsample a larger dataset in order to maximize some validation loss. By analogy, in this paper, the samples are taken from the entire dataset (i.e. the dataset is subsampled), while the validation loss is effectively an imitation loss formed by treating the black-box model predictions as a target. * Pros: * Considers an interesting dataset subsampling variant of the sample weighting meta-learning problem. In this paper, the authors aim to learn a locally interpretable model via the reinforcement learning approach, to address the fundamental challenge which is that the previous locally interpretable model has smaller representation capacity than black-box models, and causes under-fitting with conventional distillation techniques. I've given a weak accept, conditioned on being provided more evidence regarding 1) comparisons to simple differentiable alternatives, 2) sample efficiency of the RL method, and 3) basic analysis of the weighting function. In particular, they propose to use RL to learn to weight instances from datasets; RL is required as they make hard (i.e. non-differentiable) decisions to select a subset of the dataset.
Overall, I think that the authors have presented a strong meta-analysis and compelling argument for further study in rigorously identifying the presence (or lack thereof) of selective units in neural networks and the degree to which they may be considered "object detectors. The paper makes an empirical claim that CNNs for object recognition do not contain hidden neuron which is highly selective to each class, mainly based on three aspects: (a) metrics related to the maximum informedness, (b) jitterplots of activation data, and (c) a user study assessing whether generated images maximizing a given unit is perceptible to the user. The paper empirically studies the category selectivity of individual cells in hidden units of CNNs. It is a sort of "meta-study" and comparison of different metrics proposed to identify cells with a preference for a specific target category. The authors find that (1) different proposed measures of selectivity are not consistent and (2) units identified as selective cannot be considered object detectors due to the high false alarm / low hit rates, analyzing a large number of selectivity measures. Previous works have used different measures of selectivity (with sometimes contradictory results), and the authors investigate the degree to which these units qualify as "object detectors". This work investigates the collection of methods that have been proposed to find units in neural networks that are selective for certain object classes. According to that study, almost 60% 0f all fc8 units are "object detectors", with very high conherence between humans and selectivity metrics. In this context terminology matters: the study effectively tries to disprove that the network learns "near-perfect single output-category detectors", but who claims that it would do that? - Do the overall results mean that the "maximum informedness"-based metrics are superior to the others for assessing selectivity of a unit? whereas the presence of object detectors would, in the extreme case, mean that the representation is disentangled into a separate unit per category. and of course there is no guarantee that the learned "object detectors" are tuned to exactly the target categories, after all it is the purpose of the network to gradually translate the data distribution to the label distribution
While most existing works rely on classical random walks on the graph, the paper proposes to cope with the "speed of diffusion" problem by introducing ballistic walk. This paper proposed a new diffusion operation for the graph neural network. The paper "Beyond Classical Diffusion: Ballistic Graph Neural Network" tackles the problem of graph vertices representation. More datasets are needed to thoroughly verify the performance of the proposed ballistic graph neural network.
The authors discuss here the problem of isomorphism bias in graph dataset, i.e. the overfitting effect in learning networks whenever graph isomorphism features are incorporated within the model. Testing on isomorphic graphs evaluates the ability of a model to infer these equivalence classes from data which is an important property. - I am not sure if *CONF* is the right venue for this work The paper presents three contributions: (a) the observation that there's train-to-test leakage in many graph classification datasets (under isomorphism equivalence), (b) what appears to be a theoretically motivated way of improving scores on such datasets, by focusing on solving the examples that are isomorphic with training instances, and (c) a recommendation to remove such leakage from test sets. The present paper's results on graph isomorphism properties can indeed be valuable for the ablation of models and testing their performance with regard to this property.
PAPER SUMMARY: This paper proposes a fast inference method for Gaussian processes (GPs) that imposes a sparse decomposition on the VI approximation of the posterior GP (for computational efficiency) using the KNN set of each data point. This is, however, a somewhat strange direction which, to me, seems to raise extra issues that could have been avoided if one follows the conventional VI approximation: (1) As the posterior surrogate is now directly over f instead of f_I, the number of variational parameters is now proportional to the data size which requires several (redundant) extra approximations including armortized inference & the lower-bound on the entropy term that admits a sparse decomposition. 1) Summary The manuscript proposes a k-nearest-neighbor (KNN) Gaussian process (GP) approximate inference scheme to render computations more scalable. REVIEW SUMMARY: This paper adopts a VI approximation that deviates from the conventional form of (Titsias, 2009) to encode the KNN information, which causes extra computational issues (that incurs extra approximations). NOVELTY & SIGNIFICANCE: This paper adopts a different approach of characterizing the VI approximation of a GP posterior than original VI approximation that was developed in Titsias (2009): Instead of characterizing the surrogate q(f_I) of p(f_I | Y) for a small collection of inducing inputs, the proposed method characterize q(f) directly where q(f) = int_f_I q(f_I) p(f | f_I)df_I.
In this paper, the authors propose the Homotopy Training Algorithm (HTA) for neural network optimization problems. Regarding (2): The authors evaluated their procedure on CIFAR10, a relatively simple image classification task that modern neural networks can solve easily and is not representative of the types of nonlinear optimization problems prevalent in deep learning. Summary This paper proposes an algorithm to address the issue of nonlinear optimization in high dimensions and applies it to convolutional neural networks (VGG models) on CIFAR 10.
This paper presents a new discriminator metric for adversarial attack's detection by deriving the different properties of l-th neuron network layer on different adv/benign samples. This paper proposes an adversarial detection method via Fourier coefficients.
The paper then follows this framework by constructing deep neural network mapping from "observations so-far" to "sufficient statistics" and by universal approximation theorem this is possible. The paper shows that recurrent neural networks are universal approximators of optimal finite dimensional filters. Based on the well-known universal approximation property of FNNs, the paper shows that their RNN-based filter can approximate arbitrarily well the optimal filter. It considers a general state space model and uses feedforward neural nets (FNN) to learn the filtering and forecast distributions. This paper shows that RNN (of infinite horizon) can be universal approximators  for any stochastic dynamics system.
Unfortunately, their tasks seem quite easy, and it is hard to assess the impact of their method when working with more real-world data-sets, where the number of instances of every class is more loosely defined (we could always describe more objects in a real image from the COCO dataset for example). The main contribution is the introduction of a differentiable top-K region proposal that allows to train the whole model with only a supervision of the total number of instances (and their class) in the image. I like the paper and I think the problem it's tackeling is at the core for all many important image understanding tasks.
I did not check the authors' theoretical proofs, but I find the statements of the theorems interesting, especially the results about the maximum certifiable radius for L_inf robustness.
This paper generalizes the min max formulation of adversarial training, and proposes a formulation that encompasses adversarial training of an ensemble, robustness to universal adversarial examples, and robustness to non-adversarial transformations. This should be clearly surfaced in the introduction and presentation of contributions if DPAR is required for the proposed generalized min max formulation to improve robustness.
Derive a generalization bound on the performance of adversarially robust networks  that depends on the margin between training examples and the decision boundary. With proposed GE bound connecting 'margin' with AR radius via 'singular values of weight matrices of NNs', they present 3 key results with empirical experiments on CIFAR10/100 and Tiny-ImageNet. 1) AR reduces the variance of outputs at most layers given perturbations. ===== Summary ===== The paper presents new theory to develop understanding about why adversarially robust neural networks show lower test performance compared to their standard counterparts despite being more robust to perturbations in the data. The paper studies this hypothesis by deriving a bound on the generalization error based on the margin between the samples in the training set and the decision boundary. The authors provide the experiment to show that the stronger robustness for adversarial examples leads to smaller STD of singular values of parameter of layers. The main hypothesis is that the degradation in performance in adversarially robust networks is due to many samples being concentrated around the decision boundary, which makes the network less confident about its decisions.
This paper presents DIMCO, a meta-learner that is trained by maximizing mutual information between a discrete data representation and class labels across tasks. The authors further present an information theoretic generalization bound for meta learning in terms of the number of tasks, number of training samples per task and the expressively of the model.
The introduced method is well-justified and the use of concept-based self-interpretable models is very useful to the field. It is also interesting to see that the performance is comparable to non-self-nterpretable baselines which would make a case for the use of self-interpretable models. Authors should make a much more comprehensive discussion of what already exists in the concept-based interpretability literature and make the contribution of this work more clear (unsupervised concept extraction for a self-interpretable model instead of post-hoc interpretations). The paper introduces a new concept-based interpretability method that lies in the family of self-interpretable models (i.e. it's not a post-hoc method). Adding a section for human subject experiments (following the previous work in concept-based interpretability) would make the results section much crisper. The difference comes on how easy it is to interpret the methods, as these other rationale-based text processing methods would make use of captured words, while EDUCE would make use of detected "concept" clusters. The paper evaluates on this measure, which is included in the appendix, and the results are pretty disappointing compared to the existing models such as Lei et al's initial baseline or Bastings et al.
I have increased my rating.) In this paper, the authors study the generalization bound for GANs based on a new definition of generalization error where the distribution corresponding to the generator is assumed to be known for each generator (i.e., there is no empirical distribution for generators). However, since the generalization bound in Arora et al (2017) is based on a different definition of generalization error where empirical distributions are considered for both discriminators and generators, the comparison seems not fair. This paper proves generalization bounds for GANs. I think the paper can be improved significantly in several ways: 1- Writing: The first two sections are relatively well-written. b)Theorem 2.3 is a general statement but it is followed by Corollary 3.3 which is a very specific generalization bound. 2) Related Work: I think authors need to do a more comprehensive literature review on generalization bounds. The authors give new generalization bounds for GANs. Some of the things that can be improved: a) The discussion on the different definitions of generalizations is not really helpful in the current format.
If the authors wish to use settings different from the standard ones used by most other papers which test on the MoG datasets they should justify it thoroughly and include this justification in their analysis—do we have a compelling reason to believe that the LDM method is better suited to learning lower capacity models in general? From the figure, it seemed that LDMGAN improved the sample quality and diversity compared to GANs, but it is still prone to characterizing only a single or a few modes of the data distribution. My Take: This paper's only point of novelty over a vanilla VAE-GAN implementation is the inclusion of the KL(E(G(Z)) || p) term in the generator loss, which is very similar to the idea behind VEEGAN. The relative novelty over VEEGAN is also limited, the description of the method is exceptionally similar to the description used in the VEEGAN paper (going so far as to copy-paste a figure straight from VEEGAN without attribution), and the comparison to VEEGAN in the related work section is not sufficiently fleshed out.
In this paper, the authors tackle the problem of multi-modal image-to-image translation by pre-training a style-based encoder. Compared to the direct baseline BicycleGAN, the training procedure proposed in this paper replaces the simultaneous training of the encoder E and the generator G with a staged training that alternatively trains on E and G and then finetune them together. The current presentation of the paper mostly consists of detailed descriptions of the proposal training procedure, without some interesting discussions about why this pretraining makes the problem easier. I think the idea of pre-training a style-based encoder is straightforward. Is it true that the proposed approach requires semantically aligned datasets, as the style of the whole image is described with a comparatively low-dimensional vector, and the GAN objective is applied to paired outputs only?
Moreover, for generalization one can use attention dropout which is simpler instead.] This paper proposes two sparsifying methods of computing attention weights, dubbed sparsemax and TVmax, which appear to slightly improve objective and subjective image captioning scores. One problem in this paper is that the author applies their proposed TVmax on Image Captioning  task, however it only achieves a little improvement on the automated metrics compared with the baseline(softmax). This paper produces a new method call TVmax and presents that the selective visual attention could improve the score in the Image Captioning task. The authors then compare their TVMax approach with softmax and Sparsemax attention for image captioning and show improvements on the MSCOCO and Flickr datasets. The main idea is to augment the Sparsemax projection loss with a Lasso like penalty which penalizes assigning different attention probabilities to contiguous regions in the image. Compared with the softmax function, the sparsemax[1]  and the TVmax are able to sparse the visual attention very well. To this end the authors first apply full attention where the probabilities are computed using softmax before applying the recently proposed Sparsemax - which essentially computes probabilities from scores by performing a projection on to the probability simplex. Some questions: 1.From the experiments, the proposed method achieved only a little higher performance than the baseline(softmax).
However, the statement that SMiRL agents seek to visit states that will change the parametric state distribution to obtain higher intrinsic reward in the future is controversial (see e.g. at the end of Section 2.1), because optimizing a non-stationary signal is outside the scope of the problem formulation. This paper proposes Surprise Minimizing RL (SMiRL), a conceptual framework for training a reinforcement learning agent to seek out states with high likelihood under a density model trained on visited states. The proposed approach encourages an agent to visit states with high probability / density under a parametric marginal state distribution that is learned as the agent interacts with its environment.
The paper proposes an approach to learning domain invariant representations using the adaptive decomposition of the convolutional filters. This paper introduces a way to decompose features for better domain adaptation via learning domain-invariant representations. * Please do not use the Office dataset, it is commonly used in unsupervised domain adaptation papers, especially older ones, but it's hard to tell anything about proposed methods from this dataset as there is label pollution and not enough samples per class to be used with neural nets. My main concern is that this paper seems to miss a significant line of recent work on learning domain-invariant representations [2-3]. * The paper proposes to perform domain adaptation via separating domain-specific and cross-domain features, by what is referred to as "domain-adaptive filter decomposition".
The experiments on the Atari games shows that by using tensor regression to replace the dense layer of the neural nets and using K-FAC for the optimization, one can reduce around 10 times of parameters without losing too much of performance. The second method the authors have attempted is to swap the convolution layer of the deep RL architecture with wavelet scattering. These methods include tensor regression, wavelet scattering, as well as second-order optimization (K-FAC). With the combination of tensor regression layers and K-FAC, the proposed methods give comparable performance on several Atari games against 2 other methods, SimpPLe and data-efficient Rainbow, while using 2 to 10 times fewer coefficients than data-efficient Rainbow.
This work discusses how to set the projection size for each head (head size) in multi-head attention module, especially Transformer. In response to such observations, the paper proposes Fixed Multihead Attention, where the constraint that `head_size * number_of_heads = embedding_size` in standard multihead attention is lifted; and it allows for using more attention heads without making each head smaller. Details: - Theorem 1 presents a rank-based view of each attention head's capacity, which is nice. Theorem 1 is interesting, which points out a lower bound for the head size.
The paper improves the existing feature attribution method by adding regularizers to enforce (human) expectations about a model's behavior. The authors proposed the attribution priors framework to incorporate human domain knowledge as constraints when training deep neural networks. The paper presents expected gradients which is a method which looks at a difference from a baseline defined by the training data. It is not clear if the paper is presenting "expected gradients" or existing attribution priors. More impressively, the model does outperform all other controls with a good margin in the anti-cancer drug prediction experiment, which is a nice demonstration of that domain knowledge could be incorporated in a neural network training to achieve better performance. Attribution priors as you formalize it in section 2 (which seems like the core contribution of the paper) was introduced in 2017 https://arxiv.org/abs/1703.03717 where they use a mask on a saliency map to regularize the representation learned. The authors showed with a very limited amount of data, they can use sparsity prior constraints to get a model with good feature sparsity (Gini coef), and good performance (measured by ROC-AUC). It would be nice to see how integrated gradient method perform in the three experiments (image, drug data, mortality prediction), does the expected gradient method always outperform? A user study would be needed to support that the proposed method can really provide a way to incorporate humans into the modeling process. The expected gradient method does indeed also performed better than the integrated gradient method in the benchmark (see Table 1.) The results in all three experiments are impressive. Also, the introduced human priors are similar to general regularization conventions, i.e. a penalty of smoothness over adjacent pixels is commonly used in the CV community. Three different datasets (i.e. image, gene expression, health-care) are chosen to evaluate the proposed model's effectiveness, while different regularizers (i.e. image prior, graph prior, and sparsity prior) are explored for the respective task. For example, in this work, the authors proposed three reasonable priors for image input, graph data, and clinical medical data. Even though the authors has shown in Table 1 benchmark that expected gradient is performing better than integrated gradient. Unless end-users cannot understand the model's behavior, how can we expect humans can provide knowledge to model? For example, Figure 1 (left) shows an attribution map that highlights multiple intermittent regions from which I cannot understand its behavior. For the image and the graph data, the prior is basically to have smoother attributions for nearby features; while for the clinical medical data, the authors used the Gini coefficient formula to encourage sparsity, which is of several practical benefits clinical practice.
I appreciate that the paper addresses an interesting problem that is not sufficiently explored and can motivate the development of novel methods that generate 3D molecules with particular structure and multiple types of atoms, however the current work combines existing methods, without any architectural modifications that exploit the new domain. The paper's contributions can be summarized as follows: 1) A data representation that converts the 3D atom locations to a 3D voxel density map, so that they can be encoded by a standard 3D convolutional network. 2) A decoder network that first estimates a 3D density map from the latent vector using upsampling and convolutions and then classifies the atom type (atomic number) per voxel using a 3D segmentation. The paper deals with accurately encoding and decoding 3D atomic positions and the crystal's species using 2 sets of neural networks On the other hand, if the authors can show that the errors in atomic numbers suggest they correspond to similar atoms (may be along the same column in the periodic table), then one can have better confidence that the network has learned a meaningful representation. Limited methodological novelty: The methods used in the paper, i.e. the data representation (see detailed comments), the encoder network, the decoder network and the segmentation network are all existing methods without any (or with only minor) modifications. 3. Joint VAE + UNet training: The joint training of the encoder-decoder VAE and the 3D segmentation network results in a decoder that can reconstruct atom locations and atom types, being robust to mistakes in the density map reconstruction. The authors propose an auto-encoder framework for encoding the 3D locations of atoms in the crystal to a latent representation and then decoding that representation back into 3D structure. As the paper's related work section shows, this is not the first attempt to use 3D structure to create molecular representations.
Based on this observation, they propose a new normalization method not only achieves significantly better results under a variety of attack methods but ensures a comparable test accuracy to that of BatchNorm on unperturbed datasets. Afterwards, the authors propose RobustNorm, which performs better than BatchNorm for both natural and adversarial scenarios.
You may include other additional sections here This paper propose to modify the existing work [1] of self-training framework for graph convolutional networks. The proposal is inspired from Graph convolution Networks with the idea of overcoming the major drawback of these models that lies of their behavior in case of limited coverage of the labeled nodes, which implies using deeper versions of the model leading at the price of what the authors call the over-smoothing problem. #Summary This paper proposes a generalised self-training framework to build a Graph Neural Network to label graphs. The authors do not change the GCN but extend the self-training portion as per the prior GCN paper by introducing Dynamic Self-Training that keeps a confidence score of labels predicted for unlabelled nodes. Evaluation of the proposed framework is performed on four networks for semi-supervised node classification task with varying label rates. Using self training is not new in GCN but the way it is used here, computing adaptively a threshold for incorporating pseudo labels and using weights according to the confidence off predictions is new. The main idea here consists in relying on self training to get a better coverage of labeled nodes enabling learning with less deep models, this translates to a simple and intuitive algorithm.
Similar to the robust MDP work, they also show that this robust RL problem has a robust optimal Bellman operator, and the optimal value function can be computed using  robust policy iteration, which can be extended to model-free actor critic algorithm when state/action spaces are large/continuous. Similar policy gradient and actor-critic algorithms have been derived in risk-sensitive MDPs for mean-variance, mean-VaR, and mean-CVaR optimization (see the references below), but such algorithms for RMDPs is relatively new. First, the proposed robust Bellman optimality result is standard and can be found in many robust MDP work when the uncertainty set is state-action rectangular.
(Though I am less positive due to the concern of novelty raised by other reviewers.) Summary: This paper presents an efficient stochastic neural network architecture by directly modeling activation uncertainty and adding a regularization term to encourage high activation variability by maximizing the entropy of stochastic neurons. This paper proposed SE-SNN, a type of stochastic neural networks that maximize the entropy in stochastic neurons along with the prediction accuracy.
This paper proposes a neural network architecture consisting of multiple independent recurrent modules that interact sparingly. 2) I find it quite interesting that the top-down attention in selective RIM activation is corresponding to the states of these recurrent cells. In general, I like the idea of making recurrent cells operate with nearly independent transition dynamics and interact only sparingly through the attention bottleneck.
The authors found that in a setting with low budget for hyperparameter tuning, tuning only Adam optimizer's learning rate is likely to be a very good choice; it doesn't guarantee the best possible performance, but it is evidently the easiest to find well-performing hyperparameter configurations. However, I do not think the metric they introduce is good enough to be recommended in future work, when comparing tunability of optimizers (or other algorithms with hyperparameters). In addition, the proposed stability metric seems not quite related with the above intuitions, as the illustrations (1.a and 1b) define the tunability to be the flatness of hyperparameter space around the best configurations, but the proposed definition is a weighted sum of the incumbents in terms of the HPO budgets. A good prior of one optimizer could significantly affect the HPO cost or increase the tunability, i.e., the better understanding the optimizer, the less tuning cost. A comprehensive empirical comparison of deep learning optimizers, with their performance compared under different amount of hyper-parameter tuning (they perform hyper-parameter tuning using random search). This point is proven in the paper itself, where for example Figure 2 provides a more concrete and easier to interpret information than the tunability metric, similar graphs could be easily provided per dataset.
[Summary] Overall, I think the proposed ss layer is still a reasonable way to incorporate the contextual information in CNNs. However, the poor presentation and the weak experimental results and analysis make the paper overall a one under the bar of the venue. Summary: The authors extended the regular convolution and proposed spatially shuffled convolution to use the information outside of its RF, which is inspired by the idea that horizontal connections are believed to be important for visual processing in the visual cortex in biological brain.
The experimental results show that, when the data is sparse, the proposed method (CommunityMPA) worked better than MPA on Phrase Detectives 2 corpus in terms of mention-pair accuracy, silver-chain quality, and the performance of the state-of-the-art method trained on the aggregated mention pairs. Authors demonstrate that on a recent large-scale crowdsourced anaphora dataset, the proposed Community MPA works better than the unpooled counterpart in conditions of sparsity, and on par when enough observations are available. Different from the conventional crowdsourcing approaches that concentrate on classification tasks with predefined classes, the proposed Community MPA, as an extension of MPA can address anaphoric annotation well where coders relate markables to coreference chains whose number cannot be predefined. This paper extends the unpooled mention pair model of annotation (MPA) (Paun et al., 2018b) with the hierarchical priors (e.g., mean and variance) on the ability of the annotators. The paper proposes an improvement over the mention-pair model for anaphoric annotation by Paun et al. I am also wondering why evaluating portions of the dataset where annotations were made by 'sparse' users would not work to highlight the effectiveness of the proposed method for sparse users.
Section 4.2 says "Algorithm 1 with A_rms ellipsoids converges with the classical rate O(\\epsilon^{-2}, \\epsilon^{-3}) thanks to Proposition 2 above and Theorem 6.6.8 in Conn et al. The authors then show that the preconditioning matrix of RMSProp and Adam can be used as norm inducing matrices for second order trust region methods. In this paper, the authors investigate the use of ellipsoidal trust region constraints for second order optimization. The authors first show that adaptive gradient methods can be viewed as first-order trust region methods with ellipsoid constraints. My understanding is that the paper does not claim to deliver some great results here and now but instead suggest a promising direction ("that ellipsoidal constraints prove to be a very effective modification of the trust region method in the sense that they constantly outperform the spherical TR
Their results show that by including the distilled logits when computing the gradient, the generated adversarial examples can transfer better among different models using both single-model and ensemble-based attacks. ------------- This paper proposes an attack method to improve the transferability of targeted adversarial examples. This paper proposes distillation attacks to generate transferable targeted adversarial examples.
The authors use a regularized reward function that minimizes the divergence between the policy of the expert and the one followed by the agent. The end of the related work section is not very clear, you say these methods are problematic because "the adopted shaping reward yields no direct dependence on the current policy" but there's no explanation or motivation for why that would be a problem. Finally, they only test on mujoco tasks which are very specific tasks with deterministic dynamics and very dense rewards  around states visited by the optimal strategy so initializing with an expert policy that is learned from demonstrations of a similar network of course helps. Given a set of expert demonstrations, this work provides a policy-dependent reward shaping objective that can utilize demonstration information and preserves policy optimality, policy improvement, The main advantage of the proposed method is that the reward shaping function is related to the current policy. They use demonstrations obtained from a trained agent and experiment their method on several mujoco tasks. The related work section should also discuss and compare/contrast to GAIL, I was surprised that wasn't in there, especially since you also use a discriminator to differentiate expert and agent actions. Results on 5 sparse reward mujoco tasks show that it out-performs other related methods. The authors conducted sufficient experiments to demonstrate the effectiveness of the proposed method, compared with the state-of-the-art in RLfD. This paper discussed how to use the demonstration information to do exploration and maintain policy invariance at the same time, with a relatively strong assumption. It is already studied in several works and more generally it comes with some assumptions on the policy update. Converting the tasks to sparse reward in this way makes them partially observable, and then potentially the expert demonstrations are required to overcome that partial observability. It is not clear that in what type of MDPs the optimal stochastic policy exists and it can satisfy Asm. 1.
2. By introducing long-term memory on past-gradients, the authors fix the Adam's issues and they can also improve the convergence rate in a non-convex optimization (Corollary 4.2). In this paper, the authors propose a new adaptive gradient algorithm AdaX, which the authors claim results in better convergence and generalization properties compared to previous adaptive gradient methods. This paper points out that existing adaptive methods (especially for the methods designing second-order momentum estimates in a exponentially moving average fashion) do not consider gradient decrease information and this might lead to suboptimal convergences via simple non-cvx toy example. Based on this observation, the authors provide a novel optimization algorithm for long-term memory of past gradients by modifying second-order momentum design.
Moreover, it would be nice if the authors could also provide some ideas for future research directions, such as the prospects of using their approach for improving link prediction models and incorporating Meta-Graph in other domains like molecules structure. This paper proposes to provide a novel gradient-based meta-learning framework (Meta-Graph) for a few shot link prediction task. Overview: In this paper, a meta-learning approach is proposed to perform link prediction across multi-graphs with scarce data. More specifically, they generate an effective parameter initialization for a local link prediction model for any unseen graph by leveraging higher-order gradients and introducing graph signature function into graph neural network framework.
The key idea is to create the smaller possible subset with the same or similar coverage (in the paper the neurons coverage is considered) and output distribution, maintaining the difference below a (small) threshold. To better demonstrate the change between raw testing set and proposed subset, I think that it could be better to present the metrics of distribution or the accuracy of each class instead. In this paper, the work is done by adding a test-sample search algorithm on top of the HGS algorithm to balance the output distribution. 2. In Table 2, the authors try to compare the output distribution. In this way, an estimate of the output distribution is considered during the extraction of the representative subset of the testing data. The paper presents a new approach to create subsets of the testing examples that are representative of the entire test set so that the model can be tested quickly during the training and leaving the check on the full test set only at the end to validate its validity. The paper proposes  a two-phase reduction approach to select representative samples based on heuristics. Finally, it would be interesting to know the runtime to obtain the subsets of the test data of Table 2 required by the considered systems.
Additionally, they also notice that fine-tuning a _linear model_ using the learned features for the wider networks provide better accuracy for new (but related) tasks over the shallower counterparts. In particular, the central claim of this paper is what the title claims "Wider networks learn features that are better". They find that wider networks learn features in the hidden neurons that are more "interpretable" in this visualization framework.
Picking the initial centers for k-means clustering has been long known as an interesting/challenging problem, especially in a high-dimensional space (which tasks like language modeling deal with). This paper proposes a novel way to increase efficiency for self-attention based sequence modeling neural networks. With a content-based "routing" technique ("group/cluster attention" may be a more accurate term to differentiate with the dynamics of Capsule Networks), the time steps of each layer form different clusters, and the self-attention mechanism is only performed within each such cluster. However, since the cluster centroids are learned parameters, does this mean the trained routing Transformer cannot easily generalize to different sequence lengths easily? This paper proposes content-based sparse attention to reduce the time/memory complexity of attention layers in Transformer networks. + The paper addresses a very important (efficiency) question in Transformers, and gave a proper review to prior related works (such as Child et al.). The routing Transformer seems to be able to do very well on the WikiText-103 word-level language modeling task. - What is the *actual* running time/memory for local/routing/full attention layers? Are we then, in language modeling tasks, performing attentions among words only around these "anchors"? I find the discussions around NMF to be somewhat orthogonal, especially considering the paper does not use NMF techniques for their clustering algorithm in section 4.2. How does the model do if it only uses routing attention? In particular, although the routing mechanism avoids computing A, having to deal with each of the n clusters means that one will need to process the cluster-based attentions sequentially (and, as mentioned, you have 8 heads for local attention and 8 for routing attention, which I think are also processed in two steps? 7. The paper proposes to use n clustering centers, which makes sense from the perspective of minimizing total computations. It seems like the routing layer requires additional operations (i.e. online k-means) which could increase running time.
This paper proposes a very interesting idea of loss function optimization. They find that applying their EC method to mnist yields an interesting loss function that they name the 'Baikal loss.' Much of the paper is devoted to analyzing the properties and performance of the Baikal loss. It would be good to show that loss functions meta-learned on mnist generalize to larger-scale problems than cifar. And experiments show that GLO (Genetic Loss-function Optimization) based loss function can achieve better results than cross entropy.
Overall, this paper could be an interesting summary of prior works in offline RL and provide some empirical insights on the effectiveness of each building block in the previous approaches, though it neither offers theoretical explanations nor proposes a new offline RL algorithm that outperforms the existing methods under the BRAC framework. The paper introduces a general framework for behavior regularized actor-critic methods, and empirically evaluates recent offline RL algorithms and different design choices. While the experimental results demonstrate some interesting phenomenons such as combining vp and the primal form of KL divergence achieving the best performance and taking minimum over Q functions outperforming using a mix of maximum and minimum over the Q functions, I believe the paper would be greatly improved if the authors can provide a new offline RL method based on the BRAC that can achieve better performance than current approaches and is less incremental than simply combining vp and KL divergence. This paper proposes a unifying framework, BRAC, which summarizes the idea and evaluates the effectiveness of recently proposed offline reinforcement learning algorithms, specifically BEAR, BCQ, and KL control. The authors provide extensive results; but it wasn't clear whether these were "apples-to-apples" comparisons with the previous results in the papers that proposed the "unnecessary" technical complexities. This paper could be improved by providing more clear insight and intuition about the deeper meaning of these results regarding the "unnecessary" technical complexities. Results from a  thorough series of experiments are presented which suggest that certain details of recently proposed RL methods are not necessary for achieving strong performance. I am leaning to accept the paper because (1) the experimental design is rigorous and the results provide several insights into how to design a behavior regularized algorithm for offline RL. This paper presents a framework for evaluating offline reinforcement learning (RL) algorithms. The authors generalize existing offline RL approaches to an actor-critic algorithm that regularizes the learned policy such that it can stay close to the behavior policy. Though BRAC summarizes offline RL methods in a neat way, it would be more technically sound if a general theoretical analysis/insights of offline RL algorithms can be offered in the paper, e.g. showing the reason that vp is outperforming pr through convergence analysis in the tabular case. My score would be increased given some technical insights or some promising results in a relatively novel offline RL algorithm in the author's response. Does the BRAC framework reproduce the results for previous papers? And I think the authors did a good job addressing a difficult problem. For example, I didn't see the authors say that they reproduced the results of previous works, only that they tested previous methods in certain tests. If so, this should be made more clear and stated prominently in the paper so that the reader knows that BRAC is, in this reproduction of previous results sense, reliable. Could the authors suggest why certain complexities are unnecessary? Specifically, the paper does a thorough ablation study on BEAR, BCQ, and KL-control within the BRAC framework.
+ The proposed model adopts the multi-task learning idea to represent each sequence in the training set. The experiments on the synthetic data and the ocap dataset show the customization is beneficial for prediction tasks and enables for style transfer and morphing within generated sequences. - Separate parameterization of the latent variable z for different tasks seems to be a key idea in this paper.
This paper tackles the out of distribution detection problem and utilizes the property that the calculation of batch-normalization is different between training and testing for detecting out-of-distribution data with generative models. The paper first empirically demonstrates that the likelihood of out-of-distribution data has a larger difference between training mode and testing mode, then provides a possible theoretical explanation for the phenomenon. One experiment that is missing from your paper (and would prove my hypothesis wrong) would be if you adapted the OoD criteria to compare the *mean* likelihoods in the evaluation mode, and show that for OoD datasets, the difference between batches still remains small. The proposed scoring function utilizes such likelihood differences in a permutation test for detecting out-of-distribution data. This is consistent with your batch normalization experiments: in training mode, the likelihood is computed from mean activations over a batch of OoD samples, several of which probably contribute to the low likelihoods. The sentence at the bottom of page 7 should be removed ("related work still needs some work, but there seems to be some bug on overleaf right now") This paper attempts to address the problem of out-of-distribution detection with generative models.
To model functions s and t In this paper, a GraphNVP framework for molecular graph generation is proposed. Overall, I think the method proposed in this paper sacrifices some more import aspects in graph generation such as novelty and uniqueness by introducing an invertible flow architecture. This paper proposes an invertible flow-based method for the one-shot graph generation.
Summary: this work uses tensor methods to improve graph convolution for dynamic graph, where the nodes are fixed and the edges are changing. The paper proposed a new type of graph embedding technique for dynamic graphs based on tensor representation (node x feature x time). However, if in practice, you do not need the inverse transform, then do those theoretical properties still hold and what is the meaning of introducing such M-product formulation? This paper presents a M-product based temporal GCNs to handle dynamic graphs. Cons & Questions: 1, My first concern is that M-product formulation does not bring any new insights as people have already used some of the key elements in practice for a long time.
The paper proposed ``TriMap''---a novel dimensionality reduction technique that learns to preserve relative distances among points in a triplet. The illustrated S-shape example in Figure 1 somehow demonstrate the difference of the proposed method with PCA, t-SNE and UMAP, but the usage of the embedding is not clear since Figure 1(d) looks like a 2-d visualizing of the original 3-d data visualized from certain angle. Authors define the minimum reconstruction error from the embedding as the global measure in reflecting the global structure of the data similar to PCA.
The notion of feature robustness, which is a notion the paper proposes, connects the flatness measure to generalization error. This paper describes a connection between flatness of minima and generalization in deep neural networks. The authors combine their notion of feature robustness with epsilon representativeness of a function to connect flatness to generalization. Atleast on CIFAR10 and MNIST it is possible to achieve training loss <1e-4 so am not sure if the networks that the authors are testing are minima at all (It is important for them to be minima since the flatness measure is only defined at minima). This paper proposes a flatness measure that is invariant to layer-wise reparametrizations in ReLU networks. This seems to be a missing step in relating flatness/feature robustness of a layer to the generalization of the whole network. Otherwise, it is hard to claim that this paper connected the modified flatness measure to generalization error. There are many relevant work in this area that connect feature or weight robustness to generalization look at [1,2,3,4] for some examples. I suggest authors to look at the literature on PAC-Bayesian and compression-based bounds to connect their suggested measure to generalization.
The authors can directly read this information from the synthetic environments used in this paper, yet, in more complex real-world situations, we might not know the underlying causal structure for supervised training the induction model. [Summary] This paper proposes an interactive agent that tries to infer the underlying causal structure by interacting with the environment; the authors called it "causal induction."  The inferred graph will later help the agent complete goal-directed tasks referred to as a "causal inference" stage. I feel this paper makes strong assumptions on both the induction and inference stages, as well as the structure of the causal graph, which greatly limits the applicability of the approach. For learning the goal-conditioned policies, the authors also assume that they have access to the ground truth causal graph. The paper is describes a very nice new method and makes interesting points about the role that causal reasoning can play in modeling agents in goal-directed tasks. This is a paper about a very interesting topic, involving both learning (in a supervised way) to induce a causal graph and taking advantage of it in a goal-conditioned policy.
The authors claim that the proposed method for semi-supervised graph learning is based on data augmentation which is efficient and can be applied to different graph neural architectures, that is an architecture-agnostic regularization technique is considered in the paper. • The performance gain is from mostly the use of a semi-supervised learning approach based on entropy minimization, which has been developed without consideration of graph-structured data. Although it seems the additional computation cost for the proposed approach compared to the baseline such as GCN does not increase dramatically, it would be better if the authors could provide its computational cost as well as performance given the number of perturbed samples.
The authors analyze possible causes, and for the first time present two simple yet effective "black-box" methods to calibrate the GAN learned distribution, without accessing either model parameters or the original training data. It proposed a new metric to measure the diversity of the generative model's "worst" outputs based on the sample clustering patterns. This paper presents a set of statistical tools, that are applicable to quantitatively measuring the mode collapse of GANs. The authors consistently observe strong mode collapse on several state-of-the-art GANs using the proposed toolset. 1) I wonder if the proposed method work for most GAN models, more experiments evaluated on more recent GAN-based  models should be added to verify the superiority claimed in this paper, e.g., TP-GAN [Huang et al., ICCV 2017], PIM [Zhao et al., CVPR 2018], DR-GAN [Tran et al., CVPR 2017], DA-GAN [Zhao et al., NIPS 2017], MH-Parser [Li et al., 2017], 3D-PIM [Zhao et al., IJCAI 2018], SimGAN [Shrivastava et al., CVPR 2016], AIM [Zhao et al., AAAI 2019]. Rather than using the labels, the authors use an off-the-shelf model (on faces) to provide a space on which to measure distances between generated images. Unlike most existing works that address the model collapse problem, a blackbox approach does not make assumptions about having access to model weights or the artifacts produced during model training, making it more widely applicable than the white-box approaches. While this work is focusing on black box methods for evaluating and palliating mode dropping (aka collapse), it's a bit disappointing that these results are at least also evaluating on white-box type methods in settings where mode dropping is clearer, e.g. PACGAN on stacked MNIST or even normal CelebA.
On the sequence generation task (translation), the authors showed that the proposed KA strategy achieved better performance compared with KD based methods when distilling the knowledge from a teacher model to a student model. The authors proposed a new strategy to perform the knowledge distillation from a teacher model to student model for sequence generator. [Overview] In this paper, the authors proposed a new method called knowledge acquisition (KA) for distilling the learned knowledge from the teacher model to the student model.
The advantage in terms of accuracy of the proposed approach seems marginal in the experiment, and the analysis of the interpretability of the learned representations could be improved: loosely speaking, particular examples of interpretability are given but sometimes without contexts or baselines to compare to (see the two last comments below). This paper proposes a novel model of recurrent unit for RNNs which is inspired from tensor product representation (TPR) introduced by Smolensky et al. For example, tensor product representation (TPR) is introduced in the second paragraph of Introduction. More specifically, the paper claims TPRU's interpretability by Table 5. The paper is difficult to read because it assumes the reader is an expert on Tensor Product Representations. In this paper, a new unit based on the outer product called TPRU is proposed for recurrent neural networks.
The paper proposes a new way of stabilizing Wasserstein GANs by using Sinkhorn distance to upper-bound the objective of WGAN's critic's loss during the training. The paper thus requires further work in terms of design, theory and experiments; for instance the choice of Sinkhorn distance as a boundary-heuristic might be too limiting. [p.8] Discussions -> Discussion This paper proposes bounding the Wasserstein term in WGAN with an aim to stabilize training. On the other hand, in the high-resolution experiments in Section 4.3, the authors suggest that WBGAN with Sinkhorn does not scale well, and show some results with the bound determined on the basis of a separate run to investigate the converged value of the Wasserstein term, which is impractical.
This is a well written paper with good baselines.They use multiple techniques that are both domain specific (data augmentation) as well as methods from NLP adapted for this task. The main contributions are data augmentation techniques, pre-training and a mixture model that seems to improve performance on the USPTO-50K dataset. The main contribution of this paper is to apply the state-of-the-art Transformer model and other techniques in NLP to address the specific issues in retrosynthesis.
The authors consider making decisions with experts, where each expert performs well under some latent MDPs. An ensemble of experts is constructed, and then a Bayesian residual policy is learned to balance exploration-exploitation tradeoff.
The paper presents an idea on making adversarial attack on deep learning model. This paper proposed a BO-based black-box attack generation method. This paper applies Bayesian optimisation (BO), a sample efficient global optimisation technique, to the problem of finding adversarial perturbation. The authors in [1] consider using Bayesian optimization to make adversarial attack for model testing. The main contribution is to combine BO with dimension reduction, which leads to the effectiveness in generating black-box adversarial examples in the regime of limited queries.
This paper investigates the impact of using a reduced precision (i.e., quantization) in different deep reinforcement learning (DRL) algorithms. I wonder if the quantization during training has a regularization effect, which is known to improve agent's performance in reinforcement learning (e.g., Cobbe et al., 2018, Farebrother et al., 2018). This paper studies the effect of quantization on training reinforcement learning tasks. It shows that overall, reducing the precision of the neural network in DRL algorithms from 32 bits to 16 or 8 bits doesn't have much effect on the quality of the learned policy. Specifically, the paper applies post-training quantization and quantization aware learning to various tasks and record the effects on accuracy and training speed.
*CONF* 2019 This paper proposes to use a 'slot-based' (factored) representation of a 'scene' s.t. a forward model learned over some observed transitions only requires sparse updates to the current representation. Specifically: 1) It is unclear what this paper is claiming to improve over prior work: is the goal to a) learn a good forward model, or b) show that emergent entities allow better downstream tasks. al., NeurIPS 18 This paper introduces a model that learns a slot-based representation, along with a transition model to predict the evolution of these representations in a sparse fashion, all in a fully unsupervised way. 3) While the experiments in Sec 4.2 clearly demonstrate the benefits of the approach, the ones in Sec 4.1 and 4.3 are less convincing: 3a) Sec 4.1 shows that the slot based transition model generalizes better, but this is only in comparison to a naive fully-connected baseline. The results show that jointly learning the forward model and the scene representation encourages meaningful 'entities' to emerge in each slot.
As a result, the paper demonstrates that this set of entity embeddings are highly useful, and they were evaluated on 1) entity-level typing 2) entity linking 3) few-shot category reconstruction 4) answering trivia questions (TriviaQA). In the entity linking task, RELIC achieves a lower precision at two benchmarks (CoNLL-Aida, TAC-KBP 2010) than other approaches.
This paper considers how we can train image classification models so that they can ignore task irrelevant features. (approach) Actdiff: 1) The Actdiff loss requires a mask that highlights areas of the input image which have signal and not distractor regions. The authors then proposed using Actdiff loss, Reconstruction loss, and Gradmask loss that are designed to suppress the effect of irrelevant features. By designing actdiff loss and reconstruction loss, the authors demonstrate that classifiers are likely to predict using features unrelated to the task and their losses can mitigate this problem. By utilizing the dataset with masks, the authors propose a simple method to ignore the distracting features. Actdiff is compared to 5 other methods including a reconstruction loss and Gradmask (previous work). This dataset provides 3 segmentation tasks (Liver, Cardiac, Pancreas), including ground truth masks for those images. Lots of relevant experiments are reported but they don't support clear conclusions and I'm not sure how well the models were tuned. The idea of regularizing using saliency maps has been explored and even applied to medical data like the MSD used here in Gradmask (one of the strong baselines this paper compares to). If this is the case, I am not very sure if the proposed losses are essential, or tuning a right weight can occasionally provide good models.
The paper presents an unsupervised approach for learning landmarks in images or videos with single objects by separating the representation of the image into foreground and background and factorizing the representation of the foreground into pose and appearance. In this paper, authors propose to design an unsupervised learning framework, which can capture pose representation by reconstructing images or videos. The paper presents an unsupervised method to get disentanglement of pose, appearance, background from both images domain and video domain. Their methods let the network focus more on the foreground to regress the landmark and improve state-of-the-art performance on landmark regression (unsupervised.), video prediction and image reconstruction. It builds upon previous work [Jakab 2018, Lorenz 2019] who proposed to train by reconstructing the original image from one version of the image with perturbed appearance and another with perturbed pose.
Each experiment compares two highly similar tasks - as the authors themselves acknowledge - in ways that do not obviously connect to realistic transfer scenarios, such as transferring to a new environment. - Finally, I suggest revising the very vague title to the paper This paper tries to analyze the similarities and transferring abilities of learned visual representations for embodied navigation tasks. This article studies the similarities between the learned representations for different tasks when trained using reinforcement learning algorithms.
(ii) The claims of guaranteed frequentist coverage are not backed up as, according to thm.2, they only hold when n >> 0 and the number of influence functions used goes to infinity (ideally, the authors would provide non-asymptotic bounds as in [1], but at the very least, these limitations should have been clearly pointed out and their practical implications discussed). Note that I am not suggesting the above method is perfect either (it completely ignores the actual sizes of the intervals), but I'm currently having trouble interpreting the results you report, so it would be very helpful to understand how you selected this particular measure of "discriminative power" and why alternatives like the example above were discarded please. The authors propose using influence functions to efficiently estimate pointwise confidence intervals for regression models. The paper also elides the fact that the global minimizer theta^ cannot in general be computed in non-convex models like neural networks. The authors propose an algorithm, "discriminative jackknife", based on the standard jackknife confidence interval estimate which they augment by a "local uncertainty estimate" based on the variability of the n leave-one-out fitted versions of the underlying algorithm (n = # data points). https://arxiv.org/abs/1705.08292 The authors provide an interesting study on uncertainty estimation for deep learning for regression problems. This paper studies how to construct confidence intervals for deep neural networks with guaranteed coverage. The authors provide experimental support for the superiority of their method (in terms of coverage and discrimination) as well as theoretical support for consistency. In this work, the authors develop the discriminative jackknife (DJ), which is a novel way to compute estimates of predictive uncertainty. develop a way to compute higher order influence functions, which you do claim you're the first to do "to the best of your knowledge."
This paper reports the BLEU score of WMT17 Chinese-English dataset for 32.3, which significantly outperformed the best score, and improved the performance of existing state-of-the-art results. What would be the performance of an ensemble of 10 models trained with the regular 20M parallel sentence pairs? The paper proposes an approach to train NMT models on extremely large parallel corpora. It is pity that the trained model is only evaluated on one test set and experiments are conducted on one language pair. This paper investigates the effectiveness of a massively large parallel corpus in NMT training, which consists of more than 40 billion En-Zh parallel sentences. In Section 5.1 the paper mentions "the single-model achieves a BLEU score of 29.7, already outperforming the current best system", but in Tables 1 and 2 it seems that the best score with single model is 28.7, not 29.7.
The authors provide a theoretical discussion that both DAT and UMixUp converges to be equivalent to each other when the number of training samples becomes infinity. This paper introduces directional adversarial training (DAT) and UMixUP, which are extension methods of MixUp. DAT and UMixUp use the same method of MixUp for generating samples but use different label mixing ratios where DAT retains the sample's original label. Since UMixUp is also focusing on the mixing ratio between two training samples, Between-Class Learning should have been compared to the proposed method. Actually, both MixUp and UMixUp are shown to converge to DAT when the number of training samples tends to infinity.
This paper proposes an extension of the conditional GAN objective, where the generator conditions on an attention map produced by the discriminator in addition to the input image. The proposed discriminator provides the probability of real /fake and an attention map which reflects the salience for image generation. The key contribution is that the discriminator not only predicts the probability of an image being real or fake, but also outputs a map which indicates where the generator should focus in the next iteration in order to make its results more convincing. [Summary] This paper proposes a GAN with an attention-based discriminator for I2I translation, GuideGAN.
The manuscript discusses a metric-learning formulation as a quotient for reconstruction-error terms, how to optimize the quotient based on results from Wang et 2014, an iterated reweighted approach to circumvent the non-smooth part of the l1 loss in zero, and experiments on brain images of Alzeimer's disease. Brand et al., 2018; Lu et al., 2018) as example approaches which, despite their effectiveness, present an added complexity to the problem." is better written as "This problem is excessively studied by (Wang et al., 2012; With regards to the theoretical contributions, a fraction of the results in the present manuscript are trivial consequences or Wang2014, and yet it comes with errors in the statements.
- following the comment above, and assuming I understood correctly: There is an originality on the paper compared to other works that use binary codes/bloom filters: In the current paper, the authors actually predict the result of individual hashing functions. The authors present experiments on two tasks showing that keeping overall model size fixed (i..e, number of parameters), transformers with shorter (but denser) binary codes achieve better performances than standard transformers using one-hot encodings. For instance, if there are m hash functions taking values in {1, ..., P}, an approach based on Bloom filters would predict a binary output of dimension P, while here there are m multiclass problems with P classes (IIUC). The paper proposes to use codes based on multiple hashing functions to reduce the input and output dimensions of transformer networks. This is different from predicting the binary encoding that results from using the "or" of one-hot encodings generated by several hash functions, as would be done in approaches (truely) based on Bloom filters. The size of the vocabulary could be largely reduced through hashing, which makes a larger embedding dimension and more complex internal transformation eligible and thus better performance. The authors propose to learn a Transformer in embedded spaces defined via m hash functions, with application to (fixed length) sequence-to-sequence classification for NLP.
[Original reviews] This paper proposed to modeling image as the combination of a GAN with a Deep Decoder, to remove the representation error of a GAN when used as a prior in inverse problems. The proposed hybrid model is helpful on compressed sensing experiments on the CelebA dataset; however, it is only marginally better than deep decoder on image super resolution and out-of-distribution compressed sensing. Summary: This paper proposes to use a combination of a pretrained GAN and an untrained deep decoder as the image prior for image restoration problem. The method is evaluated on compressive sensing and super-resolution, where a better performance than the isolated use of Deep Decoders and GAN priors. In Alg.1, the detailed algorithmic process is presented, it is clear that authors need to pre-train the used GAN and Deep Decoder, then combine them to train one network.
The paper presents a new approach, SMOE scale, to extract saliency maps from a neural network. This paper presents a method for creating saliency maps reflecting what a network deems important while also proposing an interesting method for visualizing this. In summary, this paper presents a nice way of generating saliency maps from activations inside a network. I Summary The authors present a method that computes a saliency map after each scale block of a CNN and combines them according to the weights of the prior layers in a final saliency map. The paper gives two main contributions: SMOE, which captures the informativeness of the corresponding layers of the scale block and LOVI, a heatmap based on HSV, giving information of the region of interest according to the corresponding scale blocks. This paper is interesting in that it provides a different way to extract saliency maps from a network.
A very related work to this paper: L_{DMI}: A Novel Information-theoretic Loss Function for Training Deep Nets Robust to Label Noise, where no restrictions have been made on the class-dependent transition matrix and the proposed method does not need to estimate the transition matrix. Novelty: The paper introduced peer prediction, an area in computational economics and algorithmic game theory, to learning with noisy labels. The paper studies the label noise problem with the motivation of without estimating the flip rate or transition matrix. While I am not sure about the area of peer prediction, in the area of learning with noisy labels (in a general sense), there were often 10 to 15 papers from every NeurIPS, ICML, *CONF* and CVPR in recent years. Finally, empirical studies show that the propose peer loss indeed remedies the difficulty of determining the noise rates in noisy label learning.
Unified Language Model Pre-training for Natural Language Understanding and Generation NeurIPS 2019" which also proposes a new pretraining approach for question generation and data augmentation for question answering. The proposed pretrained model is then used to finetune on a standard question generation task, which is then used to generate synthetic QA pairs for data augmentation in QA. In particular, the paper compares its approach to Devlin's presentation (https://nlp.stanford.edu/seminar/details/jdevlin.pdf according to the references; is it not a published work?) which uses next sentence generation for pretraining, that is less related to the downstream question generation task. Not only this submission did not discuss this paper (it was posted in May 2019, which is substantially ahead of *CONF* deadline), the result is substantially worse in both BLEU score of question generation and EM/F1 of end QA performance after data augmentation on SQuAD v2. This paper proposes a pretraining technique for question generation, where an answer candidate is chosen beforehand, and the objective is to predict the answer containing sentence given a paragraph excluding this sentence and the target answer candidate. The intuition of this method is that question generation requires to generate a sentence which contains the information about the answer while being conditioned on the given paragraph. I believe this paper should discuss previous work in question generation and compare the performance with them. Third, the paper does not discuss any work in question generation overall, although the task of question generation was studied for decades, including question generation as data augmentation for question answering.
- The difference in performance on each environment with different pre-training reward function (only one in show in the paper right now) It first formulates the problem as a MDP, where the agent takes actions to explore the environment and has two special actions (Answer_True, and Answer_False) to indicate that the agent has made a prediction about the validity of the hypothesis. The paper looks into the problem of training agents that can interact with their environments to verify hypotheses about it. However, (1) the paper lacks any discussion of related work in terms of causal reasoning and partial observability, and (2) the experiments and analysis seem weak. I was though encouraged to see that one of the environment seemed to require slightly different setting in the pre-training reward setup, however the authors didn't follow up with some analysis on why there was such a difference. It does this by first pretraining the agent to perform interventions in the environment which change the states of the objects of interest, and then finetuning the agent to actually make a decision about whether the given hypothesis is correct. The authors present a framework for testing a set of structured hypotheses about environment dynamics by learning an exploratory policy using Reinforcement Learning and a evaluator through supervised learning. A second formulation uses MDP to explore the states and has a special action (Answer), which predicts the validity of the hypothesis based on the last sequence of N states visited. Then, the authors exploit the structure of some hypotheses (such as triplet hypotheses of the form pre_condition, action_sequence, post_condition), which are easier to test. Overall, I enjoyed reading this paper and thought that it provided an interesting take on the question of how to train agents that can appropriately gather information about their environments. - A baseline employing some form of memory (such as heavy usage of frame stacking or recurrency), to attempt at figuring out whether it's really not reasonable to learn the whole problem simply using RL, with ablation of pre-training (which I suspect might make a significant difference).
Motivated by the observation that powerful deep autoregressive models such as PixelCNNs lack the ability to produce semantically meaningful latent embeddings and generate visually appealing interpolated images by latent representation manipulations, this paper proposes using Fisher scores projected to a reasonably low-dimensional space as latent embeddings for image manipulations. In summary, this paper proposes a novel method based on projected Fisher scores for performing semantically meaningful image manipulations under the framework of deep autoregressive models. As mentioned in 1), it's obvious that Fisher scores contain more information than latent activations for deep autoregressive models and are better suited for manipulations. When the α is small, the learned decoder will function similarly to the original pixelCNN, therefore, latent activations produce smaller FID scores than projected Fisher scores for small α's.
Summary: This paper proposes a clustering attention-based approach to handle the problem of unsmoothness while modeling spatio-temporal data, which may be divided into several regions with unsmooth boundaries. With the help of a graph attention mechanism between vertices (which correspond to different regions), the CGT model is able to model the (originally unsmooth) cross-region interactions just like how Transformers are applied in NLP tasks (where words are discrete).
Overall, the method is promising, but I have the following concerns: * Using the reconstruction error as an anomaly score has been explored many years ago (check replicator neural networks), the novelty here is to enforce that on the latent space in the context of a variational auto-encoder. 2.I agree with authors point that WAE is a better choice than VAE for outlier detection because, former "encourages the latent representations as a whole to match the prior ". * In figure 3, in the training process, the authors have describe to add the divergence between the latent and prior distribution to the loss function, however, nothing like this is clearly shown in the figure.The references of figures in the text are either out of place or incorrect. Overall, I am hesitant to recommend the paper before cross-checking the issue with contamination proportion and learning more about how a VAE framework is indeed important for anomaly detection. This work proposes an outlier detection method based on WAE framework. This paper proposes a novel outlier detection approach, based on Wasserstein auto encoders.
------------------------- BEFORE rebuttal The paper proposed different metrics for comparing explainers based on their correctness (ability to find most relevant features in an input, used in prediction), consistency (ability to capture the relevant components while input is transformed), and the confidence of the generated explanations. To evaluate correctness, the authors proposed to study the change in the classification accuracy of the target model, under a perturbed dataset where the most relevant regions (as given by explainer) of the image is preserved and the remaining content is replaced with non-informative backgrounds for the target class. This paper proposes 3 such explanation evaluation metrics, correctness, consistency, and confidence. 1. Correctness: Classifiers have higher accuracy on explanation-masked images than on images they were least confident on (the ones used to fill in the background).
[2] MuJoCo: A physics engine for model-based control, Emanuel Todorov, Tom Erez Yuval Tassa The paper proposes a weak supervision method to obtain labels from functions that are easily programmable, and propose to use this for learning policies that can be "calibrated" for specific style. My main concern here is the technical novelty of the proposed method: it seems that once we have the labels (which are limited to programmable functions), all we need to do is to learn a policy that conditions on the labels. - Again Table 4, if the "-" is indeed a typo, then CTVAE-style is actually performing better than the baselines, especially for Cheetah (normally one wants negative log-likelihood to be as close to 0 as possible). I am concerned at all the requirements though - the method assumes the dynamics of the environment are learnable, and that we can define labeling functions that cover the space of styles we want, both of which seem like strong requirements.
The submission proposes to reduce the memory bandwidth (and energy consumption) in CNNs by applying PCA transforms on feature vectors at all spatial locations followed by uniform quantization and variable-length coding. A lossy transform coding approach was proposed to reduce the memory bandwidth of edge devices deploying CNNs. For this purpose, the proposed method compresses highly correlated feature maps using variable length coding. However, the paper and the work should be improved for a clear acceptance: - Some parts of the method need to be explained more clearly: – In the statement "Due to the choice of 1 × 1 × C blocks, the PCA transform essentially becomes a 1 × 1 tensor convolution kernel", what do you mean by "the PCA transform becomes a convolution kernel."? My concern with the paper is two-fold: 1) The major technique of transform-domain coding is borrowed from previous work (e.g., Goyal 2001), hence the novelty of the proposed method is in doubt.
A new framework for certification is proposed, which allows to use different distributions compared to previous work based on Gaussian noise. The theoretical results are interesting, showing a clear trade-off between robustness and accuracy with a new lower bound and deriving better smoothing distributions. I think the paper should be rejected because (1) For \\ell_2 perturbations, there is no major difference between this new family of distributions (d-k \\chi^2) and a Gaussian with different variance. From this framework, a trade-off between accuracy and robustness is identified and new distributions are proposed to obtain a better trade-off than with Gaussian noise. Also, the reported certified accuracy for Salman et al.'s model for L_inf on CIFAR-10 reported in the original paper is 68.2 at 2/255, which is very far from the 58 in Table 3. Summary: This paper investigates the choice of noise distributions for smoothing an arbitrary classifier for defending against adversarial attacks. This paper presents a new method for adversarial certification using non-Gaussian noise. In any case, the values reported for the proposed model in Table 3 are only a marginal improvement over Figure 1 (left) in Salman et al. Typos in Table 2.: the columns 2.0 to 3.5 are mislabeled The paper introduces an improvement to the randomized smoothing analysis in Cohen et al.
General: The paper proposed to use a causal fairness metric, then tries to identify the Pareto optimal front for the vectorized output, [accuracy, fairness]. This paper proposes a method to approximate the "fairness-accuracy landscape" of classifiers based on neural networks. There is nothing specific in the method or analysis that relates to neural networks, except for the use of the causal estimand in a 'hidden layer'. To this end, the authors propose optimising a Pareto front of the fairness-accuracy trade-off space and show that their resulting method outperforms an adversarial approach in finding non-dominated pairs. The fairness component relies on a  definition of fairness based on causal inference, relying on the idea that a sensitive attribute should not causally affect model predictions. Just use test (validation) set to estimate the accuracy & fairness, then plot the results on the 2d plane. As main contributions, the paper provides: * A Pareto objective formulation of the accuracy fairness trade-off
The proposed learning objective in this work optimizes not only the cross entropy loss, but also the difference between the CD score of a given feature and its explanation target value.
This paper proposed a method of unsupervised representation by transforming a set of data points into another space while maintaining the pairwise distance as good as possible. Overall, the idea in this paper is interesting and effective, the experiment results in two typical unsupervised tasks (anomaly detection and clustering) also look very promising. If the original space is not structured or doesn't naturally have a good distance measure, then the proposed method cannot work.
Conventionally, the sensor placement strategy is tasked to gather the most informative observations (given a limited sensing budget) for maximimally improving the model(s) of choice (in the context of this paper, the neural networks) so as to maximize the information gain. What the authors have done differently is to consider the use of neural nets (as opposed to the widely-used Gaussian process) as the learning models in this sensor placement problem, specifically to (a) approximate the expectation using a set of samples generated from a generator neural net and to (b) estimate the probability term in the entropy by a deterministic/inspector neural net. This paper describes a sensor placement strategy based on information gain on an unknown quantity of interest, which already exists in the active learning literature. The authors propose a framework for sensor placement called Two-step Uncertainty Network (TUN) based on the idea of information gain maximization. Consequently, it is not clear to me whether their proposed strategy would be general enough for use in sensor placement for a wide variety of environmental monitoring applications. More concretely, the proposed method encodes an arbitrary number of measurements, models the conditional distribution of high dimensional data, and estimates the task-specific information gain at unobserved locations.
The paper presents a visually-guided interpretation of activations of the convolution layers in the generator of StyleGAN on four semantic abstractions (Layout, Scene Category, Scene Attributes and Color), which are referred to as the "Variation Factors" and validates/corroborates these interpretations quantitatively using a re-scoring function. The results showing scene property manipulation e.g. in Fig 4 are obtained by varying a certain y_l, and it'd help to also show the results if the initial latent code w was modified directly (therefore affecting all layers!). leads the reader to believe that the findings here are generally applicable e.g. the sentence "the generative representations learned by GAN are specialized to synthesize different hierarchical semantics" should actually be something like "the per-layer latent variables for StyleGAN affect different levels of scene semantics". 3) In Sec 4, this paper only shows some sample results other models e.g. BIGGAN, but no 'semantic hierarchy in deep generative representation' is shown (not surprising given only a global latent code). Despite these positives, I am not sure about accepting the paper because I feel the investigation methods and the results are both very specific to a particular sort of GAN, and the writing (introduction, abstract, related work etc.) A scoring function is obtained to quantify (Equation 1) how the corresponding images vary in a particular semantic aspect when the latent code is moved from the separation boundary. Overall, while the results are interesting, they are only in context of a specific GAN, and using an approach that is applicable to generative models having a multi-layer code. This is repeated at every layer of the GAN generator and the same lamda is used to perturb the resulting output code from the separation boundary. The paper analyzes the relation of various scene properties w.r.t the latent variables across layers, and does convincingly show that aspects like layout, category, attribute etc, are related to different layers. The claim of the paper is that there is a hierarchical encoding in the layers of the StyleGAN generator with respect to the aforementioned "Variation Factors". This paper investigates which layer's latent codes best explain certain variations in scenes. The paper proposes an approach to analyze the latent space learned by recent GAN approaches into semantically meaningful directions of variation, thus allowing for interpretable manipulation of latent space vectors and subsequent generated images. By taking advantage of the structured composition of the latent space into per-layer contributions in the StyleGAN approach, experiments are performed to show that different levels of semantics are captured at different layers: layout being localized in lower layers, object categories in middle layers, followed by other scene attribute, and lastly the color scheme of the image in the highest layers. As a first step, StyleGAN model trained on "bedroom" scenes from LSUN dataset is used to randomly sample codes from the learned distribution, which are further passed through the generator to obtain the respective image mappings.
==== [Summary] To detect out-of-distribution (OOD) samples, the authors proposed to add an explicit "reject" class instead of producing a uniform distribution and OOD sample generation method. - Summary: This paper proposes to improve confident-classifiers for OOD detection by introducing an explicit "reject" class. Comments on rebuttal I don't think that the authors made a valid argument to address my concerns about theoretical justification and experiments. 2. Experimental results are not convincing: in the paper, only grayscale datasets, such as MNIST and FMNIST, are considered to evaluate the proposed method and I think it is not enough. Although this auxiliary reject class strategy has been explored in the literature and empirically observed that it is not better than the conventional confidence-based detection, the authors provide both theoretical and empirical justification that introducing an auxiliary reject class is indeed more effective. Other datasets are also used for OODs. Comparison are made with Confident-Classifier, ODIN, and Mahalanobis distance-based approach, and the proposed method outperforms the others.
Summary: The authors propose a method for learning hierarchical policies in a multi-task setting built on options and casts the concepts into the Planning as Inference framework. Detailed Comments: A primary weakness of this approach is that it seems like there is one network that learns the options and is shared across all task (that would be the prior) and then there is a task-specific network for all options (posterior), wouldn't this be very difficult to scale if we want to learn reusable options over the lifetime of an agent? This paper is about learning hierarchical multitask policies over options. Term 2 controls how the option posterior deviates from the prior. -Allows fine-tuning of options -Learn termination policies naturally It does not seem clear why "term 1 of the regularization is only nonzero whenever we terminate an option and the master policy needs to select another to execute." Some parts of the experiments section does not seem clear to me, Does the proposed approach use a network per task? The authors mention that unlike (Frans et al., 2018), they learn both intra-option and termination policies: there is definitely more work that aims to learn both the skill and its termination. There seems to be no other term that incentivizes the option posterior to deviate, and I do not see how the options are adapting to tasks. The proposed approach derives the reward Eq.6 by extending the graph to options for Levine's tutorial. Mann, and Shie Mannor. However, when the ratio is 1 or less than 1, the value of (6) would increase, and both cases would have made the posterior more like the prior.
This paper proposes a layer on top of BERT which is motivated by a desire to disentangle content (meaning of the tokens) and form (structural roles of the tokens). This paper proposes a fine-tune technique to help BERT models to learn & capture form and content information on textual data (without any form of structural parsing needed). In order to effectively learn R and S embeddings, the authors propose two possible ways to do so: LSTM (Fig 2) and 1-layer Transformer (Fig 3). The empirical gains in transfer learning can be simply attributed to: - More params it seems adding an LSTM over bert embeddings already does some improvement, I would have loved to see this more exploited but it wasn't. This paper proposes an alternative way of reusing pretrained BERT for downstream tasks rather than the traditional method of fine-tuning the embeddings equivalent to the CLS token. In evaluation authors design several experiments to show that: * Does transferring disentangled role & symbol embeddings improve transfer learning
(5) *Replications*: The paper presents results only a single set of experiments using the MNIST dataset with the LeNet architecture. Experiments on LeNet + MNIST show (a) different methods can achieve similar accuracy, (b) pruned sub-networks may differ significantly despite identical initialization, (c) weight reinitialization between pruning iterations yields more structured convolutional layer pruning than not reinitializing, and (d) pruning methods may differ in the stability of weights over pruning iterations. (1) *Overlap in pruned sub-networks*: In the middle of Sec. 4, Fig 3-5 examine the similarity of pruning masks between methods. *Summary* This paper compares network pruning masks learned via different iterative pruning methods. Second, the observations are only presented for LeNet and MNIST and it is non-trivial whether they extend to large scale models. The authors perform experiments mainly on LeNet with the MNIST dataset and analyze the observations.
In addition to that, it analyzed a mixture of linear and non-linear activation functions, and show that mixture is better than single nonlinearity in terms of expected training error for ridge regression estimators. They analyze the performance of a simple regression model trained on the random features and revealed several interesting and important observations. This paper analyzed the asymptotic training error of a simple regression model trained on the random features for a noisy autoencoding task and proved that a mixture of nonlinearities can outperform the best single nonlinearity on such tasks. Pros: - This paper investigates an interesting problem and it successfully extends the existing work.
The main contribution of this paper is that the authors have proved the convergence to the iterated dominance solution for two RL algorithms: REINFORCE (Section 3.1, binary action case only) and Importance Weighted Monte-Carlo Policy Improvement (IW-MCPI, Section 3.2). The main contribution of the paper is to prove that both REINFORCE in binary action case and Monte-Carlo algorithms find the agents' policies converging to the iterated dominance solution. The main idea of this paper is to solve multi-agent reinforcement learning problem in dominance solvable games. [2] Fudenberg & Levine, 1999 This paper studies independent multi-agent reinforcement learning (MARL) in dominance solvable games. The authors examined slightly different learning rules (REINFORCE and MCPI), but I would expect that almost any reasonable learning rule would converge in iterated-dominance-solvable games; if anything, it would be surprising if this were *not* the case. The paper proves the convergence of certain RL algorithms (REINFORCE in the 2-action case, and importance weighted monte-carlo policy iteration in the multi-action case) for normal-form games. The interesting aspect of this paper is that iterated dominance solution based reward scheme can guarantee convergence to the desired agents policies at a cheaper cost in practical principal-agent problems. The authors aim to recover multi-agent policies through independent MARL in norm-form dominance-solvable games. Question to the authors: - How does this work differ from the known results about convergence of naive learners in iterated-dominance-solvable games?
There is no qualitative comparison to other algorithms for two of the problems considered (colorization, super-resolution) and the comparison with other algorithms for unconditional image generation is limited to CIFAR-10; thus, the impact of this contribution is not clear. In this paper the authors present a new way to use autoregressive modeling to generate images pixel by pixel where each pixel is generated by modeling the difference between the current pixel value and  the preexistent ones.
The experimental results show that (1) the first modification, i.e., moving the layer normalization layer to the input stream significantly stabilizes the training; (2) Gated Recurrent Unit (GRU) gating seems to be most effective gating mechanism. * Summary This paper introduces architecture modifications for self-attention to stabilize transformers in reinforcement learning.
The authors conduct extensive experiments on image classification and segmentation and show that dynamic convolutional kernels with reduced number of channels lead to significant reduction in FLOPS and increase in inference speed (for batch size 1) compared to their static counterparts with higher number of channels. - The paper proposes a dynamic convolution selection method can be applied to arbitrary classification networks based on the global average pooled (GAP) feature map info. Conclusion - The author proposed a dynamic kernel selection method (add-on), which can enhance the classification accuracy of the baseline network. === Summary === The authors propose to use dynamic convolutional kernels as a means to reduce the computation cost in static CNNs while maintaining their performance. - Testing the ImageNet trained network of the proposed method into an object detection task (as the pre-trained backbone). The method also proposes the attention-based scaling of channels, where the attention comes from GAP, so the reviewer thinks that it is possible to explain this work as some variation of SEnet.
The paper introduces a regulatory ratio: the probability of using the averaged advantage estimate vs using the order statistic, for computing the policy gradients. The paper conducts experiments on  different domains (sparse and dense rewards, discrete and continuous actions, fully observable and partially observable environments) which show the effect of choosing different order statistics and regulatory ratio on the policy performance. Further, the fact that using a smaller number of advantage estimates worked better (point #2 on pg 5, Effect of Ensemble Size in Appendix A) suggests that the ensemble size is an important hyperparameter, and that risk-seeking / risk-aversion (i.e., regulatory vs promotion focus) cannot alone explain why the proposed method works. This is a bit confusing: using the order statistics (max, min and max-abs), and following the reasoning presented in the paper, I would expect that increasing the ensemble size would result in better performance ( the max/min of the 4 ensemble is n upper/lower bound of the max/min of the 12 ensemble).
###  Summary The paper proposes a method for regularizing neural networks to mitigate certain known biases from the representations learned by CNNs. The authors look at the setting in which the distribution of biases in the train and test set remains the same, but the distribution of targets given biases changes. I agree with R1 on all accounts (i.e. it is very hard to define the family of biased feature extractors, the proposed approach is ad-hoc, the authors need to compare to texture-shape disentanglement methods, etc), however at the same time, I can see that proposed approach can act as a useful heuristic for regularizing neural networks to pay attention to certain kind of information. Weaknesses: 2: In the most general case, it is not obvious why it is easier to define and learn a set of models that only use noise to make predictions (which is the required first step for their proposed solution) as opposed to learning a model that only uses signal (which is the goal of the problem).
The paper proposes a framework for learning with rejection using ideas from adversarial examples. This paper wants to study the problem of "learning with rejection under adversarial attacks". The considered problem is well-motivated and introduced, and I'm unaware of prior work studying classification with reject option in the context of adversarial examples. There is still no universal method to deal with adversarial examples, and introducing a reject option to flag potential attacks seems a sensitive choice for many applications. The paper fails to realize that the motivated application is actually called "cost-sensitive learning" and has been studied long time before. The current paper has not discussed any related work of cost-sensitive learning although they want to study a problem in its field. It then considers the classical cost-sensitive learning by transfer the multi-class problem into binary classification problem through one-vs-all and using the technique they proposed to reject predictions on non-important labels, and name such technique as "learning with protection".
The authors propose a framework to incorporate additional semantic prior knowledge into the traditional training of deep learning models such that the additional knowledge acts as both soft and hard constraints to regularize the embedding space instead of the parameter space. The domain that the method is applied to is VQA, with various relations on the questions translated into hard constraints on the embedding space. Compared with the existing models that treat the constraints as soft regularizers, the authors propose to additionally distill the knowledge using teacher-student framework. The paper argues for encoding external knowledge in the (linguistic) embedding layer of a multimodal neural network, as a set of hard constraints. To illustrate the idea, the authors use 3 different annotated knowledge that are already available in a public dataset that contains equivalent statements, entailed statements as well as functional programs and show that the final performance indeed increases.
This paper proposed to use VAE to learn a sampling strategy in neural architecture search. This paper proposes to use the variational auto-encoder (VAE) to sample the network architectures. The main idea is to use the currently high-performing networks to train a VAE from which the sampled architectures for the next iteration will likely supply both high-performing networks and better diversity coverage. This paper proposes to model the architecture distribution using a VAE instead, where the encoder and decoder are implemented using LSTMs.
The authors propose DTSIL to learn a trajectory-conditioned policy to imitate diverse trajectories from the agent's own past experience. This paper proposes an approach for diverse self-imitation for hard exploration problems. The authors identify and address the problem of sub-optimal and myopic behaviors of self-imitation learning in environments with sparse rewards. Unlike other self-imitation learning methods, the proposed method not only leverages sub-trajectories with high rewards, but lower-reward trajectories to encourage agent exploration diversity. The idea is leverage recently proposed self-imitation approaches for learning to imitate good trajectories generated by the policy itself. The proposed trajectory-conditioned policy sounds, since rewarded trajectory carries significant information of the goal in the exploration problem. By encouraging diversity in the pool of trajectories for self-imitation, the idea is to encourage faster learner -- this basic concept is also used in approaches like prioritized experience replay, albeit at the entire trajectory level rather than individual state/action level. The approach taken is to apply self-imitation to a diverse selection of trajectories from past experience -- practice re-doing the strangest things you've ever done.
*Summary* This paper considers the effect of partial models in RL, authors claim that these models can be causally wrong and hence result in a wrong policy (sub optimal set of actions). Then authors suggest a simple solution using backdoors (Pear et al) to learn causally correct models.They also conduct experiments to support their claims. 4. For sentence, "Fundamentally, the problem is due to causally incorrect reasoning: the model learns the observational condi- tional p(r|s0, a0, a1) instead of the interventional conditional given by p(r|s0, do(a0), do(a1)) = s1 p(s1|s0, a0)p(r|s1, a1)." This paper tackles the issue of identifying the causal reasoning behind why partial models in MBRL settings fail to make correct predictions under a new policy. *Decision* I vote for rejection of this paper, based on the following argument: To my understanding authors are basically solving the "off-policy policy evaluation" problem, without relating to this literature. To make the partial model causally correct, the paper considers the partial model conditioned on the backdoor that blocks all paths from the confounding variables. Authors demonstrate this issue with a simple MDP model, and emphasize the importance of behavior policy and data generation process. The authors then reformulate the model-learning problem using a causal learning framework and propose a solution to the above-mentioned problem using the concept of "backdoors." They perform experiments to demonstrate the advantages of doing so. The paper considers the problem of predicting a variable y given x where x may suffer from a policy change, e.g., x may follow a different distribution than the original data or suffer from a confounding variable. The flow of the paper proceeds in learning a causally correct model in the sense that the model is robust to any intervention changes.
The authors provide a general application on sequential data for continual learning, and show their proposed model outperforms baseline. Summary: In this paper, the authors propose a new method to apply continual learning on sequential data. The paper proposed an interesting continual learning approach for sequential data processing with recurrent neural network architecture. - In the experiments, the authors only compare the proposed model with simple LSTM or LMN. The goal of this work is to best understand the performance and benchmarking of continual learning algorithms when applied to sequential data processing problems like language or sequence data sets. The experiments on several datasets show the proposed model outperforms basic LSTM/LMN. Weakness: - In this paper, the model size linearly increases since the number of LSTM/LMN and AE increases when a new task comes in. It is natural that their naive baseline shows poor performance since they do not consider any continual learning issues like the catastrophic forgetting problem.
This paper proposed a new query efficient black-box attack algorithm using better evolution strategies. The experimental results show that the proposed method achieves state-of-the-art attack efficiency in black-box setting. This paper proposes a black box adversarial attacks to deep neural networks. The experimental results look quite promising, i.e., revealing the vulnerability of the deep neural network against black-box adversarial attacks.
In this paper, the authors proposed a training method called global momentum compression (GMC) for distributed momentum SGD with sparse gradient. The authors propose GMC, a training method for distributed momentum SGD with sparse gradient communication. This is the first work on proving the convergence rate of distributed momentum SGD using sparse communication techniques based on memory gradient. I think in general the ideas and efforts of the authors in proving the convergence rate of distributed momentum SGD with *gradient sparsification* is interesting and important. But I did not find convincing support of this advantage in the paper: Empirically, in the experiment results, I don't think GMC demonstrate better performance than DGC in a statistical meaningful way; instead they are basically demonstrating matching performance.
The algorithm selects a subset of datapoints to approximate the training loss at the beginning of each epoch in order to reduce the total amount of time necessary to solve the empirical risk minimization problem. In particular, the algorithm formulates a submodular optimization problem based on the intuition that the gradient of the problem on the selected subset approximates the gradient of the true training loss up to some tolerance. Based on the experiments provided in the paper, it does appear to yield a significant speedup in training time. The empirical evaluation of the method shows large speedups in training time without degradation in performance for reasonably large subsets (e.g. 20% of the data). Experiments demonstrate significant savings in time for training both logistic regression and small neural networks. Strengths: The proposed idea is novel and intriguing, utilizing tools from combinatorial optimization to select an appropriate subset for approximating the training loss. But if the baselines weren't thoroughly tuned, it could be the case that IG on the CRAIG subset performs similarly to IG on the full training data, but that neither is actually reaching satisfactory performance in a given domain. One of the main claims of the paper is that using the subset selected by CRAIG doesn't significantly effect the optimization performance. Given a dataset D and subset S selected by the proposed method, it is shown that training on subset S achieves nearly the same loss as training on the full dataset D would, while achieving significant computational speedups. Clarifying questions: - In results reporting speedups, does the reported training time for CRAIG
has been -> have without explicit defines -> defining This paper proposes a training objective that combines three terms: * A Stein discrepancy for learning a energy model with intractable normalizing constant The energy based model (E) is trained using Stein Divergence with a fixed kernel k or a learned critic who's parameters are denoted pi in the paper.
Summary: This paper list several limitations of translational-based Knowledge Graph embedding methods, TransE which have been identified by prior works and have theoretically/empirically shown that all limitations can be addressed by altering the loss function and shifting to Complex domain. Furthermore, the paper proposes TransComplEx -- an adaption of ideas from ComplEx/HolE  to TransE -- to mitigate issues that can not be overcome by a simply chosing a different loss. Regarding the experimental evaluation: The paper compares the results of TransComplEx and the different loss functions to results that have previously been published in this field (directly, without retraining). Even more serious: Following again Section 5 (Dataset), it seems that the paper imputes all missing triples in the training set for symmetric and transitive relations ("grounding"). The authors propose four variants of loss function which address the limitations and propose a method, RPTransComplEx which utilizes their observations for outperforming several existing Knowledge Graph embedding methods. In this paper, the authors investigate the main limitations of TransE in the light of loss function. The paper revisits limitations of relation-embedding models by taking losses into account in the derivation of these limitations. The results show that the proper selection of the loss function can mitigate the limitations of TransX (X=H, D, R, etc) models.
This paper aims to propose a speeding-up strategy to reduce the training time for existing GNN models by reducing the redundant neighbor pairs. But there are no experimental results verifying that the effectiveness of the HAG-versions which could obtain comparable performance with the original GNN models for some downstream applications (e.g., node classification). In this paper, authors propose a way to speed up the computation of GNN. 2. The authors state that the HAG can optimize various kinds of GNN models, but the experiment only shows the results on a small GCN model. More specifically, the hierarchically aggregate computation graphs are proposed to aggregate the intermediate node and utilize this to speed up a GNN computation.
This paper studies the connection between sensitivity and generalization where sensitivity is roughly defined as the variance of the output of the network when gaussian noise is added to the input data (generated from the same distribution as the training error). This paper examines generalization performance of various neural network architectures in terms of a sensitivity metric that approximates how the error responds to perturbations of the input.
The authors also propose a new dataset for theorems and proofs for a simple equational theory of arithmetic, which is again suitable for improving (via learning) and testing the ability of the prover for finding long proofs. Overall the paper attempts to explain clearly the original contribution of the proposed approach, which is using curriculum learning in RL based proof guidance. On the one hand, the paper tackles an interesting problem of improving an automated theorem prover via learning, in particular, its ability for finding long nontrivial proofs. OVERALL: I don't work on ATP and am not particularly well suited to review this paper, but I am slightly inclined to accept for the following reasons. Another thing that demotivated me is that I couldn't find the discussion about the subtleties in using curriculum learning and PPO for the theorem-proving task in the paper.
#Summary: The paper proposed a method that utilizes the model's explainability to detect adversarial images whose explanations that are not consistent with the predicted class. This paper suggests a method for detecting adversarial attacks known as EXAID, which leverages deep learning explainability techniques to detect adversarial examples. - I would also like to see how these results hold good for a complicated dataset like ImageNet A Simple method to detect adversarial examples, but needs more work.
Very good paper that studies the error rate of low-bit quantized networks and uses Pareto condition for optimization to find the best allocation of weights over all layers of a network. This works presents a method for inferring the optimal bit allocation for quantization of weights and activations in CNNs. The formulation is sound and the experiments are complete. My main concern is regarding the related work and experimental validation being incomplete, as they don't mention a very recent and similar work published in ICIP19 https://ieeexplore.ieee.org/document/8803498: "Optimizing the bit allocation for compression of weights and activations of deep neural networks".
First, the authors calculate the influence score for the models with/without pretraining, and then propose some implementation details (i.e., use CG to estimate the inversed Hessian). This paper proposes a multi-stage influence function for transfer learning to identify the impact of source samples to the performance of the learned target model on the target domain. I believe that these are useful technical contributions that will help to broaden the applicability of influence functions beyond the standard supervised setting. The authors derive the influence function of models that are first pre-trained and then fine-tuned. To calculate the influence function of a model with pretraining, the authors use an approximation f(w)+||w-w*||, where w* is pretrained. To do so, the authors make two methodological contributions: 1) working through the calculus for the pre-training setting and deriving a corresponding efficient algorithm, and 2) adding L2 regularization to approximate the effect of fine-tuning for a limited number of gradient steps. I have some questions and reservations about the current paper: 1) Does pretraining actually help in the MNIST/CIFAR settings considered? This extends influence functions beyond the standard supervised setting that they have been primarily considered in. These seem to be non-standard pretraining settings. This is an analysis paper of pretraining with the tool "influence function".
This work is focused on topological characterization of target surfaces of optimization objectives (i.e. loss functions) by computing so called barcodes, which are lists of pairs of local minima and their connected saddle points. From my perspective, the whole story revolves around how to compute persistence barcodes from the sub-levelset filtration of the loss surface, obtained from function values taken on a grid over the parameters. The authors state that "it is possible to apply it to large-scale modern neural networks", but it's not clear to me how that would work or what additional algorithmic improvements (if any) would need to be made in order to do so. Given the problems in the writing of the paper, my assessment is that this idea boils down to computing persistent homology of the sub-levelset filtration of the loss surface sampled at fixed parameter realizations. (4) The author's talk about the "minima's barcode" - I have no idea what is meant by that either; the barcode is the result of sub-levelset persistent homology of a function -> it's not associated to a minima.
This work provides  theoretical analysis for the NAS using weight sharing in two aspects: 1) The authors give non-asymptotic stationary-point convergence guarantees (based on stochastic block mirror descent (SBMD) from Dang and Lan (2015)) for the empirical risk minimization (ERM) objective associated with weight-sharing. Based on this analysis, the authors proposed to use  exponentiated gradient to update architecture parameter, which enjoys faster convergence rate than the original results in Dang and Lan (2015). The author also provided an alternative to SBMD that uses alternating successive convex approximation (ASCA) which has similar convergence rate. The author proposed ASCA, as an alternative method to SBMD. This work proposes an algorithm for handling the weight-sharing neural architecture search problem.
The authors study a combinatorial multi-robot scheduling problem (in fact the robot part is a bit inflated, since the experiments only involve agents in a simulated discrete state-space maze) using a method that builds upon recent advances from [Dai et al. In this paper, the authors propose a reinforcement learning method for multi-robot scheduling problems.
Suggestions for improvement: 1) Considering the experimental results in this paper, it is possible that the existing graph classification tasks are not that difficult so that the simplified GNN variant can also achieve comparable or even better performance (easier to learn). To study this problem, this paper proposes two models, Graph Feature Network (GFN) and Graph Linear Network (GLN). Experimental results on benchmarks datasets for graph classification show that GFN can achieve comparable or even better performance compared to recently proposed GNNs with higher computational efficiency. See: https://arxiv.org/abs/1809.02670 https://arxiv.org/abs/1905.13192 The paper dissects the importance of two parts in GCN: 1) nonlinear neighborhood aggregation; 2) nonlinear set function by linearizing the two parts and resulting in Graph Feature Network (GFN) and Graph Linear Network (GLN).
For evaluation authors report their average accuracy on Permuted MNIST, Split-MNIST, and CIFAR10-100 and achieve superior performance over EWC, DLP, SI, VCL-Coreset, and FRCL. Summary The paper proposes a method for continuous learning called Functional Regularization of Memorable Past (FROMP) which maintains the output distribution of models on memory samples. [Reviewer's response:] I still insist on the fact that simply explaining the overhead of a method is not a support for scalability claim versus showing the performance on a large scale dataset and comparing it with other CL methods that also have high scalability given the fact that authors only use MNIST and CIFAR datasets. 4- Ambiguous claims about prior work: (a) On page 1, paragraph 3, when authors mention that methods such as GEM or iCaRL use random selection to pick previous samples, I think the line of follow-up work on these methods should be mentioned as well that have explored different techniques for sample selection and have provided benchmark comparisons (ex.
I Summary The paper directly answers two sanity checks for saliency maps proposed by Adebayo et al (2018): 1. Overall the method solves the aforementioned sanity checks, the authors claim it also generates more refined saliency maps. My main concern is that the method seems to be designed only to answer the sanity checks: the resulting saliency maps can hardly be seen as more informative as other existing methods (eg figure 1). It addresses a problem posed for existing methods for characterizing saliency in activation subject to sanity checks which measure the degree to which the activation (saliency) map changes subject to different randomization tests. The proposed approach computes the saliency maps for all the classes and removes the pixels that play a role in predicting several classes. Content The paper can be hard to read, due to multiple writing mistakes, abrupt phrasing, not well-articulated sentences.
Many previous unsupervised video object segmentation methods make use of optical flow and boundary detection, which I thought are OK cues to be used, especially when both can be learned in a self-supervised manner. The paper introduces a method to self-supervised train a model for object detection/segmentation. The main benefit over many previous unsupervised object detection/segmentation approaches is that they did not make use of optical flow or other readily available cues during training.
This paper empirically examines an interesting relationship between mode connectivity and matching sparse subnetworks (lottery ticket hypothesis). As acknowledged in the limitations section, other relationships may exist between stability and matching subnetworks found by other pruning methods, or in different sparsity levels, ——— 1) In the abstract the paper claims that sparse subnetworks are matching subnetworks only when they are stable, but the results are shown in a limited setting only at a very high sparsity.
This paper proposes a new method for geometric matrix completion based on functional maps. This paper proposes a novel approach for the loss function of matrix completion when geometric information is available. In addition, the zoomout loss is motivated by the approach for shape correspondence and can be a generalization of the recent matrix completion method (deep matrix factorization). So, it is expected that the proposed method shows a better result when the given geometric model is not accurate. Second, the proposed method has a scalability issue since it requires eigendecompositions of graph Laplacians (as discussed in the paper).
as do for – > as we do for uniformally - > uniformly This paper used the field-theory formalism to derive two approximation formulas to the expected generalization error of kernel methods with n samples. This paper explores how tools from perturbative field theory can be used to shed light on properties of the generalization error of Gaussian process/kernel regression, particularly on how the error depends on the number of samples N. Given actual experiments are typically performed for a single realization of the data, I think this point should be mentioned in the main text more explicitly. I am OK with the physics jargon the authors used in the paper, as well as the non-rigorous of the result. The results presented here are interesting and I particularly liked the introduction of the renormalized kernel to study the noiseless case. This theoretical paper exploits a recent rigorous correspondence between very wide DNNs (trained in a certain way) and Neural Tangent Kernel (a case of Gaussian Process-based noiseless Bayesian Inference). This paper is technical and some readers will be interested of simply knowing the hypothesis made and type of results obtained.
This paper should be rejected due to following reasons: (1) The authors do not justify/discuss the motivation and importance of the task and corresponding applications that would require to predict topology of complete graph in the next step. The authors need to provide concrete justification for the problem they address, instances where such a task would be useful and discussion on other techniques that can do similar tasks but lack in aspects that such a method can capture. The proposed approach is validated with experiments on three synthetic datasets and one real-world dataset (Bitcoin is same dataset from two different resources with little difference in characteristics) and compared against random graph models. Specifically, the paper uses a combination of recently proposed techniques in graph representation learning (Graph Neural Network) and Graph Generation (GraphRNN [You et. Why not use various graph datasets available in papers that learn representations (e.g. cited by authors themselves) What is special about bitcoin dataset that makes it suitable for this task? This paper proposes a framework to model the evolution of dynamic graphs for the task of predicting the topology of next graph given a sequence of graphs. In fact, I appreciate the authors for reporting negative results as it provides a transparent insights into the effectiveness of model in different settings. The learned vector is then used as input to a GraphRNN decoder to generate a graph that would serve as a predicted next graph in the sequence. In this paper, the authors propose a new neural network architecture for predicting the next graph conditioned on a past graph sequence.
1. The authors claimed the proposed method could help with exploding gradient  in training the linear memories. Summary: The paper proposes an autoencoder-based initialization for RNNs with linear memory. The proposed initialization is aimed at helping to maintain longer-term memory and instability during training such as  exploding gradients (due to linearity). Summary: This paper proposes a new initialization method for recurrent neural networks. [2] Linear Memory Networks This paper proposes an initialization scheme for the recently introduced linear memory network (LMN) (Bacciu et al., 2019) and the authors claim that this initialization scheme can help improving the model performance of long-term sequential learning problems. There are some confusions, on P2 "we can construct a simple linear recurrent model which uses the autoencoder to encode the input sequences within a single vector", I think the authors meant encode the input sequences into a sequence of vectors? Reference: [1] Pre-training of Recurrent Neural Networks via Linear Autoencoders Second, the autoencoder-based init scheme (Pasa&Sperduti, 2014) is not new while the only technical contribution of this paper is a minor change of this scheme so that it works for the LMN. Strength: The method of initializing LMN using a linear RNN is natural and simple.
The paper explores multi-task learning in embodied environments and proposes a Dual-Attention Model that disentangles the knowledge of words and visual attributes in the intermediate representations. *Summary The paper describes a Dual-Attention model using Gated- and Spatial-Attention for disentanglement of attributes in feature representations for visually-grounded multitask learning. They show that both the proposed dual-attention method and multi-task learning would contribute to the performance.
Although the problem of graph summarization is a relevant task, there are a number of unclear points in this paper listed below: - In Section 3.1, the coarsening method has been proposed, which is said to be achieved by finding S such that A_C = S^T A S. This paper proposes a method to summarize a given graph based on the algebraic multigrid and optimal transport, which can be further used for the downstream ML tasks such as graph classification. It looks like the main point is  that this architecture is trying to emulate iterative coarsened residual optimization of the Wasserstein metric between a graph and its representation. The paper proposes a differentiable coarsening approach for graph neural network (GNNs). This paper proposes an unsupervised hierarchical approach for learning graph representations.
The authors first derived the theory of F-pooling to be optimal anti-aliasing down sampling and is shift-equivalent in sec 2, and then demonstrated the experimental results of 1D signals and image classification tasks. This paper proposed a new pooling method (Frequency pooling) which is strict shift equivalent and anti-aliasing in theory. This paper gives an improved definition on shift-equivalent functions and shows that the proposed F-pooling is optimal in the sense of reconstructing the orignal signal. This paper proposed "F pooling" for Frequency Pooling, which is a pooling operation satisfying shift equivalence and anti-aliasing properties. As indicated in the recent literature, enforcing shift-invariance does help to improve the performance of a CNN on classification accuracy and the robustness with respect to image shift. This paper researches the pooling operation, which is an important component in convolutional neural networks (CNN) for image classification. The authors stated that "F-pooling remarkably increases accuracy and robustness w.r.t. shifts of moderns CNNs"; however, in Table 1-3, the winning margin of accuracy is actually quite small (<2%), and the consistency (<3.5% compared to the second best baseline except resnet-18 on CIFAR 100 has large improvement ~7-8%).
2) The algorithm does not have any important contributions comparing to existing ones: they define a prototype per class based on the pre-trained model and apply the nearest neighbor classification. This paper proposed a new realistic setting for few-shot learning that we can obtain representations from a pre-trained model trained on a large-scale dataset, but cannot access its training details. My major concerns: 1) They try to propose a new problem, but their description shows that the problem is exactly the same as what most "few-shot learning" works aim to solve: use a pre-trained model, train a meta-learner on few-shot training tasks, and apply it to novel test tasks. Is the attention way used in the paper a good way to exploit the pre-trained model for few-shot classification problems? Back to the standard few-shot classification problem, they will first adapt the model with base class samples and then adapt to novel classes. A new task is suggested, similarly to FSL the test is done in an episodic manner of k-shot 5-way, but the number of samples for base classes is also limited.
- This paper experimentally shows that multi-head architecture performs well on MNIST and CIFAR-10 in terms of accuracy and uncertainty. </update> Summary & Pros - This paper proposes a simple yet effective distillation scheme from an ensemble of independent models to a multi-head architecture for preserving the diversity of the ensemble. Experiments illustrate that the multi-headed architecture approximates the ensemble marginally better than approaches that use a network with a single head. The paper proposes to distill the predictions of an ensemble with a multi-headed network, with as many heads as members in the original ensemble. The problem of making better ensemble distillation methods seems relevant as ensembles are still one of the best ways to estimate uncertainty in practice (although see concerns below). - The proposed scheme provides the same advantages of the ensemble in terms of uncertainty estimation and predictive performance, but it is computationally efficient compared to the ensemble. To verify the effectiveness of the proposed distillation method, other large-sized datasets should be tested, e.g., CIFAR-100, ImageNet. Concerns #3: Week efficiency - As reported in Table 5, in the case of CIFAR10, Hydra has 14x more parameters and 6x more FLOPs. Despite such a large number of parameters, the performance gain seems to be incremental.
The authors conduct experiments on image classification tasks to show the performance of the proposed method and compare it with two other baselines EWC and OWM. Experiments show improved results over OWM (the method that this paper builds on) and EWC. 4. It is not clear why the proposed method can solve the issue that OWM faces with (bad accuracy when tasks are not quite related). The authors propose a method based on principal components projection to tackle this issue. 2. It might strengthen the paper if the authors can show the comparison results on more other datasets, e.g., other image classification tasks. It would be better if the authors can show the proposed method can generalize to other tasks.
The paper also show ACN uses much less numbers of parameters and achieves similar accuracy when comparing with a large optima FC network on a set of datasets. This paper explores the use of replicating neurons across and within layers to compress fully connected neural networks. The idea is simple, and is evaluated on a number of datasets and compared with fully connected, single layer, and several compression schemes. This paper proposes a new way to create compact neural net, named Atomic Compression Networks (ACN).
The authors proposed a series of improvements, including alternative optimization, dynamic scheduling, detach and batch normalization to help boosting the performance to SOTA under 4-bit quantization. In this paper, the authors propose a framework towards 4-bit auantization of CNNs. Specifically, during training, the proposed method contains a full precision branch supervised by classification loss for accurate prediction and representation learning, as well as a parameterized quantization branch to approximate the full precision branch. This work introduces GQ-Net, a novel technique that trains quantization friendly networks that facilitate for 4 bit weights and activations. However, it seems to me that this drastic scheduling strategy sounds like very similar to the traditional approach that trains the floating point network first and then finetune the quantized one, except for the fact that this proposed algorithm repeats this process a few times. The authors argue that this has the effect of "guiding" the optimization procedure in finding networks that can be quantized without loss of performance. This is achieved by introducing a loss function that consists of a linear combination of two components: one that aims to minimize the error of the network on the training labels of the dataset and one that aims to minimize the discrepancy of the model output with respect to the output of the model when the weights and activations are quantized. The authors then evaluate their method on Imagenet classification using ResNet-18 and Mobilenet v1 / v2, while also performing an ablation study about the extra tricks that they propose. For example, the alternative optimization of W and \\theta is similar to alternative re-training in network pruning, although a unified loss/optimization framework is applicable in this case.
This paper only proposes some minor improvements based on the original CPC method and use a deeper network to get better performance. The paper proposes to use Contrastive Predictive Coding (CPC), an unsupervised learning approach, to learn representations for further image classification. The authors augment contrastive predictive coding (CPC), a recent representation learning technique organized around making local representations maximally useful for predicting other nearby representations, and evaluates their augmented architecture in several image classification problems. This paper improves Contrastive Predictive Coding method and reaches a good performance in several downstream tasks. I am sure that some of the methods used here could lead to improvements in the use of CPC for other types of data, but the authors currently don't provide any insight on this issue. They then use this improved model to obtain impressive performance in classification within semi-supervised and transfer learning settings, giving strong support for the use of such methods within image processing applications. The authors show that using CPC for representation learning allows to achieve better results than other self-supervised methods. My point is that the paper presents exactly the same idea as the original paper of CPC, but with new, very interesting results. The authors also make the observation that linear separability, the standard benchmark for evaluating unsupervised representations, correlates poorly with efficient prediction in the presence of limited labeled data.
Finally, it would be useful if the authors comment on existing methods for measuring music similarity in symbolic music and how their proposed feature fits into existing work. The performance conditioning vector is generated by an additional encoding transformer, compared to the Music Transformer paper (Huang et. I am not working on music generation but I list two CV related papers about conditional image translation, which mathematically describes "an image with specific style". Why not compare the conditioning melody with the generated performance similar to query-by-humming? [ref2] Conditional image-to-image translation, CVPR'18 This paper presents a technique for encoding the high level "style" of pieces of symbolic music. ## summary In this paper, the author extends the standard music Transformer into a conditional version: two encoders are evolved, one for encoding the performance and the other is used for encoding the melody. Why use this feature compared to existing techniques for measuring similarity between symbolic music pieces? I also don't see the connection between this proposed feature vector and using the IMQ kernel for measuring similarity. The following two papers are about conditional unsupervised image-to-image translation, which build a cycle-consistency loss during the feedback and might help improve the performances. The authors conduct experiments on the MAESTRO dataset and an internal, 10,000+ hour dataset of piano performances to verify the proposed algorithm. The main strategy is to condition a Music Transformer architecture on this global "style embedding". 5. In section 5.2, a conditioning sample, a generated sequence and an unconditional sample are used to compute the similarity measure. It took me a couple of passes and reading the Music Transformer paper to realise that in the melody and performance conditioning case, the aim is to generate the full score (melody and accompaniment) while conditioning on the performance style and melody (which is represented using a different vocabulary). 2. By checking the music Transformer, in Table 3, it is not surprising to see that the proposed method outperforms the corresponding baselines, because no conditional information is used. Additionally, the Music Transformer model is also conditioned on a combination of both "style" and "melody" embeddings to try and generate music "similar" to the conditioning melody but in the style of the performance embedding. The authors mention (Yang and Lerch, 2018) but use a totally different set of attributes compared to that paper. 2. Figure 1 should be clarified or another figure should be added to show how the melody conditioning works. 3. The MAESTRO dataset is described in terms of the number of performances while the internal dataset is described in terms of the number of hours of audio. The authors also mention an internal dataset of music audio and transcriptions, which can be a major contribution to the music information retrieval (MIR) community.
Typos: Page 7: with with roughly 200M ->  with roughly 200M In this paper, the authors propose a method to train deep neural networks using 8-bit floating point representation for weights, activations, errors, and gradients. Originality: The paper proposed a new scaling loss strategy for mixed-precision (8-bit mainly) training and verified the importance of rounding (quantization) error issue for low-precision training.
While potentially offering a faster convergence with respect to epochs, the nonlinear updates have two major drawbacks: 1) While there are preliminary theoretical results (fixed points of the method are critical points), it remains unclear whether the computed update is still a descent direction on the original energy. The work is based on the recent paper 'Proximal Backpropagation', Frerix et al., *CONF* 2018, which views error back propagation as block coordinate gradient descent steps on a penalty functional comprised of activations and parameters.
Since these OCHs are differentiable, the proposed DBNN model can be used for streaming input data with time-variant distributions. This paper introduces the vector quantization but doesn't mention the use of it in streaming data in related work, which kind of blurs the contribution a bit. The idea is to combine online vector quantization with Bayesian neural networks (BNN), so that only incremental computation is needed for a new data point. The paper is missing a related work section describing state of the art methods to address stream data. In the introduction, the authors motivate the proposed DBNN by saying that BNN needs dozens of samples from weight distributions and therefore is rather inefficient. The paper proposes to use an online code vector histogram (OCH) method attached to the input and output of a classical DNN.
This paper proposed a new method for knowledge distillation, which transfers knowledge from a large teacher network to a small student network in training to help network compression and acceleration. The paper suggests a new method for knowledge transfer from teacher neural network to student: student-teacher collaboration (STC). 1. To reduce model size, there are several different ways including efficient architecture design, parameter pruning, quantization, tensor decomposition and knowledge distillation. It looks to me the proposed method use an ad-hoc selected layer to transfer knowledge from teacher to student, and the transfer is indirect because it has to go through the pre-trained subnetwork in teacher. 2. Utilizing the "soft targets" to transfer knowledge from teacher to student model is not first proposed by Hinton et al. The related work section provides thorough review of different methods to decrease computational costs, including not only knowledge distillation, but also pruning, compressing and decomposition approaches.
The paper has provided consistency guarantee and several synthetic and real data experiments as support. A comparison to regularized CCA, application to more datasets and simulations under violations of the model would greatly improve the paper. This paper appears to be technically sound, but it should be rejected based on 1) the relatively limited applicability of the model and 2) a lack of thorough experimentation indicating that this is an appropriate method under more general circumstances.
(2018b) requires validation to determine weights for feature ensembling, but the validation can be done without OOD data by generating adversarial samples as proposed in the same paper. - Summary: This paper proposes an out-of-distribution detection (OOD) method under constraints that 1) no OOD is available for validation and 2) model parameters should be unchanged. =================== The work proposes a system for detecting out-of-distribution images for neural networks under strict limitations of not retraining the network or tuning parameters with out of distribution validation data in mind using Compression Distance in a novel way. Therefore, rather than comparing with the vanilla version (only using last latent space) or the alternative "assemble" method (concatenating all average-pooled features), they had to compare their method with the model validated by adversarial samples, which essentially satisfies the constraints. They specifically address a problem of the state-of-the-art method satisfying the constraints, and propose a new distance metric inspired by data compression. This paper proposes a new framework for out-of-distribution detection, based on global average pooling and spatial pattern of the feature maps to accurately identify out-of-distribution samples. (2018b) shows that the performance is not better than the case when we have an explicit OOD data for validation, it reasonably works well. They propose to use compression based distance measures off the shelf from standard compression techniques to detect spatial feature patterns in feature space and demonstrate its effectiveness on several datasets and comparison with baselines is reported and well discussed. As addressed by the authors, feature concatenation ("assemble") is not effective for the Mahalanobis method but the proposed method. After reading the other reviews and comments, I appreciate the effort by the Authors, but it looks like the paper still needs some work before being ready. To me, if the work could be changed to compare against works which are not so tightly constrained, not for the purposes of holding it to the same standard but to understand it's relative standing, or to better justify the very strict constraints which somehow, despite out-of-distribution detection being a popular upcoming topic, apparently only has one other paper that matches it. Again, weights can be validated by adversarial samples to satisfy the constraints. There is also  theoretical guarantees showing exhaustiveness of the proposed methods in detecting all possible out-of distribution examples. So, it would be interesting to see the performance of both baselines and proposed approach in those settings where inputs are similar in nature but very different in some aspects. Their main difference would be, while adversarial attack is very close to the clean data in the data space, OOD is relatively far from the in-distribution in the data space.
The paper presents an approach - Delayed and Temporally Sparse Update (DTS) - to do distributed training on nodes that are very distant from each other in terms of latency. Authors provide a technique to compensate for error introduced by stale or infrequent synchronization updates in distributed deep learning.
This paper investigates benefit of over-parameterization for latent variable generative model while existing researches typically focus on supervised learning settings. This paper performs empirical study on the influence of overparameterization to generalization performance of noisy-or networks and sparse coding, and points out overparameterization is indeed beneficial. The paper "aims to be a controlled empirical study making precise the benefits of overparameterization in unsupervised learning settings. A small gripe: the authors promise " a controlled empirical study making precise the benefits of overparameterization in unsupervised learning settings". I am expecting some theoretical analysis for tasks simple as noisy-or and sparse coding, or some experiments for more complicated (deep) models need to be done, to make the paper more solid. The generative models to obtain disentanglement representation could be investigated in the frame-work of this paper. As the authors point out (and I agree), the paper constitutes a compelling reason for theoretical research on the interplay between overparameterization and parameter recovery in latent variable neural networks trained with gradient descent methods. Hence, I think what investigated in this paper can be discussed by relating sparse coding theories. If there were thorough investigations on more modern deep generative models, then the paper would be stronger. " The author's empirical study is comprehensive, and to my knowledge the most detailed published work on this to date. In line with the findings for supervised settings, the authors find that overparameterization is often beneficial, and that overfitting is a surprisingly small issue. For example, the latent variable model is recently well discussed in the context of disentanglement representation.
Although I really liked section 3 where authors establish the different ways in which `posterior collapse' can be defined, overall I am not sure if I can extract a useful insight or solution out of this paper. In particular, I believe it would be more accurate to say that IF the autoencoder has bad local minima then the VAE is also likely to have category (v) posterior collapse. The paper first discusses various potential causes for posterior collapse before diving deeper into a particular cause: local optima. After categorizing difference causes of posterior collapse, the authors present a theoretical analysis of one such cause extending beyond the linear case covered in existing work. 2) Section 4 provides a brief overview of existing results in the affine case and introduces a non-linear counter-example showing that local minima may exist which encourage complete posterior collapse. c. I think it would be good to think about the intuition of this as well: "unavoidably high reconstruction errors, this implicitly constrains the corresponding VAE model to have a large optimal gamma value": isn't this intuitive to improve the likelihood of the hyperparameter gamma given the data? e. The experiments consider training AEs/VAEs with increasingly complex decoders/encoders and suggest there is a strong relationship between the reconstruction errors in AEs and VAEs, and this and posterior collapse.
The authors proposal methods perform nominally better on the tasks being investigated, but much of the latter portion of the paper continues to focus on the 'improvement' in the metric they use to diagnose the 'bias'. * another nit: "suggest a surprising conclusion: the environment bias is attributed to low-level visual information carried by the ResNet features." --> idk that this is that surprising, it was kind of natural given the result that removing visual features entirely doesn't hurt performance and helps generalization. I imagine that more data in the training set (if it existed) could help to reduce the gap in performance the paper is concerned with. This paper aims to identify the primary source of transfer error in vision&language navigation tasks in unseen environments. The paper is written in a way that very heavily emphasizes the 'performance gap' metric, which gets in the way of its otherwise interesting discussion diagnosing the source of overfitting and some 'strong' results on the tasks of interest. The second contribution is to use semantic information, compact statistics derived from (1) detected objects and (2) semantic segmentation, to replace the RGB image and provide input to the system in a way that maintains state-of-the-art performance but shrinks the performance gap between the seen and unseen data. In my experience, environment bias (or, more generally, dataset bias) usually implies that the training and test sets (or some subset of the data) are distinct in some way, that they are drawn from different distributions. The authors tease apart the contributions of the out-of-distribution severity of language instructions, navigation graph (environmental structure), and visual features, and conclude that visual differences are the primary form in which unseen environments are out of distribution.
I conjecture the paper would like to bring the evolution in genetics and perhap brain cirecuits as well to define a novel neural net model, called NNE by the authors. Therefore, the authors provide an alternative approach, named neural net evolution (NNE) that follows evolutionary theory. So I judge this paper in the perspective of machine learning, from which I think the current approach is a week variant of back-propagation that still relies on gradient (see detailed comments below). In this paper the authors propose a method for training neural networks using evolutionary methods.
They also use a human judgement dataset based on odd-one-out classification for triplets of inputs as comparison to evaluate whether the CNNs are able to capture the linguistic structure in the label categories as determined by the relation of the superordinate labels to the basic labels. The authors conduct a comparative study of several variants of CNNs trained on imagenent things category with different types of labeling schemes (direct, superordinate, word2vec embedding targets, etc.) This paper assesses the effects of training an image classifier with different label types: 1-hot coarse-grained labels (10 classes), 1-hot fine grained labels (30 labels which are all subcategories of the 10 coarse-grained categories), word vector representations of the 30 fine-grained labels.
SUMMARY OF REVIEW This paper discusses an interesting problem of BO in the cases of robustness and antifragility to aleatoric noise/uncertainty. Can the authors provide supporting evidence (in the form of references) that such a dataset is due to heteroscedastic aleatoric uncertainty? The y-axis in the experiment, the paper has considered the objective function value + noise. The paper utilised the existing heterogeneous Gaussian process to model the surrogate function and proposes two acquisition functions from the Expected improvement to deal with such heterogeneous noise.
This paper proposes a fractional graph convolutional networks for semi-supervised learning. The key approach of the proposed method is to apply a classification function (equation (3)) obtained by solving a GSSL problem to graph convolutional networks. The proposed method used a classification function of a fractional graph semi-supervised learning (GSSL) [De Nigris et al., 2017] as a graph filter. This paper presents a fractional generalized graph convolutional networks for semi-supervised learning. The paper provides a new model for semi-supervised node classification in directed and undirected graphs. Experimental results show that the proposed method (FGCN) shows the best accuracy compared to other recent graph-based neural networks for all datasets except one.
They introduce a method that learns a latent dynamics model exclusively from multi-step reward prediction, then use MPC to plan directly in the latent space. This is in contrast to existing work which generally use a combination of reward prediction and state reconstruction to learn the latent model. (2) given that the proposed method is a minor modification to the PlaNet paper, it seems that PlaNet should be included as a comparison (especially because it has been shown to work on high dimensional states), and Including sparse reward experiments would vastly help support the claims in the paper, as well as including the DeepMDP results for HalfCheetah and additional explanation of the difference in performance of SAC in Table 1 and Figure 3. Summary: This paper proposes a novel algorithm for planning on specific domains through latent reward prediction. To justify the proposed method, the authors provide a theoretical analysis and experimental results on specific RL domains, multi-pendulum and multi-cheetah, which contain irrelevant aspects of the state.
This paper performs a regret analysis for a new hierarchical reinforcement learning (HRL) algorithm that claims an exponential improvement over applying a naive RL approach to the same problem. My understanding is that the transitions in hMDPs work _like_ a clockwork (more on this in Mis6), the algorithm interacts with the sub-MDPs at each layer in turns according to their fixed horizons H_l's. This paper's contributions might be clearer if the authors made clearer assumptions, e.g. on action hierarchy, abstract state space structures etc..
The main idea follows the evidential deep learning work proposed in (Sensoy et al., 2018) extending it from the classification regime to the regression regime, by placing evidential priors over the Gaussian likelihood function and performing the type-II maximum likelihood estimation similar to the empirical Bayes method [1,2]. The approach starts from the standard modelling assuming iid samples from a Gaussian distribution with unknown mean and variances and places evidential priors (relying on the Dempster-Shafer Theory of Evidence [1] /subjective logic [2]) on those quantities to model uncertainty in a deterministic fashion, i.e. without relying on sampling as most previous approaches. Even though the presentation largely follows (Sensoy et al., 2018) and uses terms from theory of evidence, the derivation actually is more aligned with the prior network [3] under the Bayesian framework which is missing from the references. This paper proposed deep evidential regression, a method for training neural networks to not only estimate the output but also the associated evidence in support of that output. 2. The authors briefly mention that KL is not well defined between some NIG distributions (p.5) and propose a custom evidence regularizer, but there's very little insight given on how this connects to/departs from the ELBO approach. Predictive uncertainty estimation via prior networks. On the quantitative side, the baseline models considered in Section 4 are mainly concerned with epistemic uncertainty estimation. The authors demonstrated that the both the epistemic and aleatoric uncertainties could be estimated in one forward pass under the proposed framework without resorting to multiple passes and showed favorable uncertainty comparing to existing methods.
I much prefer this article's method, but for comparison purposes, the author(s) should similarly use a KNN classifier on their latent space to compute accuracy in the same manner. I like that the author(s) stick to 10 clusters for MNIST, but for comparison I would have liked to see a KNN generated accuracy alongside their equation 19 based accuracy. consider the exact generative process as the proposed by the author(s), but with the addition of a Dirichlet prior on the Categorical distribution. The author(s) appropriately cite VaDE (Jiang, 2017) and DEC (Xie, 2016), "Deep Unsupervised Clustering with Gaussian Mixture Variational Autoencoders" (Dilokthanakul, 2017).
I think using the embedding learned through CPC could provide meaningful semantics for representation learning and for reward shaping (as done in the current paper), and encourage the authors to continue down this line of inquiry. *Synopsis*: This paper proposes using the features learned through Contrastive Predictive Coding as a means for reward shaping. The paper presents a method to derive shaping rewards from a representation learnt with CPC. Moreover, the reward shaping method assumes we know the goal state but using CPC feature does not.
The authors provide a new universal approximation theorem on real-valued functions that doesn't require the assumption of a fixed cardinality of the input set. The paper aims to establish novel theoretical properties of known point-based architectures for deep learning, PointNet and DeepSets. This work examines the fundamental properties of two popular architectures -- PointNet and DeepSets -- for processing point clouds (and other unordered sets). To this end, the authors prove a series of theoretical results and establish limitations of these architectures for learning from point clouds. While the theorems proved in the paper are original and novel, they are a refinement of the already known results regarding approximation theorems for PointNet and DeepSets, respectively, hence only a marginal improvement in understanding these function classes. The presentation in this paper does remove the assumption of a fixed cardinality, but since this seems to be a mild assumption, it is not clear what is gained by this (beyond mathematical elegance). However, existing results on their approximation abilities are limited to fixed cardinalities. Only constant functions can be uniformly approximated by both (PointNet, Hausdorff metric) and (Deep sets, Wasserstein). This paper removes the cardinality limitation and gives two kinds of results: 1. The authors again note that this might vanish when a fixed cardinality of the input point cloud is assumed. - The paper shows two specific functions, where one can be approximated by PointNet but not by DeepSets and vice-versa. Deep sets) can approximate uniformly real-valued functions that are uniformly continuous with respect to the Hausdorff (resp.
This paper should be rejected because (1) the NCS datasets are too small to represent "numerical common sense" (2) the NCS datasets contain faulty data points and (3) the results from the experiments conducted are not sufficient to accept or refute the hypotheses. The work starts by describing the construction of interesting crowdsourced data sets that include people's estimates of typical quantities, what they would consider to be low or high values for a given object in given units e.g. the temperature of a hot spring or the height of a giraffe. This paper attempts to study if learned word embeddings for common objects contain information about "numerical common sense". The hypothesis is that if the regressors demonstrate good accuracy, then the word embeddings contained information relevant to "numerical common sense".
Summary - The paper first makes the observation that training algorithms and architectures for meta-learning have become increasingly specific to the few-shot set of tasks. In the end, the paper is essentially arguing that it's better to use different learning rates (for the inner loop) during meta-training and meta-testing. This paper proposes to apply MAML-style meta-learning to few-shot semantic segmentation in images.
The paper presents an empirical evaluation of many algorithmic choices made in the implementations of on-policy actor-critic algorithms in deep reinforcement learning (RL). ########################################################################## Summary: This paper conducted a large-scale empirical study that provides insights and practical recommendations for the training of on-policy deep actor-critic RL agents The study looks into a large choices of many implementation settings and design decisions, and investigate their impact on the task performance.
The experiments show advantages of BN initialisation, pointing to new directions of improving RBM training. The experiments show the advantages of training RBM with weights initialised from BN projection weights in generation and classification. This paper shows a relationship between the project rule weights of a Hopfield network (HN) and the interaction weights in a corresponding restricted Boltzmann machine (RBM). This mapping is shown to allow for significantly better initialization (than random) of the weights of a RBM, in the sense that the training is then much faster to reach comparable generative and/or generalization performance. The paper demonstrates a mathematical equivalence between Hopfield nets and RBMs, and it shows how this connection can be leveraged for better training of RBMs. What a great paper - well written, an enlightening mathematical connection between two well-known models that to my knowledge was not previously known.
This paper proposes a novel NAS method that searches the model architectures by grows the networks. The gold standard for comparison in both sparse-neural-network papers and NAS is to consider the accuracy at a range of different model costs.
The proposed method subsumes previous SBGM techniques of score matching with Langevin dynamics (SMLD aka NCSN) and denoising diffusion probabilistic modeling (DDPM) and shows how they correspond to different discretizations of Stochastic Differential Equations (SDEs).
Strengths: The paper provides a thorough overview of recent work towards training EBMs.The approach generates high quality image samples by combining EBMs and VAE based models. Experimental results show that the proposed method outperforms pure EBM defined on image space and also pure VAE models by large margins. This paper proposes a model that corrects VAE by an energy-based model defined on image space.
In summary, I believe this paper will foster more productive research by establishing the strong baseline on both decision-tree based method (although it has been known) and neural method (on which authors make good technical contributions). Concerns: Regarding the proposed solutions, the authors use data augmentation to improve neural LTR models. Pros: This paper discusses potential reasons why neural LTR models are worse than gradient boosted decision tree-based LTR models, and uses empirical results to show the effectiveness of the proposed solutions. The paper argues that neural models perform significantly worse than GBDT models on some learning to rank benchmarks. It would've been very interesting to see how these techniques improve the performance of previous neural LTR models; log1p transformation, data augmentation, and model ensembling would straightforwardly apply to other neural models as well. Then, it presents a few tweaks related to feature transformation and data augmentation to improve the performance of neural models. I think this paper establishes an unfair comparison between GBDT and neural-based models. Summary In this paper, the authors study the problem of neural LTR models. Comparing to traditional gradient boosted tree-based LTR models, is it really worth putting efforts into studying neural LTR models? When I was reviewing other LTR papers, I often had to point out that the proposed method significantly underperforms LightGBM.
The authors propose a novel Adversarial Sparse Convex Combination (ASCC) method in which they model the word substitution attack space as a convex hull and leverages a regularization term to enforce perturbation towards an actual substitution. Summary: In this paper, the authors aim to build a robust model against word substitution attacks.
Summary: This work focuses on the study of (global) convergence and gradient dynamics of a recently proposed family of models, the deep equilibrium  (linear) models (DELM) under common classes of loss functions.
---- Summary ---- The paper proposes a method for modifying an experience replay when learning in communication environments, by relabelling messages using the latest policy. Negatives: I feel like there's a pretty obvious question about "What happens in richer domains?" that (unless I missed it) isn't addressed in the paper - I'll expand on that in my 'Questions to clarify recommendation' section below. Updating the symbol would change it from old L to new R', but since the technique does not (and cannot, without a world model) update the environment actions, it seems like the listener and speaker would then train on this misaligned tuple of communication action R' and environment actions to move Left. The paper proposes a communication correction mechanism where, during the centralized training, messages there were received in the past from other agents are reevaluated according to the updated policy. Summary: This paper considers communication games when agents use experience replay. However, if the authors agree that the technique could fail to help or could harm convergence in richer settings than those presented in the paper to support the technique, then I think a couple of sentences about future challenges and future work are warranted.
They train a neural network consisting of a "core" and a "readout" in an end-to-end fashion to learn stimulus (visual inputs) -- response (single neuron activity) pairs. (iv) The authors report that transfered "core" representations work better than direct-training in their generalization experiments. There is closely related work in the literature, but this paper achieves very good performance, partly through a new way of accounting for neurons' receptive-field positions. "Fig 5 for the factorized readout …" (I didn't get it until reading it four times and looking for these results in Figure 5 twice.) The paper presents an experimental study on predicting the responses of mice V1 neurons with computational models. Though it needs more work to fully justify this claim, their demonstration that transferred representations seem to be more effective than direct training is surprising and interesting.
In my view, the submission's main contribution is a promise to publicly release inference time and power usage measurements and code for 5-6 different hardware devices on two existing NAS benchmark tasks: NASBench-201 and FBNet. The submission also provides some analyses on this data. The authors also study the rank correlation of the architectures from the  NAS-Bench-201 space regarding different hardware metrics (including both measured ones and theoretic ones like FLOPs) and find several pairs with low rank correlation. For this, the authors adopt two popular search spaces (NAS-Bench-201 and FBNet) and measure/estimate hardware performance metrics such as energy costs and latency for six hardware devices (spanning commercial edge devices, FPGA, and ASIC) for all architectures in this search spaces. In addition to the raw benchmark numbers, the submission presents some experiments / sanity checks on these benchmarks (Section 4): Table 2: Rank correlations between model FLOPS/Parameter counts and on-device latency/energy measurements. For example: two papers could both use the data from HW-NAS-Bench but produce incomparable results if searched for network architectures with different inference times. The network architectures that provide the search space are NAS-Bench-201 and FBNet, and the measurements/predictions are obtained from three categories of devices, i.e., commercial edge devices, FPGAs, and ASICs). The authors' datasets, which contain inference time measurements for several different hardware devices, should make it easier for researchers to experiment with NAS algorithms for finding better accuracy/inference time tradeoffs.
The paper presents a linear time and space attention mechanism based on random features to approximate the softmax. Empirical results demonstrate accuracy improvement over softmax attention, while preserving the linear time complexity. Extension of RFA with a gating mechanism appears to be effective, but I do not believe the claim that it is hard to apply this to softmax attention is valid, leaving the main contribution an exploration of a couple different kernels for linear attention. Proposes an extension of RFA with gating that improves accuracy on language modeling, relative to softmax attention. The experiments support the claims of improved accuracy via choice of kernel and gating, and preservation of speedups inherited from the linear attention formulation. Strengths The RFA-gated formulation is both more accurate and potentially faster (at least for decoding) than softmax attention, as demonstrated on language modeling. The approach is centered around a linear approximation of softmax attention, and is extended with a gating mechanism similar to a GRU.
Then, it also provides an instantiation of the lemma applicable to residual networks, an explicit compression analysis via pruning with a corresponding generalization bound, and empirical supports. Overall, I like the idea of bounding complex models by distilled models (simpler ones), and as authors point out, to make this approach work one needs to have a solid way to find such distilled models. The paper provides generalization bounds for seemingly complex neural networks on the basis of much simpler ones. 3)What are necessary/sufficient theoretical conditions for the effectiveness of distillation strategy (from the generalization error bounds)? The authors state that their work can be applied to  generalization bound of Arora et a.
This paper presents Gauge Equivariant Mesh CNNs. The method is motivated by the fact that graph convolutions can be modified for meshes to take into account the angular arrangement of local neighborhoods. The work presents a novel message passing GNN operator for meshes that is equivariant under gauge transformations. I think a toy experiment verifying it would be necessary, especially since equivariance of the non-linearity is only approximated (maybe by showing appropriate feature histograms after the non-linearity for changing reference points). The architecture is an elegant way to incorporate gauge symmetry on meshes and RegularNonlinearity addresses an important issue for equivariant neural nets. The experiments show the network is able to adapt to different mesh geometries and obtain very high accuracy in the shape correspondence task. have used local reference frames on meshes and point clouds to design equivariant point cloud networks: Zhao, Y., Birdal, T., Lenssen, J. However, in my opinion there are experiments missing: (1, crucial) verifying the gauge equivariance and (2) an additional comparison on a more complex task. It achieves that by parallel transporting features along edges and spanning a space of gauge equivariant kernels. While it is plausible and reasonable to model such data using vector features of type ρi in the hidden layers, the argument for the necessity of gauge equivariance would be even stronger if the input and/or output signal was itself vector valued, for example a velocity or gradient on the mesh. The result is a Mesh-CNN that is equivalent to GCNs with anisotropic gauge equivariant kernels.
The authors prove a theorem (thm 4.1) which describes a basis for the space of kernels in a G-steerable CNN for any compact group G. Though theorem 4.1 reduces construction of a basis of steerable kernels to 1) finding Clebsch-Gordon decomposition of tensor products, 2) describing endomorphisms of irreps, and 3) describing harmonic functions, none of these problems is trivial (or even necessarily solved) for a general compact group G. theory and so it is not necessary to use physics here to describe steerable CNN. Here, solving the constraint means to construct a basis of the space of steerable kernels. The theorem proved in this work unifies such previous efforts and provides a useful method for approaching further G. The paper under review is a very technical contribution to the study of group-equivariance of convolution kernels.
Summary To work with data that is not sampled on a grid, this work represents the activations of a neural network using a Gaussian process with RBF covariance. The proposed continuous convolutional layers can be directly applied to input Gaussian Process in a closed form, which subsequently outputs another GP with transformed mean and variance. Comments The paper references related work, makes a meaningful contribution, and I think the empirical methodology is sound. But in Section 4.7, they mention that their approach requires to evaluate Gaussian Process at a set of points which is similar to what was done in Li and Marlin(2016). Concerns: Given the time complexity of Gaussian processes, could the authors comment on the run-time of the proposed method as compared to standard CNN?
The authors studies an episodic MDP learning problem, where they propose to study an Optimistic Closure assumption which allows the Q function to be expressed as a generalized linear function plus a positive semi-definite quadratic form. The paper then proves that LSVI-UCB still enjoys sub-linear regret in the generalized linear setting with strictly weaker assumptions. The paper establishes that this is strictly more general the linear MDP assumption, where the above-discussed closure holds for backups of all functions (and not just linear Q functions). This paper analyses an existing algorithm (LSVI-UCB) with generalized linear function approximation instead of conventional linear function approximation.
The paper demonstrates the applicability of disentanglement promoting VAEs for achieving adversarial robustness and further enhancing such VAEs by considering their hierarchical counterparts. The paper considers the problem of training VAEs which are robust to adversarial attacks. Also, authors could consider decomposing the first term of \\delta for all the latent space dimensions and analyze if the disentangled dimensions are robust compared to the entangled dimension. --Summary: They proposed a robust method for the adversarial attack on VAE using a hierarchical version of β-TCVAE and conduct analysis on the relationship between disentanglement and robustness to support their choice of approach.
Based on these, the authors show that it is possible to upper bound the likelihood of reaching an unsafe state at every training step, thereby guaranteeing that all safety constraints are satisfied with high probability. The authors formally show that it is possible to upper bound the expected probability of failure during every policy update iteration (Thm1), which is a non-trivial result. A summarize of point 4 & 5: to me I don't feel the authors well-defined the `''safe exploration''  and give a rigorous analysis on the ``the safety guarantee. Another point that is worth clarifying is that the tests in Fig. 4-top number 1, we see that the performance cost reaches 10 for the proposed method with large safety constraint bound (0.2), which is intuitively an almost unconstrained case. during policy evaluation, the proposed algorithm uses the safety critic, Q_C(s,.), to estimate how unsafe a particular state is.
Questions and Clarifications: I believe that the related work section should add a clarification - [1], [2], [3] and [4] primarily deal with device placement, i.e., placing components of computation graph on different CPUs/GPUs to optimize run time via better parallelization. Summary: This paper proposes a new algorithm called EGRL to improve computation graph running time by optimizing placement of the graph's components on memory. The proposed approach builds on CERL, combining policy gradient, reinforcement learning, and graph neural network, achieving 28-78% speed-up over the native NNP-I compiler on vision and language benchmarks (ResNet-50, ResNet-101, and BERT). The paper proposes Evolutionary Graph Reinforcement Learning to solve the memory placement problem.
They then determined parameters for the modified ReLU that would minimize the deviation between these activation functions, and computed the minimum conversion error (for converting ANN -> SNN). I do not even understand if it is necessary to re-train the network to go back from the SNN to ANN or vice versa. Nice performance was obtained in all cases: better than using a normal ReLU, or other comparison activation functions, in the "target" ANN. I was therefore hoping to see an empirical study of the difference between the SNN and the ANN: do the activity of the spiking neural network match the activity of artificial network? Summary The authors suggest a relationship between a leaky relu and a spiking integrate and firing neuron model. Since I had not understood the basics of the paper, it was impossible for me to understand the later section about the conversion error. To achieve their goal, they described the spiking neuron non-linearity by a "staircase" function of the input (spiking output increases by 1 each time the input gets big enough to reach the next stair), and related that to the ReLu function used in the non-spiking neural net. The authors seek a mechanism to train a spiking neural net to duplicate the function of a non-spiking one.
While the paper's idea on applying optimal transport tools for training generative models seems interesting, the discussed theoretical and numerical results are not supportive enough to show that the proposed approach indeed improves upon WGANs. However, the paper's own formulation in (12) also leads to a non-convex optimization problem for which the authors show no convergence guarantees to a global solution.
It also presents two ways to leverage labeled outlier data if available, including an improved mahalanobis distance method and the application of supervised contrastive learning methods proposed recently. The way that the presented outlier detection methods utilizes the labeled outlier data may less effective than previous work, because the cluster-based anomaly scoring here is separated from the representation learning. the question this work intends to answer, "Can we design an effective outlier detector with access to only unlabeled data from training distribution?", is a classic and well-studied problem in the anomaly/outlier detection community. Overview: This paper proposes an outlier detection scheme based on contrastive self-supervised training for representation learning and cluster-conditioned detection using Mahalanobis distance. Some closely related methods are: self-supervised methods such as GT and E3Outlier that learns feature representations using a pre-text task in a self-supervised way; unsupervised outlier detection methods such as RDA, REPEN, ALOCC, OCGAN, etc.; methods that use a few labeled outlier data such as Deep SAD, DevNet, REPEN, etc. The key approach here is to apply existing self-supervised contrastive feature learning methods to extract feature representations and then apply a cluster-based method to calculate outlier scores.
Moreover, the paper proves that more non-smooth of the exponential power kernel leads to a larger RKHS with restriction on the sphere Sd−1 and the entire Rd. Furthermore, the authors conduct numerical experiments to verify the asymptotics of the Maclaurin coefficients of the Laplace kernel and NTKs kernel. This paper uses singularity analysis developed in the context of analytic combinatorics to study the relationship between the reproducing kernel Hilbert spaces of the NTK in a deep fully connected ReLU network, the Laplace kernel, and exponential power kernels. This paper proves that the reproducing kernel Hilbert spaces of a deep neural tangent kernel and the Laplace kernel have the same set of functions when they restricted to the sphere Sd−1, which improves the results established in Geifman et al., 2020.
This paper studies how to improve the worst-case subgroup error in overparameterized models using two simple post-hoc processing techniques: (1) learning a new linear classification layer of a network, or (2) learning new per-group threshold on the logits. I also have a few minor notes/suggestions: The findings presented in Figure 4 is interesting (mainly the fact that the overparametrization seem to be improving the worst-group performance with threshold tuning). Specifically, the paper demonstrates that this result is not necessarily due to overparameterized learning poor representations for rare subgroups, but rather mis-calibration in the classification layer that can be addressed with two simple correct techniques: thresholding and re-training the classification layer. Summary: The paper builds upon prior work that shows that overparameterized networks learned by ERM can have poor worst-case performance over pre-defined groups.
2. Diversity Objective If I understand the training objective correctly (Fig. 2b, Eq. 3), each model fi in the ensemble is encouraged to generate a random prediction when queried with out-of-distribution (OOD) inputs. Summary The paper proposes a defense against recent flavours of model stealing attacks by exploiting the insight that the recent effective attack query out of distribution examples to the victim model. Second, which also connecting to point 1, is that the paper's experimental setting assumes a significant discrepancy between the distributions e.g., Victim's Din = MNIST, Dout = KMNIST, Attacker's data = FashionMNIST. This paper proposed an Ensemble of Diverse Model to provide diversity prediction for the adversary's OOD query.
---- Decision ---- The paper proposes a relevant method for hierarchical video prediction with impressive high-resolution and long-horizon results. The model produces impressive high-resolution and long-horizon results, and is extensively evaluated on Kitti, Cityscapes, and dancing data, outperforming some previously proposed methods. This paper proposes a VAE based hierarchical model for video prediction.
This work proposes the first collective robustness certificate that considers the structure of the graph by modeling locality in order to derive stronger guarantees  that the predictions remain stable under perturbations. This paper addresses the limitation of the existing adversarial robustness certificates that ignores that a single shared input is present, and thus assumes an adversary can use different perturbed inputs to attack different predictions. A novel collective certificate fusing single certificates into a stronger one, is proposed by explicitly modeling local structure of input data using graph convolution node classifiers. ** Summary: In the context of structured prediction, where multiple predictions are simultaneously made based on a single input, this works argue that existing robustness certificates independently operating on each node prediction end up with overly pessimistic results. Summary Current methods on adversarial robustness certificates consider data points independently which are highly pessimistic for structured data. This paper proposes a collective certificate that computes the number of simultaneously certifiable nodes for which the predictions can be guaranteed to be stable (not change).
The paper discusses, at length, two previously proposed algorithms named Entropy-SGD and Replicated-SGD and demonstrates, using (i) controlled experiments where Belief Propagation (BP) can be used to estimate the local entropy integral precisely, and (ii) empirical results on deep networks that flatter minima generalize better. This paper studies local entropy measures for characterizing flat regions in the energy landscape of deep networks. The paper revisits the line of work that initiated this debate and shows that flatness, as measured by local entropy instead, indeed correlates with good generalization. The authors study two training procedures - entropy sgd and replicated sgd, and show that deep networks trained using these procedures are able to locate flatter solutions that also generalize better. While the paper presents some interesting empirical confirmation of the correlation between local entropy/energy and generalization, the algorithms presented in this paper have also been defined in previous work, and this phenomenon has been repeatedly observed with different definitions of flatness [1,2]. The paper studies (1) the relationship between the flatness of minima and their generalization properties, and (2) the connection between two measures of flatness, known as local entropy and local energy.
The authors of the paper propose a new method, the CORES (COnfidence REgularized Sample Sieve), to tackle the important problem of learning under instance dependent label noise. originality In this paper, the authors proposed a novel sample sieve approach for instance-dependent label noise learning. Using this loss function, the authors propose a dynamic sample sieve to separate the clean data and corrupted data on-the-fly, by the magnitude of CORES2 loss. The proposed method, in essence, involves the use of a confidence regularization term that encourages more confident predictions and a sieving process to remove the samples with large losses. Unfortunately, I think the theoretical analysis for the noise-robust loss is orthogonal to their sampling sieving approach. Specifically, in my opinion, the confidence regularizer is a marginal extension of the "peer loss" [1], and the sample sieve algorithm is essentially the same as that proposed in [2], the only difference being a different choice of loss function for training and sieving, to the best of my knowledge and understanding.
Summary This paper presents a new method for structure pruning called ChipNet. The ChipNet employs continuous Heaviside function with commonly used logistic curve and crispness loss to estimate sparsity masks. == Cons == Many previous works propose a new sparsity penalty function, and associated optimization, for neural network training. This submission proposes a way to prune neural networks using a continuous penalty function.
The paper under review introduces a number of geometric measures (isoperimetric, isocapacitory ratios that relate to Brownian motion or heat diffusion probabilities) that are applied to study neural network decision boundaries locally. While the Appendices provide valuable details, additional definitions and discussion, the main body of the paper lacks clarity, context, and sufficient organization to allow an average reader to follow, or even appreciate, some of the arguments and key contributions.
Summary: This paper introduces DARC, an RL approach that aims to transfer from a source environment to a target environment with different dynamics. Theorem 4.1 provides a theoretical guarantee on the performance of a policy trained on such a modified reward in the source domain by giving a bound on the performance in the target domain, under a very mild assumption that the optimal policy on the target domain achieves similar rewards when put in the source domain. Summary This paper proposes a method for domain adaptation in RL where the source and target domains differ only in the transition distriubtions. The assumption that the rewards for the target and source domains match is suitable for the choice of environments in Section 6 but seems quite limiting to a more general setting where certain rewards may never be observed in the source domain. DARC utilizes an adversarial-like reward that distinguishes between source and target domains. The idea is to modify the reward function in the source domain so the learned policy can be optimal in the target domain.
Based on their findings, they propose a new supervised pretraining method, which has a good trade-off for transfer learning applications, and validates with other contexts such as few-shot classification and landmark localization. Summary: The paper addresses the important topic of understanding why self-supervised learning methods show very good performance when used as pretraining for fine-tuning tasks. Authors analyse in detail the difference in performance between self-supervised and supervised pretraining and propose a new method to train model which improves over standard supervised models when used as pretraining.
They propose two energy functionals (first one based on maximizing the norm of the parameters and the second one making the adaptation in closed form, based on the NTK) that approximate the MAML's infeasible learning objective. -Meta-RKHS-II Here, the authors propose an adaptation function based on the NTK and the gradient flow. I am particularly impressed by the second algorithm, Meta-RKHS-II, which derives a closed form-solution to gradient-based adaptation in RKHS that they then map back into parameter space via NTK. The connection to the NTK seems a bit weak and superficial — Based on eq (3), the authors propose the energy functional in eq(4) for learning the meta-parameter and this is where the RKHS comes in. Summary In the attempt to create an adaptation-free meta-learning method, authors construct an RKHS based on the NTK and explain how to do (gradient-based/MAML-style) meta-learning in this space (instead of parameter space). Authors show theoretical arguments confirming their method is closely related to MAML as well as the results of extensive experiments in few-shot classification, regression and out-of-distribution generalization tasks. For the second algorithm (i.e. Meta-RKHS-2), the authors claim that it can have a closed-form adaptation function.
Paper summary The authors empirically investigate the calibration performance of NN-GPs in CIFAR10 and several UCI data sets, in three forms: Bayesian inference for the NN-GP function-space prior, through a softmax link function In particular, through a series of experiments, the paper investigates their uncertainty properties and answers the question "how calibrated are the predictive uncertainties for in-distribution/out-of-distribution inputs?": i) a comparison between GP classification with the infinite-width neural network kernels and finite width neural network classification was provided to test the calibration, We know that the superior performance of the kernel stops being true at a certain point for classification-as-regression, and I see no reason to think it would be different for "proper" Bayesian inference. It could be of high significance for a practitioner like myself interested in added methods improving the calibration of an existing model, but the authors set aside computational complexity issues and do not give an implementation. Next the authors "describe" their NNGP and try an impressive number of architectures over several datasets, however, assembling the paper seems to have been an hasty business where many clarity issues have been left unresolved, and these are severe, especially for reader such as myself who are not an expert in GP. The stated goal of this paper is very ambitious: how NNGPs provide better confidence prediction, in terms of calibration, OOD data  and distributional shift. Perhaps the good calibration and superior performance of the kernel will stop being true once we go to 18 or 20 layers, or once we add mean-pooling to the CNN. Update: the authors have greatly improved the figures and tables, and expanded the captions, removing my major concern about clarity.
The paper also reports two outlier data points in the experiments where the learning-based dynamics model leads to significantly higher localization errors. The probabilistic graphical model considers the observations, dynamics and latent states of the agent (i.e. the pose and dense map). Currently, the related work is too focussed on traditional SLAM approaches (like VinsMono) and 2D/2.5D methods like VAST/DVBF-LM. The approach in [1] formulates the fully differentiable dense SLAM problem (including ray casting) on real world datasets, [2] is a recent novel 3D rendering approach and also does differentiable ray casting,  [3] demonstrates an end to end approach to learning measurement likelihood models with an RL active localization framework, and [4] which is also uses a clever combination of deep learning and multi-view geometry to produce dense 3D maps. The learning part of the paper is to use neural networks to model the dynamics: the distribution of the next agent state given the current state and the IMU is modeled as a normal distribution whose mean and standard deviation are determined via a neural network that takes the current state and the IMU as the input. Specifically, the paper seems to marginalise most of the recent methods that try to combine deep learning with dense SLAM.
The authors introduce Contextual Transformation Networks (CTNs), a replay-based method for continual learning based on a dual-memory design and a controller that modulates the output of a shared based network to task-specific features. This paper tackles online continual neural network learning (following Lopez-Paz & Ranzato, 2017) with a combination of techniques: (1) a controller (or base parameter modulator, or hypernetwork) is introduced which produces task-specific scale and shift parameters, which modulate the feature maps of a base model (Perez et al., 2017); Did the authors try some other continual learning algorithms to avoid forgetting in the base network other than "behavioral cloning"? In particular, the authors introduce a dual memory framework that contains an episodic memory for base networks and semantic memory for task controllers. Summary of paper This paper introduces a continual learning method called Contextual Transformation Networks (CTNs). Cons of paper There are related works that I think the authors can mention, which have some similar ideas as in CTN (although all the ideas are never all put together as in CTN): (a) FiLM layers for meta-learning / continual learning / multi-task learning: [1] Requeima et al., 2019, "Fast and Flexible Multi-Task Classification using Conditional Neural Adaptive Processes" This paper proposes CTN (Contextual Trasnformer Networks) for online continual learning. (2) as training data set samples are only provided once, the authors maintain experience replay buffers using soft label targets (by now standard techniques in online learning);
The method employs an adversarial imitation learning objective function that incorporates proposed mutual information constraints that are intended to force the representation space to be invariant to the domain of the data sources, and instead only encode goal-completion information. Suggestions to improve the paper add an additional baseline "DisentanGAIL w/ domain confusion loss & prior data", as discussed in the "weaknesses" section, particularly for the transfer tasks on the bottom right of Fig 2 in which the discrepancy between DisentanGAIL and the baselines is the largest add evaluation of baselines (particularly DisentanGAIL w/ domain confusion loss (w/ and w/o prior data)) to the harder manipulation environments in Fig 3 to show the benefits of the introduced regularizations since the proposed method addresses a concrete problem of prior work (as explained in appendix Section A) with a clear intuition, it could be nice to add a toy experiment early in the paper that demonstrates this effect empirically for an easy-to-analyze imitation problem, showing that for MI=0 the agent cannot properly learn to imitate since it is unable to capture the relevant information --> since this is a different assumption from prior work on cross-domain imitation it would be good to mention this earlier, maybe in a dedicated "Problem Statement" section for qualitative matching results in Fig 4 in the appendix it would be nice to show the corresponding matches found when using the domain confusion loss instead of the proposed regularizations to see whether some of the failure cases are interpretable Questions from my understanding the issue of prior work that constrains the domain-related MI to 0 (described in Appendix, section A) appears for any two domains (since, particularly in the beginning of training, there will always be differences in the goal-reaching distributions of expert and policy data). This paper proposes a visual imitation learning algorithm that can handle domain shifts between the expert demonstrations and the data generated by the agent. Empirically, the paper shows that the proposed regularizations can improve performance across a wide range of third-person visual imitation tasks, transferring between simulated agents with different appearances and/or morphologies. The main difference over prior work (eg Stadie et al, 2017) that used domain confusion objectives is the introduction of new regularization objectives on the learned representation of the visual scene. Experiments illustrate that the method was able to learn and be performant despite visual and embodiment differences in the expert and agent domain on various mujoco environments, exhibiting substantially better performance to a different observational IL method. Post-rebuttal updates: After reading the author response and other reviews, I agree that the difference between the paper and prior works is now much more clear and the empirical evidence shows that the new method works well, though I'm still concerned about the part where the authors add many components and make the algorithm much more complex and potentially hard to work in practice. For pros, I think the experimental setup is reasonable and the authors conduct relatively thorough ablation studies to show the usefulness of various components such as the prior data constraint, double statistics network, and spectral normalization regularization, which is helpful for understanding the method. Overall Recommendation The paper is well written and the experiments are covering a wide array of third-person visual imitation problems on which the proposed method shows strong results. --> such an experiment could help to further motivate the need for the new regularizations the additional assumption of a "prior dataset" collected in both domains for additionally constraining the latent representation is first mentioned in section 4.2 They introduce DisentanGAIL to tackle this problem by introducing two mutual information constraints in the GAIL framework to learn domain-invariant representations of observations.
Summary: In this paper, the authors propose a multimodal transformer network for audio-visual video representation learning. This paper studies modeling and training choices when designing a single model based on ConvNets and transformers for audio-visual representation learning. Concerns The authors imply that using a partially fixed model (as has been done with multimodal vision/language tasks) is inferior to end-to-end training, hence the motivation for the proposed parameter-reducing technique, which is the major technical contribution of this work, as presented by the authors. Summary In this work, the authors present a method for learning audiovisual (AV) representations from videos using a Transformer-based model architecture. Empirical results on both audio and video understanding tasks demonstrate that the proposed method does indeed learn useful representations, and that multimodal training provides the expected boost to results. It builds on a line of research on multi-modal video understanding that utilises transformers where these works: 1) fix one of the transformer models (e.g. BERT) and 2) utilise tokens and thus do not train the approach in an end-to-end fashion.
Summary: The paper presents HICTL which enables models to learn sentence level representations and uses contrastive learning to force better language agnostic representations for large multilingual encoders. To take care of these gaps, the authors propose using HCTL as an approach that can learn more universal representations for sentences across different languages. At the end of reading this paper, I am not sure if implementing what the authors proposed, versus other variations of existing models, would have given the same improvements: While these improvements can be seen across many data sets, they are often modest. The paper proposes a pre-trained language model variant which extends XLM-R (multilingual masked model) with two new objectives. Contrastive losses are promising and the paper shows positive results when adding them to the previously proposed XLM-R model.
The paper includes ablation experiments that clearly show that current works that use the body morphology structure to constrain the graph structure of graph neural network based approaches do not actually improve the performance. This paper instead proposes to use Transformers as a simpler mechanism to be able to train and discover the helpful morphological distinctions between agents in order to better solve multitask reinforcement learning problems. While I do agree that training a graphical neural network to be able to produce a quality policy for a number of control tasks from the opening item environment is difficult the author of the paper might be missing at least one of the key points from the previous work in that you can learn a stronger modularization of policy. The author claims that the SMP paper does not work better due to the morphology encoding and then they point out that it instead works because of the encoding of the subtrees and some specific detail related to message passing. By use of the Shared Modular Policies and the NerveNet methods, authors find that restricting morphology information does not improve performances. This paper proposes that recent methods that used graphical neural networks to help solve the multitask reinforcement learning problem and assume that there's an advantage from being able to encode the agent's morphology using a graphical neural network do not provide additional generalization and benefits for learning. The paper instead forgoes trying to input the body structure and uses a transformer based architecture that is capable of learning the appropriate (even dynamic) graph structure actually useful for control. Instead, they claim that the benefits from being able to encode this morphology are counteracted by the difficulty in having to train the graphical neural network using the message passing system. This could be correct the explanation but the paper so far hasn't gone into enough detail for the reader to understand the importance of this message passing and how it works and how it is not improving training for GNNs.
Methodologically, the paper stays close to the ideas of RLSP, but instead of computing relevant quantities (optimal policy, forward and inverse dynamics) through explicit derivations, it suggests most exact steps can be replaced by leveraging deep learning, reinforcement learning and self-supervised learning. (4) Based on an educated guess of the problem setting (per my understanding), an intuitive and perhaps simpler algorithm would be to learn a goal classifier using the provided {s_0} data, i.e. P(s=goal), as well as a forward dynamics model P(st+1|st,at). This paper introduces an algorithm, called deep reward learning by simulating the past (deep RLSP), that seeks to infer a reward function by looking at states in demonstration data. To achieve this, the paper assumes a Boltzmann distribution on the demonstration policy and a reward function that is linear in some pre-trained state features. While the paper does not have strong methodological novelty, it is well written, the approach is sensible and combines well with state of the art deep RL, and the results are certainly interesting. Worryingly, appendix D states that learning a dynamics model was attempted by the authors but failed to yield good results. I think environments more aligned with the eventual application areas for a method such as Deep RLSP would make the paper much more compelling. I find the idea of the paper very interesting and the results showing meaningful behavior emerge from a single demonstration are quite nice. [1] Reward Learning by Simulating the Past (RLSP) This paper studies the question of learning rewards given only certain preferred or terminal states.
Pros: The authors combine the idea of differentiable indexing in Spatial Transformer (Jaderberg et al., 2015) into the memory of Kanerva Machine (Wu et al, 2018a;b) and prove by experiments that this allocation scheme on the memory helps improve the test negative likelihood. In contrast to the Kanerva Machine, the authors simplify the process of memory writing by treating it as a fully feed forward deterministic process, relying on the stochasticity of the read key distribution to distribute information within the memory. Specifically, this paper proposed a novel memory allocation scheme,  replacing the stochastic memory writing process in prior works KM[1]  and  DKM[2] with a set of deterministic operations. This paper proposes a new memory mechanism based on the Kanerva Machine inspired by computer heap allocation. The machine stores the knowledge more efficiently through the sharable part-based system and the authors simplified the writing mechanism by designing the memory deterministic. The paper proposes a generative memory (K++) that takes inspiration from Kanerva Machine and heap memory allocation. Given the memory and the keys,  a Spatial Transformer reads multiple contiguous memory blocks, which are then used to generate samples using a transposed convolutional decoder.
This paper proposes a novel framework called VA-RED2 to reduce spatial and temporal features to be computed for video understanding, which can reduce FLOPs when inferencing the video but remains the performance. The authors have done extensive experiments on video action recognition tasks and spatio-temporal action localization task in the area of video understanding. For the video action recognition task, experiments are carried out using Mini-Kinetics-200, Kinetics-400, and Moments-In-time datasets. Summary The paper presents a framework to reduce internal redundancy in the video recognition model. Directional Temporal Modeling for Action Recognition, ECCV2020 This submission has done experiments on both action recognition and action localization task.
-> "If local signals are not representative of the global goal" The paper proposes a method for decoupled training of neural networks called SEDONA. The approach achieves better performance than using backprop on small datasets (CIFAR10 and TinyImageNet), and comparable or slightly improved performance on ImageNet with 2x claimed training speedup. Summary This paper proposes a differentiable architecture search approach for splitting a deep network into locally-trained blocks to achieve training speedup. Weak points W1: This seems like a pretty complicated approach to only get 2x speedup in training, and besides slight improvement in performance, that seems like the only benefit this method achieves for realistically large datasets like ImageNet. W2: "Speedup" is not really defined. It seems that the decoupled neural network scheme has nearly no gains in terms of Top-1 without the auxiliary heads ensembled as shown in Table 3. S3: On ImageNet, the method achieves slightly improved performance with claimed 2x training speedup.
This paper proposes a teacher-student training scheme to incorporate the useful information of trajectory to improve the predictive performance of model-free methods. It uses a teacher model to learn to interpret a trajectory of the dynamic system, and distills target activations for a student model to learn to predict the system label based only on the current observation. Typos: using only using model-free methods This paper presents a student-teacher framework, where the teacher network can be used to select and prioritize the relevant properties of the given dynamical system that should be learned by the student. Summary: This work tries to find a compromise of model-based and model-free methods, using a teacher and student network . The proposed model is interesting and may lead to a series of follow-up studies that leverage the strengths of both model-free and model-based methods using knowledge distillation techniques. Though the authors have talked about why not to compare to a model-based method, I do not think it is convincing. It interprets the trajectories and provides activations for a student network that is supervised for a given task using the current state. From the current title "Learning to interpret trajectories", and the abstract, it was not really clear to me what the paper is about; I am not sure if the population of people who stop at the title would be the same as the population that finds the contents most interesting. The ideas of model-based and model-free methods are reinforcement learning concepts, and may not be clear to people who are not in RL.
The authors use a general construction for proving universality of equivariant networks for the point cloud group, rather than being specific to certain architectures. Recommendation: The authors proof the useful statement of universality of a prominent class of neural networks, which is why I recommend the acceptance of this paper. The authors accomplish this by writing the network as a composition of an equivariant function from a class F_feat and followed by a linear pooling layer. This work is a theoretical paper investigating the sufficient conditions for an equivariant structure to have the universal approximation property. pros: provide sufficient conditions for equivariance of shape-preserving architectures to satisfy the universal approximation property prove two methods based of Tensor Field Networks that satisfy the universal approximation property raise two simple models based on the TFN
This paper presents a new CNN module to learn video feature representations for action recognition, with a particular focus on increasing channel interactions for spatio-temporal modeling. The paper proposes a novel Channel Tensorized Module (CT-Module) to construct an efficient tensor separable convolution and learn the discriminative video representation. To achieve that, the authors propose to divide feature channels into several sub-dimensions (called channel tensorization) and then perform group convolutions at each sub-dimension sequentially to improve channel interactions. Summary The paper proposes a new architecture for lightweight action classification networks, named Channel Tensorization Network (CT-Net). My understanding is that the main idea of the paper essentially performs channel shuffling followed by group convolutions at each sub-dimension. Strengths: The paper's novelty is the first method for exploring the spatial/temporal tensor separable convolution along each sub-dimension. Paper strengths I think the ideas proposed in this paper are interesting and I would not be aware of any architecture that would use a similar arrangement of tensorization and attention in the mid-layer part to allow for a lightweight 3D convolution architecture.
Inspired by the observations of feedforward inhibition in the brain, the authors propose a novel ANN architecture that respects Dale's rule (DANN). 2. The ingredients in the proposed model is well motivated in neuroscience, such as the feedforward inhibition, and E/I balance, as well as no connections between I neurons across the different layers. Although, I find the contribution interesting, my enthusiasm is tempered by the following two issues: Although feedforward inhibition has its place in the brain, most connections of inhibitory interneurons with excitatory neurons are reciprocal, resulting in feedback inhibition. Summary: It is shown that Dale's principle can be observed in feedfoward ANNs if one uses inhibitory neurons in the form of feedforward inhibition, while the other neurons are purely excitatory. Pros:1.To my knowledge, this is the first E/I network that could achieve comparable performance with the standard ANN model on MNIST task (although at the same time, I have to say that not too many papers have studied and reported this issue). As a result, they empirically demonstrate that DANNs perform no worse than the ANNs that do not respect Dale's rule.
In particular, it focuses on the expected loss reduction (ELR) strategy, analyzes its problem, and modifies the original ELR method to make sure the active learner converges to the optimal classifier along learning iterations. Additional questions and suggestions: I think the paper would be improved if there is a discussion on how the proposed method can be extended to deal with high-dimensional data and/or using deep learning models. To achieve long run convergence of MOCU to zero (and thus get obtain the optimal classifier), the authors propose a so-called weighted strategy that solve this issue. Summary: This paper provides an interesting algorithm to address the previous Bayesian active learning query strategy in (binary) classification. The authors of this paper introduced a new acquisition function of active learning for optimal Bayesian classifier. In the synthetic experiment, it should be possible to simulate a case with ground truth optimal classifier and verify whether the proposed method actually converges to the optimal.
As mentioned above, previous methods had already proposed the usage of graph neural networks with continuous time for the learning of differential equations, and I am not sure that the addition of spatial mesh information to such a graph neural network constitutes a significant enough modification at this point. The paper proposes to use graph-based networks for evaluations of PDEs with continuous time formulations. In contrast to existing works on continuous time ODE formulations with graph structures, the proposed networks incorporate relative spatial information in order for the network to evaluate spatial derivatives in addition to the temporal dynamics. Cons Similar graph-based methods that used continuous-time to model differential equation dynamics had been previously presented, e.g. GNODE. I am not an expert on the experimental side of this area, but my sense was that the performance of the approach is relatively good, especially compared to other methods when the time step is large. For small time steps, however, PDE-Net is multiple orders of magnitude better than the present approach, a fact that should have been thoroughly discussed in the text rather than just mentioned, because the discrepancy is so large.
--The visualization of the key indicators may be helpful for understanding how batch normalization works and how to remove batch normalization.--The experiment results seem that the proposed method achieves good performance in large-scale ResNets. Based on the investigation, the authors first provide ResNet results without normalization with the proposed scaled weight standardization. The proposed method is not yet a drop in replacement for BatchNorm in general, but it can be useful in specific circumstances, i.e. small batch-size training. Cons) The authors seem to have failed to provide any reasons for needing normalization free ResNet over the original BN-ResNet. It is not clear that the trained model without NF with SWS can be used as a backbone that can be directly applied to downstream tasks (e.g., object detection). Can the proposed method decrease the batch size to a small value like 2/4/8 after removing batch normalization? With SPPs in hand, they find two key failure modes at initialization in deep ResNets and then develop Normalizer-Free ResNets using Scaled Weight Standardization, achieving competitive performance on ResNet-288 and Efficient-Net. Strength: --The overall idea makes sense and the proposed method of removing batch normalization can reduce computational resources and speed up computing greatly.
Compared with Malach 2020 "Proving the lottery ticket hypothesis: pruning is all you need", the authors can highlight what key differences are needed for proof of the binary network. The paper has many simulation results to support the theoretical guarantees, and the proposed approach on binary-weight networks has advantages over existing methods. One main concern of the reviewer is the similarity between the paper's approach---training a mask over a binary network---and the conventional ternary network. This paper propose utilizing the existing "lottery ticket" result for constructing binary neural networks.
Summary The authors present the idea of adaptive stochastic search as a building block for neural networks, as an alternative to other "inner loop" optimization methods like gradient descent. Cons There are some cases (although not in general) of meta-learning for adaptation, for example where the update can be described in an implicit fixed point way or when the method used in FirstOrderMAML , which allows not needing to unroll (when backpropagating) the function to be optimized. Their suggested NOVAS approach together with the deep FBSDE method allows for solving optimal control problems efficiently and with low memory due to not unrolling multiple optimizer steps during the backpropagation. They propose to perform the estimation of the gradient of the optimum of the embedded problem with respect to its parameterization by the differentiation of one step of a stochastic search algorithm. Pros : Using adaptive stochastic search allows the inner optimization module to take multiple iterations without unrolling the computation graph (unlike meta-learning methods), since the initial value used by the module is arbitrary, and not provided by the network. Additionally, the authors suggest a stochastic search method that due to using the log function trick has low sample efficiency compared to using actual gradient information. This paper proposes using adaptive stochastic search as an optimization module within deep neural networks to perform general non-convex optimization. The authors empirically show a 5x speed-up in using their approach compared to unrolled differentiable cross entropy and better qualitative performance than unrolled gradient descent for training an energy function for a simple regression task. It seems problematic that the comparison against the backpropagation through unrolled gradient descent is with an example where the latter finds a different (wrong) local optimum then the suggested NOVAS method.
Pros: interesting observation regarding the impact of the initial scale on generalization clearly show the effect in a two-layers MLP with various activation and loss functions propose an alignment measure which have some promising correlation with generalization Besides "toy" 2-layer model, the similar results are get with more powerful architectures, like CNN, DenseNet and so on on the set of middle-sized datasets like SVHN and CIFAR.The paper clearly states its place among related works and proposes a useful metric for diagnosing model training. The paper also provides a hypothesis why does it happen -- that the graidents of difference examples are orthogonal in the "bad" mode and proposes a measure called "alignment" to predict the generalization reghime of the network. Because the problem with large init is the bad gradient direction (as hypothesized in paper), then scaling the learning rate would not help. - maybe not necessary to comment on in the paper, but if the authors are familiar with this use of the term I'd appreciate a clarification of how it relates to the mentioned measures of alignment I'd be interested Overview The paper studies how the generalization of the neural network trained with SGD is affected by the scale of the random initialization. It then proposes an alignment measure which correlates with generalization for different initial scale.
This paper proposes ANT to solve the problem of learning Sparse embeddings instead of dense counterparts for tasks like Text Classification, Language Modeling and Recommendation Systems. It also proposes a probabilistic interpretation of their method as a non-parametric Bayesian dictionary learning model, which can be inferred by optimizing the small-variance asymptotic objective. What I agree with the authors are: i) Using properly chosen basis vectors may greatly reduce the memory cost for embeddings, especially for huge vocabulary sizes (e.g. over 100 million). In this paper, the authors proposed a method to learn efficient representations of discrete tokens. In step 2, they learn a sparse matrix that is used to relate all tokens to the set of chosen anchors. Hence the paper proposes to only store a few anchor/latent vectors (the matrix is A with |A|<<|V|). They took a two step approach: in step 1, they learn "full fledged" embeddings for a subset of anchor tokens. In the experiment section, the authors showed that their approach has good performance on several language tasks, with far fewer parameters. If you want a point estimation of sparse representation + learnable anchors, you don't need a Bayesian model.
Among others, the authors also consider early-stopping in a non-parametric RKHS setup, showing that an appropriate interpolation between NGD and GD achieves optimal rates with a much smaller number of steps compared to GD, a difference which becomes larger for "difficult" problems (which require more weight on the Fisher preconditioner). A5) For parametric least squares with a mis-aligned ground truth parameter, it is shown that early stopping with NGD achieves lower Bias than any other pre-conditioned gradient descent (Proposition 6). Summary: The authors theoretically study the prediction performance of pre-conditioned gradient descent/flow with linear models and squared loss aligning in the setting of least squares regression and non-parametric regression. The authors first consider the "ridgeless" regression setup in high-dimension, where the estimator corresponds to the limiting gradient flow iterate, and show that NGD leads to a smaller (and optimal) variance term, and can improve the bias term compared to GD particularly in the presence of strong misspecification. Specifically, gradient descent pre-conditioned with the Fisher information matrix achieves better generalisation performance when the noise is large or the model is misaligned (Section 5).
Weaknesses Neither the theoretical analysis (comparison of upper bounds) nor the experiments (comparisons for some specific biased and unbiased compressors) in this paper convincingly show that the proposed approach will always be theoretically and practically better than Error Feedback. Specifically, the authors proposed a technique that transforms any biased compressor (e.g., Top-k) into an unbiased one, referred to as induced compressor, with better convergence guarantees and properties than the former. The main contribution of the paper is the development of an induced compressor that takes as input, a possibly biased compressor, and outputs an unbiased compressor with similar error variance as compared with the original compressor. -->The paper has solid theoretical reasoning via a convergence analysis that does demonstrate that for certain (common) parameter choices the induced compressor is expected to have better convergence than the biased counter part
Using DDEs is a novel technique in machine learning and can complement and build on the framework of Neural ODEs and help model systems with time delay dependencies and overcome some limitations of ODEs. This can help in the modeling of systems with a time delay effect, and overcome many limitations of the Neural ODE framework. This work considers Delay Differential Equations instead of ODE, which allows to implement more complex dynamics and thus achieve estimation of more complex functions. This new family of deep neural networks (NODE) generalize ideas from Residual Networks and consider continuous dynamics of hidden units using an Ordinary Differential Equation (ODE) specified by a neural network. This paper presents a modeling class of parameterized first-order differential equations that are conditional on some delayed past states.
The paper proposes a new Information Bottleneck objective, which compresses the latent by learning to drop features similar to DropOut. Unlike DropOut, a different probability is learnt for each latent feature/dimension using Concrete Relaxation. Summary This paper proposes the Drop-Bottleneck (DB) method that performs feature selection during the training with the mutual information. While the approach is limited to dropping input features, which does not make it a general IB objective, it seems to work very well in the presented RL experiments as well as show robustness that is better than DVIB's. Experiments show that DB works better than VIB in VizDoom and DMLab when a noisy-TV noise is added to the input images. Originality and significance aspect This paper combines mainly two ideas 1) classic feature selection (choose Xis to drop) with respect to the mutual information (between X and Y) 2) Information Bottleneck (IB) formulation that maximizes the prediction-term mutual information term (between Z and Y) and minimizes the compression information (between X and Z) simultaneously. Summary: The paper contributes a novel method, Drop-Bottleneck (DB), for discretely dropping input features that are irrelevant for predicting the target variable. Summary: This paper proposes an information bottleneck method, Drop-Bottleneck, that allows the input to be compressed by dropping each input feature with probability p_i. The paper does not discuss connections of the presented approach to prior works for (discrete) feature selection. Key idea is to instantiate the compression term of the information bottleneck framework with learned term that sets irrelevant feature dimensions to 0.
Using comprehensive experiments on synthetic datasets and real-world datasets, the authors verify that the proposed method can improve the robustness of the classifiers against noisy labels. ------Overall------ This paper utilizes the memorization effects of deep models and aims to improve their robustness to noisy labels before early stopping. This paper tackles the problem of learning with noisy labels and proposes a novel method CDR which is inspired by the lottery ticket hypothesis. The proposed method implicitly exploits the memorization effects of deep models, and can reduce the side effect of noisy labels before early stopping. As the deep models fit training data with clean labels in the early stage of training, the authors propose a novel method to identify those more important parameters for fitting clean labels. The authors also perform an ablation study to present the proposed method is insensitive to the estimation of noise rate. This paper proposes a method for deep learning with noisy labels, which distinguishes the critical parameters and non-critical parameters for fitting clean labels and updates them by different rules.
I think the main contribution of this paper is their intuition that performing neural architecture search rapidly from the datasets, while the components are all proposed before in different NAS scenarios. The paper proposes a framework to generate good architectures according to the datasets. In particular, a framework, MetaD2A, is proposed, which yields a neural architecture for a new dataset. The experiment shows this method can fast adapt NAS from one image dataset to others and achieve SOTA performance. There are mainly three components in MetaD2A: a set encoder, a graph decoder and a meta-performance predictor. Pros: This paper proposes new scene, where a meta model is pre-trained on some datasets, and transfer the learned meta feature onto other datasets to do fast adoption. Overall Review: This paper proposes a new scene of fast adaption of NAS, which may be a good direction of NAS & meta-learning. No comparing to other methods on fast adaptation by NAS such as [2].
To start with, the authors clearly demonstrate the value of both Bayesian context aggregation and a MC based likelihood approximation scheme on precisely the same types of problems that existing neural processes papers (e.g., Garnelo et al., 2018) have considered (with the notable exception that the 2D image completion task considers only MNIST as a target dataset). The paper builds upon previous lines of research on multi-task learning problem, such as conditional latent variable models including the Neural Process. To summarize, in the context of neural processes I feel the paper makes good methodological contributions in presenting a much cleaner and more natural (from a Bayesian perspective) version of the model that has more of the flavor of standard amortized inference for latent variable models. Rather, it just seems surprising to me that the discussion of the relevant probabilistic regression literature ends at "well, it exists." What I would like to see is a discussion of where the authors' impressive improvements to neural processes leave the model family in this broader context. The authors present the Bayesian Aggregation (BA) mechanism in the context of Neural Processes (NPs) for aggregating the context information into the latent variable z in the form of posterior updates to z. The authors show that this improves predictive performance (in terms of likelihood) compared to mean aggregation MA that it replaces on various regression tasks with varying input-output dimensionality.
Summary The paper points out a limitation of the implicit option version of the Variational Intrinsic Control (VIC) [Gregor et al., 2016] algorithm in the form of a bias in stochastic environments. The differences between the proposed algorithm and VIC shows that the intrinsic reward now has an added term which depends on an approximate model of the transition probability distribution. Strengths The paper provides a sound theoretical analysis of the limitation of the VIC implicit-option algorithm, the proposed fix and a practical algorithm (Algorithm 2). Experiments on simple discrete state environments demonstrate that the original VIC algorithm works well only on deterministic environments whereas the proposed fix works well on the stochastic environments as well. They point out that, in stochastic environments, the implicit VIC formulation in the original paper is missing a term in the mutual information (involving log likelihood ratios of state transitions). The new experiments and visualizations have been helpful (I am happy with the author's responses to R3), but the overall clarity of the paper is still lacking due to the dense mathematical notation. 5-->6 The authors show that implicit VIC is biased in stochastic environment due to its blindness to the effect of its 'option' on the state transition dynamics. For this paper's extension to be truly significant, it should show a case with significant state cardinality (e.g. the 3D environment from the VIC paper) where the bias hurts more than the reliance on the learned model. Considering the experiments on partially observed environments presented in the VIC paper, this paper chooses a much simpler set of discrete environments for empirical analysis instead of stepping up to more complicated environments which would have strengthened both the motivation for fixing the bias of VIC and the empirical evidence of the GMM algorithm (Algorithm Experiments It would be great to include visualizations or ablation experiments to illustrate why implicit VIC has a lower empowerment than the two proposed methods.
Summary Using backward error analysis, the paper argues that SGD with small but finite step sizes stays on the path of a gradient flow ODE of a modified loss, which penalizes the squared norms of the mini-batch gradients. Major Comments The main result says that the expected SGD iterate after a single epoch lands close to the path of a gradient flow ODE on a modified loss. Summary: To analyze why the generalization error of SGD with larger learning rates achieves better test error, this paper analyzes the implicit regularization of SGD (with a finite step size) via a first order backward error analysis. Under this analysis the paper shows that the mean position of SGD with m minibatches effectively follows the flow according to Eq (20) for a small but finite step size, while GD effectively follows the last inline equation in section 2.1.
Summary The paper provides an extensive empirical analysis of Pruning-at-Initialization (PaI) techniques and compares it against two pruning methods after (or during) training. In fact, an intuitive ablation setting is to test for the effect of training with the same saliency measure, as done in Section 6, but unfortunately the result out of this seems to be unhelpful for figuring out the potential cause of poor performances for pruning early methods. Further, the ablation studies performed on these techniques yield surprising results, both in isolation (inverting GraSP improves accuracy!) and when compared to magnitude pruning after training (these three are invariant to shuffling and re-initialization).
The paper presents an interesting approach to solving the on-screen vs off-screen sound problem in audio-visual source separation. -----Experiments----- I understand that it is hard to obtain single source on-screen clips, but it could have been better if the authors had collected some small samples and test the single mixture separation performance. This paper proposed an unsupervised method for open-domain, audio-visual separation system. This paper describes a system for separating "on-screen" sounds from "off-screen" sounds in an audio-visual task, meaning sounds that are associated with objects that are visible in a video versus not. The authors introduced a new, large-scale, open-domain dataset for on-screen audio-visual separation. The proposed method extends the recent unsupervised source separation framework MixIT by conditioning video input.
In the first stage modeling, the authors proposed a new phoneme-level acoustic condition modeling in addition to the speaker and utterance-level approaches. In this paper, the authors present AdaSpeech, a TTS system that can adapt to a custom voice with a high quality output and a low number of additional parameters. A global acoustic embedding conditions the decoder in addition to speaker embeddings, in the hopes of accounting for recording conditions, and, I suppose, timbre, which should then be disentangled from the linguistic information from the text in the decoder during pretraining and adaptable to new recordings at fine-tuning/inference. Its multi-phonetic-level acoustic condition modeling approach seem technically new and interesting, The authors propose an interesting text-to-speech adaptation method for high quality and efficient customization of new voice. On the contrary, the phoneme hiddens used in phoneme-level acoustic predictor do not seem to contain any personal voice information because they are resulted from the phoneme encoder that uses text information only as its input in Fig 1. Similarly, I highly doubt the utterance-level acoustic condition modelling does not also capture speaker information. The proposed method consists of two-stage modeling : multi-phonetic-level acoustic condition modeling and conditional layer normalization. This paper proposes AdaSpeech, a Transformer-based TTS architecture derived from FastSpeech, but multi-speaker, and focussed on the task of low-resource, robust, and low-dimensional speaker adaptation. The overall structure of acoustic condition modeling in Figure 2 (a) is not so clear. (Both the normalization parameters and the speaker embedding itself are fine-tuned.) I am not sure what the speaker-embedding is left to do with all this acoustic-level input, but OK. There is little discussion on the theoretical side of the acoustic condition modelling, such as how the authors are able to determine that the utterance-level and phoneme-level vectors are modelling things like room condition.
"The results show that HeteroFL can boost clients' performance with low computation and communication capabilities by allowing the training of heterogeneous models with larger computation complexities." -> This sentence is unclear and leads to a wrong statement. ORIGINAL REVIEW: The authors propose three elements: A way to approach model heterogeneity across clients with different resource constraints, a 'masking trick' for approaching non-iid-ness and a modification to BatchNormalization in the Federated Learning setting. This paper proposes a new federated learning framework called HeteroFL, which supports the training of different sizes of local models in heterogeneous clients.
From a robotics point of view, I would have accepted it as a very good application paper, if the authors had also presented real-robot results that show the learned model in action (generating actual trajectories for the robot). The strength of the paper is in demonstrating that conditional latent variable models can learn disentangled low dimensional represented using weak supervision; which authors effectively demonstrated using real-world experiments. Summary Under the context of learning from demonstrations, the paper studies the problem of leaning interpretable low dimensional representations from high dimensional multimodal inputs using weak supervision. However, the idea of learning "interpretable representations" using "weak supervision" is interesting and probably the most significant bit in this work. The paper needs to present real robot results and cover related work in more detail. This paper proposes and interesting and novel way to handle weak labels from human demonstrators. Towards this end, the paper proposes to learn probabilistic generative models capturing high-level notions from demonstrations using variational inference. Authors motivate the paper using the idea that learning interpretable low dimensional concepts called "common sense" will enable generalization and adaptation. Without real robot experiments to show how the generated conditioned trajectories actually perform, in my opinion the paper is an application paper without any significant ML contributions. This paper presents a technique that uses latent variables to model the uncertainty over a group of class labels that could describe the task (e.g., slow, soft, left-of-object).
In order to fairly compare with DLG, and also to show the general effectiveness of the proposed method, the authors should also consider comparing with DLG on some standard network, such as the LeNet benchmarked in many previous gradient attack works [1,2]. In particular,  this paper tries to form a system of linear equations to find a training data point when the gradient of the deep learning model with respect to that data point is available.
Motivated by the sensitivity of RL algorithms to the choice of hyperparameters and the data efficiency issue in training RL agents, the authors propose a population-based automated RL framework which can be applied to any off-policy RL algorithms. Summary: This paper propose a population-based AutoRL framework for hyperparameter optimization of off-policy RL algorithms. To show that the neural architecture adaptation is a crucial part of the framework (which is a major difference between the proposed method and PBT), the authors might have to move to a complex domain of environments to demonstrate that. Summary The authors in this paper propose to optimize the hyperparameters and also the neural architecture while simultaneously training the agent. A shared experience replay buffer is used across the population, which as demonstrated in the experiments, substantially increase the sample efficiency compared to PBT and random search.
This paper studies the asymptotic convergence properties of (population-level) policy gradient methods with two-layer neural networks, softmax parametrization, and entropic regularization, in the mean-field regime. The optimization landscape and convergence properties of policy gradient methods have drawn attention in RL theory for a long time, and it is nice to see a work that studies this problem from the perspectives of mean-field limit of neural networks, albeit being completely asymptotic. Under certain regularity conditions, the paper shows that if the training dynamics converge to a stationary point, this limiting point is a globally optimal policy. The paper also presents results for finite-time convergence of the training dynamics for neural networks to the mean-field limit.
I think the paper provides a clear argument for simple and general exploration strategies and that ϵz-greedy seems to be an algorithm that achieves these goals. This paper proposes an easy-to-implement algorithm for the efficient exploration, which is a temporally-extended version of \\eps-greedy. Strong points This paper analyze theoretical properties of temporally extended e-greedy exploration in Theorem 1. The idea is simple (a generalization of e-greedy) and the discussions nicely illustrate the main properties of an ideal generally-applicable exploration method. ########################################################################## Summary: This paper proposes a simple yet general approach for exploration in discrete-action problems.
In particular, it proposes to introduce high dimensional and high entropy label representations for group truth, to improve image classification performance from two practical matters --- Robustness and data efficiency, while achieving comparable accuracy to text labels as the standard representation. The results show that high dimensional and high entropy label representations are more useful, which is observed in the experiments related to robustness and a limited amount of training data. ########################################################################## Questions during rebuttal period: Please address and clarify the cons above The paper proposed to use other high-dimensional representation instead of direct one-hot class labels in image classification problem. Also, the author argues that audio labels is special, but in the paper, other type of high-dimensional representation of labels that uses external information are not explored, such as word2vec. -- achieving comparable accuracy with less data in training, I would still suggest the authors to conduct the following studies to enhance the quality of the paper: It could be valuable to future investigate the inherent property that contributes to the improvement, besides high dimensionality and high entropy. The evaluation is conducted in nearest-neighbor way, measuring the distance between the image embedding feature and groundtruth high-dimensional representation (spectrogram) and decide the predicted class label.
Summary This paper considers the problem of adapting a pre-trained model for few-shot learning in case there is a shift of distribution from the meta-training set. In this setting, the authors argued further that one cannot run the meta training, so the focus of the paper is to improve stage (2) (gradient update process at the test time). Towards this end, the paper proposes to run ensembling fine-tuning with the support set of the test task, and use the weight variances from this process to guide the training. The paper also proposes to add task adversarial examples to the training set to help the meta fine-tuning process. The paper tries to solve a novel practical problem: adapting pretrained meta-learning checkpoints to out-of-domain test tasks. Bib Antoniou et al 2019 How to train your maml The paper proposes to reutilize pretrained MAML checkpoints for out-of-domain few-shot learning, combining with uncertainty-based adversarial training and deep ensembles. I do like the use of ensemble and also adversarial examples, and intuitively they can improve the test-time adaptation.
This paper provides sample complexities analysis, showing that wide two-layer neural networks with standard activation functions and SGD optimization is able to capture the data regularity between input and output, modulated by the three kinds of task code considered. If we view it as one complex task, with the input-output data relationship modulated by one of the three task codes, then the questions of "learnability" by neural networks in handling this case are well addressed by the proposed theoretical analysis and I can see the empirical study is supportive of this claim. Pros: This paper is quite novel in many aspects, including modularity v.s. monolithic, constructing task codes by SQL-style aggregation queries, "inverse counterpart" of multitask learning, connections to cognitive science. The main contributions of the authors are the following: showing that "the two layer neural network can jointly learn the task coding scheme and the task specific functions without special engineering of the architecture" Overall, I think the monolithic task formulation is novel and the problems the authors would like to address are fundamentally important. ########################################################################## Summary: The paper provides insight into the boundaries and feasibilities of a monolithic formulation of multitask learning by neural networks. The authors show how complex tasks can be modularly formulated thus yielding a joint monolithic learning possibility. This paper posits a very interesting question about provable multi-task learning by neural nets. The paper is lacking much in clarity: The definition of g(c; x) in section 2.2 should be given from the onset and the related work discussion based around that.
If instead, the fourier component is meant to be distributed among the input population, it the authors should make this clear This paper proposed a model of navigation based on grid cells and the successor representation (SR). Originality: Some key ideas presented in the paper (e.g, eigendecomposition of the transition matrix, successor representation) is fairly standard in previous literature, although the authors show that there is a way to unify some of the previous models with these notions. The paper shows that this model can generate several experimentally observed properties of grid cells, and can be used in navigation of novel/mutable environments. Pro: the paper shows that a simple model based on grid cells and SR representation can perform navigation in some simple environments. The authors show equivalence of the proposed method to classical models of path integration by grid cells - continuous attractor networks and oscillatory inferencce. Typos: fig1 caption: "of of" singel-neuron framrework discrinminative The authors propose an efficient method to predict directed transitions in spatial tasks by extending eigenbasis based prediction model. This paper shows how SR representation theory can be used in a model of grid cells to plan and navigate towards a target. It is unclear how it is implemented in the network model, and how this information can get to the grid cells. I have worked at the intersection of theoretical neuro and ML for 8 years and am familiar with some recent literature on grid cell modeling, but still couldn't make sense of many aspects of the paper because of the many assumptions of prior knowledge. it is unclear to me how the successive exploration of possible directions could be implemented in practice by the grid cell network (see below for detailed comments). the paper attempts to unify various kinds of grid cell models, which is interesting. The paper uses this idea to develop new connections with path-integration grid cell models for their prediction, illustrating the method in a gridworld and simulated grid cell domain. A key insight of the paper is that velocity instructions modify the eigenvalues of the SR but not its eigenvectors, so the eigendecomposition does not need to be recomputed for all candidate velocity instructions, and the eigenbasis can be hard-coded in the neural circuit (by the grid cells). Strong points: important contribution to our understanding of how grid cells could support navigation towards a goal rigorous mathematical derivation of the results
In this work, the authors provide a method for a posteriori calibration of DNN uncertainty with emphasis on constructing a classifier that has PAC uncertainty guarantees. Summary: This paper proposes a method for obtaining probably-approximately correct (PAC) predictions given a pre-trained classifier. Take for example a predictor \\hat{f} which assigns \\hat{p}(x) = 0.9 to every input 'x' regardless of its ultimate accuracy on the class, then given a new input with unknown label, it is not clear to me exactly how the framework would use the intervals to improve the uncertainty of this classifier, especially given that the class of this new point is unknown. Strong points: The proposed method provides a provable guarantee on the reliability of a pre-trained methods prediction, which is a very nice property to have in the reliability/safety problem. They demonstrate and explore two use cases: applying this technique to get faster inference in deep neural networks, and using the PAC predictor to do safe planning. The authors propose constructing calibrated outputs that have provable correctness guarantees, using PAC-style arguments. In particular, it seems to me like the proposed intervals only hold their PAC guarantee when the test-time distribution matches the training distribution. The authors define a 'calibrated' probability prediction to be one such that given (for example) an image labeled as a cat, that the probability assigned to the class label 'cat' by the predictor is equivalent to the probability that the classifier correctly predicts images from the class 'cat.' Experimentally, the method shows improvements over a naïve baselines, and demonstrate that it can obey a given error or safety threshold in practice, an important property I think this is a OK trade-off to make when in safety-critical scenarios, but then the authors give "fast inference" as one of their primary applications, it seems like a bit more discussion of this may be warranted.
My main problem is that I don't understand what the different method that are evaluated are: Given the abbreviation ESMN introduced in the abstract, I assume that ESMN is the proposed approach. The paper proposes an ego-centric representation that stores depth values and features at each pixel in a panorama. In the related work section, one argument made by authors is that ESMN and MemNN/NTM are two different paradigms for learning spatial memory. also provides a form of memory representation and I don't understand why the paper, and its follow-up (Zhi et al., SceneCode: Monocular Dense Semantic Reconstruction using Learned Encoded
Overall, I feel the paper provides a nice contribution, both theoretical and practical, to the problem of rank-constrained convex optimization that neatly builds upon previous work.
However, our Fourier neural operator does not have this limitation." Paper Summary: The authors proposed a novel neural Fourier operator that generalizes between different function discretization schemes, and achieves superior performance in terms of speed and accuracy compared to learned baselines. The paper below uses a spectral solver step as a differentiable layer within the neural network for enforcing hard linear constraints in CNNs, also taking the FFT -> spectral operator -> IFFT route. As one of your motivations behind this work is to learn the operator, it could have been interesting to test your approach using different sample points as in the training data. Experiments demonstrate that the Fourier neural operator significantly outperforms other neural operators and other deep learning methods on Burgers' equation, Darcy Flow, and Navier Stokes, and that that it is also significantly faster than traditional PDE solvers. This work attacks the important problem of learning PDEs with neural networks. If the underlying domain has a uniform discretization, the fast Fourier transformation (FFT) can be used, allowing for an O(nlogn) evaluation of the aforementioned convolution operator, where n is the number of points in the discretization. The remaining theoretical work, namely writing down the Fourier integral operator and analysing the discrete case, was succinctly explained. This allows the network to implicitly learn the partial differentials of the equation, and results in a parametrization which is invariant to spatial discretization. Section 6, final sentence – "Traditional Fourier methods work only with periodic boundary conditions, however, our Fourier neural operator does not have this limitation." -> "Traditional Fourier methods work only with periodic boundary conditions.
########################################################################## Summary: The paper identifies the negative effects on calibration and the robustness of the deep models when data augmentation and the ensembles are combined. The concerns: ECE is a biased estimate of true calibration with a different bias for each model, so it is not a valid metric to compare even models trained on the same data [Vaicenavicius2019]. Summary: This paper found that combining ensembles and data augmentation can harm model calibration. The vanilla deep neural networks are often in the over-confident regime, so either ensemble or mixup itself seems very effective in improving model calibration.
The authors have evaluated the method in several object pushing and robot locomotion tasks and shown superior performance over baselines that uses recurrent state representations or gradient-based meta-optimization. === Strengths This paper targets an important question of building a more generalizable dynamics model that can perform online adaptation to environments with different physical properties and scenarios that are not seen during training. === Summary This paper proposes a framework, HyperDynamics, that takes in observations of how the environment changes when applying rounds of interactions, and then, generates parameters to help a learning-based dynamics model quickly adapt to new environments. Weaknesses: The main claim of the paper is that hyperdynamics network offers better prediction accuracy and generalization than a standard dynamics model. I believe the model capacity comparison (Table 6) is especially important for demonstrating the value of the new architecture, and would recommend mentioning that result in the main paper. Strengths: The paper addresses an important question, namely, how a dynamics model may adapt to environments that don't fully match its training distribution. Overall, while the paper presents an interesting idea, the experimental evaluation is not convincing in its current state: Baseline architectures are not fully specified, many of them did not receive the same input, and no benchmark task with previously reported results has been used. However, at the end of Section 3.1, the authors suggest that they "discard the orientation information from states fed into the generated dynamics model."
[Summary] Paper proposed to generate the communication message in MARL with the predicted trajectories of all the agents (include the agent itself). After reading this paper, indeed I find the authors failed to capture some important research in this narrow area and it's still not clear how does the proposed method really works and whether it is sensitive to some specific implementing factors. -) The proposed method is essentially quite complex (stacked prediction, transformer, etc) than its Bayesian counterparts, while the authors only provide an overall evaluation against several MARL baselines. Specifically, compared to existing works, this paper proposes to generate messages based on not only current information but also future information (referred to as imagined trajectory) (Section 4.1). Specifically, I would like to see how does the proposed method formulate its intention prediction differently than the prior work and whether it could enjoy advantages in performance with such differences.
Summary This paper extends neural compression approaches by fine-tuning the decoder on individual instances and including (an update to) the decoder in the bit-stream for each image/video. This paper considers the problem of per-instance model adaptation for neural data compression, and proposes a new method for end-to-end finetuning the model that is quantization-aware, by introducing an additional term that measures the compression cost of model update to the typical rate-distortion loss. If the experiment did perform per-instance model adaptation, then it would be much more convincing to evaluate on standard datasets like Kodak and Tecnick from the image compression literature, instead of frames of UVG videos. Since the paper's contribution is about improving the existing fine-tuning strategy that tackles model update quantization after fine-tuning (e.g., Zou et al., 2020), the proposed method should then also compare to these baselines to really assess its performance. Summary The paper describes an instance specific finetuning method for image and video compression including finetuning the decoder. Strength = Method which also considers to finetune/adapt the decoder side of image compression network, for improved performance. How sensitive is the compression performance to their choice, e.g., is it possible to discretize so finely that no amount of RD improvement can overcome the model update cost? The paper claims that "In this paper we consider the extreme case where the domain of adaptation is a single instance, resulting in costs for sending model updates which become very relevant", but this would highly misleading if all the experiments were conducted in a batch compression setting.
This effect is measured by the authors by trying out the existing Siamese few-shot detector on 4 datasets: PASCAL, COCO, Objects365, and LVIS showing that the gap in performance on the seen training and the unseen (novel) testing categories is reduced when the base dataset has more classes (e.g. on LVIS where there are more than 1K classes, this "generalization" gap is shown to be minimal). Weaknesses: The paper studies the importance of number of object categories in training dataset and claims that the gap in one-shot detection can be closed by increasing the number of categories. This paper provides a variety of studies to understand the generalization gap between known and novel classes in one-shot object detection. -> either The paper suggests that a major factor for increasing few-shot performance in the few-shot object detection task is the number of categories in the base training set used to pre-train the few-shot model on a large set of data before it is adapted to novel categories using only a few (or even Pros: number of base classes is indeed an important factor in few-shot methods performance (not just in detection)
(p.1, Abstract) we generalize the IRL problem to a well-posed expectation optimization problem stochastic inverse reinforcement learning (SIRL) to recover the probability distribution over reward functions. (p.2, Introduction) The solution of SIRL is succinct and robust for the learning task in the meaning that it can generate more than one weight over feature basis functions which compose alternative solutions to the IRL problem Summary The authors proposed inverse reinforcement learning (IRL) algorithm based on Monte Carlo expectation-maximization (MCEM) that maximizes the predictive distribution of trajectories given the reward distribution parameter (eq (1)). The paper proposes a novel method for inverse reinforcement learning: inferring a (distribution over) reward functions from a set of expert demonstrations. (p.3, Problem Statement) more likely generates weights to compose reward functions as the ones derived from expert demonstrations The method uses MCEM and samples reward functions from a current estimate of the GMM, updates them via a gradient-descent based maximum likelihood approach and then updates the GMM to fit the updated parameters. Intuitively, the main benefit is to allow for k reward-function archetypes that represent the set of expert trajectories well; however, there are no examples nor any evaluation to show in which case this is beneficial. Prior work has either learned a point-estimate, notably maximum entropy IRL, or used Bayesian methods to learn a probability distribution over reward functions.
---- Summary This paper proposes FLAG (Free Large-scale Adversarial Augmentation on Graphs), an adversarial data augmentation technique that can be applied to different GNN models in order to improve their generalization. Strengths: There are various best practices which are commonly applied in common task framework competitions such as data augmentation, adversarial training, and ensembles that improve results by fractions or a small number of percentage points. ogbg-code: +2: GCN+virtual node+FLAG, +4: GIN+virtual node+FLAG, +4: GIN+FLAG, +2: GCN+FLAG This paper investigates adversarial feature augmentation for improving the generalizability of graph neural networks. Nevertheless, there are a few drawbacks: Even though the method is completely adopted from prior work and there are no novelties, the authors claim they propose a new solution for graph data augmentation. Hence, my takeaway from the paper (which is in fact a valuable takeaway) is that adversarial augmentation is not considerably effective for graph neural nets, no matter what dataset and network architecture is used. ------ Post-Rebuttal Update ------ I have read the author's response, thank you for adding more experiments with other graph data augmentation techniques. Also, there are a couple of contemporary papers on graph data augmentation [2, 3] that the authors could reference too. This paper presented an adversarial augmentation technique for graph neural networks. It is true that adversarial feature augmentation has not been studied for graph neural nets, but it is a straightforward idea to apply an existing feature augmentation method on graph nodes, which are represented using feature vectors just like other types of data. This work falls under such methods, and proposes to perform FreeLB adversarial training by computing the loss with respect to perturbations.
The paper presents a method for improving the accuracy when training CNNs from scratch on a small dataset. The authors propose an interesting problem where only a small sized labeled data set is available for supervised learning.
Summary This paper presents a new type of brain-inspired dual-pathway DNN model where the coarse (faster, less accurate) and fine (slower, more accurate) visual pathways augment each other during training and inference (via imitation and feedback) to boost the network's robustness to various noises. [8] Brain-Like Object Recognition with High-Performing Shallow Recurrent ANNs, NeurIPS, 2019 This paper proposed a two-pathway neural network to mimic the interplay between the parvocellular (slow and fine-grained) and magnocellular (fast and course) pathways in neural systems. The proposed architecture is novel and the results support the main claims regarding the improvements in robust object recognition behavior and replication of the psychological experiment. (Backward masking) Visual results alone (Fig 5 and 12) don't properly support the claim that this model "can explain visual cognitive behaviors that involve the interplay between two pathways". To justify the necessity of imitation learning, the authors should present full FineNet+CoarseNet results (not just Fig 3) using all 3 losses versus using only FineNet's classification loss. This paper proposes a dual-path CNN architecture with complementary roles (FineNet and CoarseNet) which is inspired by parvocellular and magnocellular pathways in the primate brain. In sectino 3.3, in the FineNet-only models, it is unclear how the feedback loop is resolved in the absence of CoarseNet. In section 3.3 it is stated that "this highlights an important goal for the brain employing two-pathway processing". ], SOTA in adversarial defense [4, 5], and most importantly other brain-inspired models, targeting robustness [6, 7] or not [8, Tang et al.
It would be great if authors can state the novelty of this paper compared to DeepGCNs. Also, what makes difference between the proposed aggregation function and softmax. This paper proposes a general aggregation function which summarizes sum, max, and softmax operations etc. This paper studies how to train deeper graph convolutional networks by using different aggregation function. Since the OGB benchmark is new and the reported GNN models on the OGB leaderboard were only run for 3 layers, it is crucial to analyze all the variants discussed here in detail to appreciate the performance gain achieved by the proposed generalized aggregation function. The authors propose new and, in particular, parameterized aggregation functions for GNNs in order to especially support the construction of deeper GNNs. The paper is fairly understandable and the "deeper GNN" topic has gained more attention recently. The paper is missing an explanation why these different aggregation functions are supposed to specifically support deeper GNNs. The proposed general aggregation function doesn't show significant improvement over softmax with learned temperature. The initial OGB leaderboard contains only the most basic GNNs. However, when proposing an approach for creating deeper GNNs, the paper has to compare to this kind of models too.
Summary of this paper: In this work, the authors propose a method to learn to generate long-range video sequences. ** Weaknesses (1) It seems a strong limitation that the proposed approach is not able to generalize to different videos or has to be video-specific (i.e., train a model on each input video). First, although there may not exist any resampling method that can directly perform the video texture synthesis, I believe many related graph-based methods could be used to model the transition probabilities of frames. In this paper, the authors proposed a non-parametric approach for video generation, i.e., video frame (un)conditional resampling. Specifically, during training a model is used to learn the transition probability between different video segments. Second, although the authors pointed out that the video resampling (textures) strategy is different from the recent generation-based strategy, they should provide visual results/comparisons to support their claims. ** Strengths (1) Improve the classic video texture synthesis method Video Textures by replacing pixel similarity with a distance metric learning to measure the transition probabilities It is highly recommended that the authors could present more comparison with the previous baselines in both general idea and model details. The authors made a trade-off between the audio conditioning signal and the learned transition probabilities. In addition, the authors may further discuss the limitations of the proposed method. To guarantee the smoothness of the transition between different segments, an existing interpolation method is used to connect these video segments in a sequential order. The general idea is starting from a prior work (Video Textures) and extending this work with a learning framework. (2) Extend the proposed approach to audio conditioned video synthesis Comparing to existing 'video textures' methods, the authors mainly made two improvements/contributions. The proposed method is inspired by Video Textures (Sch¨odl et al., 2000), which synthesizes new videos by stitching together snippets of an existing video. The motivation behind this work is clearly presented, i.e., to synthesize long-range video sequences. (i) a new pipeline for modeling and calculating probabilities of transitioning between frames of the same videos. During inference, long-range video synthesis is achieved through iterative sampling of new video segments.
In this paper, the authors propose a certifiable watermarking method for neural networks. The method can be used as a black-box watermark (does not require model parameters to verify),  however, the certification bounds only apply to a white-box use case in which the verification can perform inference and test accuracy for a set of trigger images for multiple smoothed versions of the parameters. The proposed method exploits the randomized smoothing techniques for a certified watermark of neural networks. 2. The concept of certifying neural network parameters is novel, and it works under a lot of model parameter modification based attacks. The proposed method is based randomized smoothing techniques together with optimizing the model on a dedicated trigger set. Comments: This paper present the first certifiable neural network watermark method. The authors show their method can guarantee persistent of watermark examples up to a predefined change in model parameters, in the l2 distance.
It shows performance improvements on ImageNet compared to previous hardware-aware NAS methods which only optimize the neural network architectures. Summary: The paper proposes an algorithm to search for hardware designs and neural architectures jointly. This paper proposes NAHAS for co-designing neural network architecture and hardware architecture. The main difference between this paper and previous hardware-aware NAS papers is that this paper has an additional hardware search space beside the neural network architecture search space. ########################################################################## Pros: The paper introduces a new dimension, hardware design, to the neural architecture search domain. ########################################################################## Summary: The paper presents NAHAS, which is a combination of Neural Architecture Search (NAS) and Hardware Architecture Search (HAS) In summary, co-designing neural network architecture and hardware architecture is not new.
The paper presents a simple addition to the Balanced Accuracy approach - which the authors refer to as 'importance'. This paper proposes a simple and general-purpose evaluation framework for imbalanced data classification that is sensitive to arbitrary skews in class cardinalities and importances. Some major comments on the paper: The proposed evaluation metric appears to be to show whether machine learning approach A is actually better than machine learning approach B. This paper presents a weighted balanced accuracy to evaulate the performance of multi-class classification. Since the weighted terms e.g., wi play an important role in the proposed framework, more examples should be given to explain how to choose these parameters in different application domains. For example, the micro average (or simply called "Accuracy" in this paper) can be regarded as a special form of equation (6), where weights or importance is given by the relative frequency of each class. Class-insensitive Metrics" in page 7, the authors states "This result validates that WBA_rarity provides a more sensitive tool for assessing classification performance".
Based on the observations and discussions, the paper then proposes a novel lower bound to regularize the neural networks and alleviate the problems of MINE. Some interesting experimental results are firstly provided on a synthetic dataset: the constant term in the statistical network is drifting after MI estimate converges. Summary The paper introduces a generalized version of the mutual information neural estimation (MINE), termed regularized MINE (ReMINE). Yes, I agree with the author the drifting phenomenon is not the only problem that the proposed method solves. Empirically, the proposed regularized term works well along with the original MINE estimator and ReMINE  has better performance in the continuous domain However, there are various ways for tackling this ( as pointed out in my original reviews, like using a non-drifted mutual information estimator with moving average or plugin some robust density estimators). In addition, the authors are severely neglecting some of the non-neural network state of the art MI estimators in their citations and comparisons (see the references below for some examples, which all have strong theoretical results). In the end, the authors really only propose a small modification to the MINE estimator to counter the supposed drifting problem and do some experiments showing some improvement.
The authors propose an algorithm to enlarge the training set for image classification problems in certain medical applications where training data of the target modality is scarce. Moreover, they demonstrate the effectiveness of the proposed algorithm via extensive numerical experiments on a prediction problem and compare their method to several different approaches ranging from transfer learning to data augmentation. This paper combines the Cycle-GAN with the predictor for downstream task to augment the training data in the target modality.
This paper studies an important problem: that differentially private algorithms can have disparate impact on model accuracy for different sub communities. This paper shows that DPSGD makes the problem of an unbalanced dataset even worse and decreases accuracy on the underrepresented class significantly. In order to make the experiment more informative, I suggest authors compared FairDP with other algorithms that have been designed for addressing unbalanced datasets. This paper addresses the problem of an unbalanced dataset and tries to equalize the accuracy for different classes (labels). If the claim isn't that Algorithm 1 is DP, then the privacy guarantee is restricted to the results in Figure 3, that attack algorithms perform similarly well on DPSGD, FairDP, and significantly better on SGD (non-private).
Towards addressing this issue, the paper proposes a distributed GCN training scheme based on subgraph approximation, and demonstrates its empirical effectiveness on medium-to-large datasets. (Motivation) While many previously proposed schemes train GCNs on a single machine, this paper considers a fully distributed setting. The paper presents a subgraph approximation method to reduce communication in distributed GCN training. (Quality) The paper can be significantly improved by positioning with and/or exploring many other (more recent) large-scale single-machine graph neural networks (GNNs).
The authors propose to adapt the model-agnostic meta-learning algorithm (MAML) of [1] to reflect this hierarchical structure by either observing (Section 4.1, FixedTree MAML) or inferring (Section 4.2, LearnedTree MAML) an assignment of tasks to clusters at each step of the inner loop (task-specific adaptation phase) of MAML; Summary In the context of gradient-based meta-learning for few-shot learning, the authors propose TreeMAML, an algorithm that leverages the existence of a tree structure in a task distribution in order to pool inner-loop gradients between tasks. Significance: Results on the hierarchically structured synthetic regression task datasets demonstrate that {Fixed|Learned}Tree MAML: is at least as good as MAML, and often outperforms MAML; The submission proposes a meta-learning algorithm attuned to the hierarchical structure of a dataset of tasks. As claimed in this paper, the proposed model lacks scalability since it works well only in the tree-structured tasks.
The authors experiment both with pre-training and fine-tuning of contextual models (BERT-{base,large}) and claim large reduction in training time, with reasonable loss in performance. The main contribution of this work is to use Early Bird Lottery Tickets to reduce pre-training and fine tuning time for BERT. More specifically, they adapt EarlyBird lottery tickets to the BERT setting in order to find winning configurations in early stages of training combine it with structured pruning methods to ensure the resulting network is more efficient to train. Experiments for both pre-training (the first of its kind) and fine-tuning show that performance does not drop all that much for GLUE and Squad which are the main set of tasks BERT is typically evaluated on. The goal of the paper is to find structured winning tickets for BERT in the early stages of training/fine-tuning. The usage of lottery tickets during the pre training phase is the biggest strength of the paper since it can result in significant computational savings. Overall, I find the approach interesting and the authors show computational savings in the pre-training models. (More of a question/nit) Is it possible to also show what happens when a winning ticket for BERT fine-tuning is selected based on the pre-training objective? This would involve computing the pre-training time (time to learn BERT parameters on Wikipedia) + total fine-tuning time across all datasets (QQP/CoLA/MNLI etc) considered. Experiments show that performance isn't that much worse when EarlyBERT is used for fine-tuning and for pre-training. The authors pitch it as a technique for reducing the training time of BERT and use LayerDrop as a baseline technique that also removes network components.
This paper presents a deeply supervised few-shot learning model via ensemble achieving state-of-the-art performance on mini-ImageNet and tiredImageNet. The authors first studied the classification accuracy on mini-Image across convolutional layers and found the network could perform well even in the middle layer. The biggest concern is the contribution of this paper, to be more specific, the proposed method might not be useful and might need to be tuned in other few-shot settings. The mini-ImageNet and tiered-ImageNet results are good, while the authors could provide more evidence to show its strength and how to balance the computation and model performance. Summary: The authors propose to tackle the problem of few-shot learning (FSL) using ensembling diverse classifiers. And the ensemble achieves the new state-of-the-art results in a few-shot setting, comparing to previous regular and ensemble approaches.
It is empirically shown that, for a specific type of initialization, for less over-parameterized neural networks, the gradient dynamics follows two phases: a phase that follows the random features model where all the neurons are "quenched", and another phase in which there are a few "activated" neurons. Since it considers largely simplified settings (specific target functions, data sampled uniformly from the unit sphere, in most experiments the true solution can be found with a sub-network consisting of only one hidden neuron) and since the networks and optimization method (gradient descent) are rather simple, one could expect that at least some theoretical contribution or explanation could be given, which the paper lacks entirely. In a controlled setting, a list of experiments investigates the dynamics and compares them for different regimes where the relation of number of hidden neurons to number of training samples is changed and for several specific target functions. This paper studies gradient descent dynamics of two-layer neural networks with ReLU activation function. In particular: The paper shows for certain artificial target functions with low-dimensional structures, the neurons start out from a random-feature behavior and then a few are selectively activated, distinct from the rest of the neurons which are "suppressed".
=== Summary This paper proposes a benchmark that aims to systematically evaluate models' ability in learning representations of high-level variables as well as causal structures among them. [2] CausalWorld: A Robotic Manipulation Benchmark for Causal Structure and Transfer Learning, https://arxiv.org/abs/2010.04296 This paper is a review of model-based approaches of integrating causal inference to reinforcement learning (RL) in different environments (application areas). The authors provide software to analyse how three types of models ("monolithic", i.e. latent space models without a graph-like structure of the latent space, graph neural networks (GNN) and "modular", i.e. the C-SWM model (Kipf et al., 2020)) perform in two artificial "environments" devised by the authors (physics and chemistry) based on a number of metrics, some of them also proposed by the authors. The authors evaluate several representation learning algorithms from the literature and find that explicitly incorporating structure and modularity in models can help causal induction in model-based reinforcement learning. === Strengths This paper targets an important problem of assessing models' ability to automate the inference and identification of the causal variables from high-dimensional inputs like images. They show that structural inductive biases are beneficial for causal relationship learning and model-based RL by testing a variety of representation learning algorithms on this benchmark.
"Nattack: Learning the distributions of adversarial examples for an improved black-box attack on deep neural networks." arXiv preprint arXiv:1905.00441 (2019). "Zoo: Zeroth order optimization based black-box attacks to deep neural networks without training substitute models." Proceedings of the 10th ACM Workshop on Artificial Intelligence and Security. This paper proposes a method to detect adversarial examples.
The paper applies it to three popular data augmentation methods: AugMix, Mixup, and adversarial training. One main disadvantage of the proposed method is that it seems to be sensitive to the distance function chosen for the label assignment and this function has to be adjusted for different data augmentation schema. This paper proposes a method to adaptively smooth the augmentation data labels based on the augmentation strength. The proposed method is general, applicable to different types of augmentations: AugMix, Mixup, and adversarial training.
Training with a large OOD dataset like [Hendrycks et al.] is not common, and the observation in this paper is limited to this setting. Instead, this paper seems to show stronger results, with a more intuitive and simpler method (just classify the outlier exposure set into a separate class) that Hendrycks et al suggest doesn't work as well. Reasons for score: The proposed setting with a large OOD dataset has already been proposed by [Hendrycks et al.], and the proposed method has been experimented in [Lee et al. More specifically, this paper found that adding an abstention class is better than prior methods when the in-distribution dataset is CIFAR and TinyImageNet is available during training as an OOD dataset. Again, [Hendrycks et al.] considered a similar setting, but they proved the effectiveness of their method in image and natural language domains, with 7 in-distribution datasets and 3 large OOD datasets. Hendrycks et al propose encouraging high entropy predictions on the outlier exposure set, instead of classifying them into a reject class. (a)] and [Dhamija et al.], I think the main reason why they didn't get the same observation is on the size of OOD dataset, i.e., they didn't train their model with a large OOD dataset like [Hendrycks et al.] or this work. ######################################################################### -- Summary: This paper presents experiments and results from using a reject-class in multi-class classification for the auxiliary task of out-of-distribution (ood) sample detection. Summary: This paper shows that introducing an abstention class for out-of-distribution (OOD) works well for detecting it when the in-distribution dataset is CIFAR and TinyImageNet is available during training as an OOD dataset. Performances of prior methods are borrowed from original works, but they are mostly experimented in settings different from this paper. (2018), we uses additional samples of real images and text from non-overlapping categories to train the model to abstain, ..." i.e., rather than elaborating/emphasizing the setting (together with their method), they just cited a prior work.
The authors propose a new method, called augmented counterfactual ordinary differential equations (ACODs), to do counterfactual inference on time series data in healthcare. ########################################################################## Summary: In this manuscript, the authors propose a novel way of performing counterfactual inference in time-series in the presence of hidden confounders.
In addition to that, the authors conduct a lot of experiments involving non-linear networks and real-world datasets, that show the inverse dependence of the variance of gradient and batch size throughout the training. The paper shows that the variance of the gradient has an inverse dependence on the batch size in linear networks, subject to the knowledge of the initial weights. It shows that the variance of stochastic gradient is a decreasing function of minibatch size for linear regression and deep linear network. For linear models, the variance of the gradients conditionned on the initial point is decreasing with the batch size b. Can the authors show why the training loss decreases with increasing variance in gradient, at least in the linear regression case?
Summary: The paper proposes a new Graph Transformer (UniMP) based model with the motive of combining two powerful semi-supervised node classification techniques, GNN and LPA. Proposed Graph Transformer unifies feature and label propagation in conjunction to provide a better performance in semi-supervised node property classification task. The authors proposed a unified message passing model to make a graph neural network to be able to incorporate both label propagation and feature propagation. This paper provides a novel method to unify GNN and LPA algorithms in order to take advantage of feature as well as label information. The UniMP first employs graph Transformer networks to jointly propagate both feature and label information. Section 4.4: model still be uncertain → model still remains uncertain The paper presents a novel unified model that jointly harnesses the power of graph convolutional networks and label propagation algorithms based on the unified message passing framework. Pros: The motivation of the paper is very clearly stated in the text and the experiments successfully show that by incorporate label propagation and feature propagation, the performance can be improved. This paper proposed a novel unified massage passing strategy for semi-supervised graph learning scenario. Pros: The method proposed for unifying feature and label propagation is simple yet sufficiently novel. Pros: This paper proposed a novel framework to more effectively and explicitly utilize the label information by GNN in semi-supervised scenario, and the experiments illustrate its effectiveness in general benchmark datasets.
The authors in the paper describe a deep learning approach to detect copy number variants (CNVs) from DNA sequencing data, CNV-Net. ########################################################################## Summary: The authors proposed CNV-Net, a deep learning-based approach for copy number variation identification.
Based on this, they propose a light and scalable GNN learning framework called LCGNN, which first adopts the local clustering method PPR-Nibble to partition full graph into subgraphs, then use GNN modules on subgraphs for training and inference. The idea is to form local graph for each node using PPR-Nibble, a local clustering method proposed before, and then use transformer on top of the local graph as encoder for node classification and link prediction. ########################################################################## Summary: This paper proposes to utilize local clustering to efficiently search for small but compact subgraphs for Graph Neural Networks (GNN) training and inference. The subgraph extraction uses PPR-Nibble with is a conductance based local clustering method. Minor comments: As the authors mentioned, the local clustering methodology is only reasonable for graphs that has low conductance with respect to all "cluster" (nodes with same label). Detailed comments: The main weakness of the paper is the claim that short random walk is sufficient to extract topology information from graph. The authors claim that 'the locality nature of LCGNN allows it to scale to graphs with 100M nodes and 1B edges on a single GPU' but use 8 NVIDIA Tesla V100 in their experiments. These works show that using merely short random walk is insufficient to fully extract the topological information from graphs and thus the claim by the authors seems questionable to me.
The authors propose a method to train deep generative models on quotient manifolds and show improved performance on some simple standard test sets. Weaknesses: For me, the main problems are the motivation of encoder compatibility, the clarity of the paper and limited evaluation: Motivation of encoder compatibility: I don't understand why the inverse of all generators needs to represented by one (!) single encoder? Summary: The author extends generative models with multi-generators by restricting the generators to share weights and all bias to be regularized in order to enforce that the inverse maps of the generators can be represented by a single encoder.
The paper nicely outlines how PPO and SAC should be configured to work with independent and autoregressive policies under the atomic factorization of the action space. Also, the paper does not set a clear agenda for what would be interesting to see in the results; Is scalability to large action spaces being investigated (in which case, comparison with the non-factored baselines of PPO and SAC should be included)? The authors develop methods based on these two factorization techniques for discrete versions of PPO and SAC (called FPPO and FSAC) and evaluate them on Gym Platform, Google Football, and discretized MuJoCo tasks. ########################################################################## Pros: -Large action space has been one of the bottlenecks for scalability in reinforcement learning and the paper studies factorization for on-policy methods which to my understanding has not been done before. Overall, the experiments show some of the possible potential of factored action spaces with PPO and SAC. Exploring and understanding factored action spaces is important, and the paper presents a reasonable step in this direction, and will therefore provide a foundation for further work. Other methods for learning the independent critic in FSAC are also possible but are not discussed in the paper (see e.g. Ref. Discussion of such a baseline would be very useful in this paper which somewhat serves to summarize deep RL in factored action spaces. Also, the original TRPO paper was used with independent discrete sub-action policies in Atari tasks (if I remember correctly).
This paper considers the problem of private sign recovery for sparse mean estimation and sparse linear regression in a distributed setting. Furthermore, the paper states that this is the first deterministic algorithm with a provable high-probability privacy guarantee. The paper shows that in the sparse mean estimation setting, Med-DC is correct with high probability under some assumptions and Med-DC satisfies a weaker notion of differential privacy proposed by the paper. Using robust estimators (which is deterministic), which gives privacy guarantees with high probability, is exactly the start point of these algorithms. They discuss the privacy implications of these robustness properties, rigorously showing that the deterministic algorithm they define satisfies random differential privacy, where the probability (over the choice of dataset) of not satisfying privacy tends to 0.
Firstly, the analysis is extended to certain ResNet and Convolutional architectures, showing that in both of these cases we can relate the neural tangent kernel matrix to the neural path kernel matrix using a result analogous to Theorem 5.1 in (Lakshminarayanan and Singh, NeurIPS 2020). A recent paper (Lakshminarayanan and Singh, NeurIPS 2020) provided a new perspective on Neural Tangent Kernels for Gated Neural Networks, by decomposing the network into independent paths. This paper builds on recent work characterising deep neural networks in terms of Neural Tangent Kernels and Neural Path Features. The discussion mentions performance when we fixed the input gram matrix to be a constant in the definition of the neural path kernel (and hence define the neural path kernel in terms of the gating structure only), but does not include numerical results for this case. I was not very familiar with the work on neural tangent kernels and encountered (Lakshminarayanan and Singh, NeurIPS 2020) for the first time when reviewing this paper. Overview: The authors examine deep learning from the perspective of kernel methods and demonstrate that convolution layers in these architectures can make DNNs a form of composite kernel learning. Specifically, they show that the value of the neural tangent kernel matrix tends to a constant multiple of the neural path kernel matrix as the width of the network goes to infinity.
Table 1 shows that in terms of mutual information, MixUp < Baseline < CutMix (and < FMix with a very small gap). In this work, authors provide an analysis of mutual information for MSDA and the develop a new variant of mixup. The authors provide an interesting mutual information analysis for different augmentation strategies to describe their motivation.
Paper makes few interesting observations about the influence of low and high frequency components of the Graph Laplacian on GCN performance.
The paper shows that neural networks in such a class allow for more efficient approximation of certain functions than (a) was previously known and (b) is possible using a related class of networks that can be implemented using an entirely classical circuit. If I understood the work properly, the role of quantum computers is to evaluate on a quantum computer the "binary" part of a Binary Polynomial Neural Network (this is what the authors call the acceleration phase), after in a prologue part the data is loaded as initial quantum state with a log-depth circuit.Then, in the epilogue phase, a nonlinearity, like a a ReLU function is applied classically,
Summary: This paper suggests a new decoding algorithm of linear error corrections codes based on self-attention. I like this point, but I think the authors need to show the performance of the other existing schemes (other than the "baseline decoders" of the current manuscript) altogether to easily show the usefulness of the proposed method. In order to select the most promising permutation without running the soft decoding algorithms, two main ingredients are used: (1) node embeddings of the Tanner graph of the code via the node2vec method, Pros: The paper utilizes the self-attention mechanism to improves the computational complexity of permutation decoding. Reasons for score: The main idea of using self-attention for decoding linear error correction codes is interesting. For me, it is not clear why the top k (k>1) performance matters to measure the quality of the decoding algorithm of error correction codes for communication. Despite the previously described issues, I believe the paper presents an interesting method to advance the current state of permutation decoding. Still, the question remains on which permutations (out of the vast set of possible ones that most of these codes allow) select to decode, in a way that we increase the chances of a successful recovery of the transmitted codeword. Summary The use of permutation decoding improves the performance of some types of structured channel codes. In the current manuscript, the authors propose a method to select the best possible permutation for every received corrupted codeword y. Minor comments: It seems that WBP is not defined (is it weighted BP?) The paper focuses on improving the computational complexity of permutation decoding. IN BPL, although it is applied to Polar codes, the authors finally make use of only 5 different permutations, which is not that ineffective, and enable them to report a great performance. This allows increasing the performance in terms of error rate, and preserve the efficiency of the decoding process, up to the overhead introduced by the permutation selection procedure. Still, there are some important aspects, concerning the results, that may require, from my point of view, some further comparison/study (more details in the section ''Questions and additional evidence''): First, I do consider the comparison of GPS + (W)BP vs rand quite unfair. The pipeline presented allows improving previous permutation decoding methods by several dBs at almost no extra cost. Concerns: My main concern is that there is an insufficient amount of reasoning to explain why we use the proposed method over the existing decoding schemes. It will make the authors claim stronger if they explain why these represent linear codes cases (unless the permutation decoding only applicable to the BCH codes).
Paper proposes Hybrid Discriminative Generative training of Energy based models (HDGE) which combines supervised and generative modeling by using a contrastive approximation of the energy based loss Error bars are missing for the classification accuracy experiments in Table 1 which makes it hard to verify improvements especially wrt supervised contrastive loss method Novelty Paper proposes a simple but unified view of contrastive training, generative and discriminative modeling - a nice, novel contribution with empirically strong results Significance Results are compelling across a wide range of tasks over existing (EBM) baselines including calibration, robustness, OOD detection, generative modeling and classification accuracy ================================== Summary of the reasons why I vote for rejection: The main contribution of the paper by connecting supervised learning and contrastive learning is overclaimed.
Summary In this paper, the authors propose a novel approach for quantifying the statistical significance of binary masks predicted by a subclass of deep neural network (DNN) models for image segmentation problems. The authors propose a framework of selective inference to rigorously study the problem of finding significant attention areas of a neural network model (with certain constraints). To address this problem, the authors developed a novel homotopy method under the Selective-Inference(SI) framework, which can derive the exact sampling distribution of the DNN-driven hypotheses. Instead, the authors propose to leverage recent advances in the field of selective inference to derive the null distribution of the test statistic conditioned on the DNN's output (plus additional constraints to get rid of nuisance parameters).
How were hyper-parameters chosen for the DDQN agent, and can the authors comment on whether such choices can have reasonably strong interplay with the transferability of perturbations? While one algorithm may perform a bit better, if the emphasis of the paper is to measure the extent of adversarial perturbations, it seems like it may paint a clearer/more convincing picture if a simpler algorithm is used, with fewer moving parts to attribute performance differences to. — idea: A framework composed of 6 different adversaries is proposed using which it is claimed that the transferability properties among Atari environments can be studied. Summary of Contributions The paper explores adversarial perturbations in deep RL, providing a new thread model where the perturbation is computed based on a single state. In the experiments section, authors have used a pre-trained DDQN agent which originally does not show significant generalizability compared to methods like A3C. The authors studied how perturbations on states would affect the performance of deep reinforcement learning. I do think the paper falls a little short in that there wasn't a representative sample of deep RL methods, as well as not commenting on design choices made and how/whether they might interplay with the transferability measured.
I am unaware of this definition of robustness of a loss function as it seems very specific to the complementary label learning problem. In particular, the paper proposes a robust loss function and an algorithm for learning from complimentary labels. Summary: This paper deals with the problem of complementary label learning, that is, when we know the set of labels which a given observation does not belong to. However, in this paper it means if the loss function with ordinary and complementary labels has the same minimizer. This paper studied a new problem, that is, learning from complementary labels. The paper presents (two) simple yet insightful sufficient conditions for a usual loss to work well as a complementary label loss.
This paper proposes an approach that adaptively decides when to update the simulation policy, based on the difference between it and the current learned policy. Summary : Authors proposed an algorithm for switching policies while using Deep Q-learning. The approach The paper investigates different simple switching criteria based on deep Q-networks that are used as baselines and it also proposes an adaptive approach based on the feature distance between Q-networks. In the RL context, this paper aims at designing a generic solution for reducing the number of policy switches during training (called switching cost) while maintaining the performance.
This paper studied a simplified image classification task with orthogonal non-overlapping patches and is learned by a 3-layer CNN. My major concern about this paper is that the theory seems to only work under orthogonal patterns and non-overlapping filters, unfortunately, neither of which is true in practice. This paper didn't prove PSI holds for SGD except in a toy example (two training points). Theorem 6.1 needs to assume that PSI holds and spurious patterns are unbiased; under these assumptions, the result is pretty straightforward. The authors observed pattern statics inductive bias (PSI) in experiments.
Summary: The paper proposes a new algorithm for multi-agent reinforcement learning (MARL) that adaptively picks learning rates for actor and critic. It proposes AdaMa, an algorithm which balances the learning rates of actors and the critic, and can also make use of the second-order information. The paper studies adaptive learning rate for Actor-Critic style MARL algorithm. This paper proposes an algorithm called AdaMa for multi-agent reinforcement learning (MARL). This paper proposed AdaMa, which can automatically use adaptive learning rates for each agent in cooperative Multi-Agent Reinforcement Learning (MARL).
The proposed approach follows the same recipe in PVI where local agents learn their own model posteriors from private data, and communicate their posterior representations to a server, which aggregate local posterior representations into a universal representation. NOVELTY & SIGNIFICANCE On the high level of idea, this paper presents an interesting perspective on a practical federated learning system: communication trade-off & trustworthy prediction. REVIEW SUMMARY In short, this paper presents an interesting perspective on non-parametric probabilistic federated learning via particle representation of posterior. On the practical aspect of this paper (i.e. communication load & trustworthy prediction), while the demonstration is sufficient against point-estimate method such as FedAvg and DSGLD, there is no comparison against other non-parametric probabilistic methods such as PVI (Bui, 2018) and/or PNFM (Yurochkin, 2019). In my opinion, there is a relevant limitation to this approach which, although acknowledged by authors, is not properly discussed: Federated updates can only be done by one of the agents at a time, which implies that this particular algorithm is of limited practical use.
However, the authors appear unfamiliar with other opponent modeling work (I'm particularly familiar with the computer poker domain, although they cite two Ganzfried and Sandholm papers from that area) where there is a rich literature of opponent modelling being successfully used under these conditions (opponent observations needed only at training time, and never at execution). - some minor clarity points on the environments, a larger (but easily fixable) point about consistently describing when opponent observations are and aren't needed, suggestions for experiments I would have loved to see to verify basic robustness, and some notes about missed related work that also occupies this setting (use opponent info for training, never at execution). This paper considers an algorithm for learning and using an opponent model that is only conditioned on an agent's local information (history of actions, observations, and rewards). These are just some of the works that I'm most familiar with; the computer poker community has been studying this topic for a while, and under the conditions described in this paper (opponent observations available during training, but never at execution time). +++ Section 4.2 "In Sections 1 and 2, it was noted that most agent modelling methods assume access to the opponent's observations and actions both during training and execution. In general, the authors should be more careful about specifying "at execution time" whenever they claim their technique does not need opponent observations, and should probably be aware of more related work in this setting. The ongoing challenge in that community (with success against human professionals!) is to go further and perform opponent modelling without needing opponent obervations at any time, by using only the agent's observations at training time as well as execution time. Again, this is rhetorically purer than the approach described in this paper, in that it never needs access to opponent information at training or execution time. Of course opponent observations should not be used at execution time, and training a VAE to recover the missing information seems like a good approach. The original A2C paper describes only taking Obs as input, but in the practical A2C implementations in the codebase I use, the last timestep's reward and last timestep's action are often used as inputs (concatenated on after the Obs convolutional layers, before the LSTM) as these "observations" often aid the agent's learning and asymptotic performance. Negatives: The paper is sometimes inconsistent in its description of techniques that need opponent observations at training time versus execution time. A variational autoencoder (VAE) is trained to predict opponent observations and actions, given the agent's local information.
The proposed method trains the policy using soft actor-critic on training tasks and trains an adapter network to predict task-specific final linear layer from a single timestep transition (s, a, r, s'). Summary: This paper proposes a novel meta-RL algorithm called Fast Linearized Adaptive Policy (FLAP), which can adapt to both in-distribution and out-of-distribution tasks. Although the idea of FLAP is simple, the strong experimental results have demonstrated that by predicting weights rather than optimizing, FLAP is a fast and effective meta-RL algorithm for adaptation to OOD tasks compared to previous approaches that did not focus on this area. This paper proposed a meta RL algorithm built upon an assumption that a shared policy with a task-specific final linear layer can maximize expected return for each task. This paper claims that the proposed linear representation meta RL results in a better generalization to out-of-distribution tasks. For example, it would appear that the adapter network should not work on sparse rewards tasks (where the transition function is unchanged between tasks) or other situations where most experience tuples do not provide information regarding the specific task. The other module is the Adapter Network, which generates task-specific policy weights from the sampled transition tuples.
This paper presents a theoretical scenario where point-wise measure of adversarial robustness falls short in comparing model robustness, then conduct experiments to show that robustness curve is a more meaningful evaluation metric from a global perspective. The authors argue that robustness curves allow to compare "global robustness properties and their dependence on a given classifier, distribution and distance function". The authors introduced the recently proposed robustness curves to provide a global perspective including the scale as a way to distinguish small and large perturbations. Other Questions or Comments: Most of the existing defenses against adversarial examples are typically trained using a specifically-chosen perturbation strength. Summary: The authors advocate for the use of Robustness curves, plotting the adversarial accuracy as a function of the size of the neighbourhood region of allowed perturbation. [1] Reliable Evaluation of Adversarial Robustness with an Ensemble of Diverse Parameter-free Attacks, Francesco Croce and Matthias Hein, ICML 2020 Summary: The paper showed that point-wise measures fail to capture important properties that are essential to compare the robustness of different classifiers. One thing that I would recommend the authors is to make clearer the distinction between robustness curves as they described them (based on finding the closest adversarial example) vs. If adopting robustness curves (or global robustness) as the evaluation criteria instead of point-wise robustness, how will this affect the existing adversarial training procedure? I mainly agree with the authors on the argument that point-wise measurement of robustness may be insufficient in explaining model robustness.
The main theoretical result comprises the population case, where infinite amount of data is sampled from the described generative model under an additional anchor word assumption, and proves that in such case the outcome of the proposed contrastive learning procedure is linearly related to all moments of the topic posterior up to a cerain degree. The algorithm is further compared on the AG news topic classification dataset with standard benchmarks and it is observed that the performance of the proposed procedure is good and overall comparable with word2vec, being slightly better in the lower sample size setting. I like the idea of  using topic models as a way to represent the document level information, but it is disappointed to see that the proposed method doesn't provide as good performance as the simply averaging word embeddings. It proves that the proposed procedure can recover a representation of documents that reveals their underlying topic posterior information in case of linear models. Synthetic experiments illustrate performance of the proposed procedure when the data is sampled from a topic model with varying degree of sparsity. Evaluation: Overall, the paper provides a thorough theoretical analysis to prove that the approach allows to recover topic posterior information. It is experimentally demonstrated that the proposed procedure performs well in a document classification task with very few training examples in a semi-supervised setting. The main strength of this paper is suggesting a sound and straightforward learning algorithm to construct document representation. And the authors also show the usefulness of the representation in semi-supervised learning by classification performance and visualization.
This paper proposed to learn a regression model using "skewed data", which is defined as the subset of training samples with true target above certain threshold. This paper proposed a semi-supervised learning approach to improve the regression model trained on output-skewed data. ########################################################################## Cons: (1) The proposed approach is based on the assumption that labeled data are skewed and unlabeled data follow the assumed true distribution. (1)     The paper assumes that the training data are often highly skewed (intentionally) but the true distribution of the output can be easily estimated or obtained. Experiments suggest that the proposed approach increases the accuracy of the regression model for all the four datasets considered in the paper. This method contains an adversarial network that forces the regression output distribution to be similar to the assumed true label distribution. ########################################################################## Summary: The paper presents a novel approach to improve the accuracy of regression models that are learned from a skew dataset. This method contains an adversarial autoencoder that propagates the information from the assumed true distribution to the latent vector of the regression model. It's not clear how p(y) was estimated from the labeled dataset, where only samples with true target above certain threshold are available. (2) In the second paragraph of Section 4.3, it is claimed that the proposed approach relies strongly on the assumed true distribution.
ii) Graph Neural Networks with Generated Parameters for Relation Extraction, In ACL'19 The authors propose a method for simultaneously learning the graph structure (or a graph generative model) and the parameters of a GNN for node classification. The paper proposes a way to use self-supervision with denoising autoencoders to improve learning of the graph structure for GNNs. The approach is compared with a number of recent approaches from the literature. "Self-supervised Training of Graph Convolutional Networks." arXiv preprint arXiv:2006.02380 (2020) This paper considers the problem of nodes classification with few labeled data and missing graph structures. The approach addresses the highly relevant problem of learning graph structure with GNNs. The paper is well written, with clear motivation and an informative summary of similar prior works. Specifically, comparisons with baselines show that the proposed model can alleviate the problems of previous works such as sensitivity to the similarity metric and high-quality initial graph structure, memory problems, etc. Motivated by the fact that GNNs tend to perform poorly in the absence of the graph structure, the paper proposes a self-supervised framework for generating the graph structure,
The paper proposed (1) to use a graph neural net to predict the performance of network architectures, (2) to query new architecture proposals to try next from the predictor, and (3) to iteratively refine the predictor using the collected dataset using gradient descent. GOAL demonstrates superior performance compared to SoTAs. Weakness: -First, the method is quite similar to NAO, where an encoder and decoder approach the maps neural architectures into a continuous space and builds a predictor based on the latent representation. The only difference is that NAO is use a decoder to decode the optimized latent representation back to architecture representation while here GOAL applies gradient descent on a graph neural network.
Strengths The paper provides empirical experimental results which suggest the superiority of the proposed method against two baselines: Learning to Sample (LTS) and random selection (RS). The proposed approach is a two-stage method involving candidate selection (learning a function ρ to determine a Bernoulli probability for each input) and AutoRegressive subset selection (learning a function f to generate probabilities for sampling elements from a reduced set); both stages use the Concrete distribution to ensure differentiability. This paper proposes a stochastic subset selection method for reducing the storage / transmission cost of datasets. The paper includes a broad set of experiments, but I have some questions and concerns regarding the baselines and the proposed SSS method.
Comments Section 4: I am not sure what is the downstream justification for the notion of spectral approximation studied here (relative condition number) in the context of directed graphs, and it would help if the authors could elaborate on this. Authors propose a novel method to approximate a given directed graph with a´nother one (the sparsifier) which has fewer edges. Section 6: The experiments in the main text concentrate on measuring the relative condition number of the sparsifier w.r.t the original graph as per the sparsification notion studied in this paper, but as written above it is not clear what do we actually get out of the sparsifier. The authors also evaluate the embedding given by the proposed symmetrized Laplacian in the context of clustering directed graphs, where the majority of existing spectral methods tend to underperform.
This submission proposes a training strategy that leverages background/noise data to learn robust representations. To improve multi-class classification problem using DNNs, authors propose to do multi-task learning of solving another auxiliary tasks which tells if data points are just noise/distractors or not. A critical point, that is missing from this paper, is how would this data augmentation work by itself without the auxiliary classifier. Part of the proposed method is building an extra class of training data that are background or noise. The auxiliary classifier is designed to encourage the early layers to learn more meaningful features, and Section 3 supports this relation. In other words, neither training nor inference of the proposed method utilizes the content of Section 3. This paper proposes a training method for classification, with the goal of training with less data.
[+] Propose a method of applying TRUST-TECH to find local optimal solutions (LOS) of DNNs[+] Introduce DSP for exploration in high-dimensional parameter space[+] High ensemble performance through DSP-TT The proposed method is somewhat novel in the aspect of suggesting TRUST-TECH for an ensemble of DNNs. However, it is difficult to give a high score because the experimental results do not support The authors propose a method to obtain multiple local optimal solutions around the existing one. This paper  proposes an intersting Dynamic Search Path TRUST-TECH training method for deep neural nets. ########################################################################## Pros: While being simple and intuitive, the proposed method appears to succesfully and efficiently identify multiple high-quality local optima of a model.
To address this task, the paper proposes a method consisting of three steps: (1) detecting out-of-class samples in the unlabeled set, (2) assigning soft-labels to the detected out-of-class samples using class-conditional likelihoods from labeled data, and (3) using auxiliary batch normalization layers  to help mitigate the class distribution mismatch problem. Request for author response: I would like to see how the authors compare their paper with [a] in terms of problem setting, idea of class-wise similarity and representation learning. ########################################################################## Summary: The paper proposes a new approach for open set semi-supervised learning, where there are unlabeled data from classes not in the labeled data. It would be great if the authors can talk about whether the semantics-oriented similarity representation in [a] could be used (and how to use it) to help improve the performance of the proposed method in the setting concerned by the paper. 2020) in the open-set SSL setting but the results of the ablation study suggest the level of improvement achieved by this normalization is negligible and the most of the improvement comes from more accurate detection of out-of-class samples through using the projection header function introduced in SimCLR paper. While the setting considered in [a] (for metric learning problems) is a bit different from that concerned in this submission (for image classification tasks), the high-level idea (learning representations that can be used to describe unlabeled images with labels different from those in the training set) is very similar. This paper considers the problem of semi-supervised learning, where the unlabeled data may include out-of-class samples. Detailed Comments: The paper uses contrastive learning idea proposed in SimCLR (Chen et al. ########################################################################## Pros: The paper addresses a very interesting and practical problem in semi-supervised learning, where the unlabeled samples may include out-class samples. In order to handle out-of-class samples in D_u, the authors present the idea of learning class-wise prototypical representation based on the above contrastive features. Class conditional likelihoods has been shown to be not very useful in detecting out-of-distribution samples in cross-entropy loss learning.
Specifically, the authors propose an algorithm with transition model approximation and analyze the regret when adopting RKHS or function classes with bounded Eluder dimension.
Theory is provided for the case of synchronous symmetric averaging methods, and the paper is complemented with detailed experiments on CIFAR and tiny-ImageNet. This is a nice contribution to the growing literature on decentralized training for deep neural networks. The authors identify the consensus distance as the key factor that affects the generalization performance of decentralized training. (Th1 is based on previous work.) The other results, e.g., remark 2, proposition 3, and lemma 4 cannot claim how the consensus distance affects the generalization error. I believe  Remark 2 is a statement that needs to proven, as represents the main issues addressed in the paper, namely, how consensus affects convergence rates. The authors consider the decentralized optimization problem and explain the generalization gap using the consensus distance. Are there connections between consensus distance and other quantities that have been considered in the literature to relate training to performance (e.g., gradient diversity as in Yin et al. On the theory side, the main contribution is Remark 2 and proposition 3, but why remark 2 relates to generalization are unclear (it only shows the convergence rate), neither does proposition 3 (it only shows the consensus distance). It focuses on the so-called "critical consensus distance" and how disagreement during different stages of training ultimately effects optimization (training loss) and learning (generalization error). In summary, I don't think the theory part is very strong in this paper, and the relation between the critical distance and the generalization error needs to be further justified. Now that we need to potentially perform multiple rounds of gossip between each optimizer update, are decentralized methods still attractive for reducing overall training time?
Summary: This paper proposes a self-supervised learning framework for 3D object classification and retrieval based on multi-view representation, where a sub-task of transformation estimation is adopted as a regularizer. This paper proposed a self-supervised learning method of 3D shape descriptors for 3D recognition through multi-view 2D image representation learning. (AVT) Another concern was critical but not yet addressed neither: The authors could propose a method that can be developed based on the "3D Transformation Equivariant" to 3D objects directly instead of its 2D projections. For instance, the authors could propose a method that can be developed based on the "3D Transformation Equivariant" to 3D objects directly instead of its 2D projections. I would urge the authors to check both papers below, and it clearly defines the Transformation Equivariant Representations learning by Autoencoding Variational Transformations, which could be applied for various types of data. For this question, I think the authors should prove the Transformation Equivariant Representations directly on a 3D object (point cloud, voxel, 3D mesh) instead of multi-view 2D images. The Unsupervised Learning of Transformation Equivariant 2D Representations by Autoencoding Variational Transformations is used for 3D shape descriptor learning, which the authors claimed as "self-supervised" learning. -Secondly, since it has been suggested by many previous work that the joint-training of multi-task is helpful for the network, it will be appreciated that the authors provide more analysis and discussion on how the MV-TER loss helps the network learn transformation equivariant representation than simply using rotation as data-augmentation or using pose-estimation as sub-task. The authors propose a self-supervised learning technique for multi-view learning based on a simple intuition that the transforms of the 2D views of a 3D object will be in an equivariant manner as the 3D object transforms.
The paper presents a model for entity grounding from its textual description for a text-based language game. In several places in the paper it is mentioned that the model learns "mapping between entity IDs in observation space and their symbols in text entirely through interaction with the environment." I am not sure what interaction means here. In order to study this problem, the authors use a new environment and dataset of natural language descriptions, as well as propose a self-attention model that matches entities to relevant sentences. The proposed RL framework achieves reasonable performance in domain games (training & test are from the same games) and also has a strong zero-shot generalization to unseen games and "entities" (thanks to the parameter sharing and multi-task learning). The general idea is to learn a parameterized policy model that inputs the pair (entity [this paper] or visual representation [1] and text) and outputs the action of the agent in the game (correct me if I am wrong). The paper considers the task of training an agent to act following a manual expressed in natural language.
Summary: The authors propose a training-free way of estimating the performance of a deep net architecture after training using correlations between linearizations of the network at initialization for different augmentations of the same image. ** Summary The paper mainly introduces a metric to benchmark the performance of neural networks without training – the correlation of Jacobian subject to different augmented versions of a single image. The authors show that with this metric, they can find architectures with reasonable accuracy on CIFAR-10/CIFAR-100 in the NAS-Bench-201, while using much less search cost compared to previous NAS methods. It will also greatly strengthen this work if the authors can show the effectiveness of the proposed metric on a more realistic search space, e.g., the DARTS search, and evaluate the found architecture on a larger dataset, e.g., ImageNet. Justification of rating Summary This paper attempts to infer a network's accuracy at initialization without training it, which can speed up neural architecture search and greatly reduce the search cost.
First of all, the manuscript proposed an adaptive personalized federated learning algorithm, where each client will train their local models while contributing to the global model. In this paper, the authors propose a variant of FedAvg that not only produce the global training, but also a mixture of the local model and the global model, which is called personalized model. They later introduce an adaptive optimization algorithm for their formulation and provide generalization bounds as well as convergence guarantees for both strongly-convex and nonconvex settings. ==================================================== This paper considers the problem of personalization in federated learning. The authors propose a new framework in which they replace the common global model in the original federated learning formulation with a convex combination of the global model and a local model. It provides generalization bounds, optimization results for both strongly convex and nonconvex functions, as well as experiments and comparison with other works.
Summary: This paper presents a method to train a neural network to predict the time-dependent costs, and start and goal states needed to run time-dependent shortest-path planning in a dynamic 2-D environment. STRENGTHS The general idea of integrating BlackBox combinatoric shortest path algorithms in a differentiable planning module is interesting and has a lot of potential to be useful. Overall, the paper could use more detail on the architecture, the hyperparameters, the baselines, the domains, and anything else needed to get this to work. SUMMARY This work proposes a novel neuro-algorithmic policy architecture for solving discrete planning tasks. Evaluations are presented on 2-D time-varying games where the addition of the path-planner is shown to improve performance over an imitation learning and PPO baseline. While planning, in general, is an important problem and differentiable planners are an important research topic, the motivation of this work is not clear. Weakness: i) The manuscript is missing very relevant pieces of works that should have been discussed in detail and included as baselines in the experimental results section.
Basically, the proposed method adds each edge in GNN in a greedy way by evaluating its causal effect on the prediction. The proposed method is a greedy approach which starts from an empty graph and gradually adds the next edge by minimizing the difference between the outputs using mutual information. This paper proposes a method, called Causal Screening, to improve the interpretability of GNN. The proposed "Causal Attribution of A Super-edge" is very useful for large-scale graphs, which can reduce the computational cost. Overall The paper proposes a different approach compared to the currently-existing explanations for graph networks. The proposed method, Causal Screening, iteratively adds edges into the explanatory subgraph. Summary The paper proposes a procedure for identifying a subgraph GK of a given size K (measured by the number of edges) whose output through the GNN function f is as close as possible to that of the full graph G. Summary: The paper introduces a novel method called Causal screening which takes a graph and the prediction made by a GNN, and returns an explanatory subgraph. This work proposes to explain graph neural networks from a causal effect view. Does table 1 contain the results of the standard causal screening or the cluster-based one?
This paper proposes an approach to reducing the sample complexity in multi-task reinforcement learning using permutation invariant policies. I feel the paper could have been better presented by starting with a motivating example where the permutation invariance property holds - for example the portfolio optimization example studied in the experiments. The authors identify a key property in the targeted resource allocation problems -- the permutation invariance -- which intrinsically implies the independency of samples at different time steps. In particular, they present an algorithm that exploits permutation invariance, study its theoretical properties, and propose examples where this property holds and their algorithm can be leveraged. The setting considered in the paper is one where the state is a concatenation of various entities, while the actions are the fraction of resources allocated to each entity. This paper addresses sequential resource allocation problem using reinforcement learning, where sample efficiency is the focus of the paper. Then, the proposal is to learn a single permutation-invariant policy which will perform well on all of them simultaneously. The paper proposes an RL algorithm for multi-task learning. The main premise of the paper is that certain families of tasks exhibit approximate forms of symmetry, i.e., applying a permutation to the state/action variables would make all tasks similar in some metric sense. The main assumption the paper makes is permutation invariance (PI). The empirical results on the task of sequential portfolio optimization shows that this approach performs better than the policy of constantly rebalanced portfolio. I didi not understand how a network trained using gradient descent alone would satisfy permutation invariance.
Summary This work proposes an approach to update feedback weights in DFA using modification of kolen-pollack method, which helps in training deep CNN network. This paper introduces a new method for computing the backward updates of a neural network called Direct Kolen-Pollack learning (DKP). First Review Citation missing for key work on assessing the scalability of bio-inspired approaches and highlighting key limitations [Bartunov 18], variants of DFA [ Moskovitz 18, Frenkel 19]  and recently an approach similar to DFA with target projection known as LRA (also has similarity with Direct Kolen-Pollack) showing promising performance on deep CNNs [ Ororbia & Mali 2020]. For LRA Delta_b(update for error weights)  = learning rate *( teaching signal(error_k) * post-activation from layer below (a_(l-1))
The authors use group theoretical constructs such as shift and rotation operators to show that a latent space representation should be equivariant such transformations. An alternative definition of disentanglement is given, where instead of confining the effect of each transformation to a subspace, an operator is used that acts on the whole latent space (this operator is chosen as a shift operator, which works for cyclic groups). Instead the authors put forth the idea of transformations of data that are equivariant to the latent space representation as a formulation of disentangled factors. Even if one can question whether Def 1 is a good formalization of disentangling, the paper does show empirically that it is easier to learn an equivariant encoder/decoder when the latent operator is a shift operator or a diagonalized complex version of it, rather than a disentangled operator (with one 2x2 rotation matrix block and an identity block; Moreover, it seems the authors suggest the definition of disentangled representations proposed in [3] requires that subspaces corresponding to factors of variation are single dimensional (section 2) which is not the case, please clarify.
The authors present conditions that need to be satisfied by the encoder and decoder parameters, and show empirically that the regularization terms that they propose ensure that the resulting autoencoder has an isometric decoder. The authors propose a new version of the regularized autoencoder where they explicitly regularizes its decoder to be locally isometric and its encoder to be the decoder's pseudo inverse. Is there any possibility that the author can provide one more toy example for the global isometry when the data lie on some manifold shape? Regarding the experiments, indeed the authors successfully show the IAE converges its decoder to be an isometry and the proposed regularizer promotes more favoured manifold. The paper suggests a novel auto-encoder based method for manifold learning, by encouraging the decoder to be an isometry and the encoder to locally be a pseudo-inverse of the decoder. While the authors assert that forcing the decoder to be an isometry is desirable since isometries preserve distances and angles, it is not clear why that is a desirable property while modeling data on a manifold. They also experiment with "real data" (e.g. MNIST), show the merits of the proposed algorithm when visualizing the 2 dimensional bottleneck of the autoencoder. In the experimental part, the authors compare the merits of this approach on synthetically generated low dimensional manifolds in high dimensional ambient spaces, against other standard manifold learning algorithms, and show that the paper's method outperforms other method using a measure of distortion of triangle edges on a grid. Strength: This paper provided a novel method to train a local isometric autoencoder, which can preserve the local Euclidean distances well between the original space and the latent space. Distances between points on a data manifold are not usually measured through L2 distances in a latent dimension, and it is not clear why one should require that L2 distances in the high dimensional space are the same as distances in the latent space. Regarding the motivation and the math, I like the idea of isometric regularizer preserving the geometric properties in the learned manifold.
In the paper, the authors proposed a novel privacy-preserving defense approach BAFFLE for federated learning which could simultaneously impede backdoor and inference attacks. To impede backdoor attacks, the Model Filtering layer (i.e., by dynamic clustering) and Poison Elimination layer (i.e., by noising and clipping) were presented respectively for the malicious updates and the weak manipulations of the model. Using this clustering algorithm combined with adaptive clipping and noising, the proposed method can mitigate the backdoor attack. Weaknesses: This paper does not provide any theoretical guarantee and only applies the existing methods to mitigate the backdoor attacks. This paper's main idea to defend against a backdoor attack is to use clustering and adaptive clipping and noising. To impede backdoor attacks, many models are marked as outliers and discarded, clipped, and noised, generally speaking, which could lead to performance degradation. Under the backdoor attack, an adversary can manipulate a few clients' weight matrices to affect the final global model. minor: please attach your main context pdf in the submission and submit the appendix in the supplementary material The paper proposes a backdoor-resilient federated learning method to defend the backdoor attack of poisoning the models. The author(s) have created many splendid terms to describe the modules used in this work, however, their implementation uses both clustering and median, which is very engineering and may not reliable with a different clustering algorithm or data set is severely unbalanced (just like the non-iid data sets among clients). This paper provides an interesting research direction for the cross-domain of federating learning and backdoor attacks. Moreover, BAFFLE, combined with the Secure-Two-Party Computation method, can protect the FL model from Inference attacks. Strengths: This paper uses an existing clustering algorithm (the HDBSCAN clustering algorithm (Campello et al., 2013)) that works best in the FL problem in identifying manipulated weight matrices. This paper suggests a new solution to protect FL models from backdoor attacks.
General statements This paper introduces an interesting parallel between SDEs and GANs, and pushes the analogy to its practical implications as a way to learn neural SDEs. Globally, I found the paper a very good read, although it sometimes lack the details that could be useful for a non-specialist. I feel the Section 3 is more related to computational issue rather than the GAN formulation of neural SDE and can be individual interest? I would like to see more show-cases on the performance of the proposed algorithm on learning given SDE with some simple drift and diffusion term if possible, that may better demonstrate the effectiveness compared with the dataset with unknown drift and diffusion.
Short summary: The paper introduces a promising new method, hierarchical nonnegative CP decomposition (HNCPD), as well as a training method for the HNCPD, neural NCPD, for topic modeling problems. It seems like the authors combine existing ideas (hierarchical and neural NMF, NCPD) into a new method. In particular, in the discussion on approximation in Section 2.1, it is not clear if this idea is used in the implementation, which might make it difficult to replicate the results. SUMMARY: This paper presents a hierarchical nonnegative CP tensor decomposition method. The last sentence in Section 3.3 help explain the benefit of a hierarchical method. In the experiments, do you use this approximation, or do you use the NCPD combined with HNMF for each factor matrix as discussed in the beginning of Section 2.1? In Section 2.1, I think the second paragraph is clear until Equation (7), but the rest of the paragraph is difficult to follow. For example, it's not clear to me how the columns of the hierarchical NMF factors are used to form NCPDs of ranks r(0), r(1), ..., r(L−2). Summary: In this paper, an extension of nonnegative CP decomposition called hierarchical nonnegative CP decomposition (HNCPD) is proposed. Potential Improvements: Equation (4) introduces the forward propagation for a NNMF, which is crucial for the HNCPD and Neural NCPD. It is not clear how the Standard NCPD in Section 3 is computed. Overall Evaluation: The paper is well and clearly written, the significance of this work, however, needs to be further proven by experiments.
The paper proves a universal approximation theorem for equivariant maps by group convolutional networks in an extremely general setting. The main issues (that are detailed below) are: the paper does not sufficiently relate the discussed model or the universality results to previous or concrete models (set and graph NN, equivariant group NN, other unused but potentially useful variations), it does not provide sufficient explanation and justification to the different conditions in Theorem 11, there are some details in the proof and the description of Theorem 11 which are missing/unclear, Theorem 16 has some unclarity. Universal approximation theorems are considered to be an important kind of result, and this paper proves a very general one for equivariant maps. Summary: The paper studies the approximation power of equivariant neural networks in a very general setup: the input space is assumed to be a function space (compared to a finite-dimensional space in standard setups) and the group is assumed to be any locally compact group. If I understand correctly, the idea of the proof is basically using the extension mechanism of Theorem 3 to extend mapping to functions over base domains to equivariant mappings. In more detail, the paper first characterizes  equivariant maps as the unique  extensions of "generator", namely regular maps that provide target functions (or vector) defined over a basic domain. This paper considers a certain generalization of convolutional neural networks and equivariant linear networks to the infinite dimensional case, while covering also the discrete case, and offers a universality result. What can be said about the equivariant tensor models and graph neural networks using the universality result in this paper? Does theorem 1 implies that Deepsets (as equivariant model) is universal (as was proved in several previous works)? Universality is then proved for the generator by a fully connected network, and separately an approximation theorem of FCNs by CNNs is proved. This is then used to derive universal approximation theorems from the well-known results on fully connected networks. The paper discuss a rather broad generalization of equivariant network (equations 4 and 5). Second, infinite dimensional fully connected networks (FNNs) and general (equivariant) convolution neural networks (CNNs) are described. Specifically, these works show that in some cases high order tensors are needed for universal approximation by invariant neural networks - can this result be recovered from your results?
The main conceptual point of the paper is simply that the weight decay should have the same effective learning rate as the gradients, it would be nice if the authors could make this more clear and more central. The stable weight decay property can be defined in dimension 1 as follow: the effective learning rate represents an amount of time ellapsed between two iteration. The authors propose to adjust for this by having the effective learning rate multiplying the weight decay term, ie Delta theta= -eta_{eff} lambda theta-eta_{eff} F(gradient). In this paper, the authors study the effect of weight decay across different optimizers. The paper main point then is to equate the learning rate in the weight decay coefficient by the effective learning rate and they show that this might be enough to bridge the gap between Adam and SGD. Summary: This paper presents a novel framework that alters the weight decay update rule and aims to improve generalization when applied to 1) momentum-based optimizers and 2) adaptive optimizers. Summary In this paper the authors introduce the notion of stable weight decay. Review The reformulation introduced in equation (3) is an interesting alterntive view to look at weight decay, but I find it is not really used by the authors. Summary: This paper presents a novel weight decay regularization, stable weight decay by using the bias correction to the decoupled weight decay in adaptive gradient descent optimizer. Additionally, I am a little confused about the statement that the effect of weight decay can be interpreted as flattening the loss landscape of θ by a factor of (1−ηλ) per iteration and increase the learning rate by a factor of (1−ηλ)−2 per iteration. When applied to Adam, the authors derive AdamS, supposed to work better than the previous AdamW, which already improved how weight decay and Adam interact. A minor point, in Eq 3, how did the authors arrive at −2t−1 in the superscript of weight decay rate, not −2t+1? They also make comments about reinterpreting weight decay as flattening the loss and increasing the learning rate which they don't pursue further nor connect with the main point and it seems a little out of context. Comments and questions: I think the concept of weight decay rate and total weight decay should be explained better, and earlier in the paper. The authors show that after correcting for the effective learning rate, which they call AdamS, they can get the same performance as what one gets with SGD in CIFAR10 and CIFAR100. Stable doesn't just mean constant, though I can completely understand what the authors really meant in the paper. The authors introduce the notion of stable weight decay and seem to right away assume it is a desirable property. In particular, the authors should strive to provide experiments on different training sets (ImageNet) with learning rate cross validation. In the draft, the authors can directly say constant or time-varying weight decay. The proposed stable weight decay can fix this issue in terms of dynamical perspective.
This paper presents an estimator that predict higher-order structure in time-varying graphs. Strengths: For the problem of predicting higher-order (incremental) structures in an evolving graph, authors propose a method which predicts formation of higher order simplices from existing ones, while capturing substructures between vertices of the simplices. More specifically, authors propose a kernel estimator which predicts the evolution of the graph through simplices and how they evolve with respect to some local neighborhoods. Weaknesses: Section 4, which is a critical part of the paper as it presents the theoretical guarantees with respect to the proposed estimator, is rather complicated and lacks explanations of theorems, definitions and assumptions.
The proposed tensor network (TN) based text classification model looks new and interesting. The authors claim that when combined with BERT for word embedding, the proposed model can outperform the SOTA methods.
The authors investigate different tokenization methods for the translation between French and Fon (an African low-resource language). Overall: Very interesting work, and can't wait to see this data be used :-) I think the paper could be greatly strengthened by taking some time to include an example that demonstrates the linguistic and typological features of Fon that makes it difficult. That being said, I strongly agree with the authors that neural machine translation of African low-resourced language is important. For example, if you could give us one or two sentences of Fon in the beginning of the paper, that demonstrate some of the difficulties of the language, I think this would greatly strengthen the motivation.
The proposed method improves ASR performance over model using the baseline GMM attention which does not take source-content into account,  generalizes better to input sequences much longer than those seen during training, while also obtaining competitive performance to other streaming seq2seq ASR models on "matched" test sets. Reasons for score: The main contribution of this paper seems to be the addition of predicted weights for each encoder step that allow the monotonic GMM attention mechanism to ignore certain input frames. High-level Comments: Despite being a relatively minor tweak, the use of source-aware weights for each encoder step is a cool idea that integrates nicely into existing GMM-based attention mechanisms. Experiments are somewhat incomplete/missing important comparisons, e.g. comparing baseline GMM attention to the proposed "source-aware" variant in Tables 1,2,5, and comparing to other location-based attention mechanisms, even if non-monotinic, e.g. from http://papers.nips.cc/paper/5847-attention-based-models-for-speech-recognition.pdf This paper proposes a novel Gaussian mixture-based attention mechanism by incorporating the source (key) information, enabling a flexible representation of the Gaussian attention pattern with the well-described formulation. Detailed comments: In the introduction, "The GMM attention is a pure location-aware algorithm in which the model selects attention windows cumulatively without considering source contents.": This is a little bit hard to understand.
The paper presents an improved positive sample selection and data augmentation method for unsupervised, contrastive representation learning. Positives: interesting proposed method for expanding the neighborhood space of considered positive matches for contrastive learning ablation study provided to show the improvement from each proposed component (sample selection, cutmix, multi-resolution) ## Summary The paper addresses the problem of contrastive representation learning, and proposes a new data augmentation, dubbed CLIM, that leverages similarity between images. -as it is standard in contrastive learning-, positive pairs are generated using those similar images to the anchor image: after clustering the representation space using k-means, the nearest neighbours that are closer to the corresponding center of the cluster where the anchor belongs to are selected. Authors propose two improvements to be used in contrastive representation learning: a positive sample selection scheme (called center-wise sample selection), that improves over previously proposed kNN or k-means methods; and multi-resolution data augmentation which is an extension of a known crop augmentation technique with multiple scales. Two components are proposed: First, to select semantically similar images that are pulled together in the contrastive learning, the paper proposes "center-wise local image mixture" (CLIM) While the improvements that the CLIM augmentation brings seem to be quite good in the linear evaluation on ImageNet by boosting the results of MoCo v2 (which is used as a baseline if I understood correctly) by 4 points, the improvement of this representation on other downstream tasks (ie. Pros The paper proposes a simple, yet effective, data augmentation method that seems to help learning stronger representations for some tasks. The proposed method achieves state-of-the-art results for unsupervised learning on Imagenet and transfer learning tasks on Pascal VOC, COCO, and LVIS. hyper-parameter selection: there are several different parameters to be set in the proposed work: multi-resolution scales, number of clusters and neighbors for CLIM, alpha in cut-mix, with the differences in accuracy between settings approaching the difference between say knn+cutmix and center-wise+cutmix. The main idea is to consider the semantic similarity between different images and incorporate it in the learning procedure, in contrast to the many contrastive learning methods which only used augmentations of the query image as positives. It sounds, like there's a serious problem with previous contrastive representation learning approaches and the proposed methods solves them.
This paper proposes a method to alleviate the over-smoothing problem of GNNs. The key idea is to generate a latent graph structure via leveraging stochastic block model to approximate the observed graph structure and label information. This paper proposed a novel architecture termed VEM-GCN to address the over-smoothing problem of GNNs in the node classification task. The main idea is to optimize the graph topology by removing the inter-class edges as well as adding the intra-class edges, and then the noise information would not pass between nodes with different categories. This paper proposes a joint learning framework for GNN classification model and graph topology, which leverages variational EM as a learning framework. The learned latent graph is expected to have a clear community structure with dense intra-class edges and sparse inter-class edges, so that labels of unlabeled nodes are better predicted based on the latent structure. This paper presents a graph topology learning algorithm based on SBM and neural networks, which employs the node embedding and labels to optimize the topology. But table 3 tells if adding a small amount of labels, the difference between GCN and the proposed method becomes small again, which draws a similar question as toward the results in table 1. The experimental results show that the proposed VEM-GCN achieves higher classification accuracy than baseline methods.
Specifically, the paper proposes replacing the encoder with a truncated butterfly network followed by a dense linear layer. The paper's aim is to establish that linear layers can be replaced by butterfly networks and uses three different experiments to show this. In that regard, the paper is very appealing, as it shows that replacing linear layers with the butterfly networks does not result in any loss in performance. questions: see the above section This paper proposes to impose a particular sparsity structure (butterfly network) to replace dense connected layers in deep neural networks.
To sum up, the synthetic tasks proposed by the authors might indeed help in learning an inductive bias capable of improving theorem provers, but there is a discrepancy between the logical notions of deduction, abduction, and induction defined by Pierce (and more generally in the Knowledge Representation literature), and the reasoning primitives (essentially some forms of pattern matching) presented in the synthetic tasks. Specifically, this paper design three synthetic tasks to teach the transformer model to first learn three primitive reasoning steps: deduction, abduction, and induction respectively. The structure of the approach is to first pretrain the model on synthetic tasks that are designed around three principles of mathematical reasoning: deduction, induction, and abduction.
** END OF UPDATE*** The paper unifies two regularisation-based continual learning methods from the literature (Synaptic Intelligence and Memory Aware Synapses) by arguing that they both approximate the "Absolute Fisher", a variant of the Fisher Information that averages absolute instead of squared gradients, to determine the regularisation weights. However, I have an issue with the proposed quicker/cheaper update for calculating the (diagonal empirical) Fisher Information Matrix for Online EWC ("Batch-EF"), as I detail later. While the paper is well-written and -structured, and SI and MAS are popular methods in the literature, I'm not convinced by the link through the "Absolute Fisher". In more recent versions, Pytorch also provides autograd functions for computing jacobians natively -- I'm not sure how efficiently they are implemented, but in any case the empirical comparison here needs to use an efficient, batched implementation for calculating the Fisher in order for it to be meaningful. Cons of paper I am not convinced that the minibatching that the authors suggest (both for SI as an approximation to the AF and for EWC in the last paragraph of Section 5) is correct ("Batch-EF"). This works aims at unifying 3 popular regularisation type continual learning methods, namely EWC, SI and MAS, by showing that under some assumptions they all relate to the Fisher Information Matrix. The authors show that the importance weights of MAS and SI are similar to the diagonal absolute Fisher. Summary This paper attempts to unify the three most prominent regularization-based continual learning methods: EWC, MAS, and SI. I think the authors should justify that the Absolute Fisher is an optimal regularization under certain assumptions. Although I am not aware of previous works using the absolute value of gradients (as in AF), this paper provides evidence that such an approximation might be worth considering. However, since the connection mainly hinges on the empirical correlation between the two, I still don't think that the paper quite establishes the theoretical connection between the three continual learning methods that it aims for. The authors merely show that MAS and SI are similar to using Absolute Fisher as importance weights. It appears to me that by minibatching instead of squaring each gradient element, one should obtain much worse approximations to the empirical Fisher information matrix ("EF"). So simply taking absolute instead of squared gradients and stating that this is "a natural variant" is not a sufficient basis for a paper, especially considering that the term has not appeared anywhere in the literature before as far as I could tell. By this the authors claim to establish a relationship between these two approaches and Elastic Weight Consolidation, which approximates the diagonal of the Fisher. Finally, I'm not quite convinced by the potential impact of the insights, which seem fairly specific to choosing the batch size of SI, so overall I think the paper still is below the acceptance threshold. Summary of paper This paper draws links between three common regularisation methods for continual learning: EWC, MAS, and SI. Therefore, I cannot agree with the claim that this paper presents a unified framework, just because it fits several methods into distinct concepts that have "Fisher" common in their names. Similarly, I think other methods, such as MAS and SI, can also be optimal under different assumptions. I think the authors really need to either provide some references to existing work or establish themselves that this variant makes sense theoretically. And the authors refuted my interpretation: We claim that SI and MAS work because they are similar to AF.
The result is improved on multiple unsupervised machine translation, and this paper claims that more diversity is brought to the synthetic data, so a better translation model can be trained. Weaknesses / Questions for authors: As with any NMT model trained with synthetic data, it would be better to report results on source and target original splits of the test data to provide a clearer evaluation [2,3]. In Appendix Summary: The paper proposes an additional stage of training for unsupervised NMT models utilizing synthetic data generated from multiple independently trained models. The authors add this additional stage of training to unsupervised NMT models using different pipelines (PB unsupervised MT, Neural Unsupervised MT, XLM) and show that their approach improves all of these approaches by 1.5-2 Bleu on WMT En-Fr, De-En and En-Ro. Strengths: The paper is well written, the approach is simple and seems to improve quality by significant amounts in a variety of experimental settings. Source of promotion: the second stage of CBD method adopts (x_s, y_t), (z_s, y_t), (y_s, x_t), (y_s, z_t) synthetic translation pairs, it is not clear how much performance growth comes from increased data and how much growth comes from the new model implementation (ott et al., 2018). Unfair comparison with the enable distillation: authors need to compare CBD with the model trained on the synthetic data (y_s, x_t) of the ensemble of agent_1 and agent_ 2. The authors show substantial (1-2) BLEU improvements with 3 different UMT systems in 5 low-data scenarios (En-Fr, Fr-En, En-De, En-Ro, Ro-En), all subsampled to 5M monolingual sentences for each language. Experimental results in several translation tasks show that the proposed approach improves the translation accuracy of the standard unsupervised machine translation models, outperforming the cross-lingual masked language model. While I would have also liked to see experiments in more realistic low-resource settings, the current paper does a good enough job of evaluating the approach in standard unsupervised NMT settings on related high resource languages. In general, the CBD method in this paper is a simple and effective data enhancement method to improve the performance of the model. They can directly add the synthetic data decoded by cross model to continually train the original XLM model with a supervised translation objective (which is naturally supported in XLM from my experience), and report the effect comparison between them. Did the authors try any experiments with unsupervised models utilizing parallel data in unrelated languages, similar to [4,5] or in real low resource settings [6]? Because the training of CBD is divided into two stages, the diversity of the second stage only brings more training data to enhance the supervised machine translation model, rather than unsupervised machine translation effect.
About the second contribution, the paper claimed that SGD with BN behaves like a variant of Adam, AdamG*. The projected dynamics (to the unit sphere) is studied, and the effective learning rate and update direction on the unit sphere are derived. The submission analyzes the behavior of gradient descent and adaptive variants for scale-invariant models, including batch-normalization, by looking at the trajectory of the iterates when projected on the unit sphere.
This paper begins with the empirical observation that adversarially trained models often exhibit a large different in clean (and robust) accuracies across different classes. The paper then proceeds to use a theoretical example (adapted from the model in Tsipras et al 2018) where adversarial training increases the accuracy difference across classes. The authors study adversarially trained classifiers and observe that the accuracy discrepancy between classes is larger than that of standard models. Summary: This paper introduces a fairness perspective on accuracy performance among distinct classes in the context of adversarial training. Should the increase in error be multiplicative (which is the most likely scenario) then it would potentially explain the main observation of the paper without taking into account adversarially training at all.
This paper proposed a powerful online sequential test which can efficiently detect qualitative treatment effects (QTE). === Contributions === This paper proposes a new framework for A/B testing in the frame of randomized online experiments. === Weak points === Minor: Authors claim that Figure 3 reports experiment results regarding QTE. My grade can be further strengthened if the authors can address the points which for me need to be clarified, the biggest one being the correction of Figure 3 to fully support the efficiency of the approach. The authors propose a scalable online algorithm for Type 1 error control. The test algorithm involves adaptive randomization, sequential monitoring and online updating.
This paper proposes PABI (PAC-Bayesian Informativeness?), a way of measuring and predicting the usefulness of "incidental supervision signal" for a downstream classification task. It also is not clear to me from the paper's text whether something close to the PABI framework can apply in broader settings like language modeling style pretraining, where the input-output format of the incidental supervision signal is different than that of the target task. Summary This paper proposes a unified measure for the informativeness of incidental signals (ie, not standard ground truth supervised labels) derived from the PAC-Bayesian theoretical framework. Weaknesses While the generality of the proposed PABI framework is great and improves over existing work, I think this paper could be scoped more carefully and the scope could be clarified better. ########################################################################## Summary: This paper proposes a unified PAC-Bayesian-based informativeness measure (PABI) to quantify the value of incidental signals. "In the common supervised learning setting, we usually assume the concept that generates data comes from the concept class": This is a surprising claim as the PAC-Bayes framework differs from the Bayesian one namely by the fact that we usually don't need to make assumptions about the data-generating distribution other than being i.i.d. Supporting arguments The work leverages a well-studied framework to answer important questions about understanding the utility of non-standard supervision signals, enabling us to reason in a unified way about varied kinds of these signals as well as their combinations. The paper needs to better situate the proposed analysis compared to classical PAC-Bayesian generalization risk bounds. Mathematical developments of PABI are given for these cases, and experiments show that PABI is nicely positively correlated with the relative improvement that comes with various methods for integrating incidental supervision signal (including one which is developed as a side note by the authors). However, even if the empirical results are good, the connection between PAC-Bayes and the proposed informativeness measure (named PABI) is vague.
The second reason is the lack of positioning of the proposed scheme/objective/data structure to a long line of research on the use of machine learning (with novel metrics/objectives and data structures) for search, including the learning of space partitioning trees [A, B, C], locality sensitive hashes [E] and representations [D] At the end of section 3, after presentation of SSWR, it is not clear why we are minimizing for a search sequence generator G that is aggregated over database-query pairs (D,q) -- wouldn't we learn a data structure per database (as is done for data structures used for nearest-neighbor search)? In this paper, the authors proposed Search Data Structure Learning (SDSL), which they claim to be a generalization of the standard Search Data Structure.
This paper presents a variation of the MONet model where an additional Region Proposal Network generates bounding boxes for various objects in the scene. [Paper Weakness] The self-supervision between segmentation masks and detection bounding boxes is the main contribution. An additional loss is introduced during training to make the segmentations produced by the MONet segmenter consistent with the proposed bounding boxes. The author should compare their R-MONet(UNet) with the baseline of R-MONet(UNet) w/o the self-supervised loss, i.e. removing the object detection branch. In general - I feel the performance on these datasets is quite saturated and I hope to see results on more challenging data in the future - the proposed method included The paper is somewhat middle of the road in most aspects - the proposed method is, in my opinion, only a slight variation on the existing MONet model. In this paper, the authors introduce a region-based approach for unsupervised scene decomposition. It extends the previous MONet by introducing the region-based self-supervised training. In Table 1, the MONet (ResNet18+FPN) is 10 percent lower than the original MONet. Does this means the network structure has a greater influence on the performance than the self-supervision component. Instead of purely generating foreground masks in an RNN, they simultaneously predict the bounding boxes and segmentation masks using a Faster-RCNN-based framework.
Ideally, I hope the theory can be extended to explain why small beta causes instability (the conclusion section mentions this as future work), and/or how neural network architecture affects optimal beta, but these extensions do not seem obvious. The theoretical analysis shows that neural networks trained with smaller inverse temperatures (beta) exit the linear regime faster, which implies better performance. Experiments on image classification and sentiment analysis confirm that tuning temperature improves neural network generalization, even for state-of-the-art models. This paper studies how temperature scaling affects training dynamics of neural networks (with softmax layer and cross-entropy loss). But this doesn't explain the experiment results: optimal beta varies a lot across models, and it is sometimes quite large (beta >= 1). To improve the theory, I hope the paper can provide more insights into the instability caused by small beta. As the CIFAR-10 experiment points out, using small beta can still lead to good performance, so the main problem for small beta seems to be instability (rather than slow training), which the current theory couldn't explain.
So my first question is: i) Can you define the likelihood in a single Poisson process model in order to consider all the interactions you mentioned in section 2.2? This paper proposes a generalized additive model to learn joint intensity functions for multiple Poisson processes. This new model, called additive Poisson process, relies on a log-linear structure of the intensity function that is motivated thanks to the Kolmogorov-Arnold theorem. In this paper, the authors seem to assume that every dimension is a different process, and somehow events in each dimension are grouped together based on their arrival order ("Each ti is the time of occurrence for the i-th event across D processes and T is the observation duration").
The paper proposes an efficient long-range convolution method for point clouds by using the non-uniform Fourier transform. The paper is clearly written, and presents an approach to efficiently utilize long range convolutions through a nonuniform FFT in for coulomb particle configurations. An efficient method for fitting long-range style interactions in point clouds is presented. In terms of results, the paper clearly shows that NUFFT scales essentially like O(N) (with N the points number) whereas naive direct space convolution scales as O(N^2). Point-cloud is indeed important for many tasks as described in the introduction part, but the authors just explore the effects of the proposed algorithm in a "synthetic" experiment. I think such a discussion (and possibly a couple of experiments) would improve the paper a lot, showing the limits of the method and letting the reader understand the reasons for its strengths (which are very real, I do believe it !)
2 is very dense and difficult to follow, and only seems to motivate the method in the special cases of a very shallow linear regression model, where the gradient is easily related to eigenvalues of the input features X (and assuming a "strong factor structure", which has been shown to be reasonable for natural images.) The paper also claims that strong factor structure (with a small number of dominant components) is prevalent in modern ultra-large scale DNN applications -- I would like to see some references / supporting evidence beyond just computer vision. The paper makes an observation that datasets used for training many deep neural nets exhibit a strong factor structure, i.e. have a small number of dominant principal components explaining most of the variance. In terms of criticisms,  there is very limited scholarship of related ideas that have been used both for linear models and for DNNs, in particular (a) various factor-based models that already exist,  (b) preconditioning of linear systems,  (c) neural nets trained on some other sort of residuals Experiments using MNIST and CIFAR10 show that the proposed method accelerates the speed to reduce the training loss. Experiments show that the proposed algorithm can speed up wall-time to  a certain accuracy on MNIST and CIFAR10 classification, across several neural network architectures and optimizers. The paper describes a training scheme based on decomposing input features into two parts which have different training dynamics: a low rank "factor feature" computed using PCA on the raw features, and a high rank "residual". Additional details: Prior work -- that should be cited / contrasted with your approach:(a) Since a substantial part of the paper analyzes linear models,  it's important to mention that factor structure has been long exploited in various ML / stats works. Summary: In this paper, a learning method that accelerates the training of DNN is proposed. Can you give some references claiming strong factor structure in several DNN applications in ultra-high dimensions? Weak points Analysis in section 2 is based on linear regression, but the proposed method is based on deep models.
This paper proposes a contrastive autoencoder approach that only requires small data to perform a multi-label classification on the long-tail problem. I like the intended focus of this paper which is to perform self-supervised training of small data for downstream tasks with applications for zero and few-shot learning. The technique proposed in this paper revolves around learning the label embeddings that would match the input embeddings, which is quite limited to multi-classification and might not carry much value to other tasks.
In the new experiments the authors used small sketch size to show clearer advantages of the learned IHS (Figure 2 & 4), but at that regime all of the methods including learned-IHS are converging only slowly and not practical compare to the slightly larger sketch size choices. The paper proposed a learned variant of the well-known iterative Hessian sketch (IHS) method of Pilanci and Wainwright, for efficiently solving least-squares regression. This paper leverages learned sketches to improve the convergence rate of second-order optimization methods. I think the idea of using learned sketches is interesting, and it seems like it has not been applied to the problems considered in this paper before. While getting a learned variant for IHS is an interesting direction, the current theoretical contribution of this paper is only incremental, and most importantly, the reviewer is unconvinced for the practicality of the current approach. For example in Figures 1, 2, 6, 7, that the learned IHS has the same linear convergence rate as the unlearn IHS, and actually only slightly faster overall (only 1 iteration faster if we view the plots horizontally), but the authors' claims are like "We can observe that the classical sketches have approximately the same order of error and the learned sketches reduce the error by a 5/6 to 7/8 factor in all iterations", "We observe that the classical sketches yield approximately the same order of error, while the learned sketches improve the error by at least 30% for the synthetic dataset and surprisingly by at least 95% for the Tunnel dataset. Proof of Lemma 3.1: In Section A, 2nd sentence, you say "Since T is a subspace embedding of the column space of A...". In order to truly demonstrate the benefits of learned-IHS (which seems to be robust to small sketch sizes), the authors should choose for each algorithm the best sketch size and then compare them in run time, at least between learned-IHS and Count-sketch IHS.
Specifically, this paper also provides a multi-modal multi-task model where the time series data are encoded by LSTM, clinical notes are encoded by text CNN and tabular data are also encoded by existing methods. For the first objective, the data supports 6 clinical classification tasks (which were proposed before in a earlier MIMICIII benchmark) but also supports 4 different modalities: physiological time series (the standard modality that used in the prior benchmark), clinical notes, baseline data, and waveform (although not being used in the method/experiments). The discussion and future work is not quite related to the paper, for example, it was mentioned about isng images, but images is not one of the modalities in the proposed benchmark.
Specifically, they propose two regularization terms to 1) capture the diversity of the tasks and 2) control the norm of the prediction layer, thereby satisfying the assumptions in meta-learning theory. To improve the practical performance of meta-learning algorithms, this paper proposes two regularization terms that are motivated by two common assumptions in some recent theoretical work on meta-learning, namely To ensure the assumptions of the theories, the authors propose a novel regularizer, which improves the generalization ability of the model. Here are the main concerns of this paper: The proposed method in this paper is based on the meta-learning theory as stated in Section 2. One main theoretical assumption in meta-learning theory is the task distribution. [2] HOW TO TRAIN YOUR MAML, *CONF* 2019 The main motivation of this paper is based on the theoretical results of meta-learning. ########################################################################## Summary: The paper reviews common assumptions made by recent theoretical analysis of meta-learning and applies them to meta-learning methods as regularization. Numerical experiments show that the proposed regularization terms help achieve better performance of meta-learning in some tasks. As to regularizing the Frobenius norm, there exist a line of literature showing weight decay works for general settings apart from meta-learning. The main idea of applying theory to practice is reasonable, but the regularization methods proposed are mainly known. In some experimental results, the improvement due to the proposed regularization seems to be at the same level of the standard deviation, as well as the difference between the reproduced results of existing meta-learning algorithms and those reported in earlier papers. Results show that these regularization terms improve over vanilla meta-learning. Summary: In this paper, the authors aim at bridging the gap between the practice and theory in meta-learning approaches.
compared to existing graph pooling methods, the authors think their methods are able to capture information from all nodes, collect second-order statistics, and leverage the ability of neural networks to learn relationships among node representations, making them more powerful. Experimental results on four datasets (PTC, PROTEINS, IMDB-BINARY, IMDB-MULTI) of two tasks (bioinformatics, social networks) show that the proposed graph pooling method can improve the performance by 0.5%-1.2% accuracy while decreasing the std. Weaknesses: My biggest concern is that the proposed approach lacks originality and novelty because it is a simplification and variant of SOPOOL from Second-Order Pooling for Graph Neural Networks (Ji and Wang, 2020) In this manuscript, the authors propose two novel methods using fully connected neural network layers for graph pooling, namely Neural Pooling Method 1 and 2. Specifically, the major idea of Neural Pooling Method 1 is to use GCN layer to learn a score for each node. In this paper, the authors proposed two graph pooling methods, i.e., Neural Pooling Method 1 and 2.
Overall, I found this paper to be a well-written, well-executed, illustration of when it is optimal to learn task-specific adaptive behaviors. Summary: This paper explores the effect of time horizon on meta-reinforcement learning agents. The bandit example in Section 3 clearly illustrates the two regimes that this paper studies: when it is necessary to learn adaptive behaviors vs. Most meta-RL papers study the case where adapting to a new task requires learning new behaviors. The authors investigate the question of when the optimal behavior for an agent is to learn from experience versus when the optimal behavior is to apply the same (memorized) policy in every scenario. I enjoyed reading the paper and although the results are intuitive and unsurprising, they nicely emphasize the importance of environment and task design choices in strategies learned via memory-based meta-learning. This is an interesting observation, but prior work [2] already shows that meta-RL is equivalent to learning the Bayes-adaptive optimal policy. If the research question is "How does the optimal policy depend on task parameters such as uncertainty and horizon?" I believe Bayes-adaptive work answers that question. This section also convincingly shows that existing meta-RL agents roughly learn the Bayes-optimal policy in this case. (ii) existing meta-RL agents are capable of choosing not to learn adaptive behaviors, when it is optimal to do so; While much meta-RL research typically focuses on the former setting, this paper studies when it is not necessary to learn adaptive behaviors.
The authors claimed that, by adjusting α through training, the trained model m(⋅;θ^) with an optimal parameter θ^ is asymptotically consistent with the model trained on a dataset with clean labels, i.e.,  the trained model without α performs well on clean test data Adding experiments with synthetic datasets with different levels of noise can be helpful in understanding the advantages of AC1/AC2 over other methods for handling noisy labels. The paper proposes an approach for handling noisy labels in predictive models without removing them. P(i|x;θ)=exp⁡(mi(x;θ))∑jexp⁡(mj(x;θ)) In the proposed method, for the training set D={xn,yn}, we first train the model by minimizing the following loss function: θ^,α^=arg⁡minθ∈Θ,α∈RN×K−1N∑n=1N∑i=1K1[yn=i]log⁡exp⁡(mi(xn;θ)+αni)∑jexp⁡(mj(xn;θ)+αnj)
I think that the authors explore some interesting connections to recent work on identifiability in latent variable models, and understanding what these results imply for causal inference is important. Summary: The present paper introduces Counterfactual VAE (CFVAE), a generative learning method to estimate treatment effects under a latent unconfoundedness assumption. Summary The authors propose a representation learning method for estimating causal effects in the presence of unobserved confounding when covariates that act as proxies for a latent confounder are available. Unlike standard model misspecification which can be detected and debugged based on observables, making the wrong identifying assumptions in a model can result in silent failure, where the model can fit the observed data perfectly, but return a causal effect estimate that converges to the wrong place, or doesn't converge at all.
Although previous papers have reported that the NAT models can achieve the same performance level with auto-regressive translation models while the decoding speed is much faster, like two to five times, this paper points out that it deeply relies on the batch size and computation environment. Besides, one thing should also be noticed: the speedup yielded by HRT seems less charming compared to recent NAT models, such as Levenshetain Transformer 3x-4x (Gu et al, NeurIPS 2019), and JM-NAT (k = 10) 5.73x (Guo et al, ACL 2020) where both models achieve similar translation quality to the AR baseline. In particular, some models like AAN [2] and Deep encoder+Shallow decoder[3] can already produce similar speedups (might be smaller) with comparable translation performance. Experiments on several MT benchmarks show that the proposed approach achieves speedup over the full AT baseline with comparable translation quality. This paper proposes a hybrid-regressive machine translation (HRT) approach—combining autoregressive (AT) and non-autoregressive (NAT) translation paradigms: it first uses an AT model to generate a "gappy" sketch (every other token in a sentence), and then applies a NAT model to fill in the gaps with a single pass.
############################################################################### Summary The paper presents a novel method for learning branching strategies within branch-and-bound solvers, which consists in a graph-convolutional network (GCN) combined with a novelty-search evolutionary strategy (NS-ES) for training, and a new representation of B&B trees for computing novelty scores. The authors have clarified some technical details and blind spots of their methods, have fixed their evaluation metric which was wrongfully comparing tree sizes on solved and unsolved instances, and have presented an additional experiment in the appendix on the original benchmark from Gasse et al. ############################################################################### Pros and cons Pros: the idea of learning branching strategies using reinforcement learning instead of imitation learning makes a lot of sense and seems like a promising direction to follow the proposed representation of B&B trees is original the presented results seem promising The paper claims this new method provides a significant improvement over state-of-the-art branching strategies, either based on expert-designed rules or on imitation learning of strong branching. But in the same table, the GCN model trained to imitate SB results in trees much smaller than the expert (418 vs 1304 on independent set), which again is suspicious, and most importantly contradicts the original claim of the authors. Also, Figure 4 indicates that a pre-trained GCN model results in a tree size of 350 on independent set problems, which does not coincide with the number 418 reported in Table 1.
The paper proposes a knowledge distillation method for face recognition, which inherits the teacher's classifier as the student's classifier and then optimizes the student model with advanced loss functions. ECCV2020 This paper proposes a new KD method to inherit classifier from teacher models and utilize it to train the student model feature representation, where previous KD methods are mostly focusing on the proxy task other than the target task itself. The paper demonstrates using an ensemble of teacher models can boost the performance of knowledge distillation. The experiment lacks comparison with the general knowledge distillation methods (Ref.2) in image classification and the specific used methods (Ref.3) in face recognition. This paper proposes ProxylessKD method from a novel perspective of knowledge distillation. "Deep face recognition model compression via knowledge transfer and distillation." arXiv preprint arXiv:1906.00619 (2019). The idea of using teacher model's classifier to directly reshape the student model's feature representation is somewhat novel. There are multiple experiments on major face recognition datasets and demonstrate superior performance against baselines such as L2KD-s. For example, when ProxylessKD is combined with the proxy task, i.e., feature distillation loss, how would it perform compared to only ProxylessKD?
Specifically, it proposes PeerPL to perform efficient policy learning from the available weak supervisions, which covers PeerRL (for RL with noisy rewards), PeerBC (for imitation learning from imperfect demonstration) and PeerCT (for hybrid setting). They also show that PeerBC outperforms standard behavioral cloning in learning from synthetic demonstrations on several Atari games, as well as the cart-pole and Acrobot tasks, all with noisy versions of the base reward signals. This paper formulates a framework for reinforcement learning and behavior cloning from weak supervisions (i.e., noisy rewards or imperfect expert demonstration). SUMMARY OF CONTRIBUTION: This work presents a pair of algorithms, PeerRL and PeerBC, which address certain forms of noise in reinforcement learning and behavioral cloning respectively.
As part of the  paper, they also evaluate and discuss the performances of four object-centric representation models, one of them (Video MONet) being an extension of an existing approach, proposed as part of this paper, and the remaining being state of the art approaches for the task. Paper Strengths Although I am not familiar with unsupervised learning of object-centric representation, I like the idea of proposing a common protocol for evaluation. The paper proposes a benchmark for the evaluation of unsupervised learning of object-centric representation. The benchmark consists of three datasets, multi-object tracking metrics and of the evaluation of four methods. Overall, this paper is interesting in setting up a benchmark for unsupervised object representations which is a very important problem in computer vision, reinforcement learning, etc. Given the cons and specifically on not a clear actionable suggestion on how to improve models and no analysis beyond synthetic datasets I am leaning towards a rating of below acceptance threshold. The paper is actually very well written and tries to answer the question of how various unsupervised learning of object-centric representations to on controlled tasks (synthetic datasets). Instead of focusing on such a comprehensive set of things - 3 datasets, five models and multiple metrics it would have been better if authors did some minor extensions of the models and showcase novel directions.
The paper proposed a novel personalized federated learning method using a mixture of global and local models. The paper proposes a federated learning framework using a mixture of experts to trade-off the local model and the global model in a federated learning setting. Authors claim the proposed method can protect user privacy since a client can select which data need to be excluded from the federation. The paper is an integration of the mixture-of-experts method with existing personalized federated learning. The mixed-use of global and local models (equation 6) is not a novel way of federated learning. I guess when the model architecture is different, the difficulty of hyper-parameter optimization will increase, which weaken the application of the proposed method.
The paper proposes a scalable approach via intention propagation to learn a multi-agent RL algorithm using communication in a structured environment. Then, the paper proposes to use mean-field approximation to approximate the optimal joint policy (Equation (3)), which leads to a concrete algorithm that relies on passing the embedding of each agent's local policy around to neighbors. I'm not an expert in the area and wouldn't expect to follow all of the reasoning in constructing the method, but I would be expect to be able to follow some clear statements of the algorithm, or its theoretical properties (guarantees of some solution quality given certain assumptions, the parameters affecting this, etc. Paper Summary The paper considers the cooperative multiagent MARL setting where each agent's reward depends on the state and the actions of itself and its neighbors The paper has a theoretical claim that, for such reward structure, the optimal maximum entropy joint policy in the form that can be factored into potential functions, one for each agent. This paper proposes a method for generating policies in cooperative games, using a neighbourhood-based factorisation of reward, and an iterative algorithm which independently updates policies based on neighbour policies and then propagates the policy to neighbours using function space embedding. For the assumptions on rewards, Proposition 1 assumes that each agent's reward depends on its neighbors, while the derivation of Equation (3) (and thus the following algorithm) further assumes that the reward depends on pairwise actions. For partial observable environments, the proposed methods needs to reply on DGN. The paper proposes a multi-hop communication method for multi-agent reinforcement learning. Compared to previous works studying communications of local observations, the proposed work (1) needs to address the problems induced by the joint policy, like sampling from it. Although the authors emphasize that they are communicating the intentions of agents, I think their method is quite similar to those communicating local observations, like NDQ (https://arxiv.org/abs/1910.05366), DGN, or CollaQ (https://arxiv.org/abs/2010.08531). For the baselines used in the experiments, it seems that only IP and DGN allow communication/message passing during execution, which makes it unsurprising that the two methods outperform other baselines. Overall (weak accept) The paper has a clear introduction and motivation of the proposed algorithm. In particular, it seems that the loss functions do not drive mu's represented by NNs to the fixed point solution of Eq (3); psi shows up in Eq (3) but does not play a role in the following development of the method. This would seem to need a clear statement: what are the exact assumptions, and what precisely is the quality of the output?
In more detail, the authors claim four proposed components: AdaQuant, Integer programming, Batch-norm tuning, and two pipelines for neural quantization. In particular, the authors focus on sub-8 bit quantization and propose a novel integer linear programming formulation to find the optimal bit width for a given model size. The methods include AdaQuant (which jointly optimizes quantization steps for weight and activation per output activation of each layer), Integer Programming (which determines bit-precision for all the layers), and the batchnorm tuning. This is a good result but please note that other work in the literature (arxiv:2001.00281) reports 72.91% for INT8 quantization of MobileNetV2 (this comparison is actually missing from the paper). Page 4: "Depending on the needs, our performance metrics P would be either the execution time of the network or its power consumption." This is good but no result on either latency or power consumption is provided in the paper. Overall the paper is strong however, please note the following: Page 3 last paragraph: it seems there are errors in the results for calibration data. Page 8: For instance, on the extensively studies -> For instance, on the extensively studied This work presents a quite comprehensive multi-step scheme for post-training neural quantization that does not rely on large datasets or large computational resources. Also, it seems that the "per-channel" quantization method is utilized in this work, but the formulation in (2) seems to be for "per-layer" optimization. "we investigate [a] mixture" "results with high degradation" -> in high degradation This paper proposed a set of methods for post-training quantization of dnns. The authors propose to use many techniques to push the limit of neural quantization, which shows reasonable improvements in some datasets. The authors presented promising experimental results on various neural networks to support the proposed methods. Pros: The empirical results are relatively strong in this method; 4-bit quantization is a good achievement in the models considered here. There is no clear explanation of how AdaQuant increases the generality of the quantized model, and the discussion about the sample size (B) is hard to understand (why there's infinite solution when B << N? How much accuracy the proposed methods can maintain if they adopt "per-layer" quantization? Summary: The paper studies the problem of Post-Training Quantization of NNs, where no fine-tuning is performed to quantize the model. For instance, the authors omit Nagel et al 2020, which seems to do better at similar quantization levels, but I am not sure the results are directly comparable. BOPS proposed by (https://arxiv.org/pdf/2005.07093.pdf) is a good metric to measure the total reduction in computations for mixed precision quantization. The paper introduces a series of techniques to quantize neural networks, and how to combine them: Layer by layer quantization where weights can change as needed (rather than to the nearest quantization error).
It proposes to use Hamiltonian Monte-Carlo (HMC) to sample the next states (instead of IID samples) and matrix completion to learn a low-rank Q matrix. The limitations of Q learning (equation (1)) should directly motivate use of Hamiltonian Monte Carlo (HMC) in Section 2.2, but instead the manner in which HMC is presented is as additional preliminary material. Experiments on discretized problems (CartPole and an ocean sampling problem) show that HMC and low-rank learning can behave more benignly compared to IID samples. The matrix completion step (equations (9)-(10) is playing the role of a proximal operator on the Q learning update, or otherwise some projection of the Bellman error onto a low-dimensional subspace of features.
Summary: This work proposes that many common issues with GAN methods are based on the weighting of the samples given to the generator's objective function. This paper proposes an explanation for mode collapse in the original GAN with the log -D objective for the generator (dubbed the non-saturating GAN or NS-GAN for short). The authors present theory that backs up these claims and they propose a new generator training objective which re-weights the gradients of the generator objective to have the same average magnitude as NS-GAN but have the same relative magnitudes of the original GAN objective. The authors show that the gradients of the respective objectives just differ from a scaling factor depending on the discriminator's output for generated samples. For this reason, a new objective (NS-GAN) was proposed which modifies the generator's objective to alleviate this gradient vanishing issue. The GAN community quickly observed that when the discriminator outperforms the generator with this objective, the saturating nature of the sigmoid function causes the gradients to vanish for the generator's objective. The paper takes the approach of comparing the gradient of the generator objective for the original GAN with cross-entropy loss (dubbed the minimax GAN or MM-GAN for short) and the log -D variant. In the next paragraph, the paper mentioned that the generator gradient "is only locally informative" - again this is true for all GANs. How is this relevant for the argument made in the paper? I am not an expert in the GAN field, but from inspecting the Conv-4 model with spectral normalization on CIFAR10, it appears like the best performing model achieves an FID of approx 42 and the worst model gets around 48 (figure 8, bottom right). The baseline NS-GAN with spectral normalization presented in this work greatly underperforms previously published methods that use the same objective. Training a 4-layer network on MNIST shows demonstrates the vanishing gradient effect on MM-GAN and the counteracting effect with a smaller ADAM beta2 parameter applied to the Generator. They present results on toy data and small image datasets like CIFAR to show that the method leads to stabler training and does not suffer from mode collapse as badly. This paper reexamines the original (MM) and the non-saturating (NS) GAN objective. Additionally the authors discuss the non-saturating effect of the ADAM beta2 parameter for the MM-GAN Generator.
The authors study exploration in procedurally-generated environments with sparse reward, proposing a new method, termed RIDE-SimCLR, for addressing this problem. Moreover, the authors form a link between RIDE-SimCLR and episodic curiosity [EC; 3] and demonstrate empirically that the proposed method is performant in procedurally-generated MiniGrid tasks. The authors borrow the idea from SimCLR [2] and use cosine similarity between successive observation embeddings as a measure of impact when the embedding function ϕ is trained with contrastive learning, treating observations that k-steps reachable as positive pairs, i.e., forcing their embeddings to be approximately equal. Their algorithmic contributions are essentially two-fold: -They propose RIDE-SimCLR, a modification of RIDE which replaces RIDE's intrinsic motivation (the l2-distance, between timesteps, of an embedding of observations) with a SimCLR-inspired contrastive learning dissimilarity measure. According to Eqn. 4, the intrinsic reward of RIDE-SimCLR is high, if the representations of two states are dissimilar with respect to each other.
The high-level idea is that similarly to the way that fair algorithm are able to improve the worst-case accuracy of predictors across different groups without knowing the sensitive attributes, perhaps we can use these ideas to domain generalization when environment partitions are not known to the algorithm. The paper develops its own algorithm EIIL which extends the Invariant Risk Minimization (IRM) of domain generalization to work in the situation when the prior knowledge of environments is not available. The authors explore cases where e is known or not and come up with some algorithms that draw connections between recent work on domain generalization, specifically invariant risk minimization (Arjovsky et al 2019) and fairness. The authors are definitely correct in crediting a prior fairness paper for the idea of  the idea of adversarially re-weighting examples with a soft groups model, but as they themselves point out this idea has existed in different forms in the domain generalization literature (e.g. DRO). Relaxed training objective lacks strong theoretical backing: The authors directly adapt the training setup of Arjovsky et al., where the goal is to train a model which learns the same conditional label distribution for any given input "x" across a set of known partitioning of the training data, dubbed as the invariance constraint . Weakness: (1) Other than the high level intuitions and examples, the paper does not provide any theoretical analysis of the performance of the EIIL for domain generalization. Similarly, the paper uses the idea from domain generalization to design fair algorithms w.r.t. a notion called "group sufficiency". The authors conclude the paper with three different examples addressing domain generalization and fairness.
The paper therefore proposes a measure based on the discrepancy of a group of segmentation models to identify more valuable images to annotate and add to the training data in a iterative fashion. Annotating images for training of segmentation models is time consuming and it can be difficult to annotate enough examples to ensure good performance on the rare difficult examples that often occur when methods are applied to real world data. Weakly-supervised labeling is more practical for segmentation; and (2) extending to active training/tuning, leveraging the selected hard examples to improve the segmentation model for multiple rounds. Leveraging those counterexamples to improve the segmentation models' generalization performance on unseen images seems to be novel in this field. It is my understanding, but I am not actually sure so it would be good to have the approach clarified, that the test datasets (T(1), T(2), and T(3)) of iteration 1, 2, and 3 are hard examples, and are thus biased towards the methods involved. The authors identify that scaling human annotation, in particular for image segmentation, can be cost prohibitive and propose a method to optimize this process. If the methods tend to disagree on a limit number of typical cases, then these cases will be added to the training set and it is not so surprising that improvements in the target model is seen. Once selected, they leverage human annotators to first filter this image set and then segment the images, which are then used to retrain the model. This approach is also an instance of the basic active learning paradigm that iterates between spotting hard examples, labeling them, and tuning the model.
Major Comments: Why two-stage approach / use Gaussian likelihood: If I understand correctly, at test time, authors trained a deep model to generate uncertainty-aware predictions ytest using Gaussian likelihood, and then conduct parameter estimation by performing MLE over a weighted likelihood constructed using the deep ensemble prediction. To solve this problem, the authors present a deep learning approach that utilises a combination of (i) deep ensemble training, (ii) post hoc model selection, and (iii) importance weighted parameter estimation. This paper proposes a method that uses ensembles of deep neural networks for parametric feature density estimation and enables control of the bias-variance tradeoff, and demonstrates the method's performance on an X-ray polarimetry task.
The paper presents a pre-training scheme (APT) for RL with two components: contrastive representation learning and particle based entropy maximization. Can authors present state entropy plots, and possibly compare the performance of APT with other methods seeking a similar objective, such as MaxEnt (with representation learning), SMM [2] or MEPOL [1]. I would like to suggest the following: Reporting the performance of zero-shot transfer for all experiments, i.e. the average return of the pre-trained policy in each task (APT@0). DETAILED COMMENTS C1) The exploration component of APT has striking similarities with the method in [1], which also seeks the optimization of a k-NN estimate of the state distribution entropy in a reward-free context. Gains could come from faster convergence on dense reward games thanks to the pre-trained representations, or from higher end performance on hard exploration tasks due to the exploratory behavior of the pre-trained policy.
The experimental results show that the method is effective to train a model in an unsupervised manner using just the raw data to predict dependency and constituency parses and performs better than the relevant baselines. This paper proposes a neural network optimized by MLM loss that has inductive bias to be useful for unsupervised constituency and dependency parsing. However, there are a number of unclarities in the paper that make it difficult to determine whether the results on unsupervised parsing are actually comparable to prior work. This paper has an interesting idea at its core: stemming from the success of models like ordered neurons, can we define a neural architecture that uses both constituency and dependency structure? The results are better than trivial baselines for unsupervised constituency and dependency parsing but not as strong as related recent work. The attention heads of the model focus on different dependency based masks for training. These examples of inaccessible writing (Algorithm and section 4.2) make the core part of the paper difficult to understand and hence I think this paper can be better with another round of thorough revision. The experimental results are difficult to evaluate, as there are different datasets and assumptions made by different work, so there does not appear to be a baseline that the new model can be compared to fairly. The model is trained for masked language modeling (MLM) and evaluated via MLM on held-out data and its ability to induce constituency and dependency trees. The start of Section 5 states that the model is trained on BLLIP datasets and unsupervised parsing evaluation is on the WSJ testset. The heights and distances are learned by a masking scheme similar to PRPN, but the model is a tranformer MLM rather than a recurrent memory network.
Advantages: l    The exploitation step and exploration step in AutoSampling is interesting, it is straightforward that this method can work well as the sampling strategy is updated dynamically according to the current state of model. To address the issue of optimizing high-dimensional sampling hyper-parameter in data sampling and release the requirement of prior knowledge from current methods, the authors introduce a searching-based method named AutoSampling. This work presents an interesting exploration of learning optimal data sampling probability. First I will comment on the listed contributions: • To our best knowledge, we are the first to propose to directly learn a robust sampling schedule from the data themselves without any human prior or condition on the dataset. To this end, authors propose to record the data frequencies through all previous steps and thereby generate the sampling policy for the next step. • We propose the AutoSampling method to handle the optimization difficulty due to the high dimension of sampling schedules, and efficiently learn a robust sampling schedule through shortened reward collection cycle and online update of the sampling schedule. The authors have conducted sufficient experiments to verify the superior of their method, especially for the effectiveness and generalizability. If so this does not seem sufficient to confirm that this method works because the data you sample from is always the same. This work is taking a novel aspect for effectively training neural networks, since rare effort has been devoted to make data sampling learnable. Authors should compare with a few state-of-the-art data-sampling or data-reweighting methods, such as Focal Loss proposed by He et al. From this description I believe this paper has done it before: "CASED: Curriculum Adaptive Sampling for Extreme Data Imbalance".
Summary The paper introduces two simple modules, SelfNorm (SN) and CrossNorm (CN), that are highly modular and can theoretically be attached to different parts of the CNNs to control the balance between style and content cues for their recognition. In particular, this paper proposes to recalibrate style using SelfNorm motivated by the fact that attention help emphasize essential styles and suppress trivial ones and reduce texture bias using CrossNorm by swapping feature maps within one instance. The paper presents two new methods to improve corruption robustness and domain generalization: SelfNorm, a way to adapt style information during inference, and CrossNorm, a simple data augmentation technique diversifying image style in feature space. Summary This paper proposes two novel norms: Selfnorm and Crossnorm for model robustness. I find it difficult to follow the rationale behind key assumptions (e.g. that shape = content and texture = style) and the experimental section is, I have to admit, a bit chaotic. Cons: - In Sec. 3 Unity of Opposites, the authors explains that SelfNorm works during testing while CrossNorm functions only in training. - The authors explains SelfNorm recalibrate feature style while  CrossNorm performs style augmentation. - Extensive experiments under different settings and tasks show the effectiveness of the proposed method. Crossnorm performs style augmentation (swap channel-wise means and variances) to reduce texture bias. For example, why is ImageNet performance for SN and CN not compared against augmentation baselines considered in the CIFAR experiments in Tab1? Detailed comments are summarized as follows: Pros: - The paper contributes a solution by forming a unity of opposites in using style for model robustness. The intuition of the proposed method is clearly presented and the paper is well structured. Selfnorm recalibrates style in features to reduce texture sensitivity. Their method shows state-of-the-art robustness performance on both fully and semi-supervised settings, and classification and segmentation tasks. This paper investigates the CNN model robustness against problems, e.g., texture sensitivity and bias.
The authors present a systematic evaluation of atomistic learning across multiple tasks and show that 3D data consistently yields better performance than 1D and 2D methods. [3] Karimi et al: DeepAffinity: Interpretable Deep Learning of Compound-Protein Affinity through Unified Recurrent and Convolutional Neural Networks In this paper, the authors introduce a repository of datasets for several atomistic learning tasks. Some tasks like Ligand Binding Affinity (LBA) and Ligand Efficacy Prediction (LEP) requires modeling representations of both proteins and small molecules. By creating a standardized set of prediction tasks and associated data sets, the authors have presented a resource that may help the community to compare 3D atomistic methods quickly and fairly. This paper presents a large benchmark of machine learning tasks for molecules represented by the 3D coordinates of their atoms. Determining the multiple possible conformations of drug-like molecules is still an ongoing research topic (see for example the review of Hawkins (2017)), not to mention the determination of the 3D structure of proteins, which is indeed the topic of one of the data sets provided. I feel that the selection of multiple tasks is limiting the authors in the amount of information that they can fit in the actual paper. Actually, the abstract (and, more generally, the paper) reads as if neural networks were the only kind of machine learning algorithms that could be applied to molecules and that very little work has been done in the past to incorporate 3D information in chemoinformatics. The authors argue that these datasets will serve as a stepping stone for machine learning researchers interested in developing methods for atomistic learning and rapidly advance this field. In addition, three deep-learning algorithms are implemented and evaluated on these benchmarks, and compared to state-of-the-art methods that do not use 3D information, and empirically demonstrate the benefit of incorporating 3D information in the networks. The vast majority of machine learning methods that have been developed for molecules use either 1D or 2D information. A systematic benchmark with atomistic learning methods is presented, showcasing the value of using 3D atom-level data instead of 1D or 2D features. The idea of using atomistic learning or at least 3D derived features have already been implemented or at least contemplated in many of the presented tasks (Gilmer et al [1], Wu et al [2], Townshend et al. "Graph kernels for molecular structure− activity relationship analysis with support vector machines." Journal of chemical information and modeling 45.4 (2005): 939-951. The availability of standard curated datasets helps to advance the field by making it easier to develop and compare new methods for these tasks.
S2) The well-designed experiments show the counterintuitive fact that even static random generated sparse network can outperform dense networks at the same capacity (number of parameters) if proper settings which improve the gradient flow are made, e.g. PReLU or SReLU as activation function, batch normalization. Compared to (unnormalized) gradient flow, the paper shows that the proposed EGF is (slightly) better correlated with metrics like test loss and test accuracy (see Table 1).
The paper proposes a sensor fusion approach combining radar and camera to improve the detection of object in an automotive sensing scenario. Cons: Magic numbers?: in paragraph 2 of section 4, the author(s) list the way to split the regions and sampling rates for different regions, but does not explain the reason to do so. However, since object detection on radar data is already proposed and studied by some researchers (e.g., [1][2][3]), the reason why this problem needs to be proposed separately is not clear to me. The authors propose an algorithm to select radar return regions that potentially contain objects inside. While it is generally a great idea to guide the selection of radar regions to be sampled at a higher rate the paper is very application-focused and lacks novelty in its method. ########################################################################## Summary: The paper develops a method to select a radar return region to be sampled at a higher rate based on a previous camera image and radar recording.
Specifically, this paper considers 1) unsupervised setting; 2) the hierarchy of concepts, and conducts experiments in two datasets. The paper introduces the solution of an important task: hierarchical concept learning(or temporal abstractions) from demonstration data. This paper addresses the problem of extracting a hierarchy of concepts in an unsupervised way from demonstration data. Strengths: The paper focuses on the important problem of exploiting weakly labeled video data, by exploiting its structure, for example by recovering temporal structure in an autoencoder fashion. This paper addresses a relatively new topic to learn the hierarchical concepts in videos and commentary in an unsupervised manner. The cons include (In my opinion, the main weakness is the experiments): Some design of model (e.g., Traversing across the concept hierarchy, Observation, and Instruction Regeneration, etc) are not fully estimated in this section: are they really useful? The authors present a Transformer-based concept abstraction architecture called UNHCLE and show how it discovers meaningful hierarchies using datasets from Chess and Cooking domains. The proposed model's performance does not exceed the simple equal division baseline a lot when 64 frames are sampled for each video. Introducing a two-level hierarchy into concept learning is also not new. The proposed model utilizes multiple regeneration module for the concept learning, which may also help for other hierarchical-aware tasks. This paper presents a method to unsupervisedly discover structure in unlabeled videos, by finding events at different temporal resolutions. The cooking video dataset does not have such annotations so hypothetically the one can set an arbitrary number for the levels of hierarchy.
Summary: This paper combines the existing scaling techniques to reduce the communication cost of distributed SGD among a large number of computing nodes. (6) On page 2, the authors said "SwarmSGD has a Θ(n) speedup in the non-convex case, matching results from previous work which considered decentralized dynamics but which synchronize upon every SGD step." What is the measure, is it the number of communications, local SGD iterations or gradient evaluations? (3) In Theorem 4.1, the assumption that T>=n^4 (n^4 can be very large) is the disadvantage of this algorithm because the same convergence rate O(1/sqrt(T)) has been achieved without such assumption in some distributed settings, including plain distributed SGD, federated average, etc. Contributions: This paper analyzes the convergence of decentralized SGD with asynchronous updates, quantization and local updates, which is novel and challenging. The authors prove that this combined algorithm converges to a local optimal point. These techniques include asynchronization, local updates, and quantized communication. Summary The paper proposes and analyses a distributed learning algorithm for training with Stochastic Gradient Descent a global model on a regular graph, that allows for local and asynchronous gradient updates. The authors claim that this is the first work to consider decentralization, local updates, asynchrony, and quantization in conjunction. Even nodes don't communicate, the algorithm should still converge because the global sampling. Still, it would help to clarify the following points from the beginning: what the authors mean by decentralized, later explained as decentralized model updates, but centralized/distributed data for the experiments; This paper considers several techniques to minimize decentralized cost for training neural network models via stochastic gradient descent (SGD). (7) In the contribution part, the authors mention that their new algorithm has lower average synchronization cost per iteration but more iterations in the experiments, how about the total synchronization cost? (8) The authors use multiple variables to denote the number of nodes, including n, P and m. asynchronous updates: the number of local gradient updates Hi can vary between nodes and between every edge update; Remarks on theoretical analysis Theorem 4.1 shows that the average second moment of the loss gradient evaluated at the average model μt is bounded and decreases with T, proving that the model updates converge to a local minimum. The authors have done extensive analysis of the convergence under different settings with detailed proofs. However, if the analysis can not explain why more local updates can reduce communications, I would not recommend to accept. In the proposed algorithm, each time an edge is activated and the two nodes connected through the edge are updated. The proposed algorithm requires significantly less communications to converge. These techniques include asynchronous, decentralized, or quantized communication. (4) The claim in the abstract that the new algorithm can converge to local minima is not supported, since the theorems only imply gradient convergence.
Prior work used uniform samples, but this paper proposes an approach based on locality-sensitive hashing. This paper proposes a neighbor search method for negative sampling in extreme classification.
Summary The paper suggests a novel framework (coined L2E) to learn a policy that is optimized to adapt quickly (and exploit) a wide range of unknown opponents. Methodological novelty: the authors propose OSG that produces effective opponents for training automatically through adversarial training and diversity-regularized policy optimization to improve the generalization ability of L2E. Summary: The authors propose an opponent modeling in 1-vs-1 games called the Learning to Exploit (L2E) framework, which exploits opponents by a few interactions with different opponents during training so that it can adapt to new opponents with unknown styles during testing quickly. Adding an algorithm in the Appendix when adapting to a new opponent after the base policy training (i.e., the meta-testing procedure) will be helpful. In particular, the authors propose Opponent Strategy Generation (OSG) that produces effective opponents for training automatically through adversarial training for eliminating its own strategy's weaknesses and diversity-regularized policy optimization to improve the generalization ability of L2E. Summary: This paper proposes the Learning to Exploit (L2E) framework that can quickly adapt to diverse opponent's unknown strategies. In fact the empirical results suggest that the base policy is breaking even against a random opponent (before adaptation) at Leduc poker, which seems rather weak to me. After exposing this training procedure, the authors evaluate L2E on 3 toy games, showing that the trained policies are indeed able to benefit from little adaptations to a variety of heuristic opponents and perform better than some baseline methods.
The paper is overall definitely interesting because adversarial attacks in few-shot learning is not a studied topic, to the best of my knowledge, and the proposed approach appears to be effective, especially compared to a meaningfully-defined baseline called MAML-AD. Recommendation Because of the lack of motivation for adversarial meta-learning, lacking intuition for ADML, and particularly the robustness evaluation against a very weak attack, the paper is a clear reject to me. The paper propose a new meta-learning algorithm ADML that uses adversarial and clean examples during meta-training (both for train-train and train-test). Summary This paper presents a variant of MAML that aims to make the meta-learned initialisation robust to adversarial examples. It could have been definitely interesting to see the robustness of the ADML model when it is attacked by a different model at test time (in generation of query and/or support samples of few-shot classes). The authors demonstrate that a simple approach of mixing adversarial and clean data in the usual MAML update doesn't work very well, but don't go in to any details as to why this failure motivates their method.
The authors propose a gradient-based meta learning method (MACAW) to approach this problem, which uses an actor-critic method combined with advantage weighting which is an offline RL method. Introducing MACAW: an algorithm for offline meta-RL that has the desirable property of being consistent (i.e. converges to a good policy if enough time and data for the meta-test task are given, regardless of meta-training). Per paper definition, if an algorithm can find a good solution to test tasks regardless of the meta-training task distribution, is called consistent. My comments: While this paper touches on a very interesting and practical problem in meta-rl and batch-rl, I didn't find their setting is very realistic with respect to batch and offline RL setup. It would still be great to add an experiment where MACAW is adapted online at test time (entirely without offline data) like in C.2 but on in-distribution tasks. The paper also proposes a method for the fully offline meta-RL problem based on the MAML method. The paper proposes the problem of fully offline meta-RL. Would MACAW also work on the standard metaRL setting, where the agent is interacting with the environment during meta training and therefore responsible for collecting the data itself? A main motivation for the offline RL setting is that you can use real-world data and train a metaRL agent using this. To do so they rely on MAML (which provides consistency) and AWR (a simple, popular offline RL algorithm) and add a couple of changes: some hyper-network like parameterization to add capacity and adding an extra objective in the policy update to enrich the inner loop. The latter is termed as fully offline meta-RL and is the problem setting considered in this work. However, in the offline setting the policy that generated the batch of data is of critical importance and should be part of the task definition. This paper introduces a new problem setting in meta reinforcement learning, namely metaRL. My understanding from this definition is training tasks are not important at all and test-tasks can have a very different distribution from training data and still work. The authors also explore settings of good/bad adaptation data showing the robustness of the proposed method to quality of offline adaptation data as compared with MAML+AWR
The authors use statistical measures such correlation between model predictions, change in performance with and without ensemble game models, and a state of the art method to characterize the functional behavior. Summary of the paper: The authors analyze the effect of sources of uncertainty on neural network performance. Positives: The paper offers some interesting revelations such as : all sources of uncertainty have similar effects, which is surprising as the authors note, and hence a valuable insight. The authors make the surprising discovery that each source of nondeterminism results in an equal amount of variability and model diversity. The authors establish an experimental protocol to understand the effect of optimization nondeterminism on model diversity, and study the independent effect of different sources of nondeterminism. The paper would be strengthened by having experiments on popular benchmark state-of-the-art networks (maybe fewer runs and not as many nondeterminism sources as in Table 1). Linking all the nondeterminism to a change of one bit in model weights is interesting and successfully highlights the instability and sensitivity of neural network optimization.
This paper proposes an efficient transformer variant by replacing softmax in the self-attention layer with a RELU activation and arranging the computation using element-wise products and global/local pooling. The evaluation performed on multiple tasks shows that the proposed approach can reach the quality of a vanilla transformer and be more memory-efficient. The paper suggest an alternative to the Multi-Head Attention (MHA)  operation, which is one of the core elements in Transformers models. For AFT models, on the other hand, this difference is negative which might suggest that they have already overfit at 70k iterations and they will never reach the resulting performance achievable by the baseline.
This paper presents an interesting collaborative MHA to enable heads to share projections, which can be easily applied to most existing transformer-based models, including NMT and pre-training models. Namely, while the original definition of attention layers does not have biases in linear projections for q/k/v in attention, the authors claim that implementations contain the bias terms, and spend some time showing how to model the biases in key and query layers properly. Since the method operated only within attention layers (reduces only dimensions of queries and keys), in terms of efficiency/quality trade-off it should be compared to other methods, e.g. simple head pruning. While the paper does not provide such comparison, it is clear from the results that the simple head pruning is likely to be superior (and is simpler implementation-wise). The authors show that query-key projections are redundant because trained concatenated heads tend to compute their attention patterns on common features. While there are some parts of the paper that I like, the main claim is not supported empirically: both in terms of baselines and the overall decrease in the number of parameters. Overall, the paper is well motivated and provides a deep analysis of redundancy of the multi-head attention. -- From Figure 3 and Table 2, even reducing the Dk^ from 768 to 128, the total number of parameters of pretrained models (e.g. BERT-base) only reduces from 108.3M to 96.6M. The authors also propose a Tensor Decomposition based method to easily convert MHA to its collaborative version without retraining. This paper analyzes the multi-head attention in transformers and suggests to use collaboration instead of concatenation of multiple heads. Pros the method is novel and well-motivated works for both original transformer and pre-trained language models All the pre-trained models (BERT/DistilBERT/ALBERT) are evaluated on the GLUE dataset, the experiments on more challenging tasks like QA (e.g. SQuAD 1.1/2.0) should be added. The proposed method seems to be not effective for pre-trained models, e.g. when the number of parameters is decreased from 108.5M to 96.6M, this reduction of model size is not that big, while the average score decreases from 83.2 to 77.6.
But for the three bullet points in section 1, the first point of "Generalizing Step-based and Trajectory-based Exploration" should not be one of the main contributions of this paper, because this paper follows the formulation of policy in van Hoof et al. Summary of the paper The basis of this work is van Hoof et al., 2017; there, "Generalized Exploration" views policy parameters as being drawn from a per-trajectory Markov chain. Following the prior work, this paper proposes an exploration method unifying the step-based and trajectory-based exploration. Summary: This paper proposes Deep Coherent Exploration that unifies step-based exploration and trajectory-based exploration on continuous control. Experiments show that the proposed exploration strategy mostly helps A2C, PPO and SAC in three Mujoco environments. This paper presents a method to combine step-based exploration with trajectory-based exploration (in the form of action-space noise and parameters-space noise) in continuous MDPs, which is scalable to deep RL methods. Pros: For combining the proposed method with on-policy learning, this paper derives the log-likelihood of whole trajectory recursively. For on-policy methods (A2C, PPO), the proposed method has large performance gain on Mujoco tasks.
Page 12: Please correct the number of table in "Table ??" This paper proposes a method for estimating the probability deinsity distribution on a low-dimensional manifold embedded in a high-dimensional space using Normalizing Flow (NF). Motivated by the fact that NF are diffeomorphic  transformations from a simple space, ideally Euclidean, the author address the problem of modeling data which distribution is defined on more complex and unknow manifolds.The idea consists in inflating the data manifold with suitable noise (normal noise) in order to make it diffeomorphic to a simpler space where a NF can be estimated. This paper argues that Gaussian noise e is a good approximation of en when the manifold dimension d is sufficiently smaller than the higher-dimensional Euclidean space dimension D. The proposal of this study is to make NF applicable by inflating low-dimensional manifolds with Gaussian noise.
Considering the significance of the improvement on computational efficiency, it will be more valuable to discuss whether such a kind of decoupled search can be generally applied, or it can only work for certain kinds of network architecture, data, or tasks. In Section 3 "In FTSO, this problem is almost nonexistent because, the skip connection has no kernel weights to tune", this is not very clear how the problem exists in the DARTS formulation and how the formulation in the proposed approach can mitigate this issue. In the first approach, the found topology is equipped with some operators (e.g., 3x3 convolution,  skip connection, and 3x3 dilated convolution) and then the architecture parameters are optimized.
Agreed that PER is a reasonable choice, and it can upper bound the EIB and EVB metrics (i have issues with this too, more on this next), it just seems to me that the paper doesn't make any convincing claim for why this helps us understand why PER works. Summary: The authors of this paper make a connection between the TD-error from a single unit of experience and various metrics of improvement for agents trained with prioritized experience replay and Q-learning or soft Q-learning. The authors proposed three different value metrics to quantify the experience, and showed that they are upper bounded by the TD error (up to a constant). Notably, that when we know it upper bounds the three metrics, we want to continue prioritizing sampling experiences when training our agent, because this will yield faster learning, since we will have larger improvements to our agent.
I am not able to follow the proofs in this paper due to missing definitions of terms and notations. The authors claim that: 1) they "build tractable algorithms with polynomial complexity", "a detection algorithm linear in time"; 2) the algorithms are very suitable for parallel architectures; 3) an improvement of the state of the art for the symmetric tensor PCA experimentally. The paper presents a pair of interesting algorithms using trace invariants to detect the signal in the signal-plus-noise tensor PCA framework. Page 8: "eg" should be "e.g." Summary: The paper provides an interesting algorithm for tensor PCA, which is based on trace invariants. At the current stage, filled with undefined or inconsistent notations and terms, this paper is not self-contained and hard to follow. The authors claim that they propose a new framework to solve the problem, by looking at the trace invariants of tensors. It is claimed in the paper that the algorithm improves the state-of-the-art (signal to noise ratio requirements) in several cases, while a brief survey/table of the recent results is missing. Summary: This paper studies the detection and recovery problem in spiked tensor models in the form T = \\beta v0^\\otimes k + Z, where v0 is the underlying spike signal and Z is a Gaussian noise. This becomes worse considering the fact that this paper studies tensor problems -- many tensor-related terms have multiple definitions (e.g., eigenvalues, ranks).
This paper introduces the problem of enforcing group-based fairness for "invisible demographics," which they define to be demographic categories that are not present in the training dataset. This paper tackles a fair classification problem with an invisible demographic, a situation where the records who have some specific target labels and sensitive attributes are missing. #Summary This paper studies zero-shot fairness where the demographic information is partially unavailable, but assuming the existence of a context dataset that contains all labels x all demographics (including the invisible). Summary This paper proposed a problem in algorithmic fairness where labeled examples for some demographic groups are completely missing in the training dataset and still the goal is to make predictions that satisfy parity-based fairness constraints. #Over recommendation I think this paper studies a very interesting problem but some further analysis, e.g., how the distribution over the context data affects the results, and how to make the algorithm work reliably better in practice, is needed. The paper proposes a disentanglement algorithm that separates information of the label and demographics, under two zero-shot settings: 1) learning with partial outcomes: both labels and both demographics are available, but for one of the demographics only negative outcome is present; The method developed to solve this problem uses a "context" dataset with unlabeled data but containing individuals from all demographics to construct a 'perfect dataset' and 'disentangled representations' Empirical: They provide experiments on two benchmark datasets (colored MNIST and Adult) comparing their proposed method to multiple baselines. The paper states an interesting and practically relevant problem of enforcing fairness with "invisible demographics." The methodology is overall well documented, and the experimental baselines make sense. They demonstrate by the empirical evaluations that the proposed disentangled representation learning algorithm success to mitigate unfair bias by utilizing the perfect dataset, a dataset in which the target label and sensitive attribute are independent.
If I understand correctly, the only difference between previous work (Eqn 2) and the proposed method (Eqn 3) is the additional parameter gamma-sa/beta-sa. The extensive experiments demonstrate the effectiveness of such methods in neural architecture search (NAS), image generation, adversarial training, and style transfer. (2)What is the difference between the searched architectures by using BN, CCBN and SaBN? (2) I think it will be better to add the SaBN in Fig3, and point out the correspondence between the candidate operations and the conditional affine layer. Additionally, since it appears to amount to a per-dimension reparameterization, the reader might expect that some of the other reparameterizations could have a similar effect (including such simple interventions as changing the learning rates for some of the gamma or beta parameters), and compellingly demonstrating that the specific reparameterization given by SaBN outperforms such alternatives would make the paper stronger.
As mentioned during the introduction, this isn't a particularly novel framework (considering RL as inference, all the work done in temporal generative model for RL, or the rather new formulation by Hafner 2020 [4]) and it doesn't appear like it provides a very interesting fit to the object-centric RL model proposed. This paper proposes an extension of the RL as Inference framework, and demonstrates how to use it to express an object-centric RL model and train it on simple environments. In my opinion, it should be stated more clearly in the paper that the main contribution is the application of the object-based perception model within the control-as-inference framework, and not an introduction of a novel framework. This paper features two complementary contributions: The perception and control as inference (PCI) framework, which describes the graphical model of a POMDP with auxiliary optimality variables, allowing for the derivation of an objective for optimizing both a perception model and policy jointly to maximize log⁡p(O,x∣a). Overall: I consider this paper interesting and agree with the authors that the proposal of joint models is appealing and worthwhile investigating, but I would require much stronger evaluation to show that the joint aspect of OPC is driving the performance here, instead of just the object-based featurization. Comments/questions: I do not feel like the way the abstract and introduction argue for a general framework "Joint Perception and Control as Inference" is productive. Incorporating a perception model into the the control-as-inference derivation to yield a single unified objective for multiple components in a pipeline is an interesting idea and complements other work on incorporating reward structure into model-learning nicely. The main contribution of the paper is the incorporation of the object-based inference module (Greff et al., 2017; van Steenkiste et al., 2018) into the control-as-inference framework under partial observability. I would also argue that COBRA should be cited in the "Model-based Deep Reinforcement Learning" as a closely related object-centric model-based RL method, it is more relevant than the World Model paper.
This conditional network approach is illustrated for two standard convolutional neural network (CNN) architectures, U-Net and VGG, on two benchmark datasets suitable for OOD detection, the Inria Aerial Image Labeling Dataset and the Tumor Infiltrating Lymphocytes classification dataset. The paper demonstrates the experiments on two tasks (i.e. semantic segmentation, image classification), but the proposed conditional network is different and does not have a unified architecture. on AMLL U-Net higher IoU scores and accuracies on the transfer (test) set are reported (table 2, page 5 in their paper) compared to table 1, which in fact exceed the performance results of the proposed method. ########################################################################## Summary: This submission proposes an approach to modulate activations of general convolutional neural networks by means of an auxiliary network trained on additional metadata to a dataset.
