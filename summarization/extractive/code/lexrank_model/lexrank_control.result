I am eager to see a hardware realization for this method because of its promising results. In overall, this paper is an accept since it shows good performance on standard problems and invent some nice tricks to implement NN in hardware, for *both* training and inference.
The results show that the agents generate effective communication systems, and some analysis is given of the extent to which these communications systems develop compositional properties – a question that is currently being explored in the literature on language creation. They provide rich analysis of the emergent languages the agents produce under different experimental conditions.
The papers encode the mode as a hidden variable in a stochastic neural network and suggest stepping around posterior inference over this hidden variable (which is generally required to do efficient maximum likelihood) with a biased importance sampling estimator. Even the control task is very similar to the current proposed task in this paper. Since the variational bound has much better sampling properties (due to recognition network, reparameterization trick and bounding to get log likelihoods instead of likelihoods) it is hard to believe that it is harder to get to work than the proposed framework. I can see how it adds value over sampling from the prior, but it's unclear if it has value over a modern approximate inference scheme like a black box variational inference algorithm or stochastic gradient MCMC. Why not adding (non Bayesian) context (not label) to the task will not work as well? They apply this approach to multi-modal (several "intentions") imitation learning and demonstrate for a real visual robotics task that the proposed framework works better than deterministic neural networks and stochastic neural networks. 2. New latent variable model bound that might work better than classic approaches.
This paper proposes a method to build a CNN in the Winograd domain, where weight pruning and ReLU can be applied in this domain to improve sparsity and reduce the number of multiplication. The paper is well-written. It provides a new way to combine the Winograd transformation and the threshold-based weight pruning strategy. - the ReLU activation function associated with the previous layer is applied to the Winograd transform of the input activations, not directly to the spatial-domain activations, also yielding sparse activations Specifically, ReLU nonlinearity was moved after Winograd transformation to increase the dynamic sparsity in the Winograd domain, while an additional pruning on low magnitude weights and re-training procedure based on pruning is used to increase static sparsity of weights, which decreases computational demand. Because this yields a network, which is not mathematically equivalent to a vanilla or Winograd CNN, the method goes through three stages: dense training, pruning and retraining. 525-542. Springer International Publishing, 2016. This paper proposes to combine Winograd transformation with sparsity to reduce the computation for deep convolutional neural network. Or is it almost exactly the same as for general Winograd CNNs? In particular, the pruned model in the Winograd domain performs comparably to the state-of-the-art dense neural networks and shows significant theoretical speedup.
depending on the application) leads to empirical improvement on three tasks - convex hull finding, k-means clustering and on TSP. Using an architecture to learn how to split the input, find solutions, then merge these is novel. - simple merge or RL and SL in an end-to-end trainable model
Given that in many applications such parent-class supervised information is not available, the authors of this paper propose domain specific pseudo parent-class labels (for example transformed images of digits) to adapt ACOL for unsupervised learning. The novelty seems to be in the adaptation to GAR from the semi-supervised to the unsupervised setting with labels indicating if data have been transformed or not. The main idea is to exploit a schema of semisupervised learning based on ACOL and GAR for an unsupervised learning task. This paper utilizes ACOL algorithm for unsupervised learning.
########## UPDATED AFTER AUTHOR RESPONSE ########## Hence my current more neutral review rating. I would like to see latent traversals of the best DIP-VAE vs beta-VAE to demonstrate good disentangling and the improvements in reconstruction quality. I would like to make some connection to some work on understanding VAE objective (Hoffman & Johnson 2016, ELBO surgery: yet another way to carve up the variational evidence lower bound) where they derived something along the same line of an aggregated posterior q(z).
Additionally, it is demonstrated that flipout allows evolution strategies utilizing GPUs. Overall this is a very nice paper. In Section 4 they provide quite varied empirical analysis: they confirm their theoretical results on four architectures; they show its use it to regularise on language models; they apply it on large minibatch settings where high variance is a main problem; and on evolution strategies. The paper also proves that this approach reduces the variance of the gradient estimates (and in practice, flipout should obtain the ideal variance reduction). Strategies) suffer from a high variance of the gradient estimates.
There is also a strong correlation between (binarized weights)*activations and (binarized weights)*(binarized activations). (2) Except the first layer, the dot product of weights*activations in each layer is highly correlated with the dot product of (binarized weights)*activations in each layer. This is claimed to entail that the continuous weights of the binarized neural net approximate the continuous weights of a non-binarized neural net trained in the same manner. This paper presents three observations to understand binary network in Courbariaux, Hubara et al. (2016). Pros: The authors lead a very nice exploration into the binary nets in the paper, from the most basic analysis on the converging angle between original and binarized weight vectors, to how this convergence could affect the weight-activation dot product, to pointing out that binarization affects differently on the first layer. c. For BNNs, where both the weights and activations are binarized, shouldn't we compare weights*activations to (binarized weights)*(binarized activations)? In general, I think the paper is written clearly and in detail. Some typos and minor issues are listed in the "Cons" part below.
The manuscript proposes a generative approach to detect which samples are within vs. As out of distribution samples are hard to obtain, authors also propose to use GAN generating "boundary" samples as out of distribution samples. Classifier is trained to not only maximize classification accuracy on the real training data but also to output a uniform distribution for the generated samples. However, equation 4 and algorithm 1 were very helpful in clarifying the cost function. The problem setting is new and objective (1) is interesting and reasonable. This paper is clearly written, proposes a simple model and seems to outperform current methods. Moreover, [1] did not seem to generate any out of distribution samples. Along those lines, it would be interesting if instead of the uniform distribution, a model that explicitly models within vs. Algorithm 1. is called "minimization for detection and generating out of distribution (samples)", but this is only gradient descent, right?
The authors at first reduce the input size by using various extractive strategies and then use the selected content as input to the abstractive stage where they leverage the Transformer architecture with interesting modifications like dropping the encoder and proposing alternate self-attention mechanisms like local and memory compressed attention. The main strength is in the task setup with the dataset and the proposed input sources for generating Wikipedia articles. This paper is quite original and clearly written. With a fixed reference/target Wikipedia article, if different models generate variable sizes of output, ROUGE evaluation could easily pose a bias on a longer output as it essentially counts overlaps between the system output and the reference. Unfortunately it is hard to judge the effectiveness of the abstractive model due to the scale of the experiments, especially with regards to the quality of the generated output in comparison to the output of the extractive stage. More importantly, it was not clear from the paper if there was a constraint on the output length when each model generated the Wikipedia content. Although authors argue in Section 2.1 that existing neural approaches are applied to other kinds of datasets where the input/output size ratios are smaller,  experiments could have been performed to show their impact. --- The additional experiments and clarifications in the updated version give substantially more evidence in support of the claims made by the paper, and I would like to see the paper accepted.
- Experiments (in main paper) only show speedups and do not show loss of accuracy due to sparsity The paper proposes improving performance of large RNNs by combing techniques of model pruning and persistent kernels. Unfortunately, due to the low-level and GPU specific nature of the work, I would think that this work will be better critiqued in a more GPU-centric conference.
The paper extends softmax consistency by adding in a relative entropy term to the entropy regularization and applying trust region policy optimization instead of gradient descent. - Trust-PCL (off-policy) significantly outperform TRPO in terms of data efficiency and final performance. - Some ablation studies (e.g., on entropy regularization and relative entropy) and sensitivity analysis on parameters (e.g. \\alpha and update frequency on \\phi) would be helpful.
However, applying a deep representation for the compression and then directly solving a vision task (classification and segmentation) can be considered as a novel idea. The most interesting part is a joint training for both compression and image classification. In fact, I am glad someone asked this question and tried to answer it. I changed my given score from 3 to 6. Neural-net based image compression is a field which is about to get hot, and this paper asks the obvious question: can we design a neural-net based image compression algorithm such that the features it produces are useful for classification & segmentation?
* The writing is reasonably clear (up to the terminology issues discussed among the weak points), and introduces properly the adversarial attacks considered in the work. Four adversarial attacks strategies are considered to attack a Resnet50 model for classification of Imagenet images. On the contrary, white-box means that the adversary knows everything about the classification method, including the transformation implemented to make it more robust to attacks. The authors show that the simple input transformations advocated work against the major kind of attacks. Another example of attack that account for transformation knowledge (and would hopefully be more robust than the attacks considered in the manuscript) could be one that alternates between a conventional attack and the transformation. The discussion remarks are particularly interesting: the non differentiability of the total variation and image quilting methods seems to be the key to their best performance in practice. Therefore, I would only recommend acceptation if room.
The paper proposes the Skip RNN model which allows a recurrent network to selectively skip updating its hidden state for some inputs, leading to reduced computation at test-time. This paper proposes an idea to do faster RNN inference via skip RNN state updates.
Overall the strength of this paper is that the main insight is quite interesting — though many people have informally thought of residual networks as having this interpretation — this paper is the first one to my knowledge to explain the intuition in a more precise way. 2. The l2 ratio. The l2 ratio is small for higher residual layers, I'm not sure how much this phenomenon can prove that resnet is actually doing iterative inference.
[1] Kulesza, Alex, and Ben Taskar. However, I am a little worried that the proposed model may be hard to reproduce due to its complexity and therefore choose to give a 6. The rebuttal is convincing and I decided to increase my rating, because adding the proposed counting module achieve 5% increase in counting accuracy. I think that it will only be a problem for extremely large numbers. The paper is revised and I saw NMS baseline is added. I would recommend the authors to use something more general, like graph convolutional neural networks (Kipf & Welling, 2017) or graph gated neural networks (Li et al., 2016).
The model proposed is a variant of the cycle GAN in which in addition embeddings helping the Generator are learned for all the values of the discrete variables. In fact the regularization of the Jacobian that will be preventing the discriminator to vary too quickly in space is more likely to explain the fact that the discrimination is not too easy to do between the true and mapped embeddings.
The paper proposes a way to speed up the inference time of RNN via Skim mechanism where only a small part of hidden variable is updated once the model has decided a corresponding word token seems irrelevant w.r.t. a given task. * One obvious way to reduce the computational complexity of RNN is to reduce the size of the hidden state. So that readers can check benefits of the skim RNN against skip-RNN and small-sized RNN.
This paper abstracts two recently-proposed RNN variants into a family of RNNs called the Linear Surrogate RNNs which satisfy  Blelloch's criteria for parallelizable sequential computation. In the following, the authors explore the landscape of RNNs satisfying the necessary conditions. Linear Surrogate RNNs is an important concept that is useful to understand RNN variants today, and potentially other future novel architectures.
The idea to constraint the dimension reduction to fit a certain model, here a GMM, is relevant, and the paper provides a thorough comparison with recent state-of-the-art methods. Significance – Constraining the dimension reduction to fit a certain model is a relevant topic, but I'm not convinced of how well the Gaussian model fits the low-dimensional representation and how well can a neural network compute the GMM mixture memberships. Perhaps the biggest innovation is the use of reconstruction error features as input to a subnetwork that predicts the E-step output in EM for a GMM.
This is complemented by the SCAN model, which is a beta-VAE trained to reconstruct symbols (y; k-hot encoded concepts like {red, suitcase}) with a slightly modified objective. * Does training SCAN with the structure provided by the logical operators lead to improved performance? The two input concepts {red} and {suitcase} are each encoded by a pre-trained SCAN encoder and then those two distributions are combined into one by a simple 1d convolution module trained to implement the AND operator (or IGNORE/IN COMMON). and baselines. 5) Concepts expressed as logical combinations of other concepts generalize well for both the SCAN representation and the baseline representations.
In this paper, the authors studied the problem of semi-supervised few-shot classification, by extending the prototypical networks into the setting of semi-supervised learning with examples from distractor classes. + the paper is well written, well organized and overall easy to read Overall, I would like to vote for a weakly acceptance regarding this paper.
This paper defines building blocks for complex-valued convolutional neural networks: complex convolutions, complex batch normalisation, several variants of the ReLU nonlinearity for complex inputs, and an initialisation strategy. Their contributions are: 1. Formulate complex valued convolution Since any complex valued computation can be done with a real-valued arithmetic, switching to complex arithmetic needs a compelling use-case. - Although care is taken to ensure that the complex and real-valued networks that are compared in the experiments have roughly the same number of parameters, doesn't the complex version always require more computation on account of there being more filters in each layer? 2. Formulate two complex-valued alternatives to ReLU and compare them 4. Formulate complex analogue of Glorot weight normalization scheme Authors present complex valued analogues of real-valued convolution, ReLU and batch normalization functions.
The authors effectively demonstrate with few toy examples the weaknesses of traditional methods, i.e max pooling and average pooling. I encourage the authors to address my concerns in their rebuttal The paper proposes to use discrete wavelet transforms combined with downsampling to achieve arguably better pooling output compared to average pooling or max pooling. As such, I think it will be a nice addition to *CONF*, especially if the authors decide to run some of the experiments I was suggesting, namely: show what happens when larger pooling windows are used (say 4x4 instead of 2x2), and compare to other lossy techniques (such as Fourier or cosine-transforms).
The algorithm seems to be easily used in practice. Otherwise, the paper is clearly written. The numerical examples are relatively clear and easy to figure out details. 1. While the paper presents the algorithm as an optimization algorithm, although it gets better learning performance, it would be interesting to see how well it is as an optimizer. I understand that you are trying to improve the existing results with their optimizer, but this paper also introduces new algorithm.
Summary of paper: The authors present a novel attack for generating adversarial examples, deemed OptMargin, in which the authors attack an ensemble of classifiers created by classifying at random L2 small perturbations. The paper presents a new approach to generate adversarial attacks to a neural network, and subsequently present a method to defend a neural network from those attacks. As a summary, the authors presented a method that successfully attacks other existing defense methods, and present a method that can successfully defend this attack. The provided analysis is insightful, though the authors mostly fail to explain how this analysis could provide further work with means to create new defenses or attacks. Results are well discussed with reasonable observations. This and / or the defense of training to adversarial examples is an important experiment to assessing the limitations of the attack.
They claim that under the condition that global optima are achieved for discriminator and generator in each iteration, the Coulomb GAN converges to the global solution. The authors draw from electrical field dynamics and propose to formulate the GAN learning problem in a way such that generated samples are attracted to training set samples, but repel each other. The assumption in this paper is too restricted, and the discussion is unfair to the existing variants of GAN, e.g., GMMN or Wasserstein GAN, which under some assumptions, there is also only one global Nash Equilibrium. The model collapsing is not just because loss function in training GAN. On the other hand, the richness of the discriminator also important in the training of GAN.
However the application to program repair is novel (as far as I know). Clarity: The paper is clearly written. - Figure 5 seems to suggest that dependencies are only enforced at points in a program where assignment is performed for a variable, is this correct? This is a fairly strong paper. The proposed models make sense and the writing is for the most part clear, though there are a few places where ambiguity arises: - The variable "Evidence" in equation (4) is never defined. What if the structure of the submitted program doesn't match the structure of the intended solution and hence adding print statements cannot be automated? (2017). Making Neural Programming Architectures Generalize via Recursion.
The fundamental contribution of this article, when put into the context of the many recent publications on the topic of automatic neural architecture search, is the introduction of a hierarchy of architectures as a way to build the search space. As a result, this would allow the evolutionary search algorithm to design modules which might be then reused in different edges of the DAG corresponding to the final architecture, which is located at the top level in the hierarchy. Exploiting compositionality in model design is not novel per se (e.g. [1,2]), but it is to the best of my knowledge the first explicit application of this idea in neural architecture search. Compared to existing work, this approach should emphasise modularity, making it easier for the evolutionary search algorithm to discover architectures that extensively reuse simpler blocks as part of the model. The fundamental contribution of the article is the explicit use of compositionality in the definition of the search space. An important contribution is to show that a well-defined architecture representation could lead to efficient cells with a simple randomized search. It seems fewer hyperparameters are needed to describe VGG-16, making this paper hardly an alternative to the "[common solution] to restrict the search space to reduce complexity and increase efficiency of architecture search."
Exploration is still a challenging problem for RL. There is a big literature on learning from demonstrations that the authors could compare with, or explain why their work is different. However experimentally the paper performs better than the previous approach. The paper is clear, very well written, and well-motivated. The paper is not self-contained. Their proposed algorithm DAgger fixes this (the mistakes by the policy are linear in the horizon length) by using an iterative procedure where the learnt policy from the previous iteration is executed and expert demonstrations on the visited states are recorded, the new data thus generated is added to the previous data and a new policy retrained. - The problem with putting in the replay buffer only episodes which yield high reward is that extrapolation will inevitably lead the learnt policy towards parts of the state space where there is actually low reward but since no support is present the policy makes such mistakes.
The novelty of this work is a refined aggregation process, which is improved in three ways: a) Gaussian instead of Laplace noise is used to achieve differential privacy. Summary: In this work, PATE, an approach for learning with privacy,  is modified to scale its application to real-world data sets. This is done by leveraging the synergy between privacy and utility, to make better use of the privacy budget spent when transferring knowledge from teachers to the student. Two key ideas in the paper include the use of Gaussian noise for the aggregation mechanism in PATE instead of Laplace noise and selective answering strategy by teacher ensemble. It is demonstrated that sampling from a Gaussian distribution (instead from a Laplacian distribution) facilitates the aggregation of teacher votes in tasks with large number of output classes. I am not familiar with privacy learning but it is interesting to see that more concentrated distribution (Gaussian) and clever aggregators provide better utility-privacy tradeoff. on the positive side: Having scalable models is important, especially models that can be applied to data with privacy concerns. The extension of an approach for learning with privacy to make it scalable is of merit. This is for sure an important topic. The paper is well written, and the idea of the model is clear. I think this is a nice modular framework form private learning, with significant refinements relative to previous work that make the algorithm more practical. That is, the privacy guarantee would depend only on public information, rather than the private data. Intuitively, if teacher ensemble does not answer, it seems that it would reveal the fact that teachers do not agree, and thus spend some privacy cost. minor comments: Figure 2, legend needs to be outside the Figure, in the current Figure a lot is covered by the legend This paper considers the problem of private learning and uses the PATE framework to achieve differential privacy.
The experimental results are convincing enough to show that it outperforms other active learning algorithms. Active learning for deep learning is an interesting topic and there is few useful tool available in the literature.
Why? As soon as you start to use clustering to help in floor estimation, you are exploiting prior knowledge about previous visits to the building. Update: Based on the discussions and the revisions, I have improved my rating.
Finally the authors show that almost all tensor train networks (exluding a set of measure zero) require exponentially large width to represent in CP networks, which is analogous to shallow networks. The result of this paper is interesting and also important from a viewpoint on analysis for the tensor train decomposition. While I enjoyed reading the gentle introduction, nice overview of past work, and the theoretical analysis that relates the rank of tensor train networks to that of CP netowkrs, I wasn't sure how to translate the finding into the corresponding neural network models, namely, recurrent neural networks and shallow MLPs. In addition, I would like to see the performance of RNNs and MLPs with the same number of units/rank in order to validate the analogy between these networks. Then they focus on one particular decompostion known as the tensor train decomposition and points out an analogy between tensor train networks and recurrent neural networks. In this paper, the expressive power of neural networks characterized by tensor train (TT) decomposition, a chain-type tensor decomposition, is investigated.
Several of the cited papers use counters to determine which states are "known" and then solve an MDP to direct exploration past immediate outcomes. It would have also been nice to do some analysis on how the update rule in a function approximation case is affecting the bonus terms. " Therefore, a satisfying approach for propagating directed exploration in model-free reinforcement learning is still missing. What is crucially lacking from the paper is any reference to model-free Bayesian methods that have very similar intuition behind them: taking into account the long term exploratory benefits of actions (passed on through the Bayesian inference). ": I think you should cite http://research.cs.rutgers.edu/~nouri/papers/nips08mre.pdf , which also combines a kind of counter idea with function approximation to improve exploration.
The work is fairly novel in its approach, combining a learned reward estimator with a contextual bandit algorithm for exploration/exploitation. -- Can the authors speculate about the difference in performance between the RL and bandit structured prediction settings? My personal conjecture is that the bandit structured prediction settings are more easily decomposable additively, which leads to a greater advantage of the proposed approach, but I would like to hear the authors' thoughts. -- I'm not convinced that the "terms in the parentheses" (Eq. 7) are "exactly the contextual bandit cost". The authors provide a few regret theoretical results (that I did not check deeply) obtained by reduction to "value-aware" contextual bandits.
I now think the paper does just enough to warrant acceptance, although I remain a bit concerned that since the benefits are only achievable with customized hardware, the relevance/applicability of the work is somewhat limited. In the end, while do like the general idea of utilizing the gradient to identify how sensitive the model might be to quantization of various parameters, there are significant clarity issues in the paper, I am a bit uneasy about some of the compression results claimed without clearer description of the bookkeeping, and I don't believe an approach of this kind has any significant practical relevance for saving runtime memory or compute resources. It would be interesting to know an analogy, for instance, saying that this adaptive compression in memory would be equivalent to quantizing all weights with n bits. For instance, in the second paragraph of the introduction, 1st paragraph of section 2. Quantization has some bookkeeping associated with it: In a per-parameter quantization setup it will be necessary to store not just the quantized parameter, but also the number of bits used in the quantization (takes e.g. 4-5 extra bits), and there will be some metadata necessary to encode how the quantized value should be converted back to floating point (e.g., for 8-bit quantization of a layer of weights, usually the min and max are stored). *Delta -> \\Delta in last paragraph of section 2.2
3. The convergence rate depends on \\gamma(\\phi_0), from the initialization, \\phi_0 is probably very close to \\pi/2 (the closeness depend on dimension), which is  also likely to make \\gamma(\\phi_0) depend on dimension (this is especially true of Gaussian). A major strength of the result is that it can work for general continuous distributions and does not really rely on the input distribution being Gaussian; the main weakness is that some of the distribution dependent quantities are not very intuitive, and the alignment requirement might be very high.
[Reviewed on January 12th] This article applies the notion of "conceptors" -- a form of regulariser introduced by the same author a few years ago, exhibiting appealing boolean logic pseudo-operations -- to prevent forgetting in continual learning,more precisely in the training of neural networks on sequential tasks. In Section 2 the authors review conceptors. The notion of conceptors is appealing in this particular use for its interpretation in terms of regularizer and in terms of Boolean logic. I realise not everyone has the computational or engineering resources to try extensively on multiple benchmarks from classification to reinforcement learning.
Summary: The authors provide another type of GAN--the Sobolev GAN--which is the typical setup of a GAN but using a function class F for which f belongs to F iff \\grad f belongs to L^2(mu).
However, as I pointed out above, there are several weaknesses in the paper. The proposed model can generate question with respect to different topic and pre-decode seems a useful trick. Experimental results on question generation are convincing and clearly indicate that the approach is effective to generate relevant and well-structured short questions. When the ground truth topic is provided, it's not fair to compare with the previous method, since knowing the similar word present in the answer will have great benefits to question generation. 4: The automatically extracted topic can be very noisy, but the paper didn't mention any of the extracted topics on AQAD dataset. Table 4 shows some examples, given the sentence, even conditioned on different topics, the generated question is similar. Also, please explain the absence of the "why" type question. "Topic" in general has a broader meaning, I would suggest authors to see this to get an idea about what topic entails to in a conversational setting: https://developer.amazon.com/alexaprize/contest-rules . Taking all these into account, I think this paper still needs more works to make it solid and comprehensive before being accepted. [Strenghts] This paper introduced a topic-based question generation model, which generate question conditioned on the topic and question type.
According to the authors, this paper extends(?) previous results on a NN with a single layer with a single unit. Leveraged by two recent results in global optimization, they showed that with a simple two-layer ReLU network with two hidden units, the problem with a standard MSE population loss function does not have spurious local minimum points. Summary: The paper considers the problem of a single hidden layer neural network, with 2 RELU units (this is what I got from the paper While the ReLU activation is very common in NN architecture, without more motivations I am not sure what are the impacts of these results. These are crucial for understanding the contribution of the paper; while reading the paper, I assumed that the authors consider the case of a single hidden unit with K = 2 RELU activations (however, that complicated my understanding on how it compares with state of the art). The title is not clear whether the paper considers a two layer RELU network or a single layer with with two RELU units. In the abstract the authors state that it has to do with a two-layer RELU network with two hidden units (per layer?
This paper studies the question: Why does SGD on deep network is often successful, despite the fact that the objective induces bad local minima? ii) Theorem 10 and Theorem 6 essentially has the same bound on the right hand side but Theorem 10 additionally divides local volume by global which decreases by exp(-2Nlog N). Pros: Authors ask the question of convergence of optimization (ignoring generalization error): how "likely" is that an over-parameterized (d1d0 > N) single hidden layer binary classifier "find" a good (possibly over-fitted) local minimum. It is not well motivated why one should study the angular volume of the global and local minima.
The paper studies locally open maps, which preserve the local minima geometry. Instead the presentation can be simplified by just discussing the equivalency between local minima and global minima, as the proposed framework cannot handle critical points directly. For example, Lemma 4 is not correct as written — an invertible mapping \\sigma is not necessarily locally open. It will attract some attention at the conference.
Pros: - Nice model which seems to perform well. If authors were interested in the tendency of real program translation task, they should arrange the experiment by collecting parallel corpora between some unrelated programming languages using resources in the real world. Probably we can suppress the range of each attention by introducing some prior knowledge about syntax trees (e.g., only paying attention to the descendants in a specific subtree).
The search result is a new activation function named Swish function. al. (2017). In terms of experimental validation, in most cases the increase is performance when using Swish as compared to other models are very small fractions. Based on the Figure 6 authors claim that the non-monotonic bump of Swish on the negative side is very important aspect. Overall, I think this paper is not meeting *CONF* novelty standard.
This paper presents a study of reinforcement learning methods applied to Erdos-Selfridge-Spencer games, a particular type of two-agent, zero-sum game. An empirical study is performed, measuring the performance of both agents, tuning the difficulty of the game for each agent by changing the starting position of the game. I am also not aware of trying to use games with known potential functions/optimal moves as a way to study the performance of RL algorithms. [p7 Fig 6 and text] Here the authors are comparing how well agents select the optimal actions as compared to how close they are to the end of the game. # Detailed notes [p4, end of sec 3] The authors say that the difficulty of the games can be varied with "continuous changes in potential", but the potential is derived from the discrete initial game state, so these values are not continuously varying (even though it is possible to adjust them by non-integer amounts). This relates to the "surprising" fact that "Reinforcement learning is better at playing the game, but does worse at predicting optimal moves.". There is value in this work, but in its current state I do not think it is ready for publicaiton. Impact: I think this is an interesting and creative contribution to studying RL, particularly the use of an easy-to-analyze game in an RL setting. Is the RL agent able to find policies with longer-term dependence that it can follow? All of this leads me to think that a linear baseline is a must-have in most of the plots, not just Figure 15 in the appendix on one task, moreso as the environment (game) is new. In fact, there is a level of non-determinism in how the attacker policies are encoded which means that an optimal policy cannot be (even up to soft-max) expressed by the agent (as I read things the number of pieces chosen in level l is always chosen uniformly randomly). - Everything leads me to believe that, up to 6.2, the game is only dealt with as a fixed MDP to be overfit by the model through RL: - there is no generalization from K=k (train) to K > k (test). Reinforcement learning >> is better at playing the game, but does worse at predicting optimal moves.
It's also important to take into account how much work general priors about video game playing (games have goals, up jumps, there is basic physics) are doing here (the authors do this when they discuss versions of the game with different physics). Given recent advances in RL and ML that eschew all manner of structured representations, I believe this is a well-timed reminder that being able to transfer know-how from human behaviour to artificially-intelligent ones. The paper is clearly written, and the experiments follow a clean and coherent narrative. Both the premises assumed and the conclusions drawn are quite reasonable given the experimental paradigm and domain in which they are conducted. This issue could be easily fixed. The authors' Main Claim appears to be: "While common wisdom might suggest that prior knowledge about game semantics such as ladders are to be climbed, jumping on spikes is dangerous or the agent must fetch the key before reaching the door are crucial to human performance, we find that instead more general and high-level priors such as the world is composed of objects, object like entities are used as subgoals for exploration, and things that look the same, act the same are more critical." In the case of human players the time needed to complete the game is plotted, and in the case of a RL agent the number of steps needed to complete the game is plotted (fig 1). So it cannot be concluded that the change of performances is due to human priors. - Secondly, with a simple modification of textures the performances of human players collapse, while those of a RL agent stay the same. The authors have to include RL agent in all their experiments to be able to dissociate what is due to human priors and what is due to the noise introduced in the game. However, I believe that many of my criticisms can be assuaged during the rebuttal period. Issue 2: Are the results here really about "high level" priors? Thus, we could consider a weaker version of the claim: semantic priors are important but even in the absence of explicit semantic cues (note, this is different from having the wrong semantic cues as above) people can do a good job on the game.
The algorithm for learning the context of sequencing reads compared to true mutations is based on a multi layered CNN, with 2/3bp long filters to capture di and trinucleotide frequencies, and a fully connected layer to a softmax function at the top. The data is based on mutations in 4 patients with lung cancer for which they have a sample both directly from the tumor and from a healthy region. Using matched samples of tumor and normal from the patients is also a nice idea to mimic cfDNA data. We also liked the thoughtful construction of the network and way the reference, the read, the CIGAR and the base quality were all combined as multi channels to make the network learn the discriminative features of from the context. If the entire point is to classify mutations versus errors it would make sense to combine their read based calls from multiple reads per mutations (if more than a single read for that mutation is available) - but the authors do not discuss/try that. The authors suggest to overcome this problem by training an algorithm that will identify the sequence context that characterize sequencing errors from true mutations. They also should compare to Strelka whic h interestingly they included only to make final calls of mutations but not in the comparison. The idea is that in the sample being sequenced there would also be circulating tumor DNA (ctDNA) so such mutations could be captured in the sequencing reads. The authors should explore this behavior in greater detail. Moreover, the authors filter the "normal" samples using those (p.7 top), which makes the entire exercise a possible circular argument. This makes the task of differentiating between sequencing errors and true variants due to ctDNA hard. Can they show a context they learned and make sense of it? It appears that the proposed method (Kittyhawk) has a steep decrease in PPV and enrichment for low tumor fraction which are presumably the parameter of greatest interest. We believe the method and paper could potentially be improved and make a good fit for a future bioinformatics focused meeting such as ISMB/RECOMB.
The paper presents a combination of evolutionary computation (EC) and variational EM for models with binary latent variables represented via a particle-based approximation. Experiments on artificial bars data and natural image patch datasets compare several variants of the proposed method, while varying a few EA method substeps such as selecting parents by fitness or randomly, including crossover or not, or using generic or specialized mutation rates. After Mutation section: Remind readers that "N_g" is number of generations This paper proposes an evolutionary algorithm for solving the variational E step in expectation-maximization algorithm for probabilistic models with binary latent variables.
Although the author claim the novelty as adding noise to the discriminator, it seems to me that at least for the RBF case it just does the following: 1. write down MMD as an integral probability metric (IPM) This manuscript explores the idea of adding noise to the adversary's play in GAN dynamics over an RKHS. ==== original review === The paper describes a generative model that replaces the GAN loss in the adversarial auto-encoder with MMD loss.
The paper describes an empirical evaluation of some of the most common metrics to evaluate GANs (inception score, mode score, kernel MMD, Wasserstein distance and LOO accuracy). In the paper, the authors discuss several GAN evaluation metrics. Specifically, the authors pointed out some desirable properties that GANS evaluation metrics should satisfy. I think this paper tackles an interesting and important problem, what metrics are preferred for evaluating GANs. In particular, the authors showed that Inception Score, which is one of the most popular metric, is actually not preferred for several reasons. Appendix G in https://arxiv.org/pdf/1706.04987.pdf) Do you think it would be useful to compare other generative models (e.g. VAEs) using these evaluation metrics? - The authors implicitly contradict the argument of Theis et al against monolithic evaluation metrics for generative models, but this is not strongly supported. Cons -It is not clear why GANs are the only generative model considered Although this work and its results are very useful for practitioners, it lacks in two aspects. The use of learned representations needs more rigorous justification The authors point out that different architectures yield similar results for their analysis, however it is not clear how the biases of the learned representations affect the results. Second, it could benefit from a deeper (maybe theoretical analysis) of some of the questions. -Unprecedented visual quality as compared to other generative models has brought the GAN to prominence and yet this is not really a big factor in this paper. The "novelty" of this paper is a bit hard to assess. Overall, I think this paper is worthy for acceptance as several GAN methods are proposed and good evaluation metrics are needed for further improvements of the research field.
The authors propose a method for graph classification by combining graph kernels and CNNs. In a first step patches are extracted via community detection algorithms. Although community detection is used before graph kernels, such subgraph extraction process is already implicitly employed in various graph kernels. Moreover, the method makes an strong assumption that the graph is strongly characterized by one of its patches, ie its subgraph communities, which might not be the case in arbitrary graph structures, thus limiting their method. The article is well-written and easily comprehensible, but suffers from several weak points: * Features are not learned directly from the graphs, but the approach merely weights graph kernel features. This paper proposes a graph classification method by integrating three techniques, community detection, graph kernels, and CNNs.
The proposed architecture could identify the 'key' states through assigning higher weights for important states, and applied reservoir sampling to control write and read on memory. Also, intuitively, this episodic memory method should work better on long-term dependencies task, while this article only shows the task with 10 timesteps. Therefore, empirically, it is really hard to justify whether this proposed method could work better. Typos didn't influence reading. It is a novel setup to consider reservoir sampling for episodic memory.
Overall, I don't think this paper meet *CONF*'s novelty standard, although the authors present some good numbers, but they are not convincing. As a result, the activations outputted by DReLU can have a mean closer to 0 and a variance closer to 1 than the standard ReLU. Using DReLU to improve the state-of-art neural network in an uncontrolled setting is important. The arguments for skipping this experiments are respectful, but not convincing enough. Although DReLU's expectation is smaller than expectation of ReLU, but it doesn't explain why DReLU is better than very leaky ReLU, ELU etc. 1) As DReLU(x) = max{-\\delta, x}, what is the optimal strategy to determine \\delta? Comments: 1. Using expectation to explain why DReLU works well is not sufficient and convincing. 2. CIFAR-10/100 is a saturated dataset and it is not convincing DReLU will perform will on complex task, such as ImageNet, object detection, etc.
One depend only on the number of observations of the given context and the average reward in this situation and the second term begin the noise. The defended idea is to use the context to fit a mixture of Gaussian with a NN and to assume that the noise could be additively split into two terms. * In the paragraph above (2) I unsure of what is a "binomial noise error distribution" for epsilon, but a few lines later epsilon becomes a gaussian why not just mention that you assume the presence of a gaussian noise on the parameters of a Bernouilli distribution ? The only positioning argument that is given in that section is the final sentence "In this paper we model measurement noise using a Gaussian model and combine it with a MDN". This paper presents a methodology to allow us to be able to measure uncertainty of the deep neural network predictions, and then apply explore-exploit algorithms such as UCB to obtain better performance in online content recommendation systems. Also, it would have been useful to compare ot to other neural models dealing with uncertainty (some of them having been applied to bandit problems- e.g., Blundell et al. (2015)). The confidence bound considered should include the uncertainty on the parameters in the predictive posterior reward distribution (as done for instance in Blundell et al. (2015) in the context of neural networks), not only the distribution of the observed data with regards to the considered probabilistic families. My main concern with the paper is that the contribution is unclear, as the authors failed from my point of view in establishing the novely w.r.t. the state of the art regarding uncertainty in neural networks. 2. In Section 4.2.1, CTR is modeled as a Gaussian mixture, which doesn't look quite right, as CTR is between (0,1). This is equivalent to separate a local estimation error from the noise.
It's unlikely that having yet another paper separating depth 2 from depth 3 with some other set of conditions will move us towards further progress in the very important question of depth separation. The paper proves the separation by constructing a very specific function that cannot be approximated by 2-layer networks. So a negative result for depth 2 is weaker; the earlier work (and almost trivially by using the work of Cybenko, Hornik, etc.) already shows that the depth -3 approximations are uniform approximators.
There are a few ideas the paper discusses: (1) compared to pruning weight matrices and making them sparse, block diagonal matrices are more efficient since they utilize level 3 BLAS rather than sparse operations which have significant overhead and are not "worth it" until the matrix is extremely sparse. For example, with sparse and block diagonal matrices, reducing the size of the matrix to fit into the cache on the GPU must obviously make a difference, but this did not seem to be investigated. There are two half-papers here, one on parameter pruning and one on applying insights from random matrix theory to neural networks, but I don't see a strong connection between them.
The approach seems to be very susceptible to the weight of the subtrace loss λ, at least when training NTMs. In my understanding each of the trace supervision information (hints, e.g. the ones listed in Appendix F) provides a sensible inductive bias we would the NTM to incorporate. p5: There is related work for incorporating trace supervision into a neural abstract machine that is otherwise trained end-to-end from input-output examples [2].
Linda Smith and Eliana Colunga have published a series of papers that explore these questions in detail: http://www.iub.edu/~cogdev/labwork/kinds.pdf http://www.iub.edu/~cogdev/labwork/Ontology2003.pdf I was impressed by the range of phenomena they tackled and their analyses were informative in understanding the behavior of deep learning models The analyses are motivated by results from cognitive and developmental psychology, exploring questions such as whether agents develop biases for shape/color, the difficulty of learning negation, the impact of curriculum format, and how representations at different levels of abstraction are acquired. The crucial question, here, would be whether, when an agent is trained in a naturalistic environment (i.e., where distributions of colors, shapes and other properties reflect those encountered by biological agents), it would show a human-like shape bias. 4.1 Word learning biases This experiment shows that, when an agent is trained on shapes only, it will exhibit a shape bias when tested on new shapes and colors. 3 A situated language learning agent When the training set is balanced, the agent shows a mild bias for the simpler color property. The extent to which it provides us with insight into human cognition depends on the degree to which we believe the structure of the agent and the task have a correspondence to the human case, which is ultimately probably quite limited. Concerning the attention analysis, it seems to me that all it's saying is that lower layers of a CNN detect lower-level properties such as colors, higher layers detect more complex properties, such as shapes characterizing objects. Alternatively, if the equivalent non-situated model does show the phenomena, then using the situated version would not be useful because the model acts equivalently in both cases. 4. The section on learning speeds could include more information on the actual patterns that are found with human learners, for example the color words are typically acquired later.
The (Mirowski et al, 2016) paper shows that a neural network-based agent with LSTM-based memory and auxiliary tasks such as depth map prediction can learn to navigate in fixed environments (3D mazes) with a fixed goal position (what they call "static maze"), and in fixed mazes with changing goal environments (what they call "environments with dynamic elements" or "random goal mazes"). However, when RL is applied to navigation problems it is tempting to evaluate the agent on unseen maps in order to assess weather the agent has learned a generic mapping & planning policy. More worryingly, when observing that the method of (Mirowski et al, 2017) may not generalize to unseen environments in claim [c], the authors of this submission seem to confuse navigation, cartography and SLAM, and attribute to that work claims that were never made in the first place, using a straw man argument. I therefore recommend not to accept this paper in its current form.
Summary: This paper empirically studies adversarial perturbations dx and what the effects are of adversarial training (AT) with respect to shared (dx fools for many x) and singular (only for a single x) perturbations. - AT decreases the effectiveness of adversarial perturbations, e.g. AT decreases the number of adversarial perturbations that fool both an input x and x with e.g. a contrast change. The paper shows that adversarial training increases the destruction rate of adversarial examples so that it still has some value though it would be good to see if other adversarial resistance techniques show the same effect. Pro: - Paper addresses an important problem: qualitative / quantitative understanding of the behavior of adversarial perturbations is still lacking.
They provide an analysis that connects aims to establish a connection between how CNNs for solving super resolution and solving sparse regularized inverse problems. So assuming an L1 regularization in this equation assumes that the signal itself is sparse. Given that much of the main result seems to already be known, I feel that this work is not novel enough at this time.
To reduce the memory required for training, the authors also propose a path-wise training procedure based on the independent convolution paths of CrescendoNet. The experimental results on CIFAR-10, CIFAR-100 and SVHN show that CrescendoNet outperforms most of the networks without residual connections. If averaging paths leads to ensembling in CrescendoNet, it leads to ensembling in FractalNet. While the longest path in FractalNet is stronger than the other members of the ensemble, it is nevertheless an ensemble. The CrescendoNet is like a variant of the FractalNet, and the only difference is that the number of convolutional layers in Crescendo blocks grows linearly. While CrescendoNet seems to slightly outperform FractalNet in the experiments conducted, it is itself outperformed by DiracNet. Hence, CrescendoNet does not have the best performance among skip connection free networks.
This paper consider a version of boosting where in each iteration only class weights are updated rather than sample weights and apply that to a series of CNNs for object recognition tasks. - While the motivation is that classes have different complexities to learn and hence you might want each base model to focus on different classes, it is not clear why this methods should be better than normal boosting: if a class is more difficult, it's expected that their samples will have higher weights and hence the next base model will focus more on them. This paper instead designed a new boosting method which puts large weights on the category with large error in this round.
The authors observe that the proposed recurrent identity network (RIN) is relatively robust to hyperparameter choices. - While the LSTM baseline matches the results of Le et al., later work such as Recurrent Batch Normalization or Unitary Evolution RNN have demonstrated much better performance with a vanilla LSTM on those tasks (outperforming both IRNN and RIN).
The paper presents a number of interesting results: 1) Larger networks are typically more learnable than smaller ones (typically we think of larger networks as being MORE complicated than smaller networks -- this result suggests that in an important sense, large networks are simpler). These results are in line with several of the observations made by Zhang et al (2017), which showed that neural networks are able to both (a) fit random data, and (b) generalize well; More importantly, this relationship between test accuracy and learnability doesn't answer the original question Q2 posed: "Do larger neural networks learn simpler patterns compared to neural networks when trained on real data". Also, since we are looking at empirically validating the learnability criterion defined by the authors, all the results (the reported confusion tables) need to be tested statistically (to see whether one dominates the other). Review Summary: The primary claim that there is "a strong correlation between small generalization errors and high learnability" is correct and supported by evidence, but it doesn't provide much insight for the questions posed at the beginning of the paper or for a general better understanding of theoretical deep learning. -As suggested in the final sentence of the discussion, it would be nice if conclusions drawn from the learnability experiments done in this paper were applied to the design new networks which better generalize Summary: This paper presents very nice experiments comparing the complexity of various different neural networks using the notion of "learnability"
- Previous compression work uses a lot of tricks to compress convolutional weights. What is your compression ratio for 0 accuracy loss?
About the content: The main problem for me is that the whole construction using field theory seems to be used to advocate for the appearence of a phase transition in neural nets and in learning. PAGE 2: * "Using a scalar field theory we show that a phase transition must exist towards the end of training based on empirical results."
experiments on the using different hyper-parameters of the energy function, as well as visual inspections on the quality of the learned images, are presented. * results are only partially motivated and analyzed This paper proposed some new energy function in the BEGAN (boundary equilibrium GAN framework), including l_1 score, Gradient magnitude similarity score, and chrominance score, which are motivated and borrowed from the image quality assessment techniques. Based on its incremental nature and weak experiments, I'm on the margin with regards to its acceptance. Using this energy function, the authors hypothesize, that it will force the generator to generate realistic images. In particular, the authors propose to change the energy function associated with the auto-encoder, from an L2 norm (a single number) to an energy function with multiple components.
Regularizers can be designed to affect statistical properties of the representations, such as sparsity, variance, or covariance. 2. Remarks Shannons channel coding theory was used by the authors to derive regularizers, that manipulate certain statistical properties of representations learned by DNNs. In the reviewers opinion, there is no theoretical connection between DNNs and channel theory. But the representations, learned by DNNs, can be affected indirectly by applying regularization.
Networks with very few connections in the upper layers are experimentally determined to perform almost as well as those with full connection masks. And p.1 says that the "CL" layers are those often referred to as "FC" layers, not "conv" (though they may be convolutionally applied with spatial 1x1 kernels). But in the upper layers, the spatial extent can be very small compared to the image size, sometimes even 1x1 depending on the network downsampling structure. Detailed comments and questions: The distribution of connections in "windows" are first described to correspond to a sort of semi-random spatial downsampling, to get different views distributed over the full image. The authors propose reducing the number of parameters learned by a deep network by setting up sparse connection weights in classification layers.
This paper's main thesis is that automatic metrics like BLEU, ROUGE, or METEOR is suitable for task-oriented natural language generation (NLG). The authors present a solid overview of unsupervised metrics for NLG, and perform a correlation analysis between these metrics and human evaluation scores on two task-oriented dialog generation datasets using three LSTM-based models. v) BLEU usually correlates with human better when 4 or more references are provided. However, there isn't enough novel contribution in this paper to warrant a publication.
For example, Lewis et al. note challengings in avoiding divergence from human language during selfplay, which are likely to be less pronounced on a synthetic dataset.
The paper proposes a method for few-shot learning using a new image representation called visual concept embedding. The used Visual Concepts (VCs) were already introduced by other works (Wangt'15), and is not a novelty. The results show promising results, yet lack exploration of the model, at least to draw conclusions like "we address the challenge of understanding the internal visual cues of CNNs". Therefore, I rate the current manuscript as a reject. Using a embedding representation based on visual concepts is straightforward.
either in terms of FLOPS or measured times This paper applies gated convolutional neural networks [1] to speech recognition, using the training criterion ASG [2]. Arguments in section 2.3 are weak because, again, all other grapheme-based end-to-end systems have the same benefit as CTC and ASG. However, all of the other grapheme-based end-to-end systems enjoy the same benefit as CTC and ASG. The connection between ASG, CTC, and marginal log loss has been addressed in [9], and it does make sense to train ASG with the partition function. The authors argue that ASG is better than CTC in section 2.3.1 because it does not use the blank symbol and can be faster during decoding. There is no reason to believe that ASG can be faster than CTC in both training and decoding. That limits the value. If rejected from here, it could perhaps be submitted as an ICASSP or Interspeech paper.
(2) The introduction and related work are well written. But it's not at all clear that the gradient of either the loss or the quantization error w.r.t. the number of bits will in general suggest increasing the number of bit (thus requiring the bit regularization term). Inherently, the objective is discontinuous since # of bits is a discrete parameter.
These results also seem inconsistent with authors statement "…and conclude that data augmentation alone - without any other explicit regularization techniques - can achieve the same performance to higher as regularized models…" The paper could potentially be made more interesting or solid if some of the followings could be investigated: - considering a wider range of different problems apart from image classification, and investigate the effectiveness of domain specific data augmentation and general data augmentation
On a slightly modified set of structured scene representations from the CLEVR dataset, this approach outperforms two LSTM baselines with incomplete information, as well as an implementation of Relation Networks. Of course many people have published on CLEVR although of its language limitations, but I was a bit surprised that only these features are enough to solve the problem completely, and this makes me curious as to how hard is it to reverse-engineer the way that the language was generated with a context-free mechanism that is similar to how the data was produced. At the same time, one of these things that's really nice about the structure-selection part of this model is that it doesn't care what kind of messages the modules send to each other! dataset. If the current paper really wants to make denotational semantics part of the core claim, I think it would help to talk about the representational implications in more detail---what kinds of things can and can't you model once you've committed to set-like bottlenecks between modules?
In overall, it is a nice idea to use DNN to represent all update operators in MCTS. The paper is thorough and well-explained. - It looks like after training MCTSnet with a massive amount of data from another MCTS, MTCSnet algorithm as in Algorithm 2 will not do very much more planning yet. I suspect that generating the training data and learning the model takes an enormous amount of CPU time, while 25 MCTS rollouts can probably be done in a second or two. Would it be fair to have a baseline that learns the MCTS coefficient on the training data? The presentation is very clear, the design of the architecture is beautiful, and I was especially impressed with the related work discussion that went back to identify other game search and RL work that attempts to learn parts of the search algorithm. + is R_1 similar to R^1 If I understand them correctly, the comparison is between a neural network that has been learned on 250,000 trajectories of 60 steps each where each step is decided by a ground truth close-to-optimal algorithm, say MCTS with 1000 rollouts (is this mentioned in the paper). How would the technique scale with more MCTS iterations? This is compared to 25 rollouts of MCTS that make the decision for the baseline. The authors propose to train this using policy gradient in which data of optimal state-action pairs is generated by a standard MCTS with a large number of simulations.
The branch&bound method is fairly standard, two benchmarks were already existing and the third one is synthetic with weights that are not even trained (so not clear how relevant it is). or something similar. In summary, I feel that while there are some issues with the paper, it presents interesting results and can be accepted.
The structured deep-in factorization machines allow higher-level interactions in embedding learning which allows the authors to learn embeddings for heterogeneous set of features. For example, I am aware of -- http://manikvarma.org/downloads/XC/XMLRepository.html contains some interesting datasets which have a large number of discrete features and classes. Summary: This paper proposes an approach to learn embeddings for structured datasets i.e. datasets which have heterogeneous set of features, as opposed to just words or just pixels. This paper provides a clean way of learning embeddings for structured features that can be discrete -- indicating presence / absence of a certain quality. I would like to see an evaluation of these features in a classification setting to further demonstrate the utility of these embeddings as compared to directly embedding the discrete features and then performing a K-way classification. Unlike, word2vec there is no hard constraint that similar objects must have similar representations and so, the learnt embeddings reflect the likelihood of the observed features. Further, these features can be structured i.e. a set of them are of the same 'type'. SUMMARY. The paper presents an extension of word2vec for structured features.
This paper proposes a fast way to learn convolutional features that later can be used with any classifier. The acceleration of the training comes from a reduced number of training epocs and a specific schedule decay of the learning rate. Pros: The paper compares different classifiers on three datasets. Two proposals in the paper are: (1) Using a learning rate decay scheme that is fixed relative to the number of epochs used in training, and And for (2), it is a relatively standard approach in utilizing CNN features.
This writeup describes an application of recurrent autoencoder to analysis of multidimensional time series. The only conclusions I can draw from the visual analysis is that the context vectors are more similar to each other when they are obtained from time steps in the data stream which are close to each other. * Very little information about the data. The benefit of only reconstructing a subset of the input dimensions seems very data specific to me and I find it hard to consider this a novel idea by itself. This paper proposes a strategy that is inspired by the recurrent auto-encoder model, such that clustering of multidimensional time series data can be performed based on the context vectors generated by the encoding process.
Given that conv nets for molecular graphs are not trivially interpretable, this would provides a useful approach to use conv nets for more explicit interpretations of how the task can be performed by neural nets. As the paper states in Introduction, the target problem is 'hard selection' of substructures, rather than 'soft selection' that neural nets (with attention, for example) or neural-net fingerprints usually provide. Cons: - it would be a bit unconvincing that identifying 'hard selection' is better suited for neural nets, rather than many existing exact methods (without using neural networks). If the conv net performs the best when we use the entire structure, then learning might be forced to ignore the selection. It would be unconvincing that the proposed neural nets approach fits to this hard combinatorial task rather than these existing (mostly exact) methods. - In the experiments, the baseline is based on LR, but this would not be fair because usually we cannot expect any linear relationship for molecular fingerprints.
The authors propose a DNN, called subspace network, for nonlinear multi-task censored regression problem. 8. The performance on One-Layer Subspace Network (with only the input features) could be added. A major merit of DNN is that it can automatically extract useful features. Conclusion: Though with a quite novel idea on solving multi-task censored regression problem, the experiments conducted on synthetic data and real data are not convincing enough to ensure the contribution of the Subspace Network. - The computation time for the linear model shown in Table 3 is quite surprising (~20 minutes for linear regression of 5k observations). Furthermore, while being the main methodological drive of this work, the paper does not show evidence about improved predictive performance and generalisation when accounting for the boundedness of the regression targets. 3. In synthetic data experiments (Table1), only small margins could be observed between SN, f-MLP and rf-MLP, and only Layer 1 of SN performs better above all others. This paper presents a new multi-task network architecture within which low-rank parameter spaces were found using matrix factorization. The authors are generating the synthetic data according to the model, and it is thus not surprising that they managed to obtain the best performance.
The paper entitled 'Siamese Survival Analysis' reports an application of a deep learning to three cases of competing risk survival analysis. That being said, this particular use of deep learning in this context might be novel. Related works: - For your consideration: is multi-task survival analysis effectively a competing risks model, except that these models also estimate risk after the first competing event (i.e. in a competing risks model the rates for other events simply go to 0 or near-zero)? However, afterwards the objective does combine time-dependent discrimination indices of several competing risks, with different denominator values. - One of the main motivations of the authors is to propose a model that is specially design to avoid the nonidentifiability issue in an scenario with competing risks.
All in all this paper reads like a tech report but not a conference publication. When the authors say "specialization" or "specialized model", they sometimes mean distillation, sometimes filter pruning, and sometimes cascades. Would it be the gain in speed much lower?
The fact that these different inverse maps arise under these conditions is interesting --- and Figure 5 is quite convincing in showing how each expert generalizes. Overall, it is nice to see the different inverse maps arise naturally in this setting. 2) The authors only run experiments on the MNIST data, where 1) the mechanisms are simulated and relatively simple, and 2) samples from the canonical distribution are also available. How D will handle an example far from fake or real ones ?
For example, in Section 3.1 the argument is made that the linear region created when all activations are equal to one, will have a local minimum, and this minimum might be suboptimal. Here, the results only hold within each local region of the space, and they don't say anything about how the global minima in different regions compare in terms of loss. The main result is that every local minimum of the total surface is a global minimum of the region where the ReLU activations corresponding to each sample do not change.
The distribution output from a given node is defined in terms of a learned conditional probability function, and the output distributions of its input nodes. The paper considers distribution to distribution regression with MLPs. It's not clear how much value there is adding yet another distribution to distribution regression approach, this time with neural networks, without some pretty strong motivation (which seems to be lacking), as well as experiments. However, this experiment uses 3BE outside of its intended use case --- which is for a single input distribution --- so it's not entirely clear how well the very simple proposed model is doing.
The paper tries to build an interpretable and accurate classifier via stacking a supervised VAE (SVAE) and a differentiable decision tree (DTT). As the decision tree, then, is merely learning hard, class-based partitioning rules for the latent space, I do not see how the tree is representing anything especially revealing. Could you, somehow, force the tree to encode an identifiable attribute at each node, which would then force that attribute to be encoded in a certain dimension of latent space?
The authors propose a particular variance regularizer on activations and connect it to the conditional entropy of the activation given the class label. But then, if the goal is to _minimize_ the mutual information between I(C;Y), it makes sense to _maximize_ the conditional entropy H(C|Y). It would have been nice if the authors more directly compared SHADE to BN.
The appropriateness of using additional pages over the recommended length will be judged by reviewers."  In the present submission, the first 8+ pages contain minimal new material, just various background topics and modified VAE update rules to account for learning noise parameters via basic EM algorithm techniques. The original Gaussian VAE proposes to use the inference network for the noise that takes latent variables as inputs and outputs the variances, but most of the existing works on Gaussian VAE just use fixed noise probably because the inference network is hard to train. This paper studies the importance of the noise modelling in Gaussian VAE.
d) I'm not able to really follow Definition 1, perhaps due to unclear notation.
This work proposed an interesting graph generator using a variational autoencoder. Pros: - the formulation of the problem as the modeling of a probabilistic graph is of interest However, there are some significant weaknesses. Please also don't make statements like "To the best of our knowledge, we are the first to address graph generation using deep learning." This is very clearly not true. Overall, the paper is about an interesting subject, but in my opinion the execution isn't strong enough to warrant publication at this point. The main issue with training models in this formulation is the alignment of the generated graph to the ground truth graph. To handle this, the paper proposes to use a simple graph  matching algorithm (Max Pooling Matching) to align nodes and edges. I see this as equivalent to choosing an order in which to linearize the order of nodes and edges in the graph. I would have at least liked to see a comparison to a method that generated SMILES format in an autoregressive manner (similar to previous work on chemical graph generation), and would ideally have liked to see an attempt at solving the alignment problem within an autoregressive formulation (e.g., by greedily constructing the alignment as the graph was generated). - some of the main issues with graph generation are acknowledged (e.g. the problem of invariance to node permutation) and a solution is proposed (the binary assignment  matrix) The work should be interesting to researchers in the various areas. - notions for measuring the quality of the output graphs are of interest: here the authors propose some ways to use domain knowledge to check simple properties of molecular graphs First, the motivation for one-shot graph construction is not very strong: - I don't understand why the non-differentiability argued in (a) above is an issue. The search space of small graph generation is usually very small, is there any other traditional methods can work on this problem? d) the graph matching procedure proposed is a rough patch for a much deeper problem Based on this motivation, the paper decides to generate a graph in "one shot", directly  outputting node and edge existence probabilities, and node attribute vectors.
Summary of the paper: This paper presents a method, called \\alpha-DM (the authors used this name because they are using \\alpha-Divergence to measure the distance between two distributions), that addresses three important problems simultaneously: (a) Objective score discrepancy: i.e., in ML we minimize a cost function but we measure performance using something else, e.g., minimizing cross entropy and then measuring performance using BLEU score in Machine Translation (MT). My comments / feedback: The paper is well written and the problem addressed by the paper is an important one. It is argued that the ML approach has some "discrepancy" between the optimization objective and the learning objective, and the RL approach suffers from bad sample complexity. (b) Sampling distribution discrepancy: The model is trained using samples from true distribution but evaluated using samples from the learned distribution Below are the points that I'm particularly confused about: 1. For the ML formulation, the paper made several particularly confusing remarks. 1. The idea is a good one and is great incremental research building on the top of previous ideas. I do not agree with statements like "We demonstrate that the proposed objective function generalizes ML and RL objective functions …" that authors have made in the abstract. The objective is based on alpha-divergence between the true input-output distribution q and the model distribution p. 1.2 I understand that the ML objective is different from what the users really care about (e.g., blue score), but this does not seem a "discrepancy" to me. 2. For the RL approach, I think it is very unclear as a formulation of an estimator. For example, 1.1 The q(.|.) distribution in Eq. The model is trained on an empirical distribution whose points are sampled from the true distribution.
Maybe removing some of those would make things easier to parse. There really isn't much discussion about the architecture of networks, but rather the dimensionality of the feature maps.
5.3: so that results more comparable In this paper the authors studied the problem of off-policy learning, in the bandit setting when a batch log of data generated by the baseline policy is given. [Minor] The derived bounds depend on M, an a priori upper bound on the Renyi divergence between the logging policy and any new policy. To exactly calculate \\lambda either requires the size of the policy class (when the policy class is finite), or the complexity constants (which exists in C_1 and C_2 in equation 7, but it is not clearly defined in this paper).
The improved upper bound given in Theorem 1 appeared in SampTA 2017 - Mathematics of deep learning ``Notes on the number of linear regions of deep neural networks'' by Montufar. I can appreciate though that this a fine line to walk. (The improvement on Zaslavsky's theorem is interesting.) The idea of counting the number of regions exactly by solving a linear program is interesting, but is not going to scale well, and as a result the experiments are on extremely small networks (width 8), which only achieve 90% accuracy on MNIST. Overall, while the paper is well written and makes some interesting points, it presently isn't a significant enough contribution to warrant acceptance. Previous work [1], [2], has derived lower and upper bounds for the number of linear regions that a particular neural network architecture can have.
A composite of transformations coupled with the LAM/RAM networks provides a highly expressive model for modelling arbitrary joint densities but retaining interpretable conditional structure. Comparing with the Masked Autoregressive Flows (Papamakarios et al., 2017) paper, it seems that the true difference is using the linear autoregressive transformation (LAM) and recurrent autoregressive transformation (RAM), already present in the Inverse Autoregressive Flow (Kingma et al., 2016) paper they cite, instead of the masked feedforward architecture used Papamakarios et al. (2017). Given that, the most important part of the paper would be to demonstrate how it performs compared to Masked Autoregressive Flows.
Based on experiments done by the authors, on MNIST, having this procedure gives the same performance with 3-4 times less memory or increase in performance of 1% for the same memory as regular network. Perhaps all aspects (training cost [computation + time], accuracy and storage) should be plotted together to see what methods form the frontier.
Without a solid and consistent basis for these hyper-parameter perturbations, I worry that this approach will fail to normalize the effect of experiment numbers while also giving researchers an excuse to avoid reporting their experimental process. Even the idea of researchers knowing their test set variance makes me very uneasy.
This paper proposes a soft relaxation of the box lattice (BL) model of Vilnis et al. 2018 and applies it to several graph prediction tasks. Specifically, the paper builds on a a geometrically inspired embedding method using box representations. Results are comparable to the BL model on existing artificially-balanced data but significantly better on more natural unbalanced data with a large number of negatives. - The main thrust of section 5.2 is that smoothed box embeddings retain better performance with increasing numbers of negatives. Also, I think some graphical illustration of the embedding would be very helpful, perhaps something like Figure 2 of "Probabilistic Embedding of Knowledge Graphs with Box Lattice Measures". ------ The paper presents smoothing probabilistic box embeddings with softplus functions, which make the optimization landscape continuous, while also presenting the theoretical background of the proposed method well. As the authors find, the smoothed function leads to improved performance against SOTA on relevant benchmark data such as WordNet hypernymy, Flick caption entailment and MovieLnes market basket data. This is a great paper and should be accepted.
To explain the difficulty of training pruned networks from scratch or why training needs the overparameterized networks that make pruning necessary,  the authors propose a lottery ticket hypothesis: unpruned, randomly initialized NNs contain subnetworks that can be trained from scratch with similar generalization accuracy. The paper follows by proposing a method to find these winning tickets by pruning methods, which are typically used for compressing networks, and then proceed to test this hypothesis on several architectures and tasks. (Score raised from 8 to 9 after rebuttal) The observation of "winning ticket weights tend to change by a larger amount then weights in the rest of the network" in Figure 19 seems natural and the conjecture of the reason "magnitude-pruning biases the winning tickets we find toward those containing weights that change in the direction of higher magnitude" sounds reasonable. This paper proposes a conjecture to explain this phenomenon that the authors call "The Lottery Ticket Hypothesis":  large networks that can be trained successfully contain at initialization time small sub-networks — which are defined by both connectivity and the initial weights that the authors call "winning tickets" — that if trained separately for similar number of iterations could reach the same performance as the large network. Most importantly, the hypothesis and experiments presented in this paper gave me a new perspective on both the generalization and optimization problem, which as a theoretician gave me new ideas on how to approach analyzing them rigorously — and that is why I strongly vote for the acceptance of this paper.
Authors introduce  a criterion to be used for identifying important parts of the network (connection sensitivity), that does not depend on the magnitude of the weights for neurons: they start by introducing a set of binary weights (one per a weight from a neuron) that indicate whether the connection is on or off and can be removed. Table 2 does look impressive and it seems that it also reduces the overfiting, and the experiment with random labels on mnist seem to demonstrate that the method indeed preserves only connections relevant to the real labels, simplifying the architecture to a point when it cant just memorize the data Please relate to it. Fundamentally, if you decouple weight pruning from initialization it also means that: - the first layer will be pruned out of connections to constant pixels (which is seen in the visualizations), this remains meaningful even after a reinitialization Does it mean that first a set of random weights is sampled, then the sensitivities are computed, then a salient set of connections is established and the weights are REINITIALIZED from a distribution possibly different than the one used to compute the sensitivity? - It is correct that the weights used to train the pruned model are possibly different from the ones used to compute the connection sensitivity. I'd like to see how using the regular df/dw criterion would fare in single-shot pruning. [4] Generalized Dropout. [5] Variational Dropout Sparsifies Deep Neural Networks. [2] A Simple Procedure for Pruning Back-Propagation Trained Neural Networks. - df/dc = df/d(cw) d(cw)/dc = df/d(cw) * w [3] Learning Sparse Neural Networks through L_0 Regularization. [1] Skeletonization: A Technique for Trimming the Fat from a Network via Relevance Assessment. Furthermore, the authors should also refer to [5] as they originally did the same experiment and showed that they can obtain the same behaviour without any hyper parameters.
In the context of vanilla Gaussian VAEs (Gaussian prior, encoders, and decoders) the authors show that if (a) the intrinsic data dimensionality r is equal to the data space dimensionality d and (b) the latent space dimensionality k is not smaller than r then there is a sequence of encoder-decoder pairs achieving the global minimum of the VAE objective and simultaneously (a) zeroing the variational gap and (b) precisely matching the true data distribution. Based on the insight from the analysis in the supplementary materials, the authors propose a two-stage VAE which separate learning the a parsimonious representation of the low-dimensional (lower than the ambient dimension of the input space), and the training a second VAE to learn the unknown approximate posterior. It has clarified a few key issues, and convinced me of the value to the community for publication in its present (slightly edited according to the reviwers' feedback) form.
Pros: - New insights to support cross-modality matching from covariates. * The image of the voice waveform in Figures 1 and 2 should be replaced by log Mel-spectrograms in order to illustrate the network's input. The key innovation of the article, compared to the aforementioned papers, lies on the idea of learning face/voice embeddings to maximise their ability to predict covariates, rather than by explicitly trying to optimise an objective related to cross-modal matching. Authors aim to reveal relevant dependencies between voice and image data (under a cross-modal matching framework) through common covariates (gender, ID, nationality). # Summary The article proposes a deep learning-based approach aimed at matching face images to voice recordings belonging to the same person.
but immediately below, "We show empirically that weight decay only improves generalization by controlling the norm, and therefore the effective learning rate." It would have been interesting to carefully study the effect of weight decay on the gamma parameter of batch-norm which controls the complexity of the network along with the softmax layer weights as it was left for future work in van Laarhoven 2017.
The proposed approach uses a pre-training step, based on a variational auto-encoder (VAE), to estimate latent variable sequences. This approach can be in particular useful for the tasks that have 2) Have you tried pre-training c_t's as continuous latent variables? 4) Have you tried training your model on the pixels on the continuous control tasks? In particular, it follows the ideas presented in InfoGAIL, that depends on a latent variable, To achieve that the paper introduces an unsupervised variational objective by maximizing the directed mutual information between the latents c's and the trajectories.
However, using this algorithm in the framework of graph convents is new, and certainly interesting. The main advantage of the proposed method as illustrated in particular by the experimental results in the citation network domain is its ability to generalize well in the presence of a small  amount of training data, which the authors attribute to its efficient capturing of both short- and long-range interactions. The authors discuss related work in a thorough and meaningful manner. Overall, the idea of using Lanczos algorithm to bypass the computation of the eigendecomposition, and thus simplify filtering operations in graph signal processing is not new [e.g., 35]. The core idea is to use the Lanczos algorithm to obtain a low-rank approximation of the graph Laplacian.
This paper proposes an approach for automatic robot design based on Neural graph evolution. [Strengths]: This paper shows some promise when graph network-based controllers augmented with evolutionary algorithms. Paper is quite easy to follow. The results in this paper are impressive, and the paper seems free of technical errors. The authors propose a scheme based on a graph representation of the robot structure, and a graph-neural-network as controllers. At first, it compares to random graph search and ES. The overall approach has a flavor of genetical algorithms, as it also performs evolutionary operations on the graph, but it also allows for a better mechanism for policy sharing across the different topologies, which is nice. - Sec 4.1:  would argue that computational cost is rarely a concern among evolutionary algorithms. Detailed comments: - in the abstract you say that "NGE is the first algorithm that can automatically discover complex robotic graph structures". Model-based RL algorithms can work in real-time (e.g. http://proceedings.mlr.press/v78/drews17a/drews17a.pdf) and have been shown to have same asymptotic performance of MB controllers for simple robot control (e.g. https://arxiv.org/abs/1805.12114) Please include further description of the ES cost function and algorithm in the main body of the paper. If you can also compare against one or two algorithms of your choice from the recent literature it would also give more value to the comparison. If so, why? Also, it is not clear what is the meaning of generations if the graph is fixed, can't it be learned altogether at once? This paper uses graph network to train each morphology using RL. In robot design for a given task using rewards, training each robot design using RL with rewards is an expensive process and not scalable. As far as I understand, the model starts with hand-engineered design and then finetuned using evolutionary process. How would the given graph network compare to this?
The authors use their derived formula for VRR to predict the minimum mantissa precision needed for accumulators for three well known networks: AlexNet, ResNet 32 and ResNet 18. Thereafter, the mantissa precision of the accumulator is predicted to maintain the error of accumulation within bounds by keeping the VRR as close to 1 as possible. The authors present their metric called Variance Retention Ratio (VRR) as a function of the mantissa length of product terms, partial sum (accumulator) terms, and the length of the accumulation. The authors address this with  an analytical method to predict the number of mantissa bits needed for partial summations during the forward, delta and gradient computation ops for convolutional and fully connected layers. The framework predicts the smallest number of bits necessary in the (multiply-add) calculations (forward propagation, backward propagation, and gradient calculation) in order to keep the precision at an acceptable level.
The authors claim the generator of RoC-Gan will span the target manifold, even in the presence of large amounts of noise. This work focuses on the robustness of conditional GAN(RoC-GAN) when facing the noise. The authors provide necessary theoretical analysis and empirical validation for their model. The proposed method is simple. The authors study the theoretical properties of RoC-GAN and prove that it shares the same properties as the vanilla GAN. ------------------------------- After Rebuttal --------------------------------- I am very satisfied with the authors' response, so I will change my vote from rejection to acceptance. As a conclusion, I vote for weak rejection.
Next this paper motivates the study of complexity quantities that tend to decrease with the number of parameters, in particular figure 3 motivates the conjecture that the complexity measure in Theorem 2 can control generalization error. Finally {\\bf Theorem 3} is presented, which provides a lower bound for the Rademacher complexity of a class of neural networks, and such bound is compared with existing lower bounds. An empirical comparison with existing generalization bounds is made and the presented bound is the only one that in practice decreases when the number of hidden units grows. These Rademacher bounds yield standard bounds on the ramp loss for fixed alpha,beta, and margin, and then a union bound argument extends the bound to data-dependent alpha,beta and margin. Next, {\\bf Theorem 2} which is the main result, presents a new upper bound for the generalization error of 1-layer networks. Then, {\\bf Theorem 1} provides an upper bound for the empirical Rademacher complexity of the class of 1-layer networks with hidden units of bounded \\textit{capacity} and \\textit{impact}.
interestingly this seems to also explain the heuristic commonly believed that to make asynchronous training work one needs to slowly anneal the number of workers (coherence is much worse in the earlier than later phases of training). I want to see the theoretical analysis of the relation between model complexity and staleness.
Based on looking at past *CONF* proceedings, this paper's topic and collection of techniques is not in the *CONF* mainstream (though it's not totally unrelated). I can accept that the model will be rather insensitive to hyper-parameters alpha and beta, but I've serious doubt about the number of clusters, especially as the evaluation is done here in the best possible setting.
One would at least expect an after-the-fact interpretation for the weighted tensor term and what this implies with regard to their method and syntactic embedding compositions in general. - the tensor model does deliver some improvement over linear composition on noun-adjective pairs when measured against human judgement Unlike Arora's original work, the assumptions they make on their subject material are not supported enough, as in their lack of explanation of why linear addition of two word embeddings should be a bad idea for composition of the embedding vectors of two syntactically related words, and why the corrective term produced by their method makes this a good idea. Building on the generative word embedding model provided by Arora et al. (2015), their solution uses the core tensor from the Tucker decomposition of a 3-way PMI tensor to generate an additive term, used in the composition of two word embedding vectors. Given that the main attraction of the paper is the potential for more performant word embeddings, I do not believe the work will have wide appeal to *CONF* attendees, because no evidence is provided that the features from the learned tensor, say [a, b, T*a*b], are more useful in downstream applications than [a,b] (one experiment in sentiment analysis is tried in the supplementary material with no compelling difference shown). Some additional citations: - the above-mentioned *CONF* paper provides a performant alternative to unweighted linear composition
Visual stimuli are presented and the responses of the neurons recorded. I raised my score accordingly.
The paper is a reasonable dataset/analysis paper. The evaluation of mathematical reasoning ability is an interesting perspective. There are a large variety of modules, and trying to not generate either trivial or impossible problems is a plus in my opinion. My main concerns are about the evaluation and comparison of standard neural models. It would have been useful to compare the general models here with some specific math problem-focused ones as well. Weaknesses: The dataset created here is entirely synthetic, and the paper only includes one single small real-world case; it seems like it would be easy to generate a larger and more varied real world dataset as well (possibly from the large literature of extant solved problems in workbooks). The insights from the analysis of the failure cases are intriguing, but it also points out that the neural networks models are not really performing mathematical reasoning since the generalization is very limited. I am biased because I once did exactly the same thing as this paper, although at a much smaller scale; I am thus happy to see such a public dataset. One other thing I want to see is a test set with multiple different difficulty levels. The paper is very clearly written. The dataset is then used to evaluate a number of recurrent models (LSTM, LSTM+attention, transformer); these are very powerful models for general sequence-sequence tasks, but they are not explicitly tailored to math problems. The authors try to do this with composition, which is good, but I am not sure whether that captures the real important thing - the ability to generalize, say learning to factorise single-variable polynomials and test it on factorising polynomials with multiple variables? Why not build datasets based on workbooks, problem solving books, etc?
========= The paper extends Bayesian GANs by altering the generator and discriminator parameter likelihood distributions and their respective priors. "true" data distribution ... impossible for a Bayesian formulation). While this is not a prior in the Bayesian sense (i.e. in the sense of an actual prior belief), it would be interesting to have a closer look at the effect this has on the sampling method.
For instance, the experiments seem to indicate that generalizing density estimation from CIFAR training set to CIFAR test set is likely challenging and thus the models underfit the true data distribution, resulting in the simpler dataset (SVHN) having higher likelihood. Specifically, density models trained on CIFAR10 have higher likelihood on SVHN than CIFAR10. - Lack of an extensive exploration of datasets Pros: - The finding that SVHN has larger likelihood than CIFAR according to networks is interesting. The SVHN/CIFAR10 phenomenon has also been shown in concurrent work [1]. Given that you observed that SVHN has higher likelihood on all three model types (PixelCNN, VAE, Glow), why investigate a component specific to just flow-based models (the volume term)?
The paper deals with the problem of recovering an exact solution for both the dictionary and the activation coefficients. I think the paper is relevant and proposes an interesting contribution. The contribution improves Arora 2015 in that it converges linearly and recovers both the dictionary and the coefficients with no bias. The paper is well written and the key elements are in the body. It would be nice to have some more intuitive explanations at least of Theorem 1.
The "no bad local valleys" implies that for any point on the loss surface there exists a continuous path starting from it, on which the loss doesn't increase and gets arbitrarily smaller and close to zero. It proves that with the proposed structure of DNN, there are uncountably many solutions with zero training error, and the landscape has no bad local valley or local extrema. The paper analyzes the loss landscape of a class of deep neural networks with skip connections added to the output layer. Also, it seems the proof of 3 is somewhat redundant, since local minimum is a special case of your "bad local valley". Maybe coming up with a toy dataset and network WITH bad local valleys will be sufficient, because after adding N skip connections the network will be free of bad local valleys. The good property of loss surface for networks with skip connections is impressive and the authors present interesting experimental results pointing out that
Original review: Summary: they propose a differentiable learning algorithm that can output a brush stroke that can approximate a pixel image input, such as MNIST or Omniglot. Good luck! [1] SPIRAL https://arxiv.org/abs/1804.01118 [2] sketch-rnn https://arxiv.org/abs/1704.03477 [3] sketch-pix2seq https://arxiv.org/abs/1709.04121 [4] http://kanjivg.tagaini.net/ [5] https://quickdraw.withgoogle.com/data [6] https://vectormagic.com/ Revision: The addition of new datasets and the qualitative demonstration of latent space interpolations and algebra are quite convincing. I think the value in this method is that it can be converted to a full generative model with latent variables (like a VAE, GAN, sketch-rnn) where you can feed in a random vector (gaussian or uniform), and get a sketch as an output, and do things like interpolate between two sketches. I think this work is a great companion to existing work such as Sketch-RNN and SPIRAL. While research from big labs [1] have the advantage of having access to massive compute so that they can run large scale RL experiments to train an agent to "sketch" something that looks like MNIST or Omniglot, the authors probably had limited resources, and had to be more creative to come up with a solution to do the same thing that trains in a couple of hours using a single P40 GPU. Minor points: a) The figures look like they are bitmap, pixel images, but for a paper advocating stroke/vector images, I recommend exporting the diagrams in SVG format and convert them to PDF so they like crisp in the paper. I don't think having a model that can output only a single stroke is scalable to other (simple) datasets such pixel versions of KangiVG [4] or QuickDraw [5]. Below are a few of my suggestions that I hope will help the authors improve their work, for either this conference, or if it gets rejected, I encourage the authors to try the next conference with these improvements: 1) multiple strokes, longe strokes. That being said, things are not all rosy, and I feel there are things that need to be done for this work to be ready for publication in a good venue like *CONF*.
This paper describes a framework - Tree Reconstruction Error (TRE) - for assessing compositionality of representations by comparing the learned outputs against those of the closest compositional approximation. This in an interesting study and attacks a very fundamental question; tracking compositionality in representations could pave the way towards representations that facilitate transfer learning and better generalization. The paper demonstrates the use of this framework to assess the role of compositionality in a hypothetical compression phase of representation learning, compares the correspondence of TRE with human judgments of compositionality of bigrams, provides an explanation of the relationship of the metric to topographic similarity, and uses the framework to draw conclusions about the role of compositionality in model generalization. I think this is a reasonable paper to accept for publication. Now, onto the measure. I like the idea of learning basis vectors from the representations and constraining to follow the primitive semantics. My understanding is that you derive basis word representations by using SGD and the phrase vectors and compute TRE with these. Reviewer 2's comments also remind me that, from a perspective of learning composition-ready primitives, Fyshe et al. (2015) is a relevant reference here, as it similarly learns primitive (word) representations to be compatible with a chosen composition function.
Experimental results on several benchmark datasets (MNIST, ImageNet) and commonly used deep nets (CNN, ResNet, Inception) are reported to show the power of adversarial deformations. The paper introduces an iterative method to generate deformed images for adversarial attack. Pros: - The way of constructing deformation adversarial is interesting and novel - experiments show what such a technique can achieve on MNIST and ImageNet. Interestingly, one can see on MNIST the parts of the numbers that the adversarial attack is trying to delete/create. The idea of gradually adding deformations based on gradient information is somewhat interesting, and novel as far as the reviewer knows about. The authors briefly discussed this point in the experiment section and provided a few numerical results in Table 2. - for instance, a study of the impact of the regularization would have been interesting (how does the sigma of the Gaussian smoothing affect the type of adversarial attacks obtained and their performance -- is it possible to fool the network with [very] smooth deformations?); However, the intuition behind the proposal does not make strong sense to the reviewer: since the main focus of this work is on model attack, why not directly (iteratively or not) adding random image deformations to fool the system? These results, as acknowledged by the authors, do not well support the effectiveness of deformation adversarial attack and defense. - Numerical study shows some promise in adversarial attack, but is not supportive to the related defense capability. - Note: about the remark in section 3.2: deformation-induced transformations are a subset of all possible transformations of the image (which are all representable with intensity changes), so it is expected that a training against attacks on the intensity performs better than a training against attacks on spatial deformations. More clearly: the space of small deformations tau comes with an inner product (here L2, but one could choose another one), and the gradient \\nabla g obtained depends on this inner product choice M, even though the derivative Dg is the same (they are related by Dg(tau) = < \\nabla_M g | tau >_M for any tau). Also, the width of the smoothing applied to the deformation field has an impact.
Overall, I think the idea is nice and the results are encouraging. But I cannot tell the proposed method is really useful, so I gave this score. Clarity: The paper is clearly written in the sense that the motivation of research is clear, the derivation of the proposed method is easy to understand. Significance: I think this kind of research makes the variational inference more useful, so this work is significant. The problem author focused on is unique and the solution is simple, experiments show that proposed method seems promising. I think author want to point that when K=1, STL is unbiased with respect to the 1 ELBO, but when k>1, it is biased with respect to IWAE estimator. The whole paper is written in a clean way and the method is effective. I checked all the derivations, and they seem to be correct. Author experimentally found that the estimator of the existing work(STL) is biased and proposed to reduce the bias by using the technique like  REINFORCE.
The tracker receives, from its own perspective, partially observed visual information o_t^{alpha} about the target (e.g., an image that may show the target) and the target receives both observations from its own perspective o_t^{beta} and a copy of the information from the tracker's perspective. This work aims to address the visual active tracking problem in which the tracker is automatically adjusted to follow the target. This paper presents a simple multi-agent Deep RL task where a moving tracker tries to follow a moving target. A training mechanism in which tracker and the target serve as mutual opponents is derived to learning the active tracker. The reward function is not completely zero-sum, as the tracked agent's reward vanishes when it gets too far from a reference point in the maze. The paper proposes a novel reward function - "partial zero sum", which only encourages the tracker-target competition when they are close and penalizes whey they are too far. The work is very incremental over Luo et al (2018) "End-to-end Active Object Tracking and Its Real-world Deployment via Reinforcement Learning", as the only two additions are extra observations o_t^{alpha} for the target, and a reward function that has a fudge factor when the target gets too far away. Clarity: the paper is well-written. The paper would have benefitted from a proper analysis of the trajectories taken by the adversarial target as opposed to the heuristic ones, and from comparison with non-RL state-of-the-art on tracking tasks.
They propose three enhancements to MILP formulations of neural network verification: Asymmetric bounds, restricted domain and progressive bound tightening, which lead to significantly more scalable verification algorithms vis-a-vis prior work. - Results: the efficiency of the MIP on the tightened model, and the improvements in the bounds on the adversarial error as compared to very recent methods from the literature are both very strong points in favor of the paper.
The key insight is that instead of training to directly predict x, the paper proposes to predict different piecewise constant projections of x from x_init , with one CNN trained for each projection, each projection space defined from a random delaunay triangulation, with the hope that learning prediction for each projection is more sample efficient. One hypothesis is that the different learned CNNs that each predict a piecewise projection are implicitly yielding an ensembling effect, and therefore a more fair baseline to compare would be a 'direct-ensemble' where many different (number = number of projections) direct CNNs (with different seeds etc.) The problem of reconstruction from these piecewise constant projections is of independent interest. For example I would recommend using Deep Image prior as an alternative technique of reconstructing a high-res image from multiple piecewise constant images, but this can be future work. 3. estimate x from the m different projections by solving "reformuated" inverse problem using TV regularization.
The method of the authors assumes that a goal-conditioned policy is already learned, and they use a Kullback-Leibler-based distance between policies conditioned by these two states as the loss that the representation learning algorithm should minimize. More precisely, it wishes to learn a representation so that two states are similar if the policies leading to them are similar. A positive side of the experimental study is that the 6 simulated environments are well-chosen, as they illustrate various aspects of what it means to learn an adequate representation. But learning the goal-conditioned policy from the raw input representation in the first place might be the most difficult task. For example, why is this particular metric used to link the feature representation to policy similarity? - Main missing details is about how the goal reaching policy is trained. The first weakness of the approach is that it assumes that a learned goal-conditioned policy is already available, and that the representation extracted from it can only be useful for learning "downstream tasks" in a second step. In that respect, wouldn't it be possible to *simultaneously* learn a goal-conditioned policy and the representation it is based on? 1. Generate data D from the environment (using an arbitrary policy). The authors should describe the oracle in more details and discuss why it does not provide a "perfect" representation. But when looking at the framework, this is close to what the authors do in practice: they use a distance between two *goal*-conditioned policies, not *state*-conditioned policies. The current discussion makes me think that the evaluation methodology may be biased. The authors admit that having one is "a significant assumption" and state that they will discuss why it is reasonable assumption but I didn't find any such discussion  (only a sentence in 6.4). - As the goal-conditional policy is quite similar to the original task of navigation, it is important to know for how long it was trained and taken into account. How is the goal-conditional policy trained? A study of the effect of various optimization effort on these goal-conditioned policies might also be of interest.
The interpolation model composes two components -- given these conditions, it first regresses weights combining a set of precomputed deformation fields, and then a second model regresses dense volumetric deformation corrections -- these are helpful as some events are not easily modeled with a set of basis deformations. The first network utilizes a set of precomputed deformations, while the weights can be set to generate different output shapes. 1. The primary novelty here is in the problem formulation (e.g., defining cost function etc.) where two networks are used, one for learning appropriate deformation parameters and the other to generate the actual liquid shapes. The specific way deformations are composed -- using v_inv to backwards correct basis deformations, following up the mixing of those with a correction model -- is intuitive and is also something I see for the first time. Right now the implicit surface deformation model is only tested on liquids examples, which limits the impact to that specialist domain -- it's a bit more of a SIGGRAPH type of paper than *CONF*. The experimental results are sufficient for simulating liquids/smoke, except I would like to also see a comparison to using deformation field network only, without its predecessor. 3. In terms of practical applications, to the best of my knowledge there are sophisticated physics-based and graphics based approaches that perform very fast fluid simulations. Experimental results are sufficient. However, it is also necessarily to add more intuitions to the current approach. This was done for Fig 6, but would be nice to also see it numerically in ablation in Fig. 4. While this is a good applied paper with a large variety of experimental results, there is a significant lack of novelty from a machine learning perspective. The synthesized simulations are not physically accurate, but with certain visual realism. So, this is closer to a graphics approach and deep learning has been used before extensively in a similar manner for shape generation, shape transformation etc. 2. But based on my understanding, this does not really explicitly incorporate the physical laws within the learning model and can't guarantee that the generated data would obey the physical laws and invariances. I found the paper hard to read at first, since the paper is heavy on terminology, only really understood what is going on when I went through the examples in the appendix, which are helpful and then on a second read the content was clear and appears technically correct. So, the authors need to provide accuracy and computation cost/time comparisons with such methods to establish the benefits of using a deep learning based surrogate model. Another useful experiment would be to vary the number of bases and/or the resolution of the deformation correction network and see the effects. If the AC decides to reject based on this fact I am ok with that as well.
The paper introduces and develops the notion of input-dependent baselines for Policy Gradient Methods in RL. Learning an input-dependent baseline function helps clear out the variance created by such perturbations in a way that does not bias the policy gradient estimate (the authors provide a theoretical proof of that fact). I'm increase the score accordingly. Is just the baseline input dependent or does the policy need to be input dependent as well?
The authors then address the problem of data drift in BMI and describe a number of domain adaptation algorithms from simple (CCA to more complex ADAN) to help ameliorate it. Here the authors define a BMI that uses an autoencoder -> LSTM -> EMG. I.e. your BMI is neural data -> AE -> LSTM -> EMG? Please clarify. Page 8, How do the AE results and architecture fit into the EMG reconstruction "BMI" results?
On three different corpora (IWSLT, WMT, TED) with into-English translation from numbers of source languages ranging from 6 (WMT) to 44 (TED), this technique outperforms standard distillation for every language pair, and outperforms the individual models for most language pairs. +++++++++++++++++++ I have updated my rating after reading author's responses The authors apply knowledge distillation for many-to-one multilingual neural machine translation, first training separate models for each language pair.
This paper proposes a variational approach to Bayesian posterior inference in phylogenetic trees. This paper explores an approximate inference solution to the challenging problem of Bayesian inference of phylogenetic trees. These metrics are important, but it would be nice to see some kind of qualitative summary of the inferences made by different methods—two methods can produce similar log-likelihoods or KL divergences but suggest different scientific conclusions.
Experiments on synthetic games show that SOS preserves the benefit of LOLA while avoiding its theoretically-predicted issues, and a more complex Gaussian mixture GAN experiment shows SOS is empirically competitive with other gradient adjustment methods. Casting the recently proposed LOLA gradient adjustment into a general matrix form, they diagnose an example where the shaping term in LOLA prevents convergence to SFP. To alleviate this issue, the authors propose a new algorithm SOS, which can be seen as an interpolation between LOLA and LookAhead, characterized by a parameter p. However, since this example can be viewed as a fully cooperative game with joint loss L = 2xy, it does not support the broader statement that Nash is undesirable in all games. While Definition 1 precisely defines differentiable games to have *twice* differentiable losses, why do the authors assume *thrice* differentiable losses at the start of Section 4?
They extend results of Bartunov et al 2018 (which found that feedback alignment fails on particular architectures on ImageNet), demonstrating that sign-symmetry performs much better, and that preserving error signal in the final layer (but using FA or SS for the rest) also improves performance. Building off a study by Bartunov et al. that shows the deficiencies of some BP algorithms when scaled to difficult datasets, the authors evaluate a different algorithm, sign-symmetry, and conclude that there are indeed situations in which BP algorithms can scale. In the submitted manuscript, the authors compare the performance of sign-symmetry and feedback alignment on ImageNet and MS COCO datasets using different network architectures, with the aim of testing biologically-plausible learning algorithms alternative to the more artificial backpropagation. On the other hand, I think the conclusions regarding the first question -- whether sign-symmetry can be useful in artificial settings -- are fine given the experiments. The obtained results are promising and quite different to those in (Bartunov , 2018) and lead to the conclusion that biologically plausible learning algorithms in general and sign- symmetry in particular are effective alternatives for ANN training. To this end, though the authors claim that the conditions on which Bartunov et al tested are "somewhat restrictive", this logic can equally be flipped on its head: the conditions under which this paper tests sign-symmetry are not restrictive enough to productively move in the direction of assessing sign-symmetry's usefulness as a description of learning in the brain, and so the conclusion that the algorithm remains a viable option for describing learning in the brain is not sufficiently supported. Instead, I would suggest keeping black (or gray) for backpropagation (the baseline), and then using two hues of one color (e.g. light blue and dark blue) for the two sign-symmetry models. Second, the work does not sufficiently weigh the "degree" of implausibility of sign-symmetry compared to the other algorithms, and implicitly speaks of feedback alignment, target propagation, and sign-symmetry as equally realistic members of a class of BP algorithms. A couple of remarks: I would be interested in understanding the robustness of the sign-symmetry algorithm w.r.t.
This paper proposes learning a latent variable deep generative model over every randomly sampled subset of observed features. While not the first model to try to handle modeling data with missing features, it is still a fairly original and elegant formulation. The qualitative results presented in this work are interesting. The method is compared against 1) classical approaches in missing data imputation on UCI benchmarks; 2) image inpainting against recently proposed GANS for the similar task, as well as; 3) against universal marginalizer, which learns conditional densities using a feedforward / autoregressive architecture. Inference in this latent variable model is achieved through the use of an inference network which conditions on the set of "missing" (to the generative model) features. The mask determines which features are observed. "Missing Value Imputation Based on Deep Generative Models." arXiv preprint arXiv:1808.01684 (2018). My concern about the experimental results on missing data imputation is that strong competition such as Gondra et al'17 and Yoon et al'18 that report better results on UCI than classical approaches are not included. Table 5 claims negative log-likelihood numbers on MNIST as low as 61 and 41 (I assume nats...). There are issues like awkward grammar, sloppy notation, and spelling mistakes (please run spell check!) throughout the manuscript. Why not run these experiments on datasets like MNIST and Omniglot?
The approach assumes a distribution on the hyperparameters, governed by a parameter, which is adapted during the course of the training to achieve a compromise between the flexibility of the best-response function and the quality of its local approximation around the current hyperparameters. Can cross-validation be adapted to this approach?
Summary: This paper introduces equi-normalization (ENorm): a normalization technique that relies on the scaling invariance properties of the ReLU, similarly to Path-SGD. The authors show that the proposed method preserve functionally equivalent property in respect of the output of the functions (Linear, Conv, and Max-Pool) and show also that ENorm converges to the global optimum through the optimization. The authors propose a new weight re-parameterization technique called Equi-normalization (ENorm) inspired by the Sinkhorn-Knopp algorithm. The experimental results show that ENorm performs better than baseline methods on CIFAR-10 and ImageNet datasets. The authors propose a new regularization method for neural networks. Since it doesn't rely on mini-batch to normalize the network, Equi-normalization could be a good alternative to BN in small mini-batch regime. (+) The computational overhead reduced by the proposed method compared with BN and GN looks good. Indeed, the authors note in their discussion that the criterion they optimize might be not optimal. I can see that Table 1 gives an overview of the number of elements accessed during normalization, but I do think that a proper plot showing the accuracy versus the wall-clock time would be a better way of showing how your method compares in practice with BN or GN.
This paper looks at solving optimization problems that arise in GANs, via a variational inequality perspective (VIP). Two techniques that have been widely used to solve VIP problems are averaging and extragradient methods. Given that, GAN formulations tend to be min-max style problems (though not necessarily 0 sum) the VIP perspective is very natural, though under-explored in machine learning. After showing theoretical guarantees of these methods (linear convergence) the authors propose to combine them with existing techniques, and show in fact this leads to better results. In this case two kinds of gradient updates can be derived. VIP entails solving an optimization problem that is related to the first order condition of the optimization problem that we wish to solve. For strongly-monotone operators (a generalization of strongly-convex functions) extrapolation updates are shown to have linear convergence. On the whole this is a really nice paper, that shows how standard ideas from VIP can be useful for training GANs. I recommend acceptance Overall, the paper is well-written and of high quality, therefore I recommend acceptance.
This paper is about issues that arise when applying Information Bottleneck (IB) concepts to machine learning, more precisely in deterministic supervised learning such as classification (deterministic in the sense that the target function to estimate is deterministic: it associates each example to one true label only, and not to a distribution over labels). This work analyses the information bottleneck (IB) method applied to the supervised learning of a deterministic rule Y=f(X). Such a deterministic relationship between outputs and inputs induces the problem that the the IB "information curve" (i.e. I(T;Y) as a function of I(X;T)) is piece-wise linear and, thus, no longer strictly concave, which is crucial for non-degenerate ("interesting") solutions. 2) They show that in the case of a deterministic rule, the information bottleneck curve has a simple shape, piecewise linear, and is not strictly concave. 3) They show that the optimization of the IB Lagrangian for different \\beta does not lead to a point by point exploration of the IB curve. - This work provides in depth clarification of the counter-intuitive behaviors of the IB method in the case to the learning of a deterministic rule. Another consequence of this degeneracy concerns the latent variable interpretation of the IB: if T is treated as a latent variable (as, for instance, in the "deep" IB models) then we have the conditional independence relation "Y independent of X given T", which simply makes no sense if Y is deterministic in X (there is, of course, a deeper underlying problem here: the IB problem is difficult in that it is difficult to define a geneative model with a faithful DAG...). 7) They use the IB method to train a neural net on MNIST, using the Kolchinsky estimate of the mutual informations. 5) They exhibit uninteresting representations (noisy versions of the output Y) that are on the IB curve. In practice, besides a few recent propositions (Kolchinsky et al., 2017; Alemi et al., 2016; Chalk et al., 2016) the IB Lagrangian is not a usual objective function for supervised learning. Questions: - Do the authors know of an application where the full probing of the IB curve would be necessary? estimates or lower bounds as here). Thus, one has to be careful when defining the mutual information I(X,Y), which explains the problems with the IB information curve (which should asymptotically converge to I(X;Y) as I(X;T) gets large. Maybe rephrase some expressions that might be wrongly perceived?
The paper proposes training generative models that produce multi-agent trajectories using heuristic functions that label variables that would otherwise be latent in training data. The approach extends VRNN to a hierarchical setup with high level coordination via a shared learned latent variable. The generative models are hierarchical, and these latent variables correspond to higher level goals in agent behavior.
This construction motivates the formulation of the auto encoder through the definition of several cost terms promoting reconstruction, clustering, and consistency across latent mappings. The authors also suggest augmenting their setup with a model of cluster transition dynamics for time-series data, which seems to improve the clustering further, as well as providing an interpretable 2D visualisation of the system's dynamics. The authors conduct experiments on both static and time series data and validate that the method perform better than related methods in terms of clustering results as well as interpretability. This work addresses the problem of learning latent embeddings of high-dimensional time series data. - Clustering of short-term time series, such as the clinical ones, is a challenging task. It also beats the k-means clustering approach. I was curious why not use the clustering accuracy as well?
Compared to the claimed baselines (Liimatainen et al. and human experts), the proposed architecture shows a much higher performance. Why is the AUC important in the biological application at hand? earlier we are told that the labels come from "a large battery of biotechnologies and approaches, such as microarrays, confocal microscopy, knowledge from literature, bioinformatics predictions and additional experimental evidence, such as western blots, or small interfering RNA knockdowns."
Novelty: (1) The error bound of value iteration with the Boltzmann softmax operator and convergence & convergence rate results in this setting seem novel. Stronger conditions are required on the sequence \\beta_t, along the lines discussed in the paragraph on Boltzmann exploration in Section 2.2 of Singh et al 2000. Theorem 4 does not imply efficient exploration, since it requires very strong conditions on the alphas, and note that the same proof applies to vanilla Q-learning, which we know does not explore well. Soundness: (1) The proof of Theorem 4 implicitly assumes that all states are visited infinitely often, which is not necessarily true with the given algorithm (if the policy used to select actions is the Boltzmann policy). I presume the title of this paper is a homage to the recent 'Boltzmann Exploration Done Right' paper, however, though the paper is cited, it is not discussed at all. Summary: This work demonstrates that, although the Boltzmann softmax operator is not a non-expansion, a proposed dynamic Boltzmann operator (DBS) can be used in conjunction with value iteration and Q-learning to achieve convergence to V* and Q*, respectively.
[Summary] This paper proposes a Graph-Sequence-to-Sequence (GraphSeq2Seq) model to fuse the dependency graph among words into the traditional Seq2Seq framework. It is a logical and practical implementation that seems to provide solid benefits over the existing state of the art. The paper is generally written fairly clearly, though I think the clarity of section 3.3 could be improved; it took me several reads to understand the architectural difference between this second variant and the original one. [clarity] This paper is basically well written though there are several grammatical errors (I guess the authors can fix them). Cons: - Somewhat incremental, not clear how much method depends on quality of the dependency parser This paper proposes a method for combining the Graph2Seq and Seq2Seq models into a unified model that captures the benefits of both.
The authors claim that zero confidence attacks pose a harder problem and hence mainly compare their experimental results to the CW attack. || from the beginning. This paper proposes an efficient zero-confidence attack algorithm, MARGINATTACK, which uses the modified Rosen's algorithm to optimize the same objective as CW attack. While one would indeed expect an overhead due to the binary search, it is not clear a priori how large this overhead needs to be to achieve a competitive zero confidence attack with PGD (especially with a tuned step size for PGD, see above). However, from an optimization point of view, these two notions are clearly related and a fixed perturbation attack can be converted to a small perturbation / zero confidence attack via a binary search over the perturbation size. I am still a bit confused about the difference between "zero-confidence attacks" and those that don't fall into that category such as PGD. - On top of Page 2, the authors claim that zero-confidence attacks are a more realistic attack setting. My main concern about this paper is why this algorithm has a better performance than CW attack? i have change my rating from 5 to 6 after reading the numerous and thorough rebuttals from the authors. Minor comment: The theoretical proof depends on the convexity assumption, I would also suggest comparing the proposed attack with CW and other benchmarks on some simple models that satisfy the assumptions. Finally, the experimental results do not show any significant advantage over PGD, either in running time (they are slower) or norm perturbation.
Authors present a novel regularizer to impose graph structure upon hidden layers of a neural Network. [2] Koutnik et al, Evolving neural networks in compressed weight space. Pros: Interesting idea for bringing some benefits of graphical models into Neural Networks using a regularizer. Experiments verify that one can successfully improve the intrepretability of hidden representations. The key points are 1) How to define the Laplacian graph for the neurons? Cons: The major flaw is the lack of comparison with ``any'' of the related work on interpretability or the prior work on imposing structure upon hidden representations. Authors propose a class of graph spectral regularizers and their performance is different in different tasks. When the layers contains thousands of neurons or more, how to add the regularizers efficiently?
This paper argues that a random orthogonal output vector encoding is more robust to adversarial attacks than the ubiquitous softmax. Basically, authors notice that gradients of a deep neural network when one hot encoding is used can be highly correlated and hence can be used in the design on an attack. These show a lower correlation in input gradients between models when using the proposed RO encoding. I did not find the discussion around Figure 1 to be very compelling, since it is only relevant to the encoding layer, while we are only interested in gradients at the input layer.
They present the distributions and gradients, discuss appropriate activation functions for the output layer, and evaluate this approach on synthetic and real datasets with mixed results. The authors briefly argue that the proposed methods are superior because they provide uncertainty estimates for the output distributions. Most of section 2 is dedicated to writing down, simplifying, and deriving gradient equations for these three distributions. As the authors note, parameterizing an exponential family distribution with the outputs of a neural network is not a novel contribution (e.g. Rudolph et al. (2016) and David Belanger's PhD thesis (2017)) and though I have never personally seen the Dirichlet, Dirichlet-multinomial, and Beta distributions used, the conceptual leap required is small. Their transformation from real-valued network output to, say, strictly positive concentration parameters in a Dirichlet are worth studying; but they don't analyze this in any detail. If the main benefit of the proposed networks is proper uncertainty quantification, then the evaluations (even if they are qualitative) should reflect that. In summary, I do not think the models proposed in section 2 are sufficiently novel to justify publication alone which means that the authors need to either: (1) evaluate novel methods that are critical for use of these models or
The authors propose to estimate and minimize the empirical Wasserstein distance between batches of samples of real and fake data, then calculate a (sub) gradient of it with respect to the generator's parameters and use it to train generative models. The bias of the empirical Wasserstein estimate requires an exponential number of samples as the number of dimensions increases to reach a certain amount of error [2-6]. Computing the gradients of Wasserstein on batches might be seen a kind of regularization, but it remains to be proved and discussed. The quantitative results-- empirical Wasserstein distance show the superiority of the proposed methods. The contribution is about combining this existing method to supervise a standard neural network parametrized generator, so I am not quite sure if this contribution is sufficient for the *CONF* submission.
This paper discusses applications of variants of RNNs and Gated CNN to acoustic modeling in embedded speech recognition systems, and the main focus of the paper is computational (memory) efficiency when we deploy the system. One of the biggest issues of this paper is that they use CTC as an acoustic model, while still many real speech recognition applications and major open source (Kaldi) use hybrid HMM/DNN(TDNN, LSTM, CNN, etc.) systems. Other comments: - in Abstract and the first part of Introduction: as I mentioned above, CTC based character-prediction modeling is not a major acoustic model. The main issue of this paper is the lack of novelty: the three evaluated approaches (Diag LSTM, QRNN and Gated ConvNet) are not novel, the only novelty is the addition of a 1D convolution, which is not enough for a conference like *CONF*. This analysis is actually valuable, and this suggested change about the position of this TIMIT experiment can avoid some confusion of the main target of this paper.) This paper investigates a number of techniques and neural network architectures for embedded acoustic modeling. Hence I argue for rejection, and suggest that the authors consider submitting the paper to a speech conference like ICASSP.
This paper propose an extension of the Neural Theorem Provers (NTP) system that addresses the main issues of this method. The contributions of this paper allow to use this model on real-word datasets by reducing the time and space complexity of the NTP model. Additionally, the paper incorporates mentions as additional facts where the predicate is the text that the entities of the mention are contained in. [Overall] It's great that NTP was scaled up to handle larger datasets, however further analysis is needed. This is the most elaborated section out of the three, yet seems like the most trivial -- unless the authors can provide an analytical bound on the loss in ntp score w.r.t the neighborhood size. - Utilizing text is an interesting direction for NTP in terms of integrating it with past work on KG completion. [Pros] - Reasonable and interesting increments on top of NTP. NTP systems by combining the advantages of neural models and symbolic reasoning are a promising research direction. The attention mechanism (essentially reducing the model capacity) is also well-known but its effect in this particular framework is not properly elaborated. Cons: I'm not convinced by the model used to integrate textual mentions. However, I feel that the paper in its current form is not yet ready for publication in *CONF*, for the following reasons: 1) The authors propose three improvements.
This paper shows that a sentence selection / evidence scoring model for QA trained on SQuAD helps for QA datasets where such explicit per-evidence annotation is not available. 1) First of all, it is hard to say there is a contribution to the idea of sentence discriminator and sentence reader — people have used this framework for large-scale QA a lot.
The paper is interesting and easy to read. I did not find many flaws to point out, except I believe the paper could benefit from more extensive  comparisons in Figure 4A against other IIG methods such as Deep Stack, as well as comparing on much larger IIG settings with many more states to see how the neural CFR methods hold up in the regime where they are most needed. By using these networks within a CFR framework, the authors manage to avoid huge memory requirements traditionally needed to save cumulative regret and average strategy values for all information sets across many iterations. In Eq. 8, they minimize the loss of CF value predictions *over the distribution of infosets in the last CFR step ("batch")*. Here, maybe a way to alleviate this problem would be to generate negative samples (where the network would be trained to predict low cumulative values) by following a different (possibly more exploratory) policy.
Contribution: - Using a known parameters crystallography simulator (X-ray beam, structure being analyzed, environment (crystalline or not)) built a dataset (called DiffraNet) of 25,000 512x512 grayscale labeled images of resulting diffraction images of various materials/structures (crystalline or not) . This paper introduces a purely synthetic dataset for crystallography diffraction patterns. - The images are classified according to the diffraction patterns they encompass into one of 5 classes: blank, no-crystal, weak, good and strong. Since the contribution is mainly on the dataset level and not on the methodological level, I suggest submitting such an article in venues more focused on the application domain.
One DL rule of thumb heard relatively often is to simply initialise LSTM forget gate biases to 1, to "remember more by default".
This submission proposes a method for learning to follow instructions by splitting the policy into two stages: human instructions to robot-interpretable goals and goals to actions. This paper contains an important core insight---much of what's hard about instruction following is generic planning behavior that doesn't depend on the semantics of instructions, and pre-learning this behavior makes it possible to use natural language supervision more effectively. As I understand (please correct me if I am wrong), the method can not work for contextual instructions where the goal depends on the environment and the same instruction can map to different goals, such as 'Go to the largest/farthest object'. It would be better to evaluate on one of the few common benchmarks for robot language understanding, e.g., the SAIL corpus, which considers trajectory-oriented instructions. - Experimental results are not convincing: - The introduction motivates the need for understanding human instructions and the abstract says 'Given a human instruction', but I believe experiments do not have any human instructions.
This paper presents a thorough and systematic study of the effect of pre-training over various NLP tasks on the GLUE multi-task learning evaluation suite, including an examination of the effect of language model-based pre-training using ELMo. The work presented in this paper relates to the impact of the dataset on the performance of contextual embedding (namely ELMO in this paper) on many downstream tasks, including GLUE tasks, but also alternative NLP tasks. The paper is clearly written. Contextualized word representations have gained a lot of interest in recent years and the NLP and ML community could benefit from such detailed comparison of such methods. Only a handful of NLP tasks have an ample amount of labeled data to get state-of-the-art results without using any form of transfer learning. Minor details: Page 1: "can yield very strong performance on NLP tasks" is a very busy way to express the fact that Sentence Encoders work well in practice. Training sentence representation in an unsupervised manner is hence crucial for real-world NLP applications. Extensive hyper-parameter tuning can make a substantial different when dealing with NN models, maybe the authors should have considered dropping some of the tasks (the article has more than enough IMHO) and focus on a smaller sub set of tasks with proper hyper-parameter tuning. The result that some tasks' performance are negatively correlated with each other is surprising. The paper seems to suggest that it consists of pre-training a model on the same task on which it is later evaluated. One of the issue is that the authors if seems to believe that ELMO is the best contextual language model.
The submission proposes to increase the variety of generated samples from GANs by a) using an ensemble of discriminators, and b) tasking them with distinguishing not only fake from real samples, but also their fake samples from the fake samples given to the respective other discriminators. The authors propose a method to improve sample diversity of GANs. They introduce multiple discriminators, each aims to not only compare real and fake examples but also compare different "micro-batch" of examples. Overall the submission is quite interesting, but not without the above-mentioned flaws. As a sanity check - given a fixed generator - if you continue to train the discriminators on randomly drawn samples from this generator distribution does the microbatch discrimination objective continue to make progress and converge to a minimum? - The paper does not have any direct/controlled comparisons with other methods that utilize multiple discriminators or batch based discrimination.
SupportNet uses resnet network with 32 layers, trains an SVM on the last layer and the support vector points from this SVM are given to the network along with the new data. This paper presents a hybrid concept of deep neural network and support vector machine (SVM) for preventing catastrophic forgetting. Pros: (1) The authors propose a sensible approach, which is also novel to be best of our knowledge, using SVM to select support data from old data to be fed to the network along with the new data in the incremental learning framework to avoid catastrophic forgetting. (3) The individual impact of the support points and the joint impact of support points with feature regularizer on accuracy is not assessed. - The authors used 2000 support vectors for MNIST, Cifar-10, and Cifar-100. As the authors referred, the features of support vector data continuously change as the learning goes on.
The agent is given an additional shaping reward that penalizes it for violating these constraints. The shaping reward used for the four Atari games is -1000. Why was this? If these are really constraints on the action sequence, isn't this showing that the algorithm does not work for the problem you are trying to solve? As of this revision, however, I'm not sure I would recommend it for publication.
This work proposes an ensemble method for convolutional neural networks wherein each convolutional layer is replicated m times and the resulting activations are averaged layerwise. I would also significantly reduce the claims of novelty, such as "We introduce the usage of such methods, specifically ensemble average inside Convolutional Neural Networks (CNNs) architectures." in the abstract, given that this is the exact idea explored in other work including followups to Maxout. Given the close similarity of this work to Maxout and others, a much stronger indication of the benefits and improvements of IEA seems necessary to prove out the concepts here. Is the performance boost greater than simply using an ensemble of m networks directly (resulting in the equivalent number of parameters overall)? Likewise for Tables 3-6. 3. Under this interpretation of the tables---again, correct me if I'm wrong---the proper comparison would be "IEA (ours)" versus "Ensemble of models using CNL", or  "(m=3, k=1)" versus "(m=1, k=3)" in my notation. There seems to be a similarity between ResNet and this method - specifically assuming the residual pathway is convolution with an identity activation, the summation that combines the two pathways bears a similarity to IEA. There are more recent followups that continued on the line of work first shown in Maxout, and there should be some greater comparison and literature review on these papers. Given the above issues of clarity and that this simple method seems to not make a favorable comparison to the comparable ensemble baseline (significance), I can't recommend acceptance at this time.
This paper deals with Architecture Compression, where the authors seem to learn a mapping from a discrete architecture space which includes various 1D convnets. By jointly training all these networks, the authors are now able to compress a given network by mapping it's discrete architecture into the latent space, then performing gradient descent towards higher accuracy and lower parameter count (according to the learned regressors). The aim is to learn a continuous latent space, and an encoder and decoder to map both directions between the two architecture spaces. Two further regressors are trained to map from the continuous latent space to accuracy, and parameter count. The continuity of the architecture characteristics can help architecture search tasks. Specifically, compared to those methods, the search space for the proposed paper is larger because although the number of layers is fixed, the connections between layers give more freedom to the compression algorithm. Overall I really like the idea in this paper, the latent space is well justified, but I cannot recommend acceptance of the current manuscript.
Summary: Proposes a framework for performing adversarial attacks on an NMT system in which perturbations to a source sentence aim to preserve its meaning, on the theory that an existing reference translation will remain valid if this is done. They then investigate three different ways of generating adversarial examples and show that a metric based on character n-gram overlap (chrF) has a stronger correlation with human judgment. It is well structured, the problem studied is highly interesting and the proposed meaning-preserving criteria and human judgement will be useful to anyone interested in adversarial attacks for natural language. This is particularly obvious in the case of using character ngram distance (chrF) to determine which character swaps preserve meaning best. The use of character swapping as an adversarial perturbation/noise and the subsequent benefits of training with adversarial noise have already been shown in Belinkov and Bisk, 2018.
This model is used for classification on ImageNet-1k, and for zero-shot classification on ImageNet-21k where a model must predict superclasses seen during training for images of leaf categories not seen during training. Instead of directly optimizing the standard cross-entropy loss, the paper considers some soft probability scores that consider some class graph taxonomy. Rather than following the existing experimental protocol for evaluating zero-shot learning from [Frome et al, 2013] and [Norouzi et al, 2013] the authors evaluate zero-shot learning by plotting SG-hit vs SG-specificity; while these are reasonable metrics, they make it difficult to compare with prior work. Nonetheless, different architectures of neural networks are tested on ImageNet and validate the fact that the soft probability strategy improves performance on the zero-shot learning task. However due to the omission of key references and incomplete comparison to prior work, the paper is not suitable for publication in its current form.
Summary: The paper proposes an approach for improving standard techniques for model compression, i.e. compressing a big model (teacher) in a smaller and more computationally efficient one (student), using data generated by a conditional GAN (cGAN). The experimental results look good, GAN generated data help train a better performed student in knowledge distillation. The paper claims that the best compression score is 1 (training student model on real data), while the paper shows that in fact, good synthetic data should produce *better* accuracy than using real data. I like the authors' explanation on why GAN is particularly good in a student-teacher setting. Strenghts: - The idea to use a GAN for model compression is something that many must have considered.
This papers uses the label hierarchy to drive the search process over a set of labels using reinforcement learning. Compared to other structured classification approaches whose scope is limited by the complexity of the inference process, this approaches is very attractive. For the scale of datasets discussed, where SVM based methods seem to be working well, it is possible that approaches [1,2] which can exploit label correlations can do even better. The paper proposes a label assignment policy for determining the appropropriate positioning of a document in a hierarchy. However, I think the authors should rather present this work as structured classification, as labels dependencies not modeled by the hierarchy are exploited, and as other graph structure could be exploited to drive the RL search. I also note the 32 batch size could be way too small for sparse label sets (I tend to use a batch size of 512 on this type of data).
To improve the robustness of neural networks under various conditions, this paper proposes a new regularizer defined on the graph of the training examples, which penalizes the large similarities between representations belonging to different classes, thus increase the stability of the transformations defined by each layer of the network. The main contribution of the article is to apply concepts of graph regularization to the robustness of neural networks. This paper proposes the interesting addition of a graph-based regularisers, in NNs architectures, for improving their robustness to different perturbations or noise. While the overall concept of graph regularization is appealing, the exact relationship between the proposed regularization and robustness to adversarial examples is unclear. Compared to the previous version, this paper made a good improvement in its experimental results, by adding two different robustness settings in section 4.1 and section 4.3, and also include DeepFool as a strong attack method for testing adversarial robustness. 2. On the other hand, while the proposed regularizer can be interpreted in a perspective of the Laplacian of the similarity graph, the third part in Equation (4), that expresses the smoothness as the sum of similarities between different classes, seems more intuitive to me.
The paper looks into improving the neural response generation task by deemphasizing the common responses using modification of the loss function and presentation the common/universal responses during the training phase. Somehow through this wrong factorization and some probabilistic jugglery, we arrive at section 3 where the takeaway from section 2 is the rather known one that the model promotes universal replies regardless of query. A lot of typos/wrong phrasing/wrong claims and here are some of them: (a) Page 1, "lead to the misrecognition of those common replies as grammatically corrected patterns"? Overall, given the problems this work is not technically sound to be accepted.
I suggest rewriting especially the abstract and the introduction and then submitting to a different venue as the approach itself seems promising. The primary technical challenge is the non-convex optimization required to search for a solution to a query. The experiments are well thought out and show the promise of the method when encoding performance measures such as entropy into the constraints. Weaknesses ----------- The statement in Theorem 1 regarding the converse case is unclear, because it says that the limit of \\delta as \\epsilon approaches zero is zero, but it is not explained what \\epsilon is or how it changes. If other cases exist, it is unclear how Theorem 1 applies. It would furthermore be interesting to inspect the corner cases of the proposed method such as what happens if two constraints are nearly opposing each other and so on.
I don't think the performance improvements are totally conclusive, but one of the most appealing properties of their proposal is that it shouldn't be much more computationally expensive than using a fixed minibatch size. Furthermore, their approach is potentially more robust, since you can presumably be less careful about choosing the set of candidate minibatch sizes, than you would be for choosing only one. The idea of viewing the choice of hyperparameters in a learning algorithm as a bandit problem is known and has been explored in different contexts, although the specific application to minibatch size is new as far as I know. This paper considers a resizable mini-batch gradient descent (RMGD) algorithm based on a multi-armed bandit for achieving best performance in grid search by selecting an appropriate batch size at each epoch with a probability defined as a function of its previous success/failure. I tend towards accepting the paper. My sense is that it is >= 0.
This paper extends the "infinitely differentiable Monte Carlo gradient estimator" (or DiCE) with a better control variate baseline for reducing the variance of the second order gradient estimates. Overview: This nicely written paper contributes a useful variance reduction baseline to make the recent formalism of the DiCE estimator more practical in application. Such issues reduce the value of the contribution in its current form and may contribute to ongoing misunderstandings of the control variate framework and action-dependent baselines in RL, to the detriment of variance reduction techniques in machine learning. (11). In particular, it would be nice to show the variance of the two terms separately (for both DiCE and this paper), to show that the reduction in variance is isolated to the second term (I get that this must be the case, given the math, but would be nice to see some verification of this). The variance of the policy gradient estimator, subject to a baseline "phi," is decomposed using the Law of Total Variance in Eq (3) of the Mirage paper.
Figure 3 shows the results on four different maps for which expert demonstrations are generated from a Finite-Machine Tree-search method (a competitive method in this environment). The strength of the paper is the ability to learn from even 10 sub-optimal demonstrator trajectory thereby achieving optimality in reaching the goal. But this line of work could be a potentially promising direction. Given the periodic drops in performance for the backplay policies, it appears that the initial states might be changing according the curriculum during evaluation. However since that work was not published, it should not be held against this paper. Thanks for your submission. The  authors present a very elegant strategy of using Backplay, that learns a curriculum around a suboptimal demonstration. I believe this paper requires substantial improvements for publication and is not up to the *CONF* standards in its current form.
Note: This is an emergency review. The paper concludes that deep RL is effective at learning agents for cooperative games in multiple ways: 1) Deep agents are better than a hand-designed agent. 3) A popular result in cooperative game theory predicts how effective agents should be. Thus I think some of the concerns above should be addressed before publication, but I would not be very disappointed if it were published as is. 2) Deep agents easily extend across negotiation protocols (something hand-designed agents don't do). In a weighted voting game each agent is given a weight and the agents attempt to form teams.
However, the limit theorem in terms of the depth D is also important for the DNN. * how the wide regime (large N) is interesting for studying deep NNs?
This paper tries to argue that pooling is unnecessary for deformation invariance, as title suggests, and proposes initialization based on smooth filters as an alternative. This paper tries to argue that pooling is unnecessary for deformation invariance, as title suggests, and proposes initialization based on smooth filters as an alternative. Pros: The paper considers a wide variety of pooling strategies and deformation techniques for evaluation. (i) the benefits of pooling in terms of deformation stability can be achieved through supervised learning the filters instead (sec It is often argued that one of the roles of pooling is to increase the stability of neural networks to deformations. Through several experimental setups, the authors conclude that, indeed, pooling is neither necessary nor sufficient to achieve deformation stability, and that its effect is essentially recovered during training.
This work proposes a hybrid VAE-based model (combined with an adversarial or maximum mean discrepancy (MMD) based loss) to perform timbre transfer on recordings of musical instruments. I appreciated that the one-to-one transfer experiments are incremental comparisons, which provides valuable information about how much each idea contributes to the final performance. High-level comments ------------------- Overall, this paper is well written, and the various design choices seem well-motivated. Some detailed comments are listed as follow, The transfer metrics (MMD and kNN) are opaque to the reader: for instance, in table 1, is a knn score of 43173 qualitatively different than 43180? * Also in the introduction, it is implied that style transfer constitutes an advance in generative models, but style transfer does not make use of / does not equate to any generative model. 3 The proposed method can transfer the positive knowledge. The authors proposed a Modulated Variational auto-Encoders (MoVE) to perform musical timbre transfer. The proposed model seems to improve on the transfer accuracy, with a slight hit to reconstruction accuracy. Overall, I feel that this paper falls short of what it promises, so I cannot recommend acceptance at this time.
[3] Exemplar Guided Unsupervised Image-to-Image Translation 1) The paper says that "For example, in a person's facial image translation, if the exemplar image has two attributes, Since the attribute information has been modeled by the network parameters, will different exemplar image lead to different translation outputs? Is the model easy to extend to novel styles for image translation? Some of these works also applied the mask technique or adaptive instance normalization to the image-to-image translation problem. [2] Diverse Image-to-Image Translation via Disentangled Representations * It is mentioned in the introduction (page 2) that "This approach has something in common with those recent approaches that have attempted to leverage an attention mask in image translation".
To force the generator to produce samples in the low density areas of the data distribution, a Complementary GAN is used. - mistakes in the text, bad wording, e.g. "Experiments demonstrate that our model outperforms the state-of-the-art one-class classification models and other anomaly detection methods on both normal data and anomalies accuracy, as well as the F1 score" (page 1, abstract) 3) the general idea (train GAN to generate samples in low-density regions of the original probability density; since a disrciminator is trained to classify such samples from real data, mainly belongning to high-density regions of the original probability density, then the discriminator can be used to detect anomalies) is nice. Overall I find that the paper is not clear and reproducible enough for me to recommend its acceptance: - Results are presented on a single private dataset, and I don't see any indication that the dataset will be shared with the community.
1) adam models keeps in memory x_t (the model parameter), g_t (the model gradient), m_t (momentum) \\hat v_t and v_{t-1} (the monotone and non monotone version of the second order moment estimation. In addition, the proof strategy is not novel enough, Theorem 1 is similar to Theorem 4 in AMSGrad paper and Theorem 2 is similar to Theorem 3.3 in Zhou et al's paper. First, the major argument of memory usage stems from 1) a miscalculation and 2) a misunderstanding of memory bottlenecks in deep learning. In contrast, the proposed model do not track v_{t-1}: this amounts to a memory saving of 20% considering all model related parameters.
The paper formulates the standard active learning problem as an MDP with the objective of minimizing the number of annotated labels required to meet a pre-specified prediction quality. Additionally, many values are chosen for the experiments without motivation and without testing a variety of values (e.g., 30 for the size of the dataset used to calculate the reward, 1000 RL iterations, and others). It would be good to evaluate this featurisation with a supervised active learner (like LAL), in order to disambiguate whether the good performance comes from these feature choices, or from the recent RL algorithms used to optimise. In the experiments, there needs to be discussion of how much variety there is in the different datasets in terms of their statistical properties that are relevant to active learning, such as how well the data cluster?
On section 2.3 and the explosion of gradients: There is a mistake in the equation on page 4 regarding the "gradient with respect to the learning rate". The authors' proposed optimizer is just a one-layer neural net with 32 hidden units that gets as input basically all the terms that the hand-designed optimizers compute, and it has everything it needs to simply use weight decay and learning rate schedules
Unlike the existing work, weight compression is applied as a form of weight distortion, i.e. the model has the full degree of freedom during fine-tuning (to recover potential compression errors). Pros: - The proposed method is shown to work with existing methods like weight pruning, low-rank compression and quantization. They used different model compression techniques in this framework to show the effectiveness of the proposed method. Overall, I think the novelty of the paper is very limited, as all the weight distortion algorithms in the paper can be formulated as the proximal function in proximal gradient descent. Therefore, it is not clear how the proposed framework is helping the model compression techniques. PS: After discussion, I think the motivation of the method is not clear to understand why the proposed method works. The paper does not really propose a new way of compressing the model weights, but rather a way of applying existing weight compression techniques. Since SGD is used for training, several minibatches are needed to achieve a relatively stable solution for projection using the proximal function, which is exactly the proposed framework in Fig. 1.
arXiv preprint arXiv:1611.03530. This paper proposes Permutation Phase Defense (PPD), a novel image hiding method to resist adversarial attacks. In summary the authors propose a  simple and intuitive method to improve the defense on adversarial attacks by combining random permutations and using a 2d DFT. While the security of a model against adversarial attacks is important, a defense should not sacrifice clean accuracy to such an extent. The experiments with regards to robustness to adversarial attacks I find convincing, however the overall performance is not very good (such as the accuracy on Cifar10). 2. The authors state "We believe that better results on clean images automatically translate to better results on adversarial examples" The permuted phase component does not admit weight sharing and invariances exploited by convolutional networks, which results in severely hindered clean accuracy -- only 96% on MNIST and 45% on CIFAR-10 for a single model. The paper demonstrated the method on MNIST and CIFAR10, and evaluates it against a number of adversarial attacks. The idea is drawn from cryptography, where the random permutation is treated as a secret key that the adversarial does not have access to. The authors should introduce an appropriate threat model and evaluate this defense against plausible attacks under that threat model.
Specifically, they showed that when features (parameters of DQN) are trained in one environment (default flavour/mode) and then used as an initialization for the same model but for a slightly different environment ( i.e. still captures key concepts of the original environment ) can boost the performance of the model in the new flavour/environment. More importantly, the performance boost is significant when DQN's parameters which are used to initialize the model for the new environment were trained using dropout and L2 regularization in the default flavour/mode. Strengths: + This paper is interesting in the sense that it empirically shows that using regularization in training deep RL can be helpful when the goal is the generalization from one flavour of an environment to another one but very similar to the original. - According to the paper (at least my understanding) DQN's hyper-parameters are tuned based on default mode/flavour environment. -The proposed method is only applicable when the default environment and the new one are very similar.
The paper proposes a sampling-based method that aims at accelerating Batch Normalization (BN) during training of a neural network. The paper has an interesting idea about sampling some features to speed up the batch normalization. (+) The method can be easily adapted to BN or other batch-based methods. - In normal BN, the gradient is propagated through the normalization factor as well, how would that change in the case of subsampled BN? Because random sampling is involved, this will result in less regular patterns of computation, this could likely make the implementation of BN to be less efficient. Clarity: I have not been able to fully understand why the proposed (uniform) sampling variant of BN is better than previous effort at making BN less computationally expensive in a GPU-based training environment by reading the paper: 1. The authors argue that the "summation" operation is the one that makes BN expensive; however, the authors have not demonstrated enough evidence of this argument
This work proposed using temporal logic formulas to augment RL learning via the composition of previously learned skills. The experiments results show that the composition method does better than soft Q-learning on composing learned policies, but how it performed compared to earlier hierarchical reinforcement learning algorithms? In section,  -> In this section are it has -> and it has This paper mainly focuses on combining RL tasks with linear temporal logic formulas and proposed a method that helps to construct policy from learned subtasks.
This paper proposes Deep Overlapping Community detection model (DOC), a graph convolutional network (GCN) based community detection algorithm for network data. In addition to the limited technical novelty, I have a few other concerns as well, including some on the experimental evaluation: - Real-valued node embeddings obtained from shallow/deep graph embedding methods can be used with *overlapping* versions of k-means. Exchangeable Random Measures for Sparse and Modular Graphs with Overlapping Communities.
Summary: The authors propose IMV-LSTM, which can handle multi-variate time series data in a manner that enables accurate forecasting, and interpretation (importance of variables across time, and importance of each variable). 2565-2573). ACM. This paper describes a recurrent model (LSTM specifically, but generalizable) which can produce variable-wise hidden states that can be further used for two types of attentions: 1) variable importance for the importance of each variable (not accounting for time), and The authors use one LSTM per variable, and propose two implementations: IMV-Full explicitly tries to capture the interaction between the variables before mixing the LSTM hidden layers with attention.
The highlighted contribution is a "stochastic generation" training procedure in which the training objective evaluates the reconstruction of output sequence elements from individual latent variables independently. - In Table 4, the difference between line 5 and line 6 is interesting and I wish it was discussed more, maybe used in the visualization experiment to show how/why "stochastic generation" with a larger window improves performance. - The approximate posterior used was used first in (Bayer & Osendorfer, "Learning stochastic recurrent networks", 2014) not (Chen 2018).
- compute the max over all pairs of these features (thus obtaining 2k values); The design of the predictor is: - for each pair, compute 2k features (of the form ReLu(linear combination of the values, without bias)); Minor Comments: 1. The statement of Theorem 4.1 itself does not show the advantage of over-parameterization because optimization is not discussed. I suggest also adding discussion on the optimization to Sec.4 as well.
This seems unlikely, and it seems more likely that given the number of conversations ~30 per participant is similar to the number of emotion words that you asked each worker to cycle through nearly all of the emotions or that given they were able to select, they might describe the same emotion, e.g. "fear" several times. I also have some doubts about the two claimed contributions of the paper (the authors actually list 3 contributions in the introduction, but for convenience I lump the 2 non-data ones together): (1) Dataset: The dataset was crowdsourced by giving workers an emotion label (e.g., afraid) and asking them to define a situation in which that emotion might occur and inviting them to have a conversation on that situation. The paper in particular is contributing its collected set of 25k empathetic dialogs, short semi-staged conversations around a particular seeded emotion and the results of various ways of incorporating this training set into a generative chatbot. E.g. would one worker write ~30 conversations on the same emotion.
The main contribution is in replacing the heuristic to find good quantization intervals of (Zhu et al, 2016) with a different heuristic based on a hierarchical clustering algorithm, and empirically validating its effectiveness. The main idea is to use 'nest' clustering for weight quantization, more specifically, it partitions the weight values by recurring partitioning the weights by arithmetic means and negative of that of that weight clustering. If that is the case, then many quantization algorithms can actually achieve better compression ratios with 2 bits quantization. 3) Activation quantization in Section 4 is a standard way for quantization, but I am curious how to filter out the outliner, and how to set the clipping interval? The proposed setup is almost identical to the one in (Zhu et al, 2016), except for the replacement of the heuristic to find quantization intervals with another one. This paper proposes to use n-ary representations for convolutional neural network model quantization. Does this quantization scheme introduce many zeros?
It shows superior performance than the baseline methods without such task-discriminator on medical image restoration and image super-resolution. This paper proposed a new method for image restoration based a task-discriminator in addition to the GAN network. 2. Actually, as the authors mentioned, GAN is not an appropriate model for image restoration when  accurate image completion is required. Please see the following comments: 1. Adding an task-discriminator in a GAN network seems straightforward to improve the specific task. For medical image reconstruction and image super-resolution, the proposed method was not compared with any of the state-of-the-art methods, but only with the same method without a task-discriminator as a baseline. and Mu Lee, K., Accurate image super-resolution using very deep convolutional networks. and Fei-Fei, L., Perceptual losses for real-time style transfer and super-resolution. Rui Huang, Shu Zhang, Tianyu Li, Ran He, Beyond Face Rotation: Global and Local Perception GAN for Photorealistic and Identity Preserving Frontal View Synthesis, ICCV 2017. 3. It is not clear how much data is used to train the super-resolution model and whether there is overlap between training data for super-resolution task and test data for recognition task.
The paper presents a convergence analysis for manifold gradient descent in complete dictionary learning. Chen et al. Gradient Descent with Random Initialization: Fast Global Convergence for Nonconvex Phase Retrieval, 2018
Stochastic EM is used for end-to-end learning, an algorithm that is L times more expensive than MAML, where L is the number of mixture components. Summary: This work tackles few-shot (or meta) learning, providing an extension of the gradient-based MAML method to using a mixture over global hyperparameters. This paper proposes a mixture of MAMLs (Finn et al., 2017) by exploiting the interpretation of MAML as a hierarchical Bayesian model (Grant et al. 2018). - BMAML: https://arxiv.org/abs/1806.03836. This is also quite complex and expensive, compared to Versa, but provides good results. - Results on miniImagenet are not encouraging; the gains on MAML are small and similar methods that generalize MAML (Kim et al., 2018, Rusu et al., 2018) achieve significantly better performance. This type of effort is needed to motivate an extension of MAML which makes everything quite a bit more expensive, and lacks behind the state-of-art, which uses amortized inference networks (Versa, neural processes) rather than gradient-based. - Nonparametric extension via Dirichlet process mixture. While the idea of task clustering is potentially useful, and may be important in practical use cases, I feel the proposed method is simply just too expensive to run in order to justify mild gains. Each task stochastically picks a mixture component, giving rise to task clustering. Ultimately, it performs on par with MAML, despite having three times the capacity. State of the art results on miniImageNet 5-way, 1-shot, the only experiments here which compare to others, show accuracies better than 53: - Versa: https://arxiv.org/abs/1805.09921. 1. The performance of few-shot classification on MiniImageNet is not comparable to the state of the art (Table 2, Table 1).
It is unclear how the tradeoffs in optimizing against multiple discriminators stack-up against bigger GANs. From my perspective, the paper is interesting because it introduces new methods into GANs from another community. + On the experiments run, the HVM method appears to be an improvement over the two previous approaches of softmax weighting and straightforward averaging for multiple discriminators. The authors find that optimizing with respect to multiple discriminators increases diversity of samples for a computational cost. SN-GAN have roughly the same computational cost and memory consumption of DC-GAN, but inception and FID are much higher. I guess that the reason may be that: the significant computational cost (both in FLOPS and memory consumption) increase due to multiple discriminators destroys the benefit from the small performance improvement.
The paper proposes a class of Evolutionary-Neural hybrid agents (Evo-NAS) to take advantage of both evolutionary algorithms and reinforcement learning algorithms for efficient neural architecture search. It would be better to use different symbols here.
However, the paper is not in a good shape for publication in its current form. More importantly, it seems that Section 3.4 is a key section to explain how to perform unsupervised classification.
1. It shows that the optimal convergence rate of BN can be faster than vanilla GD. However, this paper has a significant number of problems that need to be addressed before publication, perhaps the most important one being the overlap with prior work.
This latent space representation is used as the reinforcement learning signal for the learner (probing) agent similar to the curiosity driven techniques where larger changes in the representation of mind are sought out since they should lead to larger differences in demonstrator agent behavior. 1) Summary This paper proposes a method for learning an agent by interacting and probing an expert agents behavior. essentially, a learner agent learns how to probe a demonstrator agent to provide more information about what's being demonstrated and prevent over-fitting to a set of fixed demonstrations. 3. The core premise behind training the learner agent with RL is using a curiosity driven approach to train a probing policy to incite new demonstrator behaviors by maximizing the differences between the latent vectors of the behavior trackers at different time steps. The approximated demonstrator agent is trained through standard imitation learning techniques and the learning or probing agent is trained using reinforcement learning. When the probing agent is testing the expert, it is essentially showing the imitator many different configurations of the environment. - Same scale for the y-axes across figures This paper presents a method for interactive agent modeling that involves learning to model a demonstrator agent not only through passively viewing the demonstrator agent, but also through interactions from a learner agent that learns to probe the environment of the demonstrator agent so as to maximally change the behavior of the demonstrator agent. Comparing against this baseline could serve as evidence that we need to actually learn the probing agent to acquire a more optimal policy.
CoRR, abs/1802.03006. This paper proposed to train a forward model used in reinforcement learning (RL) by task-independent losses. Sure, those works focus on video prediction, while this work focus on building a "forward model"and is supposed to be for model-based RL, but this work has not performed any model-based RL experiments, so from my point of view, it is a video-prediction model contingent on an action input. While I think this work has potential, this paper is clearly not ready for publication, and below are a few suggestions on what I think the authors need to do to improve the work: (1) The authors emphasize novelty, and being "first" a few times in the paper, but fail to mention the large existing work done on video prediction (i.e. [1]), many of which also used these triplet loss or adversarial losses.
The primary challenge in such networks is defining a distribution over the weights connecting two layers of infinite width. Furthermore, the only application of these infinitely wide networks proposed in this paper is for initialization of the weights of finite width networks. 1. The so-called "infinite width" is just yielded by kernels in RKHS for weight initialization. Response to rebuttal: The authors have addressed my question about the weights being still in the same RKHS.
In summary, this paper does the following: - The initial problem is to analyze the trajectory of SGD in training ANNs in the space of  P of probability measures on Y \\times Y. There is no actual connection to SGD left, therefore it is even hard to argue that the predicted shape will be observed, independent of dataset or model(one could think about a model which can not model a bias and the inputs are mean-free thus it is hard to learn the marginal distribution, which might change the trajectory) For SGD, the trajectory goes to the turning point very soon (usually no more than 10% of the training steps), whereas SMLC goes to the turning point much slower. Cons 1: One of my major concern is --- if you look at the trajectory of the experiment v.s. SMLC (Figure 3), they look similar at first glance. Pros 1: The trajectory presented in this paper is much more reliable than that in (Ziv and Tishby 17'), since measuring the entropy and conditional entropy of discrete random variables are much easier.
This paper proposes an architecture search technique in which the hyperparameters are modeled as categorical distribution and learned jointly with the NN. I speculate that this parameter is essential as the categorical distribution gets a bigger search space. I very much enjoyed the simplicity of the approach, but the question of innovation is making wonder whether this paper makes the *CONF* bar of acceptance.
The paper aimed at improving the performance of recommendation systems via reinforcement learning. Summary: The paper presents a session-based recommendation approach by focusing on user purchases instead of clicks. The author proposed an Imagination Reconstruction Network for the recommendation task, which implements an imagination-augmented policy via three components: (1)  the imagination core (IC) that predicts the next time steps conditioned on actions sampled from an imagination policy; The method is inspired from concepts of cognitive science which adds an imagination reconstruction network to an actor-critic RL framework in order to encourage exploration. Strengths of the paper: (1) The research problem that the performance of recommendation systems needs to be improved is of great value to be investigated, as recommendation systems play crucial role in people's daily lives. Comments: The proposed architecture is an interesting inspiration from Neuroscience which fits into the sequential recommendation problem. The proposed algorithm with an innovative IRN architecture was intriguing. Weaknesses of the paper: (1) The motivations of applying reinforcement learning techniques are not convinced to me. There are many publicly available datasets for testing the performance of recommendation systems. The motivation of creating imagined trajectories instead of actual user trajectories is unclear. (2) State-of-the-art reinforcement learning algorithms were not taken into account for baselines in the experiments. The literature review is incomplete and misses important contributions on session-based recommendation, particularly, MDP-based methods such as Shani et al., An MDP-based recommender system, 2005 and Tavakol and Brefeld, Factored MDPs for Detecting Topics of User Sessions, 2014 (also see references therein). Why do the authors utilize reinforcement learning to the task but not other supervised learning techniques? (2) Figure 2 is not straightforward. Is it because reinforcement learning based methods work better than traditional machine learning based ones? How is theta_v associated with the parameters in LSTM. Is theta_v denoted the parameters of LSTM? As the proposed method is built based on reinforcement learning, it would be better if the authors could include state-of-the-art reinforcement learning algorithms as their baselines. How do the authors define the loss functions, i.e., \\mathcal{L}_{A3C} and \\mathcal{L}_{IRN}? What are the relationships among \\mathcal{L}_{A3C}, \\mathcal{L}_{IRN} and the one defined in equation (4)?
For this they  use one (of several possible) versions of synergy defintions and create a straight forward penalization term for a VAE objective (roughly the whole mutual information minus the maximum mutual information of its parts). This paper proposes a new approach to enforcing disentanglement in VAEs using a term that penalizes the synergistic mutual information between the latent variables, encouraging representations where any given piece of information about a datapoint can be garnered from a single latent. I commend the authors for taking a multi-disciplinary perspective and bringing the information synergy ideas to the area of unsupervised disentangled representation learning. Also why one should use the authors' suggested penalization term instead of total correlation is not discussed, nor demonstrated as they perform similarly on both disentanglement and synergy loss. Though I appreciate this is a somewhat subjective opinion, for me, penalizing the synergistic information is probably actually a bad thing to do when taking a more long-term view on disentanglement. If the authors want to continue with the synergy minimisation approach, I would recommend that they attempt to use it as a novel interpretation of the existing disentangling techniques, and maybe try to develop a more robust disentanglement metric by following this line of reasoning. Also the potential of synergy is not really demonstrated, e.g. for representation learning, causality, etc., and appears here ad hoc.
The proposed model is an extension of Matching Networks [Vinyals et al., 2016] where a different image embedding is adopted and a pixel-wise alignment step between test and reference image is added to the architecture. The motivation is partially covered by your statement "marginalizing over all possible matching is intractable", nevertheless an explanation of why it is reasonable to introduce these assumptions is not clearly stated. On the other hand, authors may argue that the hyper-column matching is not just about performance, whereas it also adds interpretability to why two images are categorized the same. ->incomprehensible sentence with two whiles: ABM networks outperforms these other state-of-the-art models on the open-set recognition task while in the one-shot setting by achieving a high accuracy of matching the non-open-set classes while maintaining a high F1 score for identifying samples in the open-set. Authors argue that using average (independent) greedy matching of pixel embedding (based on 4-6 layer cnn hypercolumns) is a better metric for one-shot learning than just using final layer embedding of a 4-6 layer cnn for the whole image. Such as getting an idea about a regularizer, a prior, a mask, etc. In this work, the authors tackle the problem of few-shot learning and open-set classification using a new type of NNs which they call alignment-based matching networks or ABM-Nets for short. One motivation for proposing an alignment-based matching is a better explanation of results.
Summary: This paper proposes a novel differentiable approximation to the curiosity reward by Pathak et al. that allows a learning agent to optimize a policy for greedy exploration directly by supervised learning, rather than RL. The authors compare the differentiable function against using prediction error via REINFORCE and DQN, showing that their intrinsic curiosity method results in more interactions with unseen objects than the other two methods. + "This leads to a significantly sample efficient exploration policy. - There is very little *science* in this paper, beyond the experiments pitting "improved algorithm" vs DQN/REINFORCE, which nobody ever claimed would be a good approach to exploration! This is incorrect. The paper is based on using the gradient of the forward model to directly optimize the policy to produce higher prediction errors, as in Pathak et al. But in order to make the prediction error differentiable, it makes the severe assumption that the next state x_{t+1} is constant and does not depend on a_t, which is false and invalidates the idea of optimizing actions for prediction error.
This paper proposes a new framework for topic modeling, which consists of two main steps: generating bag of words for topics and then using RNN to decode a sequence text. This paper proposes TopicGAN, a generative adversarial approach to topic modeling and text generation. The model basically combines two steps: first to generate words (bag-of-words) for a topic, then second to generate the sequence of the words. As a result, this paper achieved impressive outcome for topic modeling tasks. I did not see the contribution of this part to the whole model as a topic model, although the joint training shows the marginal performance gain on text generation. For the first task, classification is not the main purpose of topic models, and while text classification _is_ used in many topic modeling papers, it is almost always accompanied by other evaluation metrics such as held-out perplexity and topic coherence. (3) I did not see a major improvement of the proposed model over others, given that the only numerical result reported is classification accuracy and the state-of-the-art conventional topic models are not compared. Our assumption aligns well with human intuition that most documents are generated from a single main topic." This goes very much against the common assumption of a generative topic model, such as LDA, which the model compares against. This is because the main purpose of topic modeling is to actually infer the topics (per-topic word distribution and per-document topic distribution) and model the corpus. I would expect more comparisons than classification accuracy, such as topic coherence and perplexity (for topic modelling) and with more advanced conventional models. There are two main evaluations tasks: text classification and text generation. (2) The proposed model ignores the word counts, which can be important for topic modelling.
The primary insight here is that there exists a smaller set of unique tasks, the knowledge from which is transferable to new tasks and using these to learn an initial parametrized reward function improves the coverage for IRL. - One of main contributions is avoiding the need for hand-crafted features for the IRL reward function. + To a large extent, circumvents the need of having to manually engineered features for learning IRL reward functions > section4.1 (MandRIL) meta-training: What is the impact/sensitivity of computing the state visitation distribution with either using the average of expert demos  or the true reward?
It builds on a recent approach by Achiam et al on Constrained Policy Optimization (oft- mentioned "CPO") and an accepted NIPS paper by Chow which introduces Lyapunov constraints as an alternative method.
The paper presents a pool-based active learning method that achieves sub-linear runtime complexity while generating high-entropy samples, as opposed to linear complexity of more traditional uncertainty sampling (i.e., max-entropy) methods. -The main contribution of this algorithm is computational complexity, but I am not very persuaded by the idea of using the GAN in order to produce a sublinear (faster) time algorithm for active learning, since training the GAN may sometimes take more time that the whole active learning process. However, my only & major concern is that one has to train GANs before the active learning process, which might cost more than the whole active learning process. (2) In terms of accuracy comparison, on Cifar-10-ten classes experiments, all ASAL variants have similar accuracies as random sampling, while traditional pool-based max-entropy clearly works much better. I believe that it would be better for your paper if you work a bit more on the experimental evaluation and submit a revised version at a later deadline.
Wrt the experiments, I appreciate that the authors took the time to investigate the poor performance of MIXER. In summary, the contribution over RAML and SPG in combining them is quite incremental, and the practical importance of combining them is questionable, as is the integrity of the presented experiments, given how poorly MIXER is reported perform, and the omission of stronger baselines like SCST and AC methods. Table 1 suggests that MIXER can outpeform ML by only 0.1 Bleu points, and outpeformed by RAML? - Existing baselines in the paper (i.e. MIXER) do not perform as expected (i.e barely better than ML, worse than RAML) In particular, if this generalization can significantly outpeform existing methods it generalizes with non-degenerate settings, this would overcome the more incremental contribution of combining SPG and RAML. Furthermore, the MLE interpretation discussed is contained within the RAML paper, and the reductions to RAML and SPG are straightforward by design, and so do not really provide much new insight. Considering this, I feel that the importance of the paper largely rests on investigating and establishing the utility of the approach experimentally.
Both terms of the IB cost function are formalized as mutual informations, but since in neural nets, the latent "compression" is a deterministic function of the inputs, a severe technical problems arises: the joint distribution between p-dimensional inputs X and the q-dimensional latent compression L is degenerate in that  its support lies in a space of dimension p (and not p+q as it would be in the non-degenerate case). This paper provides a method to do explicit IB functional estimation for deep neural networks inspired from the recent mutual information estimation method (MINE). This work is about layer-wise training of networks by way of optimizing the IB cost function, which basically measures the compression of the inputs under the constraint that some degree of information with respect to the targets must be preserved. By using the method, the authors 1) validate the IB theory of deep nets using weight decay, and 2) provides a layer-wise explicit IB functional training for DNN which is shown to have better prediction accuracy. This work attempts to study the degree to which a layer by layer information bottleneck inspired objective can improve performance, as well as generally attempt to clarify some of the discussion surrounding Shwartz-Ziv & Tishby 2017. The title, abstract and especially the conclusion ("This provides, for the first time, strong and direct emperical evidence for the validity of the IB theory of deep learning") seem to present the paper as somehow offering some clarity and further support for the assertions of the Shwartz-Ziv & Tishby 2017 paper, but that paper hoped to establish that information bottleneck can explain the workings of ordinary networks. Despite a recurring focus of the text that this paper applies and information theoretic objective at each layer of the network, and hence is novel, the final sentence of the paper suggests it might not actually be needed and single layer IB objectives can work as well. - How does the beta (in IB objective) selected in the experiments for comparison? On one hand, I find this paper interesting, because it aims at carefully studying the proposed link between DNN training and IB optimization, thereby showing that layer-wise IB training indeed seems to work very well in practice. How does this justify using the individual elements of the discriminator in the functional form of the IB objective?
-The proposed technique seems to include very heavy feature engineering and several ad-hoc practical steps--that is far from the motivation of using NN in tabular data. It seems to me that (except the minor small section of streaming data), the paper is more like a proper verification of how tree-based learning algorithms work very well in tabular data--which is far from the basis of the paper and does not make the paper novel enough for *CONF*.
Authors present a set of criteria to categorize MNISt digists (e.g. slant, stroke length, ..) and a set of interesting perturbations (swelling, fractures, ...) to modify MNIST dataset. This paper discusses the problem of evaluating and diagnosing the representations learnt using a generative model. They suggest analysing performance of generative models based on these tools. This is a very important and necessary problem. Since their method is manually designed for MNIST, the manuscript would benefit from a justification or discussion on the  common pitfalls and the correlation between MNIST generation and more complex natural image generation tasks. Since the presented metrics do not show a significant difference between the VAE and Vanilla GAN model, the question remains whether evaluating on MNIST is a good proxy for the performance of the model on colored images with backgrounds or not. Studying the properties of a generative model on such datasets is very challenging and the authors have not added a discussion around that. For example sharpness and attending to details is not typically a challenge in MNIST generation where in other datasets this is usually the first challenge to be addressed. However, when the entire image is subject to the generative model, it learns multiple properties from the image apart from shape too - such as texture and color. 1. Morphological properties deals with only the "shape" properties of the image object. 2. Extracting morphological properties of the image is straight-foward for MNIST kind of objects. However, here the authors have assumed that the latent space of the generative models are influenced only by the morphological properties of the image - which is wrong. * Providing benchmark data for tasks such disentanglement is important but I am not sure generating data is sufficient contribution for a paper. Then that method/data are a benchmark.
In particular it shows the stationary point of an l1 regularized layer has bounded non-zero elements. page 4, the concept of stationary point and general position can be introduced before presenting Theorem 1 to improve readability. The perspective of the proof is interesting: By chain rule, the stationary point satisfies nnz(W^j) linear equations, but the subgradients of the loss function w.r.t. the logits have at most N\\times ks variables. 2. the claim is a little bit counter intuitive: Theorem 1 claims the sparse inequality holds for any \\lambda.
The approximated Hessian matrix is then used to estimate the increment of loss after pruning a connection. The paper proposes a multi-layer pruning method called MLPrune for neural networks, which can automatically decide appropriate compression ratios for all the layers. Summary: I do appreciate the fact that the proposed method does not require hyper-parameters and that it seems to yield higher compression rates than other pruning strategies that act on individual parameters. Also, following the pipeline [train-prune-retrain] can be substituted by pruning while training with little overhead as in recent papers: (such as Learning with structured sparsity or Learning the number of neurons in DNN both at NIPS2016 or encouraging low-rank at compression aware training of DNN, nips 2017). Is that guaranteed? This paper introduces an approach to pruning the parameters of a trained neural network. Experiments: - While the reported compression rates are good, it is not clear to me what they mean in practice, because the proposed algorithm zeroes out individual parameters in the matrix W_l of each layer.
The paper is addressing the problem of a specific multi-task learning setup such that there are two tasks namely main task and auxiliary task. When optimizing for the main loss function, the gradient of the auxiliary loss function is also used to update the shared parameters in cases of high cosine similarity with the main task. So a more general question would be: rather than define the similarity measure to measure the gradient similarity of the target and auxiliary loss, it would be more useful to try to learn or define whether the auxiliary task is good for the target task beforehand. For example, one can simply try (g(target task)-g(auxiliary task)) in the equation. There have been many interesting developments in adaptive scaling of multiple loss functions in the literature. Although the method limits the negative effect of the auxiliary task on the optimization of the main loss function, it can still slow down optimization if the auxiliary task is not well chosen. In that sense, the method is no silver bullet. ------ After rebuttal: I gave detailed responses to each part of the rebuttal below. As a minor comment, the authors might want to check that articles (such as "the") are not missing in the text. Besides, more similarity metrics are expected to be compared here to show why cosine is the optimal choice. 5) In the first reinforcement learning task, since cosine similarity is the only method used to measure the similarity between auxiliary task and the target task, it would be useful to show the comparison among other task relatedness method in reinforcement learning. In ImageNet experiment, auxiliary tasks actually hurt the final performance as the single task is better than all methods including the proposed one. Paper needs to be improved with a stronger experimental study and need to be re-submitted.
The trick that leads to computational tractability involves utilizing a latent variable and optimizing the f-divergence between joint distributions which is an upper bound to the (desired) f-divergence between the marginal distributions. This paper proposed a novel variational upper bound for f-divergence, one example of which is the famous evidence lower bound for max-loglikelihood learning. As far as I understood from the paper, changing the objective function to the upper bound of f-divergence have two merits compared to the existing methods. arXiv preprint, 2018." So I am not sure whether you can take credit from the "spread f-divergence" or not. 5) Despite the toy experiment in Sec 4.4, what are the advantages of the proposed f-divergence upper bound over the Fenchel-conjugate f-divergence lower bound? Regarding the methodology, it would have been nice to see the method of Nowozin et al. (2016) applied in all experiments since this is a direct competitor to the proposed method.
My comments are as follows: 1. About the significance and originality, although to my knowledge, there seems to be no the exact match in the existing approaches of the idea of incorporating labels into the prior of the latent variable of VAE, the idea seems a little bit trivial and less of technical depth. 4. Unclear sentence: "Compared with the VAE, latent codes where images with the same labels are clustered.";
A salient asset of the manuscript is that it avoids a pitfall of the original mixup algorithm: interpolating between inputs may result in underfitting (if inputs are far from each others: the interpolation may overlap with existing inputs). * Major remarks - There is little discussion in the manuscript about which layers should be eligible to mixup and how such layers get picked up by the algorithm. 2. The observations of mixing in the hidden space is better than mixing in the input space seem to contradictive to the observations by Mixup, it would be very useful if the paper can make that much clear to the readers. I would suggest that the authors fully compare with MixUp in the supervised learning tasks, namely using all the datasets (including ImageNet) and networks architectures used in MixUp for supervised learning.
Under several assumptions (input is Gaussian, non-linear activation is strictly increasing, stable system) it is shown that SGD converges linearly to the ground truth system with near-optimal sample complexity. However, understanding non-linear dynamical systems is extremely challenging and this paper provides strong convergence guarantees. For beta = 0 the ground truth dynamical system is linear and for beta = 1 the ground truth is a non-linear dynamical system with ReLU. I agree that the paper has nice convergence results that could possibly be building steps towards the harder problem of unobserved hidden states however, there is more work that could be done for unstable systems and possible extension to ReLU and other activations to take it a notch higher. Under this setting, the authors prove that for the given state equation for stable systems with random gaussian input at each time step, running SGD on a fixed length trajectory gives logarithmic convergence. The authors should update prior work on generalized linear models as well as neural networks.
In this, paper a GANs-based framework for additive (image) denoising and demixing is proposed. Pros: the authors develop a novel GAN-based approach to denoising, demixing, and in the process train generators for the various components (not just inference). I think this is an exciting contribution to dually learning component manifolds for demixing. On the other hand, HOW MUCH contribution is not addressed experimentally, i.e. the method is not properly compared with other denoising or demixing methods, and definitely not pushed to its limits. The proposed method is evaluated on both tasks (i.e., denoising and demixing) by conducting toy experiments on handwritten digits (MNIST). ********************* Update after author response: I think the Fashion-MNIST experiments and comparisons with ICA are many times more compelling than the original experiments. Caveats: I am knowledgeable about iterative optimization approaches to denoising and demixing, especially MCA (morphological component analysis), but *not knowledgeable about GAN-based approaches*, though I have familiarity with GANs. 2. In the experiment part, it would be nice to have Quantitive results presented, for example PSNR for denoising.
For this purpose, they show that when using dropout training we are maximizing a common lower bound on the objectives of a family of models, including most of the previously used methods for prediction with dropout. In this case, q(w) = p(w|\\Theta), as stated Eq (3) and in the corresponding equation provided in page 2 (the q(w) is not learnt because it only depends on the dropout rate, while the Θ are learnt by maximum log-likelihood and do not have a q associated). Additional experiments on popular image datasets are recommended.
The authors seek to make it practical to use the full-matrix version of Adagrad's adaptive preconditioner (usually one uses the diagonal version), by storing the r most recently-seen gradient vectors in a matrix G, and then showing that (GG^T)^(-½) can be calculated fairly efficiently (at the cost of one r*r matrix inversion, and two matrix multiplications by an r*d matrix). Rather than adapting diagonal elements of the adaptivity matrix, the paper proposes to consider a low-rank approximation to the Gram/correlation matrix. adagrad, adadelta are both popular adaptive variations of sgd. When you say that full-matrix computation "requires taking the inverse square root", I assume you know that is not really correct? Pros:  Shows how to make full matrix preconditioning efficient, via the use of clever linear algebra, and GPU computations. There is a great deal of discussion about full-matrix preconditioning, but there is no full matrix here. Given that rxr is a small constant sized matrix and that matrix-vector multiplication can be efficiently computed on GPUs, this matrix adapted SGD can be made scalable. It would help to clarify the connection between the Gram/correlation matrix of gradients and the Hessian and what is being done to ill-conditioning, since second order methods are basically designed for ill-conditioned problems..
The paper proposes a method for converting a non-differentiable machine learning pipeline into a stochastic, differentiable, pipeline that can be trained end-to-end with gradient descent approaches. * Significance: The concept of converting a non-differentiable pipeline to a differentiable version is indeed very useful and widely applicable, but the experimental section did not convince me that this particular method indeed works: the results show a very small improvement (0.7-2%) on a single system (Faster R-CNN), that has already been pretrained (so not clear if this method can learn from scratch). Clearly c(y + z) is not generally equal to c(y) + z, even if E z = 0.
The authors encourage the decomposition of the latent space into the 'template' and the 'attributes' features by training a discriminator network to predict whether the attributes and the template features come from the same image or not. The model learns how to decouple the attributes in an adversarial way by means of a discriminator. Given that the ground truth attribute decomposition for MNIST is not known, even the qualitative results are impossible to evaluate. There seems to be an interesting interaction between the encoder, discriminator and the attribute function that requires more investigation. Furthermore, while the authors spend many pages describing their methodology, the writing is often hard to follow, so I am still confused about the exact implementation of the attribute features \\phi(x, m) for example.
The paper proposes a new algorithm for implicit maximum likelihood estimation based on a fast Nearest Neighbor search. Evaluation:  This paper presents an interesting contribution: an implicit likelihood estimation algorithm amenable to theoretical analysis. Some of them I've highlighted earlier in my review (e.g., some of the theorem assumptions being typically true, comparison of likelihood and sample quality based on model capacity etc.). Algorithm - While significant advancements have indeed been made for nearest neighbor evaluation as the authors highlight, it's hard to believe without any empirical evidence that nearest neighbor evaluation is indeed efficient in comparison to other methods of likelihood evaluation. The prior work trained the same normalizing flow model via maximum likelihood and adversarial training, and observed vastly different results on likelihood and sample quality metrics. - Another possibility is to use generative models like Real-NVP for which the likelihood can also be computed in closed form. The authors acknowledge this, but then say "this is no longer the case due to recent advances in nearest neighbor search algorithms (Li & Malik 2016; 2017)" (p 3-4).
The new thing is that the authors found that quantization of activation function improves robustness, and the approach can be naturally combined with FGSM adversarial training. I would in the current form reject the paper.
A pair of sentences from a weak document pair are used as training data if their cosine similarity exceeds c1, and the similarity between this sentence pair is c2 greater than any other pair in the documents, under sentence representations formed from word embeddings trained with MUSE. Positives - Large improvement over previous attempts at unsupervised MT for the En-De language pair. c) training the usual unsup MT pipeline with two additional losses, one that encourages good translation of the extracted parallel sentences and another one forcing the distribution of words to match at the document level. - What is the total number of sentences in the weakly paired documents in Table 1? It would be useful to know the proportion of sentences you managed to extract to train your models. The model also uses the denoising autoencoding and reconstruction objectives of Lample et al. (2017). The results show improvements over the Lample et al. (2017) and that performance is heavily dependent on the number of sentences extracted from the weakly aligned documents.
Integrating CRFs with GNNs for node classification is not new ([1][2][3]), but this paper extended energy definition to capture high-order connections and use it for a new subsequent task: graph pooling. Strength: -- An interesting idea to use CRF idea to cluster the nodes on a graph for pooling purpose Otherwise, the paper is rather well written and has clarity.
This, unsurprisingly, limits the applications of DPPs, and has driven a lot of research focused on improving DPP overhead. Decision: I recommend that this paper be rejected. At a high level, this paper is experimentally focused, but I am not convinced that the experiments are sufficient for acceptance.
they show that, relying on the technique of deep taylor decomposition, their CNN relies its prediction on a different part of zebra fish than existing understanding. the paper uses model interpretation techniques to understand blackbox CNN fit of zebrafish videos. BACKGROUND: Hypothesis: Prey movements in zebrafish are characterized by specific motions, that are triggered by a specific pathway involving an area called AF7. AI: the prey stimuli was a characteristic movement that an SVM was trained to detect. In particular observations like " looking for salient features in the trunk of the tail while largely disregarding the tip", are typically absent from most deep learning studies and it's quite interesting. In order to identify which particular features the neural networks are paying attention to, the paper used Deep Taylor Decomposition, which allowed the authors to identify "clever-hans"-type phenomena (network attending to meaningless experimental setup differences that actually gave a way the ground truth classes). Furthermore, the authors conduct one good analysis (DTD) to explain the results of their CNN. SUMMARY: explore the use of CNN in a binary task on images of zebrafish the idea of a case study about the usefulness of model interpretation techniques is interesting. Perhaps the authors are also not aware that the fallacies that causes CNNs to overfit on some characteristics in the input data are also present in other machine learning tools such as SVMs.
They demonstrate that this adjustment is necessary and sufficient to benefit from the faster convergence of Nesterov gradient descent in the stochastic case. MaSS is both theoretically and empirically proved to outperform Nesterov SGD as well as SGD. The authors present a new first order optimization method that adds a corrective term to Nesterov SGD.
Summary This paper studied the expressive power of graph NNs, specifically, their universality and limitations under the non-anonymous setting, via the theory of distributed computations. Decision This paper gave us a new approach to analyzing the expressive power of graph NNs. Not only does this paper give new theoretical results, but also it opens the door to a new research direction by bridging the theories of graph NNs and distributed computations. For now, I am tending to accept the paper.
This work proposes to leverage a pre-trained semantic segmentation network to learn semantically adaptive filters for self-supervised monocular depth estimation. The authors have addressed many of my initial concerns and provided valuable additional experimental evaluations. === Post rebuttal update === Hence I think which the idea advanced in the paper has merits, the manuscript is not really ready for publication.
This paper proposed a neural iterated learning algorithm to encourage the dominance of high compositional language in the multi-agent communication game. The paper's description of the 'interval of advantage' --- the range of updates where a compositional language performs better on the task than a non-compositional language --- is insightful to me. Overall, I like the paper, but due to the concerns mentioned above I think it's borderline, with a slight lean towards rejection.
VL-BERT extend BERT by changing the input from subsequent sentence to image regions and modify the caption words has the additional visual feature embedding. They demonstrate that the pre-training procedure can help improve performance on down-streaming tasks like visual question answering, visual commonsense reasoning. While ViLBERT designs for easier extendable for other modalities, VLBERT is more focus on the representation learning on the vision and language, since the caption input also combines with the visual feature embedding. * Once textual embeddings are masked by [MASK], the related visual embedding (whole image) is also masked?
The UDR score can be used for unsupervised hyperparameter tuning and model selection for variational disentangled method. Based on the understanding of "why VAEs disentangle" [Burgess et al. 2017, Locatello et al. 2018, Mathieu et al. 2019, Rolinek et al. 2019], the authors adopt the assumption that disentangled representations are all alike (up to permutation and sign inverse) while entangled representations are different, and propose UDR method and its variants. -- To validate the fundamental assumption of UDR, the authors might consider to quantitatively validate that, disentangled representations learned by those approaches you used in the paper are almost the same (up to permutation and sign inverse). Therefore, I am not convinced that I should trust the results of the UDR, which combines multiple disentangled models. The problem this paper focuses on is essential because we usually apply unsupervised disentangled methods to analyze the data when the labels are unavailable.
This paper proposes two modifications for the MixMatch method [1] and achieves improved accuracy on a range of semi-supervised benchmarks. Mix-Match is already an elaborate method, and ReMixMatch additionally introduces learned data augmentation, an additional loss term for matching label distributions between labeled and unlabeled data, consistency-loss, and a self-supervised loss (section 3.3). For the reasons above, I think the paper is borderline, but I am currently voting for acceptance based on the strong empirical performance.
This paper proposes to use a semi-supervised VAE based text-to-speech (TTS) for expressive speech synthesis. I think it differs from standard semi-supervised training in that at test time we aren't explicitly interested in predicting labels from the semi-supervised labelled classes; rather, we feed in these labels as input to affect the generated model output.
The primary claim is that simple components which compute elementwise sum/average/max over the activations seen over time are highly robust to noisy observations (as encountered with many RL environments), as detailed with various empirical and theoretical analyses. This paper studies reinforcement learning for settings where the observations contain noise and where observations have long-range dependencies with the past. # UPDATE after rebuttal I have changed my score to 8 to reflect the clarifications in the new draft.
The authors discuss four techniques to improve Batch Normalization, including inference example weighing, medium batch size, weight decay, the combination of batch and group normalization. Equipped with the proposed techniques, the authors obtain promising results when training deep models with various batch sizes. 3. By combining all the techniques, the proposed method yields promising performance when training deep models with different batch sizes. (3) In Section 3.1, "we need only figure out …" should be "we only need to figure out …" The paper introduces four techniques to improve the deep network model through modifying Batch Normalization (BN). (2) It would also be beneficial to see the comparison with the original Ghost Batch Normalization in the final evaluation (section 4.2), since this method, according to section 3.2, was capable of the significant improvement for Caltech-256 dataset.
This paper introduces Precision Gating, a novel mechanism to quantize neural network activations to reduce the average bitwidth, resulting in networks with fewer bitwise operations. They learn a threshold value for which all activation values above the threshold are learned at full precision, while all below are learned at reduced precision. I agree that the following three key contributions listed in the paper are (slightly re-formulated): 1. Introducing Precision Gating (PG), the first end-to-end trainable method that enables dual-precision execution of DNNs and is applicable to a wide variety of network architectures. Even though at the moment it is unclear to me how statistically significant the results are, and I strongly recommend commenting on this in the paper, I think the idea of PG and the demonstrated benefits make the paper interesting enough to be accepted at *CONF*.
Their proposed VHE-GAN model encodes an image to decode its associated text and feeds the variational posterior as the source of randomness into the GAN image generator. (VHE) randomized generative adversarial network (GAN) that integrates a probabilistic text decoder, probabilistic image encoder, and GAN into an end-to-end multimodal model. attnGAN (CVPR18), b. TA-GAN (NIPS18), c. Object-GAN (CVPR19). Strengths: - The authors have proposed a nice multimodal model that allows inference of latent variables given only text or image, and also allows realistic synthesis of images from images, text, or noise. Combined with the included code release, this paper should be of interest to many. Finally, though this version of the paper includes code directly in a google drive link it would be ideal for the final version to reference a github code link - again to aid access to interested individuals. The authors have done a commendable job adding detail, further analysis, and experiments in the appendix of the paper. - while it is difficult to dilute such a complex model to 8 pages, and the included appendix clarifies many questions in the text body, it would be worth further passes through the main paper with a specific focus on clarity and brevity, to aid in the accessibility of this work.
The authors propose an agent that builds a dynamic knowledge graph of each state from the textual observation provided by the games, while choosing actions from a template-based action space. Pros: 1, I like the idea of constructing the knowledge graph as the agent roll out. However, I don't have any background in fictional games but dialog modeling. Under the general framework of A2C, the core contribution of the paper is to apply a graph attention network on the knowledge graph to help learn better representation of the game state and reduce the action space. Note that the baseline I am referring to is different from the LSTM-A2C baseline reported in the paper as: (1) with entity extraction, although you may not get a graph mask, but you can still have a object-mask which also reduces the action space;
This paper considers from a high level the problem of learning a latent representation of high dimensional observations with underlying dynamics for control. The authours could attempt to better highlight the more critical parts of their propositins (e.g. eq.
The paper tries to answer the following question: In adversarial defense training do manifold based defenses need to know the structure of the underlying data manifold? Next to some experiments on data sets with a manifold structure, the main contribution of the paper is a tandem of theorems that state the conditions under which models can recover the topology---or the number of connected components---of a data set correctly, If the number of connected components does not match, based on theorem 2, Corollary 1 argues that a generative model can generate an adversarial example that does not exist in the data-generating manifold. However, this work argues that if the generative model does not model the topology of the manifold, it can still be fooled by an adversarial example. I appreciate novel research that employs topology-based methods, but at present, I cannot fully endorse accepting the paper.
"efficiency searching scheme" This paper proposes a method, termed as Filter Summary (FS), for weight sharing across filters of each convolution layer. Compression of the convolution operation is done with weight sharing: unwrapped kernel (channel-major) is packed into 1D vector by having intersected segments (shared weights). The paper presents a novel compact parameterization of convolution filters. I would say the main concern should be "FS quantization achieves the same quality as original FS while having higher compression factor".
Statistics themselves are calculated using 32 bit floating point numbers. The key idea here is that for each tensor of 8-bit numbers, two 32 bit floating point statistics are recorded as well. Mapping between representation and actual numerical values is much more complex than when using standard floating point representation (as any given numerical value is defined by a single its 8 bit representation + alpha + beta tensor statistics), integrated circuitry used to execute arithmetic operations would likely be much more complex than when using standard floating point operations. Could be very useful for many embedded applications. The paper is well written, easy to follow, and provides a detailed background for readers who are not knowledgeable in this field. The evaluation is very convincing - the approach is demonstrated for image classification, Transformer-based translation, and neural collaborative filtering. Thus, I recommend accepting this paper.
They present a detailed comparative study spanning three datasets, four types of adversarial attacks and distortions, and two other baseline defense mechanisms, in which they demonstrate significant improvements (in some cases) of the sleep algorithm over the baselines. The paper proposes an ANN training method for improving adversarial robustness and generalization, inspired by biological sleep. 4) converting the network back to an ANN after the sleep phase has finished. The idea of a sleep phase as an alternative to explicit adversarial or generalization training is interesting. The sleep algorithm fails to outperform the baselines for each attack type (except for an almost negligible advantage in accuracy on JSMA) and barely even outperforms the control network in most cases (2/4 attacks it actually underperforms the control). I have some questions and concerns which I will detail per-section below, but overall, I believe that this paper is a valuable contribution to the literature and should be accepted once the authors have made a few necessary revisions.
The path angle visualization provides a novel tool to evaluate the empirical ability of any dynamics proposed for GANs to cancel out rotational components. Notably, a main idea to speed up convergence in GANs is to change the gradient play dynamics so rotational components are neutralized.
The authors show that when using appropriate upsampling operators, gradient descent biases the reconstructed images towards low-frequency components, while the noise components, which typically consist of high-frequency patterns, take longer to fit, so that early stopping can provide a useful bias for denoising. But, the paper would do well to embrace the Fourier domain and first discuss what is meant by "natural images" and "noise" in terms of their frequency content. After discussion: The authors have addressed my concerns, so I am changing my decision from Weak Reject to Weak Accept.
This model is used to learn a 3D visual representation that can be applied for semi-supervised 3D object detection, and for unsupervised 3D moving object detection. The results show that the proposed method: 1) has higher 3D object detection mAP than a view regression-based baseline from prior work (Tung 2019) for settings with little available 3D boundign box supervision; Results demonstrate the strengths of the proposed view-contrastive framework in feature learning, 3D moving object detection, and 3D motion estimation. Experiments are performed to evaluate the proposed method against baselines on 3D object detection (both in the semi-supervised setting and unsupervised moving object detection), 3D motion estimation, as well as sim-to-real transfer results (training in CARLA and testing on the KITTI dataset). - A natural follow-up to this paper is Contrastive Predictive Losses (which had several successes in pure vision setting[1]). It's really a good engineering work in term of integrating them together. how many labeled images? Also not quite unclear about the settings in UNSUPERVISED 3D MOVING OBJECT DETECTION. As a result, I would advocate for clear accept if we assess vision-based contribution for *CONF*; otherwise, I would only recommend weak accept the paper is solely based on ML contributions (the paper is still sound, well-written, with numerous experiments and with a semi-generic architecture)
===== Summary: To handle noisy labels, this paper proposed a curriculum loss that corresponds to the upper bound of 0-1 loss. Under symmetric label noise, "the minimizer of the expected symmetric noise risk (a risk that the label is corrupted by coin flipping noise) is identical to the minimizer of the clean risk (normal risk)". So, I'd like to increase my score as weak accept!
------------------------------------------------------------------------------------------------------------------------------------------------------------------------- Summary: This paper presents a detailed empirical study of the recent bonus based exploration method on the Atari game suite. The authors combine Rainbow with different exploration methods, such as count-based bonus methods, curiosity-driven methods, and noisy networks. #review This paper evaluates the recently proposed exploration methods that achieve ground-breaking performance in the difficult exploration problem, Montezuma's Revenge. Throughout the paper, the experiments and results raise questions on the robustness and generalization of existing exploration methods across various ATARI games, but the paper puts absolutely zero effort into investigating if there is a quick fix to the questions it poses. I think the main contribution of the paper is that it raises some questions over existing methods/trends in solving exploration problems in reinforcement learning by comparing the performance of multiple methods across various games in ATARI suite. I can appreciate the contributions of the paper and I am happy to recommend accept.
The authors provide a comprehensive study, with theory and classical simulation of the quantum system, on how to increaese the speed of CNN inference and training using qubits. it would be valuable to convert those to estimates of which values of eta would be enough (given quantum networks of the size used in the classical simulation experiment, or given larger networks). - Maybe define what you mean by "tomography" for ML folks without the quantum background? - There's a clear separation of background (which is concise and well explained) and contributions, but maybe it would be worth connecting the introduced algorithm more closely to existing work in non-convolutional quantum neural networks?
The paper proposes a method called FNA (fast network adaptation), which takes a pretrained image classification network, and produces a network for the task of object detection/semantic segmentation. Concrete comments 1. The paper's overall method is a novel one, unifying NAS on det/seg tasks, while prior works mostly only focus on one task. I like the direction this paper takes, NAS is too expensive and we need faster methods through meta learning/transfer learning. The paper is also clearly organized and written. To the best of my knowledge, the experiments setting is sensible and the results are good. It also "eliminates" the need for pretraining each instance of the subnetwork. The stuff in Table 6 is pretty interesting however, if convoluted. - Lack of error bars or comparison to random search. I am giving this paper a weak reject, as there is insufficient experimental evidence that the technique works, or generalises beyond Mobilenetv2.
11). ACM. The authors consider the alignment problem for multiple datasets with side information via entropic optimal transport (Sinkhorn). The idea of regularizing the transport cost based on the subset correspondence information sounds sensible. The main idea is to use a neural network to define a transport cost between two examples instead of using the popular Euclidean distance and have a L2 regularizer that encourages the resulting transport plan to be small if two examples belong to the same subset. I recommend that the paper be accepted.
Recommendation: Accept. Minor comments and questions for the author: - I am slightly confused by the introduction of the rtop operator. Under such a sampling mechanism, do you foresee any complications to using SpiderBoost with Sparse Gradients, eg., decrease in overall sparsity?
In this paper, the authors proposed two methods of Nesterov Iterative Fast Gradient Sign Method (NI-FGSM) and Scale-Invariant attack Method (SIM) to improve the transferability of adversarial examples. Two methods have been proposed,  namely Nesterov Iterative Fast Gradient Sign Method (NI-FGSM) and Scale-Invariant attack Method (SIM). The first method adopts Nesterov optimizer instead of momentum optimizer to generate adversarial examples. In this paper, the authors apply the Nesterov Accelerated Gradient method to the adversarial attack task and achieve better transferability of the adversarial examples.
The authors proposed a new method called "DropEdge", where they randomly drop out the edges of the input graphs and demonstrate in experiments that this technique can indeed boost up the testing accuracy of deep GCN compared to other baselines. The extensive experiment results also show that for deeper GCNs, DropEdge always win over other baselines (see Tab 1) despite most of them are marginal except the backbone being GraphSAGE on Citeseer. I vote weak-accept in light of convincing empirical results, some theoretical exploration of the method's properties, but limited novelty.
The outcome is a versatile model that enables long-range sequence modeling, achieving strong results on not only language model tasks but also RL and speech. A variety of compression techniques and training strategies have been investigated in the paper and verified using tasks from multiple domains including language modeling, speech synthesis and reinforcement learning. For testing and evaluating the modeling of really long context sequence modeling, the authors introduce PG-19, a new benchmark based on Project Gutenberg narratives. Particularly, the authors propose a new benchmark PG-19 for long-term sequence modeling. The key novelty of this model is to preserve long range memory in a compressed form, instead of discarding them as previous models have done. The paper finally presents an analysis of the compressed memory and provide some insights, including the fact that the attention model uses the compressed memory. The paper also introduces a new benchmark for long-range dependencies modelling composed of thousands of books. ## Updated review I have read the rebuttal. The latest version of the paper adressed all my concerns, hence I change my rating to Accept. I am happy with the efforts made by the authors and I am raising my score to 8 (accept). The probably more interesting part of this paper is the training schemes designed to train the memory compression network. - The presented approach is significant, as modelling long-range dependencies is an important milestone in sequence modelling. I think this paper should be accepted, mainly because: - The proposed model is novel as far as I can tell.
The authors present an algorithm CHOCO-SGD to make use of communication compression in a decentralized setting. The authors consider CHOCO-SGD for non-convex decentralized optimization and establish the convergence result based on the compression ratio. This paper studies non-convex decentralized optimization with arbitrary communication compression. This paper studies the convergence of CHOCO-SGD for nonconvex objectives and shows its linear speedup while the original paper of CHOCO-SGD only provides analysis for convex objectives. Overall, this could be a great paper if fixing the issues above. 3. About "datacenter setting" experiment, it seems not an apple to apple comparison between CHOCO-SGD and all-reduce method since CHOCO-SGD stands for the decentralized algorithm with compression and all-reduce stands for a centralized algorithm without compression.
Empirically the authors evaluate the VAEs on four different datasets (a synthetic tree dataset, binarized MNIST, Omniglot, and CIFAR-10) for various choices of product spaces (fixed curvature and learnable curvature) and choices of latent space dimensionality. Summary: This paper devised a framework towards modeling probability distributions in products of spaces with constant curvature and showed how to generalize the VAE to learn latent representations on such product spaces using Gaussian-like priors generalized for this case. --Past works have considered VAEs on single constant curvature spaces and hence it is well-motivated to consider a more flexible model that enables usage of products of such spaces. Strengths, Weakness, Recommendation I like what the authors are trying to do here; embeddings and discriminative models on non-Euclidean spaces have been developed, offer credible benefits, and generative models are the next step. I believe this work should be accepted, as while the numerical results are not particularly impressive, it provides some clear foundational work for further exploration of the use of non-euclidean latent spaces in VAEs.
The first method is to use a teacher-student mechanism that uses a fully real-valued network to teach a binary network. As the ablation study shows the gating function actually hurts for binary down-sampling layers. Therefore I am looking forward to the real-timing results. 2. Comparisons with existing methods: On ImageNet, the model is compared with a complete list of alternative methods (low-bit quantization, larger binary nets, binary nets and real-valued nets).
To achieve a more reasonable algorithm, author prune the redundant channel by controlling the deviation of the summation statistically small,  and reusing the filter by important sampling the given channel. Experiment show that this method can reach a competitive prune radio against other pruning algorithm, and show robustly in retained parameters vs error experiment. Unlike the other deterministic method, sampling skill suffer variance propagation problem, the pre-layer variance will affect the sampling probability of next layer, how this pruning work if we change status of the pre-layer,  I didn't find any theoretical guarantee and only find a proof of single layer reconstruction bound. In most case, if we want to prune the large channel network,  picking the top-1 significant filter or random sampling top-k filter will almost do the same thing. Important sampling require an input of probability [p1, p2, p3, ... Among most heuristics prune method,  pruning with mathematics guarantee is indeed more convincing. This paper attacks the problem of pruning neural networks to obtain sparser models for deployment.
Alike other HRL agents, their method has two types of policies (manager and subpolicies), but different from other works they do not keep parameters fixed in post training for new tasks. In addition to the parameters, they do not fix the time length. The motivation of this paper is "most methods still decouple the lower-level skill acquisition process and the training of a higher level that controls the skills in a new task." The paper proposes a method to learn higher-level skill selection and lower-level skill improvement jointly. 2. I think the author didn't justify his key design choices well. 1. Why is random length a valid choice? This paper is under the research area of "hierarchical reinforcement learning." However, just like temporal abstraction, the HRL is a general idea instead of an existing problem formulation or a particular algorithm. It seems that the author is not aware of this point as the paper claims a particular way of achieving HRL is the HRL itself (in section 4.1 "In the context of HRL, a hierarchical policy with a manager πθh(zt|st) selects every p time-steps one of n sub-policies to execute.").
This paper describes an approach to applying attention in equivariant image classification CNNs so that the same transformation (rotation+mirroring) is selected for each kernel. *Paper summary* The paper combines attention with group equivariance, specifically looking at the p4m group of rotations, translations, and flips. + Demonstrates a rotation-equivariant attention mechanism - I think it would have been easier just say that you are using a roto-translation or p4m equivariant CNN with attention after each convolution.
I appreciate the feedback of the Authors and I have decided to increase the rating. * It would be interesting to see the effect of varying the subspace dimension.
</update> The paper shows that the MSE of a deep network trained to match fixed random network is a conservative estimate of uncertainty in expectation over many such network pairs. <update> I would like to thank authors for verbose response and the revised version: it is a bit more clear. Wouldn't that then imply that the uncertainty would be zero for any input (even an out-of-distribution one), as the prior network and predictor network always agree?
In this paper, the authors outline a method for system control utilizing an "agent" formed by two neural networks and utilizing a differentiable grid-based PDE solver (assuming the PDE describing the system is known). The agent is split into a control force estimator (CFE) which applies a force to advance the state of the controlled system, and an observation predictor (OP) which predicts the trajectory needed to reach the target state. [Summary] This paper proposes to combine deep learning and a differentiable PDE solver for understanding and controlling complex nonlinear physical systems over a long time horizon. In particular, it would be nice to discuss what happens when the physics is a black box (i.e. we can interact with the system by applying control and observing, but we don't know the rules governing the physical system). The paper presents an interesting mix of neural networks and traditional PDE solvers for system control, and I vote for acceptance.
Summary: This paper proposes the use of two intrinsic rewards for exploration in MARL settings. To the best of my knowledge, the broad idea of applying information theory to multi-agent exploration, in addition to the specific instantiation described in the paper, is novel. My main reservation is a lack of comparisons to single agent exploration methods. -------------------UPDATE AFTER AUTHOR RESPONSE--------------------- The authors have done a great job address my two concerns (similarity to prior work and empirical comparisons with single-agent exploration).
This paper discusses an extended DSL language for answering complex questions from text and adding data augmentation as well as weak supervision for training an encoder/decoder model where the encoder is a language model and decoder a program synthesis machine generating instructions using the DSL. The key insight is to let the semantic parser point to locations in the text that can be used in further symbolic operations. This paper presents a semantic parser that operates over passages of text instead of a structured data source. This is excellent work, and it should definitely be accepted. The three claimed contributions are (1) better numbers, (2) better compositionality / domain applicability, and (3) better interpretability. Please understand them, however, in terms of my overall score and what I said above. Another question raised by the passage-span predicate: the more you use bare passage-span programs for training, the more the network learns to put all of its compositional reasoning inside, in an opaque way, instead of giving you interpretable compositionality. I found myself immediately looking for the numbers/results when you introduce the experiment. But the fact that you have this predicate lets the model do these filters and greater-than comparisons inside the network in an opaque way, while also getting interpretable operations for some questions (table 5 is further confirmation of this, and of the fact that you probably are not capturing many of more the complex, compositional questions in DROP). This predicate lets the model shortcut any interpretable reasoning and do operations entirely inside the encoder/parser.
The authors propose an approximate of the natural gradient under Wasserstein metric when optimizing some cost function over a parametric family of probability distributions. Similarly authors give a variational form of the wasserstein natural gradient . Natural Wasserstein Gradient similar to the so called natural fisher gradients preconditions the gradient using a matrix that uses the local curvature of the manifold of the parametric distribution. Overall, I lean to the acceptance side.
Summary: The authors proposed a method for generating hierarchical importance attribution for any neural sequence models (LSTM, BERT, etc.) Towards this goal, the authors propose two desired properties: 1) non-additivity, which means the importance of a phrase should be a non-linear function over the importance of its component words; 2) context independence, which means that the attribution of any given phrase should be independent of its context. For example, consider the input: "The movie is the best that I have ever seen.
The authors propose learnable "kaleidoscope matrices" (K-matrices) in place of manually engineered structured and sparse matrices. The main contribution of the paper is the introduction of a family of matrices called kaleidoscope matrices (or K-matrices) which can be represented as a product of block-diagonal matrices of a special structure.
The dataset is an extension of CLEVR using simple motions of primitive 3D objects to produce videos of primitive actions (e.g. pick and place a cube), compositional actions (e.g. "cone is rotated during the sliding of the sphere"), and finally a 3D object localization tasks (i.e. where is the "snitch" object at the end of the video). The construction of the dataset focuses on demonstrating that compositional action classification and long-term temporal reasoning for action understanding and localization in videos are largely unsolved problems, and that frame aggregation-based methods on real video data in prior work datasets, have found relative success not because the tasks are easy but because of dataset bias issues. They further conduct a variety of experiments to benchmark state-of-the-art video understanding models and show how those models more or less struggle on temporal reasoning. It is a well-argued, thoughtful dataset contribution that sets up a reasonable video understanding dataset. - p9 phenomenon -> phenomena; the the videos -> the videos; these observation -> these observations; of next -> of the next; in real world -> in the real world This paper introduces a new synthetic video understanding dataset, borrowing many ideas from the visual question answering dataset CLEVR. Due to the inherent biases in available action recognition datasets, models that simply averages video frames do nearly as well as models that take temporal dependencies into account. My primary concern is to what extent can the new dataset (CATER) add to existing video datasets that are also explicitly designed for long term spatial-temporal reasoning, such as video VQA datasets TGIF-QA[1]/SVQA[2].
As these scores may not obey the rules of RNA folding, a second post-processing network is trained end-to-end together with the "Deep Score Network" to enforce constraints. This paper proposes E2Efold, which is an RNA secondary structure prediction algorithm based on an unrolled algorithm. The method is based on an unrolled algorithm, which is motivated by the inclusion of three inductive biases / constraints important underlying RNA folding. I advocate for acceptance. *Comments* The actual specification of the output constraints doesn't occur until late in the paper.
(approach - attribution methods) An information bottleneck is introduced by replacing a layer's (e.g., conv2) output X with a noisy version Z of that output. To the best of my knowledge, the proposed method is sufficiently novel and the application of the information bottleneck framework to pixel-level attribution has not been reported before. Large parts of the field of neural network compression are concerned with a similar kind of attribution - the question is which weights/neurons/filters are relevant and which ones are not and can thus be removed from the network without loss in accuracy. [1]: Adebayo, Julius et al. "Sanity Checks for Saliency Maps." NeurIPS (2018).
The authors utilize demonstrations collected beforehand and train an option learning framework offline by minimizing the expected number of terminations while encouraging diverse options by adding a regularization term. Moreover, the larger scale ATARI experiments do not seem to include any multi-task aspects at all, with options immediately being learnt on the target task. Please clarify. -"We can obtain the estimate for equation 1 by averaging over a set of near-optimal trajectories" The aim as states is to learn options that are capable of generating near-optimal trajectories (by using a small # of terminations). Please provide clarifications. In experiments: FR rooms experiments are interesting, in the visualization of the option policies, do the figures here show the flattened policy of the options? There are a number of option induction approaches that explicitly focus on reusability of options - see e.g. [1], [4]. In particular, the visualization in 4b showing options learned in Amidar does not show much improvement from what was observed before in Harb, 2018. I currently recommend rejection. The evaluation of the proposed method is rather weak and does not clearly demonstrate that the author's goals have been achieved.
The proposed techniques use both the graph structure, and the current classifier performance/accuracy into account while (actively) selecting the next node to be labeled. 1) The propose to sample nodes nodes based on "regional" uncertainty rather than node uncertainty 2) They use an variant of pagerank to determine nodes that are central, and hence most likely to affect subsequent classification in graph convolution classifiers. Second, it proposes to adapt the page rank algorithm (APR) to determine which nodes are far away from labeled nodes. To decide which nodes in a graph to label during the active labeling, the paper proposes two approaches. 2.2. Figure 1: according to the caption, APR should point to node 15, but in the figure it points to node 14. - Table 2: unclear: "accuracy without content" 1.2. "We have here shown that the accuracy of AL when uncertainty is computed regionally is much higher than when either local uncertainty or representative nodes are used", this is not the case on CiteSeer in Table 1 This is supported neither in Table 1 nor Table 2, where APR is frequently not highest performing There seem to be two main contributions in the paper.
By analogy, in this paper, the samples are taken from the entire dataset (i.e. the dataset is subsampled), while the validation loss is effectively an imitation loss formed by treating the black-box model predictions as a target. The contribution of this paper is thus the use of RL for meta-learning how to subsample a larger dataset in order to maximize some validation loss. This work is closely related to Ren et al. 2018 [1], which proposes to meta-learn how to weight samples in a batch so as to maximize performance on a validation set. I've given a weak accept, conditioned on being provided more evidence regarding 1) comparisons to simple differentiable alternatives, 2) sample efficiency of the RL method, and 3) basic analysis of the weighting function. * This is a minor issue, but this pushes the burden of interpretability further up to the black-box sample weighting function. * Other comments/requests: * While the use of RL is certainly motivated in order to solve the problem in an unbiased way, it would be nice to see a comparison to a differentiable approximation as a baseline? * Pros: * Considers an interesting dataset subsampling variant of the sample weighting meta-learning problem. * Though there is discussion of the complexity of the overall method it would be nice to see a discussion and figures related to the sample efficiency of REINFORCE? https://arxiv.org/abs/1803.09050
The authors find that (1) different proposed measures of selectivity are not consistent and (2) units identified as selective cannot be considered object detectors due to the high false alarm / low hit rates, analyzing a large number of selectivity measures. Previous works have used different measures of selectivity (with sometimes contradictory results), and the authors investigate the degree to which these units qualify as "object detectors". Overall, I think that the authors have presented a strong meta-analysis and compelling argument for further study in rigorously identifying the presence (or lack thereof) of selective units in neural networks and the degree to which they may be considered "object detectors. This work investigates the collection of methods that have been proposed to find units in neural networks that are selective for certain object classes. In the words of the paper, the "selective units are sensitive to some feature that is frequently, but not exclusively associated with the class" - I thought this is the standard majority view, not a surprising finding. According to that study, almost 60% 0f all fc8 units are "object detectors", with very high conherence between humans and selectivity metrics. It is also noticed that the existing metrics for selectivity do not adequately discriminate highly selective units in CNN. It is a laudable effort that someone took on that job. - is an indeed important problem for *CONF* community: Personally, I feel the "existence" of selective units in RNN could be interesting, but the "non-existence" in the case of CNN is not that surprising for some readers, as it seems much likely (at least to me): The final layer of CNN would be surely selective across classes, but it may be not the case for the hidden layers In my view, which I believe is the mainstream interpretation, a distributed representation does not contradict the presence of specialised units. Some categories probably are easily identified by few distinctive features, so there will be more detector-like units;
I noticed the comment of the authors that gives a correction for the introduction. al, 2016 in that the authors want to define a propagation filter for graph neural networks. The paper "Beyond Classical Diffusion: Ballistic Graph Neural Network" tackles the problem of graph vertices representation. From my point of view, without a full re-writting of the paper, this work cannot be published in a conference like *CONF*.
- I am not sure if *CONF* is the right venue for this work The paper presents three contributions: (a) the observation that there's train-to-test leakage in many graph classification datasets (under isomorphism equivalence), (b) what appears to be a theoretically motivated way of improving scores on such datasets, by focusing on solving the examples that are isomorphic with training instances, and (c) a recommendation to remove such leakage from test sets. The authors fairly discuss the problem in the introduction, with a good coverage of the related literature; the background theory is reasonably discussed, although is not very deep. In particular, the paper analyzes the amount of isomorphic graphs in 54 graph datasets and evaluates the performance of three graph classification methods under two isomorphism settings. Moreover, being able to capture the equivalence relation can be important for various graph learning tasks, e.g., to facilitate that two topologically equivalent graphs are be classified similarly.
PAPER SUMMARY: This paper proposes a fast inference method for Gaussian processes (GPs) that imposes a sparse decomposition on the VI approximation of the posterior GP (for computational efficiency) using the KNN set of each data point. This is, however, a somewhat strange direction which, to me, seems to raise extra issues that could have been avoided if one follows the conventional VI approximation: (1) As the posterior surrogate is now directly over f instead of f_I, the number of variational parameters is now proportional to the data size which requires several (redundant) extra approximations including armortized inference & the lower-bound on the entropy term that admits a sparse decomposition. While I understand that this is in exchange for the ability to encode local information (via KNN) within the surrogate posterior, it is not clear to me why do we need to incur all these computational issues to incorporate such local information. I also find the experiment lacking as comparison with fast approximation method such as [*] that incorporate local information is not included. (1) - (4) in my original review
By varying the homotopy parameter, one can construct a continuous path from a supposedly easier to solve optimization problem to the problem of interest. and (3) the assumption of fixing the homotopy parameter in the theorem on the non-convex case directly violates the intention of the algorithm. Decision and reasoning This paper should be rejected because
This paper proposes an adversarial detection method via Fourier coefficients. However, I find that the proposed MBF detection metric is much more complicated to calculate than any of its baselines, e.g., LID or K-density.
Based on the well-known universal approximation property of FNNs, the paper shows that their RNN-based filter can approximate arbitrarily well the optimal filter. Hence the paper's title "RNNs are universal filters" and the way it is presenting are confusing. It's well-known that doing particle filters is computationally expensive, especially in high dimensions, and the RNN-based filter might have millions of parameters! Overall the paper seems to be a straightforward application of universal approximation theorem of deep neural network. This paper shows that RNN (of infinite horizon) can be universal approximators  for any stochastic dynamics system.
The main contribution is the introduction of a differentiable top-K region proposal that allows to train the whole model with only a supervision of the total number of instances (and their class) in the image. Given my concerns on how this unsupervised approach can scale to real-life datasets, I suggest a weak reject, but I think the  proposed method has some interest for the community and I strongly encourage the authors to provide further evidence of performance of their method on more complex vision tasks. Unfortunately, their tasks seem quite easy, and it is hard to assess the impact of their method when working with more real-world data-sets, where the number of instances of every class is more loosely defined (we could always describe more objects in a real image from the COCO dataset for example).
2) Page 6 third line below Theorem 16.
I do see that the proposed method outperforms the one proposed in Shafahi et al in terms of universal adversarial training. 7 - What does "robust" adversarial attack mean?
Moreover，the authors also conducted the experiment to show that stronger robustness over adversarial examples can lead to zero concentration of margin. Contributions: 1. Derive a generalization bound on the performance of adversarially robust networks  that depends on the margin between training examples and the decision boundary. - The motivation for studying the margins between the training set and the decision boundary is not clear until Section 5.2.1 where it is mentioned that this is a widely used tool in learning theory. ===== Review ===== The problem that the paper addresses is very significant to the robust optimization field and the study of adversarial robustness in neural networks. This proved that strong robustness on adversarial examples might reduce the generalization. I an inclined to increase my rating and would suggest to weak accept this paper.
The paper proposes to maximize improve generalization in meta-learning by learning discrete codes via a mutual information maximization objective. Another reservation I have is the use of mutual information between encoder representations and class labels as a loss function (Eq. 1).
Self-interpretability is achieved by a two-stage model: First, a concept-extractor finds the related pieces of consecutive words (excerpts) in a given text that are related to a concept among a set of given concepts (if any), then the model makes its predictions solely based on the presence or absence of concepts (binary). One big issue with the experiments section is the A Posteriori Concept Acc metric as it only measures the separability and consistency of discovered concepts; It's very easy to assume the introduced training procedure to extract separable but meaningless concepts(i.e. the excerpts of a concept are separable from that of other concepts and are consistent with each other in the eyes of the network while they are not consistent with a concept in the eye of human rationale; Q: what kind of classifier was used for the evaluation metric "concept accuracy" classifier? Would the concepts learned without concept loss qualitatively very different? Authors should make a much more comprehensive discussion of what already exists in the concept-based interpretability literature and make the contribution of this work more clear (unsupervised concept extraction for a self-interpretable model instead of post-hoc interpretations). The model, Explaining model Decision through Unsupervised Concepts Extraction (EDUCE), is applied to a text classification task, while the authors argue in the appendix that this is also applicable to a wider problem, such as image classification.
For this generalization error, the authors give both bounds for a fixed generator and a uniform bound for a class of generators. ****************************** After author rebuttals: I have read the authors response and looked at the revision. (Edit on 11/14/19: I have read the response and checked the revision. I suggest authors to change multiple hyper-parameters and train more networks to improve the evaluation. However, since the generalization bound in Arora et al (2017) is based on a different definition of generalization error where empirical distributions are considered for both discriminators and generators, the comparison seems not fair. b)Theorem 2.3 is a general statement but it is followed by Corollary 3.3 which is a very specific generalization bound. 4) Generalization bound for fixed g: Unfortunately, the novelty of these generalization bounds are very limited as they are a direct application of known generalization bounds in the supervised settings.
In order to address this problem, the paper proposed a framework called LDMGAN which constrains the generator to align distribution of generated samples with that of real samples in latent space by introducing a regularized AutoEncoder that maps the data distribution to prior distribution in encoded space. The authors show 1 of mode captured on 2D Grid and 2D Ring using the VEEGAN method. My Take: This paper's only point of novelty over a vanilla VAE-GAN implementation is the inclusion of the KL(E(G(Z)) || p) term in the generator loss, which is very similar to the idea behind VEEGAN. I argue strongly in favor of rejection.
The output of the encoder is a style embedding that helps differentiates different modes of image synthesis. Thirdly, style encoder and generator are simultaneously finetuned. I think the idea of pre-training a style-based encoder is straightforward. My overall rating is borderline. When training the generator for image synthesis, the input combines an image in the source and a style embedding, and the loss is essentially the sum of image conditional GAN loss and perceptual loss.
The authors then compare their TVMax approach with softmax and Sparsemax attention for image captioning and show improvements on the MSCOCO and Flickr datasets. Compared with the softmax function, the sparsemax[1]  and the TVmax are able to sparse the visual attention very well. The main idea is to augment the Sparsemax projection loss with a Lasso like penalty which penalizes assigning different attention probabilities to contiguous regions in the image. [1]From Softmax to Sparsemax: A Sparse Model of Attention and Multi-Label Classification. Moreover, for generalization one can use attention dropout which is simpler instead.] This paper proposes two sparsifying methods of computing attention weights, dubbed sparsemax and TVmax, which appear to slightly improve objective and subjective image captioning scores. The conventional softmax approach to attention weights should be capable of producing attention weights near zero, which would be effectively sparse, especially if the pre-activations, z_i, in equation (1), are allowed to have a large enough range. Therefore, My decision leans to a weak accept. Could you please show some reasons about that?
How do you think? This paper proposes Surprise Minimizing RL (SMiRL), a conceptual framework for training a reinforcement learning agent to seek out states with high likelihood under a density model trained on visited states. However, the statement that SMiRL agents seek to visit states that will change the parametric state distribution to obtain higher intrinsic reward in the future is controversial (see e.g. at the end of Section 2.1), because optimizing a non-stationary signal is outside the scope of the problem formulation. The reinforcement learning problem of maximizing intrinsic rewards does not know how the intrinsic reward signal is altered in the course of the future, i.e. how the parametric state distribution is updated.
The paper proposes an approach to learning domain invariant representations using the adaptive decomposition of the convolutional filters. The methods appear to be a useful addition to tools available for domain invariant learning. Basically, it has been shown that invariant representations provably hurt generalization on the target domain when the marginal label distributions are different between the source and target domains. Also please use citations in the tables if you did not yourselves run experiments (as to make it clear that experimental protocols also might be slightly different etc). Weak Accept. * I think the paper was very well written, the explanations were clear and the technical contributions seem sound.
The experiments on the Atari games shows that by using tensor regression to replace the dense layer of the neural nets and using K-FAC for the optimization, one can reduce around 10 times of parameters without losing too much of performance. Some writing comments and potential writing errors (did not affect the decision): Page 3, first line of "Tensor regression layer", the shape of the tensor X seems to be a typo. Tensor regression layers and K-FAC are used as is without any modification while space savings and efficiency have been reported in corresponding references. The second method the authors have attempted is to swap the convolution layer of the deep RL architecture with wavelet scattering.
In response to such observations, the paper proposes Fixed Multihead Attention, where the constraint that `head_size * number_of_heads = embedding_size` in standard multihead attention is lifted; and it allows for using more attention heads without making each head smaller. This work discusses how to set the projection size for each head (head size) in multi-head attention module, especially Transformer. For example, several recent works argues for specialized attention heads, i.e., each head has specific "job," which may not require it being very expressive [1, 2]. I'm happy to revise the score if the authors can address my concerns.)
Summary. The paper improves the existing feature attribution method by adding regularizers to enforce (human) expectations about a model's behavior. The expected gradient method does indeed also performed better than the integrated gradient method in the benchmark (see Table 1.) The results in all three experiments are impressive. It is not clear if the paper is presenting "expected gradients" or existing attribution priors. It would be nice to see how integrated gradient method perform in the three experiments (image, drug data, mortality prediction), does the expected gradient method always outperform? Would be nicer to do for example "... as measured by R^2 (Figure 2 Left). Is it just smoothing? The authors proposed the attribution priors framework to incorporate human domain knowledge as constraints when training deep neural networks. I agree (and personally like) the motivation that a method is needed to align a model's behavior with human knowledge or intuition -- model's behavior may be explained by feature attribution methods while making models accept human knowledge is challenging. I am also not clear on where the image attribution prior comes from for the image task. In section 2.2. I think a few papers to have a look at are a survey article about graph based biasing http://www.nature.com/articles/s41698-017-0029-7 as well as methods for using graph convolutions with biases based on graphs: https://arxiv.org/abs/1711.05859 and https://arxiv.org/abs/1806.06975 . 2. When the authors refer to Figure 2 and Figure 3 multiple times in the main text, they are referring to either left or right panel. This is a general framework that the users can define different attribution priors for different tasks. For example, in this work, the authors proposed three reasonable priors for image input, graph data, and clinical medical data. I am concerned that only a limited set of expert-invented human priors can be used in this approach. Most of the experiments revolve around existing attribution prior methods. However, I am concerned about what information end-users are expected to obtain from the model.
3) The network is applied on unit cells of crystals or repeated unit cells from a dataset of crystal structures and the network is shown to be able to accurately reconstruct the 3D density maps, predict the number of atoms in the cell and perform fairly well in their classification into atomic types. The authors propose an auto-encoder framework for encoding the 3D locations of atoms in the crystal to a latent representation and then decoding that representation back into 3D structure. 2) A decoder network that first estimates a 3D density map from the latent vector using upsampling and convolutions and then classifies the atom type (atomic number) per voxel using a 3D segmentation. 2. VV-NET: Voxel VAE Net with Group Convolutions for Point Cloud Segmentation Also, it is not clear whether the 3D representation is better than 1D or 2D representations especially since there have been many new 1D models that perform very well for tasks like molecular property prediction (For example All SMILES VAE https://arxiv.org/abs/1905.13343 ). Also, there is no justification of the advantages of using a voxel-based density map representation along with 3D convolutions vs, for instance, a graph representation along with graph convolutions (Xie & Grossman, [3]) or a point cloud GAN (Achlioptas). I appreciate that the paper addresses an interesting problem that is not sufficiently explored and can motivate the development of novel methods that generate 3D molecules with particular structure and multiple types of atoms, however the current work combines existing methods, without any architectural modifications that exploit the new domain. 3. Joint VAE + UNet training: The joint training of the encoder-decoder VAE and the 3D segmentation network results in a decoder that can reconstruct atom locations and atom types, being robust to mistakes in the density map reconstruction. It would be more appropriate to submit this paper to a domain-specific venue rather than to *CONF*.
Review: This paper investigates the reason behind the vulnerability of BatchNorm and proposes a Robust Normalization. However, I have several concerns: *The authors verify that the running average is the main culprit of vulnerability to adversarial attack, but provide no further investigation of why this happens.
#Summary This paper proposes a generalised self-training framework to build a Graph Neural Network to label graphs. The authors do not change the GCN but extend the self-training portion as per the prior GCN paper by introducing Dynamic Self-Training that keeps a confidence score of labels predicted for unlabelled nodes. 4. If we had soft-labelling or uncertainty on which label each node has, how would the dynamic self-training be changed? Of importance is the dynamic nature of the self-training. 1. As the self-training is going on, are there different computational costs or are they about the same? You may include other additional sections here This paper propose to modify the existing work [1] of self-training framework for graph convolutional networks. The proposal is inspired from Graph convolution Networks with the idea of overcoming the major drawback of these models that lies of their behavior in case of limited coverage of the labeled nodes, which implies using deeper versions of the model leading at the price of what the authors call the over-smoothing problem.
Finally, they propose an actor-critic algorithm to solve the Wasserstein RMDP problem (Sec. 3) and evaluate its performance using simple experiments (Sec. 4). The Wasserstein RMDP is just a state-action-rectangular RMDP with convex ambiguity set, and Lemma 1 and Theorem 1 are known to be true for such RMDPs. I will keep my score, but do believe that further experiments and small adjustments to the writing will see a future version of this accepted.
(Though I am less positive due to the concern of novelty raised by other reviewers.) Summary: This paper presents an efficient stochastic neural network architecture by directly modeling activation uncertainty and adding a regularization term to encourage high activation variability by maximizing the entropy of stochastic neurons. The idea of producing distributions in each layer (i.e., using stochastic layers) is not new and is closely related to the work on local reparameterization trick and variational dropout [1] (predecessor of the cited sparse variational dropout), and various works that directly model neurons as distribution [2, 3].
In general, I like the idea of making recurrent cells operate with nearly independent transition dynamics and interact only sparingly through the attention bottleneck. It seems to me that LSTM was often the baseline of choice but RIMs have two important components: multiple LSTMs and an attention mechanism. I am wondering what if you do not select these top K activation and directly train it using the entire distribution of the soft attention output?
The tunability of the optimizer is a weighted sum of best performance at a given budget. as the main contribution. However, I do not think the metric they introduce is good enough to be recommended in future work, when comparing tunability of optimizers (or other algorithms with hyperparameters). A good prior of one optimizer could significantly affect the HPO cost or increase the tunability, i.e., the better understanding the optimizer, the less tuning cost. The assumption of independent hyperparameters might be fine for black box optimization or with the assumption that practitioners have no knowledge of the importance of each hyperparameter, then the tunability of the optimizer could be different based on the prior knowledge of hyperparameter and their correlations. In addition, the proposed stability metric seems not quite related with the above intuitions, as the illustrations (1.a and 1b) define the tunability to be the flatness of hyperparameter space around the best configurations, but the proposed definition is a weighted sum of the incumbents in terms of the HPO budgets. The motivation of defining tunability of optimizer is a very interesting question, however, the study seems to preliminary and the conclusion is not quite convencing due to several reasons: In section 3.2, to characterize its difficulties of finding best hyperparameters or tunability, the authors seem to try to connect the concept of "sharpness" of a minima in loss surface to the tunability of an optimizer, which is similar to comparing the loss landscape of minimums.
[Overview] In this paper, the authors proposed a shuffle strategy for convolution layers in convolutional neural networks (CNNs). One minor thing, in the main paper, the abbreviation for spatial shuffled convolution (ss convolution) is mentioned multiple times.
Caption of Table 1: (Moreno et al., 2015) > Moreno et al. (2015) When designing the annotation process, it does not seem like a good idea to use CommunityMPA over asking for a minimum of 40 annotations per annotator and use MPA. In addition, I'm not convinced with the idea of "breaking the larger player workloads into smaller batches" for simulating the sparsity and communities. Minor comment In Section 2.1: It was difficult to separate which part is the base model (MPA) and the novel proposal without reading Paun et al. (2018b). I see no major issues with accepting the paper. This study extends MPA to CommunityMPA by including information about the annotators' hierarchical community profiles from Simpson et al. (2011, 2013): spammers, adversarial, biased, average, and high-quality players.
The authors then show that the preconditioning matrix of RMSProp and Adam can be used as norm inducing matrices for second order trust region methods. One can show that the empirical Fisher is not an accurate curvature matrix in general, and so there is no reason to believe this would in fact enforce the proper ellipsoidal trust region for the method? My understanding is that the paper does not claim to deliver some great results here and now but instead suggest a promising direction ("that ellipsoidal constraints prove to be a very effective modification of the trust region method in the sense that they constantly outperform the spherical TR I am also a bit confused about why different batch sizes were used for the first order gradient methods and the second order TR methods? I decide to raise my rating to 3.
Their evaluation setup largely follows the style of Liu et al., but they construct a different subset of ILSVRC validation set, and some of the model architectures in their ensemble are different from Liu et al. Their results show that by including the distilled logits when computing the gradient, the generated adversarial examples can transfer better among different models using both single-model and ensemble-based attacks. In overall, I liked its novel motivation and simplicity of the method, but it seems to me the manuscript should be improved to meet the *CONF* standard. 3. There are a lot of recent attack methods proposed to improve the transferability of adversarial example, e.g., "Improving transferability of adversarial examples with input diversity" (Xie et al., 2019); "Evading defenses to transferable adversarial example by translation-invariant attacks" (Dong et al., 2019).
Given a set of expert demonstrations, this work provides a policy-dependent reward shaping objective that can utilize demonstration information and preserves policy optimality, policy improvement, The authors use a regularized reward function that minimizes the divergence between the policy of the expert and the one followed by the agent. It combines both an augmented reward for minimizing the KL between the policy and the expert actions as well as directly minimizing that KL in the policy. Finally, they only test on mujoco tasks which are very specific tasks with deterministic dynamics and very dense rewards  around states visited by the optimal strategy so initializing with an expert policy that is learned from demonstrations of a similar network of course helps. The end of the related work section is not very clear, you say these methods are problematic because "the adopted shaping reward yields no direct dependence on the current policy" but there's no explanation or motivation for why that would be a problem. Technical concerns: The stochasticity assumption of expert policy in Asm. 1 can be contradicted with that expert policy is optimal in policy invariance proof. Converting the tasks to sparse reward in this way makes them partially observable, and then potentially the expert demonstrations are required to overcome that partial observability. The revised version of the paper addresses many of my concerns about the motivation, related works, and comparisons with GAIL, so I'm updating my score to Weak Accept. They actually propose exactly the same framework as a special case in the appendix of that paper. Also, the proposed solution here is equivalent to regularizing the MDP with a KL divergence  w.r.t. to an initial policy that would be the one of the expert. First, The state of the art is missing important pre-deep-learning references such as: 1. Direct Policy Iteration with Demonstrations: Chemali and Lazaric Experiments: It would be more convincing to show the performance of behavior cloning policy using expert trajectories. Of course the authors could not know but I'd like to have their impression about how their work is different.
AdaX builds on the ideas of the Adam algorithm to address instability and non-convergence issues. In this paper, the authors propose a new adaptive gradient algorithm AdaX, which the authors claim results in better convergence and generalization properties compared to previous adaptive gradient methods. This needs a citation. Similarly, "AdaX outperforms various tasks of computer vision and natural language processing and can catch up with SGD"; as above, I'm unaware of work (other than theoretical) that shows that SGD significantly outperforms Adam in deep neural networks. I recommend the paper be rejected.
This paper proposes to provide a novel gradient-based meta-learning framework (Meta-Graph) for a few shot link prediction task. Moreover, it would be nice if the authors could also provide some ideas for future research directions, such as the prospects of using their approach for improving link prediction models and incorporating Meta-Graph in other domains like molecules structure. My concerns are as follows: •    I am wondering if you can adopt R-GCN [1] instead of the GCN model for extending the Meta-Graph to multi-relational graphs? •    I suggest considering ranking metrics such as MRR and HITS@ to further evaluate the performance of Meta-Graph.
In this way, an estimate of the output distribution is considered during the extraction of the representative subset of the testing data. The paper presents a new approach to create subsets of the testing examples that are representative of the entire test set so that the model can be tested quickly during the training and leaving the check on the full test set only at the end to validate its validity. The key idea is to create the smaller possible subset with the same or similar coverage (in the paper the neurons coverage is considered) and output distribution, maintaining the difference below a (small) threshold. More explanation is needed. 2. In Table 2, the authors try to compare the output distribution. To better demonstrate the change between raw testing set and proposed subset, I think that it could be better to present the metrics of distribution or the accuracy of each class instead. Furthermore, the result does not present a strong success: the error of output distribution is much worse than the compared work. Although the developed technique is quite simple, they are meaningful in practice. This work tries to build a sub-pile of the test data to save the testing time with minimum effect on the test adequacy and the output distribution.
They find that wider networks learn features in the hidden neurons that are more "interpretable" in this visualization framework. Also there is no guarantee that the quality of the feature visualization follows linear relationship according to width. However, I tend to reject this paper since it doesn't show very compelling evidence through experiments.
With a content-based "routing" technique ("group/cluster attention" may be a more accurate term to differentiate with the dynamics of Capsule Networks), the time steps of each layer form different clusters, and the self-attention mechanism is only performed within each such cluster. When combined with prior work on local attention (i.e. half of the heads are local attention, the other half are the newly proposed routing attention), but model is found to outperform, or be on par with, existing Transformer models despite generally being smaller. In particular, although the routing mechanism avoids computing A, having to deal with each of the n clusters means that one will need to process the cluster-based attentions sequentially (and, as mentioned, you have 8 heads for local attention and 8 for routing attention, which I think are also processed in two steps? - What is the *actual* running time/memory for local/routing/full attention layers? However, since the cluster centroids are learned parameters, does this mean the trained routing Transformer cannot easily generalize to different sequence lengths easily? 8. What is the motivation of using half of the heads for local attention and the other half for routing attention (cf. However, the cluster attention seems to group the words together, and there is no cross-cluster attention (i.e., the graph is broken into smaller components). Why not just use all of the heads with routing attention? How does the model do if it only uses routing attention? 11. Except the computational concern, I still don't quite exactly see the motivation behind clustering attention. The proposed approach is incremental by combining content-based sparse attention with local/temporal sparse attention. routing attention via the JSD score. The routing Transformer seems to be able to do very well on the WikiText-103 word-level language modeling task. ===================== While I'm impressed with the result the routing Transformer achieved on WikiText-103, I am not fully convinced the effectiveness of the approach (e.g., on the other tasks, the routing Transformer seems to be slightly worse than the SOTA transformer;
This paper proposes a very interesting idea of loss function optimization. 1) The experiments focus almost entirely on the Baikal loss (a particular loss function found once when running EC on mnist), and do not analyze the overall behavior of EC for loss functions. They find that applying their EC method to mnist yields an interesting loss function that they name the 'Baikal loss.' Much of the paper is devoted to analyzing the properties and performance of the Baikal loss. The authors present a framework to perform meta-learning on the loss used for training.
This paper presents a framework for evaluating offline reinforcement learning (RL) algorithms. The paper introduces a general framework for behavior regularized actor-critic methods, and empirically evaluates recent offline RL algorithms and different design choices. This paper proposes a unifying framework, BRAC, which summarizes the idea and evaluates the effectiveness of recently proposed offline reinforcement learning algorithms, specifically BEAR, BCQ, and KL control. Results from a  thorough series of experiments are presented which suggest that certain details of recently proposed RL methods are not necessary for achieving strong performance. I commend the authors for performing a valuable test and comparison of existing offline RL methodology. Overall, this paper could be an interesting summary of prior works in offline RL and provide some empirical insights on the effectiveness of each building block in the previous approaches, though it neither offers theoretical explanations nor proposes a new offline RL algorithm that outperforms the existing methods under the BRAC framework. A challenging open problem is a good thing! Are they negative so not reported in the figure or just missing? 3. Do you think the conclusion will change if you use training datasets of different size (e.g. much less than 1 million)? is an challenging open problem."  Unfortunately?! While the experimental results demonstrate some interesting phenomenons such as combining vp and the primal form of KL divergence achieving the best performance and taking minimum over Q functions outperforming using a mix of maximum and minimum over the Q functions, I believe the paper would be greatly improved if the authors can provide a new offline RL method based on the BRAC that can achieve better performance than current approaches and is less incremental than simply combining vp and KL divergence. There are some comments for the experiments. 2. Missing numbers: trained_alpha in dataset 0 of Hopper-v2 in Figure 1 and SAC in dataset 0 of Hopper-v2 in Figure 6? I am leaning to accept the paper because (1) the experimental design is rigorous and the results provide several insights into how to design a behavior regularized algorithm for offline RL. Minor issue:  In the Conclusion Section, the authors say, "Unfortunately, off-policy ... Does the BRAC framework reproduce the results for previous papers? If so, this should be made more clear and stated prominently in the paper so that the reader knows that BRAC is, in this reproduction of previous results sense, reliable. This paper could be improved by providing more clear insight and intuition about the deeper meaning of these results regarding the "unnecessary" technical complexities.
<Strengths> + This paper proposes a new dynamic model named hierarchical multi-task dynamical systems (MTDSs) as a latent sequence model that enables users to directly  control the output of data sequence via a latent code z. This paper proposes to add a latent variable to a dynamical, thereby encoding the notion of "task", eg, in Mocap, the latent variable could encode the walking style in an unsupervised manner. Furthermore, it is then possible to generate new series conditioning of chosen latent variables.
The paper first empirically demonstrates that the likelihood of out-of-distribution data has a larger difference between training mode and testing mode, then provides a possible theoretical explanation for the phenomenon. The paper makes the observation that likelihood models trained with batch norm assign much lower likelihoods to "training batches" of OoD data (batch norm statistics computed over over minibatch) than evaluation batches of OoD data (batch norm statistics over entire training set). To do this they assume they are given batches of OOD examples or batches of in-distribution examples, and they detect whether the batch is in- or out-of-distribution. This is consistent with your batch normalization experiments: in training mode, the likelihood is computed from mean activations over a batch of OoD samples, several of which probably contribute to the low likelihoods. We recommend a weak accept. If you take a likelihood model and evaluate on 64 samples from SVHN, you are all but guaranteed to sample a sample with *exceedingly* low likelihood, which dominates the mean statistic, making it possible to separate SVHN batch from CIFAR10 batches.
This paper presents a new reversible flow-based graph generative model wherein the whole graph i.e., representative attributes such as node features and adjacency tensor is modeled using seperate streams of invertible flow model. [Page 7, Table 2] My first concern is: does the reconstruction performance matters in the graph generation case? I shall improve my rating if GraphNVP is applied to general graph structures - synthetic / real.
In the M-product notations, everything seems to be as neat as matrix operations. Pros: 1, The generalization brought by M-product seems to be general as it includes quite a few graph convolution elements for 3D tensors in a natural way. The technique is a trivial generalization from matrix results. Given the current status, I could not accept the paper. However, if in practice, you do not need the inverse transform, then do those theoretical properties still hold and what is the meaning of introducing such M-product formulation?
Authors define the minimum reconstruction error from the embedding as the global measure in reflecting the global structure of the data similar to PCA. Second, the definition of ``global score'' depends on another baseline method (i.e. PCA), which seems odd. Maybe what helps is to first define ``global'' in a dimensionality reduction context.
The notion of feature robustness, which is a notion the paper proposes, connects the flatness measure to generalization error. **************************** After author rebuttals: Author have added discussion of related work which was missing in the original submission (thanks!). This seems to be a missing step in relating flatness/feature robustness of a layer to the generalization of the whole network. Otherwise, it is hard to claim that this paper connected the modified flatness measure to generalization error. al. Moreover, the experiments are very limited and I suggest authors to look at more controlled setting to verify the relationship of their measure to generalization. On the theoretical side, I think the major issue is that the paper cannot connect the measure to generalization properly and ends up decomposing the test error to the sum of the robustness measure and the gap between test error and the robustness measure which is not informative. They did address some of my concerns. However, the other two issues are still present.
This is a paper about a very interesting topic, involving both learning (in a supervised way) to induce a causal graph and taking advantage of it in a goal-conditioned policy. I was a bit disappointed to see that training is mostly supervised (both providing the ground truth causal graph and an oracle policy as target) but on the other hand it is impressive to obtain these results with raw images as input and the comparative results are good. I feel this paper makes strong assumptions on both the induction and inference stages, as well as the structure of the causal graph, which greatly limits the applicability of the approach. - train an policy \\pi_G attending over the causal graph to solving tasks. * On page 1, the authors claim that empirical evidence suggests the lack of correct causal modeling is an important factor for lack of generalization, generation of unrealistic captions, and difficulties in transfer learning. My rating is weak reject but I am ready to upgrade with appropriate explanations answering the above questions.
In particular, the proposed GraphMix model has two components, one GNN and one MLP. I would be curious to see the t-SNE visualization of the GraphMix(GCN) \\ Mixup features in order to determine how much of the cluster separation is due to Mixup specifically. I would give the paper a higher score if the authors showed that GraphMix(Graph U-net) was an improvement over Graph U-net, or if it was made more clear that some substantial benefit is derived from using Mixup features.
This work addresses the important problem of generation bias and a lack of diversity in generative models, which is often called model collapse. Unlike most existing works that address the model collapse problem, a blackbox approach does not make assumptions about having access to model weights or the artifacts produced during model training, making it more widely applicable than the white-box approaches. It further showed that the proposed blackbox approaches increases the proposed diversity metric without sacrificing image quality. This paper presents a set of statistical tools, that are applicable to quantitatively measuring the mode collapse of GANs. The authors consistently observe strong mode collapse on several state-of-the-art GANs using the proposed toolset. 2. The statement "IS, FID and MODE score takes both visual fidelity and diversity into account." under "Evaluation of Mode Collapse" is contradictory to the description in sec 2.1 that IS in fact does not measure diversity. The problem is that this measure wont detect mode dropping if there's aren't samples from those modes to measure anything against. For those reasons, I propose to REJECT this paper.
On the sequence generation task (translation), the authors showed that the proposed KA strategy achieved better performance compared with KD based methods when distilling the knowledge from a teacher model to a student model. When only the top few tokens are used to transfer the knowledge from teacher model to student model, KA focus on the precision of a small subspace, which tends to have few modes. 2. The authors presented a thorough analysis on the proposed KA strategy and compared it with KL strategy in terms of the precision and recall for the student models.
This paper proposes a novel model of recurrent unit for RNNs which is inspired from tensor product representation (TPR) introduced by Smolensky et al. in 1990. * Questions / Comments * It looks, however, improper because there is no baseline and we cannot conclude that TPRU has better interpretability than others. 2-2. The paper says one of the advantages of using TPRU is in its interpretability. The advantage in terms of accuracy of the proposed approach seems marginal in the experiment, and the analysis of the interpretability of the learned representations could be improved: loosely speaking, particular examples of interpretability are given but sometimes without contexts or baselines to compare to (see the two last comments below). I think this paper is not yet ready for publication: the proposed model is interesting and relevant but its validity could be better assessed and the paper needs some thorough proof-reading.
To this end, the author(s) have proposed to combine both the primal and dual formulation of Wasserstein distance. The paper proposes a new way of stabilizing Wasserstein GANs by using Sinkhorn distance to upper-bound the objective of WGAN's critic's loss during the training. Page 3, line 42: Si(n)khorn distance Because of these I would judge the contribution of this paper not strong, so that I would recommend weak acceptance.
Both the pre-training model and the mixture model are combining the specific domain knowledge to improve the generalization and diversity of retrosynthesis. The main contributions are data augmentation techniques, pre-training and a mixture model that seems to improve performance on the USPTO-50K dataset. The authors can consider using diverse datasets or applying their techniques to another application domain to bolster their claims.
The idea of the paper is good and Algorithm 1 sets out to learn the exploration policy when the expert policies do not agree.
Since the space of input-output for the adversarial attach is huge, the paper proposes to use Bayesian optimization (BO) to sequentially select an attack. 2018. This paper applies Bayesian optimisation (BO), a sample efficient global optimisation technique, to the problem of finding adversarial perturbation. The authors in [1] consider using Bayesian optimization to make adversarial attack for model testing. ############ Post-feedback ########## Thanks for the clarification and the additional experiments. This may give a clearer picture on how BO works in the attack generation setting.
The researchers propose a benchmark called QUARL that allows them to evaluate the effectiveness of quantization as well as the impact of quantization across a set of established DRL algorithms (e.g., DQN, DDPG, PPO) and environments (e.g., OpenAI Gym, ALE). In a deep reinforcement learning algorithm, when you apply post-training quantization in a deep reinforcement learning algorithm, mainly when that algorithm uses a value function (e.g., A2C or DQN), the problem is reduced to a regression problem. The results are also not entirely surprising or impactful: how is quantization impacting reinforcement learning in a different way than supervised learning? It also shows how this quantization leads to a reduced memory cost and faster training and inference times. - I'd introduce/explain quantization in the beginning of the second paragraph of the Introduction for those not familiar with the term.
al., NeurIPS 18 This paper introduces a model that learns a slot-based representation, along with a transition model to predict the evolution of these representations in a sparse fashion, all in a fully unsupervised way. would also similarly generalize. 3b) Sec 4.2 argues that this slot-based representation can help exploration, but this is in fact a chicken-and-egg problem, as one needs to have collected interesting transition samples for a good representation to emerge. Specifically: 1) It is unclear what this paper is claiming to improve over prior work: is the goal to a) learn a good forward model, or b) show that emergent entities allow better downstream tasks. The experiments in Sec 4.2 and the visual results in Fig 3 do clearly highlight the benefits of joint learning, and show that the emergent representations are more meaningful compared to learning representations independently. As of now, my recommendation is a clear reject.
For entity typing, RELIC embeddings of entities are used as input for a 2-layer FF network, which then outputs which types belong to the entity. - It seems to be on par with or slightly worse than Yamada et al 2017 on entity linking.
By designing actdiff loss and reconstruction loss, the authors demonstrate that classifiers are likely to predict using features unrelated to the task and their losses can mitigate this problem. The authors then proposed using Actdiff loss, Reconstruction loss, and Gradmask loss that are designed to suppress the effect of irrelevant features. Actdiff is compared to 5 other methods including a reconstruction loss and Gradmask (previous work). The conclusion is that adding mask information using Actdiff doesn't improve segmentation performance. [1]: Gatys, Leon A. et al. "A Neural Algorithm of Artistic Style." ArXiv abs/1508.06576 (2015): n. The actdiff loss makes sense and masking + activation mapping have not been tried together before to my knowledge. * (Sec6) "Both actdiff and gradmask improve performance of the model, but only actdiff scores above chance, and performs similarly to a model trained with the areas outside of the mask completely removed." (evaluation - Medical Segmentation Decathalon) Suggestions --- * This paper would be a bit more convincing if it started with a concrete example of the problem illustrated on some dataset (e.g., maybe an example from Gradmask). A detailed description of the non-standard architectures would be useful for reproducibility, though probably only in the appendix.
The paper presents an unsupervised approach for learning landmarks in images or videos with single objects by separating the representation of the image into foreground and background and factorizing the representation of the foreground into pose and appearance. It extends this approach by introducing an additional separation of foreground and background in the image. Strengths: + Nicely motivates the approach of separating foreground and background The paper presents an unsupervised method to get disentanglement of pose, appearance, background from both images domain and video domain. 5 sub-network are used to model pose, appearance, foreground, background, and decoders.
- Finally, I suggest revising the very vague title to the paper This paper tries to analyze the similarities and transferring abilities of learned visual representations for embodied navigation tasks. - I think the experiments can not support the argument that residual connections help networks learn more similar representations. ========================================================= After Rebuttal: I thank the author for the response.
In this work, the authors develop the discriminative jackknife (DJ), which is a novel way to compute estimates of predictive uncertainty. This is an important open question in machine learning and the authors have made a substantial contribution towards answering the question of "can you trust a model?" DJ constructs frequentist confidence intervals via a posthoc procedure. The authors propose using influence functions to efficiently estimate pointwise confidence intervals for regression models. This paper studies how to construct confidence intervals for deep neural networks with guaranteed coverage. Next, they explain and then develop the concept of higher order influence functions. They develop an exact construction of the DJ confidence intervals in Section 3.1. (ii) The claims of guaranteed frequentist coverage are not backed up as, according to thm.2, they only hold when n >> 0 and the number of influence functions used goes to infinity (ideally, the authors would provide non-asymptotic bounds as in [1], but at the very least, these limitations should have been clearly pointed out and their practical implications discussed). - Can you please clarify if and how the use of the algorithm from (Agarwal et al., 2016) for approximation of the Hessian products affects accuracy of your confidence intervals? I suggest that this paper is weak accepted. This could be a chance to really sell your method: if it does well enough compared to more expensive LOO jackknife procedures, that would be a compelling reason to choose DJ.
Note that back translation only needs monolingual data, while the pretraining in this work needs bilingual sentence pairs. This work conducts a large scale study on pretraining for neural machine translation. What would be the performance of an ensemble of 10 models trained with the regular 20M parallel sentence pairs? Pros: 1. The data scale is huge, with 40 billion sentence pairs. 2. I'm curious how large-scale pretraining compare with large-scale back translation. Thus, it is not clear to me whether the improvement is general across datasets and language pairs.
arXiv 1801.02929 This paper proposes a novel data augmentation method, untied MixUp (UMixUp), which is a general case of both MixUp and Directional Adversarial Traning (DAT). This paper introduces directional adversarial training (DAT) and UMixUP, which are extension methods of MixUp. DAT and UMixUp use the same method of MixUp for generating samples but use different label mixing ratios where DAT retains the sample's original label. This paper shows that UMixUp and DAT are equivalent when the number of samples tends to infinity. Since UMixUp is also focusing on the mixing ratio between two training samples, Between-Class Learning should have been compared to the proposed method.
This paper proposes an extension of the conditional GAN objective, where the generator conditions on an attention map produced by the discriminator in addition to the input image. pg. 4: "like random noisy" -> "like random noise" pg. 5 - "Attention mask can potentially break good property of the raw input." - what does this mean? By conditioning on the attention map, the generator could leverage information about the regions in the image that the discriminator attends to and use it to generate a new image that better fools the discriminator.
The manuscript discusses a metric-learning formulation as a quotient for reconstruction-error terms, how to optimize the quotient based on results from Wang et 2014, an iterated reweighted approach to circumvent the non-smooth part of the l1 loss in zero, and experiments on brain images of Alzeimer's disease. If the same theorem has already been proved by previous work [Wang et al., 2014], it is not necessary to prove it again in this paper. Concerning the iterated reweighted approach, I believe that this is non smooth only for g(x)=0, which is not covered by the theorems of Wang 2014.
The paper proposes to use codes based on multiple hashing functions to reduce the input and output dimensions of transformer networks. The method differentiates itself, in large part, by formulating model outputs in terms of m distinct softmax layers, each corresponding to a hash of output space Y. Such codes provide more freedom in terms of where to put model capacity (larger embeddings, more transformer layers, etc.) which may be useful in applications where most of the model parameters are in embedding matrices. The authors borrow from Bloom filter the way to create the codes using random hash functions, but the analogy stops here. For instance, if there are m hash functions taking values in {1, ..., P}, an approach based on Bloom filters would predict a binary output of dimension P, while here there are m multiclass problems with P classes (IIUC). This makes the experiments in the paper look more like "proofs of concept", and they are less convincing than they should be. where 1[.] is the indicator function" -> what is the "indicator function" of a number?) This work presents Superbloom, which applies the bloom filter to the Transformer learning to deal with large opaque ids.
Summary: This paper proposes to use a combination of a pretrained GAN and an untrained deep decoder as the image prior for image restoration problem. Authors devote themselves to remove the representation error of the GAN image prior. In Alg.1, the detailed algorithmic process is presented, it is clear that authors need to pre-train the used GAN and Deep Decoder, then combine them to train one network. [Original reviews] This paper proposed to modeling image as the combination of a GAN with a Deep Decoder, to remove the representation error of a GAN when used as a prior in inverse problems. Therefore, I recommend weak reject.
called SMOE scale. The result of this operator can be combined through different scales to obtain a global saliency map, or visualized in such a way that shows the consistency of saliency maps at different scales. Edit: The authors have answered most of my concerns and I am happy to re-evaluate my score to weak accept. Also, superimposing these HSV maps over gray scale version of the image like is Fig.2 is difficult to analyze because the "gray" of the image can be confused with the saturation channel. The paper gives two main contributions: SMOE, which captures the informativeness of the corresponding layers of the scale block and LOVI, a heatmap based on HSV, giving information of the region of interest according to the corresponding scale blocks. Overall, in my opinion, being efficient at generating saliency maps is a nice to have, but not much more. - 3.2 The authors refer to Smoothgrad squared method, it is indeed a good process to refine saliency maps, however why it is used could be detailed, just as the parameters chosen for its implementation.
A very related work to this paper: L_{DMI}: A Novel Information-theoretic Loss Function for Training Deep Nets Robust to Label Noise, where no restrictions have been made on the class-dependent transition matrix and the proposed method does not need to estimate the transition matrix. There are 4 major issues I have found so far. Many loss correction methods can estimate the transition matrix T (which is indispensable in any loss correction) without knowing the noise rate, when there are anchor points or even no anchor points in the noisy training data. Though I'm not that familiar with learning from noisy labels, I think it is a good paper and I suggest to accept. See also the public comment posted by Nontawat when a special symmetric condition is assumed on the surrogate loss function.
NAACL 2018. - Tang et al. Question Answering and Question Generation as Dual Tasks. This paper proposes a pretraining technique for question generation, where an answer candidate is chosen beforehand, and the objective is to predict the answer containing sentence given a paragraph excluding this sentence and the target answer candidate. (By the way, I am giving 3 as a rating although my actual rating is closer to 4 or 5, because 4 or 5 is blocked in the review system. I am happy to increase the score after rebuttals.) -------------------------------------------------------------------------------------------------------------------------------------------- Now, here are clarification questions. Regarding Section 2.1 1) Did you attempt to predict 'number of answer candidates' by regression or classification? **** Update on Nov 10 **** Increasing the score to 6.
It does this by first pretraining the agent to perform interventions in the environment which change the states of the objects of interest, and then finetuning the agent to actually make a decision about whether the given hypothesis is correct. The authors present a framework for testing a set of structured hypotheses about environment dynamics by learning an exploratory policy using Reinforcement Learning and a evaluator through supervised learning. The problem is interesting, and it certainly provides a great venue for interesting and impactful research in RL, language-conditioned decision making, structured / symbolic learning, and so on. Reward the agent for observing a state of the world it has not seen before (i.e. count-based exploration) So, if the agent is trained on some hypotheses, the agent will essentially learn to identify for each h which state s that can be used to to verify h (either prove or disprove it). Second, when the agent exploits the structure of the hypotheses, the problem becomes nearly trivial. I was though encouraged to see that one of the environment seemed to require slightly different setting in the pre-training reward setup, however the authors didn't follow up with some analysis on why there was such a difference. In particular, I still feel the paper lacks sufficient discussion of the literature on causal reasoning. I also do not think it is sufficient to add an appendix with the results across multiple seeds: these results should be in the main paper. I thus am giving a score of "weak reject", though it is possible I could increase my score is some of my concerns can be addressed. Reward the agent for changing the state of any object referenced in the hypothesis
Motivated by the observation that powerful deep autoregressive models such as PixelCNNs lack the ability to produce semantically meaningful latent embeddings and generate visually appealing interpolated images by latent representation manipulations, this paper proposes using Fisher scores projected to a reasonably low-dimensional space as latent embeddings for image manipulations. When the α is small, the learned decoder will function similarly to the original pixelCNN, therefore, latent activations produce smaller FID scores than projected Fisher scores for small α's. It is very obvious that either a CNN decoder, a conditional RealNVP decoder, or a conditional Pyramid PixelCNN decoder conditioned on projected Fisher scores will produce better images because the Fisher scores simply contain much more information about the images than the latent activations. This separate powerful decoder has nothing to do with PixelCNN, which is the major reason that I vote reject.
The proposed model, CGT, has an encoder-decoder structure, and is characterized by clustering modules for spacial regions based on their temporal patterns. Eq. (4))? (The code seems to suggest it's concatenation?)
The paper proposes an improved extension of the Wasserstein auto-encoder for anomaly detection. This work proposes an outlier detection method based on WAE framework. 2.I agree with authors point that WAE is a better choice than VAE for outlier detection because, former "encourages the latent representations as a whole to match the prior ". Eg: I. Chong You, Rene Vidal, Provable Self-Representation Based Outlier Detection in a Union of Subspaces, CVPR 17 Post Rebuttal: Authors have partially addressed my concerns. Overall, I am hesitant to recommend the paper before cross-checking the issue with contamination proportion and learning more about how a VAE framework is indeed important for anomaly detection.
Several metrics are proposed, including correctness, consistency and confidence. ------------------------- BEFORE rebuttal The paper proposed different metrics for comparing explainers based on their correctness (ability to find most relevant features in an input, used in prediction), consistency (ability to capture the relevant components while input is transformed), and the confidence of the generated explanations. [2]: Zeiler, Matthew D. and Rob Fergus. This paper proposes 3 such explanation evaluation metrics, correctness, consistency, and confidence.
Standard behavioral cloning loss, and a style consistency loss that encourages labels of the generated trajectory to match labels of the target style. We assume the desired style is defined by some combination of labels, and that we know this combination (i.e. a fast trajectory to the basket should have the "speed above threshold c" label and "final location close to basket" label, which we have labeling functions for.) Same for Table 10, if the NLL column is truly actually log-likelihood, then the "style+" objective really degrades imitation quality rather than improves it. For Cheetah, it would be easy to report p(y) as a function of the target forward speed, that would give readers a sense of diversity for each label.
The submission proposes to reduce the memory bandwidth (and energy consumption) in CNNs by applying PCA transforms on feature vectors at all spatial locations followed by uniform quantization and variable-length coding. A lossy transform coding approach was proposed to reduce the memory bandwidth of edge devices deploying CNNs. For this purpose, the proposed method compresses highly correlated feature maps using variable length coding. Will the computation of PCA require a lot of on-device memory? However, the paper and the work should be improved for a clear acceptance: - Some parts of the method need to be explained more clearly: – In the statement "Due to the choice of 1 × 1 × C blocks, the PCA transform essentially becomes a 1 × 1 tensor convolution kernel", what do you mean by "the PCA transform becomes a convolution kernel."?
Summary: This paper investigates the choice of noise distributions for smoothing an arbitrary classifier for defending against adversarial attacks. From this framework, a trade-off between accuracy and robustness is identified and new distributions are proposed to obtain a better trade-off than with Gaussian noise. Also, the table of Cohen et al. was only calculated for specific values of \\sigma for Gaussian distributions (0.12, 0.25, 0.5, 1.00). Using this, it considers different adversarial smoothing distributions that yield some increase in certified adversarial accuracy. #3 (sketchy justification): The paper justifies a smoothing distribution that concentrates more mass around the center as follows: "This phenomenon makes it problematic to use standard Gaussian distribution for adversarial certification, because one would expect that the smoothing distribution should concentrate around the center (the original image) in order to make the smoothed classifier close to the original classifier (and hence accurate)." As the Gaussian distribution with standard deviation \\sigma' is supported on a shell of radius about \\sqrt{d} \\sigma', for each (k,\\sigma) in the family of Eq. 8, there is an equivalent Gaussian with appropriate \\sigma' (Theorem 3 now just compares the radius of the spherical shell). 3. On p.5, why was the toy classifier sphere-based? I think the paper should be rejected because (1) For \\ell_2 perturbations, there is no major difference between this new family of distributions (d-k \\chi^2) and a Gaussian with different variance.
General: The paper proposed to use a causal fairness metric, then tries to identify the Pareto optimal front for the vectorized output, [accuracy, fairness]. The fairness component relies on a  definition of fairness based on causal inference, relying on the idea that a sensitive attribute should not causally affect model predictions. * A new causal fairness objective based on the existing Weighted Average Treatment Effect (WATE) and Average Treatment Effect for the Overlap Population (ATO) Con & Question: The so-called causal fairness metric does not seem to be any more fundametal than the other proposed metrics. However, there may be several problems with this approach: 1) For a causal estimate to be valid we need several assumptions. 3) Why do we want U to be small, i.e., why do we want the causal effect of A to be small, is never justified. For these reasons, I give it a Weak Accept.
In particular, the regularizer considers explanations during the model training; if the explanations are not consistent with some prior knowledge, then explanation errors will be introduced.
If the original space is a structured space like Euclidean space, then effectively this paper's method coincides with regular distance preserving method in dimension reduction, and Johnson-Lindenstrauss theories. ###### Overall Recommendation I vote for the "Weak Accept" decision for this paper. ### Minor Edit Suggestions 1. It would be better to give more descriptions about Figure 1; the lower right part in Figure 1 is not explained in the caption; the shadow part in Figure 1 is not precise.
Conventionally, the sensor placement strategy is tasked to gather the most informative observations (given a limited sensing budget) for maximimally improving the model(s) of choice (in the context of this paper, the neural networks) so as to maximize the information gain. This paper describes a sensor placement strategy based on information gain on an unknown quantity of interest, which already exists in the active learning literature. What the authors have done differently is to consider the use of neural nets (as opposed to the widely-used Gaussian process) as the learning models in this sensor placement problem, specifically to (a) approximate the expectation using a set of samples generated from a generator neural net and to (b) estimate the probability term in the entropy by a deterministic/inspector neural net. Do they adopt an open-loop sensor placement strategy? Furthermore, their proposed strategy has been used to gather only 1 to 4 observations. Random sampling and GP-based sensor placement strategies do not face such a severe practical limitation.
The paper presents a visually-guided interpretation of activations of the convolution layers in the generator of StyleGAN on four semantic abstractions (Layout, Scene Category, Scene Attributes and Color), which are referred to as the "Variation Factors" and validates/corroborates these interpretations quantitatively using a re-scoring function. The paper proposes an approach to analyze the latent space learned by recent GAN approaches into semantically meaningful directions of variation, thus allowing for interpretable manipulation of latent space vectors and subsequent generated images. The claim here is that with the same perturbation of the resulting codes (lambda=2) at the output of different GAN layers, the change in the visualized output demonstrates what kind of, if any, semantic is being captured by different layers of GAN. Some of my primary concerns were regarding the presentation, and I feel they have been mostly addressed with the changes to the introduction and abstract (I'd still recommend using 'layerwise latent code' instead of 'layerwise representation' everywhere in the text). Are the four semantic abstractions decided based on the desired output? The paper analyzes the relation of various scene properties w.r.t the latent variables across layers, and does convincingly show that aspects like layout, category, attribute etc, are related to different layers. leads the reader to believe that the findings here are generally applicable e.g. the sentence "the generative representations learned by GAN are specialized to synthesize different hierarchical semantics" should actually be something like "the per-layer latent variables for StyleGAN affect different levels of scene semantics". The results showing scene property manipulation e.g. in Fig 4 are obtained by varying a certain y_l, and it'd help to also show the results if the initial latent code w was modified directly (therefore affecting all layers!). With the separation boundary (in the form of a normal vector) known for each of the four scene semantics, different feature activations are obtained by moving the latent code towards/away from the separation boundary. 2. The visual results depicting manipulation of specific properties of scenes by changing specific variables in the latent space, and the ones in Sec 3.2 studying transitions across scene types, are also impressive and interesting. In the next step, the authors sample a latent code from the learned distribution and pass it through every layer of the GAN generator. Finally, I agree that given the popularity of StyleGAN like models, the investigation methodology proposed, and the insights presented might be useful to a broad audience. On the other hand, the insight gained is fairly superficial, boiling down to the statement that the learned latent code has structure that corresponds to semantically meaningful axes of variation, and that such structure is localized to particular levels of the layer hierarchy for particular semantic axes. Despite these positives, I am not sure about accepting the paper because I feel the investigation methods and the results are both very specific to a particular sort of GAN, and the writing (introduction, abstract, related work etc.)
- Summary: This paper proposes to improve confident-classifiers for OOD detection by introducing an explicit "reject" class. Overall the paper is well-written and well-organized. The proposed method is based on the idea from theoretical analysis, and is reasonable and valid. However, I like the method and could be accepted as an *CONF* paper. - Comments: 1. In section 4, the authors conjectured the reason why the performance of reject class in Lee et al. (2018a) was worse is that the generated OOD samples do not follow the in-distribution boundaries well. ==== [Summary] To detect out-of-distribution (OOD) samples, the authors proposed to add an explicit "reject" class instead of producing a uniform distribution and OOD sample generation method.
G. (2007). Building Portable Options: Skill Transfer in Reinforcement Learning. The hierarchical prior policy is shared amongst all tasks, while the hierarchical posterior policies are allowed to adapt to specific tasks. In directional Taxi (2c) Distral(+action) manages to reach the same final performance (if we care about that), can you please comment on this. The results in moving bandits alone are very convincing. There seems to be no other term that incentivizes the option posterior to deviate, and I do not see how the options are adapting to tasks. Detailed Comments: A primary weakness of this approach is that it seems like there is one network that learns the options and is shared across all task (that would be the prior) and then there is a task-specific network for all options (posterior), wouldn't this be very difficult to scale if we want to learn reusable options over the lifetime of an agent? I liked the flow and the organization of this paper. Term 2 controls how the option posterior deviates from the prior. The authors assume that all options are present everywhere i.e. I ⊆ S. Summary: The authors propose a method for learning hierarchical policies in a multi-task setting built on options and casts the concepts into the Planning as Inference framework. [1] Mann, T. A., Mannor, S., & Precup, D.
For each bert embedded token, the proposed method aims at disentangling semantic information of the word from its structural role. This paper proposes a layer on top of BERT which is motivated by a desire to disentangle content (meaning of the tokens) and form (structural roles of the tokens). One has to wait for the figure. The empirical gains in transfer learning can be simply attributed to: - More params it seems adding an LSTM over bert embeddings already does some improvement, I would have loved to see this more exploited but it wasn't. Regarding the performance, it seems HUBERT is gaining very little over the BERT baseline. I understand the authors are just trying to make a point that BERT does worse than their model in this case and that this is not good for transfer, but still I find this to be artificially constructed.
Experiments on LeNet + MNIST show (a) different methods can achieve similar accuracy, (b) pruned sub-networks may differ significantly despite identical initialization, (c) weight reinitialization between pruning iterations yields more structured convolutional layer pruning than not reinitializing, and (d) pruning methods may differ in the stability of weights over pruning iterations. There are no guidelines how to utilize the observations in future research (e.g., how they can be used for verifying the lottery ticket hypothesis or how they affect to existing pruning techniques) while some observations might be trivial or not very interesting (e.g., contribution 1 and contribution 2) for me. (1) *Overlap in pruned sub-networks*: In the middle of Sec. 4, Fig 3-5 examine the similarity of pruning masks between methods. (2) *Weight stability during pruning*: It is difficult to discern a conclusion in Sec 5. In particular, the authors tested several different pruning techniques by varying evaluation criteria (L_1, L_2, L_-\\infty and random) and pruning structures (structured, unstructured and hybrid). First, a clarification on the figures: are lines for pruned weights terminated where they are pruned?
In addition to that, it analyzed a mixture of linear and non-linear activation functions, and show that mixture is better than single nonlinearity in terms of expected training error for ridge regression estimators. This paper analyzed the asymptotic training error of a simple regression model trained on the random features for a noisy autoencoding task and proved that a mixture of nonlinearities can outperform the best single nonlinearity on such tasks. I guess there does not appear such a trade-off for the test error and the linear activation function would be always better because the true function is the linear model. There should be more discussion about why this kind of trivial argument cannot be applied in the analysis.
The main idea of this paper is to solve multi-agent reinforcement learning problem in dominance solvable games. I did not check the proofs thoroughly. The applications of the convergence result result to "noisy effort" games is pretty standard and the results expected based on the theory. What I want to know is: what sorts of games can we compute the iterated dominance reward schemes for? The fact that standard MARL learning rules (e.g. independent Q learning) converge in games with iterated dominance solutions is a very well-known result in Learning in Games (see [1], [2]). I'll wait to hear the author response to this). To the current status of the paper, I have a few concerns below. [1] Michael Bowling, "Convergence Problems of General-Sum Multiagent Reinforcement Learning", Sec. 5.2 The interesting aspect of this paper is that iterated dominance solution based reward scheme can guarantee convergence to the desired agents policies at a cheaper cost in practical principal-agent problems.
The authors also demonstrate the use of their technique in Image to Image translation. Overall, although the paper explain clearly the intuition and the motivation of the proposed technique, I think that the paper in its present state have low novelty, weak related work analysis review and insufficient experiments to support a publication at *CONF*.
This paper is motivated by the unstable performance of Transformer in reinforcement learning, and tried several variants of Transformer to see whether some of them can stabilize the Transformer. For another example, why replacing the residual connection with the gating layer can make the Transformer more stable?
The authors conduct extensive experiments on image classification and segmentation and show that dynamic convolutional kernels with reduced number of channels lead to significant reduction in FLOPS and increase in inference speed (for batch size 1) compared to their static counterparts with higher number of channels. === Summary === The authors propose to use dynamic convolutional kernels as a means to reduce the computation cost in static CNNs while maintaining their performance. The reviewer agrees that some recent works focus more on Flops, but the number of parameters is also discussed in general, when telling about the 'model size'. The method also proposes the attention-based scaling of channels, where the attention comes from GAP, so the reviewer thinks that it is possible to explain this work as some variation of SEnet. The use of dynamic convolutions is by no means a novel idea and has been studied in multiple previous works in vision (mixture of experts, soft conditional computation, pay less attention with dynamic convolutions, ...) which the authors fail to cite/compare against. 1. The proposed module named dynamic convolution is detailed in Sec 3.2.
Risk control is instantiated by different ways of estimating the advantage of a state (max/min instead of average). If the maximum is taken, the resulting policy will be exploratory (i.e., have a "promotion focus"); if the minimum advantage is taken, the resulting policy will be risk sensitive (i.e., have a "regulatory focus"). Further, the fact that using a smaller number of advantage estimates worked better (point #2 on pg 5, Effect of Ensemble Size in Appendix A) suggests that the ensemble size is an important hyperparameter, and that risk-seeking / risk-aversion (i.e., regulatory vs promotion focus) cannot alone explain why the proposed method works. I would encourage the authors to incorporate the feedback in all reviews and submit the paper to a future conference.
The paper describes a methodology for reducing model dependance on bias by specifying a model family of biases (i.e. conv nets with only 1x1 convs to model color biases), and then forcing independence between feature representations of the bias model and the a full model (i.e. conv nets with 3x3 convs to also model edges). ### Decision and reasons I vote for a weak accept. For example, for some classification tasks, the local texture could be part of the signal and not just the bias.
The paper proposes a framework for learning with rejection using ideas from adversarial examples. For example, "Learning with rejection is a classification scenario where the learner is given the option to reject an instance instead of predicting its label.", "…classifies adversary attacks to two types of attacks, white-box attack and black-box attack.", "Methods for protecting against these adversarial examples are also being proposed.". [1] Adversarial Examples For Improving End-to-End Attention-based Small-Footprint Keyword Spotting, ICASSP 2019 However, I think there are different dimensions along which the paper could be improved: - My understanding of classification with a reject option is that the rejection cost c(x) is a design choice that can depend on the specific application. IJCAI 2001: 973-978. Where they study the same problem when misclassifying one class of data may cost a lot than misclassifying another class of data. If the definition has no relationship, it is classical learning with rejection. Rademacher and Gaussian Complexities: Risk Bounds and Structural Results.
The authors propose a framework to incorporate additional semantic prior knowledge into the traditional training of deep learning models such that the additional knowledge acts as both soft and hard constraints to regularize the embedding space instead of the parameter space. The domain that the method is applied to is VQA, with various relations on the questions translated into hard constraints on the embedding space. 2. In Section 2, the authors say "constraints on the parameter space of a model are often non-intuitive". How are they "non-intuitive" and why the proposed method is more intuitive in terms of theory? in fact, so frustrating that it is hard for me to recommend acceptance in its current form.
This paper proposed to use VAE to learn a sampling strategy in neural architecture search. ============ previous comments Neural architecture search can be formulated as learning a distribution of promising architectures (the sampling policy). Given the above, I would like to increase my score from 1 to 3 (weak reject). I'm therefore unable to recommend acceptance for the paper at the moment, but am willing to raise my score if the authors can properly address those issues in the rebuttal.
Unlike other self-imitation learning methods, the proposed method not only leverages sub-trajectories with high rewards, but lower-reward trajectories to encourage agent exploration diversity. This paper proposes an approach for diverse self-imitation for hard exploration problems. The idea is leverage recently proposed self-imitation approaches for learning to imitate good trajectories generated by the policy itself. The authors propose DTSIL to learn a trajectory-conditioned policy to imitate diverse trajectories from the agent's own past experience. In 2019 (post Go-Explore), it's not clear Montezuma's revenge poses a significant exploration challenge -- exploration doesn't even need to be interleaved with learning. So one major question is whether Go-Explore is a scientifically appropriate benchmark to compare with for this setting. The approach taken is to apply self-imitation to a diverse selection of trajectories from past experience -- practice re-doing the strangest things you've ever done. However, I have a few concerns below, that prevent me from giving a direct acceptance.
This paper tackles the issue of identifying the causal reasoning behind why partial models in MBRL settings fail to make correct predictions under a new policy. *Summary* This paper considers the effect of partial models in RL, authors claim that these models can be causally wrong and hence result in a wrong policy (sub optimal set of actions). For example, the MDP example is just an off-policy policy evaluation problem, and it is very well known that in this case you need to consider the behavior policy, for example with importance sampling. Therefore, I'm raising my score to "accept. POST-RESPONSE COMMENTS: In my opinion, the authors adequately addressed both my own concerns, and also several valid concerns from the other reviewers. The paper considers the problem of predicting a variable y given x where x may suffer from a policy change, e.g., x may follow a different distribution than the original data or suffer from a confounding variable. 4. For sentence, "Fundamentally, the problem is due to causally incorrect reasoning: the model learns the observational condi- tional p(r|s0, a0, a1) instead of the interventional conditional given by p(r|s0, do(a0), do(a1)) = s1 p(s1|s0, a0)p(r|s1, a1)." Is this the "change in the behavior policy" that you're referring to? Improvement The current manuscript needs a major revision, mainly 1) situate the work with respect to off-policy policy evaluation literature, and then 2) Considering step 1, a clarification for what is the novelty/ contribution of the current paper is needed. *Decision* I vote for rejection of this paper, based on the following argument: To my understanding authors are basically solving the "off-policy policy evaluation" problem, without relating to this literature.
The paper proposed an interesting continual learning approach for sequential data processing with recurrent neural network architecture. The authors provide a general application on sequential data for continual learning, and show their proposed model outperforms baseline. "Continual learning through synaptic intelligence." Proceedings of the 34th International Conference on Machine Learning-Volume 70. In traditional continual learning settings, researchers may not always increase the model size for overcoming catastrophic forgetting. However, most continual learning methods can still be applied in this scenario, at least regularization based methods [1,2] can be simply applied in this scenario. Could we also possibly have more baselines from continual learning? Thus, It is interesting to see that continual learning is used in sequential data. The contributions of the paper are 3 fold - new benchmarks for CL with sequential data for RNN processing, new architecture introduced for more effective processing and a thorough empirical evaluation.
This paper proposed a new query efficient black-box attack algorithm using better evolution strategies. In terms of better evolution strategies, the authors show that (1+1) and CMA-EA can achieve better attack result but it lacks intuition/explanations why these helps, what is the difference. Therefore I decided to keep my score unchanged. Li, Yandong, et al. "NATTACK: Learning the Distributions of Adversarial Examples for an Improved Black-Box Attack on Deep Neural Networks." ICML 2019.
Following existing gradient sparsification techniques such DGC, GMC is also built up on the memory gradient approach; the major distinction between GMC and existing techniques is that GMC keeps track of global gradient to maintain the memory gradient, while the existing technique keeps track of worker-local gradients for memory gradient. The primary questions and concerns (critical to the rating) are: 1. One important claimed advantage of GMC over existing method is that it uses global gradient for memory gradient, while existing methods such as DGC uses local-work gradient to do so. If it is for demonstrating the benefits of global gradient for gradient memory, I think it is more proper to also include the results of DGC without factor masking? My preliminary feeling is that by bounding the gradient variance, it should also be possible to prove a rate for DGC using worker-local gradient; this is because the difference between the global gradient and the local gradient might be bounded via the gradient variance. 2. I notice that the experiments uses conventional momentum SGD for a few epochs as warm up, is there any specific reasoning on using this warmup approach instead of the sparsity level warmup as used in DGC?
The algorithm selects a subset of datapoints to approximate the training loss at the beginning of each epoch in order to reduce the total amount of time necessary to solve the empirical risk minimization problem. 2015. [2] Defazio, Aaron, and Léon Bottou. It is interesting to observe how the order of the datapoints matter significantly for training, and that CRAIG is also able to naturally define a good ordering of the datapoints for SG training. Strengths: The proposed idea is novel and intriguing, utilizing tools from combinatorial optimization to select an appropriate subset for approximating the training loss. The paper is very clear, well-written, and was a genuinely fun read. If the comments I made above were addressed, I would be open to changing my decision. But if the baselines weren't thoroughly tuned, it could be the case that IG on the CRAIG subset performs similarly to IG on the full training data, but that neither is actually reaching satisfactory performance in a given domain. I'm wondering how much of the speedup could be attributed to something like better memory locality when using the smaller subset selected using CRAIG. random subset or 2. a subset selected via importance sampling from [1] would contribute towards understanding the particular benefits of CRAIG. This paper is a clear accept.
has been -> have without explicit defines -> defining This paper proposes a training objective that combines three terms: * A Stein discrepancy for learning a energy model with intractable normalizing constant This is my main concern about the paper if this term appears simply because that energy model has an unnormalized density while from GANs we can sample, and Stein discrepancy is best applicable to such pair of distributions.
Summary: This paper list several limitations of translational-based Knowledge Graph embedding methods, TransE which have been identified by prior works and have theoretically/empirically shown that all limitations can be addressed by altering the loss function and shifting to Complex domain. The authors propose four variants of loss function which address the limitations and propose a method, RPTransComplEx which utilizes their observations for outperforming several existing Knowledge Graph embedding methods. In this paper, the authors investigate the main limitations of TransE in the light of loss function. Eq. (3) and (5) show "a" loss function rather than "the" loss function since multiple choices are possible. However, it seems from Section 5 (Dataset), that this paper is using a modified dataset, as the TransE models are only trained on high-confidence triples. Furthermore, the paper proposes TransComplEx -- an adaption of ideas from ComplEx/HolE  to TransE -- to mitigate issues that can not be overcome by a simply chosing a different loss. "RotatE: Knowledge Graph Embedding by Relational Rotation in Complex Space." ArXiv abs/1902.10197 (2019): n. Rotate: Knowledge graph embedding by relational rotation in complex space.
More specifically, the hierarchically aggregate computation graphs are proposed to aggregate the intermediate node and utilize this to speed up a GNN computation. The experiment shows the HAG performs faster in both training and inference. However, the equal-contribution (in Comment 1) is still a big one that the authors should pay attention. I put 6 (weak accept), since we cannot put 5. --------------------------------------------------Update------------------------------------------------ Thanks very much for the authors' feedback.
A number of experimental results are presented that show strong correlation between the sensitivity metric and the empirical test loss. All told, if taken in isolation from prior work, I think the insights and empirical results presented in this paper are quite interesting and certainly sufficient for acceptance to *CONF*.
The algorithm casts this training task as a reinforcement learning problem, and employs curriculum learning and the Proximal Policy Optimization algorithm to find appropriate neural network parameters, in particular, those that make the prover good at finding long proofs. - in experiment 3, curriculum learning tends to find shorter proofs. Also, if the dataset includes variables and other propositional logic formulas, such as disjunction, negation and conjunction, so that the prover can be applied to any formulas from Peano arithmetic via Skolemization, I would be much more supportive for the paper. I would appreciate someone with more experience on that topic weighing in. OVERALL: I don't work on ATP and am not particularly well suited to review this paper, but I am slightly inclined to accept for the following reasons.
This paper suggests a method for detecting adversarial attacks known as EXAID, which leverages deep learning explainability techniques to detect adversarial examples. However, considering a tabby cat image is perturbed to become tiger cat, since two classes are very close, the resulting saliency maps should be similar and the detector may fail to detect the adversarial example. Using explainability to detect the presence of adversarial attacks seems like a nice intuitive idea and the results show that it indeed works.
This works presents a method for inferring the optimal bit allocation for quantization of weights and activations in CNNs. The formulation is sound and the experiments are complete. Based on this assumption, the authors then use a Lagrangian based constrained optimization to minimize the sum of squared errors of outputs when individual weights/activations are quantized, with the constraint being the total bit budget for weights and activations. My main concern is regarding the related work and experimental validation being incomplete, as they don't mention a very recent and similar work published in ICIP19 https://ieeexplore.ieee.org/document/8803498: "Optimizing the bit allocation for compression of weights and activations of deep neural networks".
To calculate the influence function of a model with pretraining, the authors use an approximation f(w)+||w-w*||, where w* is pretrained. This is an analysis paper of pretraining with the tool "influence function". This paper proposes a multi-stage influence function for transfer learning to identify the impact of source samples to the performance of the learned target model on the target domain. I believe that these are useful technical contributions that will help to broaden the applicability of influence functions beyond the standard supervised setting. 1. The idea of converting a pre-trained model with f(w)+||w-w*|| is interesting. More generally, can we relate influence to some objective measure that we care about (say test accuracy), for example by showing that removing the top X% of influential pretraining data hurts test accuracy as much as predicted? 2) In what situations might we want to examine the influence of pretraining data, and can we design experiments that show those situations? The authors derive the influence function of models that are first pre-trained and then fine-tuned. This extends influence functions beyond the standard supervised setting that they have been primarily considered in. For that reason, I recommend a weak accept.
From my perspective, the whole story revolves around how to compute persistence barcodes from the sub-levelset filtration of the loss surface, obtained from function values taken on a grid over the parameters. It seems like the main contribution is a new algorithm for computing barcodes of minima. (4) The author's talk about the "minima's barcode" - I have no idea what is meant by that either; the barcode is the result of sub-levelset persistent homology of a function -> it's not associated to a minima. (5) Is Theorem 2.3. not just a restatement of a theorem from Barannikov '94? There are many TDA packages these days (maybe the CRAN TDA package?)
This work provides  theoretical analysis for the NAS using weight sharing in two aspects: 1) The authors give non-asymptotic stationary-point convergence guarantees (based on stochastic block mirror descent (SBMD) from Dang and Lan (2015)) for the empirical risk minimization (ERM) objective associated with weight-sharing. The reviewer has several concerns: 1) the SBMD and ASCA algorithms are existing generic algorithms. The author proposed ASCA, as an alternative method to SBMD. Also, comparing to first order DARTS, search cost is the same and this is hard to justify the better convergence rate for EDARTS. The author also provided an alternative to SBMD that uses alternating successive convex approximation (ASCA) which has similar convergence rate.
Not being an expert in RL, my assessment should be discounted. The point of Dai et al. was to use RL to solve a wide family of combinatorial problems.
Two simplified versions of GNN is then proposed by linearizing the graph filtering function and the set function, denoted as GFN and GLN, respectively. Another possibility is that even the original GNN has larger model capacity, it is not able to capture more useful information from the graph structure, even on tasks that are more challenging than graph classification. The experiments are designed nicely: 1) it compares with various baselines on a variety of popular benchmarks; 2) ablation studies single out the importance of different graph features, such as degree, and multi-hop averages; 3) verifying whether the good performance GFN comes from easier optimization. However, I cannot accept the paper in the current form because of the following reasons.
Summary The paper proposes a method for continuous learning called Functional Regularization of Memorable Past (FROMP) which maintains the output distribution of models on memory samples. This might explain the reason for the huge forgetting reported for VCL with coreset (−9.2 ± 1.8) as opposed to −2.3 ± 1.4 for EWC which is really strange as VCL even without coreset (on permuted mnist for example) is reported superior to EWC by a large margin (6%) in the original VCL paper. 2- Forward transfer? Regularization techniques combined with memory might have an ability to perform zero-shot transfer or so called FWT. Also, in the experiment results, I feel the performance of the FROMP largely depends on the number of the coreset, while 'important' selection just shows marginal effects even on split CIFAR.
The proposed approach computes the saliency maps for all the classes and removes the pixels that play a role in predicting several classes. While the mechanism for competition is very simple, the resulting activation maps subject to randomization tests are reasonably convincing. 3. The introduced approach makes Grad.Input pass the sanity checks introduced by Adebayo et al. Weaknesses: 1. For any interpretability technique, passing the sanity check is a must, but just because a saliency technique passes the sanity checks, it doesn't mean that these maps explain the network's decision well. - Section 5 "The available code for these maps is slow, and computing even gradient for all 1000 ImageNet labels can be rather slow." What is the aim of this sentence? My main concern is that the method seems to be designed only to answer the sanity checks: the resulting saliency maps can hardly be seen as more informative as other existing methods (eg figure 1). Quantitative measures (ROAR & KAR, Hooker et al. 2018) or surveys to show that the newly obtained saliency maps are more refined or help to best localize regions of interest would be a big bonus.
[R2] Unsupervised learning of depth and ego-motion from video This submission proposes a self-supervised segmentation method, that learns from single-object videos by finding the region where it can segment an object, remove the entire bounding box around it, inpaint it, then finally put the object back. Many previous unsupervised video object segmentation methods make use of optical flow and boundary detection, which I thought are OK cues to be used, especially when both can be learned in a self-supervised manner. My decision is Weak Accept.
The lottery ticket hypothesis tells that there exist sparse subnetworks of the corresponding full dense network which can attain as strong loss / accuracy as the full dense network. Thus I give weak reject. 2) As addressed above, in the Abstract and Introduction, the paper's claims are very general about mode connectivity and sparsity, claiming in the sparse regime, "a subnetwork is matching if and only if it is stable."
This paper proposes a novel approach for the loss function of matrix completion when geometric information is available. I think besides introducing what is matrix completion and what is geometric completion, the introduction part should focus more on the motivation to propose the algorithm. Though the authors include the connection between [Arora et al. (2019)], this is not convincing enough since as explained above, the implicit regularization there depends on the smallness of the initialization. (4) As a followup question, without such implicit regularization, it is unclear why the proposed approach does not suffer from overfitting. But, I vote for weak acceptance due to its drawbacks as mentioned above.
This paper explores how tools from perturbative field theory can be used to shed light on properties of the generalization error of Gaussian process/kernel regression, particularly on how the error depends on the number of samples N. This theoretical paper exploits a recent rigorous correspondence between very wide DNNs (trained in a certain way) and Neural Tangent Kernel (a case of Gaussian Process-based noiseless Bayesian Inference). My only real concern is with the presentation. \\sum_j f_j \\phi_j  (I think). The results presented here are interesting and I particularly liked the introduction of the renormalized kernel to study the noiseless case. For instance, the sentence ''They [results] hold without any limitations on the dataset or the kernel and yield a variant of the EK result along with its sub-leading correction.'' is misleading: as stated in the previous sentence in the text, this is for the fixed-teacher learning curve, etc. Appendix B:  could you explain why this difference increases with N ?
This paper proposes a framework to model the evolution of dynamic graphs for the task of predicting the topology of next graph given a sequence of graphs. al. 2018]). Given a sequence of graphs as input, a GNN (to obtain low-dimensional representations of the graphs in this sequence) and LSTM (to model the sequence of these representations)  based encoder is used to compute a vector representation of the topology of next graph in the sequence. A better model should be proposed to address this challenge. The model does not output a graph with the right size for very simple synthetic graphs. The contribution of the paper seems to be a system of combining these to achieve graph evolution prediction. Second, the model only takes 10 graphs as input and ignores other graphs in the input graph sequences. Specifically, the paper uses a combination of recently proposed techniques in graph representation learning (Graph Neural Network) and Graph Generation (GraphRNN [You et. Did the authors try to feed the sequence of graphs to such models and then try to generate a new graph to see if they can produce similar results? The learned vector is then used as input to a GraphRNN decoder to generate a graph that would serve as a predicted next graph in the sequence.
[2] Linear Memory Networks This paper proposes an initialization scheme for the recently introduced linear memory network (LMN) (Bacciu et al., 2019) and the authors claim that this initialization scheme can help improving the model performance of long-term sequential learning problems. First, the LMN seems to be a simpler version of LSTM and it has no significant advantages compared with other recurrent structures introduced in the past several years. Do you have any experiment results on any large dataset? I am not too sure how the proposed initialization helps in this case. The proposed initialization is aimed at helping to maintain longer-term memory and instability during training such as  exploding gradients (due to linearity). Summary: The paper proposes an autoencoder-based initialization for RNNs with linear memory. And then they use the weight to initialize the Lieanr Memory Networks(LMN). Hochreiter, Sepp and Schmidhuber, Jürgen.
The paper explores multi-task learning in embodied environments and proposes a Dual-Attention Model that disentangles the knowledge of words and visual attributes in the intermediate representations. *Decision and supporting arguments I think the paper is on the borderline. If AC/meta-reviewer considers the ability of vision-and-language interactions could be effectively studied through this setup with synthetic language and simulated-unrealistic images, I am OK with acceptance.
This paper proposes a method to summarize a given graph based on the algebraic multigrid and optimal transport, which can be further used for the downstream ML tasks such as graph classification. The node distance (transport cost) used in the Wasserstein metric is also learned as an L2 distance between the embeddings of some graph embedding function. Since the proposed method is for generating coarse graphs in an unsupervised manner, graph classification cannot be directly performed by itself. Moreover, since you also employ graph convolutional networks for coarsening, you are also in the regime of this paper. However, A_C is usually not binary for S \\in R^{n x m}, hence how to get the coarse graph G_C from A_C is not clear.
The reviewer votes for rejection as the method has limited novelty. The authors stated that "F-pooling remarkably increases accuracy and robustness w.r.t. shifts of moderns CNNs"; however, in Table 1-3, the winning margin of accuracy is actually quite small (<2%), and the consistency (<3.5% compared to the second best baseline except resnet-18 on CIFAR 100 has large improvement ~7-8%). 2. Compared to AA-pooling, it seems that F-pooling has a better theoretical guarantee (i.e. the optimal anti-aliasing down sampling operation given U). 2. When showing the optimality of F-pooling in Section 2.3, the criterion is to reconstruct the original signal x. Questions: 1. For the experiment of 1D signal on sine wave, the AA-pooling and F-pooling give the same result? Taking the perspective from signal processing, this paper proposes a pooling operation called frequency pooling (F-pooling). But in the current form, the paper has less value to be published in *CONF*.
This paper proposed a new realistic setting for few-shot learning that we can obtain representations from a pre-trained model trained on a large-scale dataset, but cannot access its training details. Is the attention way used in the paper a good way to exploit the pre-trained model for few-shot classification problems? Also, from the results, the significant improvements come from the weights of the pre-trained model but not the attention used. ========================================================= After Rebuttal: I thank the author for the response. For the pre-trained model, they will not only use its weights but also use it to generate a spatial attention map and help the model focuses on objects of images. 2) The algorithm does not have any important contributions comparing to existing ones: they define a prototype per class based on the pre-trained model and apply the nearest neighbor classification.
The paper proposes to distill the predictions of an ensemble with a multi-headed network, with as many heads as members in the original ensemble. Overview: This work introduces a new method for ensemble distillation. Distillation proceeds by minimizing the KL divergence between the predictions of each ensemble member with the corresponding head in the student network. The problem of making better ensemble distillation methods seems relevant as ensembles are still one of the best ways to estimate uncertainty in practice (although see concerns below). </update> Summary & Pros - This paper proposes a simple yet effective distillation scheme from an ensemble of independent models to a multi-head architecture for preserving the diversity of the ensemble. To verify the effectiveness of the proposed distillation method, other large-sized datasets should be tested, e.g., CIFAR-100, ImageNet. - A comparison with an ensemble with M=14 models should be tested because this ensemble has the same number of parameters compared to Hydra with M=50 heads. The paper would have been more interesting if the authors had managed to demonstrated significant improvements over competitors on not toy (MNIST / CIFAR) problems.
4. It is not clear why the proposed method can solve the issue that OWM faces with (bad accuracy when tasks are not quite related). The method is an extension of recent work, called orthogonal weights modification (OWM) [Zheng,2019]. The authors conduct experiments on image classification tasks to show the performance of the proposed method and compare it with two other baselines EWC and OWM. It is not convincing. The authors claim that OWM is one of the strongest baselines, but actually it perform really badly on EMNIST-26 (5 tasks),  EMNIST-47 (5 tasks) and EMNIST-47 (10 tasks). I am therefore giving the paper a weak reject. 2. It might strengthen the paper if the authors can show the comparison results on more other datasets, e.g., other image classification tasks.
This paper proposes a new way to create compact neural net, named Atomic Compression Networks (ACN). Strengths: a lot of nice experiments with clearly advantageous results are given. Weaknesses: One obvious baseline missing is sparse compression, which can be achieved using either l1 regularization, or hard thresholding + fine tuning, both of which are easy to implement and appear in several works, e.g. Scalable Neural Network Compression and Pruning Using Hard Clustering and L1 Regularization (Yang, Ruozzi, Gogate) Also, I think this work should be compared with compression schemes that work via kronecker product, which seem very similar to this scheme (but where the kronecker matrix is binary to produce replication)
The authors proposed a series of improvements, including alternative optimization, dynamic scheduling, detach and batch normalization to help boosting the performance to SOTA under 4-bit quantization. The idea seems on a high level to be interesting and simple; train floating point models that can fit the data well while also encouraging them to be robust to quantization by enforcing the predictive distributions of the fixed and floating point models to be similar in the KL-divergence sense. I addresses the existing issues in the common paradigm, where a floating-point network is trained first, followed by a second-phase training step for the quantized version. However, it seems to me that this drastic scheduling strategy sounds like very similar to the traditional approach that trains the floating point network first and then finetune the quantized one, except for the fact that this proposed algorithm repeats this process a few times. Do you absorb the scale and shifts in the weights / biases before you perform quantization or do you quantize the weights and then apply the BN scale and shift in full precision? Furthermore, it should be noted that the discrepancy in BN in quantized models was, as far as I am aware, firstly noticed at [1] (and subsequently at RelaxedQuant) and both of these methods simply re-estimated the moving averages during the inference time. If the training algorithm focuses too much on the first term, it will make the network less friendly to the quantization process. The results on ImageNet under 4-bit quantization are strong and convincing, but the paper could benefit from conducting additional experiments on different datasets and bitwidth configurations.
The paper proposes to use Contrastive Predictive Coding (CPC), an unsupervised learning approach, to learn representations for further image classification. The authors augment contrastive predictive coding (CPC), a recent representation learning technique organized around making local representations maximally useful for predicting other nearby representations, and evaluates their augmented architecture in several image classification problems. Title: DATA-EFFICIENT IMAGE RECOGNITION [Summary] -This paper introduces Contrastive Predictive Coding (CPC) image recognition in the data-efficient regime. Please forgive me if you read it in this way. - The CPC is utilized to enhance spatially predictable representations which benefits a lot data-efficient image recognition. Pros: Owing to its generality (CPC assumes only a weak spatial prior in the input data), and cheap computational cost relative to earlier generative approaches, CPC is already a promising unsupervised representation learning technique. This paper only proposes some minor improvements based on the original CPC method and use a deeper network to get better performance. In line with that, I think their work would be improved by some commentary on this, in particular by any concrete suggestions they have about how similar augmentations to CPC could be carried out in text, audio, and/or video data. I highly appreciate new results and new architectures, but it is not enough for a full conference paper.
Additionally, the Music Transformer model is also conditioned on a combination of both "style" and "melody" embeddings to try and generate music "similar" to the conditioning melody but in the style of the performance embedding. ## summary In this paper, the author extends the standard music Transformer into a conditional version: two encoders are evolved, one for encoding the performance and the other is used for encoding the melody. The main strategy is to condition a Music Transformer architecture on this global "style embedding". [ref2] Conditional image-to-image translation, CVPR'18 This paper presents a technique for encoding the high level "style" of pieces of symbolic music. I am not working on music generation but I list two CV related papers about conditional image translation, which mathematically describes "an image with specific style". The performance conditioning vector is generated by an additional encoding transformer, compared to the Music Transformer paper (Huang et. It took me a couple of passes and reading the Music Transformer paper to realise that in the melody and performance conditioning case, the aim is to generate the full score (melody and accompaniment) while conditioning on the performance style and melody (which is represented using a different vocabulary). Why use this feature compared to existing techniques for measuring similarity between symbolic music pieces? Finally, it would be useful if the authors comment on existing methods for measuring music similarity in symbolic music and how their proposed feature fits into existing work. The authors also mention an internal dataset of music audio and transcriptions, which can be a major contribution to the music information retrieval (MIR) community. Measuring music similarity is a difficult problem and the topic has been the subject of at least 2 decades of research. 2. By checking the music Transformer, in Table 3, it is not surprising to see that the proposed method outperforms the corresponding baselines, because no conditional information is used. Since it is defined but never used. Is it real valued? The authors mention (Yang and Lerch, 2018) but use a totally different set of attributes compared to that paper. Firstly, I am not sure what the final dimensionality of the feature vector is. ## Reference [ref1] Multimodal Unsupervised Image-to-Image Translation, ECCV'18 Is the global conditioning the samples from the noise distribution? Although I understand the need for anonymity and constraints while referring to unreleased datasets, it would still be useful for the reader/reviewer to have some details of how the melody was extracted and represented.
They use enhanced loss scale, quantization and stochastic rounding techniques to balance the numerical accuracy and computational efficiency. For example, how much improvement can this work achieve when just using enhanced loss scaling method or a stochastic rounding technique?
Specifically, every update in the back propagation algorithm is being replaced by an implicit update except for the intermediate parameters that receive a "semi-implicit" update. This theory suggests a lot of stability properties for the implicit SGD update of Equation (27).
The idea is to combine online vector quantization with Bayesian neural networks (BNN), so that only incremental computation is needed for a new data point. Since these OCHs are differentiable, the proposed DBNN model can be used for streaming input data with time-variant distributions. For example, in Table 2, BNN (shown as MU) is significantly slower than DU/DBNN and DNN. In the introduction, the authors motivate the proposed DBNN by saying that BNN needs dozens of samples from weight distributions and therefore is rather inefficient. It would be better if the problem setting of online inference is introduced at the beginning, followed by the overview of DBNN and then the OCH details. I believe that this paper needs for work and is not yet suitable for acceptance.
This paper proposed a new method for knowledge distillation, which transfers knowledge from a large teacher network to a small student network in training to help network compression and acceleration. I updated my rate to weak accept. It looks to me the proposed method use an ad-hoc selected layer to transfer knowledge from teacher to student, and the transfer is indirect because it has to go through the pre-trained subnetwork in teacher. 2. Utilizing the "soft targets" to transfer knowledge from teacher to student model is not first proposed by Hinton et al. (2015). In conclusion, I will give a weak reject currently, unless the authors improve their literature survey and modify their claims. Possible improvement of the paper is the instruction on how to choose the intermediate layer from where to teach the representation, i.e. where the student sub-network ends and teacher sub-network begins.
This paper proposes "Back-to-Back" regression for estimating the causal influence between X and Y in the linear model Y=(XE+N)F, where the E denotes a diagonal matrix of causal influences. However, the authors do not provide enough evidence that this method is generally useful and better than established methods to merit acceptance to *CONF*. ############## After reading the author's feedback and the comments from other reviewers, I keep the current rating but tend to a borderline score and it is ok if it must be rejected because of the concerns of limited applicability and the experimental.
- Summary: This paper proposes an out-of-distribution detection (OOD) method under constraints that 1) no OOD is available for validation and 2) model parameters should be unchanged. 1. The problem setting is clear and their approach is interesting and makes sense. After reading the other reviews and comments, I appreciate the effort by the Authors, but it looks like the paper still needs some work before being ready. As the authors addressed, Mahalanobis detector proposed by Lee et al. (2018b) requires validation to determine weights for feature ensembling, but the validation can be done without OOD data by generating adversarial samples as proposed in the same paper. Again, weights can be validated by adversarial samples to satisfy the constraints. Detailed comments: 1-(a). Adversarial attack and OOD (which is hard to detect) are closely related: they are both in off-manifold. The difficulty of OOD detection can be considered to be coming from overlapped manifolds in the latent space. To me, if the work could be changed to compare against works which are not so tightly constrained, not for the purposes of holding it to the same standard but to understand it's relative standing, or to better justify the very strict constraints which somehow, despite out-of-distribution detection being a popular upcoming topic, apparently only has one other paper that matches it. It may be nice to cite works such as http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.132.6389&rep=rep1&type=pdf and others in that vein as this is certainly not the first work to involve compressive principles in image classification related tasks. Their main difference would be, while adversarial attack is very close to the clean data in the data space, OOD is relatively far from the in-distribution in the data space. - Comments: 1. As addressed by the authors, feature concatenation ("assemble") is not effective for the Mahalanobis method but the proposed method. Also, the performance of their replication of the prior method is far lower than reported. So, it would be interesting to see the performance of both baselines and proposed approach in those settings where inputs are similar in nature but very different in some aspects. For this reason, I am borderline unless that caveat is addressed as described below, in which case I would be happy to accept.
Unfortunately, delaying the updates opens up for the same problems as asynchronous SGD (ASGD), i.e., slower (none) convergence or staleness problems. The idea of sparse communication for SGD exists since at least 2012 [1, Algorithm 3].
As the authors point out (and I agree), the paper constitutes a compelling reason for theoretical research on the interplay between overparameterization and parameter recovery in latent variable neural networks trained with gradient descent methods. This paper performs empirical study on the influence of overparameterization to generalization performance of noisy-or networks and sparse coding, and points out overparameterization is indeed beneficial. This paper investigates benefit of over-parameterization for latent variable generative model while existing researches typically focus on supervised learning settings. The paper "aims to be a controlled empirical study making precise the benefits of overparameterization in unsupervised learning settings. More ablation study and more experiments on general models will clarify what is going on in the over-parameterized model for latent generative models. In line with the findings for supervised settings, the authors find that overparameterization is often beneficial, and that overfitting is a surprisingly small issue. The generative models to obtain disentanglement representation could be investigated in the frame-work of this paper. A small gripe: the authors promise " a controlled empirical study making precise the benefits of overparameterization in unsupervised learning settings". There could be at least some intuitions on why overparameterization helps noisy-or models. Decision: weak accept. The paper contains some new insights, but its contributions are not quite as substantial (e.g. lack of precise mathematical statements) or surprising as those in stronger *CONF* papers. I agree that overparameterization improves recovery is a new finding. I find the paper has some drawbacks.
1. Summary The paper theoretically investigates the role of "local optima" of the variational objective in ignoring latent variables (leading to posterior collapse) in variational autoencoders. This paper tries to establish an explanation for the posterior collapse by linking the phenomenon to local minima. Overall: 1) I felt that Section 3 which introduces categorizations of posterior collapse is a valuable contribution and I expect that these difference forms of posterior collapse are currently under appreciated by the ML community. The points below are all related. In particular, I believe it would be more accurate to say that IF the autoencoder has bad local minima then the VAE is also likely to have category (v) posterior collapse. Aren't they the same? e. The experiments consider training AEs/VAEs with increasingly complex decoders/encoders and suggest there is a strong relationship between the reconstruction errors in AEs and VAEs, and this and posterior collapse. 2) Section 4 provides a brief overview of existing results in the affine case and introduces a non-linear counter-example showing that local minima may exist which encourage complete posterior collapse.
The second contribution is to use semantic information, compact statistics derived from (1) detected objects and (2) semantic segmentation, to replace the RGB image and provide input to the system in a way that maintains state-of-the-art performance but shrinks the performance gap between the seen and unseen data. This paper aims to identify the primary source of transfer error in vision&language navigation tasks in unseen environments. The authors conclude that of the three sources of information provided to the agent (the natural language instruction, the graph structure of the environment, and the RGB image), the RGB image is the primary source of the overfitting. A few small questions/comments below. First, the authors perform an extensive study to understand the source of what they refer to as 'environment bias', which manifests itself as a gap in performance between environments used for training and unseen environments used for validation. As I mention below, the metric for success on these tasks is performance on the unseen data, and, though an improvement on their 'bias' metric is good anecdotal evidence their proposed methods are doing what they think, the improvements in this metric are largely due to a nontrivial decrease in performance on the training data. This narrative challenge is the most important reason I cannot recommend this paper in its current state. -------- After discussing with the reviewers about the methodological issue of the validation set, I have lowered my score to a weak accept, but I think this paper should still be published.
In this paper's formulation, W is named as a weight generation matrix, which is choosing to be random and i.i.d. with certain probabilities. Here are my concerns and questions: 1). Based on the above comments, I think the work will benefit from further developments before being ready for publication. Based on this, I give my rating.
They find that (i) the auto-encoder representation does not capture the semantic information learned by the supervised representations and (ii) representations learned by the model depend on the label taxonomy,  how the targets are represented (1-hot vs. - The authors claim "Surprisingly, the kind of supervised input that proved most effective in matching human performance on the triplet odd-one-out task was training with superordinate labels". would all likely give different ratings.
The main contribution of this paper are: 1. The use of a heteroscedastic GP when performing Bayesian Optimization, this is in contrast to the more common practice of assuming homoscedastic noise, even when this does not quite fit the data. Under the noise setting, the Random approach performs very competitive to BO. 2. They introduce two new acquisition functions that incorporate the predicted observation noise, either making candidates more likely or less likely to be chosen when predicted noise is higher, depending on the requirements. Since no convergence guarantee is given, a more extensive empirical analysis with real datasets needs to be provided to better understand the performance and behavior of the proposed BO algorithms.
The proposed method used a classification function of a fractional graph semi-supervised learning (GSSL) [De Nigris et al., 2017] as a graph filter. It is based on a novel fractional filter for graph conv networks, which generalizes several previously employed graph semi-supervised learning frameworks, by introducing a fractional hyperparameter (sigma in the paper), using fractional powers of the Laplace operator. The rating has been updated. ------------------------------------------------- The response from authors addressed many of my concerns. The authors design a new graph convolutional filter based on Levy Flights, and propose new feature propagation rules on graphs. I believe the experimental results could justify an accept, but I would not claim I am an expert in semi-supervised learning on graphs.
This paper presents a technique for model based RL/planning with latent dynamics models, which learns the latent model only using reward prediction. This is in contrast to existing work which generally use a combination of reward prediction and state reconstruction to learn the latent model. The proposed model uses an encoder to learn embedding the state to the latent state, a forward dynamics function to learn dynamical system in latent state space, and a reward function to estimate the reward given a latent state and an action. I tend to reject this work, because although I support the premise and believe it is very important, and like the style of experiments run with the use of distractors, I believe it is not impactful if only looking at the dense reward setting. The contributions consist only of learning a multi-step reward model for planning, and only provide results in two dense reward environments. Model based RL with video prediction models has been shown to work in such real cluttered robot manipulation environments.
This paper performs a regret analysis for a new hierarchical reinforcement learning (HRL) algorithm that claims an exponential improvement over applying a naive RL approach to the same problem. For instance, the paper begins by contrasting HRL approaches with a number of standard RL algorithms, saying that approaches such as AlphaGo do not require high-level planning. What is an example of an algorithm that does planning in a standard RL setting?
The approach starts from the standard modelling assuming iid samples from a Gaussian distribution with unknown mean and variances and places evidential priors (relying on the Dempster-Shafer Theory of Evidence [1] /subjective logic [2]) on those quantities to model uncertainty in a deterministic fashion, i.e. without relying on sampling as most previous approaches. The main idea follows the evidential deep learning work proposed in (Sensoy et al., 2018) extending it from the classification regime to the regression regime, by placing evidential priors over the Gaussian likelihood function and performing the type-II maximum likelihood estimation similar to the empirical Bayes method [1,2]. 2. The experimental results show consistent improvement in performance over a wide base of benchmarks, scales to large vision problems and behaves robustly against adversarial examples. This term was manually added as additional regularization to "prefer the evidence to shrink to zero for a sample if it cannot be correctly classified" in (Sensoy et al., 2018), and a different regularization was used to encourage distributional uncertainty in [3]. Predictive uncertainty estimation via prior networks. This is a very relevant topic in deep learning, as deep learning methods are increasingly deployed in safety-critical domains, and I think that this works deserves its place at *CONF*. I believe that the article would greatly benefit from a more thorough introduction of concepts linked to the theory of evidence.
This paper purposes to cluster data in an unsupervised manner that estimates the distribution with GMM in latent space instead of original data space. The author(s) posit a Mixture of Gaussian's prior for a compressed latent space representation of high-dimensional data (e.g. images and documents). The author(s) state "Colors are used to distinguish between clusters." This statement is unclear as to whether the author(s) are using the class label or learned latent cluster. Being the first to pair an existing model with an existing method, in my eyes, does not necessarily meet the *CONF* bar.
Once a reward is achieved they can compute the embedding of the corresponding state as a goal under the CPC learnt representation either directly ie. Moreover, the reward shaping method assumes we know the goal state but using CPC feature does not. As the paper discuss in introduction, learning in sparse reward environment is hard because it relies on the agent to enter the goal during exploration. Overall, I am leaning to reject this paper because (1) the main contribution of the paper is not clear (2) the experiments are missing some details and does not seem to support the claim that the proposed methods can tackle the sparse reward problem.
This work examines the fundamental properties of two popular architectures -- PointNet and DeepSets -- for processing point clouds (and other unordered sets). This paper removes the cardinality limitation and gives two kinds of results: 1. PointNet (resp. However, the rebuttal does not alleviate my concerns about additional impact beyond the UATs in the original paper. I'd be grateful if the authors could comment on this. While the theorems proved in the paper are original and novel, they are a refinement of the already known results regarding approximation theorems for PointNet and DeepSets, respectively, hence only a marginal improvement in understanding these function classes. === Post rebuttal update === It would be interesting to investigate empirically the limitations of these architectures, for instance by playing with the diameter and center of mass functions as suggested in 3.3. - The paper is clearly targeted at a specialist audience and invokes dense and advanced mathematical concepts. The authors provide a new universal approximation theorem on real-valued functions that doesn't require the assumption of a fixed cardinality of the input set. To this end, the authors prove a series of theoretical results and establish limitations of these architectures for learning from point clouds. I thus maintain my recommendation of weak reject PointNet (Qi et al, 2017) and Deep sets (Zaheer et al, 2017) have allowed to use deep architectures that deal with point clouds as inputs, taking into account the invariance in the ordering of points. The presentation in this paper does remove the assumption of a fixed cardinality, but since this seems to be a mild assumption, it is not clear what is gained by this (beyond mathematical elegance).
The hypothesis is that if the regressors demonstrate good accuracy, then the word embeddings contained information relevant to "numerical common sense". This paper attempts to study if learned word embeddings for common objects contain information about "numerical common sense". No, because of two flaws: (1) The number of samples in the dataset is too small to represent "numerical common sense". This paper should be rejected because (1) the NCS datasets are too small to represent "numerical common sense" (2) the NCS datasets contain faulty data points and (3) the results from the experiments conducted are not sufficient to accept or refute the hypotheses.
This paper proposes to apply MAML-style meta-learning to few-shot semantic segmentation in images. What is human-level performance at few-shot segmentation? It would strengthen the argument to include an existing few-shot segmentation algorithm in Figure 2.
They cover most design choices in recent works of on-policy RL methods. The authors compare various choices of configurations obtained from the Cartesian product of 8 factors which they call thematic groups: Policy Losses (Sec. Overall, this is a strong paper and I recommend it for publication.
Weak points The HN -> RBM mapping is quite clear, but the reverse RBM The paper is slightly incremental as similar mappings were known, but it remains a relevant contribution, and the aspect of using this mapping as a way to boost learning in RBM seems new, and interesting. I congratulate the authors on their spirit of maintaining a high standard on the theory, experiments and descriptions, and therefore significantly raise my score. All my concerns are addressed and reflected in the revision (though some are much better done than the rest). I hope to see this paper accepted.
Through extensive experiments, the authors show that this method produces more efficient networks while reducing the computational cost of training, still maintaining good validation accuracy, compared to other NAS or pruning/growing methods. It seems to be a more principled approach than previous NAS or separate pruning/growing approaches.
The case of using general SDEs (not only those derived from SMLD and DDPM) is mentioned only briefly, leaving it unclear if using a general SDE would require relatively simple changes, or if the proposed model is limited to SDEs derived from
The authors propose efficient training and sampling procedures, in which the VAE is trained first and during the EBM negative-phase, samples are drawn from the joint (x, The authors propose a generative model that is a combination (product) of a VAE and an EBM, where the goal of the EBM is to reduce the probability of out-of-manifold samples, which are typically generated by VAEs. With VAE as a backbone, the sampling can be transferred to the latent space and the residual \\epsilon in the image space, which is much more friendly to MCMC sampling.
It would've been very interesting to see how these techniques improve the performance of previous neural LTR models; log1p transformation, data augmentation, and model ensembling would straightforwardly apply to other neural models as well. Thanks to the authors for their hard work and the nice paper. Concerns: Regarding the proposed solutions, the authors use data augmentation to improve neural LTR models. When I was reviewing other LTR papers, I often had to point out that the proposed method significantly underperforms LightGBM. Also, although the idea of applying these standard techniques on LTR seems straightforward, but I argue that's only due to the benefit of hindsight; neural LTR has been a fairly active area of research, yet these techniques haven't yet been widely used in LTR literature. Originality, Significance: This paper establishes reference points for modern LTR research. Still, DASALC significantly outperforms previous neural LTR approaches. Pros: This paper discusses potential reasons why neural LTR models are worse than gradient boosted decision tree-based LTR models, and uses empirical results to show the effectiveness of the proposed solutions. They discuss why neural LTR models are worse than gradient boosted decision tree-based LTR models, and introduce some directions to improve neural LTR models. Understanding the reason for LightGBM's superiority could potentially help us to develop better LTR models, neural or not neural.
The authors propose a novel Adversarial Sparse Convex Combination (ASCC) method in which they model the word substitution attack space as a convex hull and leverages a regularization term to enforce perturbation towards an actual substitution. Reason for score: Overall, I vote for accepting this paper.
Pros: One of the first theoretical works on the gradient dynamics and convergence properties of the deep equilibrium models [1] (and implicit models in general [2,3]), which are quite different from conventional deep networks.
The paper proposes a communication correction mechanism where, during the centralized training, messages there were received in the past from other agents are reevaluated according to the updated policy. ---- Summary ---- The paper proposes a method for modifying an experience replay when learning in communication environments, by relabelling messages using the latest policy. Summary: This paper considers communication games when agents use experience replay. 2) seems to only have one consistent message to communicate during the entire episode (after perhaps waiting to receive a message from others, in Hierarchical Communication). 1) the speaker only has communication actions (and no environment actions), and Recommendation and Justification: Overall, I feel like this was a strong paper and should be accepted.
The NN has a "core" that is shared between all neurons, and a neuron-specific readout. Introduce a novel readout mechanism that allows models to be shared fully across neurons which in turn helps transfer learning. The paper advances a few contributions: Confirm that task-driven models based on object recognition, are outperformed by data-driven models for predicting single neuron responses. "Fig 5 for the factorized readout …" (I didn't get it until reading it four times and looking for these results in Figure 5 twice.) The paper presents an experimental study on predicting the responses of mice V1 neurons with computational models. Since the core is shared across neurons, these stimulus-response pairs can be learnt in a massively parallel manner.
The benchmark is based on extensive measurements on real hardware. For this, the authors adopt two popular search spaces (NAS-Bench-201 and FBNet) and measure/estimate hardware performance metrics such as energy costs and latency for six hardware devices (spanning commercial edge devices, FPGA, and ASIC) for all architectures in this search spaces. Summary One important application of Neural Architecture Search is to find neural network architectures with good accuracy/inference time or accuracy/energy tradeoffs on a specific hardware device. The authors convincingly argue that properly performing on-device inference time/energy benchmarks properly is challenging for practitioners because it "requires various hardware domain knowledge including machine learning development frameworks, device compilation, embedded systems, and device measurements." Notes on Rating: I've given the paper a borderline score (5) in my initial review, due to the open questions mentioned in the "cons" section above. Shafique, " NASCaps: A Framework for Neural Architecture Search to Optimize the Accuracy and Hardware Efficiency of Convolutional Capsule Networks", to appear at The IEEE/ACM 2020 International Conference On Computer Aided Design (ICCAD), November 2020 Assuming they will include it in the final version,  I am confident that this paper will meet all standards of *CONF* and recommend acceptance.
Proposes an extension of RFA with gating that improves accuracy on language modeling, relative to softmax attention. The paper presents a linear time and space attention mechanism based on random features to approximate the softmax. The results are strong. Unlike Linear Attention, the proposed RFA outperforms the original multi-head attention baseline on both LM and MT tasks. Strengths The RFA-gated formulation is both more accurate and potentially faster (at least for decoding) than softmax attention, as demonstrated on language modeling. Recommendation: Weak Accept Well-written and timely exploration of linear attention. Where "Performers" goes with positive orthogonal random features (to improve over vanilla RFA), this paper adds a gating mechanism: this adds the possibility to learn some monotically decaying attention over older context, similar to learned receptive fields of attention (as in e.g. [Sukhbaatar et al. 2019]). So my point 5 is important to answer and I would like to see all the details are clarified in order to make the contribution stronger.
This paper provides an upper boundary of the generalization error of networks: the sum of its training error, the distillation error, and the complexity of the distilled network. The paper provides generalization bounds for seemingly complex neural networks on the basis of much simpler ones. Different from the traditional error analysis, this paper focuses on bounding the divergence bettween the test error and the training error by the the corresponding distillation error and distillation complexity, e.g., test error  is bounded by training error + distillation error + distillation complexity. Essentially the bounds proved, bound the out of sample error with a form of in sample error, average difference in predictions between complex and simple network, and complexity term for the simple network in terms of Rademacher complexity. Overall, I think this is an interesting paper and should be accepted.
This paper presents Gauge Equivariant Mesh CNNs. The method is motivated by the fact that graph convolutions can be modified for meshes to take into account the angular arrangement of local neighborhoods. The result is a Mesh-CNN that is equivalent to GCNs with anisotropic gauge equivariant kernels. It achieves that by parallel transporting features along edges and spanning a space of gauge equivariant kernels. The paper contribution is elegant and significant: Gauge equivariance  is a necessity if you want an anisotropic diffusion. Weaknesses: Neither the MNIST nor the FAUST experiment verify the gauge equivariance. While it is plausible and reasonable to model such data using vector features of type ρi in the hidden layers, the argument for the necessity of gauge equivariance would be even stronger if the input and/or output signal was itself vector valued, for example a velocity or gradient on the mesh. Arguably the FAUST shape correspondence data addresses this issue better since the signal is inherently linked to the geometry. My recommendation is positive because the community needs principled ways of convolution on non-Euclidean domains, and this paper seems to make an incremental contribution towards that direction. The paper would benefit from other experiments on manifold like performing mesh convolutions for human or object reconstruction from images. Therefore, I would highly welcome one additional comparison on a different mesh task.
A practical consideration remains. Though theorem 4.1 reduces construction of a basis of steerable kernels to 1) finding Clebsch-Gordon decomposition of tensor products, 2) describing endomorphisms of irreps, and 3) describing harmonic functions, none of these problems is trivial (or even necessarily solved) for a general compact group G. However, in that case, we are still back to solving the problem on a group by group basis. Given that the purpose of this paper is partially to formulate Steerable CNN in precise terms, the fact that δx is informally considered as in L2(X) is very imprecise. To summarize, a serviceable reference work which will probably be made obsolete by the appearance of a proper textbook on group theory in ML before long. theory and so it is not necessary to use physics here to describe steerable CNN. In this way, both steerable CNN and quantum mechanics are applications of rep.
The proposed continuous convolutional layers can be directly applied to input Gaussian Process in a closed form, which subsequently outputs another GP with transformed mean and variance. If M is also fairly large, you can argue that f is close to a GP, following the theoretical arguments from the infinitely wide NN literature (Matthews et al., 2018; or generalizations thereof such as Yang, 2019). The authors repeatedly discuss that their approach does not require any discretization unlike some the previous approaches based on GP(mentioned in Section 2). Gaussian processes (GP) are used to represent the irregularly sampled input. The main technical contribution seems to be the definition of a continuous convolution on GPs with an RBF kernel.
Under this generalized linear setting, they propose a so-called "optimistic closure" assumption which is shown to be strictly weaker than the expressivity assumption in the conventional linear setting. The paper then proves that LSVI-UCB still enjoys sub-linear regret in the generalized linear setting with strictly weaker assumptions. The authors studies an episodic MDP learning problem, where they propose to study an Optimistic Closure assumption which allows the Q function to be expressed as a generalized linear function plus a positive semi-definite quadratic form. I am in favor of acceptance, given that it provides a non-trivial extension to what is known and the Optimistic Closure assumption seems to me to be closer to the reality than the linear MDP assumption.
--Summary: They proposed a robust method for the adversarial attack on VAE using a hierarchical version of β-TCVAE and conduct analysis on the relationship between disentanglement and robustness to support their choice of approach. The experiments are sound and well documented (results are reported across latent space dimensions, and adversarial attack parameters) They demonstrate that the proposed method is more robust to other VAE baselines for the attacks. (Update): The score has been updated after a rebuttal from the authors.
Based on these, the authors show that it is possible to upper bound the likelihood of reaching an unsafe state at every training step, thereby guaranteeing that all safety constraints are satisfied with high probability. The main idea is to formulate the safe RL problem as  a CMDP problem, but with worst-case bounds to ensure that the safety constrained is guaranteed throughout the learning. The authors formally show that it is possible to upper bound the expected probability of failure during every policy update iteration (Thm1), which is a non-trivial result. Furthermore, the probability bounds proposed in Theorem 1 and 2 seem to be rater weak bounds, since ξ is bounded by a term inversely proportional to the confidence parameter ω, which means the probability of being safe is high when the safety bound is loose, i.e., χ+ξ, for ξ→+∞ . The authors answers one of the most important concerns, I have raised score to 6.
The impact of memory placement for DNN has been clearly motivated and is easy to appreciate. This would be useful to get an idea of how EGRL fairs against [3] and [4] which also trained on real hardware and took many hours to finish training the policy. Summary: This paper proposes a new algorithm called EGRL to improve computation graph running time by optimizing placement of the graph's components on memory. Le, and Jeff Dean. A hierarchical model for device placement.
They then determined parameters for the modified ReLU that would minimize the deviation between these activation functions, and computed the minimum conversion error (for converting ANN -> SNN). To achieve their goal, they described the spiking neuron non-linearity by a "staircase" function of the input (spiking output increases by 1 each time the input gets big enough to reach the next stair), and related that to the ReLu function used in the non-spiking neural net. Summary The authors suggest a relationship between a leaky relu and a spiking integrate and firing neuron model. This scales with the square of the threshold voltage for spiking, divided by the simulation time. Using this, they defined their procedure for training SNN to mimic ANN as follows: they trained the ANN with their modified ReLU (which is closer to the SNN activation function but more readily differentiable), and then used the weights from that ANN in their SNN. Nice performance was obtained in all cases: better than using a normal ReLU, or other comparison activation functions, in the "target" ANN. I left a comment to the authors in the discussion below and they appropriately addressed my new recommendations. (2) This work significantly reduces the simulation time since long simulation time is usually required for converted SNN to reduce error.
The optimal transport problem is first written equivalently as a minimax problem over set of convex functions, as in Makkuva et al. 19. To further explain my concerns, let me start with section 3 which reviews the dual formulation to 2-Wasserstein distance in Eq. (8) and also the connection to the convex conjugate optimization in Eq. (9).
The limitation/assumption mentioned in weaknesses 1 must be sufficiently disclosed in section 3.2. Comments: It's interesting to see the availability of only 1~5 samples per class for OOD data can result in some noticeable gains. When training data is unlabeled, it performs significantly better than existing OOD detection algorithms based on AE, VAE, PixelCNN++, Deep-SVDD, and Rotation-loss and it performs only slightly worse than the labeled case. There are a number of studies on self-supervised outlier detection approaches as well as what is called few-shot outlier detection approaches, but I cannot find any discussion of those work and the empirical comparison to these methods. Authors say "higher eigenvalues dominates Euclidean distance but are least helpful for outlier detection", but wouldn't too small eigenvalues be less helpful for outlier detection because they are more sensitive to noise? Some closely related methods are: self-supervised methods such as GT and E3Outlier that learns feature representations using a pre-text task in a self-supervised way; unsupervised outlier detection methods such as RDA, REPEN, ALOCC, OCGAN, etc.; methods that use a few labeled outlier data such as Deep SAD, DevNet, REPEN, etc.
In addition to proving concrete results comparing the Laplace, NTK, and exponential power kernels, it is serves as a proof-of-concept for potentially using the tools of singularity analysis to understand neural networks. The authors also provide numerical results showing the empirical rate matches the theoretical asymptotic rate of the Maclaurin coefficients of the Laplace kernel and NTKs. Overall, I think the paper is well-written, and the proof is solid. Since *CONF* is a highly selective conference,
This paper studies how to improve the worst-case subgroup error in overparameterized models using two simple post-hoc processing techniques: (1) learning a new linear classification layer of a network, or (2) learning new per-group threshold on the logits. Specifically, the paper demonstrates that this result is not necessarily due to overparameterized learning poor representations for rare subgroups, but rather mis-calibration in the classification layer that can be addressed with two simple correct techniques: thresholding and re-training the classification layer. I enjoyed this paper, and I'm keeping my score unchanged. Recommendation:I recommend acceptance. While I remain concerned about the limited scope of the experiments, I believe the paper adds valuable insights to the overall important topic of robustness / worst-case generalization.
Summary The paper proposes a defense against recent flavours of model stealing attacks by exploiting the insight that the recent effective attack query out of distribution examples to the victim model. Pros: The idea of using ensemble of diverse models to create discontinuous predictions on OOD datasets is interesting. If the adversaries have access to the auxiliary OOD datasets or use other complex OOD datasets, will the defense still work? This is also shared by some other reviewers.
Variational video prediction is used to generate a sequence of segmentation masks. ---- Summary ---- The paper extends video-to-video translation model of (Wang'18) to video prediction by first generating a sequence of segmentation masks and then translating them into videos. =======================================Post-rebuttal Comments=============================== I appreciate the revisions and additional results reported by the authors.
This work proposes the first collective robustness certificate that considers the structure of the graph by modeling locality in order to derive stronger guarantees  that the predictions remain stable under perturbations. A novel collective certificate fusing single certificates into a stronger one, is proposed by explicitly modeling local structure of input data using graph convolution node classifiers. -originality & significance: it is the first attempt in considering collective robust certificates (CRCs) by fusing individual adversarial certificate. The arguments are valid on the limitations of independent based certificates for collective tasks. Pros: This is the first effort that considers collective robustness certificate. In summary, I like the novelty of this method and the through experiments that were conducted that illustrate the efficacy of the proposed collective certificate, thus I recommend an accept.
The paper studies (1) the relationship between the flatness of minima and their generalization properties, and (2) the connection between two measures of flatness, known as local entropy and local energy. The paper discusses, at length, two previously proposed algorithms named Entropy-SGD and Replicated-SGD and demonstrates, using (i) controlled experiments where Belief Propagation (BP) can be used to estimate the local entropy integral precisely, and (ii) empirical results on deep networks that flatter minima generalize better. They also empirically show that Entropy-SGD and Replicated-SGD, when used to explicitly optimize the local entropy, are able to flatter and better minima (in terms of lower generalization errors). I like the section on measuring flatness for shallow architectures using BP. (4)" and "Eq. (4)". Update after response: I appreciate the authors making their contributions clearer, and adding details about the training loss and error. I am recommending a weak accept but I am willing to increase my score if the authors make a convincing case against this concern.
[1] Yang Liu and Hongyi Guo. Peer loss functions: Learning from noisy labels without knowing noise rates. In Advances in neural information processing systems, pp. And the analysis for choosing β does not dependent on their instance-dependent noise settings. All in all, with the lack of novelty addressed above, I think the submission is marginally below the acceptance threshold. iv) One motivation of Confidence Regularizer is that confident prediction counters the overfitting of noise labels. I would appreciate if the authors of the paper could provide further insights and intuition on why the introduced confidence regularization improves noise robustness.
Summary This paper presents a new method for structure pruning called ChipNet. The ChipNet employs continuous Heaviside function with commonly used logistic curve and crispness loss to estimate sparsity masks. This paper proposes a new deterministic pruning strategy that employs continuous Heaviside function and crispness loss to identify a sparse network out of an existing dense network. ########################################################################## Reasons for score: Overall, I vote for marginal acceptance.
The paper appears to make some novel links between generalization and decision boundary diffusion geometry, offers an apparently novel analysis of the impact of common adversarial defenses to Brownian adversaries, and at a minimum offers some new insights into how we might think about, and interpret, complex decision boundaries learned by neural nets or other nonlinear classifiers. Title: HEATING UP DECISION BOUNDARIES: ISOCAPACITORY SATURATION, ADVERSARIAL SCENARIOS AND GENERALIZATION BOUNDS
Theorem 4.1 provides a theoretical guarantee on the performance of a policy trained on such a modified reward in the source domain by giving a bound on the performance in the target domain, under a very mild assumption that the optimal policy on the target domain achieves similar rewards when put in the source domain. The idea is to modify the reward function in the source domain so the learned policy can be optimal in the target domain. Summary: This paper introduces DARC, an RL approach that aims to transfer from a source environment to a target environment with different dynamics. In fact, Assumption 1 is trivially met in most of the environments used in Fig 6, e.g.: for half cheetah obstacle, the target policy does not run in to the obstacle and hence will get the same reward in the source and target environment. Having read through the other reviews and responses by the authors, I feel that most major concerns have been addressed. As such I am inclined to increase my score from 7 -> 8, recommending acceptance of the paper and entrusting the authors to include the new experiments in the main paper.
Based on their findings, they propose a new supervised pretraining method, which has a good trade-off for transfer learning applications, and validates with other contexts such as few-shot classification and landmark localization. However, I do think that this work is worthwhile for the community because 1) it shines light on a somewhat mysterious exciting new technique and 2) already shows how the findings are useful by using it to improve supervised pretraining, and a new vocabulary for evaluating pretraining techniques. Reasons for score: Overall, I vote for accepting.
This is an interesting adaptation function that evokes the NTK and in the process can help approximate a k-order inner gradient and yet be free of the computational difficulties that come due to this in the standard MAML systems. In first algorithm, no explicit adaptation function is used, whereas in the second, a close form adaptation function which invokes the NTK is proposed - which is a simpler adaptation than that of MAML and hence, offers computational efficiency. I am particularly impressed by the second algorithm, Meta-RKHS-II, which derives a closed form-solution to gradient-based adaptation in RKHS that they then map back into parameter space via NTK. The work is interesting and  supported by theory inspired from the NTK theory, and adds to the newly expanding literature in the use of kernels in meta-learning (unlike the authors' claim in the introduction, theirs is not the first meta-learning paradigm in the RKHS cf (Wang et al 2020, Cerviño et al 2019)). Can this robustness be better explained or mathematically analysed in terms of the RKHS or the NTK? The gist of this algorithm is to convert MAML into a multi-task objective with a "regulariser" that tries to maximise the gradient norm at initialisation. Thus, I recommend acceptance. Strengths This paper is generally well written and proceeds to develop insights into gradient-based few-shot adaptation on first principles from NTK theory.
In particular, through a series of experiments, the paper investigates their uncertainty properties and answers the question "how calibrated are the predictive uncertainties for in-distribution/out-of-distribution inputs?": i) a comparison between GP classification with the infinite-width neural network kernels and finite width neural network classification was provided to test the calibration, Paper summary The authors empirically investigate the calibration performance of NN-GPs in CIFAR10 and several UCI data sets, in three forms: Bayesian inference for the NN-GP function-space prior, through a softmax link function (ii) a study of these kernels on regression tasks where the GP posterior can be obtained exactly, The stated goal of this paper is very ambitious: how NNGPs provide better confidence prediction, in terms of calibration, OOD data  and distributional shift. The key innovation presented in section 5: the NNGP is just added as a calibration layer on top of a pretrained NN. Thus, I reluctantly recommend acceptance. It isn't clear what the infinite width layers give you that's better than either using Bayesian linear regression over the output weights, or deep kernel learning. High-level comments I think empirical work like this paper is important: we Bayesians like to justify ourselves using calibration, but unless the beautiful Bayesian methods are actually calibrated, they are not useful.
Experiments on a simulated dataset with a flying drone in a subway and living room environments demonstrate good SLAM performance (that approach traditional methods): bird's eye view projections of the 6 DoF poses and the emitted maps closely match the ground truth poses and the occupancy grid. Strengths The work builds on a fundamentally new and interesting line of generative variational approaches to SLAM The probabilistic graphical model considers the observations, dynamics and latent states of the agent (i.e. the pose and dense map). The approach in [1] formulates the fully differentiable dense SLAM problem (including ray casting) on real world datasets, [2] is a recent novel 3D rendering approach and also does differentiable ray casting,  [3] demonstrates an end to end approach to learning measurement likelihood models with an RL active localization framework, and [4] which is also uses a clever combination of deep learning and multi-view geometry to produce dense 3D maps. The reason I consider this a weakness is that many existing RGB-D SLAM approaches (even traditional ones like ORB-SLAM2) have very high accuracy and in some sense can be considered a "solved problem". The authors summarize the whole collection of such approaches with a single sentence: "depth or semantic representations with existing SLAM methods has also become a prevalent direction of research".
This paper tackles online continual neural network learning (following Lopez-Paz & Ranzato, 2017) with a combination of techniques: (1) a controller (or base parameter modulator, or hypernetwork) is introduced which produces task-specific scale and shift parameters, which modulate the feature maps of a base model (Perez et al., 2017); The authors introduce Contextual Transformation Networks (CTNs), a replay-based method for continual learning based on a dual-memory design and a controller that modulates the output of a shared based network to task-specific features. Some good additional experiments are also provided (different memory sizes, smaller datasets, ablations of the three major parts of CTNs). The method achieves strong performance on appropriate benchmarks; hyperparameter tuning is handled carefully, which is not always the case in continual learning studies; the authors include a comparison to many relevant methods. Pros: Results are generally strong in comparison to other memory based CL techniques on accepted benchmarks. But CTN has very strong performance in continual learning benchmarks. Cons of paper There are related works that I think the authors can mention, which have some similar ideas as in CTN (although all the ideas are never all put together as in CTN): (a) FiLM layers for meta-learning / continual learning / multi-task learning: [1] Requeima et al., 2019, "Fast and Flexible Multi-Task Classification using Conditional Neural Adaptive Processes" I do not see the intuition behind many of the design considerations in CTN: there is essentially a shared base network, with task-specific scaling/shifting (given by a controller network) and task-specific heads; a key difference to previous works is the use of two different memories for the two networks.
--> an additional baseline "DisentanGAIL w/ domain confusion loss & prior data" is needed, which additionally trains the domain confusion objective on the prior data collected for the DisentanGAIL prior regularization objective, to allow for fair comparison of both regularization approaches with access to the same data missing baseline results on harder tasks: on the harder tasks shown in Fig 3 there is no evaluation of the baseline methods, which makes it hard to judge how hard these tasks actually are for prior third-person visual imitation approaches This paper proposes a visual imitation learning algorithm that can handle domain shifts between the expert demonstrations and the data generated by the agent. [3] Kim, Kuno, et al. "Domain adaptive imitation learning." arXiv preprint arXiv:1910.00105 (2019). Particularly the clarifications about the usage of prior data in the baselines were very helpful and the added results with background differences are interesting! what differences result in the substantial performance difference between TPIL and DisentanGAIL w/ domain confusion loss? Weaknesses not fully fair comparison to baselines: since the main novelty lies in the introduction of novel regularization objectives, the "DisentanGAIL w/ domain confusion loss" is the main comparison method since the only difference to the proposed method is the representation regularization function. However, it seems that DisentanGAIL w/ domain confusion loss, which applies the MI=0 loss from Stadie et al. 2017, works well on many of the tested domains. If I understand Figure 3 correctly, the orange line is without any regularization (lower bound) and the green is learning without domain differences (upper bound). It motivates well the distinction between domain information and goal-completion, and highlights why previous methods like domain confusion loss can fail. "Domain-Adversarial and-Conditional State Space Model for Imitation Learning." arXiv preprint arXiv:2001.11628 (2020). Suggestions to improve the paper add an additional baseline "DisentanGAIL w/ domain confusion loss & prior data", as discussed in the "weaknesses" section, particularly for the transfer tasks on the bottom right of Fig 2 in which the discrepancy between DisentanGAIL and the baselines is the largest add evaluation of baselines (particularly DisentanGAIL w/ domain confusion loss (w/ and w/o prior data)) to the harder manipulation environments in Fig 3 to show the benefits of the introduced regularizations since the proposed method addresses a concrete problem of prior work (as explained in appendix Section A) with a clear intuition, it could be nice to add a toy experiment early in the paper that demonstrates this effect empirically for an easy-to-analyze imitation problem, showing that for MI=0 the agent cannot properly learn to imitate since it is unable to capture the relevant information --> since this is a different assumption from prior work on cross-domain imitation it would be good to mention this earlier, maybe in a dedicated "Problem Statement" section for qualitative matching results in Fig 4 in the appendix it would be nice to show the corresponding matches found when using the domain confusion loss instead of the proposed regularizations to see whether some of the failure cases are interpretable I wonder whether it would be possible to show imitation across agents with more drastic morphology differences in the most challenging 7DOF robotic manipulation tasks.
As strategies for negative sampling in audio and videos are different, it proposes to sample negatives that are similar in the ConvNets' embeddings. It builds on a line of research on multi-modal video understanding that utilises transformers where these works: 1) fix one of the transformer models (e.g. BERT) and 2) utilise tokens and thus do not train the approach in an end-to-end fashion. The main contributions are the proposed parameter sharing and negative sampling strategies. e) sharing position encoding parameters between modalities and transformer layers f) decomposing transformer weights, so some are distinct and others are shared For example, [1] uses an audio-visual transformer for audio event classification, and [2] proposes a new joint audio-visual transformer module with both self-attention and cross-modal attention. This makes the readability of the experimental section below acceptable bar IMO.
The work first identifies the challenges with the current landscape of Masked Language Models with limits to learning sentence-level representations and semantic alignments in sentences of different languages. Summary: The paper presents HICTL which enables models to learn sentence level representations and uses contrastive learning to force better language agnostic representations for large multilingual encoders. Contrastive losses are promising and the paper shows positive results when adding them to the previously proposed XLM-R model. Finally, the authors initialize from XLM-R and fine-tune on 15 languages with both monolingual and parallel data. Why did you not make an official submission to the XTREME leaderboard?
The motivation that graphical neural networks are bogged down by their message passing framework is not necessarily a motivation for using transformers. The author claims that the SMP paper does not work better due to the morphology encoding and then they point out that it instead works because of the encoding of the subtrees and some specific detail related to message passing. The selected approach and the authors findings are meaningful in a neuroscientific context, as the transformers approach better resembles the function of a human brain. Pros Good framing of the problem and choice of experiments. One can do state-dependent message passing in such architectures without the transformers (see DICG [1] for an example with attention and graph convolutions). In other places, the paper contrasts transformers with GNN-based methods ("substantially outperforms GNN-based methods"), as if transformers were not GNNs. To avoid confusing readers, it would help to explain that GNNs are a broad class that includes both transformers and SMP, which differ in their message passing schedules, etc. Suggestions The paper mentions in passing that this work involves agents "with each non-torso node having an action output". As the paper explains, "transformers can be seen as GNNs operating on fully connected graphs". This limitation probably deserves to be highlighted more prominently.
This paper introduces an algorithm, called deep reward learning by simulating the past (deep RLSP), that seeks to infer a reward function by looking at states in demonstration data. The gradient estimator involves simulating a possible past from a demonstration state (using a learned inverse policy and inverse transition function) and then simulating forward from the possible past (using the policy and a simulator) The gradient is then the difference between features counts from the backward and forward simulations. [1] Reward Learning by Simulating the Past (RLSP) This paper studies the question of learning rewards given only certain preferred or terminal states. (4) Based on an educated guess of the problem setting (per my understanding), an intuitive and perhaps simpler algorithm would be to learn a goal classifier using the provided {s_0} data, i.e. P(s=goal), as well as a forward dynamics model P(st+1|st,at). Methodologically, the paper stays close to the ideas of RLSP, but instead of computing relevant quantities (optimal policy, forward and inverse dynamics) through explicit derivations, it suggests most exact steps can be replaced by leveraging deep learning, reinforcement learning and self-supervised learning. The idea is to train a reward function that explains both the past trajectory and the futur trajectory from that state, assuming that the very goal was that state (hence, the assumption of larger rewards in the past than in the future induced by the gradient). To achieve this, the paper assumes a Boltzmann distribution on the demonstration policy and a reward function that is linear in some pre-trained state features. The paper is generally clearly written and works on a crucial problem in reinforcement learning, namely how to specify a human preference without resorting to tedious reward engineering. I am open to revisiting the recommendation based on author feedback.
In contrast to the Kanerva Machine, the authors simplify the process of memory writing by treating it as a fully feed forward deterministic process, relying on the stochasticity of the read key distribution to distribute information within the memory. The paper proposes a generative memory (K++) that takes inspiration from Kanerva Machine and heap memory allocation. Specifically, this paper proposed a novel memory allocation scheme,  replacing the stochastic memory writing process in prior works KM[1]  and  DKM[2] with a set of deterministic operations. 1508-1518. 2018. This paper proposes a new memory mechanism based on the Kanerva Machine inspired by computer heap allocation. Strengths: They designed a new Kanerva Machine having a simplified writing mechanism and sharable part-based memory. Pros: The authors combine the idea of differentiable indexing in Spatial Transformer (Jaderberg et al., 2015) into the memory of Kanerva Machine (Wu et al, 2018a;b) and prove by experiments that this allocation scheme on the memory helps improve the test negative likelihood. Fig. 5, How does the memory trace contribute to the read model as mentioned in Sec. 3.4?
This paper proposes a novel framework called VA-RED2 to reduce spatial and temporal features to be computed for video understanding, which can reduce FLOPs when inferencing the video but remains the performance. The performance is promising on most video networks. Cons: This submission proposes to address the problem of slow video inference, but the only metric they report is the Gflops. RubiksNet: Learnable 3D-Shift for Efficient Video Action Recognition, ECCV2020 For the video action recognition task, experiments are carried out using Mini-Kinetics-200, Kinetics-400, and Moments-In-time datasets. The authors have done extensive experiments on video action recognition tasks and spatio-temporal action localization task in the area of video understanding.
Summary This paper proposes a differentiable architecture search approach for splitting a deep network into locally-trained blocks to achieve training speedup. In the spirit of recent trends in greedy layer-wise and indirect training, SEDONA allows gradient information to flow either from the next layer as in backpropagation or from an auxiliary head, trying to make a prediction using the current layer's output. -> "If local signals are not representative of the global goal" The paper proposes a method for decoupled training of neural networks called SEDONA. Do they correlate positively? To what extent can such decoupled feedback implement credit assignment in a neural network? Strengths:  The idea to search auxiliary network for decoupled neural network is novel, and the proposed method is verified on multiple widely used datasets. -> "to flatten the learning landscape", Section 4.1: "to let auxiliary networks in the pool computationally lightweight"
Summary: This work tries to find a compromise of model-based and model-free methods, using a teacher and student network . Typos: using only using model-free methods This paper presents a student-teacher framework, where the teacher network can be used to select and prioritize the relevant properties of the given dynamical system that should be learned by the student. It uses a teacher model to learn to interpret a trajectory of the dynamic system, and distills target activations for a student model to learn to predict the system label based only on the current observation. This paper proposes a teacher-student training scheme to incorporate the useful information of trajectory to improve the predictive performance of model-free methods. The teacher network tries to "guide" the student network at the training stage by presenting an interpretation of the trajectory. Overall, I have a mixed feeling about this paper and I currently stand between scores 5 and 6. UPDATE: My major concerns were addressed in the revised version of the paper. I get that the proposed framework avoids the problems of model-based and model-free methods, but I am having difficulties identifying what advantages of the two methods that the framework is incorporating. The argument should then be if the proposed method outperform model-based methods given the same complexity (e.g. the number of parameters) or same amount of data.
First, the authors derived two sufficient conditions for equivariant architectures with the universal approximation property. The universal approximation property for equivariant architectures under shape-preserving transformations is discussed. For instance, it would be great to provide a simple implementation of the minimal universal architecture and show it indeed achieves the rotation equivariant features on the point cloud data. Recommendation: The authors proof the useful statement of universality of a prominent class of neural networks, which is why I recommend the acceptance of this paper. Post rebuttal With consideration of the authors' responses to reviewer questions and revisions to the submitted work I have changed my rating to clear accept.
The paper proposes a novel Channel Tensorized Module (CT-Module) to construct an efficient tensor separable convolution and learn the discriminative video representation. This paper presents a new CNN module to learn video feature representations for action recognition, with a particular focus on increasing channel interactions for spatio-temporal modeling. Paper strengths I think the ideas proposed in this paper are interesting and I would not be aware of any architecture that would use a similar arrangement of tensorization and attention in the mid-layer part to allow for a lightweight 3D convolution architecture. By decomposing the channel dimension into sub dimensions in the typically 4D video data (Time, Channel, Width, Height), one defines spatial-temporal separable convolution for each sub-dimension. The visualization supports the claim that the attention mechanism seems to learn to focus more on relevant parts of the video clip. Addressed Concerns: corrected the typos. Majority of the weaknesses are addressed. ii) The ablation study could have included the comparison between only channel tensorization and channel tensorization and self attention, which would highlight the new contribution of the paper.
Summary: It is shown that Dale's principle can be observed in feedfoward ANNs if one uses inhibitory neurons in the form of feedforward inhibition, while the other neurons are purely excitatory. Although, I find the contribution interesting, my enthusiasm is tempered by the following two issues: Although feedforward inhibition has its place in the brain, most connections of inhibitory interneurons with excitatory neurons are reciprocal, resulting in feedback inhibition. Most neurons in the brains are either excitatory (E) or inhibitory (I) - sometimes referred to as Dale's law. It appears to be useful for understanding the design of biological neural networks, and at least one type of uses of inhibitory neurons in them. 2. The ingredients in the proposed model is well motivated in neuroscience, such as the feedforward inhibition, and E/I balance, as well as no connections between I neurons across the different layers. Note that this does not mean that the action of a neuron is always excitatory or always inhibitory on all of its post-synaptic partners.
In particular, it focuses on the expected loss reduction (ELR) strategy, analyzes its problem, and modifies the original ELR method to make sure the active learner converges to the optimal classifier along learning iterations. The proposed algorithm can dominate the random and ELR. Strong point: The paper's finding on the existing ELR method is interesting and novel. Pros: This paper was well motivated by the drawbacks of ELR and insightful comparison of BALD and ELR. The stuck in the convergence of ELR can be due to the lack of considering the long term effects. Major Remarks : Although the authors provide some proof that the weight function can solve the long-run convergence issue, I wish they would provide intuitions as to why their particular choice can choose inputs that will be beneficial in this regard.
In contrast to existing works on continuous time ODE formulations with graph structures, the proposed networks incorporate relative spatial information in order for the network to evaluate spatial derivatives in addition to the temporal dynamics. A model for discrete vector y(t) is proposed in the form of coupled ODEs (one for each x_i) with a sparse coupling arising from a neighbouring graph on spatial inputs x, and sharing the same transition function. As mentioned above, previous methods had already proposed the usage of graph neural networks with continuous time for the learning of differential equations, and I am not sure that the addition of spatial mesh information to such a graph neural network constitutes a significant enough modification at this point. Previously proposed methods either would not work on continuous time, or unstructured grids, or would not be applicable to settings with unknown governing PDEs. This work combines all these features. I hope the authors can shed light on this aspect during the rebuttal, as apart from this relatively central open question I like the paper. Overall, given the "cons" described above, notably the potential lack of strong novelty in the proposed method, and the lacking experimental description and results, I am for now classifying this paper as marginally below the acceptance threshold.
With SPPs in hand, they find two key failure modes at initialization in deep ResNets and then develop Normalizer-Free ResNets using Scaled Weight Standardization, achieving competitive performance on ResNet-288 and Efficient-Net. Strength: --The overall idea makes sense and the proposed method of removing batch normalization can reduce computational resources and speed up computing greatly. UPDATE: The author has addressed most of my concerns, but regarding the motivations and the benefits for the community, I still keep my score. --update--- I am upgrading to 7: Good paper accept; as my concerns have been addressed by the additional experiments. (3)    It's better to add some accuracy comparisons with other removing batch normalization works. Can you give the visualization of the same indicators of other initialization methods like Fixup initialization and make some comparison? SPP could enlighten that an unusual ReLU-BN-Conv ordering would have some benefits but why should a network without normalization mimick the SPP trend of ResNet?
In particular, the new hypothesis seeks answer to the required amount of over-parameterization for a randomly initialized network to become able to compress to a sparse untrained binary subnetwork with on-par accuracy. As binary networks can largely reduce the computational complexity for inference, this work has practical importance especially for applications with constraints for memory and power. This work has some novelty, in the sense that I haven't seen any other papers on untrained binary neural networks. I am still wondering the difference of biprop vs.
Summary The authors present the idea of adaptive stochastic search as a building block for neural networks, as an alternative to other "inner loop" optimization methods like gradient descent. Pros : Using adaptive stochastic search allows the inner optimization module to take multiple iterations without unrolling the computation graph (unlike meta-learning methods), since the initial value used by the module is arbitrary, and not provided by the network. This paper proposes using adaptive stochastic search as an optimization module within deep neural networks to perform general non-convex optimization. But the approach certainly adds variance compared to having an exact gradient or unrolling many optimization steps. I do not recommend to accept the paper. Perhaps the authors could add at least some analysis of the variance of their gradient estimator compared to other gradient estimators for embedded optimization problems for different examples and show how the variance behaves depending on the difficulty of the optimization task (dimensionality, curvature) taking the above perspective into account. Since the authors explicitly treat nonconvex optimization problems they could also make more explicit that a nonconvex optimization problem does not necessarily have a unique optimum. More analysis about the quality of the gradients obtained by the suggested method compared to other methods would improve the paper: How many unrolling steps do what to the gradient variance? What is the difference of using backprop through gradients of the loss vs stochastic search gradients (log likelihood trick) gradients for different dimensions of the optimization variable?
It then proposes an alignment measure which correlates with generalization for different initial scale. Summary: This paper investigates the role of scale in generalization of neural networks. Overview The paper studies how the generalization of the neural network trained with SGD is affected by the scale of the random initialization. Summary of paper: A series of empirical observations are made about the influence of scale of init on generalization (in particular, that a continuum of generalization performance from random to very good can be generated by varying only the scale of init) , and these effects are explained in detail for different activation functions. Reasons for score: Overall, I find the paper to be a bit borderline. The combination of things is too much for me to recommend acceptance out of the box, but the things are relatively small and I think easy to address, and I'd be happy to increase my score. I strongly recommend the paper to be accepted.
This paper proposes ANT to solve the problem of learning Sparse embeddings instead of dense counterparts for tasks like Text Classification, Language Modeling and Recommendation Systems. The sparse matrix T can also encode domain knowledge (e.g. knowledge graphs). In step 2, they learn a sparse matrix that is used to relate all tokens to the set of chosen anchors. The authors provide a statistical interpretation of their approach using a generative formulation to the embedding vectors in terms of the latent vectors (using a Indian Buffet Process membership matrix Z). They took a two step approach: in step 1, they learn "full fledged" embeddings for a subset of anchor tokens. To be specific, there can be a variety of knowledge (like related, is a subset of, analogy, etc.). 2) they use a sparse T matrix to relate other tokens to anchors which again has reasonable prior: the meaning of a word can be efficiently defined by a few good chosen anchors. Although this paper is probably related to other strains of research (e.g. leaning manifolds for IR/NLP where anchoring is also a key concept, which the authors could have admittedly surveyed more), I particularly liked the fact that the two-step procedure decomposes two tasks that are often mixed together for embedding tasks: learning representation vs learning relations. What I agree with the authors are: i) Using properly chosen basis vectors may greatly reduce the memory cost for embeddings, especially for huge vocabulary sizes (e.g. over 100 million).
A5) For parametric least squares with a mis-aligned ground truth parameter, it is shown that early stopping with NGD achieves lower Bias than any other pre-conditioned gradient descent (Proposition 6). The main preconditioner that is studied in addition to vanilla GD is the (population) Fisher matrix (natural gradient descent or NGD), its empirical counterpart, and its interpolation with GD. A3) For non-parametric regression, gradient descent pre-conditioned with the inverse regularised population covariates covariance is considered. "On the Optimal Weighted ℓ2 Regularization in Overparameterized Linear Regression" NeurIPS 2020 This makes the paper a strong contribution, and I am in favor of acceptance.
The benefit of using a biased compressor is its low variance; this is what improves the performance. Empirical results show that, an induced compressor obtained from a class of biased compressors (e.g., top K gradients) has better performance than a biased compressor from that class with the same communication cost. In experiments (Figure 3), authors compare various methods with one set of compressors (1) Rand-K; (2) Top-K + EF; (3) induced compressor with C1 = Top-K/2 and C2 = Rand-K/2; (4) Top-K, where K is a tunable parameter but is the same among all compressors. I suggest the authors citing these papers and highlighting the differences.
This work considers Delay Differential Equations instead of ODE, which allows to implement more complex dynamics and thus achieve estimation of more complex functions. However, in fine, the average performance is nearly of the same order, a significant difference only on MNIST but with a very bad score considering this is an easy problem. Also, modeling the initial function as an ODE is mentioned, and I wonder whether the implication is that this would involve learning the initial function as a NODE itself. Some of them could be easily improved I think, others call for further  work. This is a promising direction for the community to go to push past current ODE modeling limitations, but I recommend for rejection in the current form due to improper characterization and evaluation with respect to prior work (more details below).
Originality and significance aspect This paper combines mainly two ideas 1) classic feature selection (choose Xis to drop) with respect to the mutual information (between X and Y) 2) Information Bottleneck (IB) formulation that maximizes the prediction-term mutual information term (between Z and Y) and minimizes the compression information (between X and Z) simultaneously. The paper proposes a new Information Bottleneck objective, which compresses the latent by learning to drop features similar to DropOut. Unlike DropOut, a different probability is learnt for each latent feature/dimension using Concrete Relaxation. Summary: This paper proposes an information bottleneck method, Drop-Bottleneck, that allows the input to be compressed by dropping each input feature with probability p_i. Experiments show that DB works better than VIB in VizDoom and DMLab when a noisy-TV noise is added to the input images. Here are a few questions to authors: What if we just drop the feature space only using the mutual information between X and Y and drop them to achieve a similar number of features that was resulted by DB -- it is basically the classic mutual information feature selection. DB cannot provide the same generality as other IB objectives: the input (latent) has to be sufficiently disentangled already as the objective itself does not encourage further disentanglement by itself. The paper does not perform experiments on datasets with meaningful features where a feature selection makes more sense than for specific pixels in images. The paper does not discuss connections of the presented approach to prior works for (discrete) feature selection. Overall, I score this paper as an accept.
Specifically, this paper finds that only partial parameters (critical parameters) are important for fitting clean labels and generalize well; while the other parameters (non-critical parameters) tend to fit noisy labels and cannot generalize well. Using comprehensive experiments on synthetic datasets and real-world datasets, the authors verify that the proposed method can improve the robustness of the classifiers against noisy labels. I recommend to accept this paper, and hope that the authors can address the above issues carefully. Since the validation set is also noisy, why does the early stopping criterion adopts the minimum classification error on it? ------Issues------ I only find the illustration of comparison between CE and CDR in the case of noisy CIFAR-100. The proposed method implicitly exploits the memorization effects of deep models, and can reduce the side effect of noisy labels before early stopping. The major comments and issues are as follows: ------Major comments------
2019. The authors address neural architecture search (NAS) scenarios. The paper proposes a framework to generate good architectures according to the datasets. In particular, a framework, MetaD2A, is proposed, which yields a neural architecture for a new dataset. The experiments demonstrate the usefullness of the approach and its improvements over conventual NAS approaches. Similar graph decoder is proposed in previous NAS works [1] and performance predictor are proposed more times. For instance, they can replace hierarchical set pooling with flatten set pooling and then see the importance of different component. Negative: The authors claim that NAS with meta learning has only been done with small datasets in the past. Overall Review: This paper proposes a new scene of fast adaption of NAS, which may be a good direction of NAS & meta-learning.
The authors present the Bayesian Aggregation (BA) mechanism in the context of Neural Processes (NPs) for aggregating the context information into the latent variable z in the form of posterior updates to z. To summarize, in the context of neural processes I feel the paper makes good methodological contributions in presenting a much cleaner and more natural (from a Bayesian perspective) version of the model that has more of the flavor of standard amortized inference for latent variable models. To start with, the authors clearly demonstrate the value of both Bayesian context aggregation and a MC based likelihood approximation scheme on precisely the same types of problems that existing neural processes papers (e.g., Garnelo et al., 2018) have considered (with the notable exception that the 2D image completion task considers only MNIST as a target dataset). ================= Score raised to 6 after inclusion of MA + SA results in rebuttal. Second, they replace the step of context aggregation with direct latent variable inference over z. I believe that this is a relevant paper for the conference.
They then compare the empowerment of implicit VIC with and without their corrections in a few toy domains, showing that their corrections do not hurt in deterministic environments, and provide a small increase in empowerment in stochastic environments. The differences between the proposed algorithm and VIC shows that the intrinsic reward now has an added term which depends on an approximate model of the transition probability distribution. The new experiments and visualizations have been helpful (I am happy with the author's responses to R3), but the overall clarity of the paper is still lacking due to the dense mathematical notation. "This type of option differs..." -- Aren't there two differences? Overall, I give this paper a score of 5 / 10, primarily because of (1) a lack of clarity and (2) the limited experiments. The rigorous mathematical derivations are simply re-deriving the VIC mutual information bounds with a new added term and with some extra details on how to do it with a gaussian mixture model. Experiments It would be great to include visualizations or ablation experiments to illustrate why implicit VIC has a lower empowerment than the two proposed methods. Strengths The paper provides a sound theoretical analysis of the limitation of the VIC implicit-option algorithm, the proposed fix and a practical algorithm (Algorithm 2). First off, its worth mentioning that this is the first real investigation into what intrinsic VIC actually optimizes. Would be a clear accept if you could show that, but as is the paper's contribution is bordering on acceptance.
Edit after Rebuttal I thank the authors for their engagement with my review. Finally in section 4, the paper analyzes SGD when for each sampled minibatch in an epoch, we apply n gradient steps with a stepsize ϵ/n and show that performance degrades as n increases, suggesting that the benefit of SGD with larger learning rates is due to the implicit regularizer and not the temperature of an associated SDE. However—as the authors themselves note in their critique of SDE approximations to SGD—the devil is in the details with continuous time approximations. As it currently stands, this paper is borderlin on the acceptance threshold for me.
Summary The paper provides an extensive empirical analysis of Pruning-at-Initialization (PaI) techniques and compares it against two pruning methods after (or during) training. In fact, one would think it is a good thing that PaI methods are robust to such variations given that there is not a lot of information available at initialization (note initialization is iid) to perform effective pruning and it seems these methods are robust and perform competitively to unpruned networks. In an extensive and comprehensive study, they show that pruning at initialization methods do naught but set per-layer sparsity rates, where the sparse initialization might as well have been random.
This paper proposed an unsupervised method for open-domain, audio-visual separation system. This paper describes a system for separating "on-screen" sounds from "off-screen" sounds in an audio-visual task, meaning sounds that are associated with objects that are visible in a video versus not. While they may not be able to decompose each sound source within the on-screen mixture, one can still leverage it to evaluate the on/off-screen separation. -----Experiments----- I understand that it is hard to obtain single source on-screen clips, but it could have been better if the authors had collected some small samples and test the single mixture separation performance. Yet another baseline might be using an audio-only mixture of mixtures separation system, perhaps with an oracle assignment system. The evaluation is sufficient for the on- vs off-screen task, although not sufficient to judge whether the system has learned a completely unsupervised source separator, making the scope of the contribution somewhat more limited than it has the potential to be.
In the first stage modeling, the authors proposed a new phoneme-level acoustic condition modeling in addition to the speaker and utterance-level approaches. This paper proposes AdaSpeech, a Transformer-based TTS architecture derived from FastSpeech, but multi-speaker, and focussed on the task of low-resource, robust, and low-dimensional speaker adaptation. A global acoustic embedding conditions the decoder in addition to speaker embeddings, in the hopes of accounting for recording conditions, and, I suppose, timbre, which should then be disentangled from the linguistic information from the text in the decoder during pretraining and adaptable to new recordings at fine-tuning/inference. Overall, this is very exciting work, as it not only promises space-efficient voice cloning, but, in doing so, suggests better disentanglement of speaker and phoneme properties in multi-speaker synthesis. Its multi-phonetic-level acoustic condition modeling approach seem technically new and interesting, However notice that, if I'm not mistaken, these acoustic embeddings are used zero-shot; it is only the speaker embedding that is the input to fine-tuning, and this only via the normalization parameters. This would be useful, or, even more welcome, an ablation study with the acoustic embeddings but not the speaker embedding. Similarly, I highly doubt the utterance-level acoustic condition modelling does not also capture speaker information. There is also a phoneme-level acoustic embedding which is used in the same way, which at inference is taken from random sentences (why not in training?), and, I guess, is supposed to cover phoneme-level idiosyncrasies of the speaker, although this isn't clear to me. On the contrary, the phoneme hiddens used in phoneme-level acoustic predictor do not seem to contain any personal voice information because they are resulted from the phoneme encoder that uses text information only as its input in Fig 1. My intuition would be that your phoneme-level predictor is trained only with phoneme hiddens (textual information only) (do these phoneme hiddens include speaker embedding information?), so at most it models some pitch or prosody information. I think the Mel features used in phoneme-level acoustic encoder contain personal voice information.
Clients with higher computation capability can train larger models while clients with less computation capability train smaller models, and all these model architectures belong to the same model class. This paper proposes a new federated learning framework called HeteroFL, which supports the training of different sizes of local models in heterogeneous clients. For the proposed 'static BN', the authors propose to not worry about running estimates across all clients until convergence of the model.
These disentangled factors of variation in the data are shown to correspond well to the 'abstract concepts' of the human demonstrations. equivallent -> equivalent force-relate -> force-related This paper presents a way to learn from demonstrations with weak or no labels. The premise behind this paper is that even when humans provide labels during a demonstration, those labels often do not fully describe the data (e.g., the human may say "soft" when "fast" would also apply). === NOTES & SOME MINOR COMMENTS === This is shown in the example of the PR2 robot dabbing demonstrations, including visual data as well robot trajectories. There are several instances in the results where the weak label models far outperform the baseline models without weak labels (e.g., Table 3 VAE for "soft"). Learning this manifold effectively and in an interpretable way, especially using weak supervision, can significantly change how robots can acquire skills from demonstrations and generalize them to new unseen scenarios. This paper presents a technique that uses latent variables to model the uncertainty over a group of class labels that could describe the task (e.g., slow, soft, left-of-object). This model was applied to a task where a human would teleoperate a robot arm and apply a dabbing motion in relation to an object in the scene. Further experiments should be done with other, more standard tasks and potentially user-provided labels to better determine the performance of the weak-label models as compared to the baselines.
This paper compares R-GAP with the DLG algorithm. The authors have improved the paper and addressed my concerns.
Motivated by the sensitivity of RL algorithms to the choice of hyperparameters and the data efficiency issue in training RL agents, the authors propose a population-based automated RL framework which can be applied to any off-policy RL algorithms. To achieve this goal, they integrate three technologies, i.e., evolutionary RL for hyperparameter search, evolvable neural network for policy network design, and shared experience replay for improving data usage. Besides, as we all know, both of the autoML and RL have a heavy computational burden, the adaption of evolvable neural network and shared experience replay greatly alleviate this dilemma. It could be more convincing if the authors can test on another benchmark, e.g. ProcGen. I think compared to computer vision tasks with huge neural networks, the search space for the architecture of RL models is much smaller, which can be observed in the ablation study. First, the author claims that the framework can optimize arbitrary off-policy RL algorithms, why only try on TD3?
The paper also presents results for finite-time convergence of the training dynamics for neural networks to the mean-field limit. This paper provides a mean-field characterization of entropy-regularized policy gradient dynamics for wide single hidden layer neural networks. The optimization landscape and convergence properties of policy gradient methods have drawn attention in RL theory for a long time, and it is nice to see a work that studies this problem from the perspectives of mean-field limit of neural networks, albeit being completely asymptotic. It would be nicer to spend more space to discuss the major differences of the theoretical analysis in this paper compared to earlier results on the mean-field limit in the supervised learning setting.
The proposed approach, called ez-greedy, combines randomly selected options with the well-adopted e-greedy exploration policy to achieve temporally-extended e-greedy exploration. Although this paper presents a general analysis on temporally extended e-greedy exploration, the presented ideas are too general. The paper overviews the publicized exploration methods from the perspective of their inductive biases, and clearly states where the inductive bias of ez-greedy would be better suited over e-greedy. The experiments clearly show where ez-greedy exploration would be useful. The idea is simple (a generalization of e-greedy) and the discussions nicely illustrate the main properties of an ideal generally-applicable exploration method.
In particular, it proposes to introduce high dimensional and high entropy label representations for group truth, to improve image classification performance from two practical matters --- Robustness and data efficiency, while achieving comparable accuracy to text labels as the standard representation. The results show that high dimensional and high entropy label representations are more useful, which is observed in the experiments related to robustness and a limited amount of training data. I am also interested in that if the audio signal is replaced by pre-trained embeddings, like glove or BERT, as label representation, how the effectiveness of labels is compared with the audio signals? My major concerns are: The clarity of the study on the underline true set of characteristics that contribute to the improvement, from speech label, shuffled-speech label, Gaussian-composition label, besides high dimension and high entropy. They used spectrogram of the pronunciations (TTS-based generated speech) of class labels as a high-dimensional representation. The traditional classification is conducted in classification, while the high-dimensional label experiment is conducted in regression.
The proposed method contains two main parts: (A) We use an ensemble method to help understand uncertainty of the network, and finally use different step sizes for different layers for updating the network parameters at test adaptation phase, (B) adversarial training as data augmentation to help the test-time adaptation process. In the original MAML setting, the test tasks can be entirely different from the training tasks (for example, we pick test tasks from different classes). It is quite novel to leverage adversarial learning as data augmentation for meta-testing in MAML. Let us denote this as θ0 (2) In the meta-testing stage, we go from θ0, given the few-shot test data, via an SGD process (typical scenario), to get a final model that has good performance on the test data of the test task. The paper also proposes to add task adversarial examples to the training set to help the meta fine-tuning process. [Summary] MAML has two stages: (1) In the meta-training stage, given various tasks (but each say only has a few labeled data), we want to arrive at a representation where it can quickly adapt to any test task later. Pros: The idea of combining meta-learning, uncertainty learning and adversarial training is well-structured.
########################################################################## Summary: The paper provides insight into the boundaries and feasibilities of a monolithic formulation of multitask learning by neural networks. Pros: This paper is quite novel in many aspects, including modularity v.s. monolithic, constructing task codes by SQL-style aggregation queries, "inverse counterpart" of multitask learning, connections to cognitive science. There is thorough mathematical justifications, case studies of monolithic formulations, guarantees on bounds and learnabilities in the supplementary material. Related to the question above, it seems very attractive to me that "the two-layer network can jointly learn the task coding scheme and the task-specific functions without special engineering of the architecture", but how to justify "task-specific" functions/task code has been successfully learned? ########################################################################## Note about the reviewer: My area of research is Bayesian non-parametrics applied on to multitask learning. The authors do not answer the grander questions they begin the paper with. Is the monolithic formulation of multitask learning effectively: joint learning of the switching function and the task function? Verdict: Recommendation to REJECT; Please consider for *CONF* Special Journal Issue with modifications. For these reasons, I must insist that the venue for submitting this work should be a suitable journal such as JMLR, ML, or specifically be rewritten for the Special Journal Issue @ *CONF* 2021.
Sorry if I am missing something. Pro: the paper shows that a simple model based on grid cells and SR representation can perform navigation in some simple environments. A key insight of the paper is that velocity instructions modify the eigenvalues of the SR but not its eigenvectors, so the eigendecomposition does not need to be recomputed for all candidate velocity instructions, and the eigenbasis can be hard-coded in the neural circuit (by the grid cells). If instead, the fourier component is meant to be distributed among the input population, it the authors should make this clear This paper proposed a model of navigation based on grid cells and the successor representation (SR). Section 3 suggests the transition structure can only derive from a 2D grid space, and that the method requires a periodic boundary condition hold. It is stated that  "a computational role for the neural grid codes: generating a "sense of direction" (eq. Given that in the previous section, the authors propose grid cells as being weighted Fourier modes (eq 10), I worry that there might be explicit band cells  in the proposed model. It is unclear how the "sense of direction" can be calculated in practice by a grid cell network. it is unclear to me how the successive exploration of possible directions could be implemented in practice by the grid cell network (see below for detailed comments). Does that predict that the grid cells are shared across the different environment? It is unclear how it is implemented in the network model, and how this information can get to the grid cells. I have worked at the intersection of theoretical neuro and ML for 8 years and am familiar with some recent literature on grid cell modeling, but still couldn't make sense of many aspects of the paper because of the many assumptions of prior knowledge. The paper would benefit by highlighting the true innovations (assuming there are some). I believe another round of editing, tightening the material, and improving the results could indeed make this a very solid paper.
In this work, the authors provide a method for a posteriori calibration of DNN uncertainty with emphasis on constructing a classifier that has PAC uncertainty guarantees. The PAC intervals are connected to calibration, and take the form of confidence intervals given the bin a prediction falls in. Strong points: The proposed method provides a provable guarantee on the reliability of a pre-trained methods prediction, which is a very nice property to have in the reliability/safety problem. This approach is a simple but good idea, seems grounded in a good motivation and the explored use cases are informative and interesting. Paper appears to be mathematically sound though I did not check all the proofs in the appendix. Pros: Paper is well written Important and timely problem, motivating arguments are well constructed Take for example a predictor \\hat{f} which assigns \\hat{p}(x) = 0.9 to every input 'x' regardless of its ultimate accuracy on the class, then given a new input with unknown label, it is not clear to me exactly how the framework would use the intervals to improve the uncertainty of this classifier, especially given that the class of this new point is unknown. In particular, it seems to me like the proposed intervals only hold their PAC guarantee when the test-time distribution matches the training distribution. When using statistical guarantees such as those given under the PAC framework, the iid assumption is almost always necessary. Another concern is the fact that getting well calibrated Clopper-Pearson intervals with good statistical guarantees takes a non-trivial number of samples and it appears this would scale with the number of classes.
The paper introduced Egocentric Spatial Memory Networks (ESMN), a novel learning paradigm and architecture for encoding spatial memory in a sphere representation. Sec. 4.1.1 mentions training with a convolutional encoder in the context of ESMN, Sec. 4.1.3 and Sec. 4.1.4 only evaluate ESM but not ESMN, while Sec. 4.2 states that "ESM represents map-only inference, while ESMN includes convolutions for both the image-level and map-level inference". After rebuttal phase The answers provided by the authors and the revised version of the paper sufficiently address my concerns. Still, I feel that in the current form, the paper can be accepted.
(i) In Table 2 and 3, could you add a comparison to SoftImpute and Alternating Minimization with L2 Regularization (say you choose the rank via cross-validation)?
Pros. The authors address an important and practical problem I like the idea of learning a mapping for a class of PDEs, rather than optimizing per instance I liked the paper a lot, and it's definitely a big step-forward in neural operators. However, our Fourier neural operator does not have this limitation." Paper Summary: The authors proposed a novel neural Fourier operator that generalizes between different function discretization schemes, and achieves superior performance in terms of speed and accuracy compared to learned baselines. The method seems to be quite generic and can be applied to a large range of PDEs. The inverse problem experiment is interesting and highlights a potentially useful application of the proposed method. The subsequent experimentation was extremely thorough (e.g. demonstrating that activation functions help in recovering high frequency modes) and, of course, the results were very impressive. To summarize, I believe that this work should be published. The numerical results are impressive
The authors note how strategies such as mixup and label smoothing, which reduce a single model's over-confidence, lead to degradation in calibration performance when such models are combined as an ensemble. This work analyses the interaction between data-augmentation strategies such as MixUp and model ensembles with regards to calibration performance. *CONF*, 2020. ########################################################################## Summary: The paper identifies the negative effects on calibration and the robustness of the deep models when data augmentation and the ensembles are combined. ########################################################################## Reasons for score: Overall, I vote for accepting.
=== Summary This paper proposes a framework, HyperDynamics, that takes in observations of how the environment changes when applying rounds of interactions, and then, generates parameters to help a learning-based dynamics model quickly adapt to new environments. == Original Review == The paper proposes a model for predicting the dynamics of a physical system based on hypernetworks: given some observed interactions and some visual input, the hypernetwork outputs the parameters of a dynamics model, which then predicts the evolution of the system's state over time. === Strengths This paper targets an important question of building a more generalizable dynamics model that can perform online adaptation to environments with different physical properties and scenarios that are not seen during training. Strengths: The paper addresses an important question, namely, how a dynamics model may adapt to environments that don't fully match its training distribution. Weaknesses: The main claim of the paper is that hyperdynamics network offers better prediction accuracy and generalization than a standard dynamics model. The authors claim that their "dataset consists of only 31 different object meshes with distinct shapes." It is important to include images of the objects to give the readers a better understanding of how diverse the dataset is and how different the geometry of the "seen" and "novel" objects are. The authors have evaluated the method in several object pushing and robot locomotion tasks and shown superior performance over baselines that uses recurrent state representations or gradient-based meta-optimization. == Update == Thank you for your detailed response.
[Summary] Paper proposed to generate the communication message in MARL with the predicted trajectories of all the agents (include the agent itself). The method involves agents producing imagined future trajectories using learned environment dynamics, and then communicating some parts of these trajectories to other agents. While the peer action prediction based on the agent's own observation supports the decentralized execution, the local observation may not include sufficient information about the others depending on a multiagent domain and thus the action prediction can fail. ---- Post-rebuttal ---- The additional ablation studies on the attention and MADDPG-p are helpful, and address most of my concerns - though it is still not clear to me whether any of the baselines compared to is exactly equivalent to the method used with attention weights (1, 0, 0, ...). [Strengh] +) The idea of communication with imagined intention is motivated properly with rich psychological background and also technically sound.
Summary This paper extends neural compression approaches by fine-tuning the decoder on individual instances and including (an update to) the decoder in the bit-stream for each image/video. Summary The paper describes an instance specific finetuning method for image and video compression including finetuning the decoder. This paper considers the problem of per-instance model adaptation for neural data compression, and proposes a new method for end-to-end finetuning the model that is quantization-aware, by introducing an additional term that measures the compression cost of model update to the typical rate-distortion loss. This paper investigates how to improve the test time performance of learned image compression models through finetuning of the full model. Quality (5/10) The proposed approach is sound and it would have been interesting to see the gains which can be achieved by fine-tuning the decoder of common neural compression approaches. Strength = Method which also considers to finetune/adapt the decoder side of image compression network, for improved performance. The paper claims that "In this paper we consider the extreme case where the domain of adaptation is a single instance, resulting in costs for sending model updates which become very relevant", but this would highly misleading if all the experiments were conducted in a batch compression setting. Although if you are finetuning (and communicating side information for the prior updates) then it is probably very little extra cost to also update the decoder.
This paper provides a variety of studies to understand the generalization gap between known and novel classes in one-shot object detection. Weaknesses: The paper studies the importance of number of object categories in training dataset and claims that the gap in one-shot detection can be closed by increasing the number of categories. -> either The paper suggests that a major factor for increasing few-shot performance in the few-shot object detection task is the number of categories in the base training set used to pre-train the few-shot model on a large set of data before it is adapted to novel categories using only a few (or even Pros: number of base classes is indeed an important factor in few-shot methods performance (not just in detection) It provides a comprehensive review on related papers on object detection especially one-shot detection and their limitations.
(p.1, Abstract) we generalize the IRL problem to a well-posed expectation optimization problem stochastic inverse reinforcement learning (SIRL) to recover the probability distribution over reward functions. Also, since Bayesian IRL also recovers the reward distribution, I couldn't get the major advantage of the SIRL from this statement. (p.3, Problem Statement) more likely generates weights to compose reward functions as the ones derived from expert demonstrations First, this is wrong: objectworld is not that unique, you could visualize reward over e.g. a gridworld or tabular MDP, and many IRL papers have done so. Also, for the linear reward function, the maximum likelihood objective is convex, so I think that the different weights should even converge to the same solution. It seems one could not use this in practice, since it requires computing the EVD, which is only computable if one knows the ground-truth reward -- in which case no need for IRL. The algorithm starts with an initial set of N0 parameter vectors, each corresponding to the weights for a linear reward function. (p.3, Problem Statement) fM How this quantity is related to reward weights is unclear to me.
---- Summary This paper proposes FLAG (Free Large-scale Adversarial Augmentation on Graphs), an adversarial data augmentation technique that can be applied to different GNN models in order to improve their generalization. This paper presented an adversarial augmentation technique for graph neural networks. It is true that adversarial feature augmentation has not been studied for graph neural nets, but it is a straightforward idea to apply an existing feature augmentation method on graph nodes, which are represented using feature vectors just like other types of data. The focus of the paper is extensive experimentation in various tasks and settings to illustrate the effectiveness of adversarial augmentation in graph-based tasks. Strengths: There are various best practices which are commonly applied in common task framework competitions such as data augmentation, adversarial training, and ensembles that improve results by fractions or a small number of percentage points. Hence, my takeaway from the paper (which is in fact a valuable takeaway) is that adversarial augmentation is not considerably effective for graph neural nets, no matter what dataset and network architecture is used. Adversarial Training for Free, Neurips 2019 This work applies FreeLB adversarial training [1] to graph neural networks. ---- Cons My main concern with this paper is that the proposed augmentation technique is not compared against any other graph augmentation techniques. The authors adopt an existing augmentation algorithm and apply on the nodes of each training graph, and use the perturbed graphs for training. ---- Justification for score The paper is well written and the method interesting and well explained, but it lacks comparisons with other graph data augmentation approaches.
There is no baseline on any of the standard dataset, not strong supervised baseline on the datasets paper proposed. [2]. https://github.com/NVIDIA/DALI/blob/1e9196702d991d3342ad7a5a7d57c2893abad832/docs/examples/use_cases/pytorch/resnet50/main.py#L116 [3]. http://cs231n.stanford.edu/reports/2015/pdfs/ondieki_final_paper.pdf After rebuttal update.
Summary This paper presents a new type of brain-inspired dual-pathway DNN model where the coarse (faster, less accurate) and fine (slower, more accurate) visual pathways augment each other during training and inference (via imitation and feedback) to boost the network's robustness to various noises. During training, cross-entropy loss are used for both pathways, and an "imitation" loss is used to encourage the CoarseNet pathway to mimic the FineNet [8] Brain-Like Object Recognition with High-Performing Shallow Recurrent ANNs, NeurIPS, 2019 This paper proposed a two-pathway neural network to mimic the interplay between the parvocellular (slow and fine-grained) and magnocellular (fast and course) pathways in neural systems. In addition, using only FGSM (targeting the FineNet) to generate adversarial noises doesn't fully test the robustness of the model, since more recent techniques [1, 2] can easily generate smooth adversarial examples that will likely severely affect the CoarseNet (unlike FGSM). (Imitation learning) It's unclear why CoarseNet activations must mimic FineNet activations, since only FineNet activations are ultimately used for inference (not CoarseNet activations, which are further transformed and used by the FineNet). Additional Feedback (1) Although using an L2 loss for imitation learning is straightforward mathematically, the authors' arguments regarding how the brain may implement imitation learning aren't very convincing (Sec 3.2). [7] Biologically Inspired Mechanisms for Adversarial Robustness, arXiv, 2020 Recommendation I recommend rejection of the paper given the following two major cons (see details above).
Since the OGB benchmark is new and the reported GNN models on the OGB leaderboard were only run for 3 layers, it is crucial to analyze all the variants discussed here in detail to appreciate the performance gain achieved by the proposed generalized aggregation function. The paper is missing an explanation why these different aggregation functions are supposed to specifically support deeper GNNs. Update after Rebuttal: I have read the authors' response but do no change my scores. --- Post rebuttal: I've read the author's response and there is no change in my scores. #####Pros##### (1) The proposed generalized aggregation functions are intuitively and empirically effective. This paper studies how to train deeper graph convolutional networks by using different aggregation function. Hence, I do not vote for acceptance. It would be great if authors can state the novelty of this paper compared to DeepGCNs. Also, what makes difference between the proposed aggregation function and softmax.
However, I still have some concerns: In the experiments of unconditional setting, the authors included Classic, Classic+, and Classical++ as their comparisons. (6) For unconditional video synthesis, the musical instrument playing videos mostly contain small motions. First, although there may not exist any resampling method that can directly perform the video texture synthesis, I believe many related graph-based methods could be used to model the transition probabilities of frames. ** Weaknesses (1) It seems a strong limitation that the proposed approach is not able to generalize to different videos or has to be video-specific (i.e., train a model on each input video). The audio conditioned video synthesis part also seems like an extra module which does not influence the completeness of the whole model if not included. ** Strengths (1) Improve the classic video texture synthesis method Video Textures by replacing pixel similarity with a distance metric learning to measure the transition probabilities Cons: originality: This work is more like a simple extension of the previous work (Video Textures) with limited novelty. Third, the video content is directly sampled from seen sequence, where the diversity is constrained to the given video. First, these are noticeable discontinuity between sampled video segments. (2) Extend the proposed approach to audio conditioned video synthesis Second, this method seems to be example-specific, which needs retraining if fed a new video sequence. Moreover, the video interpolation is directly borrowed from previous work without further improvements, where I think is still challenging and worth to explore. (7) It is claimed that the approach is able to produce infinite video, however the content is constrained in the input video, so the variation is limited. Second, although the authors pointed out that the video resampling (textures) strategy is different from the recent generation-based strategy, they should provide visual results/comparisons to support their claims. It is highly recommended that the authors could present more comparison with the previous baselines in both general idea and model details. I think the authors may at least discuss these works. To guarantee the smoothness of the transition between different segments, an existing interpolation method is used to connect these video segments in a sequential order. The proposed method is inspired by Video Textures (Sch¨odl et al., 2000), which synthesizes new videos by stitching together snippets of an existing video.
The method can be used as a black-box watermark (does not require model parameters to verify),  however, the certification bounds only apply to a white-box use case in which the verification can perform inference and test accuracy for a set of trigger images for multiple smoothed versions of the parameters. The authors did not rebut many of my negative concerns. 2020. The proposed method exploits the randomized smoothing techniques for a certified watermark of neural networks. If my understanding is correct, Col. 1 simply certifies that the lower bound on the trigger set accuracy. Con 4 is par for the course with any watermarking scheme, although the reduction 89.3->86% accuracy for CIFAR-10 is concerning, as that much accuracy loss is a significant deterrent to use of the method and the trend from MNIST CIFAR-10 makes me wonder if larger and more realistic images may show even greater reduction in accuracy. Different from the defense against adversarial example, in the case of watermark detection, not only the detection accuracy but false detection of non-watermarked models should be considered. Can we say that models without watermark cannot attain this trigger set accuracy?
########################################################################## Summary: The paper presents NAHAS, which is a combination of Neural Architecture Search (NAS) and Hardware Architecture Search (HAS) When Neural Architecture Search Meets Hardware Implementation: from Hardware Awareness to Co-Design https://ieeexplore.ieee.org/abstract/document/8839421 The main difference between this paper and previous hardware-aware NAS papers is that this paper has an additional hardware search space beside the neural network architecture search space. A highly parameterized (commercial) edge accelerator defines the hardware search space. ########################################################################## Reasons for score: The paper claims it demonstrate effectiveness of hardware aware NAS for first time, which is dubious. Limited information is provided regarding the architecture of the accelerator. Therefore, I recommend rejecting this submission.
The paper presents a simple addition to the Balanced Accuracy approach - which the authors refer to as 'importance'. Basically, the performance for a multi-class problem can be evaluated by decomposing the original multi-class problem into a number of binary ones based on one-against-rest manner, and then evaulating the performance scores for each of the binary ones using any well-known metric for binary classification, and then, aggregating the performance scores. In section 4.2, the authors state " … by modifying loss functions of DNNs to capture class importance weights…". Pros: The proposed framework is simple and effective. As such, the concept of including importance of each class into an evaluation metric has been implicitly considered. However, in order to judge if a particular approach is better or worse than another you need some way of showing that your metric is correct. the weighting schemes to combine binary metric scores to evaulate the performance of multi-class classification have been well-studied, such as macro-averaging, micro-averaging, as well as importance weighting (manual or data-driven e.g. frequency).
Empirically, the proposed regularized term works well along with the original MINE estimator and ReMINE  has better performance in the continuous domain 2015. The work studies a neural-network based estimator, referred to as MINE, for approximating the mutual information between two variables. Summary The paper introduces a generalized version of the mutual information neural estimation (MINE), termed regularized MINE (ReMINE). Some interesting experimental results are firstly provided on a synthetic dataset: the constant term in the statistical network is drifting after MI estimate converges. Also, the optimization of MINE will result in the bimodal distribution of the outputs, in which the statistical network has very distinct values for joint and non-joint samples. However, the theoretical work in MINE is also very weak and only focuses on estimation consistency and not convergence rates (i.e. the statistical bias and variance of the estimator). [R4] Singh and Poczos, "Exponential concentration of a density functional estimator," NeurIPS, 2014. Overall, I lean toward rejection given current concerns.
Summary: This paper discusses an approach to augment a medical imaging dataset using images from another modality. If latter, the entire evaluation strategy should change to include the set of images from the volume rather than single images. In Figs2-4, visual comparison of the translated images to real CT images have been provided.
In order to make sure the FairDP is (ϵ,δ)-differentially private, they have to theoretically find the privacy parameters with respect to the training datasets. If the claim isn't that Algorithm 1 is DP, then the privacy guarantee is restricted to the results in Figure 3, that attack algorithms perform similarly well on DPSGD, FairDP, and significantly better on SGD (non-private). If FairDP does not satisfy the DP definition or has a very large privacy loss compared to DPSGD, it is not fair to compare FairDP with DPSGD. Do different groups get different levels of privacy protection, and if so what does that mean, and is that considered a fairness violation as well? Other comments: -In addition to a formal privacy statement, the authors should formally define the notions of "privacy" and "fairness" that they use in the paper.
In such a setting, the training of GNNs for node classification problems substantially differs from the training of other neural networks, because if the graph and node data are partitioned and distributed across machines, the data held by each machine may not be enough to compute a local gradient. Thus, it is critical to show the percent of immediate neighbors in the overlap Graph Convolutional Networks (GCNs) have inspired state-of-the-art methods for learning representations on graphs. The paper presents a subgraph approximation method to reduce communication in distributed GCN training. The authors could consider, for example, large-scale datasets considered in publications a) and b) listed above (Microsoft Academic Graph, and Protein graph respectively) for distributed GCN training.
6430-6439. 2019. Summary In the context of gradient-based meta-learning for few-shot learning, the authors propose TreeMAML, an algorithm that leverages the existence of a tree structure in a task distribution in order to pool inner-loop gradients between tasks. The authors propose to adapt the model-agnostic meta-learning algorithm (MAML) of [1] to reflect this hierarchical structure by either observing (Section 4.1, FixedTree MAML) or inferring (Section 4.2, LearnedTree MAML) an assignment of tasks to clusters at each step of the inner loop (task-specific adaptation phase) of MAML; Novelty: The algorithm modifies and combines previously introduced components: the MAML algorithm of [1]; the online top-down clustering algorithm of [2], and the task-similarity-as-gradient-similarity approach of [3]. Significance: Results on the hierarchically structured synthetic regression task datasets demonstrate that {Fixed|Learned}Tree MAML: is at least as good as MAML, and often outperforms MAML; Recommendation I currently recommend a clear reject (3).
The main contribution of this work is to use Early Bird Lottery Tickets to reduce pre-training and fine tuning time for BERT. Experiments for both pre-training (the first of its kind) and fine-tuning show that performance does not drop all that much for GLUE and Squad which are the main set of tasks BERT is typically evaluated on. Summary: The authors propose a technique for reducing the computational requirements of training BERT early in training to reduce the overall amount of resources required. Despite very encouraging results, several important methodological questions about the source of the efficiency gains and other aspects of the paper are left unanswered. (More of a question/nit) Is it possible to also show what happens when a winning ticket for BERT fine-tuning is selected based on the pre-training objective? The authors pitch it as a technique for reducing the training time of BERT and use LayerDrop as a baseline technique that also removes network components. Implementation details are only given for the vanilla BERT Are they similar to the EarlyBERT model as well? This would involve computing the pre-training time (time to learn BERT parameters on Wikipedia) + total fine-tuning time across all datasets (QQP/CoLA/MNLI etc) considered. While a central argument is that most model distillation techniques still require expensive pre-training, it would still be useful to include some of those results in Table-2 since EarlyBERT is comparable to those techniques for the purpose of Table-2. Questions and comments: Why does the mask distance diverge for FC in pre-training (Figure 1b)? I cannot recommend accepting this paper in its current form, but am looking forward to reading the authors' response which might clarify things.
This paper presents a deeply supervised few-shot learning model via ensemble achieving state-of-the-art performance on mini-ImageNet and tiredImageNet. The authors first studied the classification accuracy on mini-Image across convolutional layers and found the network could perform well even in the middle layer. However, when digging deeper, the reason for the ensemble is that we want to find a way to calculate the features through different classifiers, maybe this is because a single classifier is not able to learn all the features from the images at once. [1] - Dvornik et.al. "Diversity with cooperation: Ensemble methods for few-shot classification" Thanks to the authors for providing such an ensemble approach. [2] - Dvornik et.al. "Selecting Relevant Features from a Multi-domain Representation for Few-shot Classification"
It is empirically shown that, for a specific type of initialization, for less over-parameterized neural networks, the gradient dynamics follows two phases: a phase that follows the random features model where all the neurons are "quenched", and another phase in which there are a few "activated" neurons. In a controlled setting, a list of experiments investigates the dynamics and compares them for different regimes where the relation of number of hidden neurons to number of training samples is changed and for several specific target functions. In the derivation of the effective dynamics, it is unclear why the second layer's weights of the quenched neurons move faster than the first layer's weights. The stated contribution is therefore that this scaling changes the observed dynamics significantly, which casts doubt when the observations generalize to other settings. It would be good if the authors can (heuristically) explain which neuron will become activated (any random neuron, or a neuron that satisfies some initial condition?), and how long it will take for those neurons to become activated (perhaps this depends on the initial weights of the neurons).
[2] CausalWorld: A Robotic Manipulation Benchmark for Causal Structure and Transfer Learning, https://arxiv.org/abs/2010.04296 This paper is a review of model-based approaches of integrating causal inference to reinforcement learning (RL) in different environments (application areas). [1] Bakhtin, A., van der Maaten, L., Johnson, J., Gustafson, L., & Girshick, R. Based on the above reasons, I do not think this paper is ready to publish. If a simple greedy algorithm can solve the tasks, does it mean that the benchmark may be a bit too simple, where a good understanding of the underlying causal structure may not be necessary? The authors state: "The main goal of our paper is NOT to introduce novel models, but rather to introduce a NOVEL benchmark and insights/ingredients to study causal induction in model-based RL" and "It is true that the models we use do not learn an explicit structure for causal learning" which corresponds to my original reservations to the novelty of this paper. In Advances in Neural Information Processing Systems (pp.
"Decision-based adversarial attacks: Reliable attacks against black-box machine learning models." arXiv preprint arXiv:1712.04248 (2017). Minors: In the introduction section, the authors claim that "Existing adversarial defense techniques could be classified into two main categories: adversarial training and detection". Especially, NATTACK learns adversarial examples' distributions, and AttackDist is based on adversarial samples' distribution.
Summary: This paper proposes to use label smoothing to determine the labels of augmented samples. One main disadvantage of the proposed method is that it seems to be sensitive to the distance function chosen for the label assignment and this function has to be adjusted for different data augmentation schema. Authors should formalise the assumption in the work including a 'feasible augmentation' of the samples (not the labels) as it is highly dependent on the augmented label; strong and weak augmentations how these concepts hold in the paper? Overall, I currently hold my score as marginally below the acceptance threshold.
######################################################################### -- Summary: This paper presents experiments and results from using a reject-class in multi-class classification for the auxiliary task of out-of-distribution (ood) sample detection. Regarding the performance of similar methods evaluated in [Lee et al. (a)] and [Dhamija et al.], I think the main reason why they didn't get the same observation is on the size of OOD dataset, i.e., they didn't train their model with a large OOD dataset like [Hendrycks et al.] or this work. Reasons for score: The proposed setting with a large OOD dataset has already been proposed by [Hendrycks et al.], and the proposed method has been experimented in [Lee et al. (a)] and [Dhamija et al.], so the technical novelty of this work is limited. More recent papers studied the use of self-supervised learning (e.g. [4, 5] ) and contrastive learning (e.g., [6, 7,8]) to improve ood detection. Optimization method). It's great that the overall method does better, but I'd like to see at experiments on some datasets investigating what happens if you use the same procedure as OE except K+1-th class instead of entropy. Training with a large OOD dataset like [Hendrycks et al.] is not common, and the observation in this paper is limited to this setting. Since the large OOD setting has already been proposed by [Hendrycks et al. ], the only contribution of this work is on the empirical observation that the proposed method is better than baselines. Instead, this paper seems to show stronger results, with a more intuitive and simpler method (just classify the outlier exposure set into a separate class) that Hendrycks et al suggest doesn't work as well. In NeurIPS, 2018. [Hendrycks et al.] Deep anomaly detection with outlier exposure. 2018.' [2] Mohseni, Sina, et al. "Self-Supervised Learning for Generalizable Out-of-Distribution Detection." AAAI.
The idea of introducing additional dynamic variables to neuralODEs to account for the bias introduced by hidden confounders is also novel. I would have expected to see the following analysis: A)    Auxiliary states vs. history-dependent (non-Markovian) neuralODE B)    Auxiliary states vs. additional parameters of neuralODE This is due to the following reasons: A) In prior work, a similar approach has been employed in various prior works to model hidden confounders, e.g. [Nodelman, U., Shelton, C. R., & Koller, D. (2012).
It shows that the variance of stochastic gradient is a decreasing function of minibatch size for linear regression and deep linear network. The paper shows that the variance of the gradient has an inverse dependence on the batch size in linear networks, subject to the knowledge of the initial weights. Having a small total variance conditioned only on the initial point means that somehow the trajectories for different samplings cannot diverge too much. My major concern is that the authors don't provide an application of their theorems, i.e. a setting where the exact knowledge of the variance of the gradient is useful. Conclusion Overall, I would say that this paper is just above the acceptance bar, because the theory holds up well and could be of interest for finer analysis of the dynamics of SGD, and in particular of different trajectory starting from the same point (how quickly will they diverge?
The UniMP first employs graph Transformer networks to jointly propagate both feature and label information. The authors proposed a unified message passing model to make a graph neural network to be able to incorporate both label propagation and feature propagation. Minor comments: Abstract: we adopt a Graph Transformer jointly [using] label embedding? Compared with other methods such as APPNP,  TPN and GCN-LPA, the major differences is another effective/unified network structure to merge feature and label information together. Pros: This paper proposed a novel framework to more effectively and explicitly utilize the label information by GNN in semi-supervised scenario, and the experiments illustrate its effectiveness in general benchmark datasets. Cons: What is the label leakage problem? It is not clear to me (1) why label will be leaked during the joint learning process and (2) what the outcome does label leakage bring. The writing of this paper is poor. Overall, I vote for accepting the paper. Section 4.4: model still be uncertain → model still remains uncertain The paper presents a novel unified model that jointly harnesses the power of graph convolutional networks and label propagation algorithms based on the unified message passing framework.
The comparison between CNV-Net and other methods is not fair as the other methods are finding CNVs in the whole genome while CNV-Net is given a pre-defined set of "candidate breakpoint regions" in which positive samples have breakpoints perfectly centered in the middle. ########################################################################## The authors present an innovative approach to CNV detection using CNNs. However, I believe that additional experiments need to performed before this paper is ready for publication.
Based on this, they propose a light and scalable GNN learning framework called LCGNN, which first adopts the local clustering method PPR-Nibble to partition full graph into subgraphs, then use GNN modules on subgraphs for training and inference. ########################################################################## Summary: This paper proposes to utilize local clustering to efficiently search for small but compact subgraphs for Graph Neural Networks (GNN) training and inference. Although the idea of using local clustering is very interesting, it is straightforward to apply an existing local clustering algorithm into GNN especially considering the existing methods that utilize the global graph partition. Notably, they show that using K≥10 gives significant improvement on Cora and PubMed dataset for node classification problem compare to using K≤5. The idea is to form local graph for each node using PPR-Nibble, a local clustering method proposed before, and then use transformer on top of the local graph as encoder for node classification and link prediction. Minor comments: As the authors mentioned, the local clustering methodology is only reasonable for graphs that has low conductance with respect to all "cluster" (nodes with same label). Using local clustering method to determine the subgraph for each node might be better than random neighborhood sampling in some cases. However, it is not clear whether the performance gain is due to the local clustering procedure or the transformer.
The authors propose a method to train deep generative models on quotient manifolds and show improved performance on some simple standard test sets. Can you elaborate? Multi-generator scheme (Sections 2.2, 3): Can you elaborate on the actual model used for Quotient Manifold Modelling (QMM)? The "orthogonal directions  to  the  equivalence-relation  contours" in section 3.2.
They consider atomic factorization of the action space (i.e. action space is factored into sub-action spaces, one per action dimension). 2) sequential/autoregressive policies (an ordering of the sub-action spaces is assumed a priori and the sub-policies receive as input the state and the selected sub-actions for the preceding sub-action spaces). Questions What is the impact of the action ordering for the autoregressive factorization? In fact, the action space in Gym Platform features discrete and continuous sub-action spaces, but they are discretized. ########################################################################## Questions during the rebuttal period: In light of new related works together with those included in the paper, I believe the novelty of the paper currently is in developing the policy optimization updates for autoregressive policies. Update: After reading the other reviews and the responses, I have changed my score to 5: marginally below acceptance, due to the framing and related work issues, as discussed by R2 and R4. ########################################################################## I would be happy to revise my score post clarifications from the authors. The paper nicely outlines how PPO and SAC should be configured to work with independent and autoregressive policies under the atomic factorization of the action space. However, at this moment, I will keep the paper below acceptance.
Summary: The paper considers the sign recovery problem in a distributed setting with privacy constraints. The paper shows that in the sparse mean estimation setting, Med-DC is correct with high probability under some assumptions and Med-DC satisfies a weaker notion of differential privacy proposed by the paper. Using robust estimators (which is deterministic), which gives privacy guarantees with high probability, is exactly the start point of these algorithms. It seems to me that the main reason for Med-DC being both deterministic and private in this new notion is the weakness of the new privacy notion but not the well-design of Med-DC. In particular the privacy guarantees are clearly stated, and their difference to pure DP highlighted.
Over the past few years, a number of papers have developed the theory of Neural Tangent Kernels, which can be used to interpret infinite width deep neural networks in the context of a particular type of kernel. Secondly, they provide an interpretation of the neural path kernel as a composite kernel composed of layer-wise kernels, giving rise to the title of the paper. The discussion mentions performance when we fixed the input gram matrix to be a constant in the definition of the neural path kernel (and hence define the neural path kernel in terms of the gating structure only), but does not include numerical results for this case. For this, I think the authors need to make concrete comparisons with methods that are deeply rooted in kernels such as GPs or BNNs. For instance, does using a particular composite kernel structure give you the same predictive performance as when using a GP? (2) Lakshminarayanan and Singh (2020) has developed a neural path framework in the NTK regime. Theorem 5.1 (in both papers) relates the neural path kernel to the neural tangent kernel by showing that the neural tangent kernel for a network in which the gates have been fixed tends to a constant multiple of the neural path kernel as the width of the nerwork goes to infinity. Firstly, the analysis is extended to certain ResNet and Convolutional architectures, showing that in both of these cases we can relate the neural tangent kernel matrix to the neural path kernel matrix using a result analogous to Theorem 5.1 in (Lakshminarayanan and Singh, NeurIPS 2020).
The paper presents an interesting analysis of CutMix and MixUp data augmentation techniques. Table 1 shows that in terms of mutual information, MixUp < Baseline < CutMix (and < FMix with a very small gap). Small performance gap, and even sometimes worse performance, compared to the baseline methods (Mixup, CutMix)
Results on more datasets with different spectral distributions would make the study more conclusive.
This work uses the model introduced by Tensorflow Quantum, where different neurons can be implemented on either quantum or classical computers. Using a technique best on Taylor polynomial approximations, the paper finds that a large class of smooth functions can be approximated using O(log(1/\\epsilon)^(n/d)) quantum gates, qubits,  and classical width.
Reasons for score: The main idea of using self-attention for decoding linear error correction codes is interesting. Pros: The paper utilizes the self-attention mechanism to improves the computational complexity of permutation decoding. Extra feedback I just want to conclude with a suggestion for a correction and a typo: In Section 2, second paragraph, the sentence ''Codes with good decoding performance are represented by graphs with cycles'' is a bit confusing, as it could be understood that indeed graphs are helping with the performance. Despite the previously described issues, I believe the paper presents an interesting method to advance the current state of permutation decoding. Thanks to the possibility of freely generating training samples on these schemes, it is possible to achieve very satisfactory training for permutation embedding and classification. It is unclear how this attention looks between the permutation vectors, but in the end what matters is the embedding obtained, and how it captures the similarity between them, and also, I believe, some relations with the syndrome. The paper empirically demonstrates the utility of their proposed method on BCH codes. For me, it is not clear why the top k (k>1) performance matters to measure the quality of the decoding algorithm of error correction codes for communication. Minor comments: It seems that WBP is not defined (is it weighted BP?) The paper focuses on improving the computational complexity of permutation decoding. Finally, I am curious about the Block error rate, as I can imagine that sometimes, when a permutation is quite wrongly chosen, it might push y to a different c, hence leading to s=0, but still resulting in a wrong decoding. In practice, one performs decoding with multiple permutations to identify the ones which lead to successful decoding. In permutation decoding, one aims to decode a permutation of the received codeword in the hope that it will lead to successful decoding as compared to applying the decoding algorithm on the received codeword. Instead of plotting the top k (>1) performance, it would be better to include the comparison of performance over the best existing decoding schemes (in terms of top 1 performance given a reasonable complexity). I am leaning towards rejecting the paper, because I find the experimental evaluation quite limited. IN BPL, although it is applied to Polar codes, the authors finally make use of only 5 different permutations, which is not that ineffective, and enable them to report a great performance. Concerns: My main concern is that there is an insufficient amount of reasoning to explain why we use the proposed method over the existing decoding schemes. Even though the proposed solution can be potentially employed to decode various codes, the treatment in the paper is restricted to the BCH code.
Paper proposes Hybrid Discriminative Generative training of Energy based models (HDGE) which combines supervised and generative modeling by using a contrastive approximation of the energy based loss Gets rid of computationally expensive SGLD by using contrastive approximation which was a key limitation of prior energy based modeling work like JEM Significance Results are compelling across a wide range of tasks over existing (EBM) baselines including calibration, robustness, OOD detection, generative modeling and classification accuracy Precisely, the approximation is for modeling p(x|y), yet the contrastive learning is modeling p(x_1|x_2) with  x_1 and x_2 being the outcomes from correlated data. It could be that HDGE is just succeeding in learning the support of the dataset.In order for this paper to reach the standard of *CONF*, I believe that the authors must revise the strength of their work and redesign their experiments as such.
Major points / suggestions for improvement Other, related aspects are (i) which type of supervision the models require for pre-training and (ii) which type of output layers are required in order for the null hypothesis to be relevant. Because of this, I am as of now leaning towards recommending rejection, as I believe the paper would greatly benefit from a strong, non-incremental revision. While it might be unrealistic to expect the authors to overcome inherent limitations of the current state of the art in selective inference, I believe they should at least carry out a much more thorough investigation of the extent to which these assumptions are applicable to real-world data, and what are the consequences of violations for the inference process.
The paper explores the impact of various types of perturbations between states in the same environment (state transferability), and between states in different environments (environment transferability). Paper Summary: This paper aims at discovering transferability of perturbations across different environments in RL. I'm willing to adjust my score should my concerns be addressed. — idea: A framework composed of 6 different adversaries is proposed using which it is claimed that the transferability properties among Atari environments can be studied. Review Summary: The idea of analyzing perturbations' transferability is interesting, but it is hard to say that the approaches are satisfactory. I do think the paper falls a little short in that there wasn't a representative sample of deep RL methods, as well as not commenting on design choices made and how/whether they might interplay with the transferability measured. Moreover, since adversarial attack is already used in this work, I don't see the reason why none of the referred methods for adversarial attacks in RL are tested.
On the basis of the ordinary-label learning, the authors defined "robust loss functions" for complementary-label learning:  a a loss function is called robust  loss function if minimizer of risk with complementary labels would be the same as with ordinary  labels. "can be summary as" However, in this paper it means if the loss function with ordinary and complementary labels has the same minimizer. I am unaware of this definition of robustness of a loss function as it seems very specific to the complementary label learning problem. It would then be necessary to connect the current work to this trend, for instance to Cour et al. "Learning from partial labels" (JMLR 2011) or the more recent works of, e.g., X Wu, ML Zhang "Towards Enabling Binary Decomposition for Partial Label Learning.". Summary: This paper deals with the problem of complementary label learning, that is, when we know the set of labels which a given observation does not belong to.
In the RL context, this paper aims at designing a generic solution for reducing the number of policy switches during training (called switching cost) while maintaining the performance. Performance : It seems to me that FIX_10^3 has always lower switching cost, and also learns a good policy. Questions : I think that maybe another good criteria is to measure the distance between the deployed and the online policy, rather than feature representation. It may be the case that features distance determines the policy distance but that's not necessarily true.
As a way out, the authors introduce the concept of Patterns Statistics Inductive Bias (PSI). They define a statistical phenomenon that holds in SGD in the proposed setting and call it Pattern Statistics Inductive Bias (PSI). They proved that if a learning algorithm satisfies PSI, the sample complexity is nearly quadratic in the filter dimension; while the VC dimension of the network is at least exponential in the filter dimension. The authors also verified PSI in some task based on MNIST that has non-orthogonal patches. Theorem 6.1 needs to assume that PSI holds and spurious patterns are unbiased; under these assumptions, the result is pretty straightforward.
This paper proposed AdaMa, which can automatically use adaptive learning rates for each agent in cooperative Multi-Agent Reinforcement Learning (MARL). AdaMa calculated the learning rate of each actor and critic according to their contributions of locally increasing value functions. The learning rate balance between actor and critic is well motivated. Comments I'm not fully convinced. The figures indeed show that AdaMa uses different LR for different actor. In Figure 3 (a), (b) and (d), AdaMa has similar performance with Fixed lr (fixed learning rate).
However, unlike PVI, the proposed method aims to replace the parametric representation of posterior with a non-parametric particle representation developed by the prior SVGD work of (Liu & Wang, 2016). A quick question: the proposed algorithm and PVI only selects ONE agent per communication round. --- post-rebuttal feedback --- The authors have addressed most of my concerns. Post-Rebutal: I really thank the authors for their efforts  following my comments. I can not recommend the acceptation of this work for the following reasons: The originality of method is low because it directly builds on top of two well-established approaches PVI and SVGD.
Of course opponent observations should not be used at execution time, and training a VAE to recover the missing information seems like a good approach. Ablations on VAE training targets are provided, as well as analysis regarding the learned opponent embeddings and VAE decoder performance. A variational autoencoder (VAE) is trained to predict opponent observations and actions, given the agent's local information. Recommendation and Justification: I think the paper should be accepted. I'm still in favour of accepting the paper. In an actual adversarial setting, even if the opponent's policy was static (i.e., not updated in response to LIOM's policy, as in the cited Ganzfried papers), the opponent could still exploit any mistakes LIOM might make through its static policy, possibly driving its performance below that of NOM. I liked the paper and I'm half joking when I say this, but: a better empirical analysis of an opponent modelling technique would involve at least one experiment that includes an opponent in the strict sense, to demonstrate that the technique is effective and robust in environments where true opponents exist. In the conclusion, you state: "LIOM is agnostic to the type of interactions in the environment (cooperative, competitive, mixed) and can model an arbitrary number of opponents simultaneously." The method presented appears to only support a single opponent per environment instance, so it would be good to clarify this statement. This paper considers an algorithm for learning and using an opponent model that is only conditioned on an agent's local information (history of actions, observations, and rewards). Some suggestions for further improvement: I found Figure 1 confusing as I understand that the actual opponent policies do not have access to the latent variable Z, and that three items are predicted from the latent space: the opponent's actions, observations and rewards. For example, in addition to the two Ganzfried and Sandholm papers cited in this work, consider: Computing Robust Counter-Strategies, NeurIPS 2007, Johanson et al. Uses opponent observations at training time to compute robust counter-strategies, uses agent observations at execution time to choose which counter-strategy to play against the current opponent (who may not be one of the opponents used at training time).
Response to the author feedback: We appreciate the authors for the extra effort to demonstrate the abiltiy of FLAP by adding more experimental results and discussions. This is not a criticism, revisiting and modifying existing ideas and demonstrating that they are able to obtain state of the art performance is useful. The proposed method trains the policy using soft actor-critic on training tasks and trains an adapter network to predict task-specific final linear layer from a single timestep transition (s, a, r, s'). In the current manuscript, it is clearer that the "adapter network" predicting the "final linear layer" of the network is a unique component in this paper. For example, it would appear that the adapter network should not work on sparse rewards tasks (where the transition function is unchanged between tasks) or other situations where most experience tuples do not provide information regarding the specific task. Recommendation I recommend rejecting this paper because it is not clear why "linear representation meta RL" is better in general. In Figure 1, the proposed method showed about -200 reward in Cheetah-Vel (hard) tasks.
Summary: The authors advocate for the use of Robustness curves, plotting the adversarial accuracy as a function of the size of the neighbourhood region of allowed perturbation. The global robustness considered in this paper is robustness for varying perturbation strength. At the end of the introduction, the author say: "It is our belief that the continued use of single perturbation thresholds in the adversarial robustness literature is due to a lack of awareness of the shortcomings of these measures". I think one reason for choosing these values is that studying robustness under perturbation with larger distortion is kind of unnecessary because then the noise added is no longer imperceptible, which is at odds with adversarial examples' definition. If adopting robustness curves (or global robustness) as the evaluation criteria instead of point-wise robustness, how will this affect the existing adversarial training procedure? However, in this adversarial robustness community, the status quo is that researchers compare with each other on some specific datasets with some specific epsilon, for example, 0.3 for MNIST, 8/255 for CIFAR. This paper presents a theoretical scenario where point-wise measure of adversarial robustness falls short in comparing model robustness, then conduct experiments to show that robustness curve is a more meaningful evaluation metric from a global perspective. While I agree that you are going to get more information if you compute a full robustness curve than if you sample it at a bunch of points, I'm not convinced that it is worth the effort. One thing that I would recommend the authors is to make clearer the distinction between robustness curves as they described them (based on finding the closest adversarial example) vs.
It proves that the proposed procedure can recover a representation of documents that reveals their underlying topic posterior information in case of linear models. Using topic modeling as a tool to understand representation learning is interesting. And the authors explain the relatedness of the representation function and topics in a document. I like the idea of  using topic models as a way to represent the document level information, but it is disappointed to see that the proposed method doesn't provide as good performance as the simply averaging word embeddings. Overall, I admit that the suggested algorithm is a sound method to construct the document representation. The main strength of this paper is suggesting a sound and straightforward learning algorithm to construct document representation. The authors compare the suggested representation with models that do not explicitly make document representation (maybe except LDA). Word2vec is not developed for the purpose of document classification, online VB does not have classification performance as a strong point and bare BOW is surely not suitable as a realistic baseline. For a new document, the speed of inference seems depends on number of landmark documents, which could be slow.
The key assumption is that, though the training outputs can be skewed, it is easy to estimate the true distribution of the output. This paper proposed to learn a regression model using "skewed data", which is defined as the subset of training samples with true target above certain threshold. ########################################################################## Cons: (1) The proposed approach is based on the assumption that labeled data are skewed and unlabeled data follow the assumed true distribution. How can the latent vectors have same distribution as a distribution of labels. (1)     The paper assumes that the training data are often highly skewed (intentionally) but the true distribution of the output can be easily estimated or obtained. Updates: I thank the authors for their response. It's not clear how p(y) was estimated from the labeled dataset, where only samples with true target above certain threshold are available. How does it connect to using the information of the unlabeled data or the true output distribution? Current decision: 5 - until I read other reviews and understand some of the moving parts better. I will keep my original score.
The latent graph structure is generated through a fully-parameterized adjacency matrix or a KNN construction subsequent to passing node features to an MLP. "Self-supervised Training of Graph Convolutional Networks." arXiv preprint arXiv:2006.02380 (2020) This paper considers the problem of nodes classification with few labeled data and missing graph structures. However, in some domains (such as brain signals, particle reconstruction, etc.), there is access to only node features (but not the underlying graph structure). Also note that LDS doesn't always use the standard benchmark graphs (it either constructs a k-NN graph where k is a hyperparameter of the method or initializes with a subgraph of the given graph). There's little comparison of the learned graph structure with the original one (Figure 3.e only utilize node labels but not the original graph structure). Overall, though I like the paper in general, I believe the paper could be further improved and thus vote for weakly rejection.
The only difference is that NAO is use a decoder to decode the optimized latent representation back to architecture representation while here GOAL applies gradient descent on a graph neural network. Also, how to decode the parameters back to the neural architecture discrete representation is not clearly explained in the paper. Training the surrogate function for a larger search space or larger models can take more samples and more training time (e.g. large models takes much longer time to train, thus even a proxy accuracy should take more time to evaluate).
Summary stochastic subset selection (SSS) is a method to learn to compress a set D by selecting a subset Ds such that the loss of a task performed on Ds is as close as possible to the loss if the task had been performed on the original D. Appendix: there are several missing closing parentheses This work introduces a method to select instances from any set (stochastic subset selection, or SSS). However, the cost of the pixel selection by SSS is never analyzed. A couple ideas: (i) Running SSS just using the first stage (candidate selection).
Summary The paper studies a certain notion of spectral sparsification of directed graphs. For example, the notion defined in Cohen et al. (2017) enables fast approximate solution of linear equations, which is a primitive in various algorithmic tasks on directed graphs. Pros: Spectral sparsification of directed graphs is a relatively new field, so far with initial results that invite further research and improvements. In general I have doubts about fit to the venue; while *CONF* scope is broad and inclusive and spectral sparsification has certain potential connections to ML, this paper does not highlight any of them, and it is not entirely clear what it is attempting to achieve.
To improve multi-class classification problem using DNNs, authors propose to do multi-task learning of solving another auxiliary tasks which tells if data points are just noise/distractors or not. This submission proposes a training strategy that leverages background/noise data to learn robust representations. The auxiliary classifier is a binary classifier that discriminates training data versus background/noise data. The auxiliary classifier and the main classifier share the early layers. In other words, what if we just train a classifier with n+1 labels, where the extra label's training data are the background or noise data points? (2) is implicitly related to a good estimation of parameters, which in turn is related to better feature representations in the early layers (an implicit assumption here is that good features for the auxiliary binary classification are also good features for the multi-class classification, which could be a question by itself). It seems that the most natural and meaningful task is where the auxiliary classifier is trained on all in-domain data, both labeled and unlabeled, while the main classifier is trained on a small number of labeled data.
[+] Propose a method of applying TRUST-TECH to find local optimal solutions (LOS) of DNNs[+] Introduce DSP for exploration in high-dimensional parameter space[+] High ensemble performance through DSP-TT The proposed method is somewhat novel in the aspect of suggesting TRUST-TECH for an ensemble of DNNs. However, it is difficult to give a high score because the experimental results do not support The authors propose a method to obtain multiple local optimal solutions around the existing one. Unfortunately, I am not an expert in this field, but two papers I came across doing a very quick search appear to be somewhat relevant: "Local minima found in the subparameter space can be effective for ensembles of deep convolutional neural networks" and "MEAL: Multi-Model Ensemble via Adversarial Learning". However, I am still hesitant to give this paper a higher rating, in part because I find Section 5.4 to be poorly written and somewhat confusing (it lacks any sort of conclusion or insight and I cannot read Figure 4 at all, the text is too small) and partly because the paper does not clearly put its results in the context of the recent model ensemble progress (and thus makes me doubt the impact of this result on the field;
Detailed Comments: The paper uses contrastive learning idea proposed in SimCLR (Chen et al. 2020) to detect out-of-class samples and treat them in a different way than in-class unlabeled samples during semi-supervised learning. The authors proposed to address the task of semi-supervised learning (SSL) by contrastive learning techniques, and the proposed techniques can be applied to handle open-set unlabeled data (i.e., the label spaces between label and unlabeled data are partially disjoint). This paper considers the problem of semi-supervised learning, where the unlabeled data may include out-of-class samples. It also explores the idea of auxiliary batch normalization (from Xie et al. 2020) in the open-set SSL setting but the results of the ablation study suggest the level of improvement achieved by this normalization is negligible and the most of the improvement comes from more accurate detection of out-of-class samples through using the projection header function introduced in SimCLR paper. To address this task, the paper proposes a method consisting of three steps: (1) detecting out-of-class samples in the unlabeled set, (2) assigning soft-labels to the detected out-of-class samples using class-conditional likelihoods from labeled data, and (3) using auxiliary batch normalization layers  to help mitigate the class distribution mismatch problem. Then the paper filters outlier samples by the similarity measurement and further utilizes outlier samples with soft labels. ########################################################################## Pros: The paper addresses a very interesting and practical problem in semi-supervised learning, where the unlabeled samples may include out-class samples. Clarify: The preliminaries section clearly describes the setting of semi-supervised learning concerned in this paper and also clearly describes the contrastive representation learning. Out-of-class samples might still exhibit similarity with selected in-class categories, and thus forcing their soft labels to be a uniform distribution might not seem to be practical (if that's the case). What about other baselines? From Table 3 one can see that the main contribution comes from the detection of out-of-class samples.
Moreover, the introduction makes it sound like the function approximation assumption is merely for the policy class, but in fact the approximator must be able to represent the entire transition kernel, which is potentially much more complicated.
The authors identify the consensus distance as the key factor that affects the generalization performance of decentralized training. The main focus is to better understand the role of consensus, or lack there of, into the generalization abilities of decentralized training. It focuses on the so-called "critical consensus distance" and how disagreement during different stages of training ultimately effects optimization (training loss) and learning (generalization error). Theory is provided for the case of synchronous symmetric averaging methods, and the paper is complemented with detailed experiments on CIFAR and tiny-ImageNet. This is a nice contribution to the growing literature on decentralized training for deep neural networks. Yin, Pananjady, Lam, Papailiopoulos, Ramchandran, and Bartlett, "Gradient diversity: A key ingredient for scalable distributed learning," AISTATS 2018 and arxiv: 1706.05699 This work investigated a very interesting topic about generalization in decentralized deep learning. The authors consider the decentralized optimization problem and explain the generalization gap using the consensus distance. In general, the paper is well written and there are several interesting observations and discoveries involved regarding the generalization performance of decentralized learning. As also pointed out by reviewers 1 and 3, the gap between the convergence rate/consensus distance and the generalization capability still exists, causing the mismatch between the theory and the simulations. I expect the results to be useful to those working on decentralized training and am supportive of accepting it. I would expect that extending later phases would potentially allow to overcome issues due to large consensus distances in phase 1.
The Unsupervised Learning of Transformation Equivariant 2D Representations by Autoencoding Variational Transformations is used for 3D shape descriptor learning, which the authors claimed as "self-supervised" learning. Summary: This paper proposes a self-supervised learning framework for 3D object classification and retrieval based on multi-view representation, where a sub-task of transformation estimation is adopted as a regularizer. The authors propose a self-supervised learning technique for multi-view learning based on a simple intuition that the transforms of the 2D views of a 3D object will be in an equivariant manner as the 3D object transforms. -Secondly, since it has been suggested by many previous work that the joint-training of multi-task is helpful for the network, it will be appreciated that the authors provide more analysis and discussion on how the MV-TER loss helps the network learn transformation equivariant representation than simply using rotation as data-augmentation or using pose-estimation as sub-task. The authors need to prove the properties for Transformation Equivariant directly for 3D objects. (AVT) Another concern was critical but not yet addressed neither: The authors could propose a method that can be developed based on the "3D Transformation Equivariant" to 3D objects directly instead of its 2D projections. I feel this does limit the contribution somewhat. In my opinion there is enough difference to still recommend acceptance. For this question, I think the authors should prove the Transformation Equivariant Representations directly on a 3D object (point cloud, voxel, 3D mesh) instead of multi-view 2D images.
*CONF* 2020 [2] Lai et al. Contextual Grounding of Natural Language Entities in Images. Originality & Significance There are a lot of good contributions in this paper: natural language crowd-sourced manuals with two layers of "turk-indirection": not only was the text crowdsourced but first the templates for the texts were crowdsourced a comparison of several different techniques with different attention mechanisms experiments with additional evaluations of generalisation and robustness, including more difficult test scenarios, adding neutral distractor entities, fine-tuning with additional sources of punishment/reward, negating the text, and replacing entities with synonyms a visualization of attention to demonstrate that the attention mechanism is working properly (2) While the challenge of needing to learn which entities map onto which words is greater, each sentence refers to exactly one entity, so the agent just needs to match 3 sentences to 3 entities, which is a marginally greater challenge (if at all). The general idea is to learn a parameterized policy model that inputs the pair (entity [this paper] or visual representation [1] and text) and outputs the action of the agent in the game (correct me if I am wrong). To test their new model named EMMA (Entity Mapper with Multi-modal Attention), they also present a new toy game framework and crowdsourced data composed of 1320 games (from 3,881 entity descriptions with crowdsourcing). [1] Zhong et al. RTFM: Generalising to New Environment Dynamics via Reading.
The authors show that with this metric, they can find architectures with reasonable accuracy on CIFAR-10/CIFAR-100 in the NAS-Bench-201, while using much less search cost compared to previous NAS methods. This paper provides a reasonable start for a new potential direction in NAS research and so may be worth presenting at the conference, but the justification and applicability of the method is somewhat limited. But the common practice is to use the validation accuracy during search and report the final test accuracy. [*4], etc.) could affect the initial Jacobian a lot, but little on the final accuracy especially with BN, which is not compared in the paper (I suppose the comparison in Fig 7 (bottom-left) is performed with different runs rather than different initialization methods, please correct me if I make some misunderstandings). Section 5 mentions that "ori-test@12" is used as the metric during search.
The proposed approach consists of allowing each client to have a local model for personalization and taking a convex combination of global and local models as personalized preditors. In overall, I think the paper is technically sound, making reasonable contributions. In this paper, the authors propose a variant of FedAvg that not only produce the global training, but also a mixture of the local model and the global model, which is called personalized model. The empirical study is not conducted carefully: Allowing for a separate local model for personalization increases the number of parameters which is not accounted for It also does not seem appropriate for studying personalization as we know that there is a single global model with good performance (i.e. trained on the full dataset) and the key challenge is the optimization over heterogeneous datasets rather than the need for personalization. First of all, the manuscript proposed an adaptive personalized federated learning algorithm, where each client will train their local models while contributing to the global model.
The non-differentiability of the path planning is handled by recent work on differentiating through blackbox combinatorial solvers from [1]. In the conclusion: What exactly is meant by knowing the topological structure of the latent planning graph a priori? I still recommend rejection of this work given that the novelty is not yet fully clear. Path Planning using Neural A* Search (2020). [3] Learning latent dynamics for planning from pixels, Hafner et al. ICML-19. [4] A benchmark and evaluation for multi-task and meta reinforcement learning, You et al. CoRL 2019. [6] Scalable Planning with Deep Neural Network Learned Transition Models, Wu et al. JAIR. [5] Nonlinear Hybrid Planning with Deep Net Learned Transition Models and Mixed-Integer Linear Programming, Say et al., IJCAI-17.
I also agree that causal attributions of edges of edges are more reliable than information from gradients, often used to explain models. (3), the learned graph is not guaranteed causal, so I would like to suggest the authors not emphasizing "causal". How does this choice affect the goal of finding causal edges? The distinction between "irrelevant edges" and "redundant edges" should be important here if the goal is to find the "causal" subgraph. (3) adds each edge in a greedy way, which may result in a suboptimal graph. Second, the formula for causal effect estimation (Eq. (2) and Eq. This type of "causal" explanation is considerably more feasible for and better fits graph networks thanks to their sparse and modularized structure. The proposed method now only considers the edge importance while other comparing methods consider the importance of edges, nodes, and features. All in all, if the paper is improved on clarity during the revision period, I would lean towards acceptance. Cons: However, I have the following two concerns.
An reinforcement learning algorithm to learn a permutation invariant policy is derived. The authors identify a key property in the targeted resource allocation problems -- the permutation invariance -- which intrinsically implies the independency of samples at different time steps. This paper proposes an approach to reducing the sample complexity in multi-task reinforcement learning using permutation invariant policies. Questions: please refer to Some comments points 2) and 3) Def 1: "A policy network is PI if it satisfies pi(sigma(a), sigma(x)) = sigma(pi(a,x))" for any permutation sigma. The main assumption the paper makes is permutation invariance (PI). The permutation invariance property is defined in Def. 1. Since the paper talks about resource allocation, does the PI mean that it does not matter which entity the resource is being allocated to as long as the share of resource does not change? Also it seems that the symbol pi switches semantics a few times - first, it denotes a deterministic policy, then a stochastic policy, and finally a policy network. How is permutation invariance property used? I didi not understand how a network trained using gradient descent alone would satisfy permutation invariance. I feel the paper could have been better presented by starting with a motivating example where the permutation invariance property holds - for example the portfolio optimization example studied in the experiments.
I enjoyed reading it. This paper introduces a new method for computing the backward updates of a neural network called Direct Kolen-Pollack learning (DKP). Summary This work proposes an approach to update feedback weights in DFA using modification of kolen-pollack method, which helps in training deep CNN network. First Review Citation missing for key work on assessing the scalability of bio-inspired approaches and highlighting key limitations [Bartunov 18], variants of DFA [ Moskovitz 18, Frenkel 19]  and recently an approach similar to DFA with target projection known as LRA (also has similarity with Direct Kolen-Pollack) showing promising performance on deep CNNs [ Ororbia & Mali 2020]. In DKP backward matrices are no longer fixed as in DFA but rather are updated after each batch with their own update rule and learning rate.
The authors then propose a relaxed definition of disentanglement and show that it can be realized by means of a shift operator in latent space. The definition is: "A representation is said to be disentangled with respect to a particular decomposition of a symmetry group into subgroups, if there is a family of known operators acting on this representation, potentially distributed across the full latent, where each operator is equivariant to the action of a single subgroup." Using operators on the entire latent space is a new direction for the study of disentanglement. Even if one can question whether Def 1 is a good formalization of disentangling, the paper does show empirically that it is easier to learn an equivariant encoder/decoder when the latent operator is a shift operator or a diagonalized complex version of it, rather than a disentangled operator (with one 2x2 rotation matrix block and an identity block; An alternative definition of disentanglement is given, where instead of confining the effect of each transformation to a subspace, an operator is used that acts on the whole latent space (this operator is chosen as a shift operator, which works for cyclic groups).
The paper suggests a novel auto-encoder based method for manifold learning, by encouraging the decoder to be an isometry and the encoder to locally be a pseudo-inverse of the decoder. The authors propose a new version of the regularized autoencoder where they explicitly regularizes its decoder to be locally isometric and its encoder to be the decoder's pseudo inverse. Through a series of experiments and visualization, the IAE exhibits better manifold structure. Regarding the experiments, indeed the authors successfully show the IAE converges its decoder to be an isometry and the proposed regularizer promotes more favoured manifold. p. 2 Manifold learning generalizeS p. 4 Is there any possibility that the author can provide one more toy example for the global isometry when the data lie on some manifold shape? The projection operator that is used to define the pseudoinverse of the encoder is not necessarily a function, since there could possibly be many points on the manifold that correspond to the same L2 distance from the point being projected. The authors claim that isometric autoencoders would "evenly sample the manifold" which is a little confusing, since the sampling of the data manifold is separate from the technique used to model the data (regular AEs vs isometric AEs). In case there is a setting where isometric AEs can be shown to model the data manifold better than regular AEs, that is not highlighted in the current draft. The benefit of the Isometry Autoencoder is not well addressed. Why not,for instance, take all the images corresponding to some fixed digit (e.g. "3"), which is presumably close to a low (but definitely more than 2....)  dimensional manifold, and see how well your manifold learning algorithm reconstructs them?
Under the backdoor attack, an adversary can manipulate a few clients' weight matrices to affect the final global model. This paper's main idea to defend against a backdoor attack is to use clustering and adaptive clipping and noising. Weaknesses: This paper does not provide any theoretical guarantee and only applies the existing methods to mitigate the backdoor attacks. The author(s) have created many splendid terms to describe the modules used in this work, however, their implementation uses both clustering and median, which is very engineering and may not reliable with a different clustering algorithm or data set is severely unbalanced (just like the non-iid data sets among clients). After reading the response, I still think that the work is promising and would like to keep my recommendation. Using this clustering algorithm combined with adaptive clipping and noising, the proposed method can mitigate the backdoor attack. To impede backdoor attacks, many models are marked as outliers and discarded, clipped, and noised, generally speaking, which could lead to performance degradation. In FL, clients locally train model updates using private data and provide these to a central aggregator. Some analysis and ablation experiments are needed. An Embarrassingly Simple Approach for Trojan Attack in Deep Neural Networks. What is the cause of this phenomenon？ I believe no matter what kind of backdoor and Trojan attack, can be easily applied to FL by applying them individually on each client without too much trouble. Strengths: This paper uses an existing clustering algorithm (the HDBSCAN clustering algorithm (Campello et al., 2013)) that works best in the FL problem in identifying manipulated weight matrices.
(iii) simplifying the construction of adjoint SDEs by a pathwise formulation. This paper connects SDEs and GANs and proposed to learn the drift and diffusions in SDE under the framework of GAN. Still, I believe that there could be some improvements to do.
Short summary: The paper introduces a promising new method, hierarchical nonnegative CP decomposition (HNCPD), as well as a training method for the HNCPD, neural NCPD, for topic modeling problems. Potential Improvements: Equation (4) introduces the forward propagation for a NNMF, which is crucial for the HNCPD and Neural NCPD. Summary: In this paper, an extension of nonnegative CP decomposition called hierarchical nonnegative CP decomposition (HNCPD) is proposed. SUMMARY: This paper presents a hierarchical nonnegative CP tensor decomposition method. This method is designed to capture the hierarchical structure in e.g., topic modeling. It seems like the authors combine existing ideas (hierarchical and neural NMF, NCPD) into a new method. The originality seems moderate, as already existing concepts of neural NMF and Hierarchical NMF are applied to NCPD. In Section 2.1, I follow the discussion up to Equation (6). Is it the expression in the Frobenius norm in the paragraph titled "Hierarchical NMF (HNMF)" above? In Section 3.2, it is claimed that HNCPD is better than NMF because "the chromatic NMFs obscure much of the chromatic interaction". In the appendix, in the section "HNCPD expansion", in the 2nd sentence (starting with "We have that by definition..."), I think the square bracket "]" should be removed on the right hand side of the equation? In the experiments, do you use this approximation, or do you use the NCPD combined with HNMF for each factor matrix as discussed in the beginning of Section 2.1?
The paper proves a universal approximation theorem for equivariant maps by group convolutional networks in an extremely general setting. The main theorem shows how to convert fully connected networks to equivariant networks. See the comments below. Strong points: Important problem - invariant/equivariant models provide a very helpful inductive bias for many tasks on symmetric inputs. The main issues (that are detailed below) are: the paper does not sufficiently relate the discussed model or the universality results to previous or concrete models (set and graph NN, equivariant group NN, other unused but potentially useful variations), it does not provide sufficient explanation and justification to the different conditions in Theorem 11, there are some details in the proof and the description of Theorem 11 which are missing/unclear, Theorem 16 has some unclarity. As a result, the paper has improved and I increased my score. Since FNNs are known to be universal this implies universality of CNNs. I think the general CNN formulation and the Conversion theorem are of merit but i think the paper should undergo a rather serious revision before ready for publication. If I understand correctly, the idea of the proof is basically using the extension mechanism of Theorem 3 to extend mapping to functions over base domains to equivariant mappings. Is that correct? Can you provide the proof for a simple example of equivariant networks such as Deepsets? Universal approximation theorems are considered to be an important kind of result, and this paper proves a very general one for equivariant maps. Theorem 11. First, looking at the proof, I feel there is a condition of the FNN ϕ that is missing from the theorem's formulation. Related to that, I couldn't exactly understand the claim of the second to last layers in the proof of Theorem 11: the first layer outputs a function in C(G/HT×B), but the second layer of the FNN maps functions from some different domain C(B2). However, invariant functions can be made equivariant by considering the trivial representation, e.g., in the discrete case let f(x) be invariant to Sn, then fi(x)=f(x) for all i∈[n] is equivariant I believe. Minor: there are some wrongs equation references in the proof of Theorem 11.
When applied to Adam, the authors derive AdamS, supposed to work better than the previous AdamW, which already improved how weight decay and Adam interact. AdamS has the stable weight decay property, unlike Adam or AdamW. Statement 4, in my opinion, is one of key results in the draft. I think this paper has done a decent investigation on this topic. The paper main point then is to equate the learning rate in the weight decay coefficient by the effective learning rate and they show that this might be enough to bridge the gap between Adam and SGD. The main conceptual point of the paper is simply that the weight decay should have the same effective learning rate as the gradients, it would be nice if the authors could make this more clear and more central. Comments and questions: I think the concept of weight decay rate and total weight decay should be explained better, and earlier in the paper. In the draft, the authors can directly say constant or time-varying weight decay. The concept of "weight decay rate" and "total weight decay" seems to be the most critical part of the paper, but the explanation surrounding them was messy and hard to understand. The definition of the weight decay rate can be done from equation (2). In particular, the authors should strive to provide experiments on different training sets (ImageNet) with learning rate cross validation. A minor point, in Eq 3, how did the authors arrive at −2t−1 in the superscript of weight decay rate, not −2t+1? For example, Statement 1 that says "Equation-1-based weight decay is unstable weight decay in the presence of learning rate scheduler." can be quickly summarized even from an intuitive sense without any derivation, which makes it trivial. There is no need to give Definition 1 formally for Stable Weight Decay as that doesn't sound like a definition. They also make comments about reinterpreting weight decay as flattening the loss and increasing the learning rate which they don't pursue further nor connect with the main point and it seems a little out of context. The authors trick is to average the value of the moving average of the squared gradients along all dimensions before using it to rescale the weight decay. Verifying the stable weight decay property is actually not optimal, because it is not isotropic. Additionally, I am a little confused about the statement that the effect of weight decay can be interpreted as flattening the loss landscape of θ by a factor of (1−ηλ) per iteration and increase the learning rate by a factor of (1−ηλ)−2 per iteration. Review The reformulation introduced in equation (3) is an interesting alterntive view to look at weight decay, but I find it is not really used by the authors. I increased my score to a 6 because I think the paper in its current form is enough to get accepted, but there are still improvements that could be done to make it much stronger.
This paper presents an estimator that predict higher-order structure in time-varying graphs. Definition of SC is not clearly presented and I believe bias/variance terms for the estimator may need further intuitions and explanations. None of the existing methods were designed to predict higher-order simplices from existing lower-order simplices. Perhaps the experiment should show its advantage when dealing with much higher-order structure predictions rather than these simple cases.
Summary: A tensor network model for text classification is introduced, which is constructed as the concatenation of a generative matrix product state (MPS) model for low-dimensional word embedding and a discriminative MPS model for classification. On that note... The "TextTN w/o word-GTNs" baseline in the ablation study (Table 3) seems rather misleading, as the paper text describes this as "we directly average the word vectors of words in a sentence to obtain the sentence representation".
The authors investigate different tokenization methods for the translation between French and Fon (an African low-resource language). That being said, I strongly agree with the authors that neural machine translation of African low-resourced language is important. I agree with all of your points about what is lacking, but in my mind, the novelty was enough to still give a 7. I think this paper can reasonably be rejected, but I'd like to give actionable of constructive criticism, since I do think the work on this low resource language is important for the NLP community.
Summary: This paper introduces "source-aware" GMM attention and applies it to offline, online, long-form ASR. The paper describes a simple extension to the location-only monotonic GMM attention mechanism from Graves (2013), which takes the source/key context into account when computing attention weights. For example, in the experiments, CTC is used as an additional loss to guide the learning of monotonic attention. Please clarify. I would even group the Transformer, GMM, and SAGMM-tr rows together, since they provide the control experiments to support the usefulness of the proposed method. This claim is repeated in paragraph 2 of Sec 3: "uni-modal similar to conventional GMM attention". Other comments: Sec 2.2: Sutskever et al., 2014 did not use content-based attention.
Center-wise Local Image Mixture For Contrastive Representation Learning Positives: interesting proposed method for expanding the neighborhood space of considered positive matches for contrastive learning ablation study provided to show the improvement from each proposed component (sample selection, cutmix, multi-resolution) While the improvements that the CLIM augmentation brings seem to be quite good in the linear evaluation on ImageNet by boosting the results of MoCo v2 (which is used as a baseline if I understood correctly) by 4 points, the improvement of this representation on other downstream tasks (ie. Ablation study proves positive impact of each of the two proposed improvements on the final performance. And the contribution of this paper is applying CutMix in the context of contrastive learning, which is yet another augmentation among a huge variety of possibilities. "Contrastive learning targets at learning an encoder that is able to map positive pairs to similar representations while push away those negative samples in the embedding space." does not sound right, especially usage of word "those", rephrase. hyper-parameter selection: there are several different parameters to be set in the proposed work: multi-resolution scales, number of clusters and neighbors for CLIM, alpha in cut-mix, with the differences in accuracy between settings approaching the difference between say knn+cutmix and center-wise+cutmix. Recommendation My initial recommendation is leaning towards reject. Neutral: overall novelty is moderate; I would consider the main novelty to be in the selection of positive matches, as the cutmix and multi-resolution augmentations are largely leveraging existing ideas. p.4 "Cluster-based method regards all samples that belong to the same center as positive pairs, which breaks the local similarity among samples especially when the anchor is around the boundary." It is not very clear how the proposed method differs from the clustering-based methods in this sense. Overall summary: Given the overall improvements from the proposed method, I'd be inclined toward accept, if the concerns I raised regarding the empirical evaluation were addressed. The paper presents an improved positive sample selection and data augmentation method for unsupervised, contrastive representation learning.
This paper presents a graph topology learning algorithm based on SBM and neural networks, which employs the node embedding and labels to optimize the topology. This paper proposes a joint learning framework for GNN classification model and graph topology, which leverages variational EM as a learning framework. Strengths: The paper tackles an important question in the GCN literature, which is how to deal with situations in which the graph is unobserved or the observed graph structure is only a fraction of the true graph. From table 2, the accuracy improvement is slightly better than table 1 in the scarce-label setting. In my opinion, the over-smoothing is caused by the depth (or receptive field) of GNN and the message passing manner, but irrelevant to the graph topology. In summary, I agree that topology optimization is beneficial to enhance the node classification performance, but its effect on surpassing over-smoothing is suspicious. Assuming that we remove all the inter-class edges and connect all the intra-class edges in the graph. The idea of joint topology optimization and node classification is not new.
The paper's aim is to establish that linear layers can be replaced by butterfly networks and uses three different experiments to show this. Specifically, the paper proposes replacing the encoder with a truncated butterfly network followed by a dense linear layer. It is a novel idea (only to my knowledge) to combine the butterfly network and FJLT to optimize the neural network architectures. I have the following comments and questions that should be clarified to evaluate the relevance of the results: When comparing with other architectures on image classification tasks (CIFAR10 and CIFAR 100), what is the dense layer that is replaced by the butterfly structure?
Summary The authors propose LIME: a pretraining strategy for learning inductive biases for mathematical reasoning. Each of these pretraining tasks is a sequence-to-sequence mapping involving 3 basic components of reasoning (Rule, Case, and Result), where two of these three components are provided as input and the third component is the target output. But this result does not necessarily mean that the model has the inductive biases that were intended to be imparted; it's possible that LIME imparted some other inductive biases that are also useful for mathematical reasoning but that are not related to induction, deduction, and abduction.
This works aims at unifying 3 popular regularisation type continual learning methods, namely EWC, SI and MAS, by showing that under some assumptions they all relate to the Fisher Information Matrix. It shows that MAS and SI approximate the Absolute Fisher matrix. Pros Connecting the path integral of SI to the Absolute Fisher is interesting. The accompanying experiments are crucial and well-conducted. The paper is well-organized and easy to follow. However, I have an issue with the proposed quicker/cheaper update for calculating the (diagonal empirical) Fisher Information Matrix for Online EWC ("Batch-EF"), as I detail later. While the paper is well-written and -structured, and SI and MAS are popular methods in the literature, I'm not convinced by the link through the "Absolute Fisher". The authors merely show that MAS and SI are similar to using Absolute Fisher as importance weights. The authors show that the importance weights of MAS and SI are similar to the diagonal absolute Fisher. Cons of paper I am not convinced that the minibatching that the authors suggest (both for SI as an approximation to the AF and for EWC in the last paragraph of Section 5) is correct ("Batch-EF"). I think the authors should justify that the Absolute Fisher is an optimal regularization under certain assumptions. I am also not convinced that OnAF ("Online Absolute Fisher") and AF are / should be the same. It appears to me that by minibatching instead of squaring each gradient element, one should obtain much worse approximations to the empirical Fisher information matrix ("EF"). First, estimating the Fisher as the square of the averaged gradients has been done before, for example it is implemented in the tensorflow KFAC codebase. The assumptions make the use of the diagonal Fisher information an optimal choice. Therefore, I cannot agree with the claim that this paper presents a unified framework, just because it fits several methods into distinct concepts that have "Fisher" common in their names. Although I am not aware of previous works using the absolute value of gradients (as in AF), this paper provides evidence that such an approximation might be worth considering. I think some claims can still be reduced, particularly, the link between AF and EF (and hence the link to EWC). Finally, I'm not quite convinced by the potential impact of the insights, which seem fairly specific to choosing the batch size of SI, so overall I think the paper still is below the acceptance threshold. I however found several limitations while reading your paper: Limitations regarding MAS Given that the relationship between AF and F has now been addressed, I will increase my score.
In Appendix Summary: The paper proposes an additional stage of training for unsupervised NMT models utilizing synthetic data generated from multiple independently trained models. References: [1] Data Diversification: A Simple Strategy For Neural Machine Translation, Nguyen et al. [2] APE at Scale and its Implications on MT Evaluation Biases, Freitag et al. [3] On The Evaluation of Machine Translation Systems Trained With Back-Translation, Edunov et al. [4] Multilingual Denoising Pre-training for Neural Machine Translation, Liu et al. [5] Leveraging Monolingual Data with Self-Supervision for Multilingual Neural Machine Translation, Siddhant et al. [6] When Does Unsupervised Machine Translation Work?, Marchisio et al. In this paper, two unsupervised agents are utilized at cross-model by using the dual nature of the unsupervised machine translation model, in which forward translation of agent_1 is combined with the backward translation of agent_2, more synthetic translation pairs are obtained to train a new supervised machine translation model. The result is improved on multiple unsupervised machine translation, and this paper claims that more diversity is brought to the synthetic data, so a better translation model can be trained. This paper uses a reconstruction BLEU or BT BLEU [1] metric to compare the effect of the inside-model with that of cross-model, and finds that cross model translation has a lower back-translation effect, which shows that the diversity is enhanced. The authors add this additional stage of training to unsupervised NMT models using different pipelines (PB unsupervised MT, Neural Unsupervised MT, XLM) and show that their approach improves all of these approaches by 1.5-2 Bleu on WMT En-Fr, De-En and En-Ro. Strengths: The paper is well written, the approach is simple and seems to improve quality by significant amounts in a variety of experimental settings. They can directly add the synthetic data decoded by cross model to continually train the original XLM model with a supervised translation objective (which is naturally supported in XLM from my experience), and report the effect comparison between them. [1] Li, Zuchao, et al. "Reference Language based Unsupervised Neural Machine Translation." arXiv preprint arXiv:2004.02127 (2020). Weaknesses / Questions for authors: As with any NMT model trained with synthetic data, it would be better to report results on source and target original splits of the test data to provide a clearer evaluation [2,3]. Because the training of CBD is divided into two stages, the diversity of the second stage only brings more training data to enhance the supervised machine translation model, rather than unsupervised machine translation effect. Source of promotion: the second stage of CBD method adopts (x_s, y_t), (z_s, y_t), (y_s, x_t), (y_s, z_t) synthetic translation pairs, it is not clear how much performance growth comes from increased data and how much growth comes from the new model implementation (ott et al., 2018). In particular, a more detailed discussion of this method's relation to multi-agent dual learning would be worth giving up parts of Related work that are already mentioned in Background (like pre-neural statistical unsupervised MT). Recommendation: Overall, this is a good paper and I would recommend acceptance.
Focusing on the projected dynamics, the equivalence is built between SGD and a type of "Adam". Hence, the sentence "performing SGD alone is actually equivalent to a variant of Adam constrained to the unit hypersphere" in the abstract is misleading and conveys over-optimistic information. I believe the manuscript would benefit from major revisions to be of interest to the community and my initial recommendation is a rejection.
Summary: This paper introduces a fairness perspective on accuracy performance among distinct classes in the context of adversarial training. This paper begins with the empirical observation that adversarially trained models often exhibit a large different in clean (and robust) accuracies across different classes. EDIT: Score changed from 6 to 5 during discussion, see comments below. In this case, it seems that even before adversarial training, we see a difference in class errors between the classes. There were some points in the paper which I believe could be improved.
This paper proposed a powerful online sequential test which can efficiently detect qualitative treatment effects (QTE). This paper studies online test for qualitative treatment tests. The test algorithm involves adaptive randomization, sequential monitoring and online updating. The authors propose a scalable online algorithm for Type 1 error control. a nonasymptotic upper bound on the type-I error for the online updating, However, I don't see that the algorithm addresses any real online challenge.
PABI can measure various types of incidental signals such as partial labels, noisy labels, constraints, auxiliary signals, cross-domain signals, and their combinations. This paper proposes PABI (PAC-Bayesian Informativeness?), a way of measuring and predicting the usefulness of "incidental supervision signal" for a downstream classification task. Mathematical developments of PABI are given for these cases, and experiments show that PABI is nicely positively correlated with the relative improvement that comes with various methods for integrating incidental supervision signal (including one which is developed as a side note by the authors). ########################################################################## Summary: This paper proposes a unified PAC-Bayesian-based informativeness measure (PABI) to quantify the value of incidental signals. In NER and QA tasks, they showed the strong correlation signals between PABI and the relative improvements for various incidental signals. Computing PABI may be challenging in some cases. Summary This paper proposes a unified measure for the informativeness of incidental signals (ie, not standard ground truth supervised labels) derived from the PAC-Bayesian theoretical framework. It also is not clear to me from the paper's text whether something close to the PABI framework can apply in broader settings like language modeling style pretraining, where the input-output format of the incidental supervision signal is different than that of the target task. However, even if the empirical results are good, the connection between PAC-Bayes and the proposed informativeness measure (named PABI) is vague. Recommendation Accept. Important problem, lots of solid content, clear benefits over previous work and directions for the future.
Then, the authors design SSWR metric evaluate the efficiency and quality of the search process. At the end of section 3, after presentation of SSWR, it is not clear why we are minimizing for a search sequence generator G that is aggregated over database-query pairs (D,q) -- wouldn't we learn a data structure per database (as is done for data structures used for nearest-neighbor search)? However, I am currently leaning towards a reject because of two main reasons: The first being the clarity of the presentation in the paper that makes it hard to identify
Although the self-supervised loss is intuitive, incorporating it into MONet is non-trivial and it does outperform MONet. Despite that such self-supervised works are hard to work on real scenes, this paper does have some merits. I am willing to increase my rating. In all, I do not think it meets the *CONF* bar. The self-supervised idea is interesting that uses the segmentation mask to get the pseudo bounding box label for object detection, which could ensure the consistency of object mask and bounding box. [Paper Weakness] The self-supervision between segmentation masks and detection bounding boxes is the main contribution. MONet uses spatial attention to identify the most salient object one by one, which makes senses. In the original MONet, the spatial attention network is an RNN-like structure, they decompose the scene step-by-step. Update: In general, I am happy with the authors' responses. Post rebuttal comments: Thank you authors for the detailed response - I think some of of my concerns have been answered - the paper may be a valid contribution to the community and I am raising my score. This paper presents a variation of the MONet model where an additional Region Proposal Network generates bounding boxes for various objects in the scene.
I don't see how the theory in Section 2 is crucial for the understanding of the impact of β. It is a close decision. The choice of optimal β still remains unclear and left to the future work. As the CIFAR-10 experiment points out, using small beta can still lead to good performance, so the main problem for small beta seems to be instability (rather than slow training), which the current theory couldn't explain. To improve the theory, I hope the paper can provide more insights into the instability caused by small beta. Ideally, I hope the theory can be extended to explain why small beta causes instability (the conclusion section mentions this as future work), and/or how neural network architecture affects optimal beta, but these extensions do not seem obvious. I am leaning towards rejecting the paper.
This paper proposes a generalized additive model to learn joint intensity functions for multiple Poisson processes. For example, a Gaussian process is a stochastic process, but doesn't have anything to do with the processes considered in this work. So my first question is: i) Can you define the likelihood in a single Poisson process model in order to consider all the interactions you mentioned in section 2.2? For example, on page 2: Given a realization of timestamps t1,t2,…,tN with ti∈[0,T]D from an inhomogeneous (multi-dimensional) Poisson process with the intensity λ.
The long-range convolutional (LRC)-layer mollifies the point cloud to an adequately sized regular grid, computes its Fourier transform, multiplies the results by a set of trainable Fourier multipliers, computes the inverse Fourier transform, and finally interpolates the result back to the point cloud. Pros: Presents a long range convolution layer, with an efficient implementation so that a neural network model can benefit from both short and long range interactions among data points. Probably there is a regime where direct convolution (which does not need the approximation introduced by g_\\tau, as far as I understood), is better then the NUFFT approach introduced here. However, the authors ignore pointing out the existing issues. An efficient method for fitting long-range style interactions in point clouds is presented. In conclusion, I think the paper is marginally above acceptance level.
The paper proposes to separate the dominant factors and the residuals,  train the original DNN on residuals with a much faster SGD learning rate,  and then recombine with a shallow small NN learned on the dominant factors trained using its own (slower) learning rate. If we were to remove the dominant factors, the residuals would have much weaker correlation structure and allow faster DNN convergence with SGD. Sec. 2.1. Using lower-case kappa for a matrix is strange,  I initially assumed it's a scalar. 2 is very dense and difficult to follow, and only seems to motivate the method in the special cases of a very shallow linear regression model, where the gradient is easily related to eigenvalues of the input features X (and assuming a "strong factor structure", which has been shown to be reasonable for natural images.) Missing obvious baselines of training separate models on either the factor features or residual alone. Another criticism -- is that there is no discussion of how to do large-scale PCA / factor analysis for high-dimensional image data arising in say modern DNN image classification pipelines, and its computational cost,  as simple numpy.linalg.svd won't work. Perhaps this is the motivation for only processing "factor features" with an extremely shallow (albeit still nonlinear) network? Sec 3.3: "a standard SGD algorithm cannot be used to train a DNN model". At least for MNIST I'd expect decent performance to be obtainable from the factor features alone. Additional details: Prior work -- that should be cited / contrasted with your approach:(a) Since a substantial part of the paper analyzes linear models,  it's important to mention that factor structure has been long exploited in various ML / stats works. Overall I feel that the paper is not ready for publication at this time since the experimental validation is incomplete and does not fully explore the benefits and potential downsides of the proposed method.
The proposed, dataset-internal contrastive self-supervised learning benefits in zero- or/and few-shot learning settings. Another concern is the unclear description of how two binary labels are optimized in a contrastive way. ########################################################################## Reasons for score: Overall, my score is marginally below the acceptance threshold.
The paper proposed a learned variant of the well-known iterative Hessian sketch (IHS) method of Pilanci and Wainwright, for efficiently solving least-squares regression. It shows how such learned sketches can be used in two types of problems: Hessian sketching (Sec. 3) and Hessian regression (Sec. 4). The present paper considers a particular kind of sketch for which the sketch matrix is learned from data. Summary: Previous work has shown that sketching the Hessian can improve the running time of second-order optimization methods. Overall, the idea of combining IHS with learned sparse sketch is interesting, but the reviewer believes that the current version needs significant amount of rework to be publishable in top conferences. For example in Figures 1, 2, 6, 7, that the learned IHS has the same linear convergence rate as the unlearn IHS, and actually only slightly faster overall (only 1 iteration faster if we view the plots horizontally), but the authors' claims are like "We can observe that the classical sketches have approximately the same order of error and the learned sketches reduce the error by a 5/6 to 7/8 factor in all iterations", "We observe that the classical sketches yield approximately the same order of error, while the learned sketches improve the error by at least 30% for the synthetic dataset and surprisingly by at least 95% for the Tunnel dataset. Theorem 3.2: Given the error in Lemma 3.1, the bound in Theorem 3.2 needs to be updated accordingly. I like the paper, but there are a few issues that must be addressed before it can be published.
In International Conference on Medical Image Computing and Computer-Assisted Intervention (pp. Considering multi-modal multi-task settings in the clinical domain is useful for the development in this area. Either alone might not be grounds for rejection, but together they likely put this work below the acceptance threshold.
Specifically, they propose two regularization terms to 1) capture the diversity of the tasks and 2) control the norm of the prediction layer, thereby satisfying the assumptions in meta-learning theory. To improve the practical performance of meta-learning algorithms, this paper proposes two regularization terms that are motivated by two common assumptions in some recent theoretical work on meta-learning, namely ########################################################################## Summary: The paper reviews common assumptions made by recent theoretical analysis of meta-learning and applies them to meta-learning methods as regularization. Results show that these regularization terms improve over vanilla meta-learning. [2] HOW TO TRAIN YOUR MAML, *CONF* 2019 The main motivation of this paper is based on the theoretical results of meta-learning. Summary: In this paper, the authors aim at bridging the gap between the practice and theory in meta-learning approaches. To ensure the assumptions of the theories, the authors propose a novel regularizer, which improves the generalization ability of the model. (1) the optimal (linear) predictors cover the embedding space evenly, and Ref: [1] Andrei A. Rusu, Dushyant Rao, Jakub Sygnowski, Oriol Vinyals, Razvan Pascanu, Simon Osindero, Raia Hadsell: Meta-Learning with Latent Embedding Optimization. This work serves as a nice attempt to instruct the practice of meta-learning with theoretical insights. These theoretical assumptions have not been paid enough attention before. In some experimental results, the improvement due to the proposed regularization seems to be at the same level of the standard deviation, as well as the difference between the reproduced results of existing meta-learning algorithms and those reported in earlier papers. *CONF* 2019 Above all, since the contribution and the technical details to calculate the subgradients are not clear to me, I have to currently recommend a weak reject.
compared to existing graph pooling methods, the authors think their methods are able to capture information from all nodes, collect second-order statistics, and leverage the ability of neural networks to learn relationships among node representations, making them more powerful. Weaknesses: My biggest concern is that the proposed approach lacks originality and novelty because it is a simplification and variant of SOPOOL from Second-Order Pooling for Graph Neural Networks (Ji and Wang, 2020) Wang and S. Ji. Second-order pooling for graph neural networks. Based on the author's writing, it is unclear what is the second-order statistics for graph pooling, why it is important to have second-order pooling, and how the proposed method can capture the second-order statistics. The proposed graph pooling method is only experimented with 1 underlying particular choice of GNN (Xu et al., 2019), so it is unclear how well the method can perform on other GNN architectures. Wang and S. Ji. Second-order pooling for graph neural networks.
[2] VariBAD: A Very Good Method for Bayes-Adaptive Deep RL via Meta-Learning. The optimal Bayes-adaptive policy explains when it is optimal to explore a new task and learn adaptive behaviors, depending on the horizon length and amount of exploration needed, and therefore, the main claim of the paper can be framed as observing that existing meta-RL agents can learn the Bayes-adaptive optimal policy in simple tasks. Summary. The authors investigate the question of when the optimal behavior for an agent is to learn from experience versus when the optimal behavior is to apply the same (memorized) policy in every scenario. If the research question is "How does the optimal policy depend on task parameters such as uncertainty and horizon?" I believe Bayes-adaptive work answers that question. Summary: This paper observes that in meta-RL (and evolutionary biology), sometimes it is advantageous to learn behaviors that adapt to the particular task, while other times not adapting to the task, and instead relying on a task-agnostic "hard-coded" behavior is sufficient. Specifically, this paper presents three main findings: (i) whether or not it is optimal to learn adaptive behavior strongly depends on the horizon of the task and complexity of learning such adaptive behaviors — if the horizon is too short, or if the adaptive behavior requires complex exploration, then exploring the new task to learn adaptive behaviors may not be worth it; Similarly, the grid world tasks in Section 4 also clearly illustrate how the behavior of meta-RL agents changes with the horizon of the task. (ii) existing meta-RL agents are capable of choosing not to learn adaptive behaviors, when it is optimal to do so; Empirical evidence in two sets of experiments, on a 2-armed bandit toy task and a grid-world navigation task, show that hard-coded strategies can be a function of training task distribution and task complexity as well as task horizon. There are a number of relatively minor weaknesses (as described above) but this is overall a nice paper and would be a good contribution to *CONF* 2021.
This work presents a theoretical model formulation to capture the biased effects of incorrect labels automatically The authors claimed that, by adjusting α through training, the trained model m(⋅;θ^) with an optimal parameter θ^ is asymptotically consistent with the model trained on a dataset with clean labels, i.e.,  the trained model without α performs well on clean test data Note that this trivial solution does not depend on the model m(⋅;θ). Adding experiments with synthetic datasets with different levels of noise can be helpful in understanding the advantages of AC1/AC2 over other methods for handling noisy labels.
I think that the authors explore some interesting connections to recent work on identifiability in latent variable models, and understanding what these results imply for causal inference is important. Feedback Identification of causal effects in the presence of proxy variables for unobserved confounders is an important but subtle problem. Thus again, could the authors provide a causal graph representing their assumptions about the role of each variable, most importantly z and x, similar to Figure 1 of Louizos et al. (2017)? Recommendation: Reject. In summary, I am rather convinced that the contribution of this paper is important, but lacks clarity in its arguments and clarifications w.r.t.
This paper targets at alleviating this IR bottleneck by proposing hybrid-regressive translation (HRT), which combines AT and NAT in two stages. This paper proposes a hybrid-regressive machine translation (HRT) approach—combining autoregressive (AT) and non-autoregressive (NAT) translation paradigms: it first uses an AT model to generate a "gappy" sketch (every other token in a sentence), and then applies a NAT model to fill in the gaps with a single pass. = If I understand correctly, the advantage of HRT in handling different batch sizes and computing devices mainly comes from its usage of one-pass NAT. Besides, one thing should also be noticed: the speedup yielded by HRT seems less charming compared to recent NAT models, such as Levenshetain Transformer 3x-4x (Gu et al, NeurIPS 2019), and JM-NAT (k = 10) 5.73x (Guo et al, ACL 2020) where both models achieve similar translation quality to the AR baseline. Non-autoregressive decoder (NAT) greatly improves translation efficiency, but often relies on iterative refinement (IR) to retain the translation performance.
############################################################################### Summary The paper presents a novel method for learning branching strategies within branch-and-bound solvers, which consists in a graph-convolutional network (GCN) combined with a novelty-search evolutionary strategy (NS-ES) for training, and a new representation of B&B trees for computing novelty scores. ############################################################################### Pros and cons Pros: the idea of learning branching strategies using reinforcement learning instead of imitation learning makes a lot of sense and seems like a promising direction to follow the proposed representation of B&B trees is original the presented results seem promising It is known that node selection and branching strategies do interact with each other, and all the evaluated branching methods were proposed in the context of a default, state-of-the-art solver. Would the "default" setting be more representative for evaluating the performance of branching strategies ? p.4 Section 4.2: Primal Dual POlicy Net -> I don't really understand this primal-dual thing, given that you use the same features as in Gasse et al. (Table 3) and just replace their GCN model by a simpler, underparameterized version (A.2.2). However, I remain concerned about the experimental setup in the paper, and therefore my final recommendation is still rejection.
The paper proposes a knowledge distillation method for face recognition, which inherits the teacher's classifier as the student's classifier and then optimizes the student model with advanced loss functions. The paper demonstrates using an ensemble of teacher models can boost the performance of knowledge distillation. The idea of using teacher model's classifier to directly reshape the student model's feature representation is somewhat novel. Second, ProxylessKD can be interpreted as initializing classifier of student model by the classifier of teacher model. ECCV2020 This paper proposes a new KD method to inherit classifier from teacher models and utilize it to train the student model feature representation, where previous KD methods are mostly focusing on the proxy task other than the target task itself. Since the optimization objective for student model is learning discriminative embeddings,  the face recognition performance is improved compared to the vanilla KL counterpart. It needs sufficient analysis to justify the authors' choice of only applying the teacher model's classifier as distillation. It would be interesting to see how performance changes with more layers of student model inherited from teacher model.
Before Author Response Of course, if the authors address my concerns then I'll increase my rating. Specifically, it proposes PeerPL to perform efficient policy learning from the available weak supervisions, which covers PeerRL (for RL with noisy rewards), PeerBC (for imitation learning from imperfect demonstration) and PeerCT (for hybrid setting). The relationship between the RL and BC solutions appears to be superficial, however, as PeerRL and PeerBC seem to address noisy supervision in very different ways. The decomposition of the noisy reward in lines (7) and (8) seems to assume that, even in the absence of noise, the expected reward signal will be zero, regardless of the policy being followed, or the current state.
The paper proposes a benchmark for the evaluation of unsupervised learning of object-centric representation. While I realize that the objective of the paper is to create a common evaluation background, I fail to see a sufficient level of quality and of novelty. Overall, this paper is interesting in setting up a benchmark for unsupervised object representations which is a very important problem in computer vision, reinforcement learning, etc. [1] SPACE: Unsupervised Object-Oriented Scene Representation via Spatial Attention and Decomposition, *CONF* 2020 The paper presents an empirical evaluation of a number of recent models for unsupervised object-based video modelling. The benchmark consists of three datasets, multi-object tracking metrics and of the evaluation of four methods. Paper Strengths Although I am not familiar with unsupervised learning of object-centric representation, I like the idea of proposing a common protocol for evaluation. Overall, I think the paper could be a nice contribution to the literature on unsupervised learning of object-centric representation, but it lacks sufficient contribution for *CONF*, in my view. the experimental and the insights it gives, again, can foster the community towards better model, but it's not a sufficient contribution for *CONF* in my view.
The paper proposes a federated learning framework using a mixture of experts to trade-off the local model and the global model in a federated learning setting. The paper proposed a novel personalized federated learning method using a mixture of global and local models. The mixed-use of global and local models (equation 6) is not a novel way of federated learning. If equation (8) is applied, this means that local data is used to learn the global model through w_g, so there is a leak of local information which contradicts the strict respect of privacy. https://arxiv.org/abs/2007.14513 Overall Score Given the above concerns, I recommend reject this paper in the current stage. "Extended phases of local training between communication rounds can similarly break training, indicating that the individual client models will over time diverge towards different local minima in the loss landscape.
Paper Summary The paper considers the cooperative multiagent MARL setting where each agent's reward depends on the state and the actions of itself and its neighbors The paper has a theoretical claim that, for such reward structure, the optimal maximum entropy joint policy in the form that can be factored into potential functions, one for each agent. For the assumptions on rewards, Proposition 1 assumes that each agent's reward depends on its neighbors, while the derivation of Equation (3) (and thus the following algorithm) further assumes that the reward depends on pairwise actions. Although the authors emphasize that they are communicating the intentions of agents, I think their method is quite similar to those communicating local observations, like NDQ (https://arxiv.org/abs/1910.05366), DGN, or CollaQ (https://arxiv.org/abs/2010.08531). Proposition 1: "The optimal policy has the form ... =-= comments after author discussion For the baselines used in the experiments, it seems that only IP and DGN allow communication/message passing during execution, which makes it unsurprising that the two methods outperform other baselines. However, the exact assumptions are not clear, and the chain of issues discussed throughout section 4 seems to include discussion of approximation. However, the exact assumptions are not clear, and the chain of issues discussed throughout section 4 seems to include discussion of approximation. Major Comments/Questions Although the motivation has an interpretation of intention propagation, the resulting architecture (Figure 1b) and loss functions (Section 4.2) seems to be a standard messaging passing architecture with SAC loss functions that loses the intention semantics. The cited PRL article (Levine 2018) seems to retain this standard use of optimal: it uses a distribution over trajectories with an equation similar to here (a softmax over accumulated trajectory rewards), and makes use of the property that trajectories corresponding to an optimal policy have maximum probability in that distribution. The usual definition of optimal policy would be a value maximising policy, which would be an argmax rather than a softmax. Proposition 1: For clarity, explain the intention of psi. What makes this principled? This would seem to need a clear statement: what are the exact assumptions, and what precisely is the quality of the output? I'm not an expert in the area and wouldn't expect to follow all of the reasoning in constructing the method, but I would be expect to be able to follow some clear statements of the algorithm, or its theoretical properties (guarantees of some solution quality given certain assumptions, the parameters affecting this, etc.
In particular, the authors focus on sub-8 bit quantization and propose a novel integer linear programming formulation to find the optimal bit width for a given model size. Tuning batch norm weights by re-computing statistics. The methods include AdaQuant (which jointly optimizes quantization steps for weight and activation per output activation of each layer), Integer Programming (which determines bit-precision for all the layers), and the batchnorm tuning. Page 8: For instance, on the extensively studies -> For instance, on the extensively studied This work presents a quite comprehensive multi-step scheme for post-training neural quantization that does not rely on large datasets or large computational resources. The quantization process covers nicely the various different parts of the errors that  post-training quantization induces and propose somewhat original solutions to them. In more detail, the authors claim four proposed components: AdaQuant, Integer programming, Batch-norm tuning, and two pipelines for neural quantization. Below are some of the errors that I caught: 3 OPTIMIZING QUANTIZATION PIPLINE -> PIPELINE Note that the impact of quantization in the earlier layers affect the quantization impact in the current layer. Also, it seems that the "per-channel" quantization method is utilized in this work, but the formulation in (2) seems to be for "per-layer" optimization. BOPS proposed by (https://arxiv.org/pdf/2005.07093.pdf) is a good metric to measure the total reduction in computations for mixed precision quantization. This is a good result but please note that other work in the literature (arxiv:2001.00281) reports 72.91% for INT8 quantization of MobileNetV2 (this comparison is actually missing from the paper). It is straightforward to think that the joint optimization of quantization step size for weight and activation would result in better quantization results. Note that the biggest merit of post-training quantization is its simplicity (cf., QAT incurs full-blown training epochs); thus increased cost for post-training quantization is not desirable. Some spelling mistakes, e.g., "Optimizing Quantization Pipline" Baselines are barely discussed. Also, I am not very familiar with quantization papers, so I might have missed relevant baselines, but they seem hard to compare. I think section 4 can be titled simply "Quantization flow". "3. OPTIMIZING [THE] QUANTIZATION PIP[E]LINE" "model['s] internal statistic[s]" -> found twice Overall: An engineering oriented paper with some lack of testable hypotheses and analysis of some parts of the methods, but the 4-bit results justify publication.
It proposes to use Hamiltonian Monte-Carlo (HMC) to sample the next states (instead of IID samples) and matrix completion to learn a low-rank Q matrix. In reference, McAllister and Rasmussen: ta-efficient -> data-efficient In this paper, a Hamiltonian Q-learning is proposed by combining Hamiltonian Monte Carlo with matrix completion. The so-called Hamiltonian Q-learning takes minimization optimization and essentially claims an equivalence of energy in physics model, which might not always make sense. More descriptions about the ocean sampling problem would be helpful for the readers to understand the task.
This work does a great job to demonstrate that NS-GAN is most definitely not "superior" to MM-GAN and may possibly be a worse choice of objective function. My recommendation: I am not an active member of the GAN community so I am more than willing to accept if my recommendation goes against more senior folks who work in that field. When GANs were originally proposed, most researchers saw the NS-GAN objective as a strict improvement over the MM-GAN objective and moved on. It sheds light on the weaknesses of the log -D variant of GANs. Weaknesses: Paper title is broader than what the paper shows - the proposes explanation only applies to GANs with the log -D generator objective and does not apply to other GAN variants, e.g.: original GAN (with cross-entropy generator objective), WGAN, LSGAN etc. Since a near 20-point increase in FID can be achieved with a few tweaks to the NS-GAN objective (SN-GAN), I am left wondering if the presented improvement from the MM-NSAT objective will vanish once those improvements are applied or if it will still hold. Also, for eq. (7), technically the optimal discriminator is only uniquely defined at the real data points (see Sinn & Rawat, AISTATS 2018) because the GAN is trained on a finite sample. I am not an expert in the GAN field, but from inspecting the Conv-4 model with spectral normalization on CIFAR10, it appears like the best performing model achieves an FID of approx 42 and the worst model gets around 48 (figure 8, bottom right). It is unclear how the number of samples from O in the minibatch could cause a difference between MM-GAN and NS-GAN - this observation is true for both MM-GAN and NS-GAN! Cons: The proposed objective is not novel [1][2], however, it is derived directly from the original MM-GAN objective and is better theoretically motivated. In the next paragraph, the paper mentioned that the generator gradient "is only locally informative" - again this is true for all GANs. How is this relevant for the argument made in the paper? The baseline NS-GAN with spectral normalization presented in this work greatly underperforms previously published methods that use the same objective. This model uses the NS-GAN objective (to my knowledge) so, I am confused as to why the authors did not simply replicate their setup Sect. 2.4 on "MM-GAN Interaction with ADAM" is not very mathematically rigorous and relies primarily on an assumption that the value of the logits of the discriminator approaches the optimum linearly. If the experimental setup was more in line with prior work and the same trend in results held, then I would be more likely to recommend acceptance of this paper.
Ride: Rewarding impact-driven exploration for procedurally-generated environments. 2020. URL https://openreview.net/forum?id=rkg-TJBFPB. [2] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. I do not agree with the statement: "We note that MiniGrid provides a sufficiently challenging suite of tasks for RL agents despite its apparent simplicity, as ICM and RND fail to learn any effective policies for some tasks due to the difficulty posed by procedurally-generated environments." RIDE (i.e., rewarding impact-driven exploration) is an intrinsic reward signal (Eqn. I recommend rejection, though I am certainly open to changing my mind, especially if the case can be made that the benchmarks are exhaustive enough, or that it's unreasonable to expect more benchmarking within a single publication.
And this extension is mainly based on the idea from algorithmic fairness literature which considers the worst-case environments and solves a bi-level optimization. I have several points for clarification that I detail below. (2) Similarly, the paper does not provide any theoretical analysis of EIIL for algorithmic fairness. ---post rebuttal--- After reading the authors' response, the other reviews, and the revision to the paper, I find that my comments are not sufficiently addressed. The author did not even acknowledge the existence of the prior work, REPAIR, in the revised paper. Firstly, how do you derive eq. The paper develops its own algorithm EIIL which extends the Invariant Risk Minimization (IRM) of domain generalization to work in the situation when the prior knowledge of environments is not available.
Annotating images for training of segmentation models is time consuming and it can be difficult to annotate enough examples to ensure good performance on the rare difficult examples that often occur when methods are applied to real world data. The paper therefore proposes a measure based on the discrepancy of a group of segmentation models to identify more valuable images to annotate and add to the training data in a iterative fashion. Leveraging those counterexamples to improve the segmentation models' generalization performance on unseen images seems to be novel in this field. Weakly-supervised labeling is more practical for segmentation; and (2) extending to active training/tuning, leveraging the selected hard examples to improve the segmentation model for multiple rounds. "This also provides direct evidence that existing segmentation models could be particularly weak at certain real-world generalization, which is not surprising because the 1,464 training images are deemed to be extremely sparsely distributed in the space of natural images." I am not sure I agree with the evidence part. Detailed comments "First, segmentation benchmarks require pixel-level dense annotation", I do not believe this is necessarily true, and there is little need to state this. "Specifically, given the target model ft, we let it compete with a group of state-of-the-art segmentation models {g_j}^m_{j=1} by maximizing the discrepancy (Wang et al., 2020) between f_t and g_j on D." They are not really competing are they? That is, they consist of examples that the proposed segmentation model disagrees with the "competing" models the most on. Recommendation: Given the lack of a competitive baseline, opaqueness around the impact of the algorithm's hyperparameters, and what's likely to be noisy estimates of improvement, I cannot recommend this paper for acceptance as is.
Based on these factors I will raise my score from 'clear rejection' (3) to 'okay, but not good enough' (4). This work, however, raises some questions. I therefor cannot support the acceptance of this paper.
arXiv preprint arXiv:1906.05274 (2019). The paper presents a pre-training scheme (APT) for RL with two components: contrastive representation learning and particle based entropy maximization. DETAILED COMMENTS C1) The exploration component of APT has striking similarities with the method in [1], which also seeks the optimization of a k-NN estimate of the state distribution entropy in a reward-free context. Strength: Pre-training good representations and policy initialization without reward is obviously an important direction in RL. Authors claim that the main benefit of APT over a prior work method (MEPOL, Mutti er al., 2020) is a lower variance of the gradient estimation, thanks to the choice of avoiding importance weights corrections. EVALUATION Unfortunately, over some concerns regarding the novelty of the presented method and its experimental validation, which I find somewhat weak for an essentially empirical work, I would lean towards rejecting the paper.
This paper proposes a neural network optimized by MLM loss that has inductive bias to be useful for unsupervised constituency and dependency parsing. From a CNN on the input sentence, the model predicts a syntactic "height" for every word in the sentence and a syntactic "distance" for every inter-word position; these heights and distances determine the joint constituency & dependency parse of the sentence. The model shows improvements in MLM perplexity over a standard Transformer baseline, and the authors present it as offering competitive unsupervised parsing scores relative to other models. The dependency parsing performance is far behind the current SOTA (neural DMVs), even when we account for the fact that it does not use the POS information. The model outperforms ON-LSTM in constituency parsing and is competitive with classical NLP methods that use gold POS tags in dependency parsing. Table 3 doesn't seem consistent with Table 1b: none of the dependency results in Table 3 match the 41.0 result for StructFormer listed in 1b. However, there are a number of unclarities in the paper that make it difficult to determine whether the results on unsupervised parsing are actually comparable to prior work. The model is trained for masked language modeling (MLM) and evaluated via MLM on held-out data and its ability to induce constituency and dependency trees. The results are better than trivial baselines for unsupervised constituency and dependency parsing but not as strong as related recent work. This paper describes a neural architecture that resembles the transformer but includes explicit representations of constituency and dependency structure. If I'm correct about this, it constitutes clear grounds for rejection of the paper.
while the exploration step estimates the sampling distribution according to the sampled data in exploitation step and rectifies it to sample all data possibly. This work presents an interesting exploration of learning optimal data sampling probability. Advantages: l    The exploitation step and exploration step in AutoSampling is interesting, it is straightforward that this method can work well as the sampling strategy is updated dynamically according to the current state of model. This work is taking a novel aspect for effectively training neural networks, since rare effort has been devoted to make data sampling learnable. First I will comment on the listed contributions: • To our best knowledge, we are the first to propose to directly learn a robust sampling schedule from the data themselves without any human prior or condition on the dataset. To address the issue of optimizing high-dimensional sampling hyper-parameter in data sampling and release the requirement of prior knowledge from current methods, the authors introduce a searching-based method named AutoSampling. The exploitation step train multi child models with current sampling strategy and save the best model for next iteration. The chosen baselines are essentially standard sampling scheme or variants of the proposed method. If H* is the "optimal sampling schedule" and then P is defined to be the ratio of x in H* over all x in H* then it is not clear to me what you are converging to. Typo: "Algorithm 2: Search based AuoSampling" The authors mainly concentrate on data sampling. l    The transferability of the gained optimal sampling schedule is discussed in Section 4.4, a simple experiment is recommended.
Summary The paper introduces two simple modules, SelfNorm (SN) and CrossNorm (CN), that are highly modular and can theoretically be attached to different parts of the CNNs to control the balance between style and content cues for their recognition. In particular, this paper proposes to recalibrate style using SelfNorm motivated by the fact that attention help emphasize essential styles and suppress trivial ones and reduce texture bias using CrossNorm by swapping feature maps within one instance. Detailed comments are summarized as follows: Pros: - The paper contributes a solution by forming a unity of opposites in using style for model robustness. In general the motivation is very high level drawing a lot on the concepts of style, texture and content but the method itself is rather down to earth modifying instance normalization parameters. It is difficult to agree that SN and CN, which are argued to control style and content for the benefit of the recognition task at hand, are really working as speculated. The rating reflects this disappointment. .  Section  . .  Data  . .  Arch  . .  Evaluation  . .  Baselines  . .  Authors' methods  . .  Tab1  . .  CIFAR  . .  4 archs  . .  mCE,CleanAcc  . .  Cutout,Mixup,Cutmix,AA,Advtr,AugMix  . .  SNCN, SNCN+AugMix .  Fig2  . .  CIFAR  . .  28-2WideResNet  . .  mCE,CleanAcc  . .  WA,RA  . .  CN .  Fig4  . .  CIFAR  . .  40-2WideResNet  . .  mCE  . .  VanillaModel  . .  SN,CN .  Tab4  . .  CIFAR  . .  40-2WideResNet  . .  mCE  . .  VanillaModel  . . CN .  Tab5  . .  CIFAR  . .  40-2WideResNet  . .  mCE  . .  VanillaModel  . . SN,CN,SNCN,SNCN+Crop,SNCN+Crop+CR .  Tab2  . .  ImageNet  . .  ResNet50 . .  mCE,CleanAcc  . .  PU,AA,MaxBlur,SIN,AugMix  . . CN,SN,SNCN+AugMix I do observe a few improvements introduced by the two modules here and there, but I can't forgo the impression that these are only selected highlights that comply with the authors' arguments. The paper presents two new methods to improve corruption robustness and domain generalization: SelfNorm, a way to adapt style information during inference, and CrossNorm, a simple data augmentation technique diversifying image style in feature space. I do however like the method a lot and I would vote for accept if it was rewritten substantially to focus more on an understandable presentation of the method than abstract concepts and colorful terms. - The authors explains SelfNorm recalibrate feature style while  CrossNorm performs style augmentation.
The benchmark is a combination of existing data sets and newly created ones, and covers a variety of applications and tasks, from small molecules to RNA or protein structures, and including classification, regression and ranking tasks. Bioinformatics, 35(3), 470-477. This paper presents a large benchmark of machine learning tasks for molecules represented by the 3D coordinates of their atoms. A systematic benchmark with atomistic learning methods is presented, showcasing the value of using 3D atom-level data instead of 1D or 2D features. By creating a standardized set of prediction tasks and associated data sets, the authors have presented a resource that may help the community to compare 3D atomistic methods quickly and fairly. Some tasks like Ligand Binding Affinity (LBA) and Ligand Efficacy Prediction (LEP) requires modeling representations of both proteins and small molecules. Actually, the abstract (and, more generally, the paper) reads as if neural networks were the only kind of machine learning algorithms that could be applied to molecules and that very little work has been done in the past to incorporate 3D information in chemoinformatics. I would really refrain from using "atomistic learning" to describe what the community has been referring to as "learning from 3D molecular representations" for decades. Swamidass, S. J., et al. "Kernels for small molecules and the prediction of mutagenicity, toxicity and anti-cancer activity." Bioinformatics 21.suppl_1 (2005): i359-i368. Mahé, P., et al. "Graph kernels for molecular structure− activity relationship analysis with support vector machines." Journal of chemical information and modeling 45.4 (2005): 939-951. The idea of using atomistic learning or at least 3D derived features have already been implemented or at least contemplated in many of the presented tasks (Gilmer et al [1], Wu et al [2], Townshend et al. [3]). [2] Liu et al: N-Gram Graph: Simple Unsupervised Representation for Graphs, with Applications to Molecules However, I personally feel that this work could be a better fit for a more biologically inclined venue.
Secondly, the paper empirically demonstrate that in sparse training, 1) weight decay and data augmentation can hurt model accuracy, 2) batch normalization plays significantly role for model accuracy in sparse training and 3) non-saturating activations boost the magnitude of effective gradient flow and consequently improve model accuracy. Overall: This paper examines many experiments to improve the optimization of sparse networks, but I am not convinced that the approaches are the most effective (how important is EGF and comparing sparse to same size dense).
Summary: In the paper, the author(s) propose 1) a method to dynamically adjust the sampling rate on radar data using 2D object detections (algo1) and previous image and radar data (algo2); 2) an end-to-end transformer-based 2D object detection model using both radar and image data. While it is generally a great idea to guide the selection of radar regions to be sampled at a higher rate the paper is very application-focused and lacks novelty in its method. Using camera to select most important radar regions contradicts the stated advantage of radars to perform better in adverse weather conditions Post-rebuttal review: I carefully read through the rebuttal and other reviews and  I would stick to my current rating. On page 3, "The radar data is split into 8 equal regions, in azimuth and 37 equal regions in range." Should the block size be 8 x 37? Overall, I think the paper is not good enough to be accepted by *CONF*.
Especially jointly learning concepts from frames and instructions while considering the hierarchy. For example: Regeneration of low level concepts from high level concepts: what are we expecting from a network that moves from a "high in the hierarchy" concept to a "low in the hierarchy" concept? The cons include (In my opinion, the main weakness is the experiments): Some design of model (e.g., Traversing across the concept hierarchy, Observation, and Instruction Regeneration, etc) are not fully estimated in this section: are they really useful? Introducing a two-level hierarchy into concept learning is also not new. The paper introduces the solution of an important task: hierarchical concept learning(or temporal abstractions) from demonstration data. Final recommendation Overall, I believe the paper as it stands is not ready to be presented to *CONF* and I recommend a rejection. This paper addresses the problem of extracting a hierarchy of concepts in an unsupervised way from demonstration data. Specifically, this paper considers 1) unsupervised setting; 2) the hierarchy of concepts, and conducts experiments in two datasets. The two-level hierarchy has been seen a lot in relevant fields such as video recognition. The authors may need to look upon those for a better variety of baselines and also evaluation metrics. As the network is unsupervised, the starting time and ending time of each concept is quite crucial and the problem was tackled by training using a soft-DTW loss. How do you define the level of concepts and how did you balance the levels in video and language?
(3) In Theorem 4.1, the assumption that T>=n^4 (n^4 can be very large) is the disadvantage of this algorithm because the same convergence rate O(1/sqrt(T)) has been achieved without such assumption in some distributed settings, including plain distributed SGD, federated average, etc. (6) On page 2, the authors said "SwarmSGD has a Θ(n) speedup in the non-convex case, matching results from previous work which considered decentralized dynamics but which synchronize upon every SGD step." What is the measure, is it the number of communications, local SGD iterations or gradient evaluations? Is the coefficient $\\frac{n - 2}{n}# in Eq (18) missing? What does the effect of quantization on the convergence rate and the communication cost? However, if the analysis can not explain why more local updates can reduce communications, I would not recommend to accept. Update Thanks for the authors to address my questions. define T (global number of edge updates) and H (number of local updates in between edge communication); Still, it would help to clarify the following points from the beginning: what the authors mean by decentralized, later explained as decentralized model updates, but centralized/distributed data for the experiments; where and when quantization is applied and why it helps in reducing communication complexity in the main text. (4) The claim in the abstract that the new algorithm can converge to local minima is not supported, since the theorems only imply gradient convergence. It would be also interesting to report communication complexities, with and without quantization, and compare them to state-of-the-art methods. Can you also show the run time plot for ResNet? For example, For Theorem 4.1, use 1≤r2λ22 can get rid of the constant 1. For example, if we optimize the convergence rate in Theorem 4.1 over H, the best choice is H=(λ22r2⋅f(μ0)−f∗L2M2)1/3. Further questions: Is it possible to merge Section I with Theorem 4.1 or show the proof? The benefit of local steps is not clear. This bound however stands for the average of all models obtained at each global step t, meaning that it is not necessarily a tight bound for the second moment of the last obtained model, which is the bound we are ultimately interested in. Optional improvements: It may be better to remove some small terms to make rate more clearer. For (14) and (19), use rλ2≤r2λ22 to get rid of the first order term. The 3rd equation in Section D, h~is also depends on g~i, which is not reflected.
The methodological contribution of the paper is more or less an off-the-shelf use of LSH for negative sampling. [Edit: thanks for clarifying this.]
Thanks for the discussions. The primary reason for my score increase is discovering that the power of the framework is finding a representation that is quick at exploiting new opponents. Re-reading after the paper update, I am worried that a significant portion of readers may fall into the same trap despite the authors' additional edits. Science, 2019. After rebuttal: The responses address most of my main concerns, and I have increased the rating from 5 to 6. Questions during rebuttal period I think several questions have already been raised in the rest of my review. After seeing the authors' responses to my concerns, I am open to raising my score. Without further assumptions on the class of games, I do not really see why the base policy would be having a good expected reward before adaptation (either in average over a broad class of opponents or against the optimal opponent), or even less why it would be hard to exploit (especially after adaptation). Then, given a base policy and a "hard-to-exploit" opponent, more diverse opponents are sequentially generated (in a procedure coined diverse-OSG) by optimizing their expected reward against the base policy while maximizing their "diversity" with the "hard-to-exploit" opponent and the already generated diverse opponents. Score I am recommending reject [UPDATE - see above] - although I will maintain an open mind due to my middling confidence in some of the background literature around the paper.
Summarization of the contribution: This paper presents a novel ADversarial Meta-Learner (ADML), the claimed first work that tackles adversarial samples in meta-learning. The idea of combining meta-learning with adversarial defense has already been proposed and well studies, such as in "Adversarial Attacks on Graph Neural Networks via Meta Learning, *CONF* 2019". In particular, ADML formulates two task optimisation problems (Eq. 2) for the same variable (the initialisation) - one is adaptation under clean data and one under adversarial data. First, only FGSM is used as the adversarial sample generation method and other well-known techniques are not considered. Quality The paper does not follow established procedures for evaluating adversarial robustness: the threat model is not clearly stated the main evaluation in the paper is done based upon a weak attack, namely FGSM Recommendation Because of the lack of motivation for adversarial meta-learning, lacking intuition for ADML, and particularly the robustness evaluation against a very weak attack, the paper is a clear reject to me.
Introducing MACAW: an algorithm for offline meta-RL that has the desirable property of being consistent (i.e. converges to a good policy if enough time and data for the meta-test task are given, regardless of meta-training). To do so they rely on MAML (which provides consistency) and AWR (a simple, popular offline RL algorithm) and add a couple of changes: some hyper-network like parameterization to add capacity and adding an extra objective in the policy update to enrich the inner loop. They argue that a naive application of MAML with advantage weighted regression (a recently proposed approach to offline RL) is insufficient for this setting and propose to make the policy more expressive by including an advantage head that regresses the advantage conditioned on the state and action. In experiments they show that this outperforms the offline metaRL method PEARL, and combining multi-task offline RL with AWR in a naive way. In the experiments on 4 MuJoCo benchmarks the proposed method outperforms two baselines, Offline PEARL and Offline MT+FT. The authors also explore settings of good/bad adaptation data showing the robustness of the proposed method to quality of offline adaptation data as compared with MAML+AWR Extensive ablations on the various modifications to MAML+AWR confirm that the utility of the approach for the fully offline meta-RL problem. However, in the offline setting the policy that generated the batch of data is of critical importance and should be part of the task definition. It could be online with the meta-train tasks remaining offline, but then we would have to meta-learn an exploration policy, which isn't done in this work. The offline meta-RL formulation should include behavior policy as part of the task definition. My comments: While this paper touches on a very interesting and practical problem in meta-rl and batch-rl, I didn't find their setting is very realistic with respect to batch and offline RL setup. It would still be great to add an experiment where MACAW is adapted online at test time (entirely without offline data) like in C.2 but on in-distribution tasks. A main motivation for the offline RL setting is that you can use real-world data and train a metaRL agent using this. This is one of the key differences, IMO, between meta-RL and offline meta-RL and given that this paper's main contribution is introducing offline meta-RL, I feel it really should be very clear about this point. For instance, IMO figure 1 should contain multiple examples of the same "RL task" that are different "offline RL tasks"; i.e. learning to swim using guidance from a 3-year-old and learning to swim using guidance from Michael Phelps.
The authors establish an experimental protocol to understand the effect of optimization nondeterminism on model diversity, and study the independent effect of different sources of nondeterminism. The empirical analysis is systematic and the two main results are thought provoking and interesting: 1) Different sources of nondeterminism (such as random initialization, data augmentation, data shuffling, etc.) causes similar levels of variability (based on standard deviation and correlation metrics), and Pros: Show that different sources of nondeterminism give similar level of variability effect on the final accuracy and loss. Some questions: given such robustness in the level of variability across different sources of nondeterminism, is it possible to predict the level of variability from the data, architecture, etc.? Is there anything more fundamental about the particular value of variability that all the different sources of nondeterminism concentrate around? If so at later epochs when the nondeterminism is activated, the learning rate is lower and the model cannot explore and change as much and so reduction in variability may in part be due to the learning rate. ######################################## Recommendation: Overall I lean towards rejection.
Pros: AFT shows better asymptotic space and time complexities than MHA. What are the benefits of AFT compared to recent established efficient transformers such as Sparse Transformer (Child et al., 2019), Reformer (Kitaev et al., 2019), or Linear transformer (Katharopoulos et al., 2020)? Under what circumstances we should expect AFT to reach vanilla transformer performance and still offer clear efficiency benefits when using the same setup? [1] https://arxiv.org/abs/1901.10430 This paper introduces the Attention Free Transformer (AFT), an alternative to multi-head attention (MHA) operation in Transformer.
The paper investigates the over-parameterization of attention heads in Transformer's multi-head attention. They propose a reparameterization of multi-head attention allowing the parameters of queries and keys to be shared between heads: this is called "collaborative attention". The authors show that query-key projections are redundant because trained concatenated heads tend to compute their attention patterns on common features. Overall, the paper is well motivated and provides a deep analysis of redundancy of the multi-head attention. Since the method operated only within attention layers (reduces only dimensions of queries and keys), in terms of efficiency/quality trade-off it should be compared to other methods, e.g. simple head pruning. If you look a deeper look into Transformers, in case of BERT, the attention block only takes around 25% of total parameters. Namely, while the original definition of attention layers does not have biases in linear projections for q/k/v in attention, the authors claim that implementations contain the bias terms, and spend some time showing how to model the biases in key and query layers properly. Additionally, the part with the biases in attention implementation is misleading. (minor) As a side contribution, the authors claim to report the discrepancy between the theory and implementation of attention layers. For example, if you state explicitly that in practice pruning may be simpler, but your results say/illustrate something other than practical applications. Update after author response Context and content attention Overall recommendation Overall, I can not recommend accepting this paper. In the current state, I think it is ok :)
There exists a prior work that bridges a gap between the two exploration methods for linear policies, and this paper generalizes the prior work for various deep RL methods: on-policy (A2C, PPO) and off-policy (SAC). Experiments show that the proposed exploration strategy mostly helps A2C, PPO and SAC in three Mujoco environments. Another important advantage of the approach is that while the policy is non-markov (due to the "global" trajectory-based exploration or coherence), the policy gradients can still be estimated in a more-or-less standard, step-based, way, thanks to analytical integration of the "latent" variables (basically the parameters of the last layer), hereby overcoming the challenge of high variance in PG estimate for non-markov policies. Originality: As far as I know, the proposed technique is novel in the literature of undirected exploration. *In section 5, apart from the comparison of the performance of the learned policy, the comparison of the complexity (which might be measured by wall time to learn the policy?) of different exploration strategies can also be interesting. But for the three bullet points in section 1, the first point of "Generalizing Step-based and Trajectory-based Exploration" should not be one of the main contributions of this paper, because this paper follows the formulation of policy in van Hoof et al. (2017) and the latter proposed the generalized exploration connecting step-based and trajectory-based exploration. The paper focuses on exploration, but the experiments only focus on the return performance of simple Mujoco tasks. Summary of the paper The basis of this work is van Hoof et al., 2017; there, "Generalized Exploration" views policy parameters as being drawn from a per-trajectory Markov chain.
Motivated by the fact that NF are diffeomorphic  transformations from a simple space, ideally Euclidean, the author address the problem of modeling data which distribution is defined on more complex and unknow manifolds.The idea consists in inflating the data manifold with suitable noise (normal noise) in order to make it diffeomorphic to a simpler space where a NF can be estimated. In practice, Proposition 1 shows that the noise can be drawn from a Gaussian with variance depending from the curvature of the manifold. The denoising auto-encoder is famous as a manifold learning method that adds pseudo noise. It is shown that when D>>d, Gaussian noise is an excellent approximation of noise restricted in the normal direction and the experiments seem to confirm this.
Examples include, this sentence from the introduction (yes, it's one sentence): "We first mathematically prove that, by greatly shrinking the graph of the search space, reducing the operators' complexity in magnitude, lowering the required searching period from 50 epochs to one iteration and significantly easing the Matthew effect, namely that the complex operators may never get the chance to be well tuned, thus the found architecture only contains very simple operators, and performs poorly on the testing set, FTSO reduces the required parameters by a factor of 0.53×108, decreases the FLOPs per iteration by a factor of 2×105 and significantly promotes the accuracy compared to the baseline, PC-DARTS." After reading the response and the comments of peer reviewers, the rating is altered as follows. Strengths: The search time is reduced significantly from normal DARTS and PC-DARTS.
Unfortunately, they are not sufficient to help the understanding, as the upper bound derived is just the TD error used in the prioritized experience replay. The second theorem shows that the surprise, multiplied by a constant that depends on the policy, is an upper bound to EVB in the soft RL setting. Could sequential replay be the result of using an underlying metric that is less myopic than EVB, which seems to shows better promise over PER? Notably, that when we know it upper bounds the three metrics, we want to continue prioritizing sampling experiences when training our agent, because this will yield faster learning, since we will have larger improvements to our agent.
Summary: This paper studies the detection and recovery problem in spiked tensor models in the form T = \\beta v0^\\otimes k + Z, where v0 is the underlying spike signal and Z is a Gaussian noise. The paper presents a pair of interesting algorithms using trace invariants to detect the signal in the signal-plus-noise tensor PCA framework. The authors claim that: 1) they "build tractable algorithms with polynomial complexity", "a detection algorithm linear in time"; 2) the algorithms are very suitable for parallel architectures; 3) an improvement of the state of the art for the symmetric tensor PCA experimentally. Page 8: "eg" should be "e.g." Summary: The paper provides an interesting algorithm for tensor PCA, which is based on trace invariants. Theorem 7 claims that Algorithm 1 and 2 run in linear time. It will be very hard to follow the proofs if the definitions are unclear. I am not able to follow the proofs in this paper due to missing definitions of terms and notations. This becomes worse considering the fact that this paper studies tensor problems -- many tensor-related terms have multiple definitions (e.g., eigenvalues, ranks). Theorem 4, 5, 9, 10 do not have complete proofs.
#Summary This paper studies zero-shot fairness where the demographic information is partially unavailable, but assuming the existence of a context dataset that contains all labels x all demographics (including the invisible). This paper introduces the problem of enforcing group-based fairness for "invisible demographics," which they define to be demographic categories that are not present in the training dataset. #Pros Zero-shot fairness is a very important topic under many practical settings, where the demographic information can be (partially) missing due to sampling bias or privacy reasons. The paper states an interesting and practically relevant problem of enforcing fairness with "invisible demographics." The methodology is overall well documented, and the experimental baselines make sense. In addition, how does the distribution of the label x demographics on the context dataset affect clustering quality? #Cons The biggest concern I have is the clustering part of the context set into a perfect set. In the case where a dataset has zero labeled examples for some demographic groups, this is such an extreme situation that it is a clear red flag that there is a large bias in the collection process and/or the data collection design was poorly done---what is the justification for continuing to use this dataset as is? In AIES'19. It is a rare situation where records with a specific combination of the target class and demographic group are missing. My recommendation is rejection. The main reason is that I have concerns about suspicious behavior in the experimental results.
In this paper, the authors propose Sandwich Affine strategy to separate the affine layer in BN into one shared sandwich affine layer, cascaded by several parallel independent affine layers. (1) "Specifically, the number of independent affine layers in the SaBN equals to the total number of candidate operation paths of the connected previous layer." Are you sure it doesn't equals to the total number of candidate operations in next layer? It does not appear that the approach imposes any restrictions or regularization on the CCBN affine parameters; therefore the only difference between CCBN and SaBN seems to be a different parameterization. (2)What is the difference between the searched architectures by using BN, CCBN and SaBN? (2) I think it will be better to add the SaBN in Fig3, and point out the correspondence between the candidate operations and the conditional affine layer.
(2020). Sophisticated Inference. arXiv preprint arXiv:2006.04120. This paper features two complementary contributions: The perception and control as inference (PCI) framework, which describes the graphical model of a POMDP with auxiliary optimality variables, allowing for the derivation of an objective for optimizing both a perception model and policy jointly to maximize log⁡p(O,x∣a). The authors propose a framework for joint perception and control as inference (PCI) to combine perception and control for the case of POMDPs. I could imagine that the joint objective would cause OPC to perform a bit better than world models at convergence, but I am surprised that the perception is apparently so difficult that a separate modeling objective never gets off the ground at all. One of the major contributions in the paper is a well worked-out joint inference derivation, which performs amortized inference in this shared model utilizing an RNN. Finally, the novel aspect of the control objective isn't actually used in practice, as a simple TD error method is used to train the policy, which others have done without taking a "RL as inference" stance. Experimentally, the authors verify their joint model on a waterworld environment, which consists of simple object shapes with semantics. Comments/questions: I do not feel like the way the abstract and introduction argue for a general framework "Joint Perception and Control as Inference" is productive. As mentioned during the introduction, this isn't a particularly novel framework (considering RL as inference, all the work done in temporal generative model for RL, or the rather new formulation by Hafner 2020 [4]) and it doesn't appear like it provides a very interesting fit to the object-centric RL model proposed. Summary This generalization of the control as inference formulation could be influential given a more didactic presentation of the PCI derivation and an evaluation better suited to its claimed strengths, but it does not seem quite ready for publication yet.
This conditional network approach is illustrated for two standard convolutional neural network (CNN) architectures, U-Net and VGG, on two benchmark datasets suitable for OOD detection, the Inria Aerial Image Labeling Dataset and the Tumor Infiltrating Lymphocytes classification dataset. The paper demonstrates the experiments on two tasks (i.e. semantic segmentation, image classification), but the proposed conditional network is different and does not have a unified architecture. It appears, instead, that the conditional network learns a smaller collection of features more relevant to the task." However, there are some concerns and questions outlined above which I believe need to be addressed / adapted in order to accept this paper.
