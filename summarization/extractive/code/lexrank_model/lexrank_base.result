The authors describe a method called WAGE, which quantize all operands and operators in a neural network, specifically, the weights (W), activations (A), gradients (G), and errors (E) . Afterward, the authors conduct experiments on MNIST, SVHN, CIFAR10, and ILSVRC12 datasets, where they show promising results compared to the errors provided by previous works.
-------------- Summary: -------------- This paper presents a series of experiments on language emergence through referential games between two agents. Then how do the agents effectively differentiate so well between 20 images leveraging primarily color and shape?
Even the control task is very similar to the current proposed task in this paper. The papers encode the mode as a hidden variable in a stochastic neural network and suggest stepping around posterior inference over this hidden variable (which is generally required to do efficient maximum likelihood) with a biased importance sampling estimator. The scheme is motivated by the fact that this estimator has a lower variance than pure sampling from the prior. They apply this approach to multi-modal (several "intentions") imitation learning and demonstrate for a real visual robotics task that the proposed framework works better than deterministic neural networks and stochastic neural networks. The authors propose a new sampling based approach for inference in latent variable models. The proposed objective is based upon sampling from the latent prior and truncating to the largest alpha-percentile likelihood values sampled. 2. New latent variable model bound that might work better than classic approaches.
This paper proposes a method to build a CNN in the Winograd domain, where weight pruning and ReLU can be applied in this domain to improve sparsity and reduce the number of multiplication. - the ReLU activation function associated with the previous layer is applied to the Winograd transform of the input activations, not directly to the spatial-domain activations, also yielding sparse activations The paper is well-written. It provides a new way to combine the Winograd transformation and the threshold-based weight pruning strategy. This modification combines the reduction of multiplications achieved by the Winograd convolution algorithm with weight pruning in the following way: - weights are pruned after the Winograd transformation, to prevent the transformation from filling in zeros, thus preserving weight sparsity Specifically, ReLU nonlinearity was moved after Winograd transformation to increase the dynamic sparsity in the Winograd domain, while an additional pruning on low magnitude weights and re-training procedure based on pruning is used to increase static sparsity of weights, which decreases computational demand. Putting ReLU in the Winograd domain (or any transformed domain, e.g., Fourier) seems to be an interesting idea, and deserves some further exploration. Because this yields a network, which is not mathematically equivalent to a vanilla or Winograd CNN, the method goes through three stages: dense training, pruning and retraining. 525-542. Springer International Publishing, 2016. This paper proposes to combine Winograd transformation with sparsity to reduce the computation for deep convolutional neural network.
However, while results on convex hull task are good, k-means ones use a single, artificial problem (and do not test DCN, but rather a part of it), and on TSP DCN performs significantly worse than baselines in-distribution, and is better when tested on bigger problems than it is trained on. depending on the application) leads to empirical improvement on three tasks - convex hull finding, k-means clustering and on TSP. In particular the authors test their approach on computing convex hulls, computing a minimum cost k-means clustering, and the Euclidean Traveling Salesman Problem (TSP) problem.
Given that in many applications such parent-class supervised information is not available, the authors of this paper propose domain specific pseudo parent-class labels (for example transformed images of digits) to adapt ACOL for unsupervised learning. The novelty seems to be in the adaptation to GAR from the semi-supervised to the unsupervised setting with labels indicating if data have been transformed or not. The main idea is to exploit a schema of semisupervised learning based on ACOL and GAR for an unsupervised learning task. This paper utilizes ACOL algorithm for unsupervised learning.
Empirically the authors demonstrate that DIP-VAE can effectively learn disentangled features, perform comparably better than beta-VAE and at the same time retain the reconstruction quality close to regular VAE (beta-VAE with beta = 1). - the covariance minimisation proposed in the paper looks like an easy to implement yet impactful change to the VAE objective to encourage disentanglement while preserving reconstruction quality Unlike the original beta-VAE objective which implicitly minimises such covariance individually for each observation x, the DIP-VAE objective does so while marginalising over x. ############################################### This paper presents a Disentangled Inferred Prior (DIP-VAE) method for learning disentangled features from unlabeled observations following the VAE framework.
In Section 4 they provide quite varied empirical analysis: they confirm their theoretical results on four architectures; they show its use it to regularise on language models; they apply it on large minibatch settings where high variance is a main problem; and on evolution strategies. The paper also proves that this approach reduces the variance of the gradient estimates (and in practice, flipout should obtain the ideal variance reduction). Additionally, it is demonstrated that flipout allows evolution strategies utilizing GPUs. Overall this is a very nice paper. The paper introduces a simple idea, flipout, to perturb the weights quasi-independently within a minibatch: a base perturbation (shared by all sample in a minibatch) is multiplied by a random rank-one sign matrix (different for every sample).
Pros: The authors lead a very nice exploration into the binary nets in the paper, from the most basic analysis on the converging angle between original and binarized weight vectors, to how this convergence could affect the weight-activation dot product, to pointing out that binarization affects differently on the first layer. There is also a strong correlation between (binarized weights)*activations and (binarized weights)*(binarized activations). (2) Except the first layer, the dot product of weights*activations in each layer is highly correlated with the dot product of (binarized weights)*activations in each layer. c. For BNNs, where both the weights and activations are binarized, shouldn't we compare weights*activations to (binarized weights)*(binarized activations)? This is claimed to entail that the continuous weights of the binarized neural net approximate the continuous weights of a non-binarized neural net trained in the same manner. This paper presents three observations to understand binary network in Courbariaux, Hubara et al. (2016). Specifically, they observe that: (1) The angle between continuous vectors sampled from a spherical symmetric distribution and their binarized version is relatively small in high dimensions (proven to be about 37 degrees when the dimension goes to infinity), and this demonstrated empirically to be true for the binarized weight matrices of a convenet. Some typos and minor issues are listed in the "Cons" part below.
As out of distribution samples are hard to obtain, authors also propose to use GAN generating "boundary" samples as out of distribution samples. Classifier is trained to not only maximize classification accuracy on the real training data but also to output a uniform distribution for the generated samples. Moreover, [1] did not seem to generate any out of distribution samples. The manuscript proposes a generative approach to detect which samples are within vs. - How does this compare with a method whereby instead of pushing the fake sample's softmax distribution to be uniform, the model is simply a trained to classify them as an additional "out of distribution" class? Suppose that theta is set appropriately so that p_theta (y|x) gives a uniform distribution over labels for out of distribution samples. Algorithm 1. is called "minimization for detection and generating out of distribution (samples)", but this is only gradient descent, right? Looking at the cost function and the intuition, the difference in figure 1 seems to be primarily due to the relative number of samples used during optimization -- and not to anything inherent about the distribution as is claimed. out of the sample space of the training distribution.
With a fixed reference/target Wikipedia article, if different models generate variable sizes of output, ROUGE evaluation could easily pose a bias on a longer output as it essentially counts overlaps between the system output and the reference. Unfortunately it is hard to judge the effectiveness of the abstractive model due to the scale of the experiments, especially with regards to the quality of the generated output in comparison to the output of the extractive stage. The authors at first reduce the input size by using various extractive strategies and then use the selected content as input to the abstractive stage where they leverage the Transformer architecture with interesting modifications like dropping the encoder and proposing alternate self-attention mechanisms like local and memory compressed attention. This paper considers the task of generating Wikipedia articles as a combination of extractive and abstractive multi-document summarization task where input is the content of reference articles listed in a Wikipedia page along with the content collected from Web search and output is the generated content for a target Wikipedia page. Extractiveness analysis: I would also have liked to see more analysis of how extractive the Wikipedia articles actually are, as well as how extractive the system outputs are. I would have liked to see the ROUGE performance of some current unsupervised multi-document extractive summarization methods, as well as some simple multi-document selection algorithms such as SumBasic. The main significance of this paper is to propose the task of generating the lead section of Wikipedia articles by viewing it as a multi-document summarization problem. Then abstractive summarization is performed using a modification of Transformer networks (Vasvani et al 2017).
- Experiments (in main paper) only show speedups and do not show loss of accuracy due to sparsity The paper proposes improving performance of large RNNs by combing techniques of model pruning and persistent kernels. The paper devises a sparse kernel for RNNs which is urgently needed because current GPU deep learning libraries (e.g., CuDNN) cannot exploit sparsity when it is presented and because a number of works have proposed to sparsify/prune RNNs so as to be able to run on devices with limited compute power (e.g., smartphones).
The paper extends softmax consistency by adding in a relative entropy term to the entropy regularization and applying trust region policy optimization instead of gradient descent. However, in many cases, softmax does not perform a good exploration, even if the entropy regularization is added. - Trust-PCL (off-policy) significantly outperform TRPO in terms of data efficiency and final performance.
The most interesting part is a joint training for both compression and image classification. However, applying a deep representation for the compression and then directly solving a vision task (classification and segmentation) can be considered as a novel idea. Neural-net based image compression is a field which is about to get hot, and this paper asks the obvious question: can we design a neural-net based image compression algorithm such that the features it produces are useful for classification & segmentation? PROS P.1 Joint training for both compression and classification. However, to me, it is not clear if the trained compression model on this specific dataset and for the task of classification can work well for other datasets or other tasks.
Four adversarial attacks strategies are considered to attack a Resnet50 model for classification of Imagenet images. On the contrary, white-box means that the adversary knows everything about the classification method, including the transformation implemented to make it more robust to attacks. Another example of attack that account for transformation knowledge (and would hopefully be more robust than the attacks considered in the manuscript) could be one that alternates between a conventional attack and the transformation. The purpose of the transformation is to erase the high-frequency signals potentially embedded by an adversarial attack. * p1: 'too simple to remove adversarial perturbations from input images sufficiently The paper investigates using input transformation techniques as a defence against adversarial examples. * In a white-box scenario, the adversary knows about the transformation and the classification model. Experiments are conducted in a black box setting (when the model to attack is unknown by the adversary) or white box setting (the model and defense strategy are known by the adversary).
The paper proposes the Skip RNN model which allows a recurrent network to selectively skip updating its hidden state for some inputs, leading to reduced computation at test-time. The experiments in the paper demonstrated skip RNNs outperformed regular LSTMs and GRUs o thee addition, pixel MNIST and video action recognition tasks.
2. The l2 ratio. The l2 ratio is small for higher residual layers, I'm not sure how much this phenomenon can prove that resnet is actually doing iterative inference. This paper shows that residual networks can be viewed as doing a sort of iterative inference, where each layer is trained to use its "nonlinear part" to push its values in the negative direction of the loss gradient.
Contribution: - This paper proposes a new object counting module which operates on a graph of object proposals. Summary: - This paper proposes a hand-designed network architecture on a graph of object proposals to perform soft non-maximum suppression to get object count. Weaknesses - Although the proposed model is helpful to model counting information in VQA, it fails to show improvement with respect to a couple of important baselines: prediction from image representation only and from the combination of image representation and attention weights. This paper tackles the object counting problem in visual question answering. 2. Useful for object counting problem. Summary - This paper mainly focuses on a counting problem in visual question answering (VQA) using attention mechanism.
The model proposed is a variant of the cycle GAN in which in addition embeddings helping the Generator are learned for all the values of the discrete variables. In fact the regularization of the Jacobian that will be preventing the discriminator to vary too quickly in space is more likely to explain the fact that the discrimination is not too easy to do between the true and mapped embeddings.
The paper proposes a way to speed up the inference time of RNN via Skim mechanism where only a small part of hidden variable is updated once the model has decided a corresponding word token seems irrelevant w.r.t. a given task. * One obvious way to reduce the computational complexity of RNN is to reduce the size of the hidden state. The heavy-weight and the light-weight RNN each controls a portion of the hidden state.
Linear Surrogate RNNs is an important concept that is useful to understand RNN variants today, and potentially other future novel architectures. The paper provides argument and experimental evidence against the rotation used typically in RNNs. While this is an interesting insight, and worthy of further discussion, such a claim needs backing up with more large-scale experiments on real datasets. This paper abstracts two recently-proposed RNN variants into a family of RNNs called the Linear Surrogate RNNs which satisfy  Blelloch's criteria for parallelizable sequential computation.
This applications paper proposes using a deep neural architecture to do unsupervised anomaly detection by learning the parameters of a GMM end-to-end with reconstruction in a low-dimensional latent space. Significance – Constraining the dimension reduction to fit a certain model is a relevant topic, but I'm not convinced of how well the Gaussian model fits the low-dimensional representation and how well can a neural network compute the GMM mixture memberships. The idea to constraint the dimension reduction to fit a certain model, here a GMM, is relevant, and the paper provides a thorough comparison with recent state-of-the-art methods.
This is complemented by the SCAN model, which is a beta-VAE trained to reconstruct symbols (y; k-hot encoded concepts like {red, suitcase}) with a slightly modified objective. The two input concepts {red} and {suitcase} are each encoded by a pre-trained SCAN encoder and then those two distributions are combined into one by a simple 1d convolution module trained to implement the AND operator (or IGNORE/IN COMMON). SCAN optimizes the ELBO plus a KL term which pushes the latent distribution of the y VAE toward the latent distribution of the x (image) VAE. and baselines. 5) Concepts expressed as logical combinations of other concepts generalize well for both the SCAN representation and the baseline representations.
In this paper, the authors studied the problem of semi-supervised few-shot classification, by extending the prototypical networks into the setting of semi-supervised learning with examples from distractor classes. These strategies are evaluated in a particular semi-supervised transfer learning setting:  the models are first trained on some source categories with few labeled data and large unlabeled samples (this setting is derived by subselecting multiple times a large dataset), then they are used on a final target task with again few labeled data and large unlabeled samples but beloning to a different set of categories. This work is highly incremental since it is an extension of existing prototypical networks by adding the way of leveraging the unlabeled data.
This paper defines building blocks for complex-valued convolutional neural networks: complex convolutions, complex batch normalisation, several variants of the ReLU nonlinearity for complex inputs, and an initialisation strategy. Authors present complex valued analogues of real-valued convolution, ReLU and batch normalization functions. Since any complex valued computation can be done with a real-valued arithmetic, switching to complex arithmetic needs a compelling use-case. - Although care is taken to ensure that the complex and real-valued networks that are compared in the experiments have roughly the same number of parameters, doesn't the complex version always require more computation on account of there being more filters in each layer? Their contributions are: 1. Formulate complex valued convolution However their application don't seem to connect to any of those uses and simply reimplement existing real-valued networks as complex valued. 3. Formulate complex batch normalization as a "whitening" operation on complex domain
The paper proposes "wavelet pooling" as an alternative for traditional subsampling methods, e.g. max/average/global pooling, etc., within convolutional neural networks. As a result, if my understanding is correct, this explains why the wavelet pooling is almost the same as average pooling in the experiments (other than MNIST). I encourage the authors to address my concerns in their rebuttal The paper proposes to use discrete wavelet transforms combined with downsampling to achieve arguably better pooling output compared to average pooling or max pooling.
1. While the paper presents the algorithm as an optimization algorithm, although it gets better learning performance, it would be interesting to see how well it is as an optimizer. E.g. there is no results of Algorithm 1. I understand that you are trying to improve the existing results with their optimizer, but this paper also introduces new algorithm. As we know, Adam is currently very well-known algorithm to train DNN. If Neumann optimizer can surpass Adam on ImageNet, I think your algorithm will be widely used after being published.
This and / or the defense of training to adversarial examples is an important experiment to assessing the limitations of the attack. 3- Authors mention that OPTSTRONG attack does not succeed in finding adversarial examples ("it succeeds on 28% of the samples on MNIST;73% on CIFAR-10"). Summary of paper: The authors present a novel attack for generating adversarial examples, deemed OptMargin, in which the authors attack an ensemble of classifiers created by classifying at random L2 small perturbations. I am not familiar with other adversarial attack strategies aside from the ones mentioned in this paper, and therefore I cannot properly assess how innovative the method is. As a summary, the authors presented a method that successfully attacks other existing defense methods, and present a method that can successfully defend this attack. This is very problematic if the authors want to make claims about their attack being effective under defenses that know OptMargin is a possible attack.
They claim that under the condition that global optima are achieved for discriminator and generator in each iteration, the Coulomb GAN converges to the global solution. The assumption in this paper is too restricted, and the discussion is unfair to the existing variants of GAN, e.g., GMMN or Wasserstein GAN, which under some assumptions, there is also only one global Nash Equilibrium. The authors draw from electrical field dynamics and propose to formulate the GAN learning problem in a way such that generated samples are attracted to training set samples, but repel each other. On the other hand, the richness of the discriminator also important in the training of GAN. The model collapsing is not just because loss function in training GAN.
The state trace embedding combines embeddings for variable traces using a second recurrent encoder. Summary of paper: The paper proposes an RNN-based neural network architecture for embedding programs, focusing on the semantics of the program rather than the syntax. - Figure 5 seems to suggest that dependencies are only enforced at points in a program where assignment is performed for a variable, is this correct? Three NN architectures are explored, which leverage program semantics rather than pure syntax. This is achieved by creating training data based on program traces obtained by instrumenting the program by adding print statements. The neural network is trained using this program traces with an objective for classifying the student error pattern (e.g. list indexing, branching conditions, looping bounds). The authors present 3 architectures for learning representations of programs from execution traces.
They find complex cells that lead to state-of-the-art performance on benchmark dataset CIFAR-10 and ImageNet. They also claim that their method is reaching a new milestone in evolutionary search strategies performance. An important contribution is to show that a well-defined architecture representation could lead to efficient cells with a simple randomized search. To speed up the search, they focus on finding cells instead of an entire network. The fundamental contribution of this article, when put into the context of the many recent publications on the topic of automatic neural architecture search, is the introduction of a hierarchy of architectures as a way to build the search space. As a result, this would allow the evolutionary search algorithm to design modules which might be then reused in different edges of the DAG corresponding to the final architecture, which is located at the top level in the hierarchy. Hence, each level is limited to a small search space while the system as a whole converges toward a complex structure. Exploiting compositionality in model design is not novel per se (e.g. [1,2]), but it is to the best of my knowledge the first explicit application of this idea in neural architecture search.
They propose a method called Workflow Guided Exploration (WGE) which is learnt from demonstrations but is environment agnostic. Episodes are generated by first turning demonstrations to a workflow lattice. This paper introduces a new exploration policy for Reinforcement Learning for agents on the web called "Workflow Guided Exploration". Their proposed algorithm DAgger fixes this (the mistakes by the policy are linear in the horizon length) by using an iterative procedure where the learnt policy from the previous iteration is executed and expert demonstrations on the visited states are recorded, the new data thus generated is added to the previous data and a new policy retrained. Use the sequences of actions to sample trajectories and use the trajectories to learn the RL policy. - The problem with putting in the replay buffer only episodes which yield high reward is that extrapolation will inevitably lead the learnt policy towards parts of the state space where there is actually low reward but since no support is present the policy makes such mistakes. In addition, they make general comparison to RL literature such as hierarchy rather than more concrete comparisons with the problem at hand (learning from demonstrations.)
minor comments: Figure 2, legend needs to be outside the Figure, in the current Figure a lot is covered by the legend This paper considers the problem of private learning and uses the PATE framework to achieve differential privacy. That is, the privacy guarantee would depend only on public information, rather than the private data. The novelty of this work is a refined aggregation process, which is improved in three ways: a) Gaussian instead of Laplace noise is used to achieve differential privacy. Intuitively, if teacher ensemble does not answer, it seems that it would reveal the fact that teachers do not agree, and thus spend some privacy cost. Summary: In this work, PATE, an approach for learning with privacy,  is modified to scale its application to real-world data sets. b) Queries to the aggregator are "filtered" so that the limited privacy budget is only expended on queries where the teachers are confident and the student is uncertain or wrong. 2. It would be great to have an intuitive explanation about differential privacy and selective aggregation mechanisms with examples. 3. It would be great if there is an explanation about the privacy cost for selective aggregation. I am not familiar with privacy learning but it is interesting to see that more concentrated distribution (Gaussian) and clever aggregators provide better utility-privacy tradeoff. This is done by leveraging the synergy between privacy and utility, to make better use of the privacy budget spent when transferring knowledge from teachers to the student. on the positive side: Having scalable models is important, especially models that can be applied to data with privacy concerns. on the negative side: In the introduction, the authors introduce the problem by the importance of privacy issues in medical and health care data. However, since the privacy guarantee now depends on the data, it is itself sensitive information. That is, if there is one label backed by a clear majority of teachers, then the privacy loss (as measured by Renyi divergence) is low.
The authors use the coreset construction with a CNN to demonstrate an active learning algorithm for multi-class classification. This paper proposes a batch mode active learning algorithm for CNN as a core-set problem.
====================== The paper introduces a system to estimate a floor-level via their mobile device's sensor data using an LSTM to determine when a smartphone enters or exits a building, then using the change in barometric pressure from the entrance of the building to indoor location. An LSTM RNN classifier analyzes the changes/fading in GPS signals to determine whether a user has entered a building.
While I enjoyed reading the gentle introduction, nice overview of past work, and the theoretical analysis that relates the rank of tensor train networks to that of CP netowkrs, I wasn't sure how to translate the finding into the corresponding neural network models, namely, recurrent neural networks and shallow MLPs. Finally the authors show that almost all tensor train networks (exluding a set of measure zero) require exponentially large width to represent in CP networks, which is analogous to shallow networks. In addition, I would like to see the performance of RNNs and MLPs with the same number of units/rank in order to validate the analogy between these networks. Here, the expressive power refers to the rank of tensor decomposition, i.e., the number of latent components. Then they focus on one particular decompostion known as the tensor train decomposition and points out an analogy between tensor train networks and recurrent neural networks. In this paper, the expressive power of neural networks characterized by tensor train (TT) decomposition, a chain-type tensor decomposition, is investigated.
While the E-value MDP is training, the proposed method uses a 1/log transformation applied to E-values to get the corresponding exploration bonus term for the original MDP. "non-Markovity" -> "non-Markovianness"? This paper presents an exploration method for model-free RL that generalizes the counter-based exploration bonus methods and takes into account long term exploratory value of actions rather than a single step look-ahead. Several of the cited papers use counters to determine which states are "known" and then solve an MDP to direct exploration past immediate outcomes. The idea is to learn a second (kind of) Q function, which could be called E-function, which captures the value of exploration (E-value). The paper proposes a novel way for trading of exploration and exploitation in model-free reinforcement learning.
The work is fairly novel in its approach, combining a learned reward estimator with a contextual bandit algorithm for exploration/exploitation. For instance, one could imagine that the policy disadvantage should be the difference between the residual costs of the bandit algorithm and the reference policy, rather than just the residual cost of the bandit algorithm. -- Can the authors speculate about the difference in performance between the RL and bandit structured prediction settings? -- It would be nice to see an end-to-end result that instantiates Theorem 1 (and/or Theorem 2) with a contextual bandit algorithm to see a fully instantiated guarantee. My personal conjecture is that the bandit structured prediction settings are more easily decomposable additively, which leads to a greater advantage of the proposed approach, but I would like to hear the authors' thoughts.
It would be interesting to know an analogy, for instance, saying that this adaptive compression in memory would be equivalent to quantizing all weights with n bits. In the end, while do like the general idea of utilizing the gradient to identify how sensitive the model might be to quantization of various parameters, there are significant clarity issues in the paper, I am a bit uneasy about some of the compression results claimed without clearer description of the bookkeeping, and I don't believe an approach of this kind has any significant practical relevance for saving runtime memory or compute resources. Quantization has some bookkeeping associated with it: In a per-parameter quantization setup it will be necessary to store not just the quantized parameter, but also the number of bits used in the quantization (takes e.g. 4-5 extra bits), and there will be some metadata necessary to encode how the quantized value should be converted back to floating point (e.g., for 8-bit quantization of a layer of weights, usually the min and max are stored). The idea presented is an interesting extension to weight pruning with a close form approximate solution for computing the adaptive quantization of the weights. From the tables and figures, it is difficult to grasp the decrease in accuracy when using the quantized model, compared to the full precision model, and also the relative memory compression. I am confused by the equality in Equation 8: What happens for values shared by many different quantization bit depths (e.g., representing 0 presumably requires 1 bit, but may be associated with a much finer tolerance)?
3. The convergence rate depends on \\gamma(\\phi_0), from the initialization, \\phi_0 is probably very close to \\pi/2 (the closeness depend on dimension), which is  also likely to make \\gamma(\\phi_0) depend on dimension (this is especially true of Gaussian). A major strength of the result is that it can work for general continuous distributions and does not really rely on the input distribution being Gaussian; the main weakness is that some of the distribution dependent quantities are not very intuitive, and the alignment requirement might be very high.
[Reviewed on January 12th] This article applies the notion of "conceptors" -- a form of regulariser introduced by the same author a few years ago, exhibiting appealing boolean logic pseudo-operations -- to prevent forgetting in continual learning,more precisely in the training of neural networks on sequential tasks. If the presented approach was an improvement over the original conceptors, the evaluation should compare the new and the original version. The paper leaves me guessing which part is a new contribution, and which one is already possible with conceptors as described in the Jaeger 2014 report. Conceptors can be trained with a number of approaches (as described both in the 2014 Jaeger tech report and in the JMLR paper), including ridge regression.
Pros: - The paper provides a nice overview of WGAN-GP, Fisher GAN and Sobolev GAN.
[Strenghts] This paper introduced a topic-based question generation model, which generate question conditioned on the topic and question type. The proposed model can generate question with respect to different topic and pre-decode seems a useful trick. When the ground truth topic is provided, it's not fair to compare with the previous method, since knowing the similar word present in the answer will have great benefits to question generation. Then, the author proposed a topic-specific question generation model by encoding the extracted topic using LSTM and a pre-decode technique that the second decoding is conditioned on the hidden representation of the first decoding result. The authors proposed heuristic method to extract the topic and question type without further annotation. Add Comment The authors propose a scheme to generate questions based on some answer sentences, topics and question types. Experimental results on question generation are convincing and clearly indicate that the approach is effective to generate relevant and well-structured short questions. [Weaknesses] This paper proposed an interesting and intuitive question generation model. The authors performed the experiment on AQAD dataset, and show their performance achieve state-of-the-art result when using automatically generated topic, and perform better when using the ground truth topic. 4: The automatically extracted topic can be very noisy, but the paper didn't mention any of the extracted topics on AQAD dataset.
Contribution: As discussed in the literature review section, apart from previous results that studied the theoretical convergence properties for problems that involves a single hidden unit NN, this paper extends the convergence results to problems that involves NN with two hidden units. While the ReLU activation is very common in NN architecture, without more motivations I am not sure what are the impacts of these results. According to the authors, this paper extends(?) previous results on a NN with a single layer with a single unit. These are crucial for understanding the contribution of the paper; while reading the paper, I assumed that the authors consider the case of a single hidden unit with K = 2 RELU activations (however, that complicated my understanding on how it compares with state of the art). Leveraged by two recent results in global optimization, they showed that with a simple two-layer ReLU network with two hidden units, the problem with a standard MSE population loss function does not have spurious local minimum points. Summary: The paper considers the problem of a single hidden layer neural network, with 2 RELU units (this is what I got from the paper The title is not clear whether the paper considers a two layer RELU network or a single layer with with two RELU units.
ii) Theorem 10 and Theorem 6 essentially has the same bound on the right hand side but Theorem 10 additionally divides local volume by global which decreases by exp(-2Nlog N). Secondly, as the authors aptly pointed out in the discussion section, this results doesn't mean neural networks will converge to good local minima because these bad local minimas can have a large basins of attraction. This paper studies the question: Why does SGD on deep network is often successful, despite the fact that the objective induces bad local minima? It is not well motivated why one should study the angular volume of the global and local minima.
The paper studies locally open maps, which preserve the local minima geometry. It uses the notion of a locally open map to draw equivalences between local optima and global optima. Hence a local minima of l(F(W)) is a local minima of l(s) when s=F(W) is a locally open map. For a pyramidal feed forward net, if the weights in each layer have full rank,  input X is full rank, and the link function is invertible, then that local minima is a global minima.
The tree encoder, however, is based on the standard Tree-LSTM and the application in this case is synthetic as the datasets are generated using a manual rule-based translation. The model uses soft attention mechanism to locate relevant sub-trees in the source program tree when decoding to generate the desired target program tree. Since any CoffeeScript programs can be compiled into the corresponding Javascript programs, we should assume that CoffeeScript is the only subset of Javascript (without physical difference of syntax), and this translation task may never capture the whole tendency of Javascript.
The search result is a new activation function named Swish function. This paper is utilizing reinforcement learning to search new activation function. The properties of Swish like allowing information flow on the negative side and linear nature on the positive have been proven to be important for better optimization in the past by other functions like LReLU, PLReLU etc. al. (2017). In terms of experimental validation, in most cases the increase is performance when using Swish as compared to other models are very small fractions.
[p7 Fig 6 and text] Here the authors are comparing how well agents select the optimal actions as compared to how close they are to the end of the game. I am also not aware of trying to use games with known potential functions/optimal moves as a way to study the performance of RL algorithms. # Detailed notes [p4, end of sec 3] The authors say that the difficulty of the games can be varied with "continuous changes in potential", but the potential is derived from the discrete initial game state, so these values are not continuously varying (even though it is possible to adjust them by non-integer amounts). This relates to the "surprising" fact that "Reinforcement learning is better at playing the game, but does worse at predicting optimal moves.". Impact: I think this is an interesting and creative contribution to studying RL, particularly the use of an easy-to-analyze game in an RL setting. In fact, there is a level of non-determinism in how the attacker policies are encoded which means that an optimal policy cannot be (even up to soft-max) expressed by the agent (as I read things the number of pieces chosen in level l is always chosen uniformly randomly). - Everything leads me to believe that, up to 6.2, the game is only dealt with as a fixed MDP to be overfit by the model through RL: - there is no generalization from K=k (train) to K > k (test). Reinforcement learning >> is better at playing the game, but does worse at predicting optimal moves. This paper presents a study of reinforcement learning methods applied to Erdos-Selfridge-Spencer games, a particular type of two-agent, zero-sum game. If there are more in the range 3-7 moves from the end of the game, than there are outside this range, then the supervised learner will An empirical study is performed, measuring the performance of both agents, tuning the difficulty of the game for each agent by changing the starting position of the game. The authors describe the game and some of its properties, notably that there exists a tractable potential function that indicates optimal play for each player for every state of the board. Secondly it compared RL and supervised learning (as they know the optimal actiona at all times).
The authors' Main Claim appears to be: "While common wisdom might suggest that prior knowledge about game semantics such as ladders are to be climbed, jumping on spikes is dangerous or the agent must fetch the key before reaching the door are crucial to human performance, we find that instead more general and high-level priors such as the world is composed of objects, object like entities are used as subgoals for exploration, and things that look the same, act the same are more critical." The authors have to include RL agent in all their experiments to be able to dissociate what is due to human priors and what is due to the noise introduced in the game. In the case of human players the time needed to complete the game is plotted, and in the case of a RL agent the number of steps needed to complete the game is plotted (fig 1). It's also important to take into account how much work general priors about video game playing (games have goals, up jumps, there is basic physics) are doing here (the authors do this when they discuss versions of the game with different physics). The authors interpret the fact that performance falls so much between conditions b and c to mean that human priors about "objects are special" are very important. Thus, we could consider a weaker version of the claim: semantic priors are important but even in the absence of explicit semantic cues (note, this is different from having the wrong semantic cues as above) people can do a good job on the game. The authors present a study of priors employed by humans in playing video games -- with a view to providing some direction for RL agents to be more human-like in their behaviour. This paper investigates human priors for playing video games. Considering a simple video game, where an agent receives a reward when she completes a game board, this paper starts by stating that: - Firstly, the humans perform better than an RL agent to complete the game board. Here without semantic priors I would hypothesize that human performance would fall quite far (whereas with semantics people would be able to figure it out quite well). RL measures. 2. And in a related note to the idea of establishing a comparison, it would be further instructive if the RL agents were also run on the different game manipulations to see what (if any) sense could be made out of their performance. This would give at least some proxy to study the importance of priors about "how video games are generally constructed" rather than priors like "objects are special". So it cannot be concluded that the change of performances is due to human priors.
The algorithm for learning the context of sequencing reads compared to true mutations is based on a multi layered CNN, with 2/3bp long filters to capture di and trinucleotide frequencies, and a fully connected layer to a softmax function at the top. If the entire point is to classify mutations versus errors it would make sense to combine their read based calls from multiple reads per mutations (if more than a single read for that mutation is available) - but the authors do not discuss/try that. The data is based on mutations in 4 patients with lung cancer for which they have a sample both directly from the tumor and from a healthy region. Summary: In this paper the authors offer a new algorithm to detect cancer mutations from sequencing cell free DNA (cfDNA). his paper proposes a deep learning framework to predict somatic mutations at extremely low frequencies which occurs in detecting tumor from cell-free DNA. The idea is that in the sample being sequenced there would also be circulating tumor DNA (ctDNA) so such mutations could be captured in the sequencing reads. Using matched samples of tumor and normal from the patients is also a nice idea to mimic cfDNA data. The authors suggest to overcome this problem by training an algorithm that will identify the sequence context that characterize sequencing errors from true mutations. They also should compare to Strelka whic h interestingly they included only to make final calls of mutations but not in the comparison. In this paper the author propose a CNN based solution for somatic mutation calling at ultra low allele frequencies. Moreover, the authors filter the "normal" samples using those (p.7 top), which makes the entire exercise a possible circular argument. This makes the task of differentiating between sequencing errors and true variants due to ctDNA hard. Can they show a context they learned and make sense of it? To this, they add channels based on low base quality, low mapping quality.
Experiments on artificial bars data and natural image patch datasets compare several variants of the proposed method, while varying a few EA method substeps such as selecting parents by fitness or randomly, including crossover or not, or using generic or specialized mutation rates. ## Significance Combining evolutionary algorithms (EA) within EM has been done previously, as in Martinez and Vitria (Pattern Recog. However, these efforts seem to use EA in an "outer loop" to refine different runs of EM, while the present approach uses EA in a substep of a single run of EM.
This manuscript explores the idea of adding noise to the adversary's play in GAN dynamics over an RKHS. Although the author claim the novelty as adding noise to the discriminator, it seems to me that at least for the RBF case it just does the following: 1. write down MMD as an integral probability metric (IPM) ==== original review === The paper describes a generative model that replaces the GAN loss in the adversarial auto-encoder with MMD loss.
The paper describes an empirical evaluation of some of the most common metrics to evaluate GANs (inception score, mode score, kernel MMD, Wasserstein distance and LOO accuracy). In the paper, the authors discuss several GAN evaluation metrics. Specifically, the authors pointed out some desirable properties that GANS evaluation metrics should satisfy. The paper evaluates popular GAN evaluation metrics to better understand their properties. I think this paper tackles an interesting and important problem, what metrics are preferred for evaluating GANs. In particular, the authors showed that Inception Score, which is one of the most popular metric, is actually not preferred for several reasons. Overall, I think this paper is worthy for acceptance as several GAN methods are proposed and good evaluation metrics are needed for further improvements of the research field. - The authors implicitly contradict the argument of Theis et al against monolithic evaluation metrics for generative models, but this is not strongly supported. Appendix G in https://arxiv.org/pdf/1706.04987.pdf) Do you think it would be useful to compare other generative models (e.g. VAEs) using these evaluation metrics? Pros -Several interesting ideas for evaluating evaluation metrics are proposed In addition to existing metrics, it would be useful to add Frechet Inception Distance (FID) and Multi-scale structural similarity (MS-SSIM). Section 4 summarizes the results, which concluded that the Kernel MMD and 1-NN classifier in the feature space are so far recommended metrics to be used. It would be interesting to see some sensitivity analysis as well as understand the correlations between different metrics for different hyperparameters (cf. Some of the metrics don't capture perceptual similarity, but I'm curious to hear what you think. For those properties raised, the authors experimentally evaluated whether existing metrics satisfy those properties or not.
Although community detection is used before graph kernels, such subgraph extraction process is already implicitly employed in various graph kernels. Moreover, the method makes an strong assumption that the graph is strongly characterized by one of its patches, ie its subgraph communities, which might not be the case in arbitrary graph structures, thus limiting their method. The article is well-written and easily comprehensible, but suffers from several weak points: * Features are not learned directly from the graphs, but the approach merely weights graph kernel features. The authors propose a method for graph classification by combining graph kernels and CNNs. In a first step patches are extracted via community detection algorithms. * The originality is not high as the application of neural networks for graph classification has already been studied elsewhere and the proposed method is a direct combination of three existing methods, community detection, graph kernels, and CNNs.
The proposed architecture could identify the 'key' states through assigning higher weights for important states, and applied reservoir sampling to control write and read on memory. The authors claim (with some degree of mathematical backing) that sampling a memory of n states where the distribution over the subsets of past states of size n is proportional to the product of the weights is desired. Typos didn't influence reading. It is a novel setup to consider reservoir sampling for episodic memory. This paper proposes one RL architecture using external memory for previous states, with the purpose of solving the non-markov tasks.
As a result, the activations outputted by DReLU can have a mean closer to 0 and a variance closer to 1 than the standard ReLU. This paper proposes an activation function, called displaced ReLU (DReLU), to improve the performance of CNNs that use batch normalization. Compared to ReLU, DReLU cut the identity function at a negative value rather than the zero. DReLU shifts ReLU from (0, 0) to (-\\sigma, -\\sigma). Although DReLU's expectation is smaller than expectation of ReLU, but it doesn't explain why DReLU is better than very leaky ReLU, ELU etc. This paper describes DReLU, a shift version of ReLU. 1) As DReLU(x) = max{-\\delta, x}, what is the optimal strategy to determine \\delta? The author runs a few CIFAR-10/100 experiments with DReLU.
The defended idea is to use the context to fit a mixture of Gaussian with a NN and to assume that the noise could be additively split into two terms. * In the paragraph above (2) I unsure of what is a "binomial noise error distribution" for epsilon, but a few lines later epsilon becomes a gaussian why not just mention that you assume the presence of a gaussian noise on the parameters of a Bernouilli distribution ? The only positioning argument that is given in that section is the final sentence "In this paper we model measurement noise using a Gaussian model and combine it with a MDN". One depend only on the number of observations of the given context and the average reward in this situation and the second term begin the noise. In the paper "DEEP DENSITY NETWORKS AND UNCERTAINTY IN RECOMMENDER SYSTEMS", the authors propose a novel neural architecture for online recommendation. The confidence bound considered should include the uncertainty on the parameters in the predictive posterior reward distribution (as done for instance in Blundell et al. (2015) in the context of neural networks), not only the distribution of the observed data with regards to the considered probabilistic families. This paper presents a methodology to allow us to be able to measure uncertainty of the deep neural network predictions, and then apply explore-exploit algorithms such as UCB to obtain better performance in online content recommendation systems. My main concern with the paper is that the contribution is unclear, as the authors failed from my point of view in establishing the novely w.r.t. the state of the art regarding uncertainty in neural networks. 2. In Section 4.2.1, CTR is modeled as a Gaussian mixture, which doesn't look quite right, as CTR is between (0,1). The state of the art section is very confusing, with works given in a random order, without any clear explanation about the limits of the existing works in the context of the task addressed in the paper.
The paper shows that there are functions that can be represented by depth 3 sigmoidal neural networks (with polynomial weights and polynomially many units), but sigmoidal networks of depth 2 with polynomially bounded weights require exponentially many units. The core of the analysis is a proof that any 2-layer neural networks can be well approximated by a polynomial function with reasonably low degrees. Specifically, the paper shows that there are functions on R^d that can be approximated well by a depth-3 sigmoidal network with poly(d) weights, that cannot be approximated by a depth-2 sigmoidal network with poly(d) weights, and with respect to any input distributions with sufficiently large density in some part of the domain.
There are a few ideas the paper discusses: (1) compared to pruning weight matrices and making them sparse, block diagonal matrices are more efficient since they utilize level 3 BLAS rather than sparse operations which have significant overhead and are not "worth it" until the matrix is extremely sparse. For example, with sparse and block diagonal matrices, reducing the size of the matrix to fit into the cache on the GPU must obviously make a difference, but this did not seem to be investigated. The paper proposes to make the inner layers in a neural network be block diagonal, mainly as an alternative to pruning.
The approach seems to be very susceptible to the weight of the subtrace loss λ, at least when training NTMs. In my understanding each of the trace supervision information (hints, e.g. the ones listed in Appendix F) provides a sensible inductive bias we would the NTM to incorporate. The observation that adding additional forms of supervision through execution traces improves generalization may be unsurprising, but from what I understand the main contribution of this paper lies in the abstraction of existing neural abstract machines to ∂NCM.
The analyses are motivated by results from cognitive and developmental psychology, exploring questions such as whether agents develop biases for shape/color, the difficulty of learning negation, the impact of curriculum format, and how representations at different levels of abstraction are acquired. The crucial question, here, would be whether, when an agent is trained in a naturalistic environment (i.e., where distributions of colors, shapes and other properties reflect those encountered by biological agents), it would show a human-like shape bias. 4.1 Word learning biases This experiment shows that, when an agent is trained on shapes only, it will exhibit a shape bias when tested on new shapes and colors. They examined a few key phenomena: shape/color bias, learning negation concepts, incremental learning, and how learning affects the representation of objects via attention-like processes. 3 A situated language learning agent When the training set is balanced, the agent shows a mild bias for the simpler color property. The authors used situated versions of human language learning tasks as simulation environments to test a CNN + LSTM deep learning network. The extent to which it provides us with insight into human cognition depends on the degree to which we believe the structure of the agent and the task have a correspondence to the human case, which is ultimately probably quite limited. Concerning the attention analysis, it seems to me that all it's saying is that lower layers of a CNN detect lower-level properties such as colors, higher layers detect more complex properties, such as shapes characterizing objects. Alternatively, if the equivalent non-situated model does show the phenomena, then using the situated version would not be useful because the model acts equivalently in both cases. 4. The section on learning speeds could include more information on the actual patterns that are found with human learners, for example the color words are typically acquired later.
The (Mirowski et al, 2016) paper shows that a neural network-based agent with LSTM-based memory and auxiliary tasks such as depth map prediction can learn to navigate in fixed environments (3D mazes) with a fixed goal position (what they call "static maze"), and in fixed mazes with changing goal environments (what they call "environments with dynamic elements" or "random goal mazes"). This proposed paper rejects some of the claims that were made in Mirowski et al. 2016, mainly the capacity of the deep RL agent to learn to navigate in such environments. The previous paper (Mirowski et al. 2016) introduced a deep RL agent with auxiliary losses that facilitates learning in navigation environments, where the tasks were to go from a location to another in a first person viewed fixed 3d maze, with the starting and goal locations being either fixed or random. However, when RL is applied to navigation problems it is tempting to evaluate the agent on unseen maps in order to assess weather the agent has learned a generic mapping & planning policy.
Summary: This paper empirically studies adversarial perturbations dx and what the effects are of adversarial training (AT) with respect to shared (dx fools for many x) and singular (only for a single x) perturbations. Pro: - Paper addresses an important problem: qualitative / quantitative understanding of the behavior of adversarial perturbations is still lacking. - AT decreases the effectiveness of adversarial perturbations, e.g. AT decreases the number of adversarial perturbations that fool both an input x and x with e.g. a contrast change. Based on experiments using CIFAR10, the authors show that adversarial training is effective in protecting against "shared" adversarial perturbation, in particular against universal perturbation.
They provide an analysis that connects aims to establish a connection between how CNNs for solving super resolution and solving sparse regularized inverse problems. The method proposes a new architecture for solving image super-resolution task. So assuming an L1 regularization in this equation assumes that the signal itself is sparse.
To reduce the memory required for training, the authors also propose a path-wise training procedure based on the independent convolution paths of CrescendoNet. The experimental results on CIFAR-10, CIFAR-100 and SVHN show that CrescendoNet outperforms most of the networks without residual connections. If averaging paths leads to ensembling in CrescendoNet, it leads to ensembling in FractalNet. While the longest path in FractalNet is stronger than the other members of the ensemble, it is nevertheless an ensemble. The CrescendoNet is like a variant of the FractalNet, and the only difference is that the number of convolutional layers in Crescendo blocks grows linearly. While CrescendoNet seems to slightly outperform FractalNet in the experiments conducted, it is itself outperformed by DiracNet. Hence, CrescendoNet does not have the best performance among skip connection free networks.
- While the motivation is that classes have different complexities to learn and hence you might want each base model to focus on different classes, it is not clear why this methods should be better than normal boosting: if a class is more difficult, it's expected that their samples will have higher weights and hence the next base model will focus more on them. This paper consider a version of boosting where in each iteration only class weights are updated rather than sample weights and apply that to a series of CNNs for object recognition tasks. This paper instead designed a new boosting method which puts large weights on the category with large error in this round.
The authors observe that the proposed recurrent identity network (RIN) is relatively robust to hyperparameter choices. - While the LSTM baseline matches the results of Le et al., later work such as Recurrent Batch Normalization or Unitary Evolution RNN have demonstrated much better performance with a vanilla LSTM on those tasks (outperforming both IRNN and RIN).
More importantly, this relationship between test accuracy and learnability doesn't answer the original question Q2 posed: "Do larger neural networks learn simpler patterns compared to neural networks when trained on real data". Also, since we are looking at empirically validating the learnability criterion defined by the authors, all the results (the reported confusion tables) need to be tested statistically (to see whether one dominates the other). -As suggested in the final sentence of the discussion, it would be nice if conclusions drawn from the learnability experiments done in this paper were applied to the design new networks which better generalize Summary: This paper presents very nice experiments comparing the complexity of various different neural networks using the notion of "learnability" Review Summary: The primary claim that there is "a strong correlation between small generalization errors and high learnability" is correct and supported by evidence, but it doesn't provide much insight for the questions posed at the beginning of the paper or for a general better understanding of theoretical deep learning. Other results presented in the paper are puzzling and require further experimentation and discussion, such as the trend that the learnability of shallow networks on random data is much higher than 10%, as discussed at the bottom of page 4. A small network (N1 = 16 neurons) with low test accuracy results in a low learnability, while a large network (N1 = 1024 neurons) gets a higher test accuracy and higher learnability.
- Previous compression work uses a lot of tricks to compress convolutional weights. What is your compression ratio for 0 accuracy loss?
About the content: The main problem for me is that the whole construction using field theory seems to be used to advocate for the appearence of a phase transition in neural nets and in learning. PAGE 2: * "Using a scalar field theory we show that a phase transition must exist towards the end of training based on empirical results."
* results are only partially motivated and analyzed This paper proposed some new energy function in the BEGAN (boundary equilibrium GAN framework), including l_1 score, Gradient magnitude similarity score, and chrominance score, which are motivated and borrowed from the image quality assessment techniques. experiments on the using different hyper-parameters of the energy function, as well as visual inspections on the quality of the learned images, are presented. Using this energy function, the authors hypothesize, that it will force the generator to generate realistic images. In particular, the authors propose to change the energy function associated with the auto-encoder, from an L2 norm (a single number) to an energy function with multiple components. Quick summary: This paper proposes an energy based formulation to the BEGAN model and modifies it to include an image quality assessment based term.
Regularizers can be designed to affect statistical properties of the representations, such as sparsity, variance, or covariance. 2. Remarks Shannons channel coding theory was used by the authors to derive regularizers, that manipulate certain statistical properties of representations learned by DNNs. In the reviewers opinion, there is no theoretical connection between DNNs and channel theory. The paper falls short in explaining how DNNs and Shannons channel coding theory fit together theoretically and how they used it to derive the proposed regularizers.
And p.1 says that the "CL" layers are those often referred to as "FC" layers, not "conv" (though they may be convolutionally applied with spatial 1x1 kernels). Networks with very few connections in the upper layers are experimentally determined to perform almost as well as those with full connection masks. These names usually indicate the name of a single layer within the network (conv2 for the second convolutional layer or series of layers in the second spatial size after downsampling, for example). But in the upper layers, the spatial extent can be very small compared to the image size, sometimes even 1x1 depending on the network downsampling structure. Detailed comments and questions: The distribution of connections in "windows" are first described to correspond to a sort of semi-random spatial downsampling, to get different views distributed over the full image.
This paper's main thesis is that automatic metrics like BLEU, ROUGE, or METEOR is suitable for task-oriented natural language generation (NLG). The authors present a solid overview of unsupervised metrics for NLG, and perform a correlation analysis between these metrics and human evaluation scores on two task-oriented dialog generation datasets using three LSTM-based models. 1) This paper conducts an empirical study of different unsupervised metrics' correlations in task-oriented dialogue generation. The authors here show that the performance of various NN models as measured by automatic metrics like BLEU and METEOR is correlated with human eval.
Summary: The paper proposes a self-play model for goal oriented dialog generation, aiming to enforce a stronger coupling between the task reward and the language model.
The used Visual Concepts (VCs) were already introduced by other works (Wangt'15), and is not a novelty. The paper proposes a method for few-shot learning using a new image representation called visual concept embedding. Visual concepts were introduced in Wang et al. 2015, which are clustering centers of feature vectors in a lattice of a CNN. I believe this paper needs to focus on the working of the VCs for few-shot experiments, showing the influences of some of the choices (layer, network layout, smoothing, clustering, etc). Using the visual concept embedding, two simple methods are used for few-shot learning: a nearest neighbor method and a probabilistic model with Bernoulli distributions.
Arguments in section 2.3 are weak because, again, all other grapheme-based end-to-end systems have the same benefit as CTC and ASG. However, all of the other grapheme-based end-to-end systems enjoy the same benefit as CTC and ASG. either in terms of FLOPS or measured times This paper applies gated convolutional neural networks [1] to speech recognition, using the training criterion ASG [2]. The connection between ASG, CTC, and marginal log loss has been addressed in [9], and it does make sense to train ASG with the partition function. The authors argue that ASG is better than CTC in section 2.3.1 because it does not use the blank symbol and can be faster during decoding. There is no reason to believe that ASG can be faster than CTC in both training and decoding. However, once the transition scores are introduced in ASG, the search space becomes quadratic in the number of characters, while CTC is still linear in the number characters.
We'd like the number of bits parameter to trade off between accuracy (at least in terms of quantization error, and ideally overall loss as well) and precision. But it's not at all clear that the gradient of either the loss or the quantization error w.r.t. the number of bits will in general suggest increasing the number of bit (thus requiring the bit regularization term). The paper proposes a technique for training quantized neural networks, where the precision (number of bits) varies per layer and is learned in an end-to-end fashion.
This paper provides a systematic study of data augmentation in image classification problems with deep neural networks and argues that data augmentation could replace some common explicit regularizers like the weight decay and dropout. This paper presents an empirical study of whether data augmentation can be a substitute for explicit regularization of weight decay and dropout.
On a slightly modified set of structured scene representations from the CLEVR dataset, this approach outperforms two LSTM baselines with incomplete information, as well as an implementation of Relation Networks. Of course many people have published on CLEVR although of its language limitations, but I was a bit surprised that only these features are enough to solve the problem completely, and this makes me curious as to how hard is it to reverse-engineer the way that the language was generated with a context-free mechanism that is similar to how the data was produced. * Related to that is that the decision for a score of a certain type t for a span (i,j) is the sum for all possible rule applications, rather than a max, which again means that there is no competition between different parse trees that result with the same type of a single span. This paper proposes for training a question answering model from answers only and a KB by learning latent trees that capture the syntax and learn the semantic of words, including referential terms like "red" and also compositional operators like "not".
- It looks like after training MCTSnet with a massive amount of data from another MCTS, MTCSnet algorithm as in Algorithm 2 will not do very much more planning yet. I suspect that generating the training data and learning the model takes an enormous amount of CPU time, while 25 MCTS rollouts can probably be done in a second or two. The authors propose to train this using policy gradient in which data of optimal state-action pairs is generated by a standard MCTS with a large number of simulations. Would it be fair to have a baseline that learns the MCTS coefficient on the training data? The proposed method allows each component of MCTS to be rich and learnable, and allows the joint training of the evaluation network, backup network, and simulation policy in optimizing the MCTS network. This paper proposes a framework for learning to search, MCTSNet. The paper proposes an idea to integrate simulation-based planning into a neural network. The computation of the network proceeds just like a simulation of MCTS, but using a simulation policy based on the memory vector to initialize the memory vector at the leaf. If I understand them correctly, the comparison is between a neural network that has been learned on 250,000 trajectories of 60 steps each where each step is decided by a ground truth close-to-optimal algorithm, say MCTS with 1000 rollouts (is this mentioned in the paper). In overall, it is a nice idea to use DNN to represent all update operators in MCTS. The key is to represent the internal state of the search by a memory vector at each node. How would the technique scale with more MCTS iterations?
The branch&bound method is fairly standard, two benchmarks were already existing and the third one is synthetic with weights that are not even trained (so not clear how relevant it is). which sounds a bit like using linear programming relaxations, which is what the approaches using branch and bound cited above use.
The structured deep-in factorization machines allow higher-level interactions in embedding learning which allows the authors to learn embeddings for heterogeneous set of features. The authors introduced a new compatibility function between features and, as in the skipgram approach, they propose a variation of negative sampling to deal with structured features. Summary: This paper proposes an approach to learn embeddings for structured datasets i.e. datasets which have heterogeneous set of features, as opposed to just words or just pixels. This paper provides a clean way of learning embeddings for structured features that can be discrete -- indicating presence / absence of a certain quality. I would like to see an evaluation of these features in a classification setting to further demonstrate the utility of these embeddings as compared to directly embedding the discrete features and then performing a K-way classification. Unlike, word2vec there is no hard constraint that similar objects must have similar representations and so, the learnt embeddings reflect the likelihood of the observed features. Further, these features can be structured i.e. a set of them are of the same 'type'. SUMMARY. The paper presents an extension of word2vec for structured features.
Two proposals in the paper are: (1) Using a learning rate decay scheme that is fixed relative to the number of epochs used in training, and This paper proposes a fast way to learn convolutional features that later can be used with any classifier. And for (2), it is a relatively standard approach in utilizing CNN features. It is well known that CNN features are much better if enough data is available. The acceleration of the training comes from a reduced number of training epocs and a specific schedule decay of the learning rate.
The only conclusions I can draw from the visual analysis is that the context vectors are more similar to each other when they are obtained from time steps in the data stream which are close to each other. This writeup describes an application of recurrent autoencoder to analysis of multidimensional time series. The paper describes a sequence to sequence auto-encoder model which is used to learn sequence representations. This paper proposes a strategy that is inspired by the recurrent auto-encoder model, such that clustering of multidimensional time series data can be performed based on the context vectors generated by the encoding process. * Very little information about the data.
The algorithm first builds two conv nets for molecular graphs, one is for searching relevant substructures (policy improvement), and another for evaluating the contribution of selected substructures to the target task (policy evaluation). Both parts are based on conv nets for molecular graphs, and this framework is a kind of 'self-supervised' scheme compared to the standard situations that the environment provides rewards. Given that conv nets for molecular graphs are not trivially interpretable, this would provides a useful approach to use conv nets for more explicit interpretations of how the task can be performed by neural nets. As the paper states in Introduction, the target problem is 'hard selection' of substructures, rather than 'soft selection' that neural nets (with attention, for example) or neural-net fingerprints usually provide. The experimental validations demonstrate that this model can learn a competitive-performed conv nets only dependent on the highlighted substructures, as well as reporting some case study on the inhibition assay for hERG proteins. The paper proposes a feature learning technique for molecular prediction using reinforcement learning.
Conclusion: Though with a quite novel idea on solving multi-task censored regression problem, the experiments conducted on synthetic data and real data are not convincing enough to ensure the contribution of the Subspace Network. The authors propose a DNN, called subspace network, for nonlinear multi-task censored regression problem. 8. The performance on One-Layer Subspace Network (with only the input features) could be added. They compare the proposed SN with other traditional approaches on a very small data  set with 670 samples and 138 features. - The computation time for the linear model shown in Table 3 is quite surprising (~20 minutes for linear regression of 5k observations). At each layer a low-rank constraint is enforced on the linear transformation, while the cost function is specified in order to differentially account for the bounds of the predicted variables. Furthermore, while being the main methodological drive of this work, the paper does not show evidence about improved predictive performance and generalisation when accounting for the boundedness of the regression targets. 3. In synthetic data experiments (Table1), only small margins could be observed between SN, f-MLP and rf-MLP, and only Layer 1 of SN performs better above all others. This paper presents a new multi-task network architecture within which low-rank parameter spaces were found using matrix factorization.
Related works: - For your consideration: is multi-task survival analysis effectively a competing risks model, except that these models also estimate risk after the first competing event (i.e. in a competing risks model the rates for other events simply go to 0 or near-zero)? The paper entitled 'Siamese Survival Analysis' reports an application of a deep learning to three cases of competing risk survival analysis. The authors tackle the problem of estimating risk in a survival analysis setting with competing risks. However, afterwards the objective does combine time-dependent discrimination indices of several competing risks, with different denominator values. This paper introduces siamese neural networks to the competing risks framework of Fine and Gray.
When the authors say "specialization" or "specialized model", they sometimes mean distillation, sometimes filter pruning, and sometimes cascades. They show using the ResNet architecture that combining distillation, pruning, and cascades are complementary and can yield pretty nice speedups. However, in my understanding the main source of speed-up is a cascade approach with a reduced model, in which is not clear how much speed-up is actually due to the specialized task.
1) those independent transformations; and 2) inverse transformations that map data from transformed distributions to their corresponding canonical distribution. Experiments on MNIST data shows that in the end of training, each expert wins almost all samples from one transformation and no other, which confirms that each expert model a single inverse transformation. Summary: Given data from a canonical distribution P and data from distributions that are independent transformations (mechanisms) applied on P, this paper aims to learn 2) The authors only run experiments on the MNIST data, where 1) the mechanisms are simulated and relatively simple, and 2) samples from the canonical distribution are also available.
The main result is that every local minimum of the total surface is a global minimum of the region where the ReLU activations corresponding to each sample do not change. For example, in Section 3.1 the argument is made that the linear region created when all activations are equal to one, will have a local minimum, and this minimum might be suboptimal. Here, the results only hold within each local region of the space, and they don't say anything about how the global minima in different regions compare in terms of loss.
The distribution output from a given node is defined in terms of a learned conditional probability function, and the output distributions of its input nodes. However, this experiment uses 3BE outside of its intended use case --- which is for a single input distribution --- so it's not entirely clear how well the very simple proposed model is doing. It's not clear how much value there is adding yet another distribution to distribution regression approach, this time with neural networks, without some pretty strong motivation (which seems to be lacking), as well as experiments. This seems to be a nice treatment of distribution to distribution regression with neural networks.
The paper tries to build an interpretable and accurate classifier via stacking a supervised VAE (SVAE) and a differentiable decision tree (DTT). Summary This paper proposes a hybrid model (C+VAE)---a variational autoencoder (VAE) composed with a differentiable decision tree (DDT)---and an accompanying training scheme. Moreover, I like the qualitative experiment (Figure 2) in which the tree is used to vary a latent dimension to change the digit's class.
The authors propose a particular variance regularizer on activations and connect it to the conditional entropy of the activation given the class label. But then, if the goal is to _minimize_ the mutual information between I(C;Y), it makes sense to _maximize_ the conditional entropy H(C|Y). The main one is that the connection between conditional entropy and the proposed variance regularizer seems tenuous.
The appropriateness of using additional pages over the recommended length will be judged by reviewers."  In the present submission, the first 8+ pages contain minimal new material, just various background topics and modified VAE update rules to account for learning noise parameters via basic EM algorithm techniques. The original Gaussian VAE proposes to use the inference network for the noise that takes latent variables as inputs and outputs the variances, but most of the existing works on Gaussian VAE just use fixed noise probably because the inference network is hard to train. This paper studies the importance of the noise modelling in Gaussian VAE.
d) I'm not able to really follow Definition 1, perhaps due to unclear notation.
The main issue with training models in this formulation is the alignment of the generated graph to the ground truth graph. I would have at least liked to see a comparison to a method that generated SMILES format in an autoregressive manner (similar to previous work on chemical graph generation), and would ideally have liked to see an attempt at solving the alignment problem within an autoregressive formulation (e.g., by greedily constructing the alignment as the graph was generated). To handle this, the paper proposes to use a simple graph  matching algorithm (Max Pooling Matching) to align nodes and edges. Pros: - the formulation of the problem as the modeling of a probabilistic graph is of interest I see this as equivalent to choosing an order in which to linearize the order of nodes and edges in the graph. First, the motivation for one-shot graph construction is not very strong: - I don't understand why the non-differentiability argued in (a) above is an issue. The search space of small graph generation is usually very small, is there any other traditional methods can work on this problem? d) the graph matching procedure proposed is a rough patch for a much deeper problem This work proposed an interesting graph generator using a variational autoencoder. Please also don't make statements like "To the best of our knowledge, we are the first to address graph generation using deep learning." This is very clearly not true. - some of the main issues with graph generation are acknowledged (e.g. the problem of invariance to node permutation) and a solution is proposed (the binary assignment  matrix) Based on this motivation, the paper decides to generate a graph in "one shot", directly  outputting node and edge existence probabilities, and node attribute vectors. - I also don't agree that the one shot graph construction sidesteps the issue of how to linearize the construction of a graph. - many crucial elements  in graph generation are not dealt with: a) the adjacency matrix and the label tensors are not independent of each other, the notion of a graph is in itself a way to represent the 'relational links' between the various components b) the boundaries between a feasible and an infeasible graph are sharp: one edge or one label can be sufficient for acting the transition independently of the graph size, this makes it a difficult task for a continuous model. Even disregarding Johnson (2017), which the authors claim to be unaware of, I would consider approaches that generate SMILES format (like Gomez-Bombarelli et al) to be doing graph generation using deep learning.
Summary of the paper: This paper presents a method, called \\alpha-DM (the authors used this name because they are using \\alpha-Divergence to measure the distance between two distributions), that addresses three important problems simultaneously: (a) Objective score discrepancy: i.e., in ML we minimize a cost function but we measure performance using something else, e.g., minimizing cross entropy and then measuring performance using BLEU score in Machine Translation (MT). It is argued that the ML approach has some "discrepancy" between the optimization objective and the learning objective, and the RL approach suffers from bad sample complexity. (b) Sampling distribution discrepancy: The model is trained using samples from true distribution but evaluated using samples from the learned distribution I do not agree with statements like "We demonstrate that the proposed objective function generalizes ML and RL objective functions …" that authors have made in the abstract. The objective is based on alpha-divergence between the true input-output distribution q and the model distribution p. Below are the points that I'm particularly confused about: 1. For the ML formulation, the paper made several particularly confusing remarks. 1.2 I understand that the ML objective is different from what the users really care about (e.g., blue score), but this does not seem a "discrepancy" to me. 2. For the RL approach, I think it is very unclear as a formulation of an estimator. For example, 1.1 The q(.|.) distribution in Eq. The model is trained on an empirical distribution whose points are sampled from the true distribution. In summary, I'm not convinced that the fact that ML optimizes a different objective than the blue score is a problem with the ML estimator. The ML estimator simply finds a parameter that is the most consistent to the observed sequences;
There really isn't much discussion about the architecture of networks, but rather the dimensionality of the feature maps. In particular, expanding the feature depth at layer f at time t, may have non trivial effect on layer f-1 at time t + 1.
[Minor] The derived bounds depend on M, an a priori upper bound on the Renyi divergence between the logging policy and any new policy. 5.3: so that results more comparable In this paper the authors studied the problem of off-policy learning, in the bandit setting when a batch log of data generated by the baseline policy is given. It develops a new learning objective where the empirical risk is regularized by the squared Chi-2 divergence between the new and old policy.
The improved upper bound given in Theorem 1 appeared in SampTA 2017 - Mathematics of deep learning ``Notes on the number of linear regions of deep neural networks'' by Montufar. Previous work [1], [2], has derived lower and upper bounds for the number of linear regions that a particular neural network architecture can have. It builds on previous works Montufar et al. (2014) and Raghu et al. (2017) and presents improved bounds on the maximum number of linear regions. (The improvement on Zaslavsky's theorem is interesting.) The idea of counting the number of regions exactly by solving a linear program is interesting, but is not going to scale well, and as a result the experiments are on extremely small networks (width 8), which only achieve 90% accuracy on MNIST. Paper Summary: This paper looks at providing better bounds for the number of linear regions in the function represented by a deep neural network.
A composite of transformations coupled with the LAM/RAM networks provides a highly expressive model for modelling arbitrary joint densities but retaining interpretable conditional structure. Comparing with the Masked Autoregressive Flows (Papamakarios et al., 2017) paper, it seems that the true difference is using the linear autoregressive transformation (LAM) and recurrent autoregressive transformation (RAM), already present in the Inverse Autoregressive Flow (Kingma et al., 2016) paper they cite, instead of the masked feedforward architecture used Papamakarios et al. (2017). It consists of a number of broad ideas regarding density estimation using transformations of autoregressive networks.
Based on experiments done by the authors, on MNIST, having this procedure gives the same performance with 3-4 times less memory or increase in performance of 1% for the same memory as regular network. The cost of reducing the memory storage is the extra computation.
Without a solid and consistent basis for these hyper-parameter perturbations, I worry that this approach will fail to normalize the effect of experiment numbers while also giving researchers an excuse to avoid reporting their experimental process. Even the idea of researchers knowing their test set variance makes me very uneasy.
------ The paper presents smoothing probabilistic box embeddings with softplus functions, which make the optimization landscape continuous, while also presenting the theoretical background of the proposed method well. - The main thrust of section 5.2 is that smoothed box embeddings retain better performance with increasing numbers of negatives. This paper proposes a soft relaxation of the box lattice (BL) model of Vilnis et al. 2018 and applies it to several graph prediction tasks. - The Gaussian relaxation (Eq. (2) and (3)) defines a particular length scale, \\sigma. Also, I think some graphical illustration of the embedding would be very helpful, perhaps something like Figure 2 of "Probabilistic Embedding of Knowledge Graphs with Box Lattice Measures". (Why would we expect the smoothed box model to handle unseen captions better?) - Flickr data: what is the encoder model that produces the embeddings here, and how does it handle unseen captions? Specifically, the paper builds on a a geometrically inspired embedding method using box representations.
To explain the difficulty of training pruned networks from scratch or why training needs the overparameterized networks that make pruning necessary,  the authors propose a lottery ticket hypothesis: unpruned, randomly initialized NNs contain subnetworks that can be trained from scratch with similar generalization accuracy. This paper proposes a conjecture to explain this phenomenon that the authors call "The Lottery Ticket Hypothesis":  large networks that can be trained successfully contain at initialization time small sub-networks — which are defined by both connectivity and the initial weights that the authors call "winning tickets" — that if trained separately for similar number of iterations could reach the same performance as the large network. The paper follows by proposing a method to find these winning tickets by pruning methods, which are typically used for compressing networks, and then proceed to test this hypothesis on several architectures and tasks. It can be summarized that there exists a sparse network that can be trained well only provided with certain weight initialization.The winning tickets can only be found via iterative pruning of the trained network. The author defines a winning lottery ticket as a sparse subnetwork that can reaching the same performance of the original network when trained from scratch with the "original initialization". For instance, there is a growing interest in the link between compression and generalization that is relevant to this work [3,4], and the effect of winning ticket leading to better generalization could be explained via other works which link structure to inductive bias [5,6].
Authors introduce  a criterion to be used for identifying important parts of the network (connection sensitivity), that does not depend on the magnitude of the weights for neurons: they start by introducing a set of binary weights (one per a weight from a neuron) that indicate whether the connection is on or off and can be removed. Please relate to it. Fundamentally, if you decouple weight pruning from initialization it also means that: - the first layer will be pruned out of connections to constant pixels (which is seen in the visualizations), this remains meaningful even after a reinitialization Does it mean that first a set of random weights is sampled, then the sensitivities are computed, then a salient set of connections is established and the weights are REINITIALIZED from a distribution possibly different than the one used to compute the sensitivity? Am I correct to assume that a conv layer has many more connections, than weights? - It is correct that the weights used to train the pruned model are possibly different from the ones used to compute the connection sensitivity. - the second and higher layers will be pruned somewhat randomly - even if the connections pruned were meaningful with the original weights, after the reinitialization the functions computed by the neurons in lower layers will be different, and have no relation to pruned weights. Thus we can trivially remove connections, without removing weights. The fact that it works is very surprising and again suggests that the method identifies constant background pixels rather than important weights. - How does it compare with just randomly dropping the connections or dropping them based on the magnitude of the initial weights. SNIP identifies prunable weights by the normalised gradient of the loss w.r.t. an implicit multiplicative factor "c" on the weights, denoted as the "sensitivity". My main concern is that paper differentiates between weights and connections (both terms are introduced on page iv to differentiate from earlier work). Finally, no experiment shows the benefit of introducing the variables "c", rather than using the gradient with respect to the weights.
In the context of vanilla Gaussian VAEs (Gaussian prior, encoders, and decoders) the authors show that if (a) the intrinsic data dimensionality r is equal to the data space dimensionality d and (b) the latent space dimensionality k is not smaller than r then there is a sequence of encoder-decoder pairs achieving the global minimum of the VAE objective and simultaneously (a) zeroing the variational gap and (b) precisely matching the true data distribution. Main algorithmic contributions: (0) A simple 2 stage algorithm, where first a vanilla Gaussian VAE is trained on the input dataset and second a separate vanilla Gaussian VAE is trained to match the aggregate posterior obtained after the first stage. However, the authors somewhat mischaracterize the scope of applicability of VAE models in contemporary machine learning, and don't show familiarity with the broad literature around VAEs outside of this case (that is, where a Gaussian model of the output would be manifestly inappropriate).
The key innovation of the article, compared to the aforementioned papers, lies on the idea of learning face/voice embeddings to maximise their ability to predict covariates, rather than by explicitly trying to optimise an objective related to cross-modal matching. Authors aim to reveal relevant dependencies between voice and image data (under a cross-modal matching framework) through common covariates (gender, ID, nationality). # Summary The article proposes a deep learning-based approach aimed at matching face images to voice recordings belonging to the same person. These include identifying a matching face in a set of faces (1,2 or N faces) for a given voice, or vice versa. The classification network predicts various combinations of covariates of faces and voices: gender, nationality, and identity.
but immediately below, "We show empirically that weight decay only improves generalization by controlling the norm, and therefore the effective learning rate." In this case it is discussed for Gauss-Newton KFAC that the benefit mostly comes from the application of weight decay on the softmax layer and the effect of weight decay on other weights cancel out due to batch normalization.
2) Have you tried pre-training c_t's as continuous latent variables? The proposed approach uses a pre-training step, based on a variational auto-encoder (VAE), to estimate latent variable sequences. In particular, it follows the ideas presented in InfoGAIL, that depends on a latent variable, The paper presents a learning-based method for learning the latent context codes from demonstrations along with a GAIL model. To achieve that the paper introduces an unsupervised variational objective by maximizing the directed mutual information between the latents c's and the trajectories. A reasonable confirmation that the model indeed learns composition is to generate a trajectory for a sequence of latent code not seen in data.
The core idea is to use the Lanczos algorithm to obtain a low-rank approximation of the graph Laplacian. This paper proposes to use a Lanczos alogrithm, to get approximate decompositions of the graph Laplacian, which would facilitate the computation and learning of spectral features in graph convnets. Overall, the idea of using Lanczos algorithm to bypass the computation of the eigendecomposition, and thus simplify filtering operations in graph signal processing is not new [e.g., 35]. (3) drawing interesting connections with graph diffusion methods which naturally arise from the matrix power computation inherent to the Lanczos iteration. The paper under review builds useful insights and novel methods for graph convolutional networks, based on the Lanczos algorithm for efficient computations involving the graph Laplacian matrices induced by the neighbor edge structure of graph networks.
At first, it compares to random graph search and ES. This paper proposes an approach for automatic robot design based on Neural graph evolution. The authors propose a scheme based on a graph representation of the robot structure, and a graph-neural-network as controllers. - Sec 4.1:  would argue that computational cost is rarely a concern among evolutionary algorithms. [Strengths]: This paper shows some promise when graph network-based controllers augmented with evolutionary algorithms. The overall approach has a flavor of genetical algorithms, as it also performs evolutionary operations on the graph, but it also allows for a better mechanism for policy sharing across the different topologies, which is nice. Detailed comments: - in the abstract you say that "NGE is the first algorithm that can automatically discover complex robotic graph structures". Please include further description of the ES cost function and algorithm in the main body of the paper. If you can also compare against one or two algorithms of your choice from the recent literature it would also give more value to the comparison. This paper uses graph network to train each morphology using RL. Model-based RL algorithms can work in real-time (e.g. http://proceedings.mlr.press/v78/drews17a/drews17a.pdf) and have been shown to have same asymptotic performance of MB controllers for simple robot control (e.g. https://arxiv.org/abs/1805.12114) How would the given graph network compare to this? If so, why? Also, it is not clear what is the meaning of generations if the graph is fixed, can't it be learned altogether at once? using ES. Given that, the novelty of the paper is fairly incremental as it uses NerveNet to evaluate fitness and ES for the main design search. #5 is weak, and tell us more about the limitations of random search and naive ES than necessarily a merit of your approach. Also, note that some of the algorithms that you are citing there have indeed applied beyond architecture search, eg. What instead is missing is an answer to the question: Is it worth using a neural graph?
The authors use their derived formula for VRR to predict the minimum mantissa precision needed for accumulators for three well known networks: AlexNet, ResNet 32 and ResNet 18. Thereafter, the mantissa precision of the accumulator is predicted to maintain the error of accumulation within bounds by keeping the VRR as close to 1 as possible. The authors present their metric called Variance Retention Ratio (VRR) as a function of the mantissa length of product terms, partial sum (accumulator) terms, and the length of the accumulation. The analysis is based on Variance Retention Ratio (VRR), and authors show the theoretical impact of reducing the number of bits in the floating point accumulator. They propose an information theoretic approach to argue that by using fewer bits of mantissa in the accumulator than necessary, the variance of the resulting sum is less than what it would have been if sufficient bits of mantissa were used.
The authors claim the generator of RoC-Gan will span the target manifold, even in the presence of large amounts of noise. This work focuses on the robustness of conditional GAN(RoC-GAN) when facing the noise. Authors propose to augment a conditional GAN model with an unsupervised branch for spanning target manifold and show better performance than the conditional GAN in natural scene generation and face generation. The main contribution of the paper is to introduce a two-pathway model, where one of them is used to perform regression as ordinary GAN while the other one helps the whole model span the target domain. The authors study the theoretical properties of RoC-GAN and prove that it shares the same properties as the vanilla GAN. - The theoretical analysis of the method relates RoC-GAN to the original GAN, rather than CGAN! If RoC-GAN is very similar to CGAN from a theoretical point of view (which it seems to be), then all the analysis to relate it to traditional GAN seem useless.
Finally {\\bf Theorem 3} is presented, which provides a lower bound for the Rademacher complexity of a class of neural networks, and such bound is compared with existing lower bounds. An empirical comparison with existing generalization bounds is made and the presented bound is the only one that in practice decreases when the number of hidden units grows. These Rademacher bounds yield standard bounds on the ramp loss for fixed alpha,beta, and margin, and then a union bound argument extends the bound to data-dependent alpha,beta and margin. From this statement and the discussion comparing the bounds, it is not clear whether this bound formally dominates existing bounds or merely does so empirically (or under empirical conditions). Next, {\\bf Theorem 2} which is the main result, presents a new upper bound for the generalization error of 1-layer networks. Then, {\\bf Theorem 1} provides an upper bound for the empirical Rademacher complexity of the class of 1-layer networks with hidden units of bounded \\textit{capacity} and \\textit{impact}.
The theoretical analysis identifies a quantity called gradient coherence and proves that a learning rate based on the coherence can lead to an optimal convergence rate even under asynchronous training. interestingly this seems to also explain the heuristic commonly believed that to make asynchronous training work one needs to slowly anneal the number of workers (coherence is much worse in the earlier than later phases of training).
This paper is not a great topic fit for *CONF*: it's primarily about a hand-designed performance metric for sequence labeling and a hierarchical Bayesian model with Gaussian observations and fit with Gibbs sampling in a full-batch setting. I was surprised by some model details: for example, in "Modeling the procedure" of Sec 4.1, it would be much more satisfying to generate the (p_1, ..., p_s) sequence from an HMM instead of sampling the elements of the sequence independently, dropping any chance to learn transition structure as part of the Bayesian inference procedure.
One would at least expect an after-the-fact interpretation for the weighted tensor term and what this implies with regard to their method and syntactic embedding compositions in general. Building on the generative word embedding model provided by Arora et al. (2015), their solution uses the core tensor from the Tucker decomposition of a 3-way PMI tensor to generate an additive term, used in the composition of two word embedding vectors. Given that the main attraction of the paper is the potential for more performant word embeddings, I do not believe the work will have wide appeal to *CONF* attendees, because no evidence is provided that the features from the learned tensor, say [a, b, T*a*b], are more useful in downstream applications than [a,b] (one experiment in sentiment analysis is tried in the supplementary material with no compelling difference shown). The authors consider the use of tensor approximations to more accurately capture syntactical aspects of compositionality for word embeddings. Unlike Arora's original work, the assumptions they make on their subject material are not supported enough, as in their lack of explanation of why linear addition of two word embeddings should be a bad idea for composition of the embedding vectors of two syntactically related words, and why the corrective term produced by their method makes this a good idea. This is not true. The results presented by Arora et al. indeed show that RAND-WALK captures syntactic information, albeit to a lesser extent than other popular methods for word embedding (Table 1, Arora et al. 2015).
Visual stimuli are presented and the responses of the neurons recorded. In this interesting study, the authors show that incorporating rotation-equivariant filters  (i.e. enforcing weight sharing across filters with different orientations) in a CNN model of the visual system is a useful prior to predict responses in V1.
The paper is a reasonable dataset/analysis paper. The dataset is then used to evaluate a number of recurrent models (LSTM, LSTM+attention, transformer); these are very powerful models for general sequence-sequence tasks, but they are not explicitly tailored to math problems. My main concerns are about the evaluation and comparison of standard neural models. The main contribution is a synthetically generated dataset that includes a variety of types and difficulties of math problems; it is both larger and more varied than previous datasets of this type. Weaknesses: The dataset created here is entirely synthetic, and the paper only includes one single small real-world case; it seems like it would be easy to generate a larger and more varied real world dataset as well (possibly from the large literature of extant solved problems in workbooks). typo: page 3: "freefrom inputs and outputs" -> "freeform inputs and outputs" Summary: This paper is about models for solving basic math problems. The results are then analyzed and insights are derived explaining where neural models seemingly cope well with math tasks, and where they fall down. The insights from the analysis of the failure cases are intriguing, but it also points out that the neural networks models are not really performing mathematical reasoning since the generalization is very limited. This paper presents a new synthetic dataset to evaluate the mathematical reasoning ability of sequence-to-sequence models. Several models including LSTM, LSTM + Attention, Transformer are evaluated on the proposed dataset. For example, EMLNP 2017 paper "Deep Neural Solver for Math Word Problems" mentions a size 60K problem dataset. It would have been useful to compare the general models here with some specific math problem-focused ones as well. But it's not clear to me why testing more sophisticated models that are tailored for math questions would *not* be useful.
The Bayesian GAN defines a posterior distribution for the generator that is proportional to the likelihood that the discriminator assigns to generated samples. ========= The paper extends Bayesian GANs by altering the generator and discriminator parameter likelihood distributions and their respective priors. arguing that it is an open research question whether the generator converges to the true data generating distribution in Bayesian GANs. I do not agree here.
For instance, the experiments seem to indicate that generalizing density estimation from CIFAR training set to CIFAR test set is likely challenging and thus the models underfit the true data distribution, resulting in the simpler dataset (SVHN) having higher likelihood. Specifically, density models trained on CIFAR10 have higher likelihood on SVHN than CIFAR10. - Lack of an extensive exploration of datasets Pros: - The finding that SVHN has larger likelihood than CIFAR according to networks is interesting. Given that you observed that SVHN has higher likelihood on all three model types (PixelCNN, VAE, Glow), why investigate a component specific to just flow-based models (the volume term)? Given the title of the paper, it would have been nice if this paper explored more than just MNIST vs NotMNIST and SVHN vs CIFAR10, so that the readers can gain a better feel for when generative models will be able to detect outliers.
The paper deals with the problem of recovering an exact solution for both the dictionary and the activation coefficients. The contribution improves Arora 2015 in that it converges linearly and recovers both the dictionary and the coefficients with no bias. As other works, the solution is based on a proper initialization of the dictionary. The paper shows that there is an alternating optimization-based algorithm for this problem that under standard assumptions provably converges exactly to the true dictionary and the true coefficients x (up to some negligible bias). The main contribution is the use of a IHT-based strategy to update the coefficients, with a gradient-based update for the dictionary (NOODL algorithm).
Maybe coming up with a toy dataset and network WITH bad local valleys will be sufficient, because after adding N skip connections the network will be free of bad local valleys. Also, it seems the proof of 3 is somewhat redundant, since local minimum is a special case of your "bad local valley". I think it's implicitly assumed in the proof that they are connected to all output nodes, but in this case Figure 2 is a bit misleading because there are hidden nodes with skip connections to only one of the output nodes. The "no bad local valleys" implies that for any point on the loss surface there exists a continuous path starting from it, on which the loss doesn't increase and gets arbitrarily smaller and close to zero. From what I read, the definition of "bad local valley" is implied by the abstract and in the proof of Theorem 3.3(2), but I did not find a formal definition anywhere else. Apart from its technicality in the proof, the statement of Lemma 3.2 is just as expected and gives me little surprise, because having more than N hidden nodes connected directly to the output looks morally "equivalent" to having a layer as wide as N, and it is known that in such settings (e.g. Nguyen & Hein 17') it is easy to attain global minima.
Original review: Summary: they propose a differentiable learning algorithm that can output a brush stroke that can approximate a pixel image input, such as MNIST or Omniglot. Since this emulation model is differentiable, they can easily train an algorithm to output a stroke to approximate the drawing via back propagation, and avoid using RL and costly compute such in earlier works such as [1]. The authors define strokes as a list of coordinates and pressure values along with the color and brush radius of a stroke. This is done by first training an encoder-decoder pair of neural networks where the latent variable is the stroke, and the encoder and decoder have specific structure which takes advantage of the known stroke structure of the latent variable. Then the authors investigate whether an agent can learn to produce the stroke corresponding to a given target image. Minor points: a) The figures look like they are bitmap, pixel images, but for a paper advocating stroke/vector images, I recommend exporting the diagrams in SVG format and convert them to PDF so they like crisp in the paper. Unlike sketch-pix2seq[3] (which is a pixel input -> sketch output model based on sketch-rnn[2]), their method trains in an unsupervised manner and does not require paired image/stroke data. Good luck! [1] SPIRAL https://arxiv.org/abs/1804.01118 [2] sketch-rnn https://arxiv.org/abs/1704.03477 [3] sketch-pix2seq https://arxiv.org/abs/1709.04121 [4] http://kanjivg.tagaini.net/ [5] https://quickdraw.withgoogle.com/data [6] https://vectormagic.com/ Revision: The addition of new datasets and the qualitative demonstration of latent space interpolations and algebra are quite convincing. While research from big labs [1] have the advantage of having access to massive compute so that they can run large scale RL experiments to train an agent to "sketch" something that looks like MNIST or Omniglot, the authors probably had limited resources, and had to be more creative to come up with a solution to do the same thing that trains in a couple of hours using a single P40 GPU.
This paper describes a framework - Tree Reconstruction Error (TRE) - for assessing compositionality of representations by comparing the learned outputs against those of the closest compositional approximation. -------------------------------------------------------------- The authors propose a measure of compositionality in representations. This in an interesting study and attacks a very fundamental question; tracking compositionality in representations could pave the way towards representations that facilitate transfer learning and better generalization. The paper demonstrates the use of this framework to assess the role of compositionality in a hypothetical compression phase of representation learning, compares the correspondence of TRE with human judgments of compositionality of bigrams, provides an explanation of the relationship of the metric to topographic similarity, and uses the framework to draw conclusions about the role of compositionality in model generalization. Nevertheless, it's a well written paper and is a helpful first step towards studying the problem of compositionality in vector representations. Now, onto the measure. I like the idea of learning basis vectors from the representations and constraining to follow the primitive semantics. There is nothing obviously unreasonable about the choices of composition operator, but it seems that the conclusions drawn cannot be construed to apply to compositionality as a general concept, but rather to compositionality when defined by these particular operators.
- for instance, a study of the impact of the regularization would have been interesting (how does the sigma of the Gaussian smoothing affect the type of adversarial attacks obtained and their performance -- is it possible to fool the network with [very] smooth deformations?); Experimental results on several benchmark datasets (MNIST, ImageNet) and commonly used deep nets (CNN, ResNet, Inception) are reported to show the power of adversarial deformations. However, the intuition behind the proposal does not make strong sense to the reviewer: since the main focus of this work is on model attack, why not directly (iteratively or not) adding random image deformations to fool the system? These results, as acknowledged by the authors, do not well support the effectiveness of deformation adversarial attack and defense. The paper introduces an iterative method to generate deformed images for adversarial attack. Pros: - The way of constructing deformation adversarial is interesting and novel - Numerical study shows some promise in adversarial attack, but is not supportive to the related defense capability. - experiments show what such a technique can achieve on MNIST and ImageNet. Interestingly, one can see on MNIST the parts of the numbers that the adversarial attack is trying to delete/create. More clearly: the space of small deformations tau comes with an inner product (here L2, but one could choose another one), and the gradient \\nabla g obtained depends on this inner product choice M, even though the derivative Dg is the same (they are related by Dg(tau) = < \\nabla_M g | tau >_M for any tau). - Note: about the remark in section 3.2: deformation-induced transformations are a subset of all possible transformations of the image (which are all representable with intensity changes), so it is expected that a training against attacks on the intensity performs better than a training against attacks on spatial deformations. - note: the approximation tau* = sum_i tau_i (section 2.3) does not stand in the case of non-small deformations. On one side (see Eq.4), the deformation \\tau should be small enough in scale to make an accurate approximation. It is possible to favor other kind of deformations (not just smooth ones, but for instance rigid ones, etc.
I think author want to point that when K=1, STL is unbiased with respect to the 1 ELBO, but when k>1, it is biased with respect to IWAE estimator. Author experimentally found that the estimator of the existing work(STL) is biased and proposed to reduce the bias by using the technique like  REINFORCE. IWAE improves the lower bound by increasing the samples, but we can also improve the bound by specifying the flexible posteriors like the mixture of Gaussians in STL. About the motivation of the paper, I think it might be better to move the Fig.1 about the Bias to the introduction and clearly state that the author found that the STL is biased "experimentally". Question and minor comments: In the original paper of STL, the author pointed out that by freezing the gradient of variational parameters to drop the score function term, we can utilize the flexible variational families like the mixture of Gaussians. I know that motivation is a bit different for STL and proposed method but some comparisons are needed. In this work, since we do not freeze the variational parameters, we cannot utilize the mixture of Gaussians as in the STL. So I do not know which is better and whether I should use this method or use the original STL with flexible posterior distribution to tighten the evidence lower bound. The reason I doubt the reason is that as I written in the below, the original STL can handle the mixture of Gaussians as the latent variable but the proposed method cannot.
The work is very incremental over Luo et al (2018) "End-to-end Active Object Tracking and Its Real-world Deployment via Reinforcement Learning", as the only two additions are extra observations o_t^{alpha} for the target, and a reward function that has a fudge factor when the target gets too far away. For active object tracking in real-world/3D environment, designing the reward function only based on the distance between the expected position and the tracked object position can not well reflect the tracker capacity. I think the contributions of this work is incremental compared with [Luo et al (2018)] in which the major difference is the partial zero sum reward structure is used and the observations and actions information from the tracker are incorporated into the target network, while the network architecture is quite similar to [Luo et al (2018)]. The scale changes of the target should also be considered when designing the reward function of the tracker. The ablation study shows that the tracker-aware observations and a target's reward structure that penalizes when it gets too far do help the tracker's performance, and that training the target agent helps the tracker agent achieve higher scores. The tracker receives, from its own perspective, partially observed visual information o_t^{alpha} about the target (e.g., an image that may show the target) and the target receives both observations from its own perspective o_t^{beta} and a copy of the information from the tracker's perspective. This work aims to address the visual active tracking problem in which the tracker is automatically adjusted to follow the target. In addition, the explanation about importance of the tracker awareness to the target network seems not sufficient. Experiments are conducted using two baselines for the target agent, one a random walk and another an agent that navigates to a target according to a shortest path planning algorithm.
They propose three enhancements to MILP formulations of neural network verification: Asymmetric bounds, restricted domain and progressive bound tightening, which lead to significantly more scalable verification algorithms vis-a-vis prior work. They study the effectiveness of MILP solvers both in terms of verifying robustness (compared to other complete/incomplete verifiers) and generating adversarial attacks (compared to PGD attacks) and show that their approach compares favorable across a number of architectures on MNIST and CIFAR-10.
The key insight is that instead of training to directly predict x, the paper proposes to predict different piecewise constant projections of x from x_init , with one CNN trained for each projection, each projection space defined from a random delaunay triangulation, with the hope that learning prediction for each projection is more sample efficient. One hypothesis is that the different learned CNNs that each predict a piecewise projection are implicitly yielding an ensembling effect, and therefore a more fair baseline to compare would be a 'direct-ensemble' where many different (number = number of projections) direct CNNs (with different seeds etc.) The problem of reconstruction from these piecewise constant projections is of independent interest. The projections correspond to computing a random Delaunay triangulation over the image domain and then computing pixel averages within each triangle. 3. estimate x from the m different projections by solving "reformuated" inverse problem using TV regularization.
The method of the authors assumes that a goal-conditioned policy is already learned, and they use a Kullback-Leibler-based distance between policies conditioned by these two states as the loss that the representation learning algorithm should minimize. For example, why is this particular metric used to link the feature representation to policy similarity? - Main missing details is about how the goal reaching policy is trained. The first weakness of the approach is that it assumes that a learned goal-conditioned policy is already available, and that the representation extracted from it can only be useful for learning "downstream tasks" in a second step. In that respect, wouldn't it be possible to *simultaneously* learn a goal-conditioned policy and the representation it is based on? But learning the goal-conditioned policy from the raw input representation in the first place might be the most difficult task. To me, the authors have established their framework thinking of the case where the state space and the goal space are identical (as they can condition the goal-conditioned policy by any state=goal). More precisely, it wishes to learn a representation so that two states are similar if the policies leading to them are similar. This is partly suggested when the authors mention that the representation could be learned from only a partial goal-conditioned policy, but this idea definitely needs to be investigated further. It is not clear if the end results is the same, meaning you just learn faster, or does the state reach a better final policy. But there are also several weaknesses: - for all experiments, the way to obtain a goal-conditioned policy in the first place is not described. In this paper, the authors propose a new approach to representation learning in the context of reinforcement learning. The paper presents a method to learn representations where proximity in euclidean distance represents states that are achieved by similar policies. 1. Generate data D from the environment (using an arbitrary policy). The authors should describe the oracle in more details and discuss why it does not provide a "perfect" representation. But when looking at the framework, this is close to what the authors do in practice: they use a distance between two *goal*-conditioned policies, not *state*-conditioned policies.
The interpolation model composes two components -- given these conditions, it first regresses weights combining a set of precomputed deformation fields, and then a second model regresses dense volumetric deformation corrections -- these are helpful as some events are not easily modeled with a set of basis deformations. Right now the implicit surface deformation model is only tested on liquids examples, which limits the impact to that specialist domain -- it's a bit more of a SIGGRAPH type of paper than *CONF*. 1. The primary novelty here is in the problem formulation (e.g., defining cost function etc.) where two networks are used, one for learning appropriate deformation parameters and the other to generate the actual liquid shapes. The specific way deformations are composed -- using v_inv to backwards correct basis deformations, following up the mixing of those with a correction model -- is intuitive and is also something I see for the first time. The experimental results are sufficient for simulating liquids/smoke, except I would like to also see a comparison to using deformation field network only, without its predecessor. Another useful experiment would be to vary the number of bases and/or the resolution of the deformation correction network and see the effects. For example, when designing the first network, can we also design another neural network that applies the deformation backwards and enforce some consistency to improve the results? The first network utilizes a set of precomputed deformations, while the weights can be set to generate different output shapes. The precomputed deformations are applied in a recurrent manner. Also, many simulations use adaptive sampling (high-resolution near the surface and low-residual in the interior). The approach combines two networks for synthesizing 4D data that represents 3D physical simulations. Given densely registered 4D implicit surfaces (volumes over time) for a structured scene, a neural-network based model is used to interpolate simulations for novel scene conditions (e.g. position and size of dropped water ball). The synthesized simulations are not physically accurate, but with certain visual realism. 3. In terms of practical applications, to the best of my knowledge there are sophisticated physics-based and graphics based approaches that perform very fast fluid simulations. If that's added it will strengthen the paper. The second network is a variant of STN. If the AC decides to reject based on this fact I am ok with that as well.
Learning an input-dependent baseline function helps clear out the variance created by such perturbations in a way that does not bias the policy gradient estimate (the authors provide a theoretical proof of that fact). Is just the baseline input dependent or does the policy need to be input dependent as well? Strengths: o) The paper is well written o) The method is novel and simple while strongly reducing variance in Monte Carlo policy gradient estimates without inducing bias. Is the variance of the proposed update always smaller than that of the standard PG update when learning a policy that is unaware of z?
Please clarify. Page 8, How do the AE results and architecture fit into the EMG reconstruction "BMI" results? I.e. your BMI is neural data -> AE -> LSTM -> EMG? Are we describing using latent variables (AE approach) for BMI? The authors then address the problem of data drift in BMI and describe a number of domain adaptation algorithms from simple (CCA to more complex ADAN) to help ameliorate it.
On three different corpora (IWSLT, WMT, TED) with into-English translation from numbers of source languages ranging from 6 (WMT) to 44 (TED), this technique outperforms standard distillation for every language pair, and outperforms the individual models for most language pairs. +++++++++++++++++++ I have updated my rating after reading author's responses The authors apply knowledge distillation for many-to-one multilingual neural machine translation, first training separate models for each language pair.
This paper proposes a variational approach to Bayesian posterior inference in phylogenetic trees. This paper explores an approximate inference solution to the challenging problem of Bayesian inference of phylogenetic trees. Its leverages recently proposed subsplit Bayesian networks (SBNs) as a variational approximation over tree space and combines this with modern gradient estimators for VI.
Experiments on synthetic games show that SOS preserves the benefit of LOLA while avoiding its theoretically-predicted issues, and a more complex Gaussian mixture GAN experiment shows SOS is empirically competitive with other gradient adjustment methods. To address these limitation, the authors propose SOS, which interpolates between LA and LOLA, and dynamically chooses the interpolation coefficient p so that their adjusted gradient preserves LOLA's shaping ability only to the extent allowed by the constraint of moving in LA's direction. To alleviate this issue, the authors propose a new algorithm SOS, which can be seen as an interpolation between LOLA and LookAhead, characterized by a parameter p. However, since this example can be viewed as a fully cooperative game with joint loss L = 2xy, it does not support the broader statement that Nash is undesirable in all games. Casting the recently proposed LOLA gradient adjustment into a general matrix form, they diagnose an example where the shaping term in LOLA prevents convergence to SFP.
They extend results of Bartunov et al 2018 (which found that feedback alignment fails on particular architectures on ImageNet), demonstrating that sign-symmetry performs much better, and that preserving error signal in the final layer (but using FA or SS for the rest) also improves performance. To this end, though the authors claim that the conditions on which Bartunov et al tested are "somewhat restrictive", this logic can equally be flipped on its head: the conditions under which this paper tests sign-symmetry are not restrictive enough to productively move in the direction of assessing sign-symmetry's usefulness as a description of learning in the brain, and so the conclusion that the algorithm remains a viable option for describing learning in the brain is not sufficiently supported. Building off a study by Bartunov et al. that shows the deficiencies of some BP algorithms when scaled to difficult datasets, the authors evaluate a different algorithm, sign-symmetry, and conclude that there are indeed situations in which BP algorithms can scale. Second, the work does not sufficiently weigh the "degree" of implausibility of sign-symmetry compared to the other algorithms, and implicitly speaks of feedback alignment, target propagation, and sign-symmetry as equally realistic members of a class of BP algorithms. More concretely, experiments are run on biologically problematic architectures such as ResNet-18, often with backpropagation in the final layer (though admittedly this doesn't seem to be an important detail with sign-symmetry, for reasons explained below). In the submitted manuscript, the authors compare the performance of sign-symmetry and feedback alignment on ImageNet and MS COCO datasets using different network architectures, with the aim of testing biologically-plausible learning algorithms alternative to the more artificial backpropagation. In particular, they examine two methods for breaking the weight symmetry required in backpropagation: feedback alignment and sign-symmetry. First, the explicit writing and underlying tone of the paper reveal a misrepresentation of the scientific argument in Bartunov et al. The scientific question in Bartunov et al. is not a matter of whether BP algorithms can be useful in purely artificial settings, but rather whether they can say anything about the way in which the brain learns. On the other hand, I think the conclusions regarding the first question -- whether sign-symmetry can be useful in artificial settings -- are fine given the experiments.
This paper proposes learning a latent variable deep generative model over every randomly sampled subset of observed features. The goal of this paper is to use deep generative models for missing data imputation. The method is compared against 1) classical approaches in missing data imputation on UCI benchmarks; 2) image inpainting against recently proposed GANS for the similar task, as well as; 3) against universal marginalizer, which learns conditional densities using a feedforward / autoregressive architecture. Inference in this latent variable model is achieved through the use of an inference network which conditions on the set of "missing" (to the generative model) features. "Missing Value Imputation Based on Deep Generative Models." arXiv preprint arXiv:1808.01684 (2018). The mask determines which features are observed. The idea is to extend the conditional VAE framework such that the posterior is a function of an arbitrary subset of observed variables. The paper presents a model for learning conditional distribution when arbitrary partitioning the input to observed and masked parts. My concern about the experimental results on missing data imputation is that strong competition such as Gondra et al'17 and Yoon et al'18 that report better results on UCI than classical approaches are not included. Table 5 claims negative log-likelihood numbers on MNIST as low as 61 and 41 (I assume nats...). There are issues like awkward grammar, sloppy notation, and spelling mistakes (please run spell check!) throughout the manuscript.
However, the combination of bilevel optimization for hyperparameter tuning with approximation is interesting. The approach assumes a distribution on the hyperparameters, governed by a parameter, which is adapted during the course of the training to achieve a compromise between the flexibility of the best-response function and the quality of its local approximation around the current hyperparameters.
I can see that Table 1 gives an overview of the number of elements accessed during normalization, but I do think that a proper plot showing the accuracy versus the wall-clock time would be a better way of showing how your method compares in practice with BN or GN. 4. ImageNet Experiment: Do you use Ghost Batch Normalization in this experiments (i.e. calculating the BN statistics on each GPU separately)? - Doesn't scale (yet) to deeper architectures, which is precisely where small batch sizes become a problem for BN and thus where ENorm would be needed. 5. Computation Time performances: It is stated in the conclusion that "using ENorm during training adds a much smaller computational overhead than BN or GN …". While the authors present the method as an alternative to batch normalization, most of the reported results show a better performance for BN. - Quite incremental (close to Path-SGD / Weight Normalization), and missing actual comparison with Weight Normalization, which seems to be the direct competitor of ENorm (see detailed comments) Summary: This paper introduces equi-normalization (ENorm): a normalization technique that relies on the scaling invariance properties of the ReLU, similarly to Path-SGD. This certainly explains the poor performances of your baseline when you increase the number of layers in Figure 4, and probably explains why you need to add a BN layer at the end of your network to help with the training of the baselines. (-)  The batch size shown in Table 2 and 3 may be intended to show the batch-independent property of the proposed method, but BN is also doing well in those tables.
For strongly-monotone operators (a generalization of strongly-convex functions) extrapolation updates are shown to have linear convergence. This paper looks at solving optimization problems that arise in GANs, via a variational inequality perspective (VIP). On the whole this is a really nice paper, that shows how standard ideas from VIP can be useful for training GANs. I recommend acceptance Overall, the paper is well-written and of high quality, therefore I recommend acceptance. Two techniques that have been widely used to solve VIP problems are averaging and extragradient methods. Extra-gradient updates perform an "extra" or fake gradient step to get to a new point, and then kind of retracks back and perform a gradient step using the gradient step obtained from the "extra step". Given that, GAN formulations tend to be min-max style problems (though not necessarily 0 sum) the VIP perspective is very natural, though under-explored in machine learning. Furthermore, the authors show that using extrapolation and averaging under the assumption that the operator is monotonic, and using constant step size SGD the rates of convergence are better than the rates obtained using plain SGD with averaging but without extrapolation. After showing theoretical guarantees of these methods (linear convergence) the authors propose to combine them with existing techniques, and show in fact this leads to better results.
This paper is about issues that arise when applying Information Bottleneck (IB) concepts to machine learning, more precisely in deterministic supervised learning such as classification (deterministic in the sense that the target function to estimate is deterministic: it associates each example to one true label only, and not to a distribution over labels). This work analyses the information bottleneck (IB) method applied to the supervised learning of a deterministic rule Y=f(X). Such a deterministic relationship between outputs and inputs induces the problem that the the IB "information curve" (i.e. I(T;Y) as a function of I(X;T)) is piece-wise linear and, thus, no longer strictly concave, which is crucial for non-degenerate ("interesting") solutions. Another consequence of this degeneracy concerns the latent variable interpretation of the IB: if T is treated as a latent variable (as, for instance, in the "deep" IB models) then we have the conditional independence relation "Y independent of X given T", which simply makes no sense if Y is deterministic in X (there is, of course, a deeper underlying problem here: the IB problem is difficult in that it is difficult to define a geneative model with a faithful DAG...). - This work provides in depth clarification of the counter-intuitive behaviors of the IB method in the case to the learning of a deterministic rule. Thus, one has to be careful when defining the mutual information I(X,Y), which explains the problems with the IB information curve (which should asymptotically converge to I(X;Y) as I(X;T) gets large. In the experiments of the present paper, the results seem to suggest that the interesting intermediate representations (separation in 10 compact clusters of the MNIST classes) is actually easier to obtain (large range of \\beta) optimizing the IB Lagrangian rather than the proposed squared IB Lagrangian. 7) They use the IB method to train a neural net on MNIST, using the Kolchinsky estimate of the mutual informations. 2) They show that in the case of a deterministic rule, the information bottleneck curve has a simple shape, piecewise linear, and is not strictly concave. 5) They exhibit uninteresting representations (noisy versions of the output Y) that are on the IB curve. In practice, besides a few recent propositions (Kolchinsky et al., 2017; Alemi et al., 2016; Chalk et al., 2016) the IB Lagrangian is not a usual objective function for supervised learning. Questions: - Do the authors know of an application where the full probing of the IB curve would be necessary? - The fact that the entire IB curve is not explored point by point by the IB Lagrangian is not necessarily an issue for learning. 3) They show that the optimization of the IB Lagrangian for different \\beta does not lead to a point by point exploration of the IB curve.
The generative models are hierarchical, and these latent variables correspond to higher level goals in agent behavior. To understand the contribution of the shared macro-intent, how would an intermediate baseline model where a set of parameters are shared between agents and each agent also has an independent set of parameters perform? The approach extends VRNN to a hierarchical setup with high level coordination via a shared learned latent variable.
This construction motivates the formulation of the auto encoder through the definition of several cost terms promoting reconstruction, clustering, and consistency across latent mappings. The authors also suggest augmenting their setup with a model of cluster transition dynamics for time-series data, which seems to improve the clustering further, as well as providing an interpretable 2D visualisation of the system's dynamics. - Clustering of short-term time series, such as the clinical ones, is a challenging task. The authors conduct experiments on both static and time series data and validate that the method perform better than related methods in terms of clustering results as well as interpretability. Moreover, motivated by practical aspects, the model optimisation relies on computational strategies not completely supported from the theoretical point of view (such as the zeroing of the gradient in backpropagation, or the approximation of the clustering function to overcome non-differentiability). This work addresses the problem of learning latent embeddings of high-dimensional time series data. This paper deals with an interesting problem as learning an interpretable representation in time series data is important in areas such as health care and business.
Compared to the claimed baselines (Liimatainen et al. and human experts), the proposed architecture shows a much higher performance. Why is the AUC important in the biological application at hand? earlier we are told that the labels come from "a large battery of biotechnologies and approaches, such as microarrays, confocal microscopy, knowledge from literature, bioinformatics predictions and additional experimental evidence, such as western blots, or small interfering RNA knockdowns."
Stronger conditions are required on the sequence \\beta_t, along the lines discussed in the paragraph on Boltzmann exploration in Section 2.2 of Singh et al 2000. Theorem 4 does not imply efficient exploration, since it requires very strong conditions on the alphas, and note that the same proof applies to vanilla Q-learning, which we know does not explore well. Soundness: (1) The proof of Theorem 4 implicitly assumes that all states are visited infinitely often, which is not necessarily true with the given algorithm (if the policy used to select actions is the Boltzmann policy). I presume the title of this paper is a homage to the recent 'Boltzmann Exploration Done Right' paper, however, though the paper is cited, it is not discussed at all. In fact it can be shown that Boltzmann exploration that incorporates a particular annealing schedule (but no notion of uncertainty) can be forced to suffer essentially linear regret even in the simple bandit case (O(T^(1-eps)) for any eps > Similarly, if the main contribution is DBS, it would be interesting to have a more in-depth empirical analysis of the method -- how does performance (in Atari or otherwise) vary with the temperature schedule, how exploration is affected, etc.?
[clarity] This paper is basically well written though there are several grammatical errors (I guess the authors can fix them). [Summary] This paper proposes a Graph-Sequence-to-Sequence (GraphSeq2Seq) model to fuse the dependency graph among words into the traditional Seq2Seq framework. Cons: - Somewhat incremental, not clear how much method depends on quality of the dependency parser For example, the current top-line NMT systems uses subword unit for input and output sentences, but this paper doesn't. The paper thoroughly describes in series of experiments that demonstrate that the authors' proposed method outperforms several of the other NMT methods on a few translation tasks. This paper proposes a method for combining the Graph2Seq and Seq2Seq models into a unified model that captures the benefits of both.
The authors claim that zero confidence attacks pose a harder problem and hence mainly compare their experimental results to the CW attack. While one would indeed expect an overhead due to the binary search, it is not clear a priori how large this overhead needs to be to achieve a competitive zero confidence attack with PGD (especially with a tuned step size for PGD, see above). || from the beginning. This paper proposes an efficient zero-confidence attack algorithm, MARGINATTACK, which uses the modified Rosen's algorithm to optimize the same objective as CW attack. Method such as projected gradient descent fall into the "fixed perturbation" category, while MarginAttack and CW belong to the "zero confidence" category. However, from an optimization point of view, these two notions are clearly related and a fixed perturbation attack can be converted to a small perturbation / zero confidence attack via a binary search over the perturbation size. Minor comment: The theoretical proof depends on the convexity assumption, I would also suggest comparing the proposed attack with CW and other benchmarks on some simple models that satisfy the assumptions. But as pointed out above, it is possible to convert a fixed perturbation attack to a zero confidence attack via a binary search. The former finds the strongest attack within a given constrained set, while the latter finds the smallest perturbation that leads to a misclassification. Since MarginAttack also contains multiple hyperparameters (see Table 4), it would be interesting to see how the running time of MarginAttack compares to that of a tuned CW implementation without the binary search. - On top of Page 2, the authors claim that zero-confidence attacks are a more realistic attack setting.
Authors present a novel regularizer to impose graph structure upon hidden layers of a neural Network. By adding edges one can impose a structure upon nodes in one layer and add for example a Laplacian regularizer rather than simple L2 norm regularizer to force the activations to follow the imposed structure. Authors highlight the contribution of graph spectral regularizer to the interpretability of neural networks. Furthermore, by extending the graph Fourier transformation to overcomplete dictionary representation, authors further propose a spectral bottleneck regularizer. The key points are 1) How to define the Laplacian graph for the neurons? Cons: The major flaw is the lack of comparison with ``any'' of the related work on interpretability or the prior work on imposing structure upon hidden representations. Authors propose a class of graph spectral regularizers and their performance is different in different tasks. Experimental results show that when suitable structural information and corresponding regularizers are imposed, the interpretability of the intermediate layers is improved.
This paper argues that a random orthogonal output vector encoding is more robust to adversarial attacks than the ubiquitous softmax. These show a lower correlation in input gradients between models when using the proposed RO encoding. -Another baseline/sanity test that should probably included is how does the 1-of-K softmax/cross-entropy compare with the proposed method where encoding length l = k and the C_{RO} codebook is just the identity matrix? I did not find the discussion around Figure 1 to be very compelling, since it is only relevant to the encoding layer, while we are only interested in gradients at the input layer.
They present the distributions and gradients, discuss appropriate activation functions for the output layer, and evaluate this approach on synthetic and real datasets with mixed results. If I were to rewrite this paper, I would focus on answering the question "What are the unique challenges of parameterizing Dirichlet, Dirichlet-multinomial, and Beta distributions with the outputs of a neural network and how can we address them?", replacing section 2 with an expanded section 3. As I see it, the paper does three main things: 1. In section 2, the authors consider parameterizing Dirichlet, Dirichlet-multinomial, and Beta distributions with the outputs of a neural network (Section 2). This paper considers parameterizing Dirichlet, Dirichlet-multinomial, and Beta distributions with the outputs of a neural network. The likelihoods discussed include Beta, Dirichlet and Dirichlet-Multinomial. The authors briefly argue that the proposed methods are superior because they provide uncertainty estimates for the output distributions. Most of section 2 is dedicated to writing down, simplifying, and deriving gradient equations for these three distributions.
Computing the gradients of Wasserstein on batches might be seen a kind of regularization, but it remains to be proved and discussed. The authors propose to estimate and minimize the empirical Wasserstein distance between batches of samples of real and fake data, then calculate a (sub) gradient of it with respect to the generator's parameters and use it to train generative models. Typos: Eq (1) and (2): when taken over the set of all Lipschitz-1 functions, the max should be a sup The paper proposed to use the exact empirical Wasserstein distance to supervise the training of generative model. The bias of the empirical Wasserstein estimate requires an exponential number of samples as the number of dimensions increases to reach a certain amount of error [2-6]. [6]: https://www.sciencedirect.com/science/article/pii/0377042794900337 The paper 'Generative model based on minimizing exact empirical Wasserstein distance' proposes a variant of Wasserstein GAN based on a primal version of the Wasserstein loss rather than the relying on the classical Kantorovich-Rubinstein duality as first proposed by Arjovsky in the GAN context.
One of the biggest issues of this paper is that they use CTC as an acoustic model, while still many real speech recognition applications and major open source (Kaldi) use hybrid HMM/DNN(TDNN, LSTM, CNN, etc.) systems. This paper discusses applications of variants of RNNs and Gated CNN to acoustic modeling in embedded speech recognition systems, and the main focus of the paper is computational (memory) efficiency when we deploy the system. This analysis is actually valuable, and this suggested change about the position of this TIMIT experiment can avoid some confusion of the main target of this paper.) This paper investigates a number of techniques and neural network architectures for embedded acoustic modeling. My suggestion is to place these TIMIT based experiments as a preliminary experiment to investigate the variants of RNN or gated CNN before the WSJ experiments. Other comments: - in Abstract and the first part of Introduction: as I mentioned above, CTC based character-prediction modeling is not a major acoustic model. The findings presented in this paper are interesting and quite useful when one wants to implement a LSTM-based acoustic model on mobile devices.
[Overall] It's great that NTP was scaled up to handle larger datasets, however further analysis is needed. This paper propose an extension of the Neural Theorem Provers (NTP) system that addresses the main issues of this method. This is the most elaborated section out of the three, yet seems like the most trivial -- unless the authors can provide an analytical bound on the loss in ntp score w.r.t the neighborhood size. 3) Section 2 on the NTP framework is not very helpful for a reader that has not read the previous paper on NTP (in particular, the part on training and rule learning). The contributions of this paper allow to use this model on real-word datasets by reducing the time and space complexity of the NTP model. - Utilizing text is an interesting direction for NTP in terms of integrating it with past work on KG completion. [Pros] - Reasonable and interesting increments on top of NTP. NTP systems by combining the advantages of neural models and symbolic reasoning are a promising research direction. - No ablation study is performed so the effect of incorporating mentions and attention are unclear. The attention mechanism (essentially reducing the model capacity) is also well-known but its effect in this particular framework is not properly elaborated. Cons: I'm not convinced by the model used to integrate textual mentions.
2) Chen et al (ACL 2017) have shown that pretraining QA model on span supervision dataset (SQuAD) is effective to train the model on distant supervision dataset. 1) First of all, it is hard to say there is a contribution to the idea of sentence discriminator and sentence reader — people have used this framework for large-scale QA a lot.
By using these networks within a CFR framework, the authors manage to avoid huge memory requirements traditionally needed to save cumulative regret and average strategy values for all information sets across many iterations. Typo:  "care algorithm design" -> "careful algorithm design" The paper proposes a neural net implementation of counterfactual regret minimization where 2 networks are learnt, one for estimating the cumulative regret (used to derive the immediate policy) and the other one for estimating a cumulative mixture policy. Here, maybe a way to alleviate this problem would be to generate negative samples (where the network would be trained to predict low cumulative values) by following a different (possibly more exploratory) policy. I did not find many flaws to point out, except I believe the paper could benefit from more extensive  comparisons in Figure 4A against other IIG methods such as Deep Stack, as well as comparing on much larger IIG settings with many more states to see how the neural CFR methods hold up in the regime where they are most needed. In Eq. 8, they minimize the loss of CF value predictions *over the distribution of infosets in the last CFR step ("batch")*.
Contribution: - Using a known parameters crystallography simulator (X-ray beam, structure being analyzed, environment (crystalline or not)) built a dataset (called DiffraNet) of 25,000 512x512 grayscale labeled images of resulting diffraction images of various materials/structures (crystalline or not) . This paper introduces a purely synthetic dataset for crystallography diffraction patterns. - The images are classified according to the diffraction patterns they encompass into one of 5 classes: blank, no-crystal, weak, good and strong. A few questions though: • In what way the diffraction images are 'synthetic'?
This paper performs an analysis of the cell-state updating scheme of LSTM and realizes that it is mainly controlled by the forget gate.
As I understand (please correct me if I am wrong), the method can not work for contextual instructions where the goal depends on the environment and the same instruction can map to different goals, such as 'Go to the largest/farthest object'. It is claimed that the natural language instruction following approaches described in the first paragraph "require a large amount of human supervision" in the form of action sequences. This paper contains an important core insight---much of what's hard about instruction following is generic planning behavior that doesn't depend on the semantics of instructions, and pre-learning this behavior makes it possible to use natural language supervision more effectively. - Experimental results are not convincing: - The introduction motivates the need for understanding human instructions and the abstract says 'Given a human instruction', but I believe experiments do not have any human instructions. This submission proposes a method for learning to follow instructions by splitting the policy into two stages: human instructions to robot-interpretable goals and goals to actions.
This paper presents a thorough and systematic study of the effect of pre-training over various NLP tasks on the GLUE multi-task learning evaluation suite, including an examination of the effect of language model-based pre-training using ELMo. The work presented in this paper relates to the impact of the dataset on the performance of contextual embedding (namely ELMO in this paper) on many downstream tasks, including GLUE tasks, but also alternative NLP tasks. Contextualized word representations have gained a lot of interest in recent years and the NLP and ML community could benefit from such detailed comparison of such methods. Only a handful of NLP tasks have an ample amount of labeled data to get state-of-the-art results without using any form of transfer learning. Minor details: Page 1: "can yield very strong performance on NLP tasks" is a very busy way to express the fact that Sentence Encoders work well in practice. Training sentence representation in an unsupervised manner is hence crucial for real-world NLP applications. The main conclusion is that both single-task and LM-based pre-training helps in most situations, but the gain is often not large, and not consistent across all GLUE tasks. Extensive hyper-parameter tuning can make a substantial different when dealing with NN models, maybe the authors should have considered dropping some of the tasks (the article has more than enough IMHO) and focus on a smaller sub set of tasks with proper hyper-parameter tuning. The paper is clearly written. I do understand the computational limitations of the authors (as they mention on HYPERPARAMETER TUINING) and I do agree with their statement " The choice not to tune limits our ability to diagnose the causes of poor performance when it occurs". Finally, the analysis is mostly descriptive and there is few insight by the author about what should be the future work, apart from "we need a better understanding".
The submission proposes to increase the variety of generated samples from GANs by a) using an ensemble of discriminators, and b) tasking them with distinguishing not only fake from real samples, but also their fake samples from the fake samples given to the respective other discriminators. Does the discriminator make use of batch level statistics in some more advanced way beyond just batchnorm such as the minibatch features in Improved GAN (Salimans 2016)? The authors propose a method to improve sample diversity of GANs. They introduce multiple discriminators, each aims to not only compare real and fake examples but also compare different "micro-batch" of examples. As a sanity check - given a fixed generator - if you continue to train the discriminators on randomly drawn samples from this generator distribution does the microbatch discrimination objective continue to make progress and converge to a minimum? Specifically, it proposes to split a minibatch of samples into further smaller minibatches (microbatches) and train different discriminators on each.
Pros: (1) The authors propose a sensible approach, which is also novel to be best of our knowledge, using SVM to select support data from old data to be fed to the network along with the new data in the incremental learning framework to avoid catastrophic forgetting. SupportNet uses resnet network with 32 layers, trains an SVM on the last layer and the support vector points from this SVM are given to the network along with the new data. This paper presents a hybrid concept of deep neural network and support vector machine (SVM) for preventing catastrophic forgetting. (3) The individual impact of the support points and the joint impact of support points with feature regularizer on accuracy is not assessed. Additionally, they offer a feature regularizer that penalizes the network for changing the feature representation of the support data when training the network on new data and an EWC regularizer that constrains the parameters that are crucial for the classification of the old data and makes it harder to change them. The authors consider the last layer and the softmax function as SVM, and obtain support vectors, which are used as important samples of the old dataset.
The agent is given an additional shaping reward that penalizes it for violating these constraints. Adding a shaping reward for some desired behavior of an agent is straightforward. What is the benefit of only biasing it to avoid violating those constraints with a shaping reward? This work aims to use formal languages to add a reward shaping signal in the form of a penalty on the system when constraints are violated.
Likewise for Tables 3-6. 3. Under this interpretation of the tables---again, correct me if I'm wrong---the proper comparison would be "IEA (ours)" versus "Ensemble of models using CNL", or  "(m=3, k=1)" versus "(m=1, k=3)" in my notation. So, instead of "CNL" and "IEA (ours)" in Table 1 and "Ensemble of models using CNL" and "Ensemble of models using IEA (ours)" in Table 2, I would recommend a single table with these headings: "(m=1, k=1)",  "(m=3, k=1)",  "(m=1, k=3)",  and "(m=3, k=3)", corresponding to the columns in Tables 1 and 2 in order. I would also significantly reduce the claims of novelty, such as "We introduce the usage of such methods, specifically ensemble average inside Convolutional Neural Networks (CNNs) architectures." in the abstract, given that this is the exact idea explored in other work including followups to Maxout. Given the close similarity of this work to Maxout and others, a much stronger indication of the benefits and improvements of IEA seems necessary to prove out the concepts here. A discussion of this comparison in greater detail, or even derivation of IEA as a special setting or extension of ResNet (coupled with stronger performance on the datasets) would help ground the work in prior publication. This work proposes an ensemble method for convolutional neural networks wherein each convolutional layer is replicated m times and the resulting activations are averaged layerwise. Is the performance boost greater than simply using an ensemble of m networks directly (resulting in the equivalent number of parameters overall)? There seems to be a similarity between ResNet and this method - specifically assuming the residual pathway is convolution with an identity activation, the summation that combines the two pathways bears a similarity to IEA.
It would also be really useful to see some concrete input / output values in discrete architecture space. This paper deals with Architecture Compression, where the authors seem to learn a mapping from a discrete architecture space which includes various 1D convnets. Specifically, compared to those methods, the search space for the proposed paper is larger because although the number of layers is fixed, the connections between layers give more freedom to the compression algorithm. By jointly training all these networks, the authors are now able to compress a given network by mapping it's discrete architecture into the latent space, then performing gradient descent towards higher accuracy and lower parameter count (according to the learned regressors). The continuity of the architecture characteristics can help architecture search tasks. Most of the Architecture Search works cited do have a search space which allows the more recent advances. The search space is not clearly defined.
Summary: Proposes a framework for performing adversarial attacks on an NMT system in which perturbations to a source sentence aim to preserve its meaning, on the theory that an existing reference translation will remain valid if this is done. They then investigate three different ways of generating adversarial examples and show that a metric based on character n-gram overlap (chrF) has a stronger correlation with human judgment. This is particularly obvious in the case of using character ngram distance (chrF) to determine which character swaps preserve meaning best. It is well structured, the problem studied is highly interesting and the proposed meaning-preserving criteria and human judgement will be useful to anyone interested in adversarial attacks for natural language. Next, standard gradient-based adversarial attacks are carried out, replacing the three tokens that result in the biggest drop in (approximate) reference probability, either 1) with no constraints, 2) constrained to character swaps of the original token, or 3) constrained be among the 10 closest embeddings to the original token.
This model is used for classification on ImageNet-1k, and for zero-shot classification on ImageNet-21k where a model must predict superclasses seen during training for images of leaf categories not seen during training. Nonetheless, different architectures of neural networks are tested on ImageNet and validate the fact that the soft probability strategy improves performance on the zero-shot learning task. Instead of directly optimizing the standard cross-entropy loss, the paper considers some soft probability scores that consider some class graph taxonomy. Rather than following the existing experimental protocol for evaluating zero-shot learning from [Frome et al, 2013] and [Norouzi et al, 2013] the authors evaluate zero-shot learning by plotting SG-hit vs SG-specificity; while these are reasonable metrics, they make it difficult to compare with prior work. The proposed method instead constructs a target distribution which places probability mass not only on leaf category nodes but also on their neighbors in a known semantic hierarchy of labels, then penalizes the KL-divergence between a model's predicted distribution and this target distribution.
Summary: The paper proposes an approach for improving standard techniques for model compression, i.e. compressing a big model (teacher) in a smaller and more computationally efficient one (student), using data generated by a conditional GAN (cGAN). This score evaluates the quality of generated data by using it in model compression: "good" synthetic data results in a smaller gap in performance between student and teacher models. Instead, the paper proposes learning a conditional GAN, which can potentially generate large amounts of realistic synthetic data, and use this data (in addition to original training data) for model compression. The paper claims that the best compression score is 1 (training student model on real data), while the paper shows that in fact, good synthetic data should produce *better* accuracy than using real data. I would expect following experiments: use mixture of training and GAN data to train teacher and student network by standard supervised loss without knowledge distillation, and compare with values in table 1.
This papers uses the label hierarchy to drive the search process over a set of labels using reinforcement learning. Compared to other structured classification approaches whose scope is limited by the complexity of the inference process, this approaches is very attractive. [4] Recursive regularization for large-scale classification with hierarchical and graphical dependencies, https://dl.acm.org/citation.cfm?id=2487644 This work proposes an RL approach for hierarchical text classification by learning to navigating the hierarchy given a document. While those tricks may have been used for other applications, they seem new in the context of hierarchical/multi-label/structured classification. For the scale of datasets discussed, where SVM based methods seem to be working well, it is possible that approaches [1,2] which can exploit label correlations can do even better. The paper proposes a label assignment policy for determining the appropropriate positioning of a document in a hierarchy.
While the overall concept of graph regularization is appealing, the exact relationship between the proposed regularization and robustness to adversarial examples is unclear. To improve the robustness of neural networks under various conditions, this paper proposes a new regularizer defined on the graph of the training examples, which penalizes the large similarities between representations belonging to different classes, thus increase the stability of the transformations defined by each layer of the network. The main contribution of the article is to apply concepts of graph regularization to the robustness of neural networks. 1. It is still not clear why would this regularization help robustness especially when considering adversarial examples. 2. On the other hand, while the proposed regularizer can be interpreted in a perspective of the Laplacian of the similarity graph, the third part in Equation (4), that expresses the smoothness as the sum of similarities between different classes, seems more intuitive to me. On the other hand, for adversarial robustness, the authors should have compared to the method of adversarial training as well.
Somehow through this wrong factorization and some probabilistic jugglery, we arrive at section 3 where the takeaway from section 2 is the rather known one that the model promotes universal replies regardless of query. A lot of typos/wrong phrasing/wrong claims and here are some of them: (a) Page 1, "lead to the misrecognition of those common replies as grammatically corrected patterns"? There are many assumptions in lemma 3 that are quite difficult to unpack to verify the correctness e.g. can the most frequent words not occur at all in "non-universal" replies? For example, for lemma 3, M is supposed to be some universal constant defined to be the frequency of universal replies while all other replies seem to have a frequency of 1.
The paper provides a concise specification of the query language (a mixture of logical and numeric operators) and asserts a theorem that the given procedure for constructing the query loss produces a function such that anytime the function is 0, the constraints are satisfied. In this paper the authors propose DL2 a system for training and querying neural networks with logical constraints The primary technical challenge is the non-convex optimization required to search for a solution to a query. Weaknesses ----------- The statement in Theorem 1 regarding the converse case is unclear, because it says that the limit of \\delta as \\epsilon approaches zero is zero, but it is not explained what \\epsilon is or how it changes. If other cases exist, it is unclear how Theorem 1 applies. PSL by construction produces convex loss functions, and so the constraint that all outputs for a group of classes is either high OR low would probably not work well.
This paper considers a resizable mini-batch gradient descent (RMGD) algorithm based on a multi-armed bandit for achieving best performance in grid search by selecting an appropriate batch size at each epoch with a probability defined as a function of its previous success/failure. The idea of viewing the choice of hyperparameters in a learning algorithm as a bandit problem is known and has been explored in different contexts, although the specific application to minibatch size is new as far as I know. The paper applies multi-armed bandits for choosing the size of the minibatch to be used in each training epoch of a standard CNN. Let y(w,b) be 0 if training one epoch starting at w with batch size b would improve the validation error, and 1 otherwise. A validation error is then computed, and if it is lower than that of the last epoch, the cost of the minibatch is taken to be zero (otherwise one), and the distribution is updated. The results show that the bandit approach allows to obtain a test error better (although not significantly better) than the test error corresponding to the best minibatch size among those considered by the bandit.
This paper extends the "infinitely differentiable Monte Carlo gradient estimator" (or DiCE) with a better control variate baseline for reducing the variance of the second order gradient estimates. This paper extends the recently published DiCE estimator for gradients of SCGs and proposed a control variate method for the second order gradient. In this paper, the author proposed a better control variate formula for second-order Monte Carlo gradient estimators, based on a special version of DiCE (Foerster et al, 2018). (11). In particular, it would be nice to show the variance of the two terms separately (for both DiCE and this paper), to show that the reduction in variance is isolated to the second term (I get that this must be the case, given the math, but would be nice to see some verification of this). Overview: This nicely written paper contributes a useful variance reduction baseline to make the recent formalism of the DiCE estimator more practical in application.
Given the periodic drops in performance for the backplay policies, it appears that the initial states might be changing according the curriculum during evaluation. That being said, sampling initial states from demonstrations is a tried-and-true strategy in RL, and the manually designed curriculum is also not particularly novel. Thanks for your submission. The  authors present a very elegant strategy of using Backplay, that learns a curriculum around a suboptimal demonstration. The method, Backplay, starts by sampling states near the end of a demonstration trajectory, so that the agent will be initialized to states near the goal. I'm slightly confused by the plots and the significant drops in performance once the curriculum is finished and agent encounters the start position of the demonstration trajectory (epoch 250). This paper presents a method for increasing the efficiency of sparse reward RL methods through a backward curriculum on expert demonstrations. 2) "Reverse Curriculum Generation for Reinforcement Learning" by Florensa et al. (CoRL 2017) , where they start the training to reach a goal from start states nearby a given goal state and gradually the agent is trained to solve the task from increasingly distant start states.
The paper concludes that deep RL is effective at learning agents for cooperative games in multiple ways: 1) Deep agents are better than a hand-designed agent. 3) A popular result in cooperative game theory predicts how effective agents should be. 2) Deep agents easily extend across negotiation protocols (something hand-designed agents don't do). In a weighted voting game each agent is given a weight and the agents attempt to form teams. Summary --- This paper studies deep multi-agent RL in settings where all of the agents must cooperate to accomplish a task (e.g., search and rescue, multi-player video games). It uses simple cooperative weighted voting games 1) to study the efficacy of deep RL in theoretically hard environments and 2) to compare solutions found by deep RL to a fair solution concept known in the literature on cooperative game theory.
Summary: the paper proves the convergence of empirical length map (length process) in NN to the length map for a permissible activation functions in a wide-network limit. When the activation function is permissible and the weights of DNN are generated from the Gaussian distribution,
(i) the benefits of pooling in terms of deformation stability can be achieved through supervised learning the filters instead (sec This paper tries to argue that pooling is unnecessary for deformation invariance, as title suggests, and proposes initialization based on smooth filters as an alternative. This paper tries to argue that pooling is unnecessary for deformation invariance, as title suggests, and proposes initialization based on smooth filters as an alternative. It is often argued that one of the roles of pooling is to increase the stability of neural networks to deformations. It shows different pooling strategies reach to similar levels of deformation stability after sufficient training. It shows different pooling strategies reach to similar levels of deformation stability after sufficient training.
This work proposes a hybrid VAE-based model (combined with an adversarial or maximum mean discrepancy (MMD) based loss) to perform timbre transfer on recordings of musical instruments. The method is compared against the UNIT model under a variety of training conditions, and evaluated for within-domain reconstruction and transfer accuracy as measured by maximum mean discrepancy. The transfer metrics (MMD and kNN) are opaque to the reader: for instance, in table 1, is a knn score of 43173 qualitatively different than 43180? * Also in the introduction, it is implied that style transfer constitutes an advance in generative models, but style transfer does not make use of / does not equate to any generative model. 3 The proposed method can transfer the positive knowledge. The authors proposed a Modulated Variational auto-Encoders (MoVE) to perform musical timbre transfer. I appreciated that the one-to-one transfer experiments are incremental comparisons, which provides valuable information about how much each idea contributes to the final performance. I think the main point of transfer over a regular generative model that goes from labels to audio is precisely that it can be done without label information. The proposed model seems to improve on the transfer accuracy, with a slight hit to reconstruction accuracy. Summary ------- This paper describes a model for musical timbre transfer which builds on recent developments in domain- and style transfer.
The paper deals with image to image (of faces) translation solving two main typical issues: 1) the style information comes from the entire region of a given exemplar, collecting information from the background too, without properly isolating the face area; 1) The paper says that "For example, in a person's facial image translation, if the exemplar image has two attributes, This paper proposes an unpaired image-to-image translation method which applies the co-segmentation network and adaptive instance normalization techniques to enable the manipulation on the local regions. * As the main experiments are about facial attributes translation, I strongly recommend to the author to compare their work with StarGAN [4]. Since the attribute information has been modeled by the network parameters, will different exemplar image lead to different translation outputs? Is the model easy to extend to novel styles for image translation? The paper propose to use co-segmentation to find the common areas to for image translation.
- mistakes in the text, bad wording, e.g. "Experiments demonstrate that our model outperforms the state-of-the-art one-class classification models and other anomaly detection methods on both normal data and anomalies accuracy, as well as the F1 score" (page 1, abstract) 3) the general idea (train GAN to generate samples in low-density regions of the original probability density; since a disrciminator is trained to classify such samples from real data, mainly belongning to high-density regions of the original probability density, then the discriminator can be used to detect anomalies) is nice. Comments 1) the authors wrote that they develop a multi-modal one-class generative adversarial network based detector to distinguish anomalies from normal data (products). To force the generator to produce samples in the low density areas of the data distribution, a Complementary GAN is used.
1) adam models keeps in memory x_t (the model parameter), g_t (the model gradient), m_t (momentum) \\hat v_t and v_{t-1} (the monotone and non monotone version of the second order moment estimation. Compared to the existing methods AMSGrad and PAdam, the new method Game tracks only two parameters in iteration and hence saves memory. Their experiment shows Game may produce better performance than AMSGrad and PAdam with a little bit sacrifice of convergence speed. Summary ------ The authors propose an adaptation of the Adam method, with the AMSGrad correction and an additional parameter to p to exponentiate the diagonal conditioning matrix V (Padam).
Additionally, many values are chosen for the experiments without motivation and without testing a variety of values (e.g., 30 for the size of the dataset used to calculate the reward, 1000 RL iterations, and others). It would be good to evaluate this featurisation with a supervised active learner (like LAL), in order to disambiguate whether the good performance comes from these feature choices, or from the recent RL algorithms used to optimise. Among the components of the actions are statistics related to the dataset---the average distance from the chosen point to all the labeled data, and the average distance from the chosen point to all the unlabeled data. The only differences are smallish design parameters like: slightly different reward function definition, use Q-learning instead of policy-gradient optimiser, and slightly different state featurisation.
On section 2.3 and the explosion of gradients: There is a mistake in the equation on page 4 regarding the "gradient with respect to the learning rate". The paper also presents issues with the standard "learning to learn" optimizers, one being the short-horizon bias and as credited by the authors has been observed before in the literature, and the second one is what is termed the "exponential explosion of gradients" which I think lacks enough justification as currently presented (see below for details).
Pros: - The proposed method is shown to work with existing methods like weight pruning, low-rank compression and quantization. Unlike the existing work, weight compression is applied as a form of weight distortion, i.e. the model has the full degree of freedom during fine-tuning (to recover potential compression errors). Specifically, the proposed solution is to repeatedly apply weight compression and fine-tuning over the entire training process. They used different model compression techniques in this framework to show the effectiveness of the proposed method. Overall, I think the novelty of the paper is very limited, as all the weight distortion algorithms in the paper can be formulated as the proximal function in proximal gradient descent. The paper does not really propose a new way of compressing the model weights, but rather a way of applying existing weight compression techniques. Therefore, it is not clear how the proposed framework is helping the model compression techniques. A model compression framework, DeepTwist, was proposed which makes the weights zero if they are small in magnitude.
arXiv preprint arXiv:1611.03530. This paper proposes Permutation Phase Defense (PPD), a novel image hiding method to resist adversarial attacks. While the security of a model against adversarial attacks is important, a defense should not sacrifice clean accuracy to such an extent. The experiments with regards to robustness to adversarial attacks I find convincing, however the overall performance is not very good (such as the accuracy on Cifar10). This paper explores the idea of utilizing a secret random permutation in the Fourier phase domain to defense against adversarial examples. The paper demonstrated the method on MNIST and CIFAR10, and evaluates it against a number of adversarial attacks. In summary the authors propose a  simple and intuitive method to improve the defense on adversarial attacks by combining random permutations and using a 2d DFT. The authors should introduce an appropriate threat model and evaluate this defense against plausible attacks under that threat model. 2. The authors state "We believe that better results on clean images automatically translate to better results on adversarial examples" The permuted phase component does not admit weight sharing and invariances exploited by convolutional networks, which results in severely hindered clean accuracy -- only 96% on MNIST and 45% on CIFAR-10 for a single model.
Strengths: + This paper is interesting in the sense that it empirically shows that using regularization in training deep RL can be helpful when the goal is the generalization from one flavour of an environment to another one but very similar to the original. Specifically, they showed that when features (parameters of DQN) are trained in one environment (default flavour/mode) and then used as an initialization for the same model but for a slightly different environment ( i.e. still captures key concepts of the original environment ) can boost the performance of the model in the new flavour/environment. - According to the paper (at least my understanding) DQN's hyper-parameters are tuned based on default mode/flavour environment. More importantly, the performance boost is significant when DQN's parameters which are used to initialize the model for the new environment were trained using dropout and L2 regularization in the default flavour/mode. This is an empirical study on the ability for DQNs trained with/without regularization to perform well on variants of the same environment (e.g. increasing difficulty of a game).
The paper proposes a sampling-based method that aims at accelerating Batch Normalization (BN) during training of a neural network. - In normal BN, the gradient is propagated through the normalization factor as well, how would that change in the case of subsampled BN? Because random sampling is involved, this will result in less regular patterns of computation, this could likely make the implementation of BN to be less efficient. Clarity: I have not been able to fully understand why the proposed (uniform) sampling variant of BN is better than previous effort at making BN less computationally expensive in a GPU-based training environment by reading the paper: 1. The authors argue that the "summation" operation is the one that makes BN expensive; however, the authors have not demonstrated enough evidence of this argument 2. If "summation" operation is what makes BN expensive, then in a GPU-based environment, can we simply divide the data into smaller batch, and train on each GPU using a smaller batch (this is way, each GPU is essentially calculating the statistics based on a sub-sample) 3. The authors discussed Micro-BN, which "alleviate the diminishing of BN's effectiveness when the amount of data in each GPU node is too small..." This seems to show that in practice, training with BN does not suffer from having a large batch, but instead suffers from having too small batch size on each GPU node.
This work proposed using temporal logic formulas to augment RL learning via the composition of previously learned skills. The experiments results show that the composition method does better than soft Q-learning on composing learned policies, but how it performed compared to earlier hierarchical reinforcement learning algorithms? The topic of the composition of skills is interesting.
This paper proposes Deep Overlapping Community detection model (DOC), a graph convolutional network (GCN) based community detection algorithm for network data. In addition to the limited technical novelty, I have a few other concerns as well, including some on the experimental evaluation: - Real-valued node embeddings obtained from shallow/deep graph embedding methods can be used with *overlapping* versions of k-means. - There has been some recent work on using deep generative models for overlapping community detection with node side information.
Summary: The authors propose IMV-LSTM, which can handle multi-variate time series data in a manner that enables accurate forecasting, and interpretation (importance of variables across time, and importance of each variable). 2565-2573). ACM. This paper describes a recurrent model (LSTM specifically, but generalizable) which can produce variable-wise hidden states that can be further used for two types of attentions: 1) variable importance for the importance of each variable (not accounting for time), and The authors use one LSTM per variable, and propose two implementations: IMV-Full explicitly tries to capture the interaction between the variables before mixing the LSTM hidden layers with attention.
Empirically, when used in conjunction with prior updating and/or hierarchical latent variables, the proposed "stochastic generation" approach improves upon the baselines, but not when used in isolation. The highlighted contribution is a "stochastic generation" training procedure in which the training objective evaluates the reconstruction of output sequence elements from individual latent variables independently. - In Table 4, the difference between line 5 and line 6 is interesting and I wish it was discussed more, maybe used in the visualization experiment to show how/why "stochastic generation" with a larger window improves performance.
- compute the max over all pairs of these features (thus obtaining 2k values); The design of the predictor is: - for each pair, compute 2k features (of the form ReLu(linear combination of the values, without bias)); Minor Comments: 1. The statement of Theorem 4.1 itself does not show the advantage of over-parameterization because optimization is not discussed. The considered CNN is a basic one: only the weights of one layer is trained (others are fixed), and the only non-linearities are max-pooling and RELU (one can remove these two max-based operators with one appropriately defined max operator).
This seems unlikely, and it seems more likely that given the number of conversations ~30 per participant is similar to the number of emotion words that you asked each worker to cycle through nearly all of the emotions or that given they were able to select, they might describe the same emotion, e.g. "fear" several times. The paper in particular is contributing its collected set of 25k empathetic dialogs, short semi-staged conversations around a particular seeded emotion and the results of various ways of incorporating this training set into a generative chatbot. In section 4, Empathetic dialog generator, you state that the dialog model has access to the situation description given by the speaker (also later called the situational prompt) but not the emotion word prompt. I also have some doubts about the two claimed contributions of the paper (the authors actually list 3 contributions in the introduction, but for convenience I lump the 2 non-data ones together): (1) Dataset: The dataset was crowdsourced by giving workers an emotion label (e.g., afraid) and asking them to define a situation in which that emotion might occur and inviting them to have a conversation on that situation.
The main contribution is in replacing the heuristic to find good quantization intervals of (Zhu et al, 2016) with a different heuristic based on a hierarchical clustering algorithm, and empirically validating its effectiveness. The main idea is to use 'nest' clustering for weight quantization, more specifically, it partitions the weight values by recurring partitioning the weights by arithmetic means and negative of that of that weight clustering. This paper is about CNN model compression and inference acceleration using quantization. If that is the case, then many quantization algorithms can actually achieve better compression ratios with 2 bits quantization. The proposed setup is almost identical to the one in (Zhu et al, 2016), except for the replacement of the heuristic to find quantization intervals with another one. Experiments on both weight quantization and activation quantization are conducted and show effectiveness. 2. It is not clear how the weight and activation quantization are addressed together.
2. Actually, as the authors mentioned, GAN is not an appropriate model for image restoration when  accurate image completion is required. Please see the following comments: 1. Adding an task-discriminator in a GAN network seems straightforward to improve the specific task. It shows superior performance than the baseline methods without such task-discriminator on medical image restoration and image super-resolution. This paper proposed a new method for image restoration based a task-discriminator in addition to the GAN network. For medical image reconstruction and image super-resolution, the proposed method was not compared with any of the state-of-the-art methods, but only with the same method without a task-discriminator as a baseline. In this paper, the authors propose a novel method of Task-GAN of image coupling by coupling GAN and a task-specific network, which alleviates  to  avoid hallucination or mode collapse. For example, a simple L1/L2 or perceptual loss probably leads to better PSNR than the GAN loss, which is not compared at all. and Mu Lee, K., Accurate image super-resolution using very deep convolutional networks. and Shi, W., Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network.
The paper presents a convergence analysis for manifold gradient descent in complete dictionary learning. The authors analyze the convergence performance of Riemannian gradient descent based algorithm for the dictionary learning problem with orthogonal dictionary and sparse factors.
Stochastic EM is used for end-to-end learning, an algorithm that is L times more expensive than MAML, where L is the number of mixture components. Summary: This work tackles few-shot (or meta) learning, providing an extension of the gradient-based MAML method to using a mixture over global hyperparameters. - Results on miniImagenet are not encouraging; the gains on MAML are small and similar methods that generalize MAML (Kim et al., 2018, Rusu et al., 2018) achieve significantly better performance. This type of effort is needed to motivate an extension of MAML which makes everything quite a bit more expensive, and lacks behind the state-of-art, which uses amortized inference networks (Versa, neural processes) rather than gradient-based. This paper proposes a mixture of MAMLs (Finn et al., 2017) by exploiting the interpretation of MAML as a hierarchical Bayesian model (Grant et al. 2018). These results are not near the state-of-the art anymore, and some of the state-of-art methods are simpler and faster than even MAML. The proposed method is tested in a few-shot learning setup on miniImagenet, on a synthetic continual learning problem, and an evolutionary version of miniImagenet. While the idea of task clustering is potentially useful, and may be important in practical use cases, I feel the proposed method is simply just too expensive to run in order to justify mild gains. Ultimately, it performs on par with MAML, despite having three times the capacity. State of the art results on miniImageNet 5-way, 1-shot, the only experiments here which compare to others, show accuracies better than 53: - Versa: https://arxiv.org/abs/1805.09921. - Nonparametric extension via Dirichlet process mixture. 1. The performance of few-shot classification on MiniImageNet is not comparable to the state of the art (Table 2, Table 1).
It is unclear how the tradeoffs in optimizing against multiple discriminators stack-up against bigger GANs. From my perspective, the paper is interesting because it introduces new methods into GANs from another community. SN-GAN have roughly the same computational cost and memory consumption of DC-GAN, but inception and FID are much higher. The authors find that optimizing with respect to multiple discriminators increases diversity of samples for a computational cost. I guess that the reason may be that: the significant computational cost (both in FLOPS and memory consumption) increase due to multiple discriminators destroys the benefit from the small performance improvement. + On the experiments run, the HVM method appears to be an improvement over the two previous approaches of softmax weighting and straightforward averaging for multiple discriminators.
The paper proposes a class of Evolutionary-Neural hybrid agents (Evo-NAS) to take advantage of both evolutionary algorithms and reinforcement learning algorithms for efficient neural architecture search. 1. Doesn't explain how exactly the mutation action is learned, and missing the explanation of how RL acts on its modification on NAS (Evo-NAS).
This paper addresses an important problem, namely, unsupervised image classification, and may present interesting ideas. Unsupervised clustering (even when number of classes is not known) is a very well studied problem in machine learning.
1. It shows that the optimal convergence rate of BN can be faster than vanilla GD. The author analyze the convergence properties of batch normalization for the ordinary least square (OLS) objective.
This latent space representation is used as the reinforcement learning signal for the learner (probing) agent similar to the curiosity driven techniques where larger changes in the representation of mind are sought out since they should lead to larger differences in demonstrator agent behavior. 1) Summary This paper proposes a method for learning an agent by interacting and probing an expert agents behavior. 3. The core premise behind training the learner agent with RL is using a curiosity driven approach to train a probing policy to incite new demonstrator behaviors by maximizing the differences between the latent vectors of the behavior trackers at different time steps. The approximated demonstrator agent is trained through standard imitation learning techniques and the learning or probing agent is trained using reinforcement learning. The method builds on imitation learning (behavioural cloning) to model the agent's behaviour and reinforcement learning to learn a probing policy to more broadly explore different target agent behaviours. When the probing agent is testing the expert, it is essentially showing the imitator many different configurations of the environment. Overall, the approach falls into the field of intrinsic motivation / curiosity-like reward generation procedures but with respect to target agent behaviour instead of the agent's environment. - Same scale for the y-axes across figures This paper presents a method for interactive agent modeling that involves learning to model a demonstrator agent not only through passively viewing the demonstrator agent, but also through interactions from a learner agent that learns to probe the environment of the demonstrator agent so as to maximally change the behavior of the demonstrator agent.
Sure, those works focus on video prediction, while this work focus on building a "forward model"and is supposed to be for model-based RL, but this work has not performed any model-based RL experiments, so from my point of view, it is a video-prediction model contingent on an action input. CoRR, abs/1802.03006. This paper proposed to train a forward model used in reinforcement learning (RL) by task-independent losses. This paper describes an approach for training conditional future frame prediction models, where the conditioning is with respect to the current frame and additional inputs - specifically actions performed in a reinforcement learning (RL) setting.
Furthermore, the only application of these infinitely wide networks proposed in this paper is for initialization of the weights of finite width networks. The primary challenge in such networks is defining a distribution over the weights connecting two layers of infinite width. 1. The so-called "infinite width" is just yielded by kernels in RKHS for weight initialization. Summary: The paper attempts to proposal a weight initialization scheme to enable infinite deep infinite-width networks.
There is no actual connection to SGD left, therefore it is even hard to argue that the predicted shape will be observed, independent of dataset or model(one could think about a model which can not model a bias and the inputs are mean-free thus it is hard to learn the marginal distribution, which might change the trajectory) For SGD, the trajectory goes to the turning point very soon (usually no more than 10% of the training steps), whereas SMLC goes to the turning point much slower. Cons 2: This paper is going to be more meaningful if the author can provide some discussions, especially about (1) what does the shape trajectory mean (2) what do the connection between the trajectory and Markov chain means (3) how can these connections be potentially useful to improve training algorithm? In summary, this paper does the following: - The initial problem is to analyze the trajectory of SGD in training ANNs in the space of  P of probability measures on Y \\times Y. It is an interesting observation that the trajectory of \\alpha-SMLC  is similar to that of SGD in these plots, but the authors have not made a sufficient effort to interpret this.
This paper proposes an architecture search technique in which the hyperparameters are modeled as categorical distribution and learned jointly with the NN. The method then follows by alternating gradient descent on the weights and the parameters of the categorical distribution. I speculate that this parameter is essential as the categorical distribution gets a bigger search space.
The paper aimed at improving the performance of recommendation systems via reinforcement learning. Summary: The paper presents a session-based recommendation approach by focusing on user purchases instead of clicks. The author proposed an Imagination Reconstruction Network for the recommendation task, which implements an imagination-augmented policy via three components: (1)  the imagination core (IC) that predicts the next time steps conditioned on actions sampled from an imagination policy; There are many publicly available datasets for testing the performance of recommendation systems. The motivation of creating imagined trajectories instead of actual user trajectories is unclear. (2) State-of-the-art reinforcement learning algorithms were not taken into account for baselines in the experiments. As the proposed method is built based on reinforcement learning, it would be better if the authors could include state-of-the-art reinforcement learning algorithms as their baselines. The literature review is incomplete and misses important contributions on session-based recommendation, particularly, MDP-based methods such as Shani et al., An MDP-based recommender system, 2005 and Tavakol and Brefeld, Factored MDPs for Detecting Topics of User Sessions, 2014 (also see references therein). Strengths of the paper: (1) The research problem that the performance of recommendation systems needs to be improved is of great value to be investigated, as recommendation systems play crucial role in people's daily lives. The method is inspired from concepts of cognitive science which adds an imagination reconstruction network to an actor-critic RL framework in order to encourage exploration. Why do the authors utilize reinforcement learning to the task but not other supervised learning techniques? (2) Figure 2 is not straightforward. (2) the trajectory manager (TM) that determines how to roll out the IC under the planning strategy and produces a set of imagined item trajectories; The paper proposed a new framework for session-based recommendation system that can optimize for sparse and delayed signal like purchase. Is it because reinforcement learning based methods work better than traditional machine learning based ones? Weaknesses of the paper: (1) The motivations of applying reinforcement learning techniques are not convinced to me. Usually, users tend to search quite a lot before converging; hence, longer sessions possibly better reflect user interests. Nevertheless, the length of the trajectories is only 2 and instead should be varied empirically to show the usefulness of the reconstruction network. On the other hand, there are many traditional planning approaches which are not mentioned such as Monte Carlo Tree Search that simultaneously trade-off exploration and exploitation. Comments: The proposed architecture is an interesting inspiration from Neuroscience which fits into the sequential recommendation problem.
Also why one should use the authors' suggested penalization term instead of total correlation is not discussed, nor demonstrated as they perform similarly on both disentanglement and synergy loss. If the authors want to continue with the synergy minimisation approach, I would recommend that they attempt to use it as a novel interpretation of the existing disentangling techniques, and maybe try to develop a more robust disentanglement metric by following this line of reasoning. For this they  use one (of several possible) versions of synergy defintions and create a straight forward penalization term for a VAE objective (roughly the whole mutual information minus the maximum mutual information of its parts). This paper proposes a new approach to enforcing disentanglement in VAEs using a term that penalizes the synergistic mutual information between the latent variables, encouraging representations where any given piece of information about a datapoint can be garnered from a single latent. I commend the authors for taking a multi-disciplinary perspective and bringing the information synergy ideas to the area of unsupervised disentangled representation learning. The approach attempts to minimise the synergy of the information provided by the independent latent dimensions of the model. I hope the authors find more relevant applications or data sets in the future to demonstrate the importance of synergy.
The proposed model is an extension of Matching Networks [Vinyals et al., 2016] where a different image embedding is adopted and a pixel-wise alignment step between test and reference image is added to the architecture. Authors argue that using average (independent) greedy matching of pixel embedding (based on 4-6 layer cnn hypercolumns) is a better metric for one-shot learning than just using final layer embedding of a 4-6 layer cnn for the whole image. The motivation is partially covered by your statement "marginalizing over all possible matching is intractable", nevertheless an explanation of why it is reasonable to introduce these assumptions is not clearly stated. On the other hand, authors may argue that the hyper-column matching is not just about performance, whereas it also adds interpretability to why two images are categorized the same. ->incomprehensible sentence with two whiles: ABM networks outperforms these other state-of-the-art models on the open-set recognition task while in the one-shot setting by achieving a high accuracy of matching the non-open-set classes while maintaining a high F1 score for identifying samples in the open-set. One motivation for proposing an alignment-based matching is a better explanation of results. In this work, the authors tackle the problem of few-shot learning and open-set classification using a new type of NNs which they call alignment-based matching networks or ABM-Nets for short. Essentially due to computational costs authors are sacrificing a more thorough matching system (non-greedy) for a richer embedding and they don't get better results.
Summary: This paper proposes a novel differentiable approximation to the curiosity reward by Pathak et al. that allows a learning agent to optimize a policy for greedy exploration directly by supervised learning, rather than RL. The authors compare the differentiable function against using prediction error via REINFORCE and DQN, showing that their intrinsic curiosity method results in more interactions with unseen objects than the other two methods. + "This leads to a significantly sample efficient exploration policy. - There is very little *science* in this paper, beyond the experiments pitting "improved algorithm" vs DQN/REINFORCE, which nobody ever claimed would be a good approach to exploration! - also sec3, "[REINFORCE] gives no signal as to what action to take"; the signal has high variance but it works (see all policy gradient work)
This paper proposes a new framework for topic modeling, which consists of two main steps: generating bag of words for topics and then using RNN to decode a sequence text. I did not see the contribution of this part to the whole model as a topic model, although the joint training shows the marginal performance gain on text generation. This paper proposes TopicGAN, a generative adversarial approach to topic modeling and text generation. For the first task, classification is not the main purpose of topic models, and while text classification _is_ used in many topic modeling papers, it is almost always accompanied by other evaluation metrics such as held-out perplexity and topic coherence. (3) I did not see a major improvement of the proposed model over others, given that the only numerical result reported is classification accuracy and the state-of-the-art conventional topic models are not compared. The model basically combines two steps: first to generate words (bag-of-words) for a topic, then second to generate the sequence of the words. Our assumption aligns well with human intuition that most documents are generated from a single main topic." This goes very much against the common assumption of a generative topic model, such as LDA, which the model compares against. Specifically, the paper adopts the framework of InfoGAN to generates the bag-of-words of a document and the latent codes in InfoGAN correspond to the latent topics in topic modelling. This is because the main purpose of topic modeling is to actually infer the topics (per-topic word distribution and per-document topic distribution) and model the corpus. I would expect more comparisons than classification accuracy, such as topic coherence and perplexity (for topic modelling) and with more advanced conventional models. (2) The proposed model ignores the word counts, which can be important for topic modelling. This paper presents a topic model based on adversarial training.
The primary insight here is that there exists a smaller set of unique tasks, the knowledge from which is transferable to new tasks and using these to learn an initial parametrized reward function improves the coverage for IRL. - One of main contributions is avoiding the need for hand-crafted features for the IRL reward function. IRL tasks are provided to the learner and the learner aims to learn a strategy to quickly recover a good reward function for a new task that is assumed to be sampled from the same task distribution. + To a large extent, circumvents the need of having to manually engineered features for learning IRL reward functions
It builds on a recent approach by Achiam et al on Constrained Policy Optimization (oft- mentioned "CPO") and an accepted NIPS paper by Chow which introduces Lyapunov constraints as an alternative method.
The paper presents a pool-based active learning method that achieves sub-linear runtime complexity while generating high-entropy samples, as opposed to linear complexity of more traditional uncertainty sampling (i.e., max-entropy) methods. This paper proposes adversarial sampling for pool-based active learning, which is a sublinear-time algorithm based on 1) generating "uncertain" synthetic examples and 2) using the generated example to find "uncertain" real examples from the pool. Adversarial Active Learning (GAAL). The main difference is the added nearest neighbor component, as GAAL is directly using the generated examples, thus achieving constant runtime complexity, rather than sub-linear. GAAL, or uncertainty sampling, for example. This is achieved by using a generative adversarial network (GAN) to generate high-entropy samples that are then used by a nearest neighbor method to pick samples from a pool, that are closest to the generated samples.
This paper introduces an interesting unifying perspective on several sequence generation training algorithms, exposing both MLE, RAML and SPG as special cases of this unified framework. In summary, the contribution over RAML and SPG in combining them is quite incremental, and the practical importance of combining them is questionable, as is the integrity of the presented experiments, given how poorly MIXER is reported perform, and the omission of stronger baselines like SCST and AC methods. Table 1 suggests that MIXER can outpeform ML by only 0.1 Bleu points, and outpeformed by RAML? - Existing baselines in the paper (i.e. MIXER) do not perform as expected (i.e barely better than ML, worse than RAML) Confidence      5/5 Pros: - Generalizes RAML and SPG (and also standard entropy-regularized policy gradient). Furthermore, the MLE interpretation discussed is contained within the RAML paper, and the reductions to RAML and SPG are straightforward by design, and so do not really provide much new insight. - For instance, it's clear from Figure 3 that both MLE and RAML are overfitting and would benefit from more dropout (in the literature, 0.3 is commonly used for this type of encoder-decoder architecture).
Both terms of the IB cost function are formalized as mutual informations, but since in neural nets, the latent "compression" is a deterministic function of the inputs, a severe technical problems arises: the joint distribution between p-dimensional inputs X and the q-dimensional latent compression L is degenerate in that  its support lies in a space of dimension p (and not p+q as it would be in the non-degenerate case). This paper provides a method to do explicit IB functional estimation for deep neural networks inspired from the recent mutual information estimation method (MINE). The title, abstract and especially the conclusion ("This provides, for the first time, strong and direct emperical evidence for the validity of the IB theory of deep learning") seem to present the paper as somehow offering some clarity and further support for the assertions of the Shwartz-Ziv & Tishby 2017 paper, but that paper hoped to establish that information bottleneck can explain the workings of ordinary networks. Second, for the layer-wise training, the paper only compares the layer-wise IB objective with the cross entropy loss. This work is about layer-wise training of networks by way of optimizing the IB cost function, which basically measures the compression of the inputs under the constraint that some degree of information with respect to the targets must be preserved. Despite a recurring focus of the text that this paper applies and information theoretic objective at each layer of the network, and hence is novel, the final sentence of the paper suggests it might not actually be needed and single layer IB objectives can work as well. - How does the beta (in IB objective) selected in the experiments for comparison? On one hand, I find this paper interesting, because it aims at carefully studying the proposed link between DNN training and IB optimization, thereby showing that layer-wise IB training indeed seems to work very well in practice. By using the method, the authors 1) validate the IB theory of deep nets using weight decay, and 2) provides a layer-wise explicit IB functional training for DNN which is shown to have better prediction accuracy. How does this justify using the individual elements of the discriminator in the functional form of the IB objective?
-The proposed technique seems to include very heavy feature engineering and several ad-hoc practical steps--that is far from the motivation of using NN in tabular data. It seems to me that (except the minor small section of streaming data), the paper is more like a proper verification of how tree-based learning algorithms work very well in tabular data--which is far from the basis of the paper and does not make the paper novel enough for *CONF*.
However, when the entire image is subject to the generative model, it learns multiple properties from the image apart from shape too - such as texture and color. 2. Extracting morphological properties of the image is straight-foward for MNIST kind of objects. Since their method is manually designed for MNIST, the manuscript would benefit from a justification or discussion on the  common pitfalls and the correlation between MNIST generation and more complex natural image generation tasks. However, here the authors have assumed that the latent space of the generative models are influenced only by the morphological properties of the image - which is wrong. Since the presented metrics do not show a significant difference between the VAE and Vanilla GAN model, the question remains whether evaluating on MNIST is a good proxy for the performance of the model on colored images with backgrounds or not. * Providing benchmark data for tasks such disentanglement is important but I am not sure generating data is sufficient contribution for a paper. Authors present a set of criteria to categorize MNISt digists (e.g. slant, stroke length, ..) and a set of interesting perturbations (swelling, fractures, ...) to modify MNIST dataset. This paper discusses the problem of evaluating and diagnosing the representations learnt using a generative model. Studying the properties of a generative model on such datasets is very challenging and the authors have not added a discussion around that. For example sharpness and attending to details is not typically a challenge in MNIST generation where in other datasets this is usually the first challenge to be addressed. They perform a thorough study regarding MNIST. They suggest analysing performance of generative models based on these tools. 1. Morphological properties deals with only the "shape" properties of the image object. Therefore, statistically comparing the distribution of generated vs test data and binning the generated data is now possible.
page 4, the concept of stationary point and general position can be introduced before presenting Theorem 1 to improve readability. The perspective of the proof is interesting: By chain rule, the stationary point satisfies nnz(W^j) linear equations, but the subgradients of the loss function w.r.t. the logits have at most N\\times ks variables. 2. the claim is a little bit counter intuitive: Theorem 1 claims the sparse inequality holds for any \\lambda. Experiments are conducted to show that reaching a stationary point of the optimization can help to deliver good performance.
Also, following the pipeline [train-prune-retrain] can be substituted by pruning while training with little overhead as in recent papers: (such as Learning with structured sparsity or Learning the number of neurons in DNN both at NIPS2016 or encouraging low-rank at compression aware training of DNN, nips 2017). The approximated Hessian matrix is then used to estimate the increment of loss after pruning a connection. Summary: I do appreciate the fact that the proposed method does not require hyper-parameters and that it seems to yield higher compression rates than other pruning strategies that act on individual parameters. The paper proposes a multi-layer pruning method called MLPrune for neural networks, which can automatically decide appropriate compression ratios for all the layers. Experiments: - While the reported compression rates are good, it is not clear to me what they mean in practice, because the proposed algorithm zeroes out individual parameters in the matrix W_l of each layer. This paper proposes a multi-layer pruning technique based on the Hessian.
5) In the first reinforcement learning task, since cosine similarity is the only method used to measure the similarity between auxiliary task and the target task, it would be useful to show the comparison among other task relatedness method in reinforcement learning. The paper is addressing the problem of a specific multi-task learning setup such that there are two tasks namely main task and auxiliary task. When optimizing for the main loss function, the gradient of the auxiliary loss function is also used to update the shared parameters in cases of high cosine similarity with the main task. The simple and sensible approach proposed in the paper is using cosine similarity between the gradients of two loss functions and incorporating the auxiliary one if it is positively aligned with the main gradient. So a more general question would be: rather than define the similarity measure to measure the gradient similarity of the target and auxiliary loss, it would be more useful to try to learn or define whether the auxiliary task is good for the target task beforehand. The author(s) also experiment the proposed method on three tasks, one supervised learning image classification task, two reinforcement learning tasks, and show improved results respectively. In ImageNet experiment, auxiliary tasks actually hurt the final performance as the single task is better than all methods including the proposed one. The paper proposes a method for using auxiliary tasks to support the optimization with respect to a main task. In particular, the method assumes the existence of a loss function for the main task that we are interested in, and a loss function for an auxiliary task that shares at least some of the parameters with the main loss function. - What happens if there are multiple auxiliary tasks? The paper is definitely addressing an important problem as the authors cite many previous work which uses the setup of set of auxiliary tasks helping a main one. Although the method limits the negative effect of the auxiliary task on the optimization of the main loss function, it can still slow down optimization if the auxiliary task is not well chosen. The paper studies the problem of how to measure the similarity between an auxiliary task and the target tasks, and further decide when to use the auxiliary loss in the training epoches.
As far as I understood from the paper, changing the objective function to the upper bound of f-divergence have two merits compared to the existing methods. The trick that leads to computational tractability involves utilizing a latent variable and optimizing the f-divergence between joint distributions which is an upper bound to the (desired) f-divergence between the marginal distributions. arXiv preprint, 2018." So I am not sure whether you can take credit from the "spread f-divergence" or not. This paper proposed a novel variational upper bound for f-divergence, one example of which is the famous evidence lower bound for max-loglikelihood learning. The second contribution might be the spread f-divergence for distributions having different supports. The main contribution of our paper is the introduction of an upper bound on the f-divergence."
Summary ========= The authors present an extension to the VAE model by exploring the possibility of using the label space to create a new embedding space, which they call Probabilistic Semantic Embedding (PSE). My comments are as follows: 1. About the significance and originality, although to my knowledge, there seems to be no the exact match in the existing approaches of the idea of incorporating labels into the prior of the latent variable of VAE, the idea seems a little bit trivial and less of technical depth.
* Major remarks - There is little discussion in the manuscript about which layers should be eligible to mixup and how such layers get picked up by the algorithm. 2. The observations of mixing in the hidden space is better than mixing in the input space seem to contradictive to the observations by Mixup, it would be very useful if the paper can make that much clear to the readers. The authors experimentally show that networks with Manifold Mixup as regularizer can improve accuracy for both supervised and semi-supervised learning, are robust to adversarial attacks, and obtain promising results on Negative Log-Likelihood on held out samples. The paper proposes a novel method called Manifold Mixup, which linearly interpolating (with a careful selected mixing ratio) two feature maps in latent space as well as their labels during training, aiming at regularizing deep neural networks for better generalization and robust to adversarial attacks.
Under several assumptions (input is Gaussian, non-linear activation is strictly increasing, stable system) it is shown that SGD converges linearly to the ground truth system with near-optimal sample complexity. For beta = 0 the ground truth dynamical system is linear and for beta = 1 the ground truth is a non-linear dynamical system with ReLU. I agree that the paper has nice convergence results that could possibly be building steps towards the harder problem of unobserved hidden states however, there is more work that could be done for unstable systems and possible extension to ReLU and other activations to take it a notch higher. - The proof seems to rely on the fact that due to the gaussian input added each time step and stable system assumption after a sufficient number of time steps, the input-output pairs will not be highly correlated. This paper studies the ability of SGD to learn dynamics of a linear system + non-linear activation. Under this setting, the authors prove that for the given state equation for stable systems with random gaussian input at each time step, running SGD on a fixed length trajectory gives logarithmic convergence.
Pros: the authors develop a novel GAN-based approach to denoising, demixing, and in the process train generators for the various components (not just inference). On the other hand, HOW MUCH contribution is not addressed experimentally, i.e. the method is not properly compared with other denoising or demixing methods, and definitely not pushed to its limits. The proposed method is evaluated on both tasks (i.e., denoising and demixing) by conducting toy experiments on handwritten digits (MNIST). In this, paper a GANs-based framework for additive (image) denoising and demixing is proposed. Next, considering additive demixing, the authors assume that the corruption/structured signal is unknown but it can be modelled using a convolutional network (using the architecture of DCGAN). It's hard to assess the difficulty of the denoising problem because their method does so well, and it's hard to assess the difficulty of demixing because of the lack of comparators. Caveats: I am knowledgeable about iterative optimization approaches to denoising and demixing, especially MCA (morphological component analysis), but *not knowledgeable about GAN-based approaches*, though I have familiarity with GANs. Demixing is performed by solving a similar by solving a similar ridge regularized non-convex inverse problem as in the case of denoising (i.e., Eq. 4).
In this case, q(w) = p(w|\\Theta), as stated Eq (3) and in the corresponding equation provided in page 2 (the q(w) is not learnt because it only depends on the dropout rate, while the Θ are learnt by maximum log-likelihood and do not have a q associated). Minor comments: 1. The generative model for Variational dropout is the same than the generative model for the "conditional model", eq. For this purpose, they show that when using dropout training we are maximizing a common lower bound on the objectives of a family of models, including most of the previously used methods for prediction with dropout.
The authors seek to make it practical to use the full-matrix version of Adagrad's adaptive preconditioner (usually one uses the diagonal version), by storing the r most recently-seen gradient vectors in a matrix G, and then showing that (GG^T)^(-½) can be calculated fairly efficiently (at the cost of one r*r matrix inversion, and two matrix multiplications by an r*d matrix). Given that rxr is a small constant sized matrix and that matrix-vector multiplication can be efficiently computed on GPUs, this matrix adapted SGD can be made scalable. Rather than adapting diagonal elements of the adaptivity matrix, the paper proposes to consider a low-rank approximation to the Gram/correlation matrix. There is a great deal of discussion about full-matrix preconditioning, but there is no full matrix here. There are several issues convolved here: one is ``full-matrix,'' another is that this is really a low-rank approximation to a matrix and so not full matrix, another is that this may or may not be implementable on GPUs. However, because this matrix turns out to be a pxp matrix where p is the number of parameters in the model, maintaining and performing linear algebra with this pxp matrix is computationally intensive. The latter may be important in practice, but it is orthogonal to the full matrix theory. If there were theory to be had here, then I would guess that the low-rank approximation may work even when full matrix did not, e.g., since the full matrix case would involve too may parameters.
* Significance: The concept of converting a non-differentiable pipeline to a differentiable version is indeed very useful and widely applicable, but the experimental section did not convince me that this particular method indeed works: the results show a very small improvement (0.7-2%) on a single system (Faster R-CNN), that has already been pretrained (so not clear if this method can learn from scratch). Similar efforts in this direction, namely making various modules of the Faster R-CNN pipeline differentiable, have shown little gains as well. We can just treat the pipeline as a black box mapping parameters to training set performance, and so any black-box optimization method can be applied to this problem.
Given that the ground truth attribute decomposition for MNIST is not known, even the qualitative results are impossible to evaluate. The authors encourage the decomposition of the latent space into the 'template' and the 'attributes' features by training a discriminator network to predict whether the attributes and the template features come from the same image or not. Furthermore, while the authors spend many pages describing their methodology, the writing is often hard to follow, so I am still confused about the exact implementation of the attribute features \\phi(x, m) for example. There seems to be an interesting interaction between the encoder, discriminator and the attribute function that requires more investigation. The model learns how to decouple the attributes in an adversarial way by means of a discriminator.
Some of them I've highlighted earlier in my review (e.g., some of the theorem assumptions being typically true, comparison of likelihood and sample quality based on model capacity etc.). Algorithm - While significant advancements have indeed been made for nearest neighbor evaluation as the authors highlight, it's hard to believe without any empirical evidence that nearest neighbor evaluation is indeed efficient in comparison to other methods of likelihood evaluation. The paper proposes a new algorithm for implicit maximum likelihood estimation based on a fast Nearest Neighbor search. The prior work trained the same normalizing flow model via maximum likelihood and adversarial training, and observed vastly different results on likelihood and sample quality metrics. The current title is Flow-GAN: Combining maximum likelihood and adversarial learning in generative models. The paper shows that under some conditions the optimal solution of the algorithm corresponds to the MLE solution and provides some experimental evidence that the method leads to a higher likelihood. - Another possibility is to use generative models like Real-NVP for which the likelihood can also be computed in closed form.
The new thing is that the authors found that quantization of activation function improves robustness, and the approach can be naturally combined with FGSM adversarial training. 2. There are several important papers missing in the discussion/comparisons: - Quantization improves robustness has been reported in a previous paper: "Defend Deep Neural Networks Against Adversarial Examples via Fixed andDynamic Quantized Activation Functions".
A pair of sentences from a weak document pair are used as training data if their cosine similarity exceeds c1, and the similarity between this sentence pair is c2 greater than any other pair in the documents, under sentence representations formed from word embeddings trained with MUSE. c) training the usual unsup MT pipeline with two additional losses, one that encourages good translation of the extracted parallel sentences and another one forcing the distribution of words to match at the document level. The model is also trained to minimise the KL divergence between the distribution of terms in the target language document and the distribution of terms in the current model output. The model also uses the denoising autoencoding and reconstruction objectives of Lample et al. (2017). The results show improvements over the Lample et al. (2017) and that performance is heavily dependent on the number of sentences extracted from the weakly aligned documents. - What is the total number of sentences in the weakly paired documents in Table 1? b) extracting from these documents parallel sentences,
Integrating CRFs with GNNs for node classification is not new ([1][2][3]), but this paper extended energy definition to capture high-order connections and use it for a new subsequent task: graph pooling. This paper studied to use CRF to define the cluster assignment of the nodes  for graph pooling, which model the dependency of the cluster assignments between the nodes. Strength: -- An interesting idea to use CRF idea to cluster the nodes on a graph for pooling purpose
This, unsurprisingly, limits the applications of DPPs, and has driven a lot of research focused on improving DPP overhead. Better marriage between deep learning and DPPs is an important problem as diversity is crucial in many machine learning problems (even beyond computer vision - the experimental domain of this paper - e.g. in machine translation and document summarization). However, learning diverse features via DPPs with deep learning frameworks is challenging due the instability in computing the gradient which involves a matrix inverse operation.
they show that, relying on the technique of deep taylor decomposition, their CNN relies its prediction on a different part of zebra fish than existing understanding. the paper uses model interpretation techniques to understand blackbox CNN fit of zebrafish videos. Furthermore, the authors conduct one good analysis (DTD) to explain the results of their CNN. SUMMARY: explore the use of CNN in a binary task on images of zebrafish while the experimental studies rely on our belief that the interpretation technique indeed interprets, the result that removing experimental features and improving predictive performance is convincing and interesting. the idea of a case study about the usefulness of model interpretation techniques is interesting. BACKGROUND: Hypothesis: Prey movements in zebrafish are characterized by specific motions, that are triggered by a specific pathway involving an area called AF7. AI: the prey stimuli was a characteristic movement that an SVM was trained to detect. Perhaps the authors are also not aware that the fallacies that causes CNNs to overfit on some characteristics in the input data are also present in other machine learning tools such as SVMs. Perhaps the authors are not aware that CNNs are hardly black boxes, their inner workings quite transparent in mathematical terms, which the submitted paper itself explores.
2. The encouraging result in Table 1 in EMPIRICAL EVALUATION shows the consistent outperformance of MaSS over SGD and Nesterov SGD regardless of the changing learning rates. They demonstrate that this adjustment is necessary and sufficient to benefit from the faster convergence of Nesterov gradient descent in the stochastic case. 5) It is true that Nesterov SGD is very efficient for training neural networks and MaSS may have some effect in practice.
Summary This paper studied the expressive power of graph NNs, specifically, their universality and limitations under the non-anonymous setting, via the theory of distributed computations. The impossibility results show that for certain problems, e.g., subgraph detection, there exists a graph such that GNN can not solve the problem unless the product of a GNN's depth and width exceeds (a function of) the graph size. Decision This paper gave us a new approach to analyzing the expressive power of graph NNs. Not only does this paper give new theoretical results, but also it opens the door to a new research direction by bridging the theories of graph NNs and distributed computations.
The apparent requirement to have a reasonable semantic segmentation model available, make it important to evaluate also in other settings (for example on an indoor dataset like NYU) to show that the approach works beyond street scenes (which is one of the in practice not so interesting settings for monocular depth estimation since it is rather easy to just equip cars with additional cameras to solve the depth estimation problem). This work proposes to leverage a pre-trained semantic segmentation network to learn semantically adaptive filters for self-supervised monocular depth estimation. The paper proposes a using pixel-adaptive convolutions to leverage semantic labels in self-supervised monocular depth estimation. The novelty here is to integrate a semantic classification model along with depth inference.
At first I was wondering why such compositional language messages would be desirable and was a bit negative on this work. This paper proposed a neural iterated learning algorithm to encourage the dominance of high compositional language in the multi-agent communication game. Emerging the compositional language from uniform prior can be very challenging, as mentioned in the paper, "high-\\rho language only represents a small portion of all possible unambiguous language" and "high-\\rho do not seem to be directly preferred during the interaction phase, they can be favored by the neural agent during the learning phase."
VL-BERT extend BERT by changing the input from subsequent sentence to image regions and modify the caption words has the additional visual feature embedding. They demonstrate that the pre-training procedure can help improve performance on down-streaming tasks like visual question answering, visual commonsense reasoning. While ViLBERT designs for easier extendable for other modalities, VLBERT is more focus on the representation learning on the vision and language, since the caption input also combines with the visual feature embedding. * Once textual embeddings are masked by [MASK], the related visual embedding (whole image) is also masked?
The UDR score can be used for unsupervised hyperparameter tuning and model selection for variational disentangled method. -- To validate the fundamental assumption of UDR, the authors might consider to quantitatively validate that, disentangled representations learned by those approaches you used in the paper are almost the same (up to permutation and sign inverse). Based on the understanding of "why VAEs disentangle" [Burgess et al. 2017, Locatello et al. 2018, Mathieu et al. 2019, Rolinek et al. 2019], the authors adopt the assumption that disentangled representations are all alike (up to permutation and sign inverse) while entangled representations are different, and propose UDR method and its variants. Therefore, I am not convinced that I should trust the results of the UDR, which combines multiple disentangled models. There is no theoretical guarantee that UDR should be a useful disentanglement metric.
The major contributions include: (1) distribution alignment to calibrate the predicted distribution of unlabeled data; (2) augmentation anchoring to allow more aggressive data augmentation; and (3) CTAugment to train the augmentation policy alongside the semi-supervised model. Mix-Match is already an elaborate method, and ReMixMatch additionally introduces learned data augmentation, an additional loss term for matching label distributions between labeled and unlabeled data, consistency-loss, and a self-supervised loss (section 3.3). Augmentation anchoring instead of computing the guessed probabilities on unlabelled data as the average probabilities on transformed samples (as in MixMatch), it considers as guessed labels the average probabilities obtained from weak transformations (flip+crop) even when using stronger transformations (Autoaugment like).
This paper proposes to use a semi-supervised VAE based text-to-speech (TTS) for expressive speech synthesis. I think it differs from standard semi-supervised training in that at test time we aren't explicitly interested in predicting labels from the semi-supervised labelled classes; rather, we feed in these labels as input to affect the generated model output.
The primary claim is that simple components which compute elementwise sum/average/max over the activations seen over time are highly robust to noisy observations (as encountered with many RL environments), as detailed with various empirical and theoretical analyses. Overall it is an interesting paper in the sense that the introduced aggregation layer is so simple but effective in noisy RL. Also in the RL section - the notation of τt∈Ωt should probably also contain the sequence of t−1 actions taken throughout the trajectory - this is made clear in the text, but not in the set notation.
The authors discuss four techniques to improve Batch Normalization, including inference example weighing, medium batch size, weight decay, the combination of batch and group normalization. (3) In Section 3.1, "we need only figure out …" should be "we only need to figure out …" The paper introduces four techniques to improve the deep network model through modifying Batch Normalization (BN). Experimental evidence seems sufficient and there are some theoretical derivations, but it looks incremental that the paper presents some techniques in improving Batch Normalization only. 3. By combining all the techniques, the proposed method yields promising performance when training deep models with different batch sizes. (2) In Section 3.1, "Batch Normalization has a disparity in function between training inference".
I agree that the following three key contributions listed in the paper are (slightly re-formulated): 1. Introducing Precision Gating (PG), the first end-to-end trainable method that enables dual-precision execution of DNNs and is applicable to a wide variety of network architectures. Notes which did not affect the review score: - There are some typos, e.g. "PG computes most features in a low precision and only a small proportion of important features in a higher precision." Saying "PG computes most features using reduced precision and only a small proportion of important features using high precision" would be more correct. 2. Precision gating enables DNN computation with a better average bitwidth to accuracy tradeoff than other state-ofthe- This paper introduces Precision Gating, a novel mechanism to quantize neural network activations to reduce the average bitwidth, resulting in networks with fewer bitwise operations.
Strengths: - The authors have proposed a nice multimodal model that allows inference of latent variables given only text or image, and also allows realistic synthesis of images from images, text, or noise. Their proposed VHE-GAN model encodes an image to decode its associated text and feeds the variational posterior as the source of randomness into the GAN image generator. Combined with the included code release, this paper should be of interest to many. (VHE) randomized generative adversarial network (GAN) that integrates a probabilistic text decoder, probabilistic image encoder, and GAN into an end-to-end multimodal model. The proposed method utilizes the off-the-shell modules and feeds the VHE variational posterior into the generator. Finally, though this version of the paper includes code directly in a google drive link it would be ideal for the final version to reference a github code link - again to aid access to interested individuals. - while it is difficult to dilute such a complex model to 8 pages, and the included appendix clarifies many questions in the text body, it would be worth further passes through the main paper with a specific focus on clarity and brevity, to aid in the accessibility of this work. Weaknesses: - The experimental comparison only included old baselines and the authors should compare to some more recent work such as TA-GAN (NIPS18), and Object-GAN (CVPR19).
The authors propose an agent that builds a dynamic knowledge graph of each state from the textual observation provided by the games, while choosing actions from a template-based action space. This paper proposes a knowledge graph advantage actor critic (KG-A2C) model to allow an agent to do reinforcement learning in the interactive fiction game. Under the general framework of A2C, the core contribution of the paper is to apply a graph attention network on the knowledge graph to help learn better representation of the game state and reduce the action space. Note that the baseline I am referring to is different from the LSTM-A2C baseline reported in the paper as: (1) with entity extraction, although you may not get a graph mask, but you can still have a object-mask which also reduces the action space; One of the main contribution in the paper is to represent the partial observations of the world as a knowledge graph, and so as to efficiently infer appropriate actions.
For example, it is a bit unclear why f_Z should be with low curvature -- does it mean that you wish the control problem in the latent domain is more like a linear dynamical system, so that the LLC algorithm works better? This paper considers from a high level the problem of learning a latent representation of high dimensional observations with underlying dynamics for control.
If the number of connected components does not match, based on theorem 2, Corollary 1 argues that a generative model can generate an adversarial example that does not exist in the data-generating manifold. However, this work argues that if the generative model does not model the topology of the manifold, it can still be fooled by an adversarial example. The paper tries to answer the following question: In adversarial defense training do manifold based defenses need to know the structure of the underlying data manifold? Next to some experiments on data sets with a manifold structure, the main contribution of the paper is a tandem of theorems that state the conditions under which models can recover the topology---or the number of connected components---of a data set correctly, They argue that a generative model needs to be at least aware of the number of connected components of the data-generating manifold.
I would say the main concern should be "FS quantization achieves the same quality as original FS while having higher compression factor". "efficiency searching scheme" This paper proposes a method, termed as Filter Summary (FS), for weight sharing across filters of each convolution layer. Please shed more insights on why weight sharing by Filter Summary can have such a high compression ratio, even with the help of quantization. 3 Please explain why the simple linear quantization is compatible with Filter Summary for image classification and object detection.
Statistics themselves are calculated using 32 bit floating point numbers. The key idea here is that for each tensor of 8-bit numbers, two 32 bit floating point statistics are recorded as well. The method requires a hardware modification to evaluate per tensor statistics, which intuitively capture dynamic range that a given tensor values may fall into. Mapping between representation and actual numerical values is much more complex than when using standard floating point representation (as any given numerical value is defined by a single its 8 bit representation + alpha + beta tensor statistics), integrated circuitry used to execute arithmetic operations would likely be much more complex than when using standard floating point operations. The paper suggest the method to train neural networks using 8 bit floating point precision values. I am by no means not a hardware design expert, but I am not convinced that the gain of using 8 vs 16 bit floating point numbers outweights any extra complexity of hardware implementation. 1. There is no comparisons with bfloat16, which is becoming a widely used approach to lower precision and is gaining significant hardware support [1].
The sleep algorithm fails to outperform the baselines for each attack type (except for an almost negligible advantage in accuracy on JSMA) and barely even outperforms the control network in most cases (2/4 attacks it actually underperforms the control). Section 3: Adversarial defenses Regarding distillation: "We use T=50 to compare with the sleep algorithm" They present a detailed comparative study spanning three datasets, four types of adversarial attacks and distortions, and two other baseline defense mechanisms, in which they demonstrate significant improvements (in some cases) of the sleep algorithm over the baselines. 6. In the analysis of JSMA, as noted before,, it's rather dubious to claim that sleep had any kind of significant effect on the attack success rate (or distance) for CUB-200. The paper proposes an ANN training method for improving adversarial robustness and generalization, inspired by biological sleep. Also this sentence should be reworded to be less visual and more quantitative (e.g. sleep tends to have higher median accuracy scores than the other methods for eta < 0.1).
The path angle visualization provides a novel tool to evaluate the empirical ability of any dynamics proposed for GANs to cancel out rotational components. This paper tries to provide a deeper understanding of the training dynamics of GANs in practice via characterizing and visualizing the rotation and attraction phenomena nearby a locally stable stationary point (LSSP) and questions the necessity to access a differential/local Nash equilibrium (LNE).
But, the paper would do well to embrace the Fourier domain and first discuss what is meant by "natural images" and "noise" in terms of their frequency content. The authors show that when using appropriate upsampling operators, gradient descent biases the reconstructed images towards low-frequency components, while the noise components, which typically consist of high-frequency patterns, take longer to fit, so that early stopping can provide a useful bias for denoising. Denoising is demonstrated on two images with additive white Gaussian noise and the approach under study is shown to provide a better signal-to-noise ratio than BM3D, another untrained denoising approach.
This model is used to learn a 3D visual representation that can be applied for semi-supervised 3D object detection, and for unsupervised 3D moving object detection. The results show that the proposed method: 1) has higher 3D object detection mAP than a view regression-based baseline from prior work (Tung 2019) for settings with little available 3D boundign box supervision; Results demonstrate the strengths of the proposed view-contrastive framework in feature learning, 3D moving object detection, and 3D motion estimation. Experiments are performed to evaluate the proposed method against baselines on 3D object detection (both in the semi-supervised setting and unsupervised moving object detection), 3D motion estimation, as well as sim-to-real transfer results (training in CARLA and testing on the KITTI dataset). 3) outperforms the view regression baseline and a 3D motion flow-based baseline on 3D moving object detection (measured through precision-recall curves and mAP); how many labeled images? Also not quite unclear about the settings in UNSUPERVISED 3D MOVING OBJECT DETECTION. They then assess their approach on numerous tasks such as 3D-object detection, 3D-moving object detection, and 3D motion estimation. Moreover, the proposed model is evaluated on the downstream 3D object detection tasks to demonstrate the utility of the learned 3D visual representation.
======================================================== Decision. I strongly feel that motivating the noise robustness of 0-1 loss by discussing about the adversarial risk (Hu et al.) is misleading. Then, it was extended to the multiclass loss by the following paper: [3] Ghosh et al.: Robust loss functions under label noise for deep neural networks. I believe the story to motivate the robustness of 0-1 loss under label noise should be
Throughout the paper, the experiments and results raise questions on the robustness and generalization of existing exploration methods across various ATARI games, but the paper puts absolutely zero effort into investigating if there is a quick fix to the questions it poses. I think the main contribution of the paper is that it raises some questions over existing methods/trends in solving exploration problems in reinforcement learning by comparing the performance of multiple methods across various games in ATARI suite. Here are a couple of points that I felt conflicted/confused about the paper: - The conclusion of the paper is that 'progress of exploration in ATARI suite is obfuscated by good results in single domain'. ------------------------------------------------------------------------------------------------------------------------------------------------------------------------- Summary: This paper presents a detailed empirical study of the recent bonus based exploration method on the Atari game suite. To support their claim, the authors firstly compare bonus exploration methods, noisy networks, and epsilon-greedy on hard exploration games. The authors combine Rainbow with different exploration methods, such as count-based bonus methods, curiosity-driven methods, and noisy networks.
The authors provide a comprehensive study, with theory and classical simulation of the quantum system, on how to increaese the speed of CNN inference and training using qubits. This submission proposed a quantum convolutional neural network (QCNN). The authors present a quantum algorithm for approximating the forward pass and gradient computation of a classical convolutional neural network layer with pooling and a bounded rectifier activation. it would be valuable to convert those to estimates of which values of eta would be enough (given quantum networks of the size used in the classical simulation experiment, or given larger networks).
For example, using exactly the same NAS search method and supernet, and comparing the FNA method with that not using a pretrained model (i.e., directly search on det/seg) could be a good experiment to showcase the importance of adaptation. The paper proposes a method called FNA (fast network adaptation), which takes a pretrained image classification network, and produces a network for the task of object detection/semantic segmentation. I like the direction this paper takes, NAS is too expensive and we need faster methods through meta learning/transfer learning. Concrete comments 1. The paper's overall method is a novel one, unifying NAS on det/seg tasks, while prior works mostly only focus on one task. If the authors faithfully compared with state-of-the-art methods in search det/seg architectures, but I'm not super familiar with this literature. - Lack of error bars or comparison to random search. On object detection the method does not improve the model size or accuracy, but reduces the search time a lot compared with DetNAS. They do this by first expanding the network into a "supernet" and copy weights  in an ad-hoc manner, then, they perform DARTS-style architecture search before fine-tuning for the task at hand. As mentioned earlier, the choices for remapping weights seem very ad-hoc.
11). ACM. The authors consider the alignment problem for multiple datasets with side information via entropic optimal transport (Sinkhorn). # Summary This paper proposes a new way to learn the optimal transport (OT) cost between two datasets that can utilize subset correspondence information. The main idea is to use a neural network to define a transport cost between two examples instead of using the popular Euclidean distance and have a L2 regularizer that encourages the resulting transport plan to be small if two examples belong to the same subset. The cost function is optimized via a loss function derived from side information, specifically subsets of the two datasets to be aligned that contain elements that should be matched in the optimal transport plan.
The paper shows that in the worst case, the proposed method has the same gradient query complexity as the SpiderBoost variance reduction method. Based on this observation, the paper proposes a modified variance reduction method (by modifying the SpiderBoost method), where a 'memory vector' keeps track of the coordinates of the gradient vectors with large variance.
In this paper, the authors proposed two methods of Nesterov Iterative Fast Gradient Sign Method (NI-FGSM) and Scale-Invariant attack Method (SIM) to improve the transferability of adversarial examples. In this paper, the authors apply the Nesterov Accelerated Gradient method to the adversarial attack task and achieve better transferability of the adversarial examples. Two methods have been proposed,  namely Nesterov Iterative Fast Gradient Sign Method (NI-FGSM) and Scale-Invariant attack Method (SIM). 2. The authors are expected to make more comprehensive comparisons with the recent methods in adversarial attacks, e.g, PGD, and C&W even if some methods are designed for white-box attack.
The extensive experiment results also show that for deeper GCNs, DropEdge always win over other baselines (see Tab 1) despite most of them are marginal except the backbone being GraphSAGE on Citeseer. The authors proposed a new method called "DropEdge", where they randomly drop out the edges of the input graphs and demonstrate in experiments that this technique can indeed boost up the testing accuracy of deep GCN compared to other baselines. I also like the discussion in sec 4.3 where the authors explicitly clarify what are the difference between DropEdge, Dropout and DropNode, as the other two are the methods that will pop up during reading this paper.
2) The authors mention that this memory compression architecture enables long sequence modeling. The outcome is a versatile model that enables long-range sequence modeling, achieving strong results on not only language model tasks but also RL and speech. Detailed comments: - About the character-level language modelling on Enwik8, the improvement is very small, it seems that the task doesn't benefit from have long-range memory, could it be because character-level modelling is less dependent on the long-range past? A variety of compression techniques and training strategies have been investigated in the paper and verified using tasks from multiple domains including language modeling, speech synthesis and reinforcement learning. What is the difference between sequence, memory, and compressed memory? The probably more interesting part of this paper is the training schemes designed to train the memory compression network. For testing and evaluating the modeling of really long context sequence modeling, the authors introduce PG-19, a new benchmark based on Project Gutenberg narratives. Particularly, the authors propose a new benchmark PG-19 for long-term sequence modeling. The key novelty of this model is to preserve long range memory in a compressed form, instead of discarding them as previous models have done. The paper finally presents an analysis of the compressed memory and provide some insights, including the fact that the attention model uses the compressed memory. ## Updated review I have read the rebuttal. It would also been interesting to evaluate the gain of the memory, for instance by varying the size of the compressed memory from 0 to 1152. However when comparing with the literature, it's not clear if the performance gain is due to the compressed memory or to the network capacity.
The authors present an algorithm CHOCO-SGD to make use of communication compression in a decentralized setting. 3. About "datacenter setting" experiment, it seems not an apple to apple comparison between CHOCO-SGD and all-reduce method since CHOCO-SGD stands for the decentralized algorithm with compression and all-reduce stands for a centralized algorithm without compression. The authors consider CHOCO-SGD for non-convex decentralized optimization and establish the convergence result based on the compression ratio. Second, on the practical part, there have 3 main results: 1. They compare CHOCO-SGD under various compression schemes with the baseline. This paper studies non-convex decentralized optimization with arbitrary communication compression. Thus, the advantage of vanilla CHOCO-SGD over other alternatives is not convincing.
Empirically the authors evaluate the VAEs on four different datasets (a synthetic tree dataset, binarized MNIST, Omniglot, and CIFAR-10) for various choices of product spaces (fixed curvature and learnable curvature) and choices of latent space dimensionality. Summary: This paper devised a framework towards modeling probability distributions in products of spaces with constant curvature and showed how to generalize the VAE to learn latent representations on such product spaces using Gaussian-like priors generalized for this case. It follows the current trend of learning representations on curved spaces by proposing a formulation of the latent distributions of the VAE in a variety of fixed-curvature spaces, and introduces an approach to learn the curvature of the space itself. Fairly recently, ML researchers have developed non-Euclidean embeddings, initially in hyperbolic space (constant negative curvature), and then in product spaces that have varying curvatures. Especially in the case the VAE on MNIST with a 72-dimensional latent, as I suspect the 6 and 12 dimensional spaces are not "large enough" for this phenomenon to appear.
The first method is to use a teacher-student mechanism that uses a fully real-valued network to teach a binary network. The steps for building binary network takes several components: traditional strategy to binary/optimize a model (like data augmentation,  binary initialization using 2-stage optimization, etc), real-to-binary attention matching that tries to match the output of real values and binarized model, and data-driven channel rescaling to better approximate real convolutions. As the ablation study shows the gating function actually hurts for binary down-sampling layers. 2. Comparisons with existing methods: On ImageNet, the model is compared with a complete list of alternative methods (low-bit quantization, larger binary nets, binary nets and real-valued nets).
Unlike the other deterministic method, sampling skill suffer variance propagation problem, the pre-layer variance will affect the sampling probability of next layer, how this pruning work if we change status of the pre-layer,  I didn't find any theoretical guarantee and only find a proof of single layer reconstruction bound. To achieve a more reasonable algorithm, author prune the redundant channel by controlling the deviation of the summation statistically small,  and reusing the filter by important sampling the given channel. Among most heuristics prune method,  pruning with mathematics guarantee is indeed more convincing. Experiment show that this method can reach a competitive prune radio against other pruning algorithm, and show robustly in retained parameters vs error experiment. https://openreview.net/forum?id=rJl-b3RcF7 This paper studies the tasks of pruning filters, a provable, sampling-based approach for generating compact Convolutional Neural Networks (CNNs). *CONF* 2019. Summary: In this paper, the author propose a provable pruning method, and also provide a bound for the final pruning error. In most case, if we want to prune the large channel network,  picking the top-1 significant filter or random sampling top-k filter will almost do the same thing.
This paper is under the research area of "hierarchical reinforcement learning." However, just like temporal abstraction, the HRL is a general idea instead of an existing problem formulation or a particular algorithm. 2. I think the author didn't justify his key design choices well. Alike other HRL agents, their method has two types of policies (manager and subpolicies), but different from other works they do not keep parameters fixed in post training for new tasks. 1. Why is random length a valid choice? - Fig 4/5a, some agents (blue) seems to have undesired behaviour (until half of the iterations). It seems that the author is not aware of this point as the paper claims a particular way of achieving HRL is the HRL itself (in section 4.1 "In the context of HRL, a hierarchical policy with a manager πθh(zt|st) selects every p time-steps one of n sub-policies to execute."). Since bounding the random length needs prior knowledge, how difficult is it to come up with the prior knowledge.
Minor comments/questions: - If the attention is applied over the orientations of the same feature, why does it improve the performance on Rotated MNIST (which is rotation invariant)? 1. The motivation for the attention mechanism (as discussed in the introduction and illustrated in Fig. 2) seems to be to find patterns of features which commonly get activated together (or often co-occur in the training set). This paper describes an approach to applying attention in equivariant image classification CNNs so that the same transformation (rotation+mirroring) is selected for each kernel. Weaknesses: - Unclear how the proposed attention mechanism accomplishes the goal outlined in Fig. 2d
This paper proposes to use the robust subspace recovery layer (RSR) in the autoencoder model for unsupervised anomaly detection. This paper adapts the concept of Robust Subspace Recovery (RSR) as a layer in an auto-encoder model for anomaly detection.
Wouldn't that then imply that the uncertainty would be zero for any input (even an out-of-distribution one), as the prior network and predictor network always agree? </update> The paper shows that the MSE of a deep network trained to match fixed random network is a conservative estimate of uncertainty in expectation over many such network pairs. Essentially, instead of training a single predictor that outputs means and uncertainty estimates together, authors propose to have two separate models: one that outputs means, and one that outputs uncertainties.
In particular, it would be nice to discuss what happens when the physics is a black box (i.e. we can interact with the system by applying control and observing, but we don't know the rules governing the physical system). In this paper, the authors outline a method for system control utilizing an "agent" formed by two neural networks and utilizing a differentiable grid-based PDE solver (assuming the PDE describing the system is known). I like the idea of splitting the control problem into a prediction and a correction phase, which leverages the power of deep neural networks and also incorporates our understanding of physics. The paper presents an interesting mix of neural networks and traditional PDE solvers for system control, and I vote for acceptance. The agent is split into a control force estimator (CFE) which applies a force to advance the state of the controlled system, and an observation predictor (OP) which predicts the trajectory needed to reach the target state.
Summary: This paper proposes the use of two intrinsic rewards for exploration in MARL settings. EITI uses mutual information to capture the influence of one agent on the transition dynamics of others,  while EDTI uses an intrinsic reward called Value of Interaction (VoI) to quantify the influence of one agent's behavior on expected returns of other agents. My main reservation is a lack of comparisons to single agent exploration methods. The key idea is to define one agent's exploration in terms of its interactions with other agents.
Another question raised by the passage-span predicate: the more you use bare passage-span programs for training, the more the network learns to put all of its compositional reasoning inside, in an opaque way, instead of giving you interpretable compositionality. But the fact that you have this predicate lets the model do these filters and greater-than comparisons inside the network in an opaque way, while also getting interpretable operations for some questions (table 5 is further confirmation of this, and of the fact that you probably are not capturing many of more the complex, compositional questions in DROP). Because you're selecting passage spans directly instead of performing some kind of matching operation, you have to have your search select all of the appropriate spans for this to be "interpretable", and not just hiding the logic inside of the network. This predicate lets the model shortcut any interpretable reasoning and do operations entirely inside the encoder/parser. With only weak supervision, and with the parser having the ability to shortcut these more interpretable operations, how often are you actually getting the interpretable one, and what's causing the model to choose it? This paper discusses an extended DSL language for answering complex questions from text and adding data augmentation as well as weak supervision for training an encoder/decoder model where the encoder is a language model and decoder a program synthesis machine generating instructions using the DSL. With the weak supervision that you have, are you actually able to find more complex programs during your search? At the other extreme, where you don't have passage-span, you are left with a crippled semantic parser that can't handle most of the questions. In Fig 1 for example there is an arrow that connects passage to compositional programs. This also seems like a really hard search problem in how you've set up your DSL - what would make your search over programs actually select all of the correct arguments?
Questions: - There is nothing special about the wasserstein natural gradient flow variational form and implicit model, once can apply the same to the variational form of Fisher, that would be probably more efficient? The idea to restrict into RKHS and use low-rank approach is interesting to approximate for the natural gradient under Wasserstein metric. Natural Wasserstein Gradient similar to the so called natural fisher gradients preconditions the gradient using a matrix that uses the local curvature of the manifold of the parametric distribution. AISTATS 2019. We encourage the authors to layout in the beginning the derivations form this point of view which will make the paper easier to digest, the expression in Equation 7 seems mysterious and pulled out of a hat, but it is easier to understand by going to perturbation analysis usually done on KL for Fisher Natural gradient and to do it also here starting from the linearization of W2 with ||.||H−1(q)  , and how to approximate it in RKHS as it was already proposed in the literature in Mroueh et al Sobolev Descent.
Formulation: 1. In eq 3, the expectation is taken over the difference between the prediction on the sampled sentence and the one with the phrase removed. Summary: The authors proposed a method for generating hierarchical importance attribution for any neural sequence models (LSTM, BERT, etc.) Towards this goal, the authors propose two desired properties: 1) non-additivity, which means the importance of a phrase should be a non-linear function over the importance of its component words; 2) context independence, which means that the attribution of any given phrase should be independent of its context.
The authors propose learnable "kaleidoscope matrices" (K-matrices) in place of manually engineered structured and sparse matrices. Summary The authors introduce kaleidoscope matrices (K-matrices) and propose to use them as a substitute for structured matrices arising in ML applications (e.g. circulant matrix used for the convolution operation).
The dataset is an extension of CLEVR using simple motions of primitive 3D objects to produce videos of primitive actions (e.g. pick and place a cube), compositional actions (e.g. "cone is rotated during the sliding of the sphere"), and finally a 3D object localization tasks (i.e. where is the "snitch" object at the end of the video). The construction of the dataset focuses on demonstrating that compositional action classification and long-term temporal reasoning for action understanding and localization in videos are largely unsolved problems, and that frame aggregation-based methods on real video data in prior work datasets, have found relative success not because the tasks are easy but because of dataset bias issues. - p9 phenomenon -> phenomena; the the videos -> the videos; these observation -> these observations; of next -> of the next; in real world -> in the real world This paper introduces a new synthetic video understanding dataset, borrowing many ideas from the visual question answering dataset CLEVR. Due to the inherent biases in available action recognition datasets, models that simply averages video frames do nearly as well as models that take temporal dependencies into account. They further conduct a variety of experiments to benchmark state-of-the-art video understanding models and show how those models more or less struggle on temporal reasoning. The authors argue that since current video datasets are heavily biased over static scenes and object structures, it is unclear whether modern spatial-temporal video models can learn to reason over temporal dimension. My primary concern is to what extent can the new dataset (CATER) add to existing video datasets that are also explicitly designed for long term spatial-temporal reasoning, such as video VQA datasets TGIF-QA[1]/SVQA[2].
The method is based on an unrolled algorithm, which is motivated by the inclusion of three inductive biases / constraints important underlying RNA folding. As these scores may not obey the rules of RNA folding, a second post-processing network is trained end-to-end together with the "Deep Score Network" to enforce constraints. (8) is cool! RNA Secondary Structure Prediction by Learning Unrolled Algorithms This paper proposes E2Efold, which is an RNA secondary structure prediction algorithm based on an unrolled algorithm.
(approach - attribution methods) An information bottleneck is introduced by replacing a layer's (e.g., conv2) output X with a noisy version Z of that output. Summary --- (motivation) Lots of methods produce attribution maps (heat maps, saliency maps, visual explantions) that aim to highlight input regions with respect to a given CNN. By choosing reasonable beta (similar to variational information bottleneck), the proposed approaches are capable to highlight key regions used for prediction. To the best of my knowledge, the proposed method is sufficiently novel and the application of the information bottleneck framework to pixel-level attribution has not been reported before.
Please provide clarifications. In experiments: FR rooms experiments are interesting, in the visualization of the option policies, do the figures here show the flattened policy of the options? Moreover, the larger scale ATARI experiments do not seem to include any multi-task aspects at all, with options immediately being learnt on the target task. Please clarify. -"We can obtain the estimate for equation 1 by averaging over a set of near-optimal trajectories" The aim as states is to learn options that are capable of generating near-optimal trajectories (by using a small # of terminations). The authors utilize demonstrations collected beforehand and train an option learning framework offline by minimizing the expected number of terminations while encouraging diverse options by adding a regularization term. There are a number of option induction approaches that explicitly focus on reusability of options - see e.g. [1], [4]. With the motivation of this paper, I am unable to convince myself about options being "reusable" for multi-task here. Overall: An interesting objective function, Learn not only option set but also the number of options needed and incrementally learn new options.
The proposed techniques use both the graph structure, and the current classifier performance/accuracy into account while (actively) selecting the next node to be labeled. 1) The propose to sample nodes nodes based on "regional" uncertainty rather than node uncertainty 2) They use an variant of pagerank to determine nodes that are central, and hence most likely to affect subsequent classification in graph convolution classifiers. Second, it proposes to adapt the page rank algorithm (APR) to determine which nodes are far away from labeled nodes. There are experiments to show effectiveness of these techniques, and there are some interesting observations (for example, that the APR technique works better for smaller sample sizes, while the regional uncertainty methods do better for larger sampling fractions.). To decide which nodes in a graph to label during the active labeling, the paper proposes two approaches. 2.2. Figure 1: according to the caption, APR should point to node 15, but in the figure it points to node 14. - Table 2: unclear: "accuracy without content" 1.2. "We have here shown that the accuracy of AL when uncertainty is computed regionally is much higher than when either local uncertainty or representative nodes are used", this is not the case on CiteSeer in Table 1 This is supported neither in Table 1 nor Table 2, where APR is frequently not highest performing
I've given a weak accept, conditioned on being provided more evidence regarding 1) comparisons to simple differentiable alternatives, 2) sample efficiency of the RL method, and 3) basic analysis of the weighting function. * This is a minor issue, but this pushes the burden of interpretability further up to the black-box sample weighting function. * Other comments/requests: * While the use of RL is certainly motivated in order to solve the problem in an unbiased way, it would be nice to see a comparison to a differentiable approximation as a baseline? * Pros: * Considers an interesting dataset subsampling variant of the sample weighting meta-learning problem. * Though there is discussion of the complexity of the overall method it would be nice to see a discussion and figures related to the sample efficiency of REINFORCE? By analogy, in this paper, the samples are taken from the entire dataset (i.e. the dataset is subsampled), while the validation loss is effectively an imitation loss formed by treating the black-box model predictions as a target. The contribution of this paper is thus the use of RL for meta-learning how to subsample a larger dataset in order to maximize some validation loss. A few ideas: * Randomly sample a (possibly large batch) and learn to weight it (closely related to the straight through estimator) This work is closely related to Ren et al. 2018 [1], which proposes to meta-learn how to weight samples in a batch so as to maximize performance on a validation set.
The authors find that (1) different proposed measures of selectivity are not consistent and (2) units identified as selective cannot be considered object detectors due to the high false alarm / low hit rates, analyzing a large number of selectivity measures. According to that study, almost 60% 0f all fc8 units are "object detectors", with very high conherence between humans and selectivity metrics. Previous works have used different measures of selectivity (with sometimes contradictory results), and the authors investigate the degree to which these units qualify as "object detectors". The paper makes an empirical claim that CNNs for object recognition do not contain hidden neuron which is highly selective to each class, mainly based on three aspects: (a) metrics related to the maximum informedness, (b) jitterplots of activation data, and (c) a user study assessing whether generated images maximizing a given unit is perceptible to the user. It is also noticed that the existing metrics for selectivity do not adequately discriminate highly selective units in CNN. Overall, I think that the authors have presented a strong meta-analysis and compelling argument for further study in rigorously identifying the presence (or lack thereof) of selective units in neural networks and the degree to which they may be considered "object detectors. The paper empirically studies the category selectivity of individual cells in hidden units of CNNs. It is a sort of "meta-study" and comparison of different metrics proposed to identify cells with a preference for a specific target category. This work investigates the collection of methods that have been proposed to find units in neural networks that are selective for certain object classes. In the words of the paper, the "selective units are sensitive to some feature that is frequently, but not exclusively associated with the class" - I thought this is the standard majority view, not a surprising finding. If one collects statistics over >2000 units of a fully connected layer, one might as well do the complete job and use all 4096 units. - the lack of highly selective units in CNNs
The paper "Beyond Classical Diffusion: Ballistic Graph Neural Network" tackles the problem of graph vertices representation. While most existing works rely on classical random walks on the graph, the paper proposes to cope with the "speed of diffusion" problem by introducing ballistic walk. al, 2016 in that the authors want to define a propagation filter for graph neural networks. More datasets are needed to thoroughly verify the performance of the proposed ballistic graph neural network.
- I am not sure if *CONF* is the right venue for this work The paper presents three contributions: (a) the observation that there's train-to-test leakage in many graph classification datasets (under isomorphism equivalence), (b) what appears to be a theoretically motivated way of improving scores on such datasets, by focusing on solving the examples that are isomorphic with training instances, and (c) a recommendation to remove such leakage from test sets. In particular, the paper analyzes the amount of isomorphic graphs in 54 graph datasets and evaluates the performance of three graph classification methods under two isomorphism settings. Moreover, being able to capture the equivalence relation can be important for various graph learning tasks, e.g., to facilitate that two topologically equivalent graphs are be classified similarly. However, the main assumption of the paper -- which equates the quality of a graph learning benchmark with the amount of isomorphic graphs that it contains, i.e., the lower the better -- seems questionable.
PAPER SUMMARY: This paper proposes a fast inference method for Gaussian processes (GPs) that imposes a sparse decomposition on the VI approximation of the posterior GP (for computational efficiency) using the KNN set of each data point. This is, however, a somewhat strange direction which, to me, seems to raise extra issues that could have been avoided if one follows the conventional VI approximation: (1) As the posterior surrogate is now directly over f instead of f_I, the number of variational parameters is now proportional to the data size which requires several (redundant) extra approximations including armortized inference & the lower-bound on the entropy term that admits a sparse decomposition. REVIEW SUMMARY: This paper adopts a VI approximation that deviates from the conventional form of (Titsias, 2009) to encode the KNN information, which causes extra computational issues (that incurs extra approximations). [**] A distributed variational inference framework for unifying parallel sparse gaussian process regression models (ICML-16) While I understand that this is in exchange for the ability to encode local information (via KNN) within the surrogate posterior, it is not clear to me why do we need to incur all these computational issues to incorporate such local information.
By varying the homotopy parameter, one can construct a continuous path from a supposedly easier to solve optimization problem to the problem of interest. and (3) the assumption of fixing the homotopy parameter in the theorem on the non-convex case directly violates the intention of the algorithm. When L = 0, we recover the original optimization problem.
This paper proposes an adversarial detection method via Fourier coefficients. This paper presents a new discriminator metric for adversarial attack's detection by deriving the different properties of l-th neuron network layer on different adv/benign samples.
Based on the well-known universal approximation property of FNNs, the paper shows that their RNN-based filter can approximate arbitrarily well the optimal filter. The paper then follows this framework by constructing deep neural network mapping from "observations so-far" to "sufficient statistics" and by universal approximation theorem this is possible. This paper shows that RNN (of infinite horizon) can be universal approximators  for any stochastic dynamics system. The paper shows that recurrent neural networks are universal approximators of optimal finite dimensional filters. Hence the paper's title "RNNs are universal filters" and the way it is presenting are confusing.
The main contribution is the introduction of a differentiable top-K region proposal that allows to train the whole model with only a supervision of the total number of instances (and their class) in the image. Unfortunately, their tasks seem quite easy, and it is hard to assess the impact of their method when working with more real-world data-sets, where the number of instances of every class is more loosely defined (we could always describe more objects in a real image from the COCO dataset for example). It seems of great importance to evaluate the limitations of their method in this direction, as the source of supervision might be too weak in the cases where the generated dataset might not have all the combinations of number of instances per image (as the cluttered MNIST has given that it's procedurally generated).
- A comparison is made between Theorem 10 & Corollary 11 in the paper to Theorem 1 in Cohen et al. However, it is not clear how the result in the paper is better or even equivalent to the one in Cohen et al. D_{MR} robustness seems to be an approximate notion of robustness, while the result in Cohen et al gives perfect robustness within a ball of a certain radius.
This paper generalizes the min max formulation of adversarial training, and proposes a formulation that encompasses adversarial training of an ensemble, robustness to universal adversarial examples, and robustness to non-adversarial transformations. The paper studies how a min-max framework can incorporate different tasks related to adversarial robustness.
Contributions: 1. Derive a generalization bound on the performance of adversarially robust networks  that depends on the margin between training examples and the decision boundary. Moreover，the authors also conducted the experiment to show that stronger robustness over adversarial examples can lead to zero concentration of margin. The paper proposes to explain a phenomenon that the increasing robustness for adversarial examples might lead to performance degradation on natural examples. This proved that strong robustness on adversarial examples might reduce the generalization. The paper studies this hypothesis by deriving a bound on the generalization error based on the margin between the samples in the training set and the decision boundary. ===== Review ===== The problem that the paper addresses is very significant to the robust optimization field and the study of adversarial robustness in neural networks.
The paper proposes to maximize improve generalization in meta-learning by learning discrete codes via a mutual information maximization objective. Another reservation I have is the use of mutual information between encoder representations and class labels as a loss function (Eq. 1).
One big issue with the experiments section is the A Posteriori Concept Acc metric as it only measures the separability and consistency of discovered concepts; Would the concepts learned without concept loss qualitatively very different? It's very easy to assume the introduced training procedure to extract separable but meaningless concepts(i.e. the excerpts of a concept are separable from that of other concepts and are consistent with each other in the eyes of the network while they are not consistent with a concept in the eye of human rationale; Authors should make a much more comprehensive discussion of what already exists in the concept-based interpretability literature and make the contribution of this work more clear (unsupervised concept extraction for a self-interpretable model instead of post-hoc interpretations). Self-interpretability is achieved by a two-stage model: First, a concept-extractor finds the related pieces of consecutive words (excerpts) in a given text that are related to a concept among a set of given concepts (if any), then the model makes its predictions solely based on the presence or absence of concepts (binary). The system is evaluated under two measure: 1) classification accuracy and 2) concept accuracy. The model, Explaining model Decision through Unsupervised Concepts Extraction (EDUCE), is applied to a text classification task, while the authors argue in the appendix that this is also applicable to a wider problem, such as image classification.
However, since the generalization bound in Arora et al (2017) is based on a different definition of generalization error where empirical distributions are considered for both discriminators and generators, the comparison seems not fair. b)Theorem 2.3 is a general statement but it is followed by Corollary 3.3 which is a very specific generalization bound. For this generalization error, the authors give both bounds for a fixed generator and a uniform bound for a class of generators. 4) Generalization bound for fixed g: Unfortunately, the novelty of these generalization bounds are very limited as they are a direct application of known generalization bounds in the supervised settings. I have increased my rating.) In this paper, the authors study the generalization bound for GANs based on a new definition of generalization error where the distribution corresponding to the generator is assumed to be known for each generator (i.e., there is no empirical distribution for generators). 5) Generalization bounds for all generators: Again, here the novelty and final result is very limited since the bounds achieved by a union bound arguments and does not really go beyond that. This paper proves generalization bounds for GANs. I think the paper can be improved significantly in several ways: 1- Writing: The first two sections are relatively well-written.
The authors show 1 of mode captured on 2D Grid and 2D Ring using the VEEGAN method. In order to address this problem, the paper proposed a framework called LDMGAN which constrains the generator to align distribution of generated samples with that of real samples in latent space by introducing a regularized AutoEncoder that maps the data distribution to prior distribution in encoded space. My Take: This paper's only point of novelty over a vanilla VAE-GAN implementation is the inclusion of the KL(E(G(Z)) || p) term in the generator loss, which is very similar to the idea behind VEEGAN. LDMGAN: Reducing Mode Collapse in GANs with Latent Distribution Matching
The output of the encoder is a style embedding that helps differentiates different modes of image synthesis. Thirdly, style encoder and generator are simultaneously finetuned. Compared to the direct baseline BicycleGAN, the training procedure proposed in this paper replaces the simultaneous training of the encoder E and the generator G with a staged training that alternatively trains on E and G and then finetune them together. Secondly, the generator is trained on a supervised image translation tasks: the original image and the style, extracted from the target image, are fed to the generator, and the output is a translated image. When training the generator for image synthesis, the input combines an image in the source and a style embedding, and the loss is essentially the sum of image conditional GAN loss and perceptual loss.
Moreover, for generalization one can use attention dropout which is simpler instead.] This paper proposes two sparsifying methods of computing attention weights, dubbed sparsemax and TVmax, which appear to slightly improve objective and subjective image captioning scores. The authors then compare their TVMax approach with softmax and Sparsemax attention for image captioning and show improvements on the MSCOCO and Flickr datasets. Compared with the softmax function, the sparsemax[1]  and the TVmax are able to sparse the visual attention very well. [1]From Softmax to Sparsemax: A Sparse Model of Attention and Multi-Label Classification. The conventional softmax approach to attention weights should be capable of producing attention weights near zero, which would be effectively sparse, especially if the pre-activations, z_i, in equation (1), are allowed to have a large enough range. The main idea is to augment the Sparsemax projection loss with a Lasso like penalty which penalizes assigning different attention probabilities to contiguous regions in the image. To this end the authors first apply full attention where the probabilities are computed using softmax before applying the recently proposed Sparsemax - which essentially computes probabilities from scores by performing a projection on to the probability simplex. The idea of applying additional structural constraints on the sparsity structure induced by Sparsemax is a cool idea, and I like the idea of incentivizing contiguous pixels to have similar attention probabilities.
How do you think? This paper proposes Surprise Minimizing RL (SMiRL), a conceptual framework for training a reinforcement learning agent to seek out states with high likelihood under a density model trained on visited states. However, the statement that SMiRL agents seek to visit states that will change the parametric state distribution to obtain higher intrinsic reward in the future is controversial (see e.g. at the end of Section 2.1), because optimizing a non-stationary signal is outside the scope of the problem formulation. To formulate this notion of surprise as a meaningful RL problem it is necessary to include some representation of the state likelihood in the agent's state representation; so that the agent can learn how it's actions affect not only the environment state but it's own potential for future surprise.
The paper proposes an approach to learning domain invariant representations using the adaptive decomposition of the convolutional filters. The authors argue that the basis learned for each domain can be understood as correction/alignment mappings to bring together the representations of each domain. The authors also show that plugging the decomposition scheme into existing CNN based unsupervised domain adaptation algorithms results in consistent improvements across methods and datasets. This paper introduces a way to decompose features for better domain adaptation via learning domain-invariant representations. Basically, it has been shown that invariant representations provably hurt generalization on the target domain when the marginal label distributions are different between the source and target domains.
The experiments on the Atari games shows that by using tensor regression to replace the dense layer of the neural nets and using K-FAC for the optimization, one can reduce around 10 times of parameters without losing too much of performance. It would also be interesting to compare the distributions of the eigenvalues of the tensor layers versus the dense layers in deep RL which may provide insights on the achieved savings and the compression trade-off. These methods include tensor regression, wavelet scattering, as well as second-order optimization (K-FAC). With the combination of tensor regression layers and K-FAC, the proposed methods give comparable performance on several Atari games against 2 other methods, SimpPLe and data-efficient Rainbow, while using 2 to 10 times fewer coefficients than data-efficient Rainbow.
In response to such observations, the paper proposes Fixed Multihead Attention, where the constraint that `head_size * number_of_heads = embedding_size` in standard multihead attention is lifted; and it allows for using more attention heads without making each head smaller. This work discusses how to set the projection size for each head (head size) in multi-head attention module, especially Transformer. For example, several recent works argues for specialized attention heads, i.e., each head has specific "job," which may not require it being very expressive [1, 2]. It argues and formally establishes that (1) the expressivity of an attention head is determined by its dimension and (b) fixing the head dimension, one gains additional expressive power by using more heads.
It is not clear if the paper is presenting "expected gradients" or existing attribution priors. Is it just smoothing? The authors proposed the attribution priors framework to incorporate human domain knowledge as constraints when training deep neural networks. I agree (and personally like) the motivation that a method is needed to align a model's behavior with human knowledge or intuition -- model's behavior may be explained by feature attribution methods while making models accept human knowledge is challenging. Summary. The paper improves the existing feature attribution method by adding regularizers to enforce (human) expectations about a model's behavior. I am also not clear on where the image attribution prior comes from for the image task. For example, Figure 1 (left) shows an attribution map that highlights multiple intermittent regions from which I cannot understand its behavior. Also, the introduced human priors are similar to general regularization conventions, i.e. a penalty of smoothness over adjacent pixels is commonly used in the CV community. The structure of the paper is strange because it discusses attribution priors but then they are not used for the method. Attribution priors as you formalize it in section 2 (which seems like the core contribution of the paper) was introduced in 2017 https://arxiv.org/abs/1703.03717 where they use a mask on a saliency map to regularize the representation learned. This is a general framework that the users can define different attribution priors for different tasks. For example, in this work, the authors proposed three reasonable priors for image input, graph data, and clinical medical data. It would be nice to see how integrated gradient method perform in the three experiments (image, drug data, mortality prediction), does the expected gradient method always outperform? I am concerned that only a limited set of expert-invented human priors can be used in this approach. Most of the experiments revolve around existing attribution prior methods. However, I am concerned about what information end-users are expected to obtain from the model. The expected gradient method does indeed also performed better than the integrated gradient method in the benchmark (see Table 1.) The results in all three experiments are impressive.
3) The network is applied on unit cells of crystals or repeated unit cells from a dataset of crystal structures and the network is shown to be able to accurately reconstruct the 3D density maps, predict the number of atoms in the cell and perform fairly well in their classification into atomic types. The authors propose an auto-encoder framework for encoding the 3D locations of atoms in the crystal to a latent representation and then decoding that representation back into 3D structure. 2) A decoder network that first estimates a 3D density map from the latent vector using upsampling and convolutions and then classifies the atom type (atomic number) per voxel using a 3D segmentation. a) a VAE that builds a compressed latent space representation of a crystal and 3. Joint VAE + UNet training: The joint training of the encoder-decoder VAE and the 3D segmentation network results in a decoder that can reconstruct atom locations and atom types, being robust to mistakes in the density map reconstruction. b) a UNET for segmenting the latent space into atoms and assigns each atom to its atomic number. Also, it is not clear whether the 3D representation is better than 1D or 2D representations especially since there have been many new 1D models that perform very well for tasks like molecular property prediction (For example All SMILES VAE https://arxiv.org/abs/1905.13343 ). The paper's contributions can be summarized as follows: 1) A data representation that converts the 3D atom locations to a 3D voxel density map, so that they can be encoded by a standard 3D convolutional network. Also, there is no justification of the advantages of using a voxel-based density map representation along with 3D convolutions vs, for instance, a graph representation along with graph convolutions (Xie & Grossman, [3]) or a point cloud GAN (Achlioptas).
However, I have several concerns: *The authors verify that the running average is the main culprit of vulnerability to adversarial attack, but provide no further investigation of why this happens. Review: This paper investigates the reason behind the vulnerability of BatchNorm and proposes a Robust Normalization.
The authors do not change the GCN but extend the self-training portion as per the prior GCN paper by introducing Dynamic Self-Training that keeps a confidence score of labels predicted for unlabelled nodes. You may include other additional sections here This paper propose to modify the existing work [1] of self-training framework for graph convolutional networks. #Summary This paper proposes a generalised self-training framework to build a Graph Neural Network to label graphs. 4. If we had soft-labelling or uncertainty on which label each node has, how would the dynamic self-training be changed? The paper proposes an approach for learning graph convolutional networks for inferring labels on the nodes of a partially labeled graph  when only limited amount of labeled nodes are available. The proposal is inspired from Graph convolution Networks with the idea of overcoming the major drawback of these models that lies of their behavior in case of limited coverage of the labeled nodes, which implies using deeper versions of the model leading at the price of what the authors call the over-smoothing problem. Of importance is the dynamic nature of the self-training.
Finally, they propose an actor-critic algorithm to solve the Wasserstein RMDP problem (Sec. 3) and evaluate its performance using simple experiments (Sec. 4). Similar to the robust MDP work, they also show that this robust RL problem has a robust optimal Bellman operator, and the optimal value function can be computed using  robust policy iteration, which can be extended to model-free actor critic algorithm when state/action spaces are large/continuous. Summary: In this paper, the authors study the problem of robust MDP (RMDP), where the feasibility set is defined as a Wasserstein ball around the reference transition probability.
(Though I am less positive due to the concern of novelty raised by other reviewers.) Summary: This paper presents an efficient stochastic neural network architecture by directly modeling activation uncertainty and adding a regularization term to encourage high activation variability by maximizing the entropy of stochastic neurons. The idea of producing distributions in each layer (i.e., using stochastic layers) is not new and is closely related to the work on local reparameterization trick and variational dropout [1] (predecessor of the cited sparse variational dropout), and various works that directly model neurons as distribution [2, 3].
This subset of active modules is chosen through an attention mechanism. In general, I like the idea of making recurrent cells operate with nearly independent transition dynamics and interact only sparingly through the attention bottleneck. This paper proposes a neural network architecture consisting of multiple independent recurrent modules that interact sparingly.
as the main contribution. However, I do not think the metric they introduce is good enough to be recommended in future work, when comparing tunability of optimizers (or other algorithms with hyperparameters). A good prior of one optimizer could significantly affect the HPO cost or increase the tunability, i.e., the better understanding the optimizer, the less tuning cost. The tunability of the optimizer is a weighted sum of best performance at a given budget. The assumption of independent hyperparameters might be fine for black box optimization or with the assumption that practitioners have no knowledge of the importance of each hyperparameter, then the tunability of the optimizer could be different based on the prior knowledge of hyperparameter and their correlations. In addition, the proposed stability metric seems not quite related with the above intuitions, as the illustrations (1.a and 1b) define the tunability to be the flatness of hyperparameter space around the best configurations, but the proposed definition is a weighted sum of the incumbents in terms of the HPO budgets. I enjoyed reading the submission, which is very clearly written, but due to the relatively limited value of the contributions, and excessive focus on the tunability metric which I do not feel is giustified, I slightly lean against acceptance here at *CONF*.
The idea is borrowed from the biological domain, and then transformed to a spatial shuffling layer which can shuffle the feature response at each convolutional layer. One minor thing, in the main paper, the abbreviation for spatial shuffled convolution (ss convolution) is mentioned multiple times.
When designing the annotation process, it does not seem like a good idea to use CommunityMPA over asking for a minimum of 40 annotations per annotator and use MPA. This study extends MPA to CommunityMPA by including information about the annotators' hierarchical community profiles from Simpson et al. (2011, 2013): spammers, adversarial, biased, average, and high-quality players. Different from the conventional crowdsourcing approaches that concentrate on classification tasks with predefined classes, the proposed Community MPA, as an extension of MPA can address anaphoric annotation well where coders relate markables to coreference chains whose number cannot be predefined. This paper extends the unpooled mention pair model of annotation (MPA) (Paun et al., 2018b) with the hierarchical priors (e.g., mean and variance) on the ability of the annotators. Authors demonstrate that on a recent large-scale crowdsourced anaphora dataset, the proposed Community MPA works better than the unpooled counterpart in conditions of sparsity, and on par when enough observations are available. The experimental results show that, when the data is sparse, the proposed method (CommunityMPA) worked better than MPA on Phrase Detectives 2 corpus in terms of mention-pair accuracy, silver-chain quality, and the performance of the state-of-the-art method trained on the aggregated mention pairs.
The authors then show that the preconditioning matrix of RMSProp and Adam can be used as norm inducing matrices for second order trust region methods. This ellipsoidal trust region method is empirically compared with first order gradient methods and spherical second order trust region methods. One can show that the empirical Fisher is not an accurate curvature matrix in general, and so there is no reason to believe this would in fact enforce the proper ellipsoidal trust region for the method? This approach is motivated by the adaptive gradient methods and classical trust region methods. My understanding is that the paper does not claim to deliver some great results here and now but instead suggest a promising direction ("that ellipsoidal constraints prove to be a very effective modification of the trust region method in the sense that they constantly outperform the spherical TR
3. There are a lot of recent attack methods proposed to improve the transferability of adversarial example, e.g., "Improving transferability of adversarial examples with input diversity" (Xie et al., 2019); "Evading defenses to transferable adversarial example by translation-invariant attacks" (Dong et al., 2019). Their evaluation setup largely follows the style of Liu et al., but they construct a different subset of ILSVRC validation set, and some of the model architectures in their ensemble are different from Liu et al. Their results show that by including the distilled logits when computing the gradient, the generated adversarial examples can transfer better among different models using both single-model and ensemble-based attacks. ------------- This paper proposes an attack method to improve the transferability of targeted adversarial examples.
Given a set of expert demonstrations, this work provides a policy-dependent reward shaping objective that can utilize demonstration information and preserves policy optimality, policy improvement, Finally, they only test on mujoco tasks which are very specific tasks with deterministic dynamics and very dense rewards  around states visited by the optimal strategy so initializing with an expert policy that is learned from demonstrations of a similar network of course helps. The authors use a regularized reward function that minimizes the divergence between the policy of the expert and the one followed by the agent. The end of the related work section is not very clear, you say these methods are problematic because "the adopted shaping reward yields no direct dependence on the current policy" but there's no explanation or motivation for why that would be a problem. It combines both an augmented reward for minimizing the KL between the policy and the expert actions as well as directly minimizing that KL in the policy. Technical concerns: The stochasticity assumption of expert policy in Asm. 1 can be contradicted with that expert policy is optimal in policy invariance proof. and the convergence of policy iteration at the same time, under the assumption that expert policy is optimal and stochastic. The main advantage of the proposed method is that the reward shaping function is related to the current policy. Converting the tasks to sparse reward in this way makes them partially observable, and then potentially the expert demonstrations are required to overcome that partial observability. Also, the proposed solution here is equivalent to regularizing the MDP with a KL divergence  w.r.t. to an initial policy that would be the one of the expert. This work also developed a practical expert policy support estimation algorithm to measure the uncertainty of expert policy. This paper discussed how to use the demonstration information to do exploration and maintain policy invariance at the same time, with a relatively strong assumption. Using the framework from SAC, the algorithm is shown to converge to the optimal via policy iteration, in tabular case.
This needs a citation. Similarly, "AdaX outperforms various tasks of computer vision and natural language processing and can catch up with SGD"; as above, I'm unaware of work (other than theoretical) that shows that SGD significantly outperforms Adam in deep neural networks. Statements that need citation or revision: - "Adaptive optimization algorithms such as RMSProp and Adam... I am a bit confused about the changes proposed to Adam that gives rise to the AdaX algorithm. For this, it would be interesting to see how the trajectories of second-order momentum estimates of Adam, AMSGrad, Ada-X are different.
This paper proposes to provide a novel gradient-based meta-learning framework (Meta-Graph) for a few shot link prediction task. Moreover, it would be nice if the authors could also provide some ideas for future research directions, such as the prospects of using their approach for improving link prediction models and incorporating Meta-Graph in other domains like molecules structure. My concerns are as follows: •    I am wondering if you can adopt R-GCN [1] instead of the GCN model for extending the Meta-Graph to multi-relational graphs? More specifically, they generate an effective parameter initialization for a local link prediction model for any unseen graph by leveraging higher-order gradients and introducing graph signature function into graph neural network framework.
In this way, an estimate of the output distribution is considered during the extraction of the representative subset of the testing data. More explanation is needed. 2. In Table 2, the authors try to compare the output distribution. The paper presents a new approach to create subsets of the testing examples that are representative of the entire test set so that the model can be tested quickly during the training and leaving the check on the full test set only at the end to validate its validity. The key idea is to create the smaller possible subset with the same or similar coverage (in the paper the neurons coverage is considered) and output distribution, maintaining the difference below a (small) threshold. Furthermore, the result does not present a strong success: the error of output distribution is much worse than the compared work. This work tries to build a sub-pile of the test data to save the testing time with minimum effect on the test adequacy and the output distribution. The whole process is divided into two phases, the first is to create a first subset using HGS, the second refines this subset in order to achieve the desired precision in the output distribution. To better demonstrate the change between raw testing set and proposed subset, I think that it could be better to present the metrics of distribution or the accuracy of each class instead.
The authors observed that the wider deep neural networks can learn much rich representative features than shallower deep neural networks while both networks show similar level of the test performance. They find that wider networks learn features in the hidden neurons that are more "interpretable" in this visualization framework. Additionally, they also notice that fine-tuning a _linear model_ using the learned features for the wider networks provide better accuracy for new (but related) tasks over the shallower counterparts.
With a content-based "routing" technique ("group/cluster attention" may be a more accurate term to differentiate with the dynamics of Capsule Networks), the time steps of each layer form different clusters, and the self-attention mechanism is only performed within each such cluster. In particular, although the routing mechanism avoids computing A, having to deal with each of the n clusters means that one will need to process the cluster-based attentions sequentially (and, as mentioned, you have 8 heads for local attention and 8 for routing attention, which I think are also processed in two steps? When combined with prior work on local attention (i.e. half of the heads are local attention, the other half are the newly proposed routing attention), but model is found to outperform, or be on par with, existing Transformer models despite generally being smaller. - What is the *actual* running time/memory for local/routing/full attention layers? This paper proposes content-based sparse attention to reduce the time/memory complexity of attention layers in Transformer networks. However, since the cluster centroids are learned parameters, does this mean the trained routing Transformer cannot easily generalize to different sequence lengths easily? 8. What is the motivation of using half of the heads for local attention and the other half for routing attention (cf. However, the cluster attention seems to group the words together, and there is no cross-cluster attention (i.e., the graph is broken into smaller components). Why not just use all of the heads with routing attention? routing attention via the JSD score. How does the model do if it only uses routing attention? Summary: This paper proposes a new mechanism for computing the attention scores efficiently in Transformers to avoid the quadratic computational cost in conventional Transformers (i.e., when computing the QK^T for self-attention). 11. Except the computational concern, I still don't quite exactly see the motivation behind clustering attention. The routing Transformer seems to be able to do very well on the WikiText-103 word-level language modeling task.
This paper proposes a very interesting idea of loss function optimization. 1) The experiments focus almost entirely on the Baikal loss (a particular loss function found once when running EC on mnist), and do not analyze the overall behavior of EC for loss functions. The evaluation of these loss functions is performed on the MNIST and CIFAR-10, and according to the results they converge faster, towards lower test error and need fewer samples to obtain results similar to the cross-entropy loss. Why should our goal be to find loss functions that lead to fast optimization, instead of loss functions that lead to models that generalize best?
While the experimental results demonstrate some interesting phenomenons such as combining vp and the primal form of KL divergence achieving the best performance and taking minimum over Q functions outperforming using a mix of maximum and minimum over the Q functions, I believe the paper would be greatly improved if the authors can provide a new offline RL method based on the BRAC that can achieve better performance than current approaches and is less incremental than simply combining vp and KL divergence. Though BRAC summarizes offline RL methods in a neat way, it would be more technically sound if a general theoretical analysis/insights of offline RL algorithms can be offered in the paper, e.g. showing the reason that vp is outperforming pr through convergence analysis in the tabular case. Overall, this paper could be an interesting summary of prior works in offline RL and provide some empirical insights on the effectiveness of each building block in the previous approaches, though it neither offers theoretical explanations nor proposes a new offline RL algorithm that outperforms the existing methods under the BRAC framework. My score would be increased given some technical insights or some promising results in a relatively novel offline RL algorithm in the author's response. This paper presents a framework for evaluating offline reinforcement learning (RL) algorithms. I am leaning to accept the paper because (1) the experimental design is rigorous and the results provide several insights into how to design a behavior regularized algorithm for offline RL. The paper introduces a general framework for behavior regularized actor-critic methods, and empirically evaluates recent offline RL algorithms and different design choices. This paper proposes a unifying framework, BRAC, which summarizes the idea and evaluates the effectiveness of recently proposed offline reinforcement learning algorithms, specifically BEAR, BCQ, and KL control. Results from a  thorough series of experiments are presented which suggest that certain details of recently proposed RL methods are not necessary for achieving strong performance. The authors generalize existing offline RL approaches to an actor-critic algorithm that regularizes the learned policy such that it can stay close to the behavior policy. These results suggests that some of the complexity in RL design can be ignored. I commend the authors for performing a valuable test and comparison of existing offline RL methodology. Are they negative so not reported in the figure or just missing? Minor comment: Error bars should be added to the all the bar plots. **UPDATE** After reading the rebuttal, I think my concerns are addressed and thus I updated my rating. 3. Do you think the conclusion will change if you use training datasets of different size (e.g. much less than 1 million)? A challenging open problem is a good thing! is an challenging open problem."  Unfortunately?!
<Strengths> + This paper proposes a new dynamic model named hierarchical multi-task dynamical systems (MTDSs) as a latent sequence model that enables users to directly  control the output of data sequence via a latent code z. This paper proposes to add a latent variable to a dynamical, thereby encoding the notion of "task", eg, in Mocap, the latent variable could encode the walking style in an unsupervised manner. Another strength of the model is flexibility as different base models and latent models can be chosen.
To do this they assume they are given batches of OOD examples or batches of in-distribution examples, and they detect whether the batch is in- or out-of-distribution. The paper first empirically demonstrates that the likelihood of out-of-distribution data has a larger difference between training mode and testing mode, then provides a possible theoretical explanation for the phenomenon. The likelihood difference itself seems like it could be a good indication for OoD detection. This is consistent with your batch normalization experiments: in training mode, the likelihood is computed from mean activations over a batch of OoD samples, several of which probably contribute to the low likelihoods. The paper makes the observation that likelihood models trained with batch norm assign much lower likelihoods to "training batches" of OoD data (batch norm statistics computed over over minibatch) than evaluation batches of OoD data (batch norm statistics over entire training set). If you take a likelihood model and evaluate on 64 samples from SVHN, you are all but guaranteed to sample a sample with *exceedingly* low likelihood, which dominates the mean statistic, making it possible to separate SVHN batch from CIFAR10 batches.
This paper presents a new reversible flow-based graph generative model wherein the whole graph i.e., representative attributes such as node features and adjacency tensor is modeled using seperate streams of invertible flow model. [Page 7, Table 2] My first concern is: does the reconstruction performance matters in the graph generation case? 3. As in MolGAN, the direct generation of adjacency tensor leads to training with fixed size graphs i.e., through the addition of virtual nodes.
In the M-product notations, everything seems to be as neat as matrix operations. Pros: 1, The generalization brought by M-product seems to be general as it includes quite a few graph convolution elements for 3D tensors in a natural way. This paper presents a M-product based temporal GCNs to handle dynamic graphs. Overall, I think this paper make a few contributions to advocate tensor M-product. The paper proposed a new type of graph embedding technique for dynamic graphs based on tensor representation (node x feature x time).
Authors define the minimum reconstruction error from the embedding as the global measure in reflecting the global structure of the data similar to PCA. A measure of global accuracy is proposed to reflect the global accuracy of the embedding. What's currently used is only intrinsic---the embeddings are trained to preserve relative distances and are evaluated on a trade-off between local accuracy and the defined global score.
The notion of feature robustness, which is a notion the paper proposes, connects the flatness measure to generalization error. The authors combine their notion of feature robustness with epsilon representativeness of a function to connect flatness to generalization. This seems to be a missing step in relating flatness/feature robustness of a layer to the generalization of the whole network. Otherwise, it is hard to claim that this paper connected the modified flatness measure to generalization error. This paper describes a connection between flatness of minima and generalization in deep neural networks. I believe this paper is able to once again confirm the relationship between flatness and generalization in an empirical manner with their layerwise measure of flatness. al. Moreover, the experiments are very limited and I suggest authors to look at more controlled setting to verify the relationship of their measure to generalization. On the theoretical side, I think the major issue is that the paper cannot connect the measure to generalization properly and ends up decomposing the test error to the sum of the robustness measure and the gap between test error and the robustness measure which is not informative.
This is a paper about a very interesting topic, involving both learning (in a supervised way) to induce a causal graph and taking advantage of it in a goal-conditioned policy. During the induction phase, the agent incrementally updates the predicted causal graph through each interaction using an attention-based edge decoder. I was a bit disappointed to see that training is mostly supervised (both providing the ground truth causal graph and an oracle policy as target) but on the other hand it is impressive to obtain these results with raw images as input and the comparative results are good. - train an policy \\pi_G attending over the causal graph to solving tasks. The key to the improvements is in the iterative prediction of the causal graph (for F) and in having attention over the causal graph (in \\pi_G). I feel this paper makes strong assumptions on both the induction and inference stages, as well as the structure of the causal graph, which greatly limits the applicability of the approach.
I would give the paper a higher score if the authors showed that GraphMix(Graph U-net) was an improvement over Graph U-net, or if it was made more clear that some substantial benefit is derived from using Mixup features. I would be curious to see the t-SNE visualization of the GraphMix(GCN) \\ Mixup features in order to determine how much of the cluster separation is due to Mixup specifically. In particular, the proposed GraphMix model has two components, one GNN and one MLP.
2. The statement "IS, FID and MODE score takes both visual fidelity and diversity into account." under "Evaluation of Mode Collapse" is contradictory to the description in sec 2.1 that IS in fact does not measure diversity. While this work is focusing on black box methods for evaluating and palliating mode dropping (aka collapse), it's a bit disappointing that these results are at least also evaluating on white-box type methods in settings where mode dropping is clearer, e.g. PACGAN on stacked MNIST or even normal CelebA. The problem is that this measure wont detect mode dropping if there's aren't samples from those modes to measure anything against. This work addresses the important problem of generation bias and a lack of diversity in generative models, which is often called model collapse. Unlike most existing works that address the model collapse problem, a blackbox approach does not make assumptions about having access to model weights or the artifacts produced during model training, making it more widely applicable than the white-box approaches. Other comments page 1 I'm not sure the connection between mode collapse and instability is well-established. Copying is indeed an issue that FID cannot detect, but it may be tangential to model collapse for real world concerns like privacy.
On the sequence generation task (translation), the authors showed that the proposed KA strategy achieved better performance compared with KD based methods when distilling the knowledge from a teacher model to a student model. When only the top few tokens are used to transfer the knowledge from teacher model to student model, KA focus on the precision of a small subspace, which tends to have few modes. 2. The authors presented a thorough analysis on the proposed KA strategy and compared it with KL strategy in terms of the precision and recall for the student models.
It looks, however, improper because there is no baseline and we cannot conclude that TPRU has better interpretability than others. 2-2. The paper says one of the advantages of using TPRU is in its interpretability. This paper proposes a novel model of recurrent unit for RNNs which is inspired from tensor product representation (TPR) introduced by Smolensky et al. in 1990. The advantage in terms of accuracy of the proposed approach seems marginal in the experiment, and the analysis of the interpretability of the learned representations could be improved: loosely speaking, particular examples of interpretability are given but sometimes without contexts or baselines to compare to (see the two last comments below). In this paper, a new unit based on the outer product called TPRU is proposed for recurrent neural networks. However, the term "interpretability" is very vague and it is not properly defined in this paper.
To this end, the author(s) have proposed to combine both the primal and dual formulation of Wasserstein distance. The paper proposes a new way of stabilizing Wasserstein GANs by using Sinkhorn distance to upper-bound the objective of WGAN's critic's loss during the training. Page 3, line 42: Si(n)khorn distance [p.8] Discussions -> Discussion This paper proposes bounding the Wasserstein term in WGAN with an aim to stabilize training.
Both the pre-training model and the mixture model are combining the specific domain knowledge to improve the generalization and diversity of retrosynthesis. The main contribution of this paper is to apply the state-of-the-art Transformer model and other techniques in NLP to address the specific issues in retrosynthesis. The main contributions are data augmentation techniques, pre-training and a mixture model that seems to improve performance on the USPTO-50K dataset.
The authors consider making decisions with experts, where each expert performs well under some latent MDPs. An ensemble of experts is constructed, and then a Bayesian residual policy is learned to balance exploration-exploitation tradeoff.
Since the space of input-output for the adversarial attach is huge, the paper proposes to use Bayesian optimization (BO) to sequentially select an attack. 2018. This paper applies Bayesian optimisation (BO), a sample efficient global optimisation technique, to the problem of finding adversarial perturbation. This may give a clearer picture on how BO works in the attack generation setting. First, the application is starightforward application of BO to the well-known problem of adversarial perturbation. The main contribution is to combine BO with dimension reduction, which leads to the effectiveness in generating black-box adversarial examples in the regime of limited queries.
The researchers propose a benchmark called QUARL that allows them to evaluate the effectiveness of quantization as well as the impact of quantization across a set of established DRL algorithms (e.g., DQN, DDPG, PPO) and environments (e.g., OpenAI Gym, ALE). This paper investigates the impact of using a reduced precision (i.e., quantization) in different deep reinforcement learning (DRL) algorithms. In a deep reinforcement learning algorithm, when you apply post-training quantization in a deep reinforcement learning algorithm, mainly when that algorithm uses a value function (e.g., A2C or DQN), the problem is reduced to a regression problem. The results are also not entirely surprising or impactful: how is quantization impacting reinforcement learning in a different way than supervised learning? It also shows how this quantization leads to a reduced memory cost and faster training and inference times.
al., NeurIPS 18 This paper introduces a model that learns a slot-based representation, along with a transition model to predict the evolution of these representations in a sparse fashion, all in a fully unsupervised way. would also similarly generalize. 3b) Sec 4.2 argues that this slot-based representation can help exploration, but this is in fact a chicken-and-egg problem, as one needs to have collected interesting transition samples for a good representation to emerge. This paper proposes a model that learns to disentangle visual scene into objects (slots), and simultaneously learns a dynamics model to capture how these objects interact with each other. The experiments in Sec 4.2 and the visual results in Fig 3 do clearly highlight the benefits of joint learning, and show that the emergent representations are more meaningful compared to learning representations independently. *CONF* 2019 This paper proposes to use a 'slot-based' (factored) representation of a 'scene' s.t. a forward model learned over some observed transitions only requires sparse updates to the current representation.
For entity typing, RELIC embeddings of entities are used as input for a 2-layer FF network, which then outputs which types belong to the entity. In the entity linking task, RELIC achieves a lower precision at two benchmarks (CoNLL-Aida, TAC-KBP 2010) than other approaches.
By designing actdiff loss and reconstruction loss, the authors demonstrate that classifiers are likely to predict using features unrelated to the task and their losses can mitigate this problem. The authors then proposed using Actdiff loss, Reconstruction loss, and Gradmask loss that are designed to suppress the effect of irrelevant features. * (Sec6) "Both actdiff and gradmask improve performance of the model, but only actdiff scores above chance, and performs similarly to a model trained with the areas outside of the mask completely removed." Actdiff is compared to 5 other methods including a reconstruction loss and Gradmask (previous work). The authors mention that "Conv AE Actdiff" is the one using both actdiff loss and reconstruction loss. The conclusion is that adding mask information using Actdiff doesn't improve segmentation performance. * (Sec5) "For the Cardiac dataset, the best performing method was classification with gradmask for the Convolutional AutoEncoder and with actdiff for the UNet, and each achieve similar performance." All the losses except Actdiff achieve at best 50% accuracy on the val set, but Actdiff gets 80% or more accuracy on 3 of 4 tested model variations. The train set is largely from one place and the test set is mostly from the other place, and masks are constructed so Actdiff can try to eliminate this bias. (approach) Actdiff: 1) The Actdiff loss requires a mask that highlights areas of the input image which have signal and not distractor regions.
The paper presents an unsupervised approach for learning landmarks in images or videos with single objects by separating the representation of the image into foreground and background and factorizing the representation of the foreground into pose and appearance. The paper presents an unsupervised method to get disentanglement of pose, appearance, background from both images domain and video domain. 5 sub-network are used to model pose, appearance, foreground, background, and decoders. Their methods let the network focus more on the foreground to regress the landmark and improve state-of-the-art performance on landmark regression (unsupervised.), video prediction and image reconstruction. For instance, on CelebA your baseline seems to fall short of Lorenz 2019, which may suggest that your substantial looking improvement on BBC Pose is indeed due to more favorable cropping.
- I think the experiments can not support the argument that residual connections help networks learn more similar representations. - Finally, I suggest revising the very vague title to the paper This paper tries to analyze the similarities and transferring abilities of learned visual representations for embodied navigation tasks. The paper is called "Insights on visual representations for embodied navigation tasks," but I am left wondering what those insights are.
In this work, the authors develop the discriminative jackknife (DJ), which is a novel way to compute estimates of predictive uncertainty. They develop an exact construction of the DJ confidence intervals in Section 3.1. This is an important open question in machine learning and the authors have made a substantial contribution towards answering the question of "can you trust a model?" DJ constructs frequentist confidence intervals via a posthoc procedure. The authors propose using influence functions to efficiently estimate pointwise confidence intervals for regression models. Next, they explain and then develop the concept of higher order influence functions. (ii) The claims of guaranteed frequentist coverage are not backed up as, according to thm.2, they only hold when n >> 0 and the number of influence functions used goes to infinity (ideally, the authors would provide non-asymptotic bounds as in [1], but at the very least, these limitations should have been clearly pointed out and their practical implications discussed). This could be a chance to really sell your method: if it does well enough compared to more expensive LOO jackknife procedures, that would be a compelling reason to choose DJ. This paper studies how to construct confidence intervals for deep neural networks with guaranteed coverage. The authors propose an algorithm, "discriminative jackknife", based on the standard jackknife confidence interval estimate which they augment by a "local uncertainty estimate" based on the variability of the n leave-one-out fitted versions of the underlying algorithm (n = # data points). - Can you please clarify if and how the use of the algorithm from (Agarwal et al., 2016) for approximation of the Hessian products affects accuracy of your confidence intervals?
Note that back translation only needs monolingual data, while the pretraining in this work needs bilingual sentence pairs. Pros: 1. The data scale is huge, with 40 billion sentence pairs. This work conducts a large scale study on pretraining for neural machine translation. What would be the performance of an ensemble of 10 models trained with the regular 20M parallel sentence pairs? Large scale pretraining like in BERT usually benefits from very large datasets, but also very large models. 2. I'm curious how large-scale pretraining compare with large-scale back translation.
Since UMixUp is also focusing on the mixing ratio between two training samples, Between-Class Learning should have been compared to the proposed method. arXiv 1801.02929 This paper proposes a novel data augmentation method, untied MixUp (UMixUp), which is a general case of both MixUp and Directional Adversarial Traning (DAT). This paper introduces directional adversarial training (DAT) and UMixUP, which are extension methods of MixUp. DAT and UMixUp use the same method of MixUp for generating samples but use different label mixing ratios where DAT retains the sample's original label. Actually, both MixUp and UMixUp are shown to converge to DAT when the number of training samples tends to infinity.
This paper proposes an extension of the conditional GAN objective, where the generator conditions on an attention map produced by the discriminator in addition to the input image. pg. 5 - "Attention mask can potentially break good property of the raw input." - what does this mean? By conditioning on the attention map, the generator could leverage information about the regions in the image that the discriminator attends to and use it to generate a new image that better fools the discriminator. As a result, when training the discriminator, the discriminator is encouraged to reveal as little information as possible via the attention map, so that the loss maximized.
If the same theorem has already been proved by previous work [Wang et al., 2014], it is not necessary to prove it again in this paper. For instance, in equation (9), the present manuscript writes greater or equal, while I believe that it should be strictly greater, as in Wang. Concerning the iterated reweighted approach, I believe that this is non smooth only for g(x)=0, which is not covered by the theorems of Wang 2014.
The authors borrow from Bloom filter the way to create the codes using random hash functions, but the analogy stops here. For instance, if there are m hash functions taking values in {1, ..., P}, an approach based on Bloom filters would predict a binary output of dimension P, while here there are m multiclass problems with P classes (IIUC). - following the comment above, and assuming I understood correctly: There is an originality on the paper compared to other works that use binary codes/bloom filters: In the current paper, the authors actually predict the result of individual hashing functions. Such codes provide more freedom in terms of where to put model capacity (larger embeddings, more transformer layers, etc.) which may be useful in applications where most of the model parameters are in embedding matrices. This is different from predicting the binary encoding that results from using the "or" of one-hot encodings generated by several hash functions, as would be done in approaches (truely) based on Bloom filters. The paper proposes to use codes based on multiple hashing functions to reduce the input and output dimensions of transformer networks. While the technical contribution is limited, because most of the principles are already known or straightforward, the main contribution of the paper is to show that random hash functions are sufficient to create significantly shorter codes and maintain good performances.
Summary: This paper proposes to use a combination of a pretrained GAN and an untrained deep decoder as the image prior for image restoration problem. In Alg.1, the detailed algorithmic process is presented, it is clear that authors need to pre-train the used GAN and Deep Decoder, then combine them to train one network. Conclusion: The paper proposes a simple combination of a trained GAN and an untrained decoder model for the task of image restoration. [Original reviews] This paper proposed to modeling image as the combination of a GAN with a Deep Decoder, to remove the representation error of a GAN when used as a prior in inverse problems. The proposed hybrid model is helpful on compressed sensing experiments on the CelebA dataset; however, it is only marginally better than deep decoder on image super resolution and out-of-distribution compressed sensing.
called SMOE scale. The result of this operator can be combined through different scales to obtain a global saliency map, or visualized in such a way that shows the consistency of saliency maps at different scales. The paper presents a new approach, SMOE scale, to extract saliency maps from a neural network. Also, superimposing these HSV maps over gray scale version of the image like is Fig.2 is difficult to analyze because the "gray" of the image can be confused with the saturation channel. The paper gives two main contributions: SMOE, which captures the informativeness of the corresponding layers of the scale block and LOVI, a heatmap based on HSV, giving information of the region of interest according to the corresponding scale blocks. Overall, in my opinion, being efficient at generating saliency maps is a nice to have, but not much more. - 3.2 The authors refer to Smoothgrad squared method, it is indeed a good process to refine saliency maps, however why it is used could be detailed, just as the parameters chosen for its implementation.
Many loss correction methods can estimate the transition matrix T (which is indispensable in any loss correction) without knowing the noise rate, when there are anchor points or even no anchor points in the noisy training data. Finally, empirical studies show that the propose peer loss indeed remedies the difficulty of determining the noise rates in noisy label learning. While I am not sure about the area of peer prediction, in the area of learning with noisy labels (in a general sense), there were often 10 to 15 papers from every NeurIPS, ICML, *CONF* and CVPR in recent years. Some claims are strong. In the literature, with a mild assumption, the class-dependent transition matrix can accurately be estimated just from noisy data with theoretical guarantees. Then the authors use the setting of CA to induces a scoring matrix, and then the peer loss.
This paper proposes a pretraining technique for question generation, where an answer candidate is chosen beforehand, and the objective is to predict the answer containing sentence given a paragraph excluding this sentence and the target answer candidate. The intuition of this method is that question generation requires to generate a sentence which contains the information about the answer while being conditioned on the given paragraph. In fact, I think the fact that answer candidate is given makes it closer to question generation task, rather than "generating answer-containing sentence" is the key. NAACL 2018. - Tang et al. Question Answering and Question Generation as Dual Tasks. I believe this paper should discuss previous work in question generation and compare the performance with them. ", and <Sentence 3>, and the answer prediction model (described in Section 2.1) chooses "49.6%", the question generation model is given "<Sentence 1> 49.6% <Sentence 3>", and the model is supposed to generate  "The racial makeup of Fresno was 245,306 (49.6%). Not only this submission did not discuss this paper (it was posted in May 2019, which is substantially ahead of *CONF* deadline), the result is substantially worse in both BLEU score of question generation and EM/F1 of end QA performance after data augmentation on SQuAD v2.
It does this by first pretraining the agent to perform interventions in the environment which change the states of the objects of interest, and then finetuning the agent to actually make a decision about whether the given hypothesis is correct. Reward the agent for changing the state of any object referenced in the hypothesis The authors present a framework for testing a set of structured hypotheses about environment dynamics by learning an exploratory policy using Reinforcement Learning and a evaluator through supervised learning. It first formulates the problem as a MDP, where the agent takes actions to explore the environment and has two special actions (Answer_True, and Answer_False) to indicate that the agent has made a prediction about the validity of the hypothesis. For example, here are a few alternate ways of rewarding the agent that seem intuitively like they could also work: Reward the agent for changing the state of any of the objects in the environment Reward the agent for observing a state of the world it has not seen before (i.e. count-based exploration) So, if the agent is trained on some hypotheses, the agent will essentially learn to identify for each h which state s that can be used to to verify h (either prove or disprove it). In contrast, an agent which is trained directly on the hypothesis verification task is unable to learn to do it. Second, when the agent exploits the structure of the hypotheses, the problem becomes nearly trivial. I would like to see a comparison where the RL Baseline agent is trained for 1.5e8 steps as well. I was though encouraged to see that one of the environment seemed to require slightly different setting in the pre-training reward setup, however the authors didn't follow up with some analysis on why there was such a difference.
When the α is small, the learned decoder will function similarly to the original pixelCNN, therefore, latent activations produce smaller FID scores than projected Fisher scores for small α's. It is very obvious that either a CNN decoder, a conditional RealNVP decoder, or a conditional Pyramid PixelCNN decoder conditioned on projected Fisher scores will produce better images because the Fisher scores simply contain much more information about the images than the latent activations. Motivated by the observation that powerful deep autoregressive models such as PixelCNNs lack the ability to produce semantically meaningful latent embeddings and generate visually appealing interpolated images by latent representation manipulations, this paper proposes using Fisher scores projected to a reasonably low-dimensional space as latent embeddings for image manipulations. This work proposes to learn a latent space for the PixelCNN by first computing the Fisher score of the PixelCNN model and then projecting it onto a lower-dimensional space using a sparse random matrix.
The proposed model, CGT, has an encoder-decoder structure, and is characterized by clustering modules for spacial regions based on their temporal patterns. Summary: This paper proposes a clustering attention-based approach to handle the problem of unsmoothness while modeling spatio-temporal data, which may be divided into several regions with unsmooth boundaries.
2.I agree with authors point that WAE is a better choice than VAE for outlier detection because, former "encourages the latent representations as a whole to match the prior ". I am not sure if, from anomaly detection perspective, this is any better than simply using the reconstruction score. The idea being that an outlier would have a higher reconstruction error, and hence should be mapped to low-probability region of the latent distribution. Figure 2 is the misleading figure as it doesn't illustrate the anomaly detection process. Overall, the method is promising, but I have the following concerns: * Using the reconstruction error as an anomaly score has been explored many years ago (check replicator neural networks), the novelty here is to enforce that on the latent space in the context of a variational auto-encoder. The paper proposes an improved extension of the Wasserstein auto-encoder for anomaly detection.
------------------------- BEFORE rebuttal The paper proposed different metrics for comparing explainers based on their correctness (ability to find most relevant features in an input, used in prediction), consistency (ability to capture the relevant components while input is transformed), and the confidence of the generated explanations. This paper proposes 3 such explanation evaluation metrics, correctness, consistency, and confidence. Several metrics are proposed, including correctness, consistency and confidence. I still recommend rejection because the paper relies on a theoretical understanding of what makes confidence and correctness metrics useful and that understanding is not provided.
We assume the desired style is defined by some combination of labels, and that we know this combination (i.e. a fast trajectory to the basket should have the "speed above threshold c" label and "final location close to basket" label, which we have labeling functions for.) Same for Table 10, if the NLL column is truly actually log-likelihood, then the "style+" objective really degrades imitation quality rather than improves it. Standard behavioral cloning loss, and a style consistency loss that encourages labels of the generated trajectory to match labels of the target style. We do not directly have style labels, so to get around this, we define labeling functions, which take a trajectory and output some boolean value.
The submission proposes to reduce the memory bandwidth (and energy consumption) in CNNs by applying PCA transforms on feature vectors at all spatial locations followed by uniform quantization and variable-length coding. A lossy transform coding approach was proposed to reduce the memory bandwidth of edge devices deploying CNNs. For this purpose, the proposed method compresses highly correlated feature maps using variable length coding. Will the computation of PCA require a lot of on-device memory? - Could you please explain the following settings, more precisely: direct quantization of the activations; quantization of PCA coefficients; direct quantization followed by VLC; and full encoder chain comprising PCA, quantization, and VLC?
Also, the table of Cohen et al. was only calculated for specific values of \\sigma for Gaussian distributions (0.12, 0.25, 0.5, 1.00). Summary: This paper investigates the choice of noise distributions for smoothing an arbitrary classifier for defending against adversarial attacks. From this framework, a trade-off between accuracy and robustness is identified and new distributions are proposed to obtain a better trade-off than with Gaussian noise. The theoretical results are interesting, showing a clear trade-off between robustness and accuracy with a new lower bound and deriving better smoothing distributions. Using this, it considers different adversarial smoothing distributions that yield some increase in certified adversarial accuracy. #3 (sketchy justification): The paper justifies a smoothing distribution that concentrates more mass around the center as follows: "This phenomenon makes it problematic to use standard Gaussian distribution for adversarial certification, because one would expect that the smoothing distribution should concentrate around the center (the original image) in order to make the smoothed classifier close to the original classifier (and hence accurate)." A new framework for certification is proposed, which allows to use different distributions compared to previous work based on Gaussian noise. The toughest classifier for Gaussian smoothing (the one achieving the lower bound for Gaussian smoothing) is actually a linear classifier.
General: The paper proposed to use a causal fairness metric, then tries to identify the Pareto optimal front for the vectorized output, [accuracy, fairness]. The fairness component relies on a  definition of fairness based on causal inference, relying on the idea that a sensitive attribute should not causally affect model predictions. * A new causal fairness objective based on the existing Weighted Average Treatment Effect (WATE) and Average Treatment Effect for the Overlap Population (ATO) Con & Question: The so-called causal fairness metric does not seem to be any more fundametal than the other proposed metrics. As main contributions, the paper provides: * A Pareto objective formulation of the accuracy fairness trade-off However, there may be several problems with this approach: 1) For a causal estimate to be valid we need several assumptions. 3) Why do we want U to be small, i.e., why do we want the causal effect of A to be small, is never justified.
In particular, the regularizer considers explanations during the model training; if the explanations are not consistent with some prior knowledge, then explanation errors will be introduced.
This paper proposed a method of unsupervised representation by transforming a set of data points into another space while maintaining the pairwise distance as good as possible. If the original space is a structured space like Euclidean space, then effectively this paper's method coincides with regular distance preserving method in dimension reduction, and Johnson-Lindenstrauss theories. However, there's no good distance metric for the document space.
Conventionally, the sensor placement strategy is tasked to gather the most informative observations (given a limited sensing budget) for maximimally improving the model(s) of choice (in the context of this paper, the neural networks) so as to maximize the information gain. This paper describes a sensor placement strategy based on information gain on an unknown quantity of interest, which already exists in the active learning literature. What the authors have done differently is to consider the use of neural nets (as opposed to the widely-used Gaussian process) as the learning models in this sensor placement problem, specifically to (a) approximate the expectation using a set of samples generated from a generator neural net and to (b) estimate the probability term in the entropy by a deterministic/inspector neural net. Do they adopt an open-loop sensor placement strategy? Consequently, it is not clear to me whether their proposed strategy would be general enough for use in sensor placement for a wide variety of environmental monitoring applications. The authors propose a framework for sensor placement called Two-step Uncertainty Network (TUN) based on the idea of information gain maximization.
leads the reader to believe that the findings here are generally applicable e.g. the sentence "the generative representations learned by GAN are specialized to synthesize different hierarchical semantics" should actually be something like "the per-layer latent variables for StyleGAN affect different levels of scene semantics". The paper presents a visually-guided interpretation of activations of the convolution layers in the generator of StyleGAN on four semantic abstractions (Layout, Scene Category, Scene Attributes and Color), which are referred to as the "Variation Factors" and validates/corroborates these interpretations quantitatively using a re-scoring function. The paper proposes an approach to analyze the latent space learned by recent GAN approaches into semantically meaningful directions of variation, thus allowing for interpretable manipulation of latent space vectors and subsequent generated images. By taking advantage of the structured composition of the latent space into per-layer contributions in the StyleGAN approach, experiments are performed to show that different levels of semantics are captured at different layers: layout being localized in lower layers, object categories in middle layers, followed by other scene attribute, and lastly the color scheme of the image in the highest layers. The results showing scene property manipulation e.g. in Fig 4 are obtained by varying a certain y_l, and it'd help to also show the results if the initial latent code w was modified directly (therefore affecting all layers!). In the next step, the authors sample a latent code from the learned distribution and pass it through every layer of the GAN generator. The paper analyzes the relation of various scene properties w.r.t the latent variables across layers, and does convincingly show that aspects like layout, category, attribute etc, are related to different layers. The 'StyleGAN' work first-generates a per-layer latent code y_l (from a global latent variable w), and uses these in a generative model. The claim here is that with the same perturbation of the resulting codes (lambda=2) at the output of different GAN layers, the change in the visualized output demonstrates what kind of, if any, semantic is being captured by different layers of GAN. A scoring function is obtained to quantify (Equation 1) how the corresponding images vary in a particular semantic aspect when the latent code is moved from the separation boundary. On the other hand, the insight gained is fairly superficial, boiling down to the statement that the learned latent code has structure that corresponds to semantically meaningful axes of variation, and that such structure is localized to particular levels of the layer hierarchy for particular semantic axes. --------------------------- This paper investigates the aspects encoded by the latent variables input to different layers in StyleGAN (Karras et. With the separation boundary (in the form of a normal vector) known for each of the four scene semantics, different feature activations are obtained by moving the latent code towards/away from the separation boundary. By forming a decision boundary in the latent space for each of these classifiers, the latent code is then manipulated along the boundary normal direction, and re-scored by the classifiers to determine the extent to which the boundary is coupled to the semantic attribute.
- Comments: 1. In section 4, the authors conjectured the reason why the performance of reject class in Lee et al. (2018a) was worse is that the generated OOD samples do not follow the in-distribution boundaries well. - Summary: This paper proposes to improve confident-classifiers for OOD detection by introducing an explicit "reject" class. Although this auxiliary reject class strategy has been explored in the literature and empirically observed that it is not better than the conventional confidence-based detection, the authors provide both theoretical and empirical justification that introducing an auxiliary reject class is indeed more effective. ==== [Summary] To detect out-of-distribution (OOD) samples, the authors proposed to add an explicit "reject" class instead of producing a uniform distribution and OOD sample generation method. I think Appendix E in the Lee et al.'s paper corresponds to this reasoning, but Lee et al. actually didn't generate OOD samples but simply optimized the confidence loss with a "seen OOD." Lee et al. didn't experiment on MNIST variations but many natural image datasets. Therefore, if there is an auxiliary reject class, only data in the same direction will be detected as OOD; in other words, OOD is "modeled" to be in the same direction with the weight vector of the auxiliary reject class.
The hierarchical prior policy is shared amongst all tasks, while the hierarchical posterior policies are allowed to adapt to specific tasks. Summary: The authors propose a method for learning hierarchical policies in a multi-task setting built on options and casts the concepts into the Planning as Inference framework. There seems to be no other term that incentivizes the option posterior to deviate, and I do not see how the options are adapting to tasks. Detailed Comments: A primary weakness of this approach is that it seems like there is one network that learns the options and is shared across all task (that would be the prior) and then there is a task-specific network for all options (posterior), wouldn't this be very difficult to scale if we want to learn reusable options over the lifetime of an agent? 2016. This paper is about learning hierarchical multitask policies over options. -Allows fine-tuning of options -Learn termination policies naturally The parameters of the posterior policies are adjusted via an adapted version of A2C. G. (2007). Building Portable Options: Skill Transfer in Reinforcement Learning. Term 2 controls how the option posterior deviates from the prior. The authors assume that all options are present everywhere i.e. I ⊆ S. Eq.6 is simply the extension of the reward of maximum entropy RL to the options framework.
The empirical gains in transfer learning can be simply attributed to: - More params it seems adding an LSTM over bert embeddings already does some improvement, I would have loved to see this more exploited but it wasn't. For each bert embedded token, the proposed method aims at disentangling semantic information of the word from its structural role. This paper proposes a layer on top of BERT which is motivated by a desire to disentangle content (meaning of the tokens) and form (structural roles of the tokens). Regarding the performance, it seems HUBERT is gaining very little over the BERT baseline. This paper proposes an alternative way of reusing pretrained BERT for downstream tasks rather than the traditional method of fine-tuning the embeddings equivalent to the CLS token. I understand the authors are just trying to make a point that BERT does worse than their model in this case and that this is not good for transfer, but still I find this to be artificially constructed.
Experiments on LeNet + MNIST show (a) different methods can achieve similar accuracy, (b) pruned sub-networks may differ significantly despite identical initialization, (c) weight reinitialization between pruning iterations yields more structured convolutional layer pruning than not reinitializing, and (d) pruning methods may differ in the stability of weights over pruning iterations. There are no guidelines how to utilize the observations in future research (e.g., how they can be used for verifying the lottery ticket hypothesis or how they affect to existing pruning techniques) while some observations might be trivial or not very interesting (e.g., contribution 1 and contribution 2) for me. (1) *Overlap in pruned sub-networks*: In the middle of Sec. 4, Fig 3-5 examine the similarity of pruning masks between methods. (2) *Weight stability during pruning*: It is difficult to discern a conclusion in Sec 5. It is concerned with the examination of pruning experiments for a LeNet on the MNIST dataset. Second, the observations are only presented for LeNet and MNIST and it is non-trivial whether they extend to large scale models.
In addition to that, it analyzed a mixture of linear and non-linear activation functions, and show that mixture is better than single nonlinearity in terms of expected training error for ridge regression estimators. I guess there does not appear such a trade-off for the test error and the linear activation function would be always better because the true function is the linear model. This paper analyzed the asymptotic training error of a simple regression model trained on the random features for a noisy autoencoding task and proved that a mixture of nonlinearities can outperform the best single nonlinearity on such tasks. - The effect of mixture of activation functions is investigated in the "training error," but I don't see much significance on investigating the training error thoroughly.
The main idea of this paper is to solve multi-agent reinforcement learning problem in dominance solvable games. The main contribution of this paper is that the authors have proved the convergence to the iterated dominance solution for two RL algorithms: REINFORCE (Section 3.1, binary action case only) and Importance Weighted Monte-Carlo Policy Improvement (IW-MCPI, Section 3.2). The interesting aspect of this paper is that iterated dominance solution based reward scheme can guarantee convergence to the desired agents policies at a cheaper cost in practical principal-agent problems. [2] Fudenberg & Levine, 1999 This paper studies independent multi-agent reinforcement learning (MARL) in dominance solvable games. The main contribution of the paper is to prove that both REINFORCE in binary action case and Monte-Carlo algorithms find the agents' policies converging to the iterated dominance solution. This work studies learning under independent MARL, and shows theoretically and experimentally that two independent MARL algorithms converge for games that can be solved by iterated dominance. I appreciate that, while the main results in the paper are limited to normal-form games (which are quite restricted), there are empirical results in the appendix showing the extension to Markov games with multiple timesteps, suggesting that the applicability of iterated dominance reward schemes extend beyond the simple two-action case, where no temporally extended decisions need to be made. The applications of the convergence result result to "noisy effort" games is pretty standard and the results expected based on the theory. What I want to know is: what sorts of games can we compute the iterated dominance reward schemes for?
The authors also demonstrate the use of their technique in Image to Image translation. Since image generation and image to image translation are targeted, comparison and/or combined use with Sota methods, i.e., GANs should be examined.
For another example, why replacing the residual connection with the gating layer can make the Transformer more stable? This paper is motivated by the unstable performance of Transformer in reinforcement learning, and tried several variants of Transformer to see whether some of them can stabilize the Transformer.
The authors conduct extensive experiments on image classification and segmentation and show that dynamic convolutional kernels with reduced number of channels lead to significant reduction in FLOPS and increase in inference speed (for batch size 1) compared to their static counterparts with higher number of channels. The reviewer agrees that some recent works focus more on Flops, but the number of parameters is also discussed in general, when telling about the 'model size'. The method also proposes the attention-based scaling of channels, where the attention comes from GAP, so the reviewer thinks that it is possible to explain this work as some variation of SEnet. However, most previous work focuses on leveraging dynamic kernels to use more parameters so the focus on accelerating CNNs is novel. === Summary === The authors propose to use dynamic convolutional kernels as a means to reduce the computation cost in static CNNs while maintaining their performance. Conclusion - The author proposed a dynamic kernel selection method (add-on), which can enhance the classification accuracy of the baseline network.
Further, the fact that using a smaller number of advantage estimates worked better (point #2 on pg 5, Effect of Ensemble Size in Appendix A) suggests that the ensemble size is an important hyperparameter, and that risk-seeking / risk-aversion (i.e., regulatory vs promotion focus) cannot alone explain why the proposed method works. Risk control is instantiated by different ways of estimating the advantage of a state (max/min instead of average). If the maximum is taken, the resulting policy will be exploratory (i.e., have a "promotion focus"); if the minimum advantage is taken, the resulting policy will be risk sensitive (i.e., have a "regulatory focus"). The paper introduces a regulatory ratio: the probability of using the averaged advantage estimate vs using the order statistic, for computing the policy gradients.
One aspect that worries me about this paper is that most of the biases studied are synthetic, so specification of the bias family is trivial, in contrast to the works I mentioned above (where the bias model is potentially somewhat misspecified and needed to be discovered by different researchers). Moreover, the experiments on the toy dataset help clarify the proposed solution whereas experiments on Biased MNIST and Imagenet show that it successfully mitigates the bias. The paper describes a methodology for reducing model dependance on bias by specifying a model family of biases (i.e. conv nets with only 1x1 convs to model color biases), and then forcing independence between feature representations of the bias model and the a full model (i.e. conv nets with 3x3 convs to also model edges).
For example, "Learning with rejection is a classification scenario where the learner is given the option to reject an instance instead of predicting its label.", "…classifies adversary attacks to two types of attacks, white-box attack and black-box attack.", "Methods for protecting against these adversarial examples are also being proposed.". The paper proposes a framework for learning with rejection using ideas from adversarial examples. However, I think there are different dimensions along which the paper could be improved: - My understanding of classification with a reject option is that the rejection cost c(x) is a design choice that can depend on the specific application. This paper wants to study the problem of "learning with rejection under adversarial attacks". It first naively extends the learning with rejection framework for handling adversarial examples. - The rejection function r(x) relies on z^* which essentially amounts to computing an adversarial perturbation for a given testing point, see (2). The paper does not show any connection between "learning with rejection" and "adversarial learning".
More discussions and comparisons with these models should be addressed, and even experimental comparisons if possible, since they also use knowledge distillation to convey the knowledge expressed in the constraints. The domain that the method is applied to is VQA, with various relations on the questions translated into hard constraints on the embedding space. The authors propose a framework to incorporate additional semantic prior knowledge into the traditional training of deep learning models such that the additional knowledge acts as both soft and hard constraints to regularize the embedding space instead of the parameter space. A technique which involves distillation is used to satisfy those constraints during learning. However, there are several considerations which limits the contribution of this paper: 1. As a teach-student distillation framework, there are several papers using a posterior regularizer with hard constraints, e.g., "Harnessing deep neural networks with logic rules", "Constrained Convolutional Neural Networks for Weakly Supervised Segmentation".
This paper proposed to use VAE to learn a sampling strategy in neural architecture search. ============ previous comments Neural architecture search can be formulated as learning a distribution of promising architectures (the sampling policy). The idea of representing the architecture distribution using VAEs is very natural, which in principle could offer better coverage over interesting regions in the search space as compared to traditional factorized distribution representation (which has a single mode only). As for the authors' comments for Q1, I'd like to point out that a "larger" search space is not necessarily more difficult (a more meaningful metric would be the average accuracy of random architectures).
In 2019 (post Go-Explore), it's not clear Montezuma's revenge poses a significant exploration challenge -- exploration doesn't even need to be interleaved with learning. Unlike other self-imitation learning methods, the proposed method not only leverages sub-trajectories with high rewards, but lower-reward trajectories to encourage agent exploration diversity. This paper proposes an approach for diverse self-imitation for hard exploration problems. The proposed trajectory-conditioned policy sounds, since rewarded trajectory carries significant information of the goal in the exploration problem. The idea is leverage recently proposed self-imitation approaches for learning to imitate good trajectories generated by the policy itself. The authors propose DTSIL to learn a trajectory-conditioned policy to imitate diverse trajectories from the agent's own past experience. By encouraging diversity in the pool of trajectories for self-imitation, the idea is to encourage faster learner -- this basic concept is also used in approaches like prioritized experience replay, albeit at the entire trajectory level rather than individual state/action level. For instance, one could conduct a systematic study (say of the Apple domain) where one varies the degree of stochasticity and measures how the performance the proposed algorithm changes, perhaps relative to Go-Explore on the purely deterministic version of the environment.
This paper tackles the issue of identifying the causal reasoning behind why partial models in MBRL settings fail to make correct predictions under a new policy. *Summary* This paper considers the effect of partial models in RL, authors claim that these models can be causally wrong and hence result in a wrong policy (sub optimal set of actions). *Decision* I vote for rejection of this paper, based on the following argument: To my understanding authors are basically solving the "off-policy policy evaluation" problem, without relating to this literature. Authors demonstrate this issue with a simple MDP model, and emphasize the importance of behavior policy and data generation process. To make the partial model causally correct, the paper considers the partial model conditioned on the backdoor that blocks all paths from the confounding variables. For example, the MDP example is just an off-policy policy evaluation problem, and it is very well known that in this case you need to consider the behavior policy, for example with importance sampling. The authors then reformulate the model-learning problem using a causal learning framework and propose a solution to the above-mentioned problem using the concept of "backdoors." They perform experiments to demonstrate the advantages of doing so. Improvement The current manuscript needs a major revision, mainly 1) situate the work with respect to off-policy policy evaluation literature, and then 2) Considering step 1, a clarification for what is the novelty/ contribution of the current paper is needed. The paper considers the problem of predicting a variable y given x where x may suffer from a policy change, e.g., x may follow a different distribution than the original data or suffer from a confounding variable. 4. For sentence, "Fundamentally, the problem is due to causally incorrect reasoning: the model learns the observational condi- tional p(r|s0, a0, a1) instead of the interventional conditional given by p(r|s0, do(a0), do(a1)) = s1 p(s1|s0, a0)p(r|s1, a1)."
The paper proposed an interesting continual learning approach for sequential data processing with recurrent neural network architecture. The authors provide a general application on sequential data for continual learning, and show their proposed model outperforms baseline. The goal of this work is to best understand the performance and benchmarking of continual learning algorithms when applied to sequential data processing problems like language or sequence data sets. In traditional continual learning settings, researchers may not always increase the model size for overcoming catastrophic forgetting. Summary: In this paper, the authors propose a new method to apply continual learning on sequential data. Thus, It is interesting to see that continual learning is used in sequential data. The contributions of the paper are 3 fold - new benchmarks for CL with sequential data for RNN processing, new architecture introduced for more effective processing and a thorough empirical evaluation. Introduction: I think a little more insight into why the sequential data processing CL scenario is any different than the vision scenario would be quite helpful.
This paper proposed a new query efficient black-box attack algorithm using better evolution strategies. Li, Yandong, et al. "NATTACK: Learning the Distributions of Adversarial Examples for an Improved Black-Box Attack on Deep Neural Networks." ICML 2019. 2) In addition to attack success rate and query complexity, it might be useful to compare different attacks in terms of ℓp distortion, where p≠∞. The authors also add tiling trick to make the attack even more efficient.
The primary questions and concerns (critical to the rating) are: 1. One important claimed advantage of GMC over existing method is that it uses global gradient for memory gradient, while existing methods such as DGC uses local-work gradient to do so. If it is for demonstrating the benefits of global gradient for gradient memory, I think it is more proper to also include the results of DGC without factor masking? Following existing gradient sparsification techniques such DGC, GMC is also built up on the memory gradient approach; the major distinction between GMC and existing techniques is that GMC keeps track of global gradient to maintain the memory gradient, while the existing technique keeps track of worker-local gradients for memory gradient. It uses global gradient (but still achieve sparse communication) to maintain the gradient memory while existing approaches such as DGC use worker-local gradient to do so. My preliminary feeling is that by bounding the gradient variance, it should also be possible to prove a rate for DGC using worker-local gradient; this is because the difference between the global gradient and the local gradient might be bounded via the gradient variance.
But if the baselines weren't thoroughly tuned, it could be the case that IG on the CRAIG subset performs similarly to IG on the full training data, but that neither is actually reaching satisfactory performance in a given domain. I'm wondering how much of the speedup could be attributed to something like better memory locality when using the smaller subset selected using CRAIG. random subset or 2. a subset selected via importance sampling from [1] would contribute towards understanding the particular benefits of CRAIG. chosen? One of the main claims of the paper is that using the subset selected by CRAIG doesn't significantly effect the optimization performance. The algorithm selects a subset of datapoints to approximate the training loss at the beginning of each epoch in order to reduce the total amount of time necessary to solve the empirical risk minimization problem. The paper proposes a theoretically founded method to generate subsets of a dataset, together with corresponding sample weights in a way that the average gradient of the subset is at most epsilon far from the average gradient of the full dataset. The proposed algorithm to create such a subset is a greedy algorithm that relies on parameter independent similarities between samples, namely similarity scores that are true regardless of the current value of the function parameters. Given a dataset D and subset S selected by the proposed method, it is shown that training on subset S achieves nearly the same loss as training on the full dataset D would, while achieving significant computational speedups. This paper proposes a novel extension to SGD/incremental gradient methods called CRAIG. - If one were to consider an algorithm that samples points from this new distribution over the data given by CRAIG, if one were to include the weight γj into the algorithm, would the sample gradient be unbiased?
has been -> have without explicit defines -> defining This paper proposes a training objective that combines three terms: * A Stein discrepancy for learning a energy model with intractable normalizing constant This paper propose to add a regularization loss on the stein divergence between the generator G (implicit model ) and the explicit model (E).
Summary: This paper list several limitations of translational-based Knowledge Graph embedding methods, TransE which have been identified by prior works and have theoretically/empirically shown that all limitations can be addressed by altering the loss function and shifting to Complex domain. The authors propose four variants of loss function which address the limitations and propose a method, RPTransComplEx which utilizes their observations for outperforming several existing Knowledge Graph embedding methods. arXiv preprint arXiv:1902.10197, 2019. The paper analyses the effect of different loss functions for TransE and argues that certain limitations of TransE can be mitigated by chosing more appropriate loss functions. They propose and evaluate a new relation encoding (TransComplEx) and show that this encoding can address the limitations previously underlined in the literature when using the right loss. In this paper, the authors investigate the main limitations of TransE in the light of loss function. Furthermore, the paper proposes TransComplEx -- an adaption of ideas from ComplEx/HolE  to TransE -- to mitigate issues that can not be overcome by a simply chosing a different loss. "RotatE: Knowledge Graph Embedding by Relational Rotation in Complex Space." ArXiv abs/1902.10197 (2019): n. Rotate: Knowledge graph embedding by relational rotation in complex space.
More specifically, the hierarchically aggregate computation graphs are proposed to aggregate the intermediate node and utilize this to speed up a GNN computation. 2. The authors state that the HAG can optimize various kinds of GNN models, but the experiment only shows the results on a small GCN model. 3. There is no effectiveness evaluation comparing the original GNN models with the versions with HAG. But there are no experimental results verifying that the effectiveness of the HAG-versions which could obtain comparable performance with the original GNN models for some downstream applications (e.g., node classification). The HAG aiming at eliminating the redundancy during the aggregation stage In Graph Convolution networks.
There are however 3 major issues with the current approach: 1- Novelty: Novak et al. 2018 suggests a very similar notation of sensitivity and they show correlation with generalization. A number of experimental results are presented that show strong correlation between the sensitivity metric and the empirical test loss.
The algorithm casts this training task as a reinforcement learning problem, and employs curriculum learning and the Proximal Policy Optimization algorithm to find appropriate neural network parameters, in particular, those that make the prover good at finding long proofs. - in experiment 3, curriculum learning tends to find shorter proofs. If one advances curriculum, one takes lesser number of proof steps according to stored proofs. Overall the paper attempts to explain clearly the original contribution of the proposed approach, which is using curriculum learning in RL based proof guidance. It would be nice if there was a clear explanation of the role of curriculum in the learning algorithm.
This paper suggests a method for detecting adversarial attacks known as EXAID, which leverages deep learning explainability techniques to detect adversarial examples. However, considering a tabby cat image is perturbed to become tiger cat, since two classes are very close, the resulting saliency maps should be similar and the detector may fail to detect the adversarial example. - I would also like to see how these results hold good for a complicated dataset like ImageNet A Simple method to detect adversarial examples, but needs more work.
This works presents a method for inferring the optimal bit allocation for quantization of weights and activations in CNNs. The formulation is sound and the experiments are complete. Very good paper that studies the error rate of low-bit quantized networks and uses Pareto condition for optimization to find the best allocation of weights over all layers of a network. My main concern is regarding the related work and experimental validation being incomplete, as they don't mention a very recent and similar work published in ICIP19 https://ieeexplore.ieee.org/document/8803498: "Optimizing the bit allocation for compression of weights and activations of deep neural networks".
To calculate the influence function of a model with pretraining, the authors use an approximation f(w)+||w-w*||, where w* is pretrained. More generally, can we relate influence to some objective measure that we care about (say test accuracy), for example by showing that removing the top X% of influential pretraining data hurts test accuracy as much as predicted? First, the authors calculate the influence score for the models with/without pretraining, and then propose some implementation details (i.e., use CG to estimate the inversed Hessian). This is an analysis paper of pretraining with the tool "influence function". 2) In what situations might we want to examine the influence of pretraining data, and can we design experiments that show those situations? This paper proposes a multi-stage influence function for transfer learning to identify the impact of source samples to the performance of the learned target model on the target domain. The authors derive the influence function of models that are first pre-trained and then fine-tuned. This extends influence functions beyond the standard supervised setting that they have been primarily considered in. I believe that these are useful technical contributions that will help to broaden the applicability of influence functions beyond the standard supervised setting. In this case, can influence function be used?
From my perspective, the whole story revolves around how to compute persistence barcodes from the sub-levelset filtration of the loss surface, obtained from function values taken on a grid over the parameters. I believe the concept of barcodes will be new to most members of the *CONF* community (at least it was to me), and I appreciate the authors' effort to convey the ideas through multiple definitions in Section 2. This paper introduces the notion of barcodes as a topological invariant of loss surfaces that encodes the "depth" of local minima by associating to each minimum the lowest index-one saddle. This work is focused on topological characterization of target surfaces of optimization objectives (i.e. loss functions) by computing so called barcodes, which are lists of pairs of local minima and their connected saddle points. The authors present an algorithm for computing the barcodes from graph-based representation of a surface, and present barcodes computed on toy examples in numerical analysis.
This work provides  theoretical analysis for the NAS using weight sharing in two aspects: 1) The authors give non-asymptotic stationary-point convergence guarantees (based on stochastic block mirror descent (SBMD) from Dang and Lan (2015)) for the empirical risk minimization (ERM) objective associated with weight-sharing. The author also provided an alternative to SBMD that uses alternating successive convex approximation (ASCA) which has similar convergence rate. The reviewer has several concerns: 1) the SBMD and ASCA algorithms are existing generic algorithms. The author proposed ASCA, as an alternative method to SBMD. Based on this analysis, the authors proposed to use  exponentiated gradient to update architecture parameter, which enjoys faster convergence rate than the original results in Dang and Lan (2015).
The point of Dai et al. was to use RL to solve a wide family of combinatorial problems. Therefore, I feel there is something of a fundamental contradiction going on at a fairly high-level, in the sense that the paper "uses RL to solve a subset of combinatorial problems that were studied by RL before".
Two simplified versions of GNN is then proposed by linearizing the graph filtering function and the set function, denoted as GFN and GLN, respectively. Another possibility is that even the original GNN has larger model capacity, it is not able to capture more useful information from the graph structure, even on tasks that are more challenging than graph classification. The experiments are designed nicely: 1) it compares with various baselines on a variety of popular benchmarks; 2) ablation studies single out the importance of different graph features, such as degree, and multi-hop averages; 3) verifying whether the good performance GFN comes from easier optimization. Suggestions for improvement: 1) Considering the experimental results in this paper, it is possible that the existing graph classification tasks are not that difficult so that the simplified GNN variant can also achieve comparable or even better performance (easier to learn).
This might explain the reason for the huge forgetting reported for VCL with coreset (−9.2 ± 1.8) as opposed to −2.3 ± 1.4 for EWC which is really strange as VCL even without coreset (on permuted mnist for example) is reported superior to EWC by a large margin (6%) in the original VCL paper. Also, in the experiment results, I feel the performance of the FROMP largely depends on the number of the coreset, while 'important' selection just shows marginal effects even on split CIFAR. FROMP first uses NTK in Gaussian process for continual learning and proposes a new strategy of selecting memory samples. [Reviewer's response:] I disagree with authors on this because GEM, its faster version (A-GEM (Chaudhry et al. 2018)), and all other methods explored in the recent study which I mentioned in my review (Ref#2) use the single epoch protocol and are perfect match to be compared with this method but there is no memory-based baseline except for VCL with coreset and FRCL (only for MNIST variations) which makes it difficult to measure this method's capabilities (performance, memory size, and computational time) against methods which only require one epoch to be trained.
The proposed approach computes the saliency maps for all the classes and removes the pixels that play a role in predicting several classes. Quantitative measures (ROAR & KAR, Hooker et al. 2018) or surveys to show that the newly obtained saliency maps are more refined or help to best localize regions of interest would be a big bonus. The proposed solution involves a simple competition mechanism across saliency maps produced when different logits are considered such that small values are zeroed out in favor of larger values across the logits. It addresses a problem posed for existing methods for characterizing saliency in activation subject to sanity checks which measure the degree to which the activation (saliency) map changes subject to different randomization tests. 3. The introduced approach makes Grad.Input pass the sanity checks introduced by Adebayo et al. Weaknesses: 1. For any interpretability technique, passing the sanity check is a must, but just because a saliency technique passes the sanity checks, it doesn't mean that these maps explain the network's decision well. - Section 5 "The available code for these maps is slow, and computing even gradient for all 1000 ImageNet labels can be rather slow." What is the aim of this sentence?
Many previous unsupervised video object segmentation methods make use of optical flow and boundary detection, which I thought are OK cues to be used, especially when both can be learned in a self-supervised manner. [R2] Unsupervised learning of depth and ego-motion from video This submission proposes a self-supervised segmentation method, that learns from single-object videos by finding the region where it can segment an object, remove the entire bounding box around it, inpaint it, then finally put the object back. The loss is a balance between a reconstruction error and a negative inpainting error (high error means object is probably present, due to the weak correlation with the background).
2) As addressed above, in the Abstract and Introduction, the paper's claims are very general about mode connectivity and sparsity, claiming in the sparse regime, "a subnetwork is matching if and only if it is stable." This paper empirically examines an interesting relationship between mode connectivity and matching sparse subnetworks (lottery ticket hypothesis). or alternatively modify the claim (to what it actually shows) and highlight the significance of the connection between matching subnetworks and stability in this highly sparse subnetwork regime.
I think besides introducing what is matrix completion and what is geometric completion, the introduction part should focus more on the motivation to propose the algorithm. This paper proposes a novel approach for the loss function of matrix completion when geometric information is available. Why geometric matrix completion generalizes the standard deep learning approaches is not clear. The second paragraph and the fourth one are redundant in the same way since they both focus on geometric matrix completion. (2) The authors claim the first contribution is to provide the geometric interpretation of deep matrix factorization via the functional maps framework.
This paper explores how tools from perturbative field theory can be used to shed light on properties of the generalization error of Gaussian process/kernel regression, particularly on how the error depends on the number of samples N. For instance, the sentence ''They [results] hold without any limitations on the dataset or the kernel and yield a variant of the EK result along with its sub-leading correction.'' is misleading: as stated in the previous sentence in the text, this is for the fixed-teacher learning curve, etc. I don't think readers can figure out what is exactly the kernel and the target function from this description. The authors described the kernel and the target function as "the NTK kernel implied by a fully connected network of depth 4 with σσσw2=σb2=1 and ReLU activations" and "a target function with equal spectral weights at l=1,2", without other explanations. as do for – > as we do for uniformally - > uniformly This paper used the field-theory formalism to derive two approximation formulas to the expected generalization error of kernel methods with n samples. This theoretical paper exploits a recent rigorous correspondence between very wide DNNs (trained in a certain way) and Neural Tangent Kernel (a case of Gaussian Process-based noiseless Bayesian Inference). Finally, I want to point out that, the generalization of kernel methods have been intensively studied in the machine learning literature, for example using the RKHS theory.
al. 2018]). Given a sequence of graphs as input, a GNN (to obtain low-dimensional representations of the graphs in this sequence) and LSTM (to model the sequence of these representations)  based encoder is used to compute a vector representation of the topology of next graph in the sequence. This paper proposes a framework to model the evolution of dynamic graphs for the task of predicting the topology of next graph given a sequence of graphs. The model does not output a graph with the right size for very simple synthetic graphs. Second, the model only takes 10 graphs as input and ignores other graphs in the input graph sequences. Did the authors try to feed the sequence of graphs to such models and then try to generate a new graph to see if they can produce similar results? The proposed approach is validated with experiments on three synthetic datasets and one real-world dataset (Bitcoin is same dataset from two different resources with little difference in characteristics) and compared against random graph models. Why not use various graph datasets available in papers that learn representations (e.g. cited by authors themselves) What is special about bitcoin dataset that makes it suitable for this task? The contribution of the paper seems to be a system of combining these to achieve graph evolution prediction. (b) Technical: The technical contributions of this paper lack novelty and has several flows: - Figure 1 seems to show that graph only grows in size.
[2] Linear Memory Networks This paper proposes an initialization scheme for the recently introduced linear memory network (LMN) (Bacciu et al., 2019) and the authors claim that this initialization scheme can help improving the model performance of long-term sequential learning problems. Long short-term memory. Neural computation, 9(8): 1735–1780, 1997. Summary: This paper proposes a new initialization method for recurrent neural networks. The proposed initialization is aimed at helping to maintain longer-term memory and instability during training such as  exploding gradients (due to linearity). Summary: The paper proposes an autoencoder-based initialization for RNNs with linear memory. And then they use the weight to initialize the Lieanr Memory Networks(LMN). They first obtain the weight from a linear optimal autoencoder. First, the LMN seems to be a simpler version of LSTM and it has no significant advantages compared with other recurrent structures introduced in the past several years. (section 3.2) The proposed initialization outperforms the baselines on the MNIST dataset.
Though the devised module performs effectively in this specific simulation environment and specific two tasks, an explanation of the theoretical basis and generality of dual attention seem to be missing. The paper explores multi-task learning in embodied environments and proposes a Dual-Attention Model that disentangles the knowledge of words and visual attributes in the intermediate representations. (2) I am not sure why the visual attention map x_S could be used as the state of the module.
This paper proposes a method to summarize a given graph based on the algebraic multigrid and optimal transport, which can be further used for the downstream ML tasks such as graph classification. The node distance (transport cost) used in the Wasserstein metric is also learned as an L2 distance between the embeddings of some graph embedding function. The paper proposes a differentiable coarsening approach for graph neural network (GNNs). The approach is compared against 6 other state of the art approaches on 5 graph classification tasks, showing significant improvements 4 of them. Since the proposed method is for generating coarse graphs in an unsupervised manner, graph classification cannot be directly performed by itself.
Taking the perspective from signal processing, this paper proposes a pooling operation called frequency pooling (F-pooling). 2. Compared to AA-pooling, it seems that F-pooling has a better theoretical guarantee (i.e. the optimal anti-aliasing down sampling operation given U). The authors first derived the theory of F-pooling to be optimal anti-aliasing down sampling and is shift-equivalent in sec 2, and then demonstrated the experimental results of 1D signals and image classification tasks. 2. When showing the optimality of F-pooling in Section 2.3, the criterion is to reconstruct the original signal x. The F-pooling is then implemented with matrix multiplications and tested with recent convolutional neural networks for image classifiation on CIFAR-100 and a subset of ImageNet dataset. Experiments could be conducted on more benchmark datasets with more CNN architectures to convincingly show the effectiveness of the proposed F-pooling. This paper gives an improved definition on shift-equivalent functions and shows that the proposed F-pooling is optimal in the sense of reconstructing the orignal signal.
Is the attention way used in the paper a good way to exploit the pre-trained model for few-shot classification problems? 4) The "spatial attention" only makes sense when the pre-trained domain's classes can describe the main concepts appearing in the images of novel classes. Also, from the results, the significant improvements come from the weights of the pre-trained model but not the attention used. For the pre-trained model, they will not only use its weights but also use it to generate a spatial attention map and help the model focuses on objects of images. This paper proposed a new realistic setting for few-shot learning that we can obtain representations from a pre-trained model trained on a large-scale dataset, but cannot access its training details. During the test, they use the weighted average of samples' representations per class as the prototype of each class, where the weight is large for samples with more discriminative prediction over pre-trained domain's classes.
The problem of making better ensemble distillation methods seems relevant as ensembles are still one of the best ways to estimate uncertainty in practice (although see concerns below). - A comparison with an ensemble with M=14 models should be tested because this ensemble has the same number of parameters compared to Hydra with M=50 heads. </update> Summary & Pros - This paper proposes a simple yet effective distillation scheme from an ensemble of independent models to a multi-head architecture for preserving the diversity of the ensemble. - The proposed scheme provides the same advantages of the ensemble in terms of uncertainty estimation and predictive performance, but it is computationally efficient compared to the ensemble. To achieve similar performance to the ensemble, how many heads are required? The method itself is a simple extension of earlier "prior networks": the original method suggested to fit a single network to mimick a distribution produce by given ensemble, and here authors suggest to use multi-head (one head per individual ensemble member) in order to better capture the ensemble diversity. "Knowledge Distillation by On-the-Fly Native Ensemble." Advances in Neural Information Processing Systems. The paper proposes to distill the predictions of an ensemble with a multi-headed network, with as many heads as members in the original ensemble.
It is not convincing. The authors claim that OWM is one of the strongest baselines, but actually it perform really badly on EMNIST-26 (5 tasks),  EMNIST-47 (5 tasks) and EMNIST-47 (10 tasks). 4. It is not clear why the proposed method can solve the issue that OWM faces with (bad accuracy when tasks are not quite related). The method is an extension of recent work, called orthogonal weights modification (OWM) [Zheng,2019]. Minor remarks: - I wonder if you use OWM or PCP you discard the possibility of positive backward transfer. The authors conduct experiments on image classification tasks to show the performance of the proposed method and compare it with two other baselines EWC and OWM. 3. The authors claim that OWM is effective if tasks are similar, but not when dissimilar.
This paper proposes a new way to create compact neural net, named Atomic Compression Networks (ACN). The experiments shows ACN produces better accuracy than baseline models including a FC network, a Baysesina compression method, etc. The idea is simple, and is evaluated on a number of datasets and compared with fully connected, single layer, and several compression schemes. Compression of Fully-Connected Layer in Neural Network by Kronecker Product (Zhou, Wu)
However, it seems to me that this drastic scheduling strategy sounds like very similar to the traditional approach that trains the floating point network first and then finetune the quantized one, except for the fact that this proposed algorithm repeats this process a few times. The authors proposed a series of improvements, including alternative optimization, dynamic scheduling, detach and batch normalization to help boosting the performance to SOTA under 4-bit quantization. The idea seems on a high level to be interesting and simple; train floating point models that can fit the data well while also encouraging them to be robust to quantization by enforcing the predictive distributions of the fixed and floating point models to be similar in the KL-divergence sense. This is achieved by introducing a loss function that consists of a linear combination of two components: one that aims to minimize the error of the network on the training labels of the dataset and one that aims to minimize the discrepancy of the model output with respect to the output of the model when the weights and activations are quantized. Do you absorb the scale and shifts in the weights / biases before you perform quantization or do you quantize the weights and then apply the BN scale and shift in full precision? using separate batch normalization statistics for the floating point and quantized models. Furthermore, it should be noted that the discrepancy in BN in quantized models was, as far as I am aware, firstly noticed at [1] (and subsequently at RelaxedQuant) and both of these methods simply re-estimated the moving averages during the inference time. The results on ImageNet under 4-bit quantization are strong and convincing, but the paper could benefit from conducting additional experiments on different datasets and bitwidth configurations.
The paper proposes to use Contrastive Predictive Coding (CPC), an unsupervised learning approach, to learn representations for further image classification. The authors augment contrastive predictive coding (CPC), a recent representation learning technique organized around making local representations maximally useful for predicting other nearby representations, and evaluates their augmented architecture in several image classification problems. - The CPC is utilized to enhance spatially predictable representations which benefits a lot data-efficient image recognition. Title: DATA-EFFICIENT IMAGE RECOGNITION [Summary] -This paper introduces Contrastive Predictive Coding (CPC) image recognition in the data-efficient regime. The extensive experiments show that CPC enables data-efficient image classification and surpassed other unsupervised approaches. This paper only proposes some minor improvements based on the original CPC method and use a deeper network to get better performance. Pros: Owing to its generality (CPC assumes only a weak spatial prior in the input data), and cheap computational cost relative to earlier generative approaches, CPC is already a promising unsupervised representation learning technique. [Cons] - In Sec. 4.1, four axes are identified to upgrade CPC v1 to CPC v2. + Figure 3 shows clearly the performance improvements of a series of incremental modifications to the original CPC methods.
Additionally, the Music Transformer model is also conditioned on a combination of both "style" and "melody" embeddings to try and generate music "similar" to the conditioning melody but in the style of the performance embedding. The performance conditioning vector is generated by an additional encoding transformer, compared to the Music Transformer paper (Huang et. It took me a couple of passes and reading the Music Transformer paper to realise that in the melody and performance conditioning case, the aim is to generate the full score (melody and accompaniment) while conditioning on the performance style and melody (which is represented using a different vocabulary). Why use this feature compared to existing techniques for measuring similarity between symbolic music pieces? Finally, it would be useful if the authors comment on existing methods for measuring music similarity in symbolic music and how their proposed feature fits into existing work. ## summary In this paper, the author extends the standard music Transformer into a conditional version: two encoders are evolved, one for encoding the performance and the other is used for encoding the melody. The authors also mention an internal dataset of music audio and transcriptions, which can be a major contribution to the music information retrieval (MIR) community. Measuring music similarity is a difficult problem and the topic has been the subject of at least 2 decades of research. The main strategy is to condition a Music Transformer architecture on this global "style embedding". [ref2] Conditional image-to-image translation, CVPR'18 This paper presents a technique for encoding the high level "style" of pieces of symbolic music. I am not working on music generation but I list two CV related papers about conditional image translation, which mathematically describes "an image with specific style". 2. By checking the music Transformer, in Table 3, it is not surprising to see that the proposed method outperforms the corresponding baselines, because no conditional information is used. The music is represented as a variant of the MIDI format. 3.     It is better to give some mathematical definition of music generation with specific style. Is it computational efficiency? Why not compare the conditioning melody with the generated performance similar to query-by-humming? 5. In section 5.2, a conditioning sample, a generated sequence and an unconditional sample are used to compute the similarity measure. I also don't see the connection between this proposed feature vector and using the IMQ kernel for measuring similarity. 2. Figure 1 should be clarified or another figure should be added to show how the melody conditioning works.
They use enhanced loss scale, quantization and stochastic rounding techniques to balance the numerical accuracy and computational efficiency. For example, how much improvement can this work achieve when just using enhanced loss scaling method or a stochastic rounding technique?
Specifically, every update in the back propagation algorithm is being replaced by an implicit update except for the intermediate parameters that receive a "semi-implicit" update. This theory suggests a lot of stability properties for the implicit SGD update of Equation (27).
For example, in Table 2, BNN (shown as MU) is significantly slower than DU/DBNN and DNN. The idea is to combine online vector quantization with Bayesian neural networks (BNN), so that only incremental computation is needed for a new data point. In the introduction, the authors motivate the proposed DBNN by saying that BNN needs dozens of samples from weight distributions and therefore is rather inefficient. This paper proposed differentiable Bayesian neural networks (DBNN) to speed up inference in an online setting. Since these OCHs are differentiable, the proposed DBNN model can be used for streaming input data with time-variant distributions. It would be better if the problem setting of online inference is introduced at the beginning, followed by the overview of DBNN and then the OCH details.
This paper proposed a new method for knowledge distillation, which transfers knowledge from a large teacher network to a small student network in training to help network compression and acceleration. It looks to me the proposed method use an ad-hoc selected layer to transfer knowledge from teacher to student, and the transfer is indirect because it has to go through the pre-trained subnetwork in teacher. 2. Utilizing the "soft targets" to transfer knowledge from teacher to student model is not first proposed by Hinton et al. (2015). The paper suggests a new method for knowledge transfer from teacher neural network to student: student-teacher collaboration (STC). Finally, the choice of teacher output as target over soft target and ground truth, which was previously motivated in the theoretical section, is shown to be superior in the experiment. Interspeech-2014, pp.1910-1914. 3. Leveraging back part of teacher model's guidance to improve student performance has been investigated by other researchers on OCR tasks in
The writing is well and clear and there are some minors issues: - Further analysis and explanation for using \\hat{E}=Diag(H)to estimate the causal influence might be needed. Technical questions: 1) The estimation of binary causal influence matrix E is set as \\hat{E} = Diag(\\hat{H}). This paper proposes "Back-to-Back" regression for estimating the causal influence between X and Y in the linear model Y=(XE+N)F, where the E denotes a diagonal matrix of causal influences.
As the authors addressed, Mahalanobis detector proposed by Lee et al. (2018b) requires validation to determine weights for feature ensembling, but the validation can be done without OOD data by generating adversarial samples as proposed in the same paper. - Summary: This paper proposes an out-of-distribution detection (OOD) method under constraints that 1) no OOD is available for validation and 2) model parameters should be unchanged. =================== The work proposes a system for detecting out-of-distribution images for neural networks under strict limitations of not retraining the network or tuning parameters with out of distribution validation data in mind using Compression Distance in a novel way. They propose to use compression based distance measures off the shelf from standard compression techniques to detect spatial feature patterns in feature space and demonstrate its effectiveness on several datasets and comparison with baselines is reported and well discussed. Again, weights can be validated by adversarial samples to satisfy the constraints. 1. The problem setting is clear and their approach is interesting and makes sense. Therefore, rather than comparing with the vanilla version (only using last latent space) or the alternative "assemble" method (concatenating all average-pooled features), they had to compare their method with the model validated by adversarial samples, which essentially satisfies the constraints. Detailed comments: 1-(a). Adversarial attack and OOD (which is hard to detect) are closely related: they are both in off-manifold. The difficulty of OOD detection can be considered to be coming from overlapped manifolds in the latent space. Their main difference would be, while adversarial attack is very close to the clean data in the data space, OOD is relatively far from the in-distribution in the data space. - Comments: 1. As addressed by the authors, feature concatenation ("assemble") is not effective for the Mahalanobis method but the proposed method. This paper proposes a new framework for out-of-distribution detection, based on global average pooling and spatial pattern of the feature maps to accurately identify out-of-distribution samples. To me, if the work could be changed to compare against works which are not so tightly constrained, not for the purposes of holding it to the same standard but to understand it's relative standing, or to better justify the very strict constraints which somehow, despite out-of-distribution detection being a popular upcoming topic, apparently only has one other paper that matches it. Although Table 2 in Lee et al. (2018b) shows that the performance is not better than the case when we have an explicit OOD data for validation, it reasonably works well.
Unfortunately, delaying the updates opens up for the same problems as asynchronous SGD (ASGD), i.e., slower (none) convergence or staleness problems. The paper presents an approach - Delayed and Temporally Sparse Update (DTS) - to do distributed training on nodes that are very distant from each other in terms of latency.
As the authors point out (and I agree), the paper constitutes a compelling reason for theoretical research on the interplay between overparameterization and parameter recovery in latent variable neural networks trained with gradient descent methods. This paper performs empirical study on the influence of overparameterization to generalization performance of noisy-or networks and sparse coding, and points out overparameterization is indeed beneficial. The paper "aims to be a controlled empirical study making precise the benefits of overparameterization in unsupervised learning settings. A small gripe: the authors promise " a controlled empirical study making precise the benefits of overparameterization in unsupervised learning settings". This paper investigates benefit of over-parameterization for latent variable generative model while existing researches typically focus on supervised learning settings. More ablation study and more experiments on general models will clarify what is going on in the over-parameterized model for latent generative models. There could be at least some intuitions on why overparameterization helps noisy-or models. In line with the findings for supervised settings, the authors find that overparameterization is often beneficial, and that overfitting is a surprisingly small issue. 1. Overparameterization is better than underparamterization and exact parameterization is not surprising. - the effects of extreme overparameterization The generative models to obtain disentanglement representation could be investigated in the frame-work of this paper. Hence, I think what investigated in this paper can be discussed by relating sparse coding theories.
In particular, I believe it would be more accurate to say that IF the autoencoder has bad local minima then the VAE is also likely to have category (v) posterior collapse. 1. Summary The paper theoretically investigates the role of "local optima" of the variational objective in ignoring latent variables (leading to posterior collapse) in variational autoencoders. Aren't they the same? e. The experiments consider training AEs/VAEs with increasingly complex decoders/encoders and suggest there is a strong relationship between the reconstruction errors in AEs and VAEs, and this and posterior collapse. I think the authors should think about the cases where the reconstruction error is low, and see if there is an issue of posterior collapse in those setups. This paper tries to establish an explanation for the posterior collapse by linking the phenomenon to local minima. If I am understanding correctly, the final conclusion reads along the lines of 'if the reconstruction error is high, then the posterior distribution will follow the prior distribution'. The paper first discusses various potential causes for posterior collapse before diving deeper into a particular cause: local optima.
First, the authors perform an extensive study to understand the source of what they refer to as 'environment bias', which manifests itself as a gap in performance between environments used for training and unseen environments used for validation. The authors use this rather extensive study to motivate the need for new features (semantic features) to replace the RGB image that their investigation finds is where much of this 'environment bias' is located. The second contribution is to use semantic information, compact statistics derived from (1) detected objects and (2) semantic segmentation, to replace the RGB image and provide input to the system in a way that maintains state-of-the-art performance but shrinks the performance gap between the seen and unseen data. This paper aims to identify the primary source of transfer error in vision&language navigation tasks in unseen environments. As I mention below, the metric for success on these tasks is performance on the unseen data, and, though an improvement on their 'bias' metric is good anecdotal evidence their proposed methods are doing what they think, the improvements in this metric are largely due to a nontrivial decrease in performance on the training data. This paper has some pretty exhaustive treatment diagnosing the source of the agent's 'environment bias' (which, as I discuss below, I believe is more accurately referred to as 'overfitting') in Sec. 4. The authors conclude that of the three sources of information provided to the agent (the natural language instruction, the graph structure of the environment, and the RGB image), the RGB image is the primary source of the overfitting. They show that using ImageNet class scores as visual features results in significantly less transfer gap than using low-level visual features themselves.
In this paper's formulation, W is named as a weight generation matrix, which is choosing to be random and i.i.d. with certain probabilities. With regard to the biological plausibility of the method, it is unclear to me how the evolutionary method proposed here can enable learning in typical scenarios such as conditioning experiments in animals. Therefore, the authors provide an alternative approach, named neural net evolution (NNE) that follows evolutionary theory. 2). in the 3rd papragraph in Introduction: "Suppose that the brain circuitry for a particular classiﬁcation task, such as "food/not food",is encoded in the animal's genes, assuming each gene to have two alleles 0 and 1".
They find that (i) the auto-encoder representation does not capture the semantic information learned by the supervised representations and (ii) representations learned by the model depend on the label taxonomy,  how the targets are represented (1-hot vs. They also use a human judgement dataset based on odd-one-out classification for triplets of inputs as comparison to evaluate whether the CNNs are able to capture the linguistic structure in the label categories as determined by the relation of the superordinate labels to the basic labels. was there any relationship between these categories and the similarity of representations learned by the convnet?
The main contribution of this paper are: 1. The use of a heteroscedastic GP when performing Bayesian Optimization, this is in contrast to the more common practice of assuming homoscedastic noise, even when this does not quite fit the data. Under the noise setting, the Random approach performs very competitive to BO. The y-axis in the experiment, the paper has considered the objective function value + noise. 2. They introduce two new acquisition functions that incorporate the predicted observation noise, either making candidates more likely or less likely to be chosen when predicted noise is higher, depending on the requirements.
The proposed method used a classification function of a fractional graph semi-supervised learning (GSSL) [De Nigris et al., 2017] as a graph filter. It is based on a novel fractional filter for graph conv networks, which generalizes several previously employed graph semi-supervised learning frameworks, by introducing a fractional hyperparameter (sigma in the paper), using fractional powers of the Laplace operator. This paper presents a fractional generalized graph convolutional networks for semi-supervised learning. This paper proposes a fractional graph convolutional networks for semi-supervised learning. The authors design a new graph convolutional filter based on Levy Flights, and propose new feature propagation rules on graphs. The key approach of the proposed method is to apply a classification function (equation (3)) obtained by solving a GSSL problem to graph convolutional networks.
This paper presents a technique for model based RL/planning with latent dynamics models, which learns the latent model only using reward prediction. This is in contrast to existing work which generally use a combination of reward prediction and state reconstruction to learn the latent model. The proposed model uses an encoder to learn embedding the state to the latent state, a forward dynamics function to learn dynamical system in latent state space, and a reward function to estimate the reward given a latent state and an action. This paper claims that one only needs a reward prediction model to learn a good latent representation for model-based reinforcement learning. Summary: This paper proposes a novel algorithm for planning on specific domains through latent reward prediction. They introduce a method that learns a latent dynamics model exclusively from multi-step reward prediction, then use MPC to plan directly in the latent space.
This paper performs a regret analysis for a new hierarchical reinforcement learning (HRL) algorithm that claims an exponential improvement over applying a naive RL approach to the same problem. For instance, the paper begins by contrasting HRL approaches with a number of standard RL algorithms, saying that approaches such as AlphaGo do not require high-level planning. What is an example of an algorithm that does planning in a standard RL setting?
The approach starts from the standard modelling assuming iid samples from a Gaussian distribution with unknown mean and variances and places evidential priors (relying on the Dempster-Shafer Theory of Evidence [1] /subjective logic [2]) on those quantities to model uncertainty in a deterministic fashion, i.e. without relying on sampling as most previous approaches. This term was manually added as additional regularization to "prefer the evidence to shrink to zero for a sample if it cannot be correctly classified" in (Sensoy et al., 2018), and a different regularization was used to encourage distributional uncertainty in [3]. Predictive uncertainty estimation via prior networks. Even though the presentation largely follows (Sensoy et al., 2018) and uses terms from theory of evidence, the derivation actually is more aligned with the prior network [3] under the Bayesian framework which is missing from the references. The main idea follows the evidential deep learning work proposed in (Sensoy et al., 2018) extending it from the classification regime to the regression regime, by placing evidential priors over the Gaussian likelihood function and performing the type-II maximum likelihood estimation similar to the empirical Bayes method [1,2]. On the quantitative side, the baseline models considered in Section 4 are mainly concerned with epistemic uncertainty estimation. So it would be good to explicitly discuss which uncertainty estimation was compared with.
The author(s) state "Colors are used to distinguish between clusters." This statement is unclear as to whether the author(s) are using the class label or learned latent cluster. The variational model can follow with the Q distributions as the author(s) have it. However, the author(s) unfortunately did not mention "Approximate Inference for Deep Latent Gaussian Mixtures" (Nalisnick, 2016). The author(s) appropriately cite VaDE (Jiang, 2017) and DEC (Xie, 2016), "Deep Unsupervised Clustering with Gaussian Mixture Variational Autoencoders" (Dilokthanakul, 2017).
Once a reward is achieved they can compute the embedding of the corresponding state as a goal under the CPC learnt representation either directly ie. Moreover, the reward shaping method assumes we know the goal state but using CPC feature does not. In light of these other methods, I'm not sure your discussion on only using predictive features for reward shaping is accurate, and instead these claims should be softened for only features learned through CPC. I think using the embedding learned through CPC could provide meaningful semantics for representation learning and for reward shaping (as done in the current paper), and encourage the authors to continue down this line of inquiry.
This paper removes the cardinality limitation and gives two kinds of results: 1. PointNet (resp. While the theorems proved in the paper are original and novel, they are a refinement of the already known results regarding approximation theorems for PointNet and DeepSets, respectively, hence only a marginal improvement in understanding these function classes. This work examines the fundamental properties of two popular architectures -- PointNet and DeepSets -- for processing point clouds (and other unordered sets). The paper aims to establish novel theoretical properties of known point-based architectures for deep learning, PointNet and DeepSets. The authors provide a new universal approximation theorem on real-valued functions that doesn't require the assumption of a fixed cardinality of the input set. Wasserstein) metric; 2. Only constant functions can be uniformly approximated by both (PointNet, Hausdorff metric) and (Deep sets, Wasserstein). To this end, the authors prove a series of theoretical results and establish limitations of these architectures for learning from point clouds. I thus maintain my recommendation of weak reject PointNet (Qi et al, 2017) and Deep sets (Zaheer et al, 2017) have allowed to use deep architectures that deal with point clouds as inputs, taking into account the invariance in the ordering of points. - The paper shows two specific functions, where one can be approximated by PointNet but not by DeepSets and vice-versa. The presentation in this paper does remove the assumption of a fixed cardinality, but since this seems to be a mild assumption, it is not clear what is gained by this (beyond mathematical elegance). They further provide examples of functions that can't be mutually approximated by PointNets and DeepSets. It is not clear what is gained by the main difference, i.e. removing the cardinality.
The hypothesis is that if the regressors demonstrate good accuracy, then the word embeddings contained information relevant to "numerical common sense". This paper attempts to study if learned word embeddings for common objects contain information about "numerical common sense". To the best of my knowledge, this is the first paper that attempts to analyze learned word embeddings in the context of numerical common sense. This paper aims to predict typical "common sense" values of quantities using word embeddings.
This paper proposes to apply MAML-style meta-learning to few-shot semantic segmentation in images. It would strengthen the argument to include an existing few-shot segmentation algorithm in Figure 2. Finally, it is shown that pre-training using meta-learning on similar segmentation tasks works better then just using ImageNet based model pre-training.
They cover most design choices in recent works of on-policy RL methods. The paper presents an empirical evaluation of many algorithmic choices made in the implementations of on-policy actor-critic algorithms in deep reinforcement learning (RL). As many recent work investigates the design choice of only on-policy RL methods, it would be interesting if in introduction there is discussion on why off-policy methods are not considered or should it be addressed in a different way in another research?
Weak points The HN -> RBM mapping is quite clear, but the reverse RBM This paper shows a relationship between the project rule weights of a Hopfield network (HN) and the interaction weights in a corresponding restricted Boltzmann machine (RBM). The mapping from HN to RBM is facilitated by realising that the partition function of BN can be seen as the partition function of a binary-continuous (Bernoulli-Gaussian) RBM. This mapping is shown to allow for significantly better initialization (than random) of the weights of a RBM, in the sense that the training is then much faster to reach comparable generative and/or generalization performance. The paper is slightly incremental as similar mappings were known, but it remains a relevant contribution, and the aspect of using this mapping as a way to boost learning in RBM seems new, and interesting.
Through extensive experiments, the authors show that this method produces more efficient networks while reducing the computational cost of training, still maintaining good validation accuracy, compared to other NAS or pruning/growing methods. It seems to be a more principled approach than previous NAS or separate pruning/growing approaches.
The case of using general SDEs (not only those derived from SMLD and DDPM) is mentioned only briefly, leaving it unclear if using a general SDE would require relatively simple changes, or if the proposed model is limited to SDEs derived from
I'm also curious on what sampling only the trained energy model looks like (without using the trained VAE parameterization) at evaluation time With VAE as a backbone, the sampling can be transferred to the latent space and the residual \\epsilon in the image space, which is much more friendly to MCMC sampling. The authors propose efficient training and sampling procedures, in which the VAE is trained first and during the EBM negative-phase, samples are drawn from the joint (x,
It would've been very interesting to see how these techniques improve the performance of previous neural LTR models; log1p transformation, data augmentation, and model ensembling would straightforwardly apply to other neural models as well. When I was reviewing other LTR papers, I often had to point out that the proposed method significantly underperforms LightGBM. Concerns: Regarding the proposed solutions, the authors use data augmentation to improve neural LTR models. Also, although the idea of applying these standard techniques on LTR seems straightforward, but I argue that's only due to the benefit of hindsight; neural LTR has been a fairly active area of research, yet these techniques haven't yet been widely used in LTR literature. Originality, Significance: This paper establishes reference points for modern LTR research. Still, DASALC significantly outperforms previous neural LTR approaches. Comparing to traditional gradient boosted tree-based LTR models, is it really worth putting efforts into studying neural LTR models? Pros: This paper discusses potential reasons why neural LTR models are worse than gradient boosted decision tree-based LTR models, and uses empirical results to show the effectiveness of the proposed solutions. They discuss why neural LTR models are worse than gradient boosted decision tree-based LTR models, and introduce some directions to improve neural LTR models. Understanding the reason for LightGBM's superiority could potentially help us to develop better LTR models, neural or not neural.
The authors propose a novel Adversarial Sparse Convex Combination (ASCC) method in which they model the word substitution attack space as a convex hull and leverages a regularization term to enforce perturbation towards an actual substitution. Therefore, they can adversarially train the perturbations and model to obtain a robust model and robust word embeddings.
Pros: One of the first theoretical works on the gradient dynamics and convergence properties of the deep equilibrium models [1] (and implicit models in general [2,3]), which are quite different from conventional deep networks.
Updating the symbol would change it from old L to new R', but since the technique does not (and cannot, without a world model) update the environment actions, it seems like the listener and speaker would then train on this misaligned tuple of communication action R' and environment actions to move Left. It seems like conditioning the communication action on the agent's environment action for that timestep might help, but would only be a partial solution: if an agent must speak now but take their first significant environment action in the future, we would have the same problem. More generally: how can we make sure that the updated communication actions still align in intent with the agent's environment actions that we cannot change? The paper proposes a communication correction mechanism where, during the centralized training, messages there were received in the past from other agents are reevaluated according to the updated policy. 1) the speaker only has communication actions (and no environment actions), and ---- Summary ---- The paper proposes a method for modifying an experience replay when learning in communication environments, by relabelling messages using the latest policy.
"Fig 5 for the factorized readout …" (I didn't get it until reading it four times and looking for these results in Figure 5 twice.) The paper presents an experimental study on predicting the responses of mice V1 neurons with computational models. The NN has a "core" that is shared between all neurons, and a neuron-specific readout. Since the core is shared across neurons, these stimulus-response pairs can be learnt in a massively parallel manner. They train a neural network consisting of a "core" and a "readout" in an end-to-end fashion to learn stimulus (visual inputs) -- response (single neuron activity) pairs. Introduce a novel readout mechanism that allows models to be shared fully across neurons which in turn helps transfer learning.
In my view, the submission's main contribution is a promise to publicly release inference time and power usage measurements and code for 5-6 different hardware devices on two existing NAS benchmark tasks: NASBench-201 and FBNet. The submission also provides some analyses on this data. Originality The design of HW-NAS-Bench is mostly straight-forward in that it builds upon established search spaces and NAS benchmarks and "only" estimates hardware metrics such as latency and energy consumption on target hardware. The authors' datasets, which contain inference time measurements for several different hardware devices, should make it easier for researchers to experiment with NAS algorithms for finding better accuracy/inference time tradeoffs. The benchmark is based on extensive measurements on real hardware. In addition, Appendix A only includes one "Pearson correlation" measurement per hardware device, and it's not clear to me whether this number is for the authors' CIFAR-10 benchmark, their ImageNet benchmark, or the union of the two. For this, the authors adopt two popular search spaces (NAS-Bench-201 and FBNet) and measure/estimate hardware performance metrics such as energy costs and latency for six hardware devices (spanning commercial edge devices, FPGA, and ASIC) for all architectures in this search spaces. This code would be potentially very valuable for practitioners that plan to estimate hardware costs for different search spaces or devices.
Proposes an extension of RFA with gating that improves accuracy on language modeling, relative to softmax attention. Where "Performers" goes with positive orthogonal random features (to improve over vanilla RFA), this paper adds a gating mechanism: this adds the possibility to learn some monotically decaying attention over older context, similar to learned receptive fields of attention (as in e.g. [Sukhbaatar et al. 2019]). The results are strong. Unlike Linear Attention, the proposed RFA outperforms the original multi-head attention baseline on both LM and MT tasks. Extension of RFA with a gating mechanism appears to be effective, but I do not believe the claim that it is hard to apply this to softmax attention is valid, leaving the main contribution an exploration of a couple different kernels for linear attention. Questions Can you clarify how the RFA defined in section 3.1 differs from the linear attention in Katharopoulos et. The paper presents a linear time and space attention mechanism based on random features to approximate the softmax. Strengths The RFA-gated formulation is both more accurate and potentially faster (at least for decoding) than softmax attention, as demonstrated on language modeling.
This paper provides an upper boundary of the generalization error of networks: the sum of its training error, the distillation error, and the complexity of the distilled network. 3)What are necessary/sufficient theoretical conditions for the effectiveness of distillation strategy (from the generalization error bounds)? Essentially the bounds proved, bound the out of sample error with a form of in sample error, average difference in predictions between complex and simple network, and complexity term for the simple network in terms of Rademacher complexity. 4)It may be better to state some discussions for the lower bound on the generalization error. Cons A comparison (e.g. on the tightness of the bound, or the correlation with generalization) between the proposed distillation-based generalization bound and existing generalization bounds in literature may help demonstrating the effectiveness of the proposed bound.
While it is plausible and reasonable to model such data using vector features of type ρi in the hidden layers, the argument for the necessity of gauge equivariance would be even stronger if the input and/or output signal was itself vector valued, for example a velocity or gradient on the mesh. This paper presents Gauge Equivariant Mesh CNNs. The method is motivated by the fact that graph convolutions can be modified for meshes to take into account the angular arrangement of local neighborhoods. The result is a Mesh-CNN that is equivalent to GCNs with anisotropic gauge equivariant kernels. It achieves that by parallel transporting features along edges and spanning a space of gauge equivariant kernels. The architecture is an elegant way to incorporate gauge symmetry on meshes and RegularNonlinearity addresses an important issue for equivariant neural nets. Weaknesses: Neither the MNIST nor the FAUST experiment verify the gauge equivariance. Would it be possible to come up with a space of two-dimensional kernels K_neigh (dependent on the full polar coordinates, including r) while keeping the gauge equivariance? The work presents a novel message passing GNN operator for meshes that is equivariant under gauge transformations. The paper contribution is elegant and significant: Gauge equivariance  is a necessity if you want an anisotropic diffusion. It bridges the more theoretical equivariant convolutions with graph neural networks for mesh processing, which are more commonly used in practice.
The authors prove a theorem (thm 4.1) which describes a basis for the space of kernels in a G-steerable CNN for any compact group G. A practical consideration remains. Though theorem 4.1 reduces construction of a basis of steerable kernels to 1) finding Clebsch-Gordon decomposition of tensor products, 2) describing endomorphisms of irreps, and 3) describing harmonic functions, none of these problems is trivial (or even necessarily solved) for a general compact group G. Steerable CNNs are similar to CNNs but replace channels with G-reps and enforce an equivariance constraint on the kernels. Given that the purpose of this paper is partially to formulate Steerable CNN in precise terms, the fact that δx is informally considered as in L2(X) is very imprecise. theory and so it is not necessary to use physics here to describe steerable CNN. Here, solving the constraint means to construct a basis of the space of steerable kernels.
Gaussian processes (GP) are used to represent the irregularly sampled input. The proposed continuous convolutional layers can be directly applied to input Gaussian Process in a closed form, which subsequently outputs another GP with transformed mean and variance. Summary To work with data that is not sampled on a grid, this work represents the activations of a neural network using a Gaussian process with RBF covariance. These are then approximated with a GP with an RBF kernel using Monte Carlo to produce observations with input dependent noise. This involves defining continuous convolutional layers as affine transformations of the input GP and use rectified GPs to include the nonlinearity in the mapping.
Under this generalized linear setting, they propose a so-called "optimistic closure" assumption which is shown to be strictly weaker than the expressivity assumption in the conventional linear setting. In linear MDPs, typically the transition function is assumed to be a low rank matrix in the span of d feature vectors (over S, A); such an assumption lends itself to regret bounds that only scale with d (and not explicitly with the size of the state space). The paper then proves that LSVI-UCB still enjoys sub-linear regret in the generalized linear setting with strictly weaker assumptions. The authors studies an episodic MDP learning problem, where they propose to study an Optimistic Closure assumption which allows the Q function to be expressed as a generalized linear function plus a positive semi-definite quadratic form.
--Summary: They proposed a robust method for the adversarial attack on VAE using a hierarchical version of β-TCVAE and conduct analysis on the relationship between disentanglement and robustness to support their choice of approach. The paper considers the regularization of latent space toward achieving adversarial robustness against latent space attack. In particular it is found that disentanglement constraints may improve the robustness to adversarial attacks, to the detriment of the performance. In section 3.2, the paper empirically demonstrates the connection between disentanglement and adversarial robustness.
Furthermore, the probability bounds proposed in Theorem 1 and 2 seem to be rater weak bounds, since ξ is bounded by a term inversely proportional to the confidence parameter ω, which means the probability of being safe is high when the safety bound is loose, i.e., χ+ξ, for ξ→+∞ . Also, from the traditional exploration analysis, we have the regret lower bound, which says we must suffer from failure to get the best policy. Based on these, the authors show that it is possible to upper bound the likelihood of reaching an unsafe state at every training step, thereby guaranteeing that all safety constraints are satisfied with high probability. A summarize of point 4 & 5: to me I don't feel the authors well-defined the `''safe exploration''  and give a rigorous analysis on the ``the safety guarantee. A natural comparison against methods that do not utilize the safety constraint features is to include a very large negative reward for failure and see if the algorithms can avoid failure via this signal instead of the separate safety signal.
Summary: This paper proposes a new algorithm called EGRL to improve computation graph running time by optimizing placement of the graph's components on memory. arXiv preprint arXiv:2004.10746, 2020. The paper proposes Evolutionary Graph Reinforcement Learning to solve the memory placement problem. Overall novelty of the paper comes from the neat combination of RL, EA, and GNN, and applying it to memory placement (ML for Systems). The impact of memory placement for DNN has been clearly motivated and is easy to appreciate.
They then determined parameters for the modified ReLU that would minimize the deviation between these activation functions, and computed the minimum conversion error (for converting ANN -> SNN). I was therefore hoping to see an empirical study of the difference between the SNN and the ANN: do the activity of the spiking neural network match the activity of artificial network? To achieve their goal, they described the spiking neuron non-linearity by a "staircase" function of the input (spiking output increases by 1 each time the input gets big enough to reach the next stair), and related that to the ReLu function used in the non-spiking neural net. Summary The authors suggest a relationship between a leaky relu and a spiking integrate and firing neuron model. (3) It's interesting that conversion to SNN actually improves rather than damage the accuracy on ResNet. Weakness: (1) Although the proposed method is much more efficient, it does not show obvious performance improvement compared to existing methods. This scales with the square of the threshold voltage for spiking, divided by the simulation time. (2) This work significantly reduces the simulation time since long simulation time is usually required for converted SNN to reduce error. Using this, they defined their procedure for training SNN to mimic ANN as follows: they trained the ANN with their modified ReLU (which is closer to the SNN activation function but more readily differentiable), and then used the weights from that ANN in their SNN.
The optimal transport problem is first written equivalently as a minimax problem over set of convex functions, as in Makkuva et al. 19. To further explain my concerns, let me start with section 3 which reviews the dual formulation to 2-Wasserstein distance in Eq. (8) and also the connection to the convex conjugate optimization in Eq. (9).
When training data is unlabeled, it performs significantly better than existing OOD detection algorithms based on AE, VAE, PixelCNN++, Deep-SVDD, and Rotation-loss and it performs only slightly worse than the labeled case. This work investigates a classic unsupervised outlier detection problem, in which we do not have any label information and need to learn a detection model from those unlabeled data to identify any inconsistent data points as outliers. This is not surprising since I don't believe label information is crucial for OOD detection anyway, but it's good to confirm it. Overview: This paper proposes an outlier detection scheme based on contrastive self-supervised training for representation learning and cluster-conditioned detection using Mahalanobis distance. There are a number of studies on self-supervised outlier detection approaches as well as what is called few-shot outlier detection approaches, but I cannot find any discussion of those work and the empirical comparison to these methods. It also presents two ways to leverage labeled outlier data if available, including an improved mahalanobis distance method and the application of supervised contrastive learning methods proposed recently.
This paper uses singularity analysis developed in the context of analytic combinatorics to study the relationship between the reproducing kernel Hilbert spaces of the NTK in a deep fully connected ReLU network, the Laplace kernel, and exponential power kernels. In particular, the authors show that, as vector spaces, the RKHS on the unit sphere of the NTK for a ReLU network of any fixed depth are the same and in fact coincides with that of the Laplace kernel. In addition to proving concrete results comparing the Laplace, NTK, and exponential power kernels, it is serves as a proof-of-concept for potentially using the tools of singularity analysis to understand neural networks.
This paper studies how to improve the worst-case subgroup error in overparameterized models using two simple post-hoc processing techniques: (1) learning a new linear classification layer of a network, or (2) learning new per-group threshold on the logits. Summary: The paper builds upon prior work that shows that overparameterized networks learned by ERM can have poor worst-case performance over pre-defined groups. Specifically, the paper demonstrates that this result is not necessarily due to overparameterized learning poor representations for rare subgroups, but rather mis-calibration in the classification layer that can be addressed with two simple correct techniques: thresholding and re-training the classification layer. In the experiments presented, both the thresholding method and the "learn a new classification layer" method significantly improve worst-case group error over ERM and are competitive with DRO.
Summary The paper proposes a defense against recent flavours of model stealing attacks by exploiting the insight that the recent effective attack query out of distribution examples to the victim model. If the adversaries have access to the auxiliary OOD datasets or use other complex OOD datasets, will the defense still work? 2. Diversity Objective If I understand the training objective correctly (Fig. 2b, Eq. 3), each model fi in the ensemble is encouraged to generate a random prediction when queried with out-of-distribution (OOD) inputs. My understanding is that since JBDA and JDBA-TR do not reply on a fixed OOD dataset, EDM trained on auxiliary OOD dataset may be well generalized to these attacks' OOD data.
Variational video prediction is used to generate a sequence of segmentation masks. ---- Summary ---- The paper extends video-to-video translation model of (Wang'18) to video prediction by first generating a sequence of segmentation masks and then translating them into videos. The authors demonstrate the value of modeling temporal evolution of these representations which enables long term video prediction.
-originality & significance: it is the first attempt in considering collective robust certificates (CRCs) by fusing individual adversarial certificate. This work proposes the first collective robustness certificate that considers the structure of the graph by modeling locality in order to derive stronger guarantees  that the predictions remain stable under perturbations. A novel collective certificate fusing single certificates into a stronger one, is proposed by explicitly modeling local structure of input data using graph convolution node classifiers. The arguments are valid on the limitations of independent based certificates for collective tasks. In terms of certified ratio, the collective certificate significantly improve the results compared with existing individual certificates. Although it is a common problem, it was not explored with respect to collective robustness certificates before this work.
The paper studies (1) the relationship between the flatness of minima and their generalization properties, and (2) the connection between two measures of flatness, known as local entropy and local energy. The paper discusses, at length, two previously proposed algorithms named Entropy-SGD and Replicated-SGD and demonstrates, using (i) controlled experiments where Belief Propagation (BP) can be used to estimate the local entropy integral precisely, and (ii) empirical results on deep networks that flatter minima generalize better. While the paper presents some interesting empirical confirmation of the correlation between local entropy/energy and generalization, the algorithms presented in this paper have also been defined in previous work, and this phenomenon has been repeatedly observed with different definitions of flatness [1,2]. They also empirically show that Entropy-SGD and Replicated-SGD, when used to explicitly optimize the local entropy, are able to flatter and better minima (in terms of lower generalization errors). Comments. The paper, as noted above, heavily builds upon existing work, in particular series of works of Baldassi et al. and Chaudhari et al. The papers introduce local entropy, focussing, replicated SGD, BP for calculating the local entropy etc. In this paper the authors compare the performance of broadly these three categories of networks in terms of Local Entropy (Eq. 3) and local energy (Eq. 4) (i.e. difference in training error when the weights are perturbed by factor \\sigma), which was developed in [1].
The authors of the paper propose a new method, the CORES (COnfidence REgularized Sample Sieve), to tackle the important problem of learning under instance dependent label noise. originality In this paper, the authors proposed a novel sample sieve approach for instance-dependent label noise learning. iv) One motivation of Confidence Regularizer is that confident prediction counters the overfitting of noise labels. I would appreciate if the authors of the paper could provide further insights and intuition on why the introduced confidence regularization improves noise robustness. Specifically, in my opinion, the confidence regularizer is a marginal extension of the "peer loss" [1], and the sample sieve algorithm is essentially the same as that proposed in [2], the only difference being a different choice of loss function for training and sieving, to the best of my knowledge and understanding. Since this work is an extension of cross-entropy loss to dealing with label noise, I think the comparison with "Symmetric Cross Entropy for Robust Learning with Noisy Labels"(ICCV 2019) is necessary, as this paper also aims to design a robust loss via modifying cross-entropy loss.
Summary This paper presents a new method for structure pruning called ChipNet. The ChipNet employs continuous Heaviside function with commonly used logistic curve and crispness loss to estimate sparsity masks. There is a submitted paper (to this *CONF*) that includes continuous relaxation of discrete network structure optimization for network growing (not pruning). This paper proposes a new deterministic pruning strategy that employs continuous Heaviside function and crispness loss to identify a sparse network out of an existing dense network.
The paper appears to make some novel links between generalization and decision boundary diffusion geometry, offers an apparently novel analysis of the impact of common adversarial defenses to Brownian adversaries, and at a minimum offers some new insights into how we might think about, and interpret, complex decision boundaries learned by neural nets or other nonlinear classifiers. The paper under review introduces a number of geometric measures (isoperimetric, isocapacitory ratios that relate to Brownian motion or heat diffusion probabilities) that are applied to study neural network decision boundaries locally.
Comments/Questions The assumption on the reward maximizing policy from the target domain is quite strong, this also translates to an assumption about how much the dynamics are allowed to change. Theorem 4.1 provides a theoretical guarantee on the performance of a policy trained on such a modified reward in the source domain by giving a bound on the performance in the target domain, under a very mild assumption that the optimal policy on the target domain achieves similar rewards when put in the source domain. In fact, Assumption 1 is trivially met in most of the environments used in Fig 6, e.g.: for half cheetah obstacle, the target policy does not run in to the obstacle and hence will get the same reward in the source and target environment. The assumption that the rewards for the target and source domains match is suitable for the choice of environments in Section 6 but seems quite limiting to a more general setting where certain rewards may never be observed in the source domain. Further, there is an implicit assumption that the source domain is "free" and the target domain is "restrictive" (e.g. broken ant, obstacle halfcheetah). In Theorem 4.1, the reward-maximizing (entropy-regularized) policy in the target domain is said to satisfy Assumption 1.
Based on their findings, they propose a new supervised pretraining method, which has a good trade-off for transfer learning applications, and validates with other contexts such as few-shot classification and landmark localization. However, I do think that this work is worthwhile for the community because 1) it shines light on a somewhat mysterious exciting new technique and 2) already shows how the findings are useful by using it to improve supervised pretraining, and a new vocabulary for evaluating pretraining techniques. Authors analyse in detail the difference in performance between self-supervised and supervised pretraining and propose a new method to train model which improves over standard supervised models when used as pretraining.
In the case of MetaRKHS-II, the NTK gradient flow based adaptation in Eq(6) forms the inner-loop— just that it is a more efficient inner update than the MAML. This is an interesting adaptation function that evokes the NTK and in the process can help approximate a k-order inner gradient and yet be free of the computational difficulties that come due to this in the standard MAML systems. In first algorithm, no explicit adaptation function is used, whereas in the second, a close form adaptation function which invokes the NTK is proposed - which is a simpler adaptation than that of MAML and hence, offers computational efficiency. I am particularly impressed by the second algorithm, Meta-RKHS-II, which derives a closed form-solution to gradient-based adaptation in RKHS that they then map back into parameter space via NTK. In particular, they establish that parameter adaptation trajectories are equivalent to functional trajectories in some RKHS under the induced NTK, which allows us to bring to bear tools and analysis from that field. The work is interesting and  supported by theory inspired from the NTK theory, and adds to the newly expanding literature in the use of kernels in meta-learning (unlike the authors' claim in the introduction, theirs is not the first meta-learning paradigm in the RKHS cf (Wang et al 2020, Cerviño et al 2019)). In particular, the authors provide rigorous mathematical derivations and show that gradient-based few-shot adaptation of the initialisation can be approximated without inner-loop adaptation under certain assumptions on the NTK (that it induces a relatively linear adaptation space), from which they derive two algorithms that avoid gradient-based inner-loop adaptation.
In particular, through a series of experiments, the paper investigates their uncertainty properties and answers the question "how calibrated are the predictive uncertainties for in-distribution/out-of-distribution inputs?": i) a comparison between GP classification with the infinite-width neural network kernels and finite width neural network classification was provided to test the calibration, It isn't clear what the infinite width layers give you that's better than either using Bayesian linear regression over the output weights, or deep kernel learning. The key innovation presented in section 5: the NNGP is just added as a calibration layer on top of a pretrained NN. Paper summary The authors empirically investigate the calibration performance of NN-GPs in CIFAR10 and several UCI data sets, in three forms: Bayesian inference for the NN-GP function-space prior, through a softmax link function We know that the superior performance of the kernel stops being true at a certain point for classification-as-regression, and I see no reason to think it would be different for "proper" Bayesian inference. Whilst the theoretical contribution is on the light side (no new models or algorithms proposed in this submission), the results from the experiments outweighs this weakness and thus make this paper potentially relevant to the large Bayesian deep learning/GP community. High-level comments I think empirical work like this paper is important: we Bayesians like to justify ourselves using calibration, but unless the beautiful Bayesian methods are actually calibrated, they are not useful. Detailed comments: For a reader such as myself who is not an expert in GP but interested in their application to calibration and OOD data, the general introduction in Eq.(1) and (2) makes no sense.
The approach in [1] formulates the fully differentiable dense SLAM problem (including ray casting) on real world datasets, [2] is a recent novel 3D rendering approach and also does differentiable ray casting,  [3] demonstrates an end to end approach to learning measurement likelihood models with an RL active localization framework, and [4] which is also uses a clever combination of deep learning and multi-view geometry to produce dense 3D maps. The probabilistic graphical model considers the observations, dynamics and latent states of the agent (i.e. the pose and dense map). Strengths The work builds on a fundamentally new and interesting line of generative variational approaches to SLAM It also builds on recent work in differentiable SLAM systems Currently, the related work is too focussed on traditional SLAM approaches (like VinsMono) and 2D/2.5D methods like VAST/DVBF-LM. Experiments on a simulated dataset with a flying drone in a subway and living room environments demonstrate good SLAM performance (that approach traditional methods): bird's eye view projections of the 6 DoF poses and the emitted maps closely match the ground truth poses and the occupancy grid.
This paper tackles online continual neural network learning (following Lopez-Paz & Ranzato, 2017) with a combination of techniques: (1) a controller (or base parameter modulator, or hypernetwork) is introduced which produces task-specific scale and shift parameters, which modulate the feature maps of a base model (Perez et al., 2017); The authors introduce Contextual Transformation Networks (CTNs), a replay-based method for continual learning based on a dual-memory design and a controller that modulates the output of a shared based network to task-specific features. Did the authors try some other continual learning algorithms to avoid forgetting in the base network other than "behavioral cloning"? Pros: Results are generally strong in comparison to other memory based CL techniques on accepted benchmarks. Some good additional experiments are also provided (different memory sizes, smaller datasets, ablations of the three major parts of CTNs). But CTN has very strong performance in continual learning benchmarks. Cons of paper There are related works that I think the authors can mention, which have some similar ideas as in CTN (although all the ideas are never all put together as in CTN): (a) FiLM layers for meta-learning / continual learning / multi-task learning: [1] Requeima et al., 2019, "Fast and Flexible Multi-Task Classification using Conditional Neural Adaptive Processes" The method achieves strong performance on appropriate benchmarks; hyperparameter tuning is handled carefully, which is not always the case in continual learning studies; the authors include a comparison to many relevant methods.
--> an additional baseline "DisentanGAIL w/ domain confusion loss & prior data" is needed, which additionally trains the domain confusion objective on the prior data collected for the DisentanGAIL prior regularization objective, to allow for fair comparison of both regularization approaches with access to the same data missing baseline results on harder tasks: on the harder tasks shown in Fig 3 there is no evaluation of the baseline methods, which makes it hard to judge how hard these tasks actually are for prior third-person visual imitation approaches Suggestions to improve the paper add an additional baseline "DisentanGAIL w/ domain confusion loss & prior data", as discussed in the "weaknesses" section, particularly for the transfer tasks on the bottom right of Fig 2 in which the discrepancy between DisentanGAIL and the baselines is the largest add evaluation of baselines (particularly DisentanGAIL w/ domain confusion loss (w/ and w/o prior data)) to the harder manipulation environments in Fig 3 to show the benefits of the introduced regularizations since the proposed method addresses a concrete problem of prior work (as explained in appendix Section A) with a clear intuition, it could be nice to add a toy experiment early in the paper that demonstrates this effect empirically for an easy-to-analyze imitation problem, showing that for MI=0 the agent cannot properly learn to imitate since it is unable to capture the relevant information This paper proposes a visual imitation learning algorithm that can handle domain shifts between the expert demonstrations and the data generated by the agent. what differences result in the substantial performance difference between TPIL and DisentanGAIL w/ domain confusion loss? The main difference over prior work (eg Stadie et al, 2017) that used domain confusion objectives is the introduction of new regularization objectives on the learned representation of the visual scene. Weaknesses not fully fair comparison to baselines: since the main novelty lies in the introduction of novel regularization objectives, the "DisentanGAIL w/ domain confusion loss" is the main comparison method since the only difference to the proposed method is the representation regularization function. Experiments illustrate that the method was able to learn and be performant despite visual and embodiment differences in the expert and agent domain on various mujoco environments, exhibiting substantially better performance to a different observational IL method. --> since this is a different assumption from prior work on cross-domain imitation it would be good to mention this earlier, maybe in a dedicated "Problem Statement" section for qualitative matching results in Fig 4 in the appendix it would be nice to show the corresponding matches found when using the domain confusion loss instead of the proposed regularizations to see whether some of the failure cases are interpretable This paper studies observational imitation learning, a problem setting in which the agent wishes to learn from expert observations, but the state, action, and observational spaces of the expert and agent domains can vary. The method employs an adversarial imitation learning objective function that incorporates proposed mutual information constraints that are intended to force the representation space to be invariant to the domain of the data sources, and instead only encode goal-completion information. However, it seems that DisentanGAIL w/ domain confusion loss, which applies the MI=0 loss from Stadie et al. 2017, works well on many of the tested domains. I wonder whether it would be possible to show imitation across agents with more drastic morphology differences in the most challenging 7DOF robotic manipulation tasks. Summary This paper proposes a method for performing observational imitation learning -- an existing task that seeks to enable an agent to learn from visual observations of expert behavior in order to roughly imitate the expert behavior.
As strategies for negative sampling in audio and videos are different, it proposes to sample negatives that are similar in the ConvNets' embeddings. For example, [1] uses an audio-visual transformer for audio event classification, and [2] proposes a new joint audio-visual transformer module with both self-attention and cross-modal attention. It builds on a line of research on multi-modal video understanding that utilises transformers where these works: 1) fix one of the transformer models (e.g. BERT) and 2) utilise tokens and thus do not train the approach in an end-to-end fashion. Summary: In this paper, the authors propose a multimodal transformer network for audio-visual video representation learning. The main contributions are the proposed parameter sharing and negative sampling strategies. e) sharing position encoding parameters between modalities and transformer layers f) decomposing transformer weights, so some are distinct and others are shared
The work first identifies the challenges with the current landscape of Masked Language Models with limits to learning sentence-level representations and semantic alignments in sentences of different languages. Summary: The paper presents HICTL which enables models to learn sentence level representations and uses contrastive learning to force better language agnostic representations for large multilingual encoders. Contrastive losses are promising and the paper shows positive results when adding them to the previously proposed XLM-R model. To take care of these gaps, the authors propose using HCTL as an approach that can learn more universal representations for sentences across different languages. Finally, the authors initialize from XLM-R and fine-tune on 15 languages with both monolingual and parallel data.
One can do state-dependent message passing in such architectures without the transformers (see DICG [1] for an example with attention and graph convolutions). The motivation that graphical neural networks are bogged down by their message passing framework is not necessarily a motivation for using transformers. The author claims that the SMP paper does not work better due to the morphology encoding and then they point out that it instead works because of the encoding of the subtrees and some specific detail related to message passing. Instead, they claim that the benefits from being able to encode this morphology are counteracted by the difficulty in having to train the graphical neural network using the message passing system. Although Fig. 5 shows changing attention patterns, it doesn't warrant the confirmation that the proposed architecture benefits from "state-dependent message passing of transformers" which itself consists of two things. The manuscript studies the usefulness of Graph Neural Networks (GNN) in incomplete environments for Multitask Reinforcement Learning (MTRL). In other places, the paper contrasts transformers with GNN-based methods ("substantially outperforms GNN-based methods"), as if transformers were not GNNs. To avoid confusing readers, it would help to explain that GNNs are a broad class that includes both transformers and SMP, which differ in their message passing schedules, etc. The paper instead forgoes trying to input the body structure and uses a transformer based architecture that is capable of learning the appropriate (even dynamic) graph structure actually useful for control. This paper proposes that recent methods that used graphical neural networks to help solve the multitask reinforcement learning problem and assume that there's an advantage from being able to encode the agent's morphology using a graphical neural network do not provide additional generalization and benefits for learning.
To achieve this, the paper assumes a Boltzmann distribution on the demonstration policy and a reward function that is linear in some pre-trained state features. This paper introduces an algorithm, called deep reward learning by simulating the past (deep RLSP), that seeks to infer a reward function by looking at states in demonstration data. The gradient estimator involves simulating a possible past from a demonstration state (using a learned inverse policy and inverse transition function) and then simulating forward from the possible past (using the policy and a simulator) The gradient is then the difference between features counts from the backward and forward simulations. [1] Reward Learning by Simulating the Past (RLSP) This paper studies the question of learning rewards given only certain preferred or terminal states. (4) Based on an educated guess of the problem setting (per my understanding), an intuitive and perhaps simpler algorithm would be to learn a goal classifier using the provided {s_0} data, i.e. P(s=goal), as well as a forward dynamics model P(st+1|st,at). Methodologically, the paper stays close to the ideas of RLSP, but instead of computing relevant quantities (optimal policy, forward and inverse dynamics) through explicit derivations, it suggests most exact steps can be replaced by leveraging deep learning, reinforcement learning and self-supervised learning. The idea is to train a reward function that explains both the past trajectory and the futur trajectory from that state, assuming that the very goal was that state (hence, the assumption of larger rewards in the past than in the future induced by the gradient). Also, I would have expected a discussion regarding the environmental limitation of this approach, for example, when a state can be completely misleading, or at least not providing any information (for example, the initial state of an environment). It requires access to a simulator of the environment which requires being able to reset the environment to arbitrary states.
Pros: The authors combine the idea of differentiable indexing in Spatial Transformer (Jaderberg et al., 2015) into the memory of Kanerva Machine (Wu et al, 2018a;b) and prove by experiments that this allocation scheme on the memory helps improve the test negative likelihood. In contrast to the Kanerva Machine, the authors simplify the process of memory writing by treating it as a fully feed forward deterministic process, relying on the stochasticity of the read key distribution to distribute information within the memory. Strengths: They designed a new Kanerva Machine having a simplified writing mechanism and sharable part-based memory. The paper proposes a generative memory (K++) that takes inspiration from Kanerva Machine and heap memory allocation. The Kanerva machine: A generative distributed memory. Specifically, this paper proposed a novel memory allocation scheme,  replacing the stochastic memory writing process in prior works KM[1]  and  DKM[2] with a set of deterministic operations. 1508-1518. 2018. This paper proposes a new memory mechanism based on the Kanerva Machine inspired by computer heap allocation.
This paper proposes a novel framework called VA-RED2 to reduce spatial and temporal features to be computed for video understanding, which can reduce FLOPs when inferencing the video but remains the performance. RubiksNet: Learnable 3D-Shift for Efficient Video Action Recognition, ECCV2020 Summary The paper presents a framework to reduce internal redundancy in the video recognition model. Cons: This submission proposes to address the problem of slow video inference, but the only metric they report is the Gflops. For example, AR-Net already has shown that a policy network can decide the video input resolution, i.e., spatial dimension, adaptively, leading to improve both efficiency and accuracy. For the video action recognition task, experiments are carried out using Mini-Kinetics-200, Kinetics-400, and Moments-In-time datasets.
One of the advantages to adopt a decoupled neural network scheme is training speedup, while the network based on compared methods is instantly available, the proposed method requires extra time to search before training. The results seem decent, though a little underwhelming (see W1), but the biggest concern for me is that key concepts are unclear or ambiguous (e.g. definition of speedup is unclear, see W2, "confidence of \\alpha at lower layers", see C2, and how the "backprop" gradients are computed and used in equation Strengths:  The idea to search auxiliary network for decoupled neural network is novel, and the proposed method is verified on multiple widely used datasets. Summary This paper proposes a differentiable architecture search approach for splitting a deep network into locally-trained blocks to achieve training speedup. Q2: Given that the gradients used to train each block are a combination of local and backpropped gradients, how does this method avoid the locking problem, and thus provide speedup? In the spirit of recent trends in greedy layer-wise and indirect training, SEDONA allows gradient information to flow either from the next layer as in backpropagation or from an auxiliary head, trying to make a prediction using the current layer's output.
Summary: This work tries to find a compromise of model-based and model-free methods, using a teacher and student network . Typos: using only using model-free methods This paper presents a student-teacher framework, where the teacher network can be used to select and prioritize the relevant properties of the given dynamical system that should be learned by the student. I get that the proposed framework avoids the problems of model-based and model-free methods, but I am having difficulties identifying what advantages of the two methods that the framework is incorporating. It uses a teacher model to learn to interpret a trajectory of the dynamic system, and distills target activations for a student model to learn to predict the system label based only on the current observation. The ideas of model-based and model-free methods are reinforcement learning concepts, and may not be clear to people who are not in RL. The proposed model is interesting and may lead to a series of follow-up studies that leverage the strengths of both model-free and model-based methods using knowledge distillation techniques. This paper proposes a teacher-student training scheme to incorporate the useful information of trajectory to improve the predictive performance of model-free methods. The teacher network tries to "guide" the student network at the training stage by presenting an interpretation of the trajectory. Unlike existing model-based approaches and model-free approaches, the proposed model takes a middle ground and uses a knowledge distillation-based framework.
First, the authors derived two sufficient conditions for equivariant architectures with the universal approximation property. The universal approximation property for equivariant architectures under shape-preserving transformations is discussed. For instance, it would be great to provide a simple implementation of the minimal universal architecture and show it indeed achieves the rotation equivariant features on the point cloud data. This work is a theoretical paper investigating the sufficient conditions for an equivariant structure to have the universal approximation property. It might be nice to point out rotation equivariant architecture that is not universal
The paper proposes a novel Channel Tensorized Module (CT-Module) to construct an efficient tensor separable convolution and learn the discriminative video representation. This paper presents a new CNN module to learn video feature representations for action recognition, with a particular focus on increasing channel interactions for spatio-temporal modeling. By decomposing the channel dimension into sub dimensions in the typically 4D video data (Time, Channel, Width, Height), one defines spatial-temporal separable convolution for each sub-dimension. Summary The paper proposes a new architecture for lightweight action classification networks, named Channel Tensorization Network (CT-Net). ii) The ablation study could have included the comparison between only channel tensorization and channel tensorization and self attention, which would highlight the new contribution of the paper. Paper strengths I think the ideas proposed in this paper are interesting and I would not be aware of any architecture that would use a similar arrangement of tensorization and attention in the mid-layer part to allow for a lightweight 3D convolution architecture. The visualization supports the claim that the attention mechanism seems to learn to focus more on relevant parts of the video clip.
Summary: It is shown that Dale's principle can be observed in feedfoward ANNs if one uses inhibitory neurons in the form of feedforward inhibition, while the other neurons are purely excitatory. Most neurons in the brains are either excitatory (E) or inhibitory (I) - sometimes referred to as Dale's law. Although, I find the contribution interesting, my enthusiasm is tempered by the following two issues: Although feedforward inhibition has its place in the brain, most connections of inhibitory interneurons with excitatory neurons are reciprocal, resulting in feedback inhibition. 2. The ingredients in the proposed model is well motivated in neuroscience, such as the feedforward inhibition, and E/I balance, as well as no connections between I neurons across the different layers. It appears to be useful for understanding the design of biological neural networks, and at least one type of uses of inhibitory neurons in them. Inspired by the observations of feedforward inhibition in the brain, the authors propose a novel ANN architecture that respects Dale's rule (DANN).
In particular, it focuses on the expected loss reduction (ELR) strategy, analyzes its problem, and modifies the original ELR method to make sure the active learner converges to the optimal classifier along learning iterations. To achieve long run convergence of MOCU to zero (and thus get obtain the optimal classifier), the authors propose a so-called weighted strategy that solve this issue. By the simple modification, the algorithm can overcome the drawbacks of ELR in the convergence to the optimal classifier parameterized by θr. The stuck in the convergence of ELR can be due to the lack of considering the long term effects. Can you show the details of what happened in the ELR active learning? More precisely, the authors introduce the Mean Objective Cost of Uncertainty (MOCU) that captures the expected difference between the error of Bayesian optimal classifier (BOC) and the expectation (against the parameter theta posterior) of the error of the theta-best classifier.
As mentioned above, previous methods had already proposed the usage of graph neural networks with continuous time for the learning of differential equations, and I am not sure that the addition of spatial mesh information to such a graph neural network constitutes a significant enough modification at this point. In contrast to existing works on continuous time ODE formulations with graph structures, the proposed networks incorporate relative spatial information in order for the network to evaluate spatial derivatives in addition to the temporal dynamics. A model for discrete vector y(t) is proposed in the form of coupled ODEs (one for each x_i) with a sparse coupling arising from a neighbouring graph on spatial inputs x, and sharing the same transition function. I suppose the argument is that with arbitrary spatial and time discretization, the method still able to be formulated whereas PDE-Net requires dense spatial discretizations and time discretizations to train. For the latter, the authors demonstrate that for simple cases (pure diffusion) the GNODE approach does a good job to identify dynamics purely over time, while including advection terms significantly increases the error without spatial information. Previously proposed methods either would not work on continuous time, or unstructured grids, or would not be applicable to settings with unknown governing PDEs. This work combines all these features.
With SPPs in hand, they find two key failure modes at initialization in deep ResNets and then develop Normalizer-Free ResNets using Scaled Weight Standardization, achieving competitive performance on ResNet-288 and Efficient-Net. Strength: --The overall idea makes sense and the proposed method of removing batch normalization can reduce computational resources and speed up computing greatly. --The visualization of the key indicators may be helpful for understanding how batch normalization works and how to remove batch normalization.--The experiment results seem that the proposed method achieves good performance in large-scale ResNets. SPP could enlighten that an unusual ReLU-BN-Conv ordering would have some benefits but why should a network without normalization mimick the SPP trend of ResNet? Based on the investigation, the authors first provide ResNet results without normalization with the proposed scaled weight standardization. Cons) The authors seem to have failed to provide any reasons for needing normalization free ResNet over the original BN-ResNet. It is not clear that the trained model without NF with SWS can be used as a backbone that can be directly applied to downstream tasks (e.g., object detection). You don't include any experiments showing your method works well in the small batch regime where batchnorm is problematic.
Compared with Malach 2020 "Proving the lottery ticket hypothesis: pruning is all you need", the authors can highlight what key differences are needed for proof of the binary network. Still, the question to clarify here is that whether "subset (lottery ticket) + binary network" equals to "ternary network". The theory works of this paper are strong and prove that the expressive power of redundant binary(or ternary?) networks can match their denser counterpart. One main concern of the reviewer is the similarity between the paper's approach---training a mask over a binary network---and the conventional ternary network.
Summary The authors present the idea of adaptive stochastic search as a building block for neural networks, as an alternative to other "inner loop" optimization methods like gradient descent. The approach presented in this paper relies on adaptive stochastic search as a differentiable optimization procedure. Pros : Using adaptive stochastic search allows the inner optimization module to take multiple iterations without unrolling the computation graph (unlike meta-learning methods), since the initial value used by the module is arbitrary, and not provided by the network. This paper proposes using adaptive stochastic search as an optimization module within deep neural networks to perform general non-convex optimization. Authors confirm that their approach of using the optimization approach within a deep FBSDE works as expected by getting the optimal solution for cartpole, then use their method to beat random and constant strategies on a high dimensional portfolio optimization problem, for which it is not possible to use methods that unroll the computation graph (due to memory issues). Perhaps the authors could add at least some analysis of the variance of their gradient estimator compared to other gradient estimators for embedded optimization problems for different examples and show how the variance behaves depending on the difficulty of the optimization task (dimensionality, curvature) taking the above perspective into account. Since the authors explicitly treat nonconvex optimization problems they could also make more explicit that a nonconvex optimization problem does not necessarily have a unique optimum. But the approach certainly adds variance compared to having an exact gradient or unrolling many optimization steps. They propose to perform the estimation of the gradient of the optimum of the embedded problem with respect to its parameterization by the differentiation of one step of a stochastic search algorithm.
It then proposes an alignment measure which correlates with generalization for different initial scale. Pros: interesting observation regarding the impact of the initial scale on generalization clearly show the effect in a two-layers MLP with various activation and loss functions propose an alignment measure which have some promising correlation with generalization The observation regarding the scale impacting generalization is novel and interesting as I would have assumed that large initial scale would lead to bad optimization rather than a lack of generalization. Summary: This paper investigates the role of scale in generalization of neural networks. Overview The paper studies how the generalization of the neural network trained with SGD is affected by the scale of the random initialization. Summary of paper: A series of empirical observations are made about the influence of scale of init on generalization (in particular, that a continuum of generalization performance from random to very good can be generated by varying only the scale of init) , and these effects are explained in detail for different activation functions. Following Q1, one also could use different learning rates for different layers depending on the scale of weights and/or activation.
What I agree with the authors are: i) Using properly chosen basis vectors may greatly reduce the memory cost for embeddings, especially for huge vocabulary sizes (e.g. over 100 million). This paper proposes ANT to solve the problem of learning Sparse embeddings instead of dense counterparts for tasks like Text Classification, Language Modeling and Recommendation Systems. The sparse matrix T can also encode domain knowledge (e.g. knowledge graphs). In step 2, they learn a sparse matrix that is used to relate all tokens to the set of chosen anchors. 2) they use a sparse T matrix to relate other tokens to anchors which again has reasonable prior: the meaning of a word can be efficiently defined by a few good chosen anchors. If you want a point estimation of sparse representation + learnable anchors, you don't need a Bayesian model. The authors provide a statistical interpretation of their approach using a generative formulation to the embedding vectors in terms of the latent vectors (using a Indian Buffet Process membership matrix Z). They took a two step approach: in step 1, they learn "full fledged" embeddings for a subset of anchor tokens. Normally the embedding size is around 16 to 64 in real systems.
Preconditioning with the inverse Fisher information matrix (covariates population covariance) is shown to achieve the optimal variance among preconditioned updates (Theorem 1). A5) For parametric least squares with a mis-aligned ground truth parameter, it is shown that early stopping with NGD achieves lower Bias than any other pre-conditioned gradient descent (Proposition 6). A3) For non-parametric regression, gradient descent pre-conditioned with the inverse regularised population covariates covariance is considered. For example, after theorem 1, it says that "Theorem 1 implies that preconditioning with the inverse population Fisher results in the optimal stationary variance... The main preconditioner that is studied in addition to vanilla GD is the (population) Fisher matrix (natural gradient descent or NGD), its empirical counterpart, and its interpolation with GD.
Overall, while I am not quite convinced that the paper achieves its stated goal of showing that the proposed unbiased compression approach always outperforms biased compression + EF, I believe it adds enough to the discussion in this space to merit acceptance. In experiments (Figure 3), authors compare various methods with one set of compressors (1) Rand-K; (2) Top-K + EF; (3) induced compressor with C1 = Top-K/2 and C2 = Rand-K/2; (4) Top-K, where K is a tunable parameter but is the same among all compressors. Empirical results show that, an induced compressor obtained from a class of biased compressors (e.g., top K gradients) has better performance than a biased compressor from that class with the same communication cost. Summary: This paper proposes a framework of compressed communication that can be used to deal with error induced by contractive compressors and can serve as a superior alternative to the existing framework (compressed communication with error feedback (EF)).
This work considers Delay Differential Equations instead of ODE, which allows to implement more complex dynamics and thus achieve estimation of more complex functions. The paper builds on the Neural ODE framework by using delay differential equations (DDEs) instead of ordinary differential equations (ODEs). This can help in the modeling of systems with a time delay effect, and overcome many limitations of the Neural ODE framework. This new family of deep neural networks (NODE) generalize ideas from Residual Networks and consider continuous dynamics of hidden units using an Ordinary Differential Equation (ODE) specified by a neural network. Also, modeling the initial function as an ODE is mentioned, and I wonder whether the implication is that this would involve learning the initial function as a NODE itself.
Originality and significance aspect This paper combines mainly two ideas 1) classic feature selection (choose Xis to drop) with respect to the mutual information (between X and Y) 2) Information Bottleneck (IB) formulation that maximizes the prediction-term mutual information term (between Z and Y) and minimizes the compression information (between X and Z) simultaneously. The paper proposes a new Information Bottleneck objective, which compresses the latent by learning to drop features similar to DropOut. Unlike DropOut, a different probability is learnt for each latent feature/dimension using Concrete Relaxation. Summary: This paper proposes an information bottleneck method, Drop-Bottleneck, that allows the input to be compressed by dropping each input feature with probability p_i. Key idea is to instantiate the compression term of the information bottleneck framework with learned term that sets irrelevant feature dimensions to 0. The improvement on the RL tasks seems to be substantial; however, I would like to know how DB performs when there is correlation between feature dimensions (see below clarification question to see more details). Here are a few questions to authors: What if we just drop the feature space only using the mutual information between X and Y and drop them to achieve a similar number of features that was resulted by DB -- it is basically the classic mutual information feature selection. Experiments show that DB works better than VIB in VizDoom and DMLab when a noisy-TV noise is added to the input images. DB cannot provide the same generality as other IB objectives: the input (latent) has to be sufficiently disentangled already as the objective itself does not encourage further disentanglement by itself. The Drop-Bottleneck objective works directly on the input/latent layer, which means that the compression objective is easy to compute.
Specifically, this paper finds that only partial parameters (critical parameters) are important for fitting clean labels and generalize well; while the other parameters (non-critical parameters) tend to fit noisy labels and cannot generalize well. This paper proposes a method for deep learning with noisy labels, which distinguishes the critical parameters and non-critical parameters for fitting clean labels and updates them by different rules. Using comprehensive experiments on synthetic datasets and real-world datasets, the authors verify that the proposed method can improve the robustness of the classifiers against noisy labels. Extensive experiments are performed to verify that the proposed method indeed helps over the baselines at fighting noisy labels. The view of updating different parameters by different rules is quite novel for learning with noisy labels. The proposed method implicitly exploits the memorization effects of deep models, and can reduce the side effect of noisy labels before early stopping. Since the validation set is also noisy, why does the early stopping criterion adopts the minimum classification error on it?
Similar graph decoder is proposed in previous NAS works [1] and performance predictor are proposed more times. Overall Review: This paper proposes a new scene of fast adaption of NAS, which may be a good direction of NAS & meta-learning. 2019. The authors address neural architecture search (NAS) scenarios. I think the main contribution of this paper is their intuition that performing neural architecture search rapidly from the datasets, while the components are all proposed before in different NAS scenarios. Negative: The authors claim that NAS with meta learning has only been done with small datasets in the past. The experiment shows this method can fast adapt NAS from one image dataset to others and achieve SOTA performance. No comparing to other methods on fast adaptation by NAS such as [2]. Moreover, there are also different kinds of performance predictors in NAS field like LSTM and GCN predictor [1].
The authors present the Bayesian Aggregation (BA) mechanism in the context of Neural Processes (NPs) for aggregating the context information into the latent variable z in the form of posterior updates to z. To summarize, in the context of neural processes I feel the paper makes good methodological contributions in presenting a much cleaner and more natural (from a Bayesian perspective) version of the model that has more of the flavor of standard amortized inference for latent variable models. To start with, the authors clearly demonstrate the value of both Bayesian context aggregation and a MC based likelihood approximation scheme on precisely the same types of problems that existing neural processes papers (e.g., Garnelo et al., 2018) have considered (with the notable exception that the 2D image completion task considers only MNIST as a target dataset). The authors show that this improves predictive performance (in terms of likelihood) compared to mean aggregation MA that it replaces on various regression tasks with varying input-output dimensionality. Second, they replace the step of context aggregation with direct latent variable inference over z. Hence for the experiments, I strongly suggest comparing against CNP/NP/ANP with self-attention in the deterministic/latent/latent path of the encoder.
Experiments It would be great to include visualizations or ablation experiments to illustrate why implicit VIC has a lower empowerment than the two proposed methods. They then compare the empowerment of implicit VIC with and without their corrections in a few toy domains, showing that their corrections do not hurt in deterministic environments, and provide a small increase in empowerment in stochastic environments. Summary The paper points out a limitation of the implicit option version of the Variational Intrinsic Control (VIC) [Gregor et al., 2016] algorithm in the form of a bias in stochastic environments. For this paper's extension to be truly significant, it should show a case with significant state cardinality (e.g. the 3D environment from the VIC paper) where the bias hurts more than the reliance on the learned model. They point out that, in stochastic environments, the implicit VIC formulation in the original paper is missing a term in the mutual information (involving log likelihood ratios of state transitions). The rigorous mathematical derivations are simply re-deriving the VIC mutual information bounds with a new added term and with some extra details on how to do it with a gaussian mixture model. 5-->6 The authors show that implicit VIC is biased in stochastic environment due to its blindness to the effect of its 'option' on the state transition dynamics. Considering the experiments on partially observed environments presented in the VIC paper, this paper chooses a much simpler set of discrete environments for empirical analysis instead of stepping up to more complicated environments which would have strengthened both the motivation for fixing the bias of VIC and the empirical evidence of the GMM algorithm (Algorithm The differences between the proposed algorithm and VIC shows that the intrinsic reward now has an added term which depends on an approximate model of the transition probability distribution. Experiments on simple discrete state environments demonstrate that the original VIC algorithm works well only on deterministic environments whereas the proposed fix works well on the stochastic environments as well.
Finally in section 4, the paper analyzes SGD when for each sampled minibatch in an epoch, we apply n gradient steps with a stepsize ϵ/n and show that performance degrades as n increases, suggesting that the benefit of SGD with larger learning rates is due to the implicit regularizer and not the temperature of an associated SDE. Under this analysis the paper shows that the mean position of SGD with m minibatches effectively follows the flow according to Eq (20) for a small but finite step size, while GD effectively follows the last inline equation in section 2.1. The paper shows empirically on an image classification task that by explicitly including the (implicit SGD) regularizer, SGD on the modified loss behaves similarly to using a larger learning rate when evaluating on the test set. Summary Using backward error analysis, the paper argues that SGD with small but finite step sizes stays on the path of a gradient flow ODE of a modified loss, which penalizes the squared norms of the mini-batch gradients.
In fact, one would think it is a good thing that PaI methods are robust to such variations given that there is not a lot of information available at initialization (note initialization is iid) to perform effective pruning and it seems these methods are robust and perform competitively to unpruned networks. In an extensive and comprehensive study, they show that pruning at initialization methods do naught but set per-layer sparsity rates, where the sparse initialization might as well have been random. However, this comparison is unfair as LTR has additional information which is obtained after training (pruning mask is obtained after training but applied during training) and those PaI methods are not specifically designed to be performed during/after training (even so some perform reasonably well).
Yet another baseline might be using an audio-only mixture of mixtures separation system, perhaps with an oracle assignment system. This paper proposed an unsupervised method for open-domain, audio-visual separation system. While they may not be able to decompose each sound source within the on-screen mixture, one can still leverage it to evaluate the on/off-screen separation. This paper describes a system for separating "on-screen" sounds from "off-screen" sounds in an audio-visual task, meaning sounds that are associated with objects that are visible in a video versus not. The paper presents an interesting approach to solving the on-screen vs off-screen sound problem in audio-visual source separation. The evaluation is sufficient for the on- vs off-screen task, although not sufficient to judge whether the system has learned a completely unsupervised source separator, making the scope of the contribution somewhat more limited than it has the potential to be.
However notice that, if I'm not mistaken, these acoustic embeddings are used zero-shot; it is only the speaker embedding that is the input to fine-tuning, and this only via the normalization parameters. In the first stage modeling, the authors proposed a new phoneme-level acoustic condition modeling in addition to the speaker and utterance-level approaches. This would be useful, or, even more welcome, an ablation study with the acoustic embeddings but not the speaker embedding. Similarly, I highly doubt the utterance-level acoustic condition modelling does not also capture speaker information. This paper proposes AdaSpeech, a Transformer-based TTS architecture derived from FastSpeech, but multi-speaker, and focussed on the task of low-resource, robust, and low-dimensional speaker adaptation. Overall, this is very exciting work, as it not only promises space-efficient voice cloning, but, in doing so, suggests better disentanglement of speaker and phoneme properties in multi-speaker synthesis. There is also a phoneme-level acoustic embedding which is used in the same way, which at inference is taken from random sentences (why not in training?), and, I guess, is supposed to cover phoneme-level idiosyncrasies of the speaker, although this isn't clear to me. On the contrary, the phoneme hiddens used in phoneme-level acoustic predictor do not seem to contain any personal voice information because they are resulted from the phoneme encoder that uses text information only as its input in Fig 1. My intuition would be that your phoneme-level predictor is trained only with phoneme hiddens (textual information only) (do these phoneme hiddens include speaker embedding information?), so at most it models some pitch or prosody information. A global acoustic embedding conditions the decoder in addition to speaker embeddings, in the hopes of accounting for recording conditions, and, I suppose, timbre, which should then be disentangled from the linguistic information from the text in the decoder during pretraining and adaptable to new recordings at fine-tuning/inference. I think the Mel features used in phoneme-level acoustic encoder contain personal voice information. There is little discussion on the theoretical side of the acoustic condition modelling, such as how the authors are able to determine that the utterance-level and phoneme-level vectors are modelling things like room condition.
This paper proposes a new federated learning framework called HeteroFL, which supports the training of different sizes of local models in heterogeneous clients. Lastly, the authors mention that the masking trick 'allows local clients to switch to another subtask simply by changing its mask...'. This paper proposes a framework named HeteroFL to address heterogeneous clients equipped with very different computation and communication capabilities.
There are several instances in the results where the weak label models far outperform the baseline models without weak labels (e.g., Table 3 VAE for "soft"). These disentangled factors of variation in the data are shown to correspond well to the 'abstract concepts' of the human demonstrations. equivallent -> equivalent force-relate -> force-related This paper presents a way to learn from demonstrations with weak or no labels. The premise behind this paper is that even when humans provide labels during a demonstration, those labels often do not fully describe the data (e.g., the human may say "soft" when "fast" would also apply). This is shown in the example of the PR2 robot dabbing demonstrations, including visual data as well robot trajectories. Summary Under the context of learning from demonstrations, the paper studies the problem of leaning interpretable low dimensional representations from high dimensional multimodal inputs using weak supervision. Learning this manifold effectively and in an interpretable way, especially using weak supervision, can significantly change how robots can acquire skills from demonstrations and generalize them to new unseen scenarios. This allows the human provided labels to be decoupled (or disentangled as the paper calls it) from the observations. This paper proposes and interesting and novel way to handle weak labels from human demonstrators. The variables are modeled such that the observation is conditionally independent of the human provided labels given the latent variables.
As an analytical method to solve the input reversion problem, R-GAP should have its intrinsic advantage over previous optimization-based gradient attack (O-GAP) methods such as DLG. This shows that if the attacked model satisfies the full-rank condition, R-GAP can be both faster and more effective than DLG.
Motivated by the sensitivity of RL algorithms to the choice of hyperparameters and the data efficiency issue in training RL agents, the authors propose a population-based automated RL framework which can be applied to any off-policy RL algorithms. It could be more convincing if the authors can test on another benchmark, e.g. ProcGen. I think compared to computer vision tasks with huge neural networks, the search space for the architecture of RL models is much smaller, which can be observed in the ablation study. To achieve this goal, they integrate three technologies, i.e., evolutionary RL for hyperparameter search, evolvable neural network for policy network design, and shared experience replay for improving data usage. Summary: This paper propose a population-based AutoRL framework for hyperparameter optimization of off-policy RL algorithms. First, the author claims that the framework can optimize arbitrary off-policy RL algorithms, why only try on TD3?
The paper also presents results for finite-time convergence of the training dynamics for neural networks to the mean-field limit. The optimization landscape and convergence properties of policy gradient methods have drawn attention in RL theory for a long time, and it is nice to see a work that studies this problem from the perspectives of mean-field limit of neural networks, albeit being completely asymptotic. Although the paper does not provide the convergence guarantee of the mean-field density flow to a stationary point (please correct me if this is wrong), the characterization of the optimality is still a good contribution. Under mild conditions, it demonstrates interesting convergence properties of the particle dynamics to the mean-field counterpart and further, the mean-field dynamics to the global optima.
The proposed approach, called ez-greedy, combines randomly selected options with the well-adopted e-greedy exploration policy to achieve temporally-extended e-greedy exploration. The paper overviews the publicized exploration methods from the perspective of their inductive biases, and clearly states where the inductive bias of ez-greedy would be better suited over e-greedy. Strong points This paper analyze theoretical properties of temporally extended e-greedy exploration in Theorem 1. Basic principle of temporally extended e-greedy exploration is to apply the e-greedy exploration policy for an extended period of time. Although this paper presents a general analysis on temporally extended e-greedy exploration, the presented ideas are too general.
In particular, it proposes to introduce high dimensional and high entropy label representations for group truth, to improve image classification performance from two practical matters --- Robustness and data efficiency, while achieving comparable accuracy to text labels as the standard representation. The traditional classification is conducted in classification, while the high-dimensional label experiment is conducted in regression. The evaluation is conducted in nearest-neighbor way, measuring the distance between the image embedding feature and groundtruth high-dimensional representation (spectrogram) and decide the predicted class label. I am also interested in that if the audio signal is replaced by pre-trained embeddings, like glove or BERT, as label representation, how the effectiveness of labels is compared with the audio signals? The results show that high dimensional and high entropy label representations are more useful, which is observed in the experiments related to robustness and a limited amount of training data. So, to verify the former factor, I think high-dimensional version of class label that does not use the external information should be added, such as after running topic modeling algorithm within the image classification data, and use them as a high-dimensional representation of a class label.
The proposed method contains two main parts: (A) We use an ensemble method to help understand uncertainty of the network, and finally use different step sizes for different layers for updating the network parameters at test adaptation phase, (B) adversarial training as data augmentation to help the test-time adaptation process. In the original MAML setting, the test tasks can be entirely different from the training tasks (for example, we pick test tasks from different classes). Let us denote this as θ0 (2) In the meta-testing stage, we go from θ0, given the few-shot test data, via an SGD process (typical scenario), to get a final model that has good performance on the test data of the test task. It is quite novel to leverage adversarial learning as data augmentation for meta-testing in MAML. The paper also proposes to add task adversarial examples to the training set to help the meta fine-tuning process. Bib Antoniou et al 2019 How to train your maml The paper proposes to reutilize pretrained MAML checkpoints for out-of-domain few-shot learning, combining with uncertainty-based adversarial training and deep ensembles. [Summary] MAML has two stages: (1) In the meta-training stage, given various tasks (but each say only has a few labeled data), we want to arrive at a representation where it can quickly adapt to any test task later.
Pros: This paper is quite novel in many aspects, including modularity v.s. monolithic, constructing task codes by SQL-style aggregation queries, "inverse counterpart" of multitask learning, connections to cognitive science. ########################################################################## Summary: The paper provides insight into the boundaries and feasibilities of a monolithic formulation of multitask learning by neural networks. There is thorough mathematical justifications, case studies of monolithic formulations, guarantees on bounds and learnabilities in the supplementary material. The main contributions of the authors are the following: showing that "the two layer neural network can jointly learn the task coding scheme and the task specific functions without special engineering of the architecture" Is the monolithic formulation of multitask learning effectively: joint learning of the switching function and the task function? Related to the question above, it seems very attractive to me that "the two-layer network can jointly learn the task coding scheme and the task-specific functions without special engineering of the architecture", but how to justify "task-specific" functions/task code has been successfully learned? ########################################################################## Note about the reviewer: My area of research is Bayesian non-parametrics applied on to multitask learning. I feel it is important, the authors highlight that the Simple Programming Cosntructs part of their research derives from their novelty in the formulation of modules in terms of mathematical functions! This makes me expect a more involved discussion about the topic "Modular versus Monolithic Task Formulations".
If instead, the fourier component is meant to be distributed among the input population, it the authors should make this clear This paper proposed a model of navigation based on grid cells and the successor representation (SR). A key insight of the paper is that velocity instructions modify the eigenvalues of the SR but not its eigenvectors, so the eigendecomposition does not need to be recomputed for all candidate velocity instructions, and the eigenbasis can be hard-coded in the neural circuit (by the grid cells). Section 3 suggests the transition structure can only derive from a 2D grid space, and that the method requires a periodic boundary condition hold. This paper shows how SR representation theory can be used in a model of grid cells to plan and navigate towards a target. Pro: the paper shows that a simple model based on grid cells and SR representation can perform navigation in some simple environments. It is stated that  "a computational role for the neural grid codes: generating a "sense of direction" (eq. Given that in the previous section, the authors propose grid cells as being weighted Fourier modes (eq 10), I worry that there might be explicit band cells  in the proposed model. The paper shows that this model can generate several experimentally observed properties of grid cells, and can be used in navigation of novel/mutable environments. It is unclear how the "sense of direction" can be calculated in practice by a grid cell network. it is unclear to me how the successive exploration of possible directions could be implemented in practice by the grid cell network (see below for detailed comments). Does that predict that the grid cells are shared across the different environment? The paper uses this idea to develop new connections with path-integration grid cell models for their prediction, illustrating the method in a gridworld and simulated grid cell domain. Strong points: important contribution to our understanding of how grid cells could support navigation towards a goal rigorous mathematical derivation of the results It is unclear how it is implemented in the network model, and how this information can get to the grid cells.
In this work, the authors provide a method for a posteriori calibration of DNN uncertainty with emphasis on constructing a classifier that has PAC uncertainty guarantees. They demonstrate and explore two use cases: applying this technique to get faster inference in deep neural networks, and using the PAC predictor to do safe planning. The PAC intervals are connected to calibration, and take the form of confidence intervals given the bin a prediction falls in. When using statistical guarantees such as those given under the PAC framework, the iid assumption is almost always necessary. Another concern is the fact that getting well calibrated Clopper-Pearson intervals with good statistical guarantees takes a non-trivial number of samples and it appears this would scale with the number of classes. Take for example a predictor \\hat{f} which assigns \\hat{p}(x) = 0.9 to every input 'x' regardless of its ultimate accuracy on the class, then given a new input with unknown label, it is not clear to me exactly how the framework would use the intervals to improve the uncertainty of this classifier, especially given that the class of this new point is unknown. In particular, it seems to me like the proposed intervals only hold their PAC guarantee when the test-time distribution matches the training distribution. Summary: This paper proposes a method for obtaining probably-approximately correct (PAC) predictions given a pre-trained classifier. Thus, the PAC guarantees in this paper would seemingly be invalidated. In the presence of adversarial examples the uncertainty guarantees presented here are rendered void.
The paper introduced Egocentric Spatial Memory Networks (ESMN), a novel learning paradigm and architecture for encoding spatial memory in a sphere representation. Sec. 4.1.1 mentions training with a convolutional encoder in the context of ESMN, Sec. 4.1.3 and Sec. 4.1.4 only evaluate ESM but not ESMN, while Sec. 4.2 states that "ESM represents map-only inference, while ESMN includes convolutions for both the image-level and map-level inference". The PO baseline performs similarly well as ESM and ESMN in Tab. 1, which makes me wonder whether Tab. 1 really shows the superiority of the proposed approach. In the related work section, one argument made by authors is that ESMN and MemNN/NTM are two different paradigms for learning spatial memory.
Paper builds on an algorithm (GECO, for Greedy Efficient Component Optimization) proposed by Shalev-Shwartz et al. (2011), which was already proven to find a solution to the problem in: O(r* kappa_{r+r*} R(0) / epsilon) where kappa is related to the function's condition number.
If the underlying domain has a uniform discretization, the fast Fourier transformation (FFT) can be used, allowing for an O(nlogn) evaluation of the aforementioned convolution operator, where n is the number of points in the discretization. Experiments demonstrate that the Fourier neural operator significantly outperforms other neural operators and other deep learning methods on Burgers' equation, Darcy Flow, and Navier Stokes, and that that it is also significantly faster than traditional PDE solvers. The Fourier neural operator is by construction (like all neural operators) a map between function spaces, and invariance to discretization follows immediately from the nature of a Fourier transform (just project onto the usual basis). However, our Fourier neural operator does not have this limitation." Paper Summary: The authors proposed a novel neural Fourier operator that generalizes between different function discretization schemes, and achieves superior performance in terms of speed and accuracy compared to learned baselines. Section 6, final sentence – "Traditional Fourier methods work only with periodic boundary conditions, however, our Fourier neural operator does not have this limitation." -> "Traditional Fourier methods work only with periodic boundary conditions. Paper summary: Building on previous work on neural operators, the paper introduces the Fourier neural operator, which uses a convolution operator defined in Fourier space in place of the usual kernel integral operator. Each step of the neural operator then amounts to applying a Fourier transform to a vector (or rather, a set of vectors on a mesh), performing a linear transform (learnt parameters in this model) on the transformed vector, before performing an inverse Fourier transform on the result, recombining it with a linear map of the original vector, and passing the total result through a non-linearity. The paper below uses a spectral solver step as a differentiable layer within the neural network for enforcing hard linear constraints in CNNs, also taking the FFT -> spectral operator -> IFFT route.
"... poor calibration of combining ensembles and Mixup on CIFAR", here it is worth to introduce the kind of ensemble(s) that are used in combining. The authors note how strategies such as mixup and label smoothing, which reduce a single model's over-confidence, lead to degradation in calibration performance when such models are combined as an ensemble. The concerns: ECE is a biased estimate of true calibration with a different bias for each model, so it is not a valid metric to compare even models trained on the same data [Vaicenavicius2019]. The calibration pathology is observed for ensembles with mixup, is it true for other data augmentations such as rotation, cropping, etc?
=== Summary This paper proposes a framework, HyperDynamics, that takes in observations of how the environment changes when applying rounds of interactions, and then, generates parameters to help a learning-based dynamics model quickly adapt to new environments. == Original Review == The paper proposes a model for predicting the dynamics of a physical system based on hypernetworks: given some observed interactions and some visual input, the hypernetwork outputs the parameters of a dynamics model, which then predicts the evolution of the system's state over time. Weaknesses: The main claim of the paper is that hyperdynamics network offers better prediction accuracy and generalization than a standard dynamics model. The authors claim that their "dataset consists of only 31 different object meshes with distinct shapes." It is important to include images of the objects to give the readers a better understanding of how diverse the dataset is and how different the geometry of the "seen" and "novel" objects are. No results are reported for the prediction accuracy on the locomotion task, which would have helped evaluate the performance of the dynamics models more directly than the task scores. === Strengths This paper targets an important question of building a more generalizable dynamics model that can perform online adaptation to environments with different physical properties and scenarios that are not seen during training. The authors have evaluated the method in several object pushing and robot locomotion tasks and shown superior performance over baselines that uses recurrent state representations or gradient-based meta-optimization. === Weaknesses Although I like the idea of this paper, I believe the authors should provide more clarification and illustration of the experimental results to solidify the claims in the paper: (1) What are the objects used in the pushing task?
[Summary] Paper proposed to generate the communication message in MARL with the predicted trajectories of all the agents (include the agent itself). While the peer action prediction based on the agent's own observation supports the decentralized execution, the local observation may not include sufficient information about the others depending on a multiagent domain and thus the action prediction can fail. The method involves agents producing imagined future trajectories using learned environment dynamics, and then communicating some parts of these trajectories to other agents. The motivation is clear. Weaknesses: This work wants to see the effectiveness of the use of intention as a communication scheme under MARL. [Strengh] +) The idea of communication with imagined intention is motivated properly with rich psychological background and also technically sound.
Summary This paper extends neural compression approaches by fine-tuning the decoder on individual instances and including (an update to) the decoder in the bit-stream for each image/video. Summary The paper describes an instance specific finetuning method for image and video compression including finetuning the decoder. Although if you are finetuning (and communicating side information for the prior updates) then it is probably very little extra cost to also update the decoder. The paper claims that "In this paper we consider the extreme case where the domain of adaptation is a single instance, resulting in costs for sending model updates which become very relevant", but this would highly misleading if all the experiments were conducted in a batch compression setting. Quality (5/10) The proposed approach is sound and it would have been interesting to see the gains which can be achieved by fine-tuning the decoder of common neural compression approaches. Alternatively, they could have chosen a different video compression architecture of low complexity but one which is still practically relevant. Strength = Method which also considers to finetune/adapt the decoder side of image compression network, for improved performance. Weakness = Method has only been evaluated with respect to its own baseline method (image compression model without finetuning).
This paper provides a variety of studies to understand the generalization gap between known and novel classes in one-shot object detection. Weaknesses: The paper studies the importance of number of object categories in training dataset and claims that the gap in one-shot detection can be closed by increasing the number of categories. 1) examples. This effect is measured by the authors by trying out the existing Siamese few-shot detector on 4 datasets: PASCAL, COCO, Objects365, and LVIS showing that the gap in performance on the seen training and the unseen (novel) testing categories is reduced when the base dataset has more classes (e.g. on LVIS where there are more than 1K classes, this "generalization" gap is shown to be minimal). The most notable observation was that it was more important to increase the number of object category than to increase the number of instances per each category in order to reduce the generalization gap. -> either The paper suggests that a major factor for increasing few-shot performance in the few-shot object detection task is the number of categories in the base training set used to pre-train the few-shot model on a large set of data before it is adapted to novel categories using only a few (or even
(p.1, Abstract) we generalize the IRL problem to a well-posed expectation optimization problem stochastic inverse reinforcement learning (SIRL) to recover the probability distribution over reward functions. Summary The authors proposed inverse reinforcement learning (IRL) algorithm based on Monte Carlo expectation-maximization (MCEM) that maximizes the predictive distribution of trajectories given the reward distribution parameter (eq (1)). Also, since Bayesian IRL also recovers the reward distribution, I couldn't get the major advantage of the SIRL from this statement. (p.3, Problem Statement) more likely generates weights to compose reward functions as the ones derived from expert demonstrations First, this is wrong: objectworld is not that unique, you could visualize reward over e.g. a gridworld or tabular MDP, and many IRL papers have done so. Prior work has either learned a point-estimate, notably maximum entropy IRL, or used Bayesian methods to learn a probability distribution over reward functions. The paper proposes a novel method for inverse reinforcement learning: inferring a (distribution over) reward functions from a set of expert demonstrations. Originality Exploiting the distribution of reward is considered in Bayesian IRL.
---- Summary This paper proposes FLAG (Free Large-scale Adversarial Augmentation on Graphs), an adversarial data augmentation technique that can be applied to different GNN models in order to improve their generalization. It is true that adversarial feature augmentation has not been studied for graph neural nets, but it is a straightforward idea to apply an existing feature augmentation method on graph nodes, which are represented using feature vectors just like other types of data. This paper presented an adversarial augmentation technique for graph neural networks. ------ Post-Rebuttal Update ------ I have read the author's response, thank you for adding more experiments with other graph data augmentation techniques. Hence, my takeaway from the paper (which is in fact a valuable takeaway) is that adversarial augmentation is not considerably effective for graph neural nets, no matter what dataset and network architecture is used. The focus of the paper is extensive experimentation in various tasks and settings to illustrate the effectiveness of adversarial augmentation in graph-based tasks. As I said in my concerns, if this kind of feature data augmentation is orthogonal to structure-based data augmentation that adds or removes edges (or nodes), it would be very interesting to do an experiment showing to which degree it can be used together with other graph augmentation techniques. Adversarial Training for Free, Neurips 2019 This work applies FreeLB adversarial training [1] to graph neural networks. The authors adopt an existing augmentation algorithm and apply on the nodes of each training graph, and use the perturbed graphs for training. ---- Cons My main concern with this paper is that the proposed augmentation technique is not compared against any other graph augmentation techniques.
This paper presents an interesting approach for training neural networks with a small dataset. Pros: This paper presents a good idea for training with small dataset.
Summary This paper presents a new type of brain-inspired dual-pathway DNN model where the coarse (faster, less accurate) and fine (slower, more accurate) visual pathways augment each other during training and inference (via imitation and feedback) to boost the network's robustness to various noises. During training, cross-entropy loss are used for both pathways, and an "imitation" loss is used to encourage the CoarseNet pathway to mimic the FineNet The two pathways are named as FineNet and CourseNet. During inference, the FineNet received recurrent feedback signals from the CoarseNet via an attention layer and memory. In addition, using only FGSM (targeting the FineNet) to generate adversarial noises doesn't fully test the robustness of the model, since more recent techniques [1, 2] can easily generate smooth adversarial examples that will likely severely affect the CoarseNet (unlike FGSM). This paper proposes a dual-path CNN architecture with complementary roles (FineNet and CoarseNet) which is inspired by parvocellular and magnocellular pathways in the primate brain. (Imitation learning) It's unclear why CoarseNet activations must mimic FineNet activations, since only FineNet activations are ultimately used for inference (not CoarseNet activations, which are further transformed and used by the FineNet). [8] Brain-Like Object Recognition with High-Performing Shallow Recurrent ANNs, NeurIPS, 2019 This paper proposed a two-pathway neural network to mimic the interplay between the parvocellular (slow and fine-grained) and magnocellular (fast and course) pathways in neural systems. Additional Feedback (1) Although using an L2 loss for imitation learning is straightforward mathematically, the authors' arguments regarding how the brain may implement imitation learning aren't very convincing (Sec 3.2).
The paper is missing an explanation why these different aggregation functions are supposed to specifically support deeper GNNs. Since the OGB benchmark is new and the reported GNN models on the OGB leaderboard were only run for 3 layers, it is crucial to analyze all the variants discussed here in detail to appreciate the performance gain achieved by the proposed generalized aggregation function. This paper studies how to train deeper graph convolutional networks by using different aggregation function. —Summary: The authors propose a generalized neighborhood message aggregation function for GNNs. The proposed choice of generalized aggregation functions is SoftMax and PowerMean, which generalizes Max and Mean functions and interpolates them. The authors propose new and, in particular, parameterized aggregation functions for GNNs in order to especially support the construction of deeper GNNs. The paper is fairly understandable and the "deeper GNN" topic has gained more attention recently. It would be great if authors can state the novelty of this paper compared to DeepGCNs. Also, what makes difference between the proposed aggregation function and softmax. This paper proposes a general aggregation function which summarizes sum, max, and softmax operations etc. This generalized aggregation function can cover commonly used aggregation functions (i.e., mean, max, and sum) by particular setting of hyperparameters.
(6) For unconditional video synthesis, the musical instrument playing videos mostly contain small motions. ** Strengths (1) Improve the classic video texture synthesis method Video Textures by replacing pixel similarity with a distance metric learning to measure the transition probabilities First, although there may not exist any resampling method that can directly perform the video texture synthesis, I believe many related graph-based methods could be used to model the transition probabilities of frames. To guarantee the smoothness of the transition between different segments, an existing interpolation method is used to connect these video segments in a sequential order. ** Weaknesses (1) It seems a strong limitation that the proposed approach is not able to generalize to different videos or has to be video-specific (i.e., train a model on each input video). The proposed method is inspired by Video Textures (Sch¨odl et al., 2000), which synthesizes new videos by stitching together snippets of an existing video. In the introduction part, the authors present the basis of their work (Video Textures) and present a comprehensive comparison with previous works. The audio conditioned video synthesis part also seems like an extra module which does not influence the completeness of the whole model if not included. Specifically, during training a model is used to learn the transition probability between different video segments. (2) Extend the proposed approach to audio conditioned video synthesis (ii) Extending the model to a conditional situation and performing the task of audio conditioned video synthesis. Cons: originality: This work is more like a simple extension of the previous work (Video Textures) with limited novelty. The general idea is starting from a prior work (Video Textures) and extending this work with a learning framework. Third, the video content is directly sampled from seen sequence, where the diversity is constrained to the given video. Specifically, giving a video clip, the authors first extract overlapping segments from it and fit a bi-gram model. Moreover, the video interpolation is directly borrowed from previous work without further improvements, where I think is still challenging and worth to explore. The motivation behind this work is clearly presented, i.e., to synthesize long-range video sequences. Summary of this paper: In this work, the authors propose a method to learn to generate long-range video sequences.
The method can be used as a black-box watermark (does not require model parameters to verify),  however, the certification bounds only apply to a white-box use case in which the verification can perform inference and test accuracy for a set of trigger images for multiple smoothed versions of the parameters. 2020. The proposed method exploits the randomized smoothing techniques for a certified watermark of neural networks. Can we say that models without watermark cannot attain this trigger set accuracy? If my understanding is correct, Col. 1 simply certifies that the lower bound on the trigger set accuracy. Different from the defense against adversarial example, in the case of watermark detection, not only the detection accuracy but false detection of non-watermarked models should be considered. Comments: This paper present the first certifiable neural network watermark method. Con 4 is par for the course with any watermarking scheme, although the reduction 89.3->86% accuracy for CIFAR-10 is concerning, as that much accuracy loss is a significant deterrent to use of the method and the trend from MNIST CIFAR-10 makes me wonder if larger and more realistic images may show even greater reduction in accuracy.
########################################################################## Summary: The paper presents NAHAS, which is a combination of Neural Architecture Search (NAS) and Hardware Architecture Search (HAS) The main difference between this paper and previous hardware-aware NAS papers is that this paper has an additional hardware search space beside the neural network architecture search space. for software-hardware co-design. It uses PPO with joint search space: model accuracy and hardware constraints. A highly parameterized (commercial) edge accelerator defines the hardware search space. ########################################################################## Pros: The paper introduces a new dimension, hardware design, to the neural architecture search domain. When Neural Architecture Search Meets Hardware Implementation: from Hardware Awareness to Co-Design https://ieeexplore.ieee.org/abstract/document/8839421 Summary: The paper proposes an algorithm to search for hardware designs and neural architectures jointly.
As such, the concept of including importance of each class into an evaluation metric has been implicitly considered. The paper presents a simple addition to the Balanced Accuracy approach - which the authors refer to as 'importance'. the weighting schemes to combine binary metric scores to evaulate the performance of multi-class classification have been well-studied, such as macro-averaging, micro-averaging, as well as importance weighting (manual or data-driven e.g. frequency). The paper evaluates the new metric - but only agains the Balanced Accuracy metric (which seems quite restrictive). However, there is nothing in the formulation of this concept which requires that this is an importance and could in fact be any form of weighting. I do not agree with this statement because it is no surprise that different evaluation metrics give different evaluation results, and this alone is not the ground for the validity of the proposed metric. As such one can use the metric to give a value which can be used to compare different approaches.
However, the theoretical work in MINE is also very weak and only focuses on estimation consistency and not convergence rates (i.e. the statistical bias and variance of the estimator). In the end, the authors really only propose a small modification to the MINE estimator to counter the supposed drifting problem and do some experiments showing some improvement. Empirically, the proposed regularized term works well along with the original MINE estimator and ReMINE  has better performance in the continuous domain 2015. The work studies a neural-network based estimator, referred to as MINE, for approximating the mutual information between two variables. iv) The paper states that MINE must have a batch size proportional to the exponential of true MI to control the variance. Summary The paper introduces a generalized version of the mutual information neural estimation (MINE), termed regularized MINE (ReMINE). However, the variance of MINE is not proportional to the exponential of true MI in finite sample case (in asymptotic maybe), due to the log function. Some interesting experimental results are firstly provided on a synthetic dataset: the constant term in the statistical network is drifting after MI estimate converges.
A network consisting of a prediction network for the mixture of augmented and target images & another network based on CycleGAN for the image translation network are jointly optimized using end to end training. Summary: This paper discusses an approach to augment a medical imaging dataset using images from another modality. The proposed approach is to set up a CycleGAN to translate between the modalities and a predictor from the required modality to the prediction target.
If the claim isn't that Algorithm 1 is DP, then the privacy guarantee is restricted to the results in Figure 3, that attack algorithms perform similarly well on DPSGD, FairDP, and significantly better on SGD (non-private). If FairDP does not satisfy the DP definition or has a very large privacy loss compared to DPSGD, it is not fair to compare FairDP with DPSGD. Do different groups get different levels of privacy protection, and if so what does that mean, and is that considered a fairness violation as well? In order to make sure the FairDP is (ϵ,δ)-differentially private, they have to theoretically find the privacy parameters with respect to the training datasets. In particular, the constraint set of the problem seems to be all models with optimal risk (absent any fairness, privacy).
In such a setting, the training of GNNs for node classification problems substantially differs from the training of other neural networks, because if the graph and node data are partitioned and distributed across machines, the data held by each machine may not be enough to compute a local gradient. This paper studies how to do distributed training for GNNs. For GNNs, nodes are connected so that it is not trivial to do distributed training, because it needs message passing across machines, incurring communication costs. Thus, it is critical to show the percent of immediate neighbors in the overlap Graph Convolutional Networks (GCNs) have inspired state-of-the-art methods for learning representations on graphs. The paper presents a subgraph approximation method to reduce communication in distributed GCN training.
6430-6439. 2019. Summary In the context of gradient-based meta-learning for few-shot learning, the authors propose TreeMAML, an algorithm that leverages the existence of a tree structure in a task distribution in order to pool inner-loop gradients between tasks. The authors propose to adapt the model-agnostic meta-learning algorithm (MAML) of [1] to reflect this hierarchical structure by either observing (Section 4.1, FixedTree MAML) or inferring (Section 4.2, LearnedTree MAML) an assignment of tasks to clusters at each step of the inner loop (task-specific adaptation phase) of MAML; ########################################################################## Summary: The paper studies a simple modification of MAML to address the problem of meta-learning hierarchical task distributions. Novelty: The algorithm modifies and combines previously introduced components: the MAML algorithm of [1]; the online top-down clustering algorithm of [2], and the task-similarity-as-gradient-similarity approach of [3]. The paper compared the results of TreeMAML with MAML and the Baseline on SinusRegression Task and Linear Regression Task.
The main contribution of this work is to use Early Bird Lottery Tickets to reduce pre-training and fine tuning time for BERT. Experiments for both pre-training (the first of its kind) and fine-tuning show that performance does not drop all that much for GLUE and Squad which are the main set of tasks BERT is typically evaluated on. This would involve computing the pre-training time (time to learn BERT parameters on Wikipedia) + total fine-tuning time across all datasets (QQP/CoLA/MNLI etc) considered. Summary: The authors propose a technique for reducing the computational requirements of training BERT early in training to reduce the overall amount of resources required. More specifically, they adapt EarlyBird lottery tickets to the BERT setting in order to find winning configurations in early stages of training combine it with structured pruning methods to ensure the resulting network is more efficient to train. (More of a question/nit) Is it possible to also show what happens when a winning ticket for BERT fine-tuning is selected based on the pre-training objective? The goal of the paper is to find structured winning tickets for BERT in the early stages of training/fine-tuning. What sets this work apart from prior work on model compression is that while prior works attempt to compress a pre-trained BERT, the authors in this work attempt to learn a sparsified BERT for the purpose of speeding up pre-training. This results in a sparser BERT model, leading to faster pre-training. The authors pitch it as a technique for reducing the training time of BERT and use LayerDrop as a baseline technique that also removes network components. The method essentially involves learning an independent bernoulli mask for all BERT heads along with Bernoulli masks for some later intermediate neurons.
This paper presents a deeply supervised few-shot learning model via ensemble achieving state-of-the-art performance on mini-ImageNet and tiredImageNet. The authors first studied the classification accuracy on mini-Image across convolutional layers and found the network could perform well even in the middle layer. However, when digging deeper, the reason for the ensemble is that we want to find a way to calculate the features through different classifiers, maybe this is because a single classifier is not able to learn all the features from the images at once. [1] - Dvornik et.al. "Diversity with cooperation: Ensemble methods for few-shot classification" And the ensemble achieves the new state-of-the-art results in a few-shot setting, comparing to previous regular and ensemble approaches. The final result is the ensemble of all the select layer predictions, called the Multiple Representation Emsemble.
It is empirically shown that, for a specific type of initialization, for less over-parameterized neural networks, the gradient dynamics follows two phases: a phase that follows the random features model where all the neurons are "quenched", and another phase in which there are a few "activated" neurons. In a controlled setting, a list of experiments investigates the dynamics and compares them for different regimes where the relation of number of hidden neurons to number of training samples is changed and for several specific target functions. In the derivation of the effective dynamics, it is unclear why the second layer's weights of the quenched neurons move faster than the first layer's weights. The stated contribution is therefore that this scaling changes the observed dynamics significantly, which casts doubt when the observations generalize to other settings. The paper suggests that due to quenching of most neurons, the path norm remains small since it is essentially a sum over only activated neurons.
[2] CausalWorld: A Robotic Manipulation Benchmark for Causal Structure and Transfer Learning, https://arxiv.org/abs/2010.04296 This paper is a review of model-based approaches of integrating causal inference to reinforcement learning (RL) in different environments (application areas). The authors introduce a benchmark / software for evaluating causal induction in RL models, where the user can specify a causal model and its influence on the environment can be examined. The construction of the benchmark allows building causal graphs with varying complexity, such as the size of the graph, the sparsity of the graph, and the length of cause-effect chains. If a simple greedy algorithm can solve the tasks, does it mean that the benchmark may be a bit too simple, where a good understanding of the underlying causal structure may not be necessary? The authors state: "The main goal of our paper is NOT to introduce novel models, but rather to introduce a NOVEL benchmark and insights/ingredients to study causal induction in model-based RL" and "It is true that the models we use do not learn an explicit structure for causal learning" which corresponds to my original reservations to the novelty of this paper. ########################################################################## cons: Would you explain more about the physics environment about why the underlying graph is a causal graph, not just a statistical graphical model?
The detection scheme is based on the observations that typical adversarial attacks generate adversarial examples on the decision boundary, so if we use a "counter attack" Zero-day attacks are challenging but more practical than other adversarial samples detection problems. The authors show experiment results across different attacks and adversarial distances; AttackDist is consistently better than baselines.
Authors should formalise the assumption in the work including a 'feasible augmentation' of the samples (not the labels) as it is highly dependent on the augmented label; strong and weak augmentations how these concepts hold in the paper? One main disadvantage of the proposed method is that it seems to be sensitive to the distance function chosen for the label assignment and this function has to be adjusted for different data augmentation schema. The label smoothing rule, i.e., Equation 1, is similar to previous work 1, which smoothes labels for adversarial examples. Summary: This paper proposes to use label smoothing to determine the labels of augmented samples.
Regarding the performance of similar methods evaluated in [Lee et al. (a)] and [Dhamija et al.], I think the main reason why they didn't get the same observation is on the size of OOD dataset, i.e., they didn't train their model with a large OOD dataset like [Hendrycks et al.] or this work. Reasons for score: The proposed setting with a large OOD dataset has already been proposed by [Hendrycks et al.], and the proposed method has been experimented in [Lee et al. (a)] and [Dhamija et al.], so the technical novelty of this work is limited. Training with a large OOD dataset like [Hendrycks et al.] is not common, and the observation in this paper is limited to this setting. Since the large OOD setting has already been proposed by [Hendrycks et al. ], the only contribution of this work is on the empirical observation that the proposed method is better than baselines. Again, [Hendrycks et al.] considered a similar setting, but they proved the effectiveness of their method in image and natural language domains, with 7 in-distribution datasets and 3 large OOD datasets. Conceptually, the approach in Hendrycks et al also seems more brittle and there are distinctions between these two methods (e.g. see Vernekar et al 2019, "Analysis of Confident-Classifiers for Out-of-distribution Detection"). Note that unlike most works on OOD detection, they use more data, \\tilde{D}_out, similar to outlier exposure. In NeurIPS, 2018. [Hendrycks et al.] Deep anomaly detection with outlier exposure. ######################################################################### -- Summary: This paper presents experiments and results from using a reject-class in multi-class classification for the auxiliary task of out-of-distribution (ood) sample detection. Hendrycks et al propose encouraging high entropy predictions on the outlier exposure set, instead of classifying them into a reject class.
This is done by modelling interventions in continuous time with differential equations augmented by auxiliary confounding variables to reduce bias. ########################################################################## Summary: In this manuscript, the authors propose a novel way of performing counterfactual inference in time-series in the presence of hidden confounders.
It shows that the variance of stochastic gradient is a decreasing function of minibatch size for linear regression and deep linear network. The paper shows that the variance of the gradient has an inverse dependence on the batch size in linear networks, subject to the knowledge of the initial weights. The main novelty of the paper is the computation of an exact dependence between batch size and variance of the gradient in the linear regression setting. For linear models, the variance of the gradients conditionned on the initial point is decreasing with the batch size b. Can the authors show why the training loss decreases with increasing variance in gradient, at least in the linear regression case?
Minor comments: Abstract: we adopt a Graph Transformer jointly [using] label embedding? The UniMP first employs graph Transformer networks to jointly propagate both feature and label information. The authors proposed a unified message passing model to make a graph neural network to be able to incorporate both label propagation and feature propagation. Proposed Graph Transformer unifies feature and label propagation in conjunction to provide a better performance in semi-supervised node property classification task. UniMP also benefits from a masked label prediction strategy which is inspired by Devlin et al. Proposed method is able to achieve SOTA performance on several datasets from Open Graph Benchmark. Compared with other methods such as APPNP,  TPN and GCN-LPA, the major differences is another effective/unified network structure to merge feature and label information together. Pros: This paper proposed a novel framework to more effectively and explicitly utilize the label information by GNN in semi-supervised scenario, and the experiments illustrate its effectiveness in general benchmark datasets. Section 4.4: model still be uncertain → model still remains uncertain The paper presents a novel unified model that jointly harnesses the power of graph convolutional networks and label propagation algorithms based on the unified message passing framework. Pros: The method proposed for unifying feature and label propagation is simple yet sufficiently novel. In the model, the label information is projected into the feature space, then merged/added with the node feature for the GNN feature aggregation module.
The comparison between CNV-Net and other methods is not fair as the other methods are finding CNVs in the whole genome while CNV-Net is given a pre-defined set of "candidate breakpoint regions" in which positive samples have breakpoints perfectly centered in the middle. Currently, the CNV-Net is evaluated with mapped and pre-preprocessed reads with CNV centered breakpoints.
The idea is to form local graph for each node using PPR-Nibble, a local clustering method proposed before, and then use transformer on top of the local graph as encoder for node classification and link prediction. Based on this, they propose a light and scalable GNN learning framework called LCGNN, which first adopts the local clustering method PPR-Nibble to partition full graph into subgraphs, then use GNN modules on subgraphs for training and inference. Although the idea of using local clustering is very interesting, it is straightforward to apply an existing local clustering algorithm into GNN especially considering the existing methods that utilize the global graph partition. ########################################################################## Summary: This paper proposes to utilize local clustering to efficiently search for small but compact subgraphs for Graph Neural Networks (GNN) training and inference. From Table 2, it seems that the improvement of incorporating local clustering (comparing LCGNN-GCN/LCGNN-SAGE with naïve GCN/SAGE) is not very large, and we could not find how much the local clustering improves Transformer. Minor comments: As the authors mentioned, the local clustering methodology is only reasonable for graphs that has low conductance with respect to all "cluster" (nodes with same label). [Summary] In this paper, the authors study the connection between GNNs and local clustering, and find that short random-walks in GNNs have a high probability to be stuck at a local cluster. Using local clustering method to determine the subgraph for each node might be better than random neighborhood sampling in some cases.
A major difference in this paper is that the additional regularizer enforcing the encoder compatibility and the quotient of the plausible equivalence relation. The authors propose a method to train deep generative models on quotient manifolds and show improved performance on some simple standard test sets. Summary: The author extends generative models with multi-generators by restricting the generators to share weights and all bias to be regularized in order to enforce that the inverse maps of the generators can be represented by a single encoder.
The paper nicely outlines how PPO and SAC should be configured to work with independent and autoregressive policies under the atomic factorization of the action space. [II] Learning to Factor Policies and Action-Value Functions: Factored Action Space Representations for Deep Reinforcement Learning, arXiv 2017. (iii) IF-SAC vs IF-PPO. Overall, the experiments show some of the possible potential of factored action spaces with PPO and SAC. They consider atomic factorization of the action space (i.e. action space is factored into sub-action spaces, one per action dimension). Questions What is the impact of the action ordering for the autoregressive factorization? In fact, the action space in Gym Platform features discrete and continuous sub-action spaces, but they are discretized. Also, the paper does not set a clear agenda for what would be interesting to see in the results; Is scalability to large action spaces being investigated (in which case, comparison with the non-factored baselines of PPO and SAC should be included)? The experiments show some of the potential of factored action spaces, both IF and AF. As such, I think this paper should state dealing with discrete action spaces or continuous ones via discretization.
Using robust estimators (which is deterministic), which gives privacy guarantees with high probability, is exactly the start point of these algorithms. It seems to me that the main reason for Med-DC being both deterministic and private in this new notion is the weakness of the new privacy notion but not the well-design of Med-DC. Summary: The paper considers the sign recovery problem in a distributed setting with privacy constraints. The paper shows that in the sparse mean estimation setting, Med-DC is correct with high probability under some assumptions and Med-DC satisfies a weaker notion of differential privacy proposed by the paper. Apart from group privacy, no other important properties of the "definition" have been proved.
Over the past few years, a number of papers have developed the theory of Neural Tangent Kernels, which can be used to interpret infinite width deep neural networks in the context of a particular type of kernel. Secondly, they provide an interpretation of the neural path kernel as a composite kernel composed of layer-wise kernels, giving rise to the title of the paper. Lakshminarayanan and Singh showed that under certain assumptions, a kernel defined in terms of the neural path feature is approximately equal to the neural tangent kernel (up to a constant). Paper proposes and extension of neural path framework to include composite kernels which comprise of a) FC networks (Hadamard product of gram matrices), b) residual networks (sum of products of base kernels), and c) CNN max-pooling layer. Theorem 5.1 (in both papers) relates the neural path kernel to the neural tangent kernel by showing that the neural tangent kernel for a network in which the gates have been fixed tends to a constant multiple of the neural path kernel as the width of the nerwork goes to infinity. Firstly, the analysis is extended to certain ResNet and Convolutional architectures, showing that in both of these cases we can relate the neural tangent kernel matrix to the neural path kernel matrix using a result analogous to Theorem 5.1 in (Lakshminarayanan and Singh, NeurIPS 2020). A recent paper (Lakshminarayanan and Singh, NeurIPS 2020) provided a new perspective on Neural Tangent Kernels for Gated Neural Networks, by decomposing the network into independent paths.
Table 1 shows that in terms of mutual information, MixUp < Baseline < CutMix (and < FMix with a very small gap). The paper presents an interesting analysis of CutMix and MixUp data augmentation techniques. This paper argues that "Mixup uses 1024 batch size and CutMix uses 300 epochs".
Pros: MLP fed with node features and spectral coordinates of a given node is competitive with a GCN and more efficient than GCN.
Using a technique best on Taylor polynomial approximations, the paper finds that a large class of smooth functions can be approximated using O(log(1/\\epsilon)^(n/d)) quantum gates, qubits,  and classical width. Cons: The results in their current form do not really show any benefit (from the point of view of approximation) for quantum neural networks, since the bounds are only logarithmicallly better than classical bounds.
Reasons for score: The main idea of using self-attention for decoding linear error correction codes is interesting. Some structured codes have a permutation group (PG) that maps codewords via permutations. It will make the authors claim stronger if they explain why these represent linear codes cases (unless the permutation decoding only applicable to the BCH codes). Hence, it renders an interesting alternative to improve further the decoding of powerful and widely used coding schemes such as Polar codes, that also accept permutation decoding. Summary: This paper suggests a new decoding algorithm of linear error corrections codes based on self-attention. Summary The use of permutation decoding improves the performance of some types of structured channel codes. Extra feedback I just want to conclude with a suggestion for a correction and a typo: In Section 2, second paragraph, the sentence ''Codes with good decoding performance are represented by graphs with cycles'' is a bit confusing, as it could be understood that indeed graphs are helping with the performance. For me, it is not clear why the top k (k>1) performance matters to measure the quality of the decoding algorithm of error correction codes for communication. Pros: The paper utilizes the self-attention mechanism to improves the computational complexity of permutation decoding. This allows increasing the performance in terms of error rate, and preserve the efficiency of the decoding process, up to the overhead introduced by the permutation selection procedure. Still, the question remains on which permutations (out of the vast set of possible ones that most of these codes allow) select to decode, in a way that we increase the chances of a successful recovery of the transmitted codeword. In order to select the most promising permutation without running the soft decoding algorithms, two main ingredients are used: (1) node embeddings of the Tanner graph of the code via the node2vec method, The methodology is based on a permutation classification procedure (trained beforehand), where each permutation is encoded into an embedding vector obtained using the concept of self-attention, to account for the geometric similarity between transformations. Minor comments: It seems that WBP is not defined (is it weighted BP?) The paper focuses on improving the computational complexity of permutation decoding. The pipeline presented allows improving previous permutation decoding methods by several dBs at almost no extra cost. Finally, I am curious about the Block error rate, as I can imagine that sometimes, when a permutation is quite wrongly chosen, it might push y to a different c, hence leading to s=0, but still resulting in a wrong decoding. In practice, one performs decoding with multiple permutations to identify the ones which lead to successful decoding.
Paper proposes Hybrid Discriminative Generative training of Energy based models (HDGE) which combines supervised and generative modeling by using a contrastive approximation of the energy based loss Gets rid of computationally expensive SGLD by using contrastive approximation which was a key limitation of prior energy based modeling work like JEM Precisely, the approximation is for modeling p(x|y), yet the contrastive learning is modeling p(x_1|x_2) with  x_1 and x_2 being the outcomes from correlated data. Experiments in appendix show that HDGE can be used in combination with JEM for generative modeling, but this again requires running SGLD. Error bars are missing for the classification accuracy experiments in Table 1 which makes it hard to verify improvements especially wrt supervised contrastive loss method
Summary In this paper, the authors propose a novel approach for quantifying the statistical significance of binary masks predicted by a subclass of deep neural network (DNN) models for image segmentation problems. This paper proposed a novel method which to quantify the reliability of DNN-driven hypotheses in a statistical hypothesis testing framework. Since the linear contrast is a function of the input, in this case, a nonlinear function implemented by the pre-trained DNN model, classical statistical inference (such as e.g. a two-sided t-test) does not apply. Other, related aspects are (i) which type of supervision the models require for pre-training and (ii) which type of output layers are required in order for the null hypothesis to be relevant.
How were hyper-parameters chosen for the DDQN agent, and can the authors comment on whether such choices can have reasonably strong interplay with the transferability of perturbations? The paper explores the impact of various types of perturbations between states in the same environment (state transferability), and between states in different environments (environment transferability). Paper Summary: This paper aims at discovering transferability of perturbations across different environments in RL. Summary of Contributions The paper explores adversarial perturbations in deep RL, providing a new thread model where the perturbation is computed based on a single state. — idea: A framework composed of 6 different adversaries is proposed using which it is claimed that the transferability properties among Atari environments can be studied. Review Summary: The idea of analyzing perturbations' transferability is interesting, but it is hard to say that the approaches are satisfactory. While one algorithm may perform a bit better, if the emphasis of the paper is to measure the extent of adversarial perturbations, it seems like it may paint a clearer/more convincing picture if a simpler algorithm is used, with fewer moving parts to attribute performance differences to.
On the basis of the ordinary-label learning, the authors defined "robust loss functions" for complementary-label learning:  a a loss function is called robust  loss function if minimizer of risk with complementary labels would be the same as with ordinary  labels. The paper presents (two) simple yet insightful sufficient conditions for a usual loss to work well as a complementary label loss. However, in this paper it means if the loss function with ordinary and complementary labels has the same minimizer. I am unaware of this definition of robustness of a loss function as it seems very specific to the complementary label learning problem. It would then be necessary to connect the current work to this trend, for instance to Cour et al. "Learning from partial labels" (JMLR 2011) or the more recent works of, e.g., X Wu, ML Zhang "Towards Enabling Binary Decomposition for Partial Label Learning.". Summary: This paper deals with the problem of complementary label learning, that is, when we know the set of labels which a given observation does not belong to.
In the RL context, this paper aims at designing a generic solution for reducing the number of policy switches during training (called switching cost) while maintaining the performance. Performance : It seems to me that FIX_10^3 has always lower switching cost, and also learns a good policy. Experiments on a medical treatment environment and Atari show that the approach obtains similar performance to on-policy RL with fewer changes of the simulation policy. Hope it was helpful, Thanks. This paper studies RL with low switching cost under the deep RL setting.
Theorem 6.1 needs to assume that PSI holds and spurious patterns are unbiased; under these assumptions, the result is pretty straightforward. As a way out, the authors introduce the concept of Patterns Statistics Inductive Bias (PSI). We formally define this as the "Pattern Statistics Inductive Bias" condition (PSI) and provide empirical evidence that PSI holds across a large number of instances. They define a statistical phenomenon that holds in SGD in the proposed setting and call it Pattern Statistics Inductive Bias (PSI). They proved that if a learning algorithm satisfies PSI, the sample complexity is nearly quadratic in the filter dimension; while the VC dimension of the network is at least exponential in the filter dimension.
This paper proposed AdaMa, which can automatically use adaptive learning rates for each agent in cooperative Multi-Agent Reinforcement Learning (MARL). AdaMa calculated the learning rate of each actor and critic according to their contributions of locally increasing value functions. It proposes AdaMa, an algorithm which balances the learning rates of actors and the critic, and can also make use of the second-order information. The paper studies adaptive learning rate for Actor-Critic style MARL algorithm. Simple experiments using toy examples show that the proposed AdaMa method can improve fixed learning rate method and other heuristics.
These practical advantages however were not demonstrated against existing probabilistic non-parametric federated learning works such as PVI & PNFM -- this is perhaps strange given that DVSGD builds on PVI and is mostly different only in terms of posterior representation. A quick question: the proposed algorithm and PVI only selects ONE agent per communication round. However, unlike PVI, the proposed method aims to replace the parametric representation of posterior with a non-parametric particle representation developed by the prior SVGD work of (Liu & Wang, 2016). This paper proposes a federated version of the Stein Variational Gradient Descent (SVGD) method. PAPER SUMMARY This paper introduces a new approach to probabilistic federated learning, which builds on the previous PVI work of (Bui, 2018).
In an actual adversarial setting, even if the opponent's policy was static (i.e., not updated in response to LIOM's policy, as in the cited Ganzfried papers), the opponent could still exploit any mistakes LIOM might make through its static policy, possibly driving its performance below that of NOM. In the conclusion, you state: "LIOM is agnostic to the type of interactions in the environment (cooperative, competitive, mixed) and can model an arbitrary number of opponents simultaneously." The method presented appears to only support a single opponent per environment instance, so it would be good to clarify this statement. Some suggestions for further improvement: I found Figure 1 confusing as I understand that the actual opponent policies do not have access to the latent variable Z, and that three items are predicted from the latent space: the opponent's actions, observations and rewards. A variational autoencoder (VAE) is trained to predict opponent observations and actions, given the agent's local information. For example, in addition to the two Ganzfried and Sandholm papers cited in this work, consider: Computing Robust Counter-Strategies, NeurIPS 2007, Johanson et al. Uses opponent observations at training time to compute robust counter-strategies, uses agent observations at execution time to choose which counter-strategy to play against the current opponent (who may not be one of the opponents used at training time). Speaker-Listener and Double Speaker-Listener. This is pretty minor, but I'm a bit concerned about a specific detail of these environments (especially S-L) and A2C that might give LIOM an advantage over NOM by leaking information, without having to model the opponent well at all (although from Fig3, Fig5, and Tab1, it appears to indeed model the opponent well). This paper considers an algorithm for learning and using an opponent model that is only conditioned on an agent's local information (history of actions, observations, and rewards). So here's the catch. If the VAE encodes something about the last timestep reward, and the A2C component does not (i.e., NOM doesn't see it), then LIOM might have access to a valuable feature that NOM does not, unrelated to opponent modelling altogether. During training, a VAE is trained to encode the agent's observed trajectory to a latent space, and then decode it back to the full trajectory (including opponent observations and actions). In other opponent modelling work, I've seen opponent-aware agents do far worse than a baseline agent in those settings. In the paper's experiments against artificial opponents, opponent observations were used at training time but not execution time, and some experiments involved a holdout opponent for which no opponent observations were provided at any time.
In the current manuscript, it is clearer that the "adapter network" predicting the "final linear layer" of the network is a unique component in this paper. The proposed method trains the policy using soft actor-critic on training tasks and trains an adapter network to predict task-specific final linear layer from a single timestep transition (s, a, r, s'). This paper proposed a meta RL algorithm built upon an assumption that a shared policy with a task-specific final linear layer can maximize expected return for each task. I found that using an adapter network trained to predict linear layer parameters during training and used to infer parameters during testing is a setting that was not explored. Does the adapter network change the linear layer parameter in every time step during meta-testing? The main assumption about the existence of an optimal policy with a task-specific linear layer is not justified well. In such cases, the adapter can be confused and the performance of FLAP may deteriorate.
At the end of the introduction, the author say: "It is our belief that the continued use of single perturbation thresholds in the adversarial robustness literature is due to a lack of awareness of the shortcomings of these measures". Summary: The authors advocate for the use of Robustness curves, plotting the adversarial accuracy as a function of the size of the neighbourhood region of allowed perturbation. I think one reason for choosing these values is that studying robustness under perturbation with larger distortion is kind of unnecessary because then the noise added is no longer imperceptible, which is at odds with adversarial examples' definition. If adopting robustness curves (or global robustness) as the evaluation criteria instead of point-wise robustness, how will this affect the existing adversarial training procedure? One thing that I would recommend the authors is to make clearer the distinction between robustness curves as they described them (based on finding the closest adversarial example) vs. However, in this adversarial robustness community, the status quo is that researchers compare with each other on some specific datasets with some specific epsilon, for example, 0.3 for MNIST, 8/255 for CIFAR. The author argues that robustness for a specific epsilon may not be enough and suggests robustness curves as an alternative. While I agree that you are going to get more information if you compute a full robustness curve than if you sample it at a bunch of points, I'm not convinced that it is worth the effort. The global robustness considered in this paper is robustness for varying perturbation strength.
I like the idea of  using topic models as a way to represent the document level information, but it is disappointed to see that the proposed method doesn't provide as good performance as the simply averaging word embeddings. It proves that the proposed procedure can recover a representation of documents that reveals their underlying topic posterior information in case of linear models. The authors compare the suggested representation with models that do not explicitly make document representation (maybe except LDA). This submission considers contrastive learning approach to representation learning under topic modeling assumptions. ψ(x)k=P(x(1)=x|w=ek)/P(x(2)=x′)? Reference Yurochkin, Mikhail and Claici, Sebastian and Chien, Edward and Mirzazadeh, Farzaneh and Solomon, Justin M, Hierarchical Optimal Transport for Document Representation, NeurIPS 2019, http://papers.nips.cc/paper/8438-hierarchical-optimal-transport-for-document-representation Summary: Contrastive learning is applied in a semi-supervised setting with few training examples to provide features for a linear classifier. Using topic modeling as a tool to understand representation learning is interesting. This paper presents a new contrastive learning algorithm for document representation. The main theoretical result comprises the population case, where infinite amount of data is sampled from the described generative model under an additional anchor word assumption, and proves that in such case the outcome of the proposed contrastive learning procedure is linearly related to all moments of the topic posterior up to a cerain degree. It is experimentally demonstrated that the proposed procedure performs well in a document classification task with very few training examples in a semi-supervised setting.
########################################################################## Cons: (1) The proposed approach is based on the assumption that labeled data are skewed and unlabeled data follow the assumed true distribution. The key assumption is that, though the training outputs can be skewed, it is easy to estimate the true distribution of the output. This paper proposed to learn a regression model using "skewed data", which is defined as the subset of training samples with true target above certain threshold. This method contains an adversarial network that forces the regression output distribution to be similar to the assumed true label distribution. This method contains an adversarial autoencoder that propagates the information from the assumed true distribution to the latent vector of the regression model. How can the latent vectors have same distribution as a distribution of labels. (1)     The paper assumes that the training data are often highly skewed (intentionally) but the true distribution of the output can be easily estimated or obtained. The training data does not represent the true real world distribution. The predictive distribution was forced to match the true target distribution p(y) through an adversarial network. ########################################################################## Pros: (1) In practice, the distribution of reported data could be different from the true distribution.
The latent graph structure is generated through a fully-parameterized adjacency matrix or a KNN construction subsequent to passing node features to an MLP. "Self-supervised Training of Graph Convolutional Networks." arXiv preprint arXiv:2006.02380 (2020) This paper considers the problem of nodes classification with few labeled data and missing graph structures. ii) Graph Neural Networks with Generated Parameters for Relation Extraction, In ACL'19 The authors propose a method for simultaneously learning the graph structure (or a graph generative model) and the parameters of a GNN for node classification. However, in some domains (such as brain signals, particle reconstruction, etc.), there is access to only node features (but not the underlying graph structure). For example, the authors may want to follow previous works to design certain metrics to compare the learned graph structure with the original one, provide some visualizations, or case studies in synthesis graphs. Graph neural networks (GNNs) have become de facto methods for integrating the input graph structure and node features to learn effective node representations.
[1] "Neural Architecture Optimization", https://arxiv.org/abs/1808.07233 The authors address the Neural Architecture Search problem. More specifically, the surrogate model takes a graph structure as the neural architecture embedding and predicts a relative ranking, then applies gradient descent on the input graph structure to optimize the neural architecture. Summary: This work propose Graph Optimized Neural Architecture Learning, that uses a differentiable surrogate model to directly optimize the graph structures.
Summary stochastic subset selection (SSS) is a method to learn to compress a set D by selecting a subset Ds such that the loss of a task performed on Ds is as close as possible to the loss if the task had been performed on the original D. Appendix: there are several missing closing parentheses This work introduces a method to select instances from any set (stochastic subset selection, or SSS). A couple ideas: (i) Running SSS just using the first stage (candidate selection). However, the cost of the pixel selection by SSS is never analyzed.
For example, the notion defined in Cohen et al. (2017) enables fast approximate solution of linear equations, which is a primitive in various algorithmic tasks on directed graphs. Summary The paper studies a certain notion of spectral sparsification of directed graphs. Comments Section 4: I am not sure what is the downstream justification for the notion of spectral approximation studied here (relative condition number) in the context of directed graphs, and it would help if the authors could elaborate on this. Authors propose a novel method to approximate a given directed graph with a´nother one (the sparsifier) which has fewer edges.
To improve multi-class classification problem using DNNs, authors propose to do multi-task learning of solving another auxiliary tasks which tells if data points are just noise/distractors or not. In other words, what if we just train a classifier with n+1 labels, where the extra label's training data are the background or noise data points? This submission proposes a training strategy that leverages background/noise data to learn robust representations. The auxiliary classifier is designed to encourage the early layers to learn more meaningful features, and Section 3 supports this relation. The auxiliary classifier is a binary classifier that discriminates training data versus background/noise data. (2) is implicitly related to a good estimation of parameters, which in turn is related to better feature representations in the early layers (an implicit assumption here is that good features for the auxiliary binary classification are also good features for the multi-class classification, which could be a question by itself). It seems that the most natural and meaningful task is where the auxiliary classifier is trained on all in-domain data, both labeled and unlabeled, while the main classifier is trained on a small number of labeled data.
The proposed method is somewhat novel in the aspect of suggesting TRUST-TECH for an ensemble of DNNs. However, it is difficult to give a high score because the experimental results do not support The authors propose a method to obtain multiple local optimal solutions around the existing one. [+] Propose a method of applying TRUST-TECH to find local optimal solutions (LOS) of DNNs[+] Introduce DSP for exploration in high-dimensional parameter space[+] High ensemble performance through DSP-TT Unfortunately, I am not an expert in this field, but two papers I came across doing a very quick search appear to be somewhat relevant: "Local minima found in the subparameter space can be effective for ensembles of deep convolutional neural networks" and "MEAL: Multi-Model Ensemble via Adversarial Learning". The following is the part that mentioned the quality of the local solution on page 1.
Detailed Comments: The paper uses contrastive learning idea proposed in SimCLR (Chen et al. 2020) to detect out-of-class samples and treat them in a different way than in-class unlabeled samples during semi-supervised learning. ########################################################################## Pros: The paper addresses a very interesting and practical problem in semi-supervised learning, where the unlabeled samples may include out-class samples. The authors proposed to address the task of semi-supervised learning (SSL) by contrastive learning techniques, and the proposed techniques can be applied to handle open-set unlabeled data (i.e., the label spaces between label and unlabeled data are partially disjoint). Out-of-class samples might still exhibit similarity with selected in-class categories, and thus forcing their soft labels to be a uniform distribution might not seem to be practical (if that's the case). This paper considers the problem of semi-supervised learning, where the unlabeled data may include out-of-class samples. It also explores the idea of auxiliary batch normalization (from Xie et al. 2020) in the open-set SSL setting but the results of the ablation study suggest the level of improvement achieved by this normalization is negligible and the most of the improvement comes from more accurate detection of out-of-class samples through using the projection header function introduced in SimCLR paper. To address this task, the paper proposes a method consisting of three steps: (1) detecting out-of-class samples in the unlabeled set, (2) assigning soft-labels to the detected out-of-class samples using class-conditional likelihoods from labeled data, and (3) using auxiliary batch normalization layers  to help mitigate the class distribution mismatch problem. Then the paper filters outlier samples by the similarity measurement and further utilizes outlier samples with soft labels. The use of such soft labels allows the training of such unlabeled and out-of-class samples, which would be the major novelty of this work. In order to handle out-of-class samples in D_u, the authors present the idea of learning class-wise prototypical representation based on the above contrastive features.
Moreover, the introduction makes it sound like the function approximation assumption is merely for the policy class, but in fact the approximator must be able to represent the entire transition kernel, which is potentially much more complicated.
The authors consider the decentralized optimization problem and explain the generalization gap using the consensus distance. The authors identify the consensus distance as the key factor that affects the generalization performance of decentralized training. The main focus is to better understand the role of consensus, or lack there of, into the generalization abilities of decentralized training. So I would say the abstract is a bit overclaiming, the authors better tune down their claims to practical only, without any theoretical guarantees -- "We identify the changing consensus distance between devices as a key parameter to explain the gap between centralized and decentralized training. The conducted experiments are extensive and the delivered message is pretty clear -- Critical consensus distance exists in the initial training phase and ensures good optimization and generalization, while a non-negligible consensus distance at middle phases can improve generalization over centralized training. (Th1 is based on previous work.) The other results, e.g., remark 2, proposition 3, and lemma 4 cannot claim how the consensus distance affects the generalization error. Is it possible that in the centralized setting, ResNet-20 is overfitting, and the error from decentralized SGD has a regularizing effect, leading to better generalization? On the theory side, the main contribution is Remark 2 and proposition 3, but why remark 2 relates to generalization are unclear (it only shows the convergence rate), neither does proposition 3 (it only shows the consensus distance). As also pointed out by reviewers 1 and 3, the gap between the convergence rate/consensus distance and the generalization capability still exists, causing the mismatch between the theory and the simulations. From the numerical results, the authors at least claim two points of linking the critical consensus distance and the performance: i) the critical distance is important to the initial training phase; ii) a non-negligible consensus distance can improve the generalization performance.
The Unsupervised Learning of Transformation Equivariant 2D Representations by Autoencoding Variational Transformations is used for 3D shape descriptor learning, which the authors claimed as "self-supervised" learning. Summary: This paper proposes a self-supervised learning framework for 3D object classification and retrieval based on multi-view representation, where a sub-task of transformation estimation is adopted as a regularizer. (AET) [2] Qi et al., AVT: Unsupervised Learning of Transformation Equivariant Representations by Autoencoding Variational Transformations, in ICCV 2019. (AET) [2] Qi et al., AVT: Unsupervised Learning of Transformation Equivariant Representations by Autoencoding Variational Transformations, in ICCV 2019. The authors propose a self-supervised learning technique for multi-view learning based on a simple intuition that the transforms of the 2D views of a 3D object will be in an equivariant manner as the 3D object transforms. The features that facilitate 3D transformation prediction then generalize to other 3D object recognition tasks such as object classification and retrieval. For this question, I think the authors should prove the Transformation Equivariant Representations directly on a 3D object (point cloud, voxel, 3D mesh) instead of multi-view 2D images. Obviously, the authors applied the "Transformation Equivariant Representations by Autoencoding Variational Transformations" directly to 2D projections of a 3D object and then fused the deep representation by a shared weight NN (shown in Figure 1). Strengths: -The paper is well written, and the idea of transformation equivariant representation of 3D objects is easy to understand.
To test their new model named EMMA (Entity Mapper with Multi-modal Attention), they also present a new toy game framework and crowdsourced data composed of 1320 games (from 3,881 entity descriptions with crowdsourcing). A model called EMMA is proposed to change entity representations based on the manual, thereby making the agent aware of the entities' roles. Unlike recent prior work (Narasimhan et al 2018, Zhong et al 2020) where the entities in the environment are already partly grounded to text (e.g. by representing them as their textual description), the agent here needs to learn the mapping between entities and corresponding text. Their architecture deviates from existing approaches (Zhong et al 20, Narasimhan et al 18) which did not require the model to learn a mapping from an entity to its description either by providing a mapping between objects and their textual descriptions or using entity names plainly. The game is described in Figure 1 and section 5.1: there are three entities to each game (messenger, enemy, goal), and each entity can be stationary, chasing, or fleeing movement. (2) While the challenge of needing to learn which entities map onto which words is greater, each sentence refers to exactly one entity, so the agent just needs to match 3 sentences to 3 entities, which is a marginally greater challenge (if at all).
The authors show that with this metric, they can find architectures with reasonable accuracy on CIFAR-10/CIFAR-100 in the NAS-Bench-201, while using much less search cost compared to previous NAS methods. The effectiveness of the proposed metric is mainly verified in two NAS benchmarks (NAS-Bench-101 and NAS-Bench-201), whose correlation to the actual accuracy is relatively significant (Fig 3). ** Summary The paper mainly introduces a metric to benchmark the performance of neural networks without training – the correlation of Jacobian subject to different augmented versions of a single image. It will also greatly strengthen this work if the authors can show the effectiveness of the proposed metric on a more realistic search space, e.g., the DARTS search, and evaluate the found architecture on a larger dataset, e.g., ImageNet. Justification of rating Summary This paper attempts to infer a network's accuracy at initialization without training it, which can speed up neural architecture search and greatly reduce the search cost.
The proposed approach consists of allowing each client to have a local model for personalization and taking a convex combination of global and local models as personalized preditors. First of all, the manuscript proposed an adaptive personalized federated learning algorithm, where each client will train their local models while contributing to the global model. The authors propose a new framework in which they replace the common global model in the original federated learning formulation with a convex combination of the global model and a local model. In this paper, the authors propose a variant of FedAvg that not only produce the global training, but also a mixture of the local model and the global model, which is called personalized model. The empirical study is not conducted carefully: Allowing for a separate local model for personalization increases the number of parameters which is not accounted for It also does not seem appropriate for studying personalization as we know that there is a single global model with good performance (i.e. trained on the full dataset) and the key challenge is the optimization over heterogeneous datasets rather than the need for personalization.
STRENGTHS The general idea of integrating BlackBox combinatoric shortest path algorithms in a differentiable planning module is interesting and has a lot of potential to be useful. The non-differentiability of the path planning is handled by recent work on differentiating through blackbox combinatorial solvers from [1]. SUMMARY This work proposes a novel neuro-algorithmic policy architecture for solving discrete planning tasks. In the conclusion: What exactly is meant by knowing the topological structure of the latent planning graph a priori? Path Planning using Neural A* Search (2020). Summary: This paper presents a method to train a neural network to predict the time-dependent costs, and start and goal states needed to run time-dependent shortest-path planning in a dynamic 2-D environment. The evaluation considers only shortest-path planning scenarios that are amenable to the proposed architecture (see evaluation section).
Furthermore, to reduce the computational complexity, a node clustering is done on the graph and the attribution is applied first on the edges between C identified clusters and then transferred to all edges. Summary The paper proposes a procedure for identifying a subgraph GK of a given size K (measured by the number of edges) whose output through the GNN function f is as close as possible to that of the full graph G. Summary: The paper introduces a novel method called Causal screening which takes a graph and the prediction made by a GNN, and returns an explanatory subgraph. This work proposes to explain graph neural networks from a causal effect view. To be precise, it starts from an empty set as the explanatory subgraph, and incrementally adds the edges, testing them for the individual causal effect. Basically, the proposed method adds each edge in GNN in a greedy way by evaluating its causal effect on the prediction. I also agree that causal attributions of edges of edges are more reliable than information from gradients, often used to explain models. The proposed method, Causal Screening, iteratively adds edges into the explanatory subgraph. This type of "causal" explanation is considerably more feasible for and better fits graph networks thanks to their sparse and modularized structure. (3), the learned graph is not guaranteed causal, so I would like to suggest the authors not emphasizing "causal".
An reinforcement learning algorithm to learn a permutation invariant policy is derived. The authors identify a key property in the targeted resource allocation problems -- the permutation invariance -- which intrinsically implies the independency of samples at different time steps. Def 1: "A policy network is PI if it satisfies pi(sigma(a), sigma(x)) = sigma(pi(a,x))" for any permutation sigma. The main assumption the paper makes is permutation invariance (PI). This paper proposes an approach to reducing the sample complexity in multi-task reinforcement learning using permutation invariant policies. I feel the paper could have been better presented by starting with a motivating example where the permutation invariance property holds - for example the portfolio optimization example studied in the experiments. From this property the paper extends the work of D'Eramo et al. (2020) and derives a potentially tighter bound for the gap between the optimal policy and the policy learned from multiple tasks. In particular, they present an algorithm that exploits permutation invariance, study its theoretical properties, and propose examples where this property holds and their algorithm can be leveraged. The permutation invariance property is defined in Def. 1. Since the paper talks about resource allocation, does the PI mean that it does not matter which entity the resource is being allocated to as long as the share of resource does not change? The main premise of the paper is that certain families of tasks exhibit approximate forms of symmetry, i.e., applying a permutation to the state/action variables would make all tasks similar in some metric sense. Also it seems that the symbol pi switches semantics a few times - first, it denotes a deterministic policy, then a stochastic policy, and finally a policy network.
In DKP backward matrices are no longer fixed as in DFA but rather are updated after each batch with their own update rule and learning rate. First Review Citation missing for key work on assessing the scalability of bio-inspired approaches and highlighting key limitations [Bartunov 18], variants of DFA [ Moskovitz 18, Frenkel 19]  and recently an approach similar to DFA with target projection known as LRA (also has similarity with Direct Kolen-Pollack) showing promising performance on deep CNNs [ Ororbia & Mali 2020]. I enjoyed reading it. This paper introduces a new method for computing the backward updates of a neural network called Direct Kolen-Pollack learning (DKP). (2) In the experiments, the authors only compare the DKP with the DFA and BP.
Even if one can question whether Def 1 is a good formalization of disentangling, the paper does show empirically that it is easier to learn an equivariant encoder/decoder when the latent operator is a shift operator or a diagonalized complex version of it, rather than a disentangled operator (with one 2x2 rotation matrix block and an identity block; The authors then propose a relaxed definition of disentanglement and show that it can be realized by means of a shift operator in latent space. The definition is: "A representation is said to be disentangled with respect to a particular decomposition of a symmetry group into subgroups, if there is a family of known operators acting on this representation, potentially distributed across the full latent, where each operator is equivariant to the action of a single subgroup." I would further note that what is done in practice in the paper is different from this definition, because we have one latent space per operator, not multiple operators acting on the same space. An alternative definition of disentanglement is given, where instead of confining the effect of each transformation to a subspace, an operator is used that acts on the whole latent space (this operator is chosen as a shift operator, which works for cyclic groups).
The paper suggests a novel auto-encoder based method for manifold learning, by encouraging the decoder to be an isometry and the encoder to locally be a pseudo-inverse of the decoder. Regarding the experiments, indeed the authors successfully show the IAE converges its decoder to be an isometry and the proposed regularizer promotes more favoured manifold. While the authors assert that forcing the decoder to be an isometry is desirable since isometries preserve distances and angles, it is not clear why that is a desirable property while modeling data on a manifold. Is there any possibility that the author can provide one more toy example for the global isometry when the data lie on some manifold shape? The projection operator that is used to define the pseudoinverse of the encoder is not necessarily a function, since there could possibly be many points on the manifold that correspond to the same L2 distance from the point being projected. The authors propose a new version of the regularized autoencoder where they explicitly regularizes its decoder to be locally isometric and its encoder to be the decoder's pseudo inverse. The authors claim that isometric autoencoders would "evenly sample the manifold" which is a little confusing, since the sampling of the data manifold is separate from the technique used to model the data (regular AEs vs isometric AEs). Through a series of experiments and visualization, the IAE exhibits better manifold structure. Strength: This paper provided a novel method to train a local isometric autoencoder, which can preserve the local Euclidean distances well between the original space and the latent space. The authors present conditions that need to be satisfied by the encoder and decoder parameters, and show empirically that the regularization terms that they propose ensure that the resulting autoencoder has an isometric decoder. In case there is a setting where isometric AEs can be shown to model the data manifold better than regular AEs, that is not highlighted in the current draft.
Under the backdoor attack, an adversary can manipulate a few clients' weight matrices to affect the final global model. Using this clustering algorithm combined with adaptive clipping and noising, the proposed method can mitigate the backdoor attack. This paper's main idea to defend against a backdoor attack is to use clustering and adaptive clipping and noising. To impede backdoor attacks, the Model Filtering layer (i.e., by dynamic clustering) and Poison Elimination layer (i.e., by noising and clipping) were presented respectively for the malicious updates and the weak manipulations of the model. I believe no matter what kind of backdoor and Trojan attack, can be easily applied to FL by applying them individually on each client without too much trouble. In the paper, the authors proposed a novel privacy-preserving defense approach BAFFLE for federated learning which could simultaneously impede backdoor and inference attacks. Weaknesses: This paper does not provide any theoretical guarantee and only applies the existing methods to mitigate the backdoor attacks. Strengths: This paper uses an existing clustering algorithm (the HDBSCAN clustering algorithm (Campello et al., 2013)) that works best in the FL problem in identifying manipulated weight matrices. This paper suggests a new solution to protect FL models from backdoor attacks. minor: please attach your main context pdf in the submission and submit the appendix in the supplementary material The paper proposes a backdoor-resilient federated learning method to defend the backdoor attack of poisoning the models. The author(s) have created many splendid terms to describe the modules used in this work, however, their implementation uses both clustering and median, which is very engineering and may not reliable with a different clustering algorithm or data set is severely unbalanced (just like the non-iid data sets among clients). Moreover, BAFFLE, combined with the Secure-Two-Party Computation method, can protect the FL model from Inference attacks. To impede backdoor attacks, many models are marked as outliers and discarded, clipped, and noised, generally speaking, which could lead to performance degradation.
This paper connects SDEs and GANs and proposed to learn the drift and diffusions in SDE under the framework of GAN. (iii) simplifying the construction of adjoint SDEs by a pathwise formulation. SDEs as GANs please explain the "Initial condition" statement better: why is it important that there be an additional source of noise here ?
Short summary: The paper introduces a promising new method, hierarchical nonnegative CP decomposition (HNCPD), as well as a training method for the HNCPD, neural NCPD, for topic modeling problems. Potential Improvements: Equation (4) introduces the forward propagation for a NNMF, which is crucial for the HNCPD and Neural NCPD. It seems like the authors combine existing ideas (hierarchical and neural NMF, NCPD) into a new method. The originality seems moderate, as already existing concepts of neural NMF and Hierarchical NMF are applied to NCPD. In the appendix, in the section "HNCPD expansion", in the 2nd sentence (starting with "We have that by definition..."), I think the square bracket "]" should be removed on the right hand side of the equation? Summary: In this paper, an extension of nonnegative CP decomposition called hierarchical nonnegative CP decomposition (HNCPD) is proposed. (2007) Hierarchical ALS Algorithms for Nonnegative Matrix and 3D Tensor Factorization. SUMMARY: This paper presents a hierarchical nonnegative CP tensor decomposition method. In Section 2.1, I follow the discussion up to Equation (6). Is it the expression in the Frobenius norm in the paragraph titled "Hierarchical NMF (HNMF)" above? In Section 3.2, it is claimed that HNCPD is better than NMF because "the chromatic NMFs obscure much of the chromatic interaction". In the experiments, do you use this approximation, or do you use the NCPD combined with HNMF for each factor matrix as discussed in the beginning of Section 2.1?
The main issues (that are detailed below) are: the paper does not sufficiently relate the discussed model or the universality results to previous or concrete models (set and graph NN, equivariant group NN, other unused but potentially useful variations), it does not provide sufficient explanation and justification to the different conditions in Theorem 11, there are some details in the proof and the description of Theorem 11 which are missing/unclear, Theorem 16 has some unclarity. If I understand correctly, the idea of the proof is basically using the extension mechanism of Theorem 3 to extend mapping to functions over base domains to equivariant mappings. The paper proves a universal approximation theorem for equivariant maps by group convolutional networks in an extremely general setting. Does theorem 1 implies that Deepsets (as equivariant model) is universal (as was proved in several previous works)? The main theorem shows how to convert fully connected networks to equivariant networks. Is that correct? Can you provide the proof for a simple example of equivariant networks such as Deepsets? Universal approximation theorems are considered to be an important kind of result, and this paper proves a very general one for equivariant maps. This paper considers a certain generalization of convolutional neural networks and equivariant linear networks to the infinite dimensional case, while covering also the discrete case, and offers a universality result. In this case, Theorem 3 states that any function f1(x) can be extended to be equivariant via fj(x)=f1(g⋅x), where g is a permutation such that g(j)=1. Theorem 11. First, looking at the proof, I feel there is a condition of the FNN ϕ that is missing from the theorem's formulation. What can be said about the equivariant tensor models and graph neural networks using the universality result in this paper? Related to that, I couldn't exactly understand the claim of the second to last layers in the proof of Theorem 11: the first layer outputs a function in C(G/HT×B), but the second layer of the FNN maps functions from some different domain C(B2). Summary: The paper studies the approximation power of equivariant neural networks in a very general setup: the input space is assumed to be a function space (compared to a finite-dimensional space in standard setups) and the group is assumed to be any locally compact group.
The paper main point then is to equate the learning rate in the weight decay coefficient by the effective learning rate and they show that this might be enough to bridge the gap between Adam and SGD. When applied to Adam, the authors derive AdamS, supposed to work better than the previous AdamW, which already improved how weight decay and Adam interact. AdamS has the stable weight decay property, unlike Adam or AdamW. The framework is applied to "fix" or "stabilize" weight decay when applied to SGD with momentum and Adam (which effectively amounts to changing the weight decay parameter). The authors propose to adjust for this by having the effective learning rate multiplying the weight decay term, ie Delta theta= -eta_{eff} lambda theta-eta_{eff} F(gradient). The framework includes the concept of "weight decay rate" and "total weight decay", where the idea is to make the "weight decay rate" constant throughout training. Summary: This paper presents a novel framework that alters the weight decay update rule and aims to improve generalization when applied to 1) momentum-based optimizers and 2) adaptive optimizers. The main conceptual point of the paper is simply that the weight decay should have the same effective learning rate as the gradients, it would be nice if the authors could make this more clear and more central. Comments and questions: I think the concept of weight decay rate and total weight decay should be explained better, and earlier in the paper. The stable weight decay property can be defined in dimension 1 as follow: the effective learning rate represents an amount of time ellapsed between two iteration. In the draft, the authors can directly say constant or time-varying weight decay. For SGD, this implies that one can roughly equate weight decay and L2 regularization by rescaling the L2-parameter. AdamS weight decay amount is scaled by the denominator of Adam, taking the average over all dimensions to make it isotropic. The concept of "weight decay rate" and "total weight decay" seems to be the most critical part of the paper, but the explanation surrounding them was messy and hard to understand. The definition of the weight decay rate can be done from equation (2). A minor point, in Eq 3, how did the authors arrive at −2t−1 in the superscript of weight decay rate, not −2t+1? Without knowing what "stable" meant, I had a hard time taking the introduction seriously, because I didn't know what "unstable" meant exactly, and that the decoupled weight decay method was "unstable". For example, Statement 1 that says "Equation-1-based weight decay is unstable weight decay in the presence of learning rate scheduler." can be quickly summarized even from an intuitive sense without any derivation, which makes it trivial. The weight decay factor normalized (in log space) by the time ellasped should be constant across iterations. There is no need to give Definition 1 formally for Stable Weight Decay as that doesn't sound like a definition.
This paper presents an estimator that predict higher-order structure in time-varying graphs. For strengths, first, the kernel estimator based method does not require learning process and shows good efficiency in the prediction tasks. More specifically, authors propose a kernel estimator which predicts the evolution of the graph through simplices and how they evolve with respect to some local neighborhoods. The kernel estimator also plays an important role in capturing the high-order interactions in the evolving graphs.
Summary: A tensor network model for text classification is introduced, which is constructed as the concatenation of a generative matrix product state (MPS) model for low-dimensional word embedding and a discriminative MPS model for classification. On that note... The "TextTN w/o word-GTNs" baseline in the ablation study (Table 3) seems rather misleading, as the paper text describes this as "we directly average the word vectors of words in a sentence to obtain the sentence representation".
The authors investigate different tokenization methods for the translation between French and Fon (an African low-resource language). I hope that the authors would add more persuasive results and analysis to realize a practical translation of Fon languages. Thus, I think this work is important to be able to provide speakers of Fon with a functioning translation system. You tell us that Fon is "a language with special tokenization needs" and that "standard tokenization methods do not alwaysadequately deal with the grammatical, diacritical, and tonal properties of some African language", and you cite the relevant papers.
This claim is repeated in paragraph 2 of Sec 3: "uni-modal similar to conventional GMM attention". Reasons for score: The main contribution of this paper seems to be the addition of predicted weights for each encoder step that allow the monotonic GMM attention mechanism to ignore certain input frames. The precise monotonic attention gains a lot of attention for streaming ASR and simultaneous machine translation, and this paper would gein broad interests. Summary: This paper introduces "source-aware" GMM attention and applies it to offline, online, long-form ASR. The paper describes a simple extension to the location-only monotonic GMM attention mechanism from Graves (2013), which takes the source/key context into account when computing attention weights. Pros: Incorporating source content is an obvious and useful extension to monotonic GMM attention, combining the strengths of content-based approaches such as additive or dot-product attention.
The paper presents an improved positive sample selection and data augmentation method for unsupervised, contrastive representation learning. Positives: interesting proposed method for expanding the neighborhood space of considered positive matches for contrastive learning ablation study provided to show the improvement from each proposed component (sample selection, cutmix, multi-resolution) ## Summary The paper addresses the problem of contrastive representation learning, and proposes a new data augmentation, dubbed CLIM, that leverages similarity between images. Authors propose two improvements to be used in contrastive representation learning: a positive sample selection scheme (called center-wise sample selection), that improves over previously proposed kNN or k-means methods; and multi-resolution data augmentation which is an extension of a known crop augmentation technique with multiple scales. The main contribution is 2-fold: a) use nearest neighbors from the same cluster which are closer to the centroid than the anchor as positive samples; b) use more complex augmentations, i.e. [CutMix] and multi-resolution during training. While the improvements that the CLIM augmentation brings seem to be quite good in the linear evaluation on ImageNet by boosting the results of MoCo v2 (which is used as a baseline if I understood correctly) by 4 points, the improvement of this representation on other downstream tasks (ie. And the contribution of this paper is applying CutMix in the context of contrastive learning, which is yet another augmentation among a huge variety of possibilities. -as it is standard in contrastive learning-, positive pairs are generated using those similar images to the anchor image: after clustering the representation space using k-means, the nearest neighbours that are closer to the corresponding center of the cluster where the anchor belongs to are selected. Two components are proposed: First, to select semantically similar images that are pulled together in the contrastive learning, the paper proposes "center-wise local image mixture" (CLIM) Ablation study proves positive impact of each of the two proposed improvements on the final performance. "Contrastive learning targets at learning an encoder that is able to map positive pairs to similar representations while push away those negative samples in the embedding space." does not sound right, especially usage of word "those", rephrase. The main idea is to consider the semantic similarity between different images and incorporate it in the learning procedure, in contrast to the many contrastive learning methods which only used augmentations of the query image as positives.
This paper presents a graph topology learning algorithm based on SBM and neural networks, which employs the node embedding and labels to optimize the topology. In my opinion, the over-smoothing is caused by the depth (or receptive field) of GNN and the message passing manner, but irrelevant to the graph topology. The main idea is to optimize the graph topology by removing the inter-class edges as well as adding the intra-class edges, and then the noise information would not pass between nodes with different categories. This paper proposes a joint learning framework for GNN classification model and graph topology, which leverages variational EM as a learning framework. They incorporate a variational approach to GCNs, as a novel architecture that iteratively refines the node labels and the graph. In summary, I agree that topology optimization is beneficial to enhance the node classification performance, but its effect on surpassing over-smoothing is suspicious. The authors present an EM variational algorithm for approximating both this latent graph and using it to improve the estimation of a GCN. Strengths: The paper tackles an important question in the GCN literature, which is how to deal with situations in which the graph is unobserved or the observed graph structure is only a fraction of the true graph.
I have the following comments and questions that should be clarified to evaluate the relevance of the results: When comparing with other architectures on image classification tasks (CIFAR10 and CIFAR 100), what is the dense layer that is replaced by the butterfly structure? Theorem 1 shows that replacing a dense layer by two truncated butterfly networks and a dense layer will not be too different from the original network. I think a comparison replacing a convolutional layer by a butterfly layer could bring some useful insights. The paper's aim is to establish that linear layers can be replaced by butterfly networks and uses three different experiments to show this.
Summary The authors propose LIME: a pretraining strategy for learning inductive biases for mathematical reasoning. But this result does not necessarily mean that the model has the inductive biases that were intended to be imparted; it's possible that LIME imparted some other inductive biases that are also useful for mathematical reasoning but that are not related to induction, deduction, and abduction. As Table 4 shows, the nature of the pretraining task is very important; and although the authors were able to create some successful pretraining tasks for mathematical reasoning, it might be harder to create similarly useful tasks for larger-scale tasks in, e.g., language or vision.
This works aims at unifying 3 popular regularisation type continual learning methods, namely EWC, SI and MAS, by showing that under some assumptions they all relate to the Fisher Information Matrix. However, I have an issue with the proposed quicker/cheaper update for calculating the (diagonal empirical) Fisher Information Matrix for Online EWC ("Batch-EF"), as I detail later. While the paper is well-written and -structured, and SI and MAS are popular methods in the literature, I'm not convinced by the link through the "Absolute Fisher". The authors merely show that MAS and SI are similar to using Absolute Fisher as importance weights. It shows that MAS and SI approximate the Absolute Fisher matrix. The authors show that the importance weights of MAS and SI are similar to the diagonal absolute Fisher. Pros Connecting the path integral of SI to the Absolute Fisher is interesting. Cons of paper I am not convinced that the minibatching that the authors suggest (both for SI as an approximation to the AF and for EWC in the last paragraph of Section 5) is correct ("Batch-EF"). ** END OF UPDATE*** The paper unifies two regularisation-based continual learning methods from the literature (Synaptic Intelligence and Memory Aware Synapses) by arguing that they both approximate the "Absolute Fisher", a variant of the Fisher Information that averages absolute instead of squared gradients, to determine the regularisation weights. I am also not convinced that OnAF ("Online Absolute Fisher") and AF are / should be the same. I think the authors should justify that the Absolute Fisher is an optimal regularization under certain assumptions. Summary This paper attempts to unify the three most prominent regularization-based continual learning methods: EWC, MAS, and SI. Summary of paper This paper draws links between three common regularisation methods for continual learning: EWC, MAS, and SI. It appears to me that by minibatching instead of squaring each gradient element, one should obtain much worse approximations to the empirical Fisher information matrix ("EF"). First, estimating the Fisher as the square of the averaged gradients has been done before, for example it is implemented in the tensorflow KFAC codebase. The assumptions make the use of the diagonal Fisher information an optimal choice. Therefore, I cannot agree with the claim that this paper presents a unified framework, just because it fits several methods into distinct concepts that have "Fisher" common in their names. By this the authors claim to establish a relationship between these two approaches and Elastic Weight Consolidation, which approximates the diagonal of the Fisher. Although I am not aware of previous works using the absolute value of gradients (as in AF), this paper provides evidence that such an approximation might be worth considering. I think some claims can still be reduced, particularly, the link between AF and EF (and hence the link to EWC). In more recent versions, Pytorch also provides autograd functions for computing jacobians natively -- I'm not sure how efficiently they are implemented, but in any case the empirical comparison here needs to use an efficient, batched implementation for calculating the Fisher in order for it to be meaningful.
Because the training of CBD is divided into two stages, the diversity of the second stage only brings more training data to enhance the supervised machine translation model, rather than unsupervised machine translation effect. In Appendix Summary: The paper proposes an additional stage of training for unsupervised NMT models utilizing synthetic data generated from multiple independently trained models. References: [1] Data Diversification: A Simple Strategy For Neural Machine Translation, Nguyen et al. [2] APE at Scale and its Implications on MT Evaluation Biases, Freitag et al. [3] On The Evaluation of Machine Translation Systems Trained With Back-Translation, Edunov et al. [4] Multilingual Denoising Pre-training for Neural Machine Translation, Liu et al. [5] Leveraging Monolingual Data with Self-Supervision for Multilingual Neural Machine Translation, Siddhant et al. [6] When Does Unsupervised Machine Translation Work?, Marchisio et al. In this paper, two unsupervised agents are utilized at cross-model by using the dual nature of the unsupervised machine translation model, in which forward translation of agent_1 is combined with the backward translation of agent_2, more synthetic translation pairs are obtained to train a new supervised machine translation model. The result is improved on multiple unsupervised machine translation, and this paper claims that more diversity is brought to the synthetic data, so a better translation model can be trained. The authors add this additional stage of training to unsupervised NMT models using different pipelines (PB unsupervised MT, Neural Unsupervised MT, XLM) and show that their approach improves all of these approaches by 1.5-2 Bleu on WMT En-Fr, De-En and En-Ro. Strengths: The paper is well written, the approach is simple and seems to improve quality by significant amounts in a variety of experimental settings. Source of promotion: the second stage of CBD method adopts (x_s, y_t), (z_s, y_t), (y_s, x_t), (y_s, z_t) synthetic translation pairs, it is not clear how much performance growth comes from increased data and how much growth comes from the new model implementation (ott et al., 2018). The idea is pretty straight-forward, if not altogether intuitive, you begin by training two bidirectional (i.e.: they can translate source to target and target to source) unsupervised MT systems A and B. This paper uses a reconstruction BLEU or BT BLEU [1] metric to compare the effect of the inside-model with that of cross-model, and finds that cross model translation has a lower back-translation effect, which shows that the diversity is enhanced. They can directly add the synthetic data decoded by cross model to continually train the original XLM model with a supervised translation objective (which is naturally supported in XLM from my experience), and report the effect comparison between them. [1] Li, Zuchao, et al. "Reference Language based Unsupervised Neural Machine Translation." arXiv preprint arXiv:2004.02127 (2020). Experimental results in several translation tasks show that the proposed approach improves the translation accuracy of the standard unsupervised machine translation models, outperforming the cross-lingual masked language model. This paper introduces a new component to the unsupervised machine translation framework called cross-model back-translated distillation.
Focusing on the projected dynamics, the equivalence is built between SGD and a type of "Adam". It contributes a formula for the equivalent learning rate if the gradient step was to be taken on the unit sphere for SGD and Adam and shows an approximate equivalence between gradient steps and normalized gradient steps taken on the unit sphere. Hence, the sentence "performing SGD alone is actually equivalent to a variant of Adam constrained to the unit hypersphere" in the abstract is misleading and conveys over-optimistic information.
Summary: This paper introduces a fairness perspective on accuracy performance among distinct classes in the context of adversarial training. In this case, it seems that even before adversarial training, we see a difference in class errors between the classes. This paper begins with the empirical observation that adversarially trained models often exhibit a large different in clean (and robust) accuracies across different classes. Clearly, given that this is a real-world dataset, it is natural to expect that the effect of adversarial training is not perfectly linear across all classes. Strength: The paper makes an interesting observation on a fairness perspective in adversarial training.
This paper proposed a powerful online sequential test which can efficiently detect qualitative treatment effects (QTE). This paper studies online test for qualitative treatment tests. However, I don't see that the algorithm addresses any real online challenge. However, there are some questions that I did not fully understand. The test algorithm involves adaptive randomization, sequential monitoring and online updating. The challenge of online testing is that we are doing testing at each iteration, and it is difficult to adjust all p-values.
It also is not clear to me from the paper's text whether something close to the PABI framework can apply in broader settings like language modeling style pretraining, where the input-output format of the incidental supervision signal is different than that of the target task. PABI can measure various types of incidental signals such as partial labels, noisy labels, constraints, auxiliary signals, cross-domain signals, and their combinations. This paper proposes PABI (PAC-Bayesian Informativeness?), a way of measuring and predicting the usefulness of "incidental supervision signal" for a downstream classification task. Mathematical developments of PABI are given for these cases, and experiments show that PABI is nicely positively correlated with the relative improvement that comes with various methods for integrating incidental supervision signal (including one which is developed as a side note by the authors). ########################################################################## Summary: This paper proposes a unified PAC-Bayesian-based informativeness measure (PABI) to quantify the value of incidental signals. In NER and QA tasks, they showed the strong correlation signals between PABI and the relative improvements for various incidental signals. The definition and approximation of PABI and its generalization to different inductive signals look sound to me. However, even if the empirical results are good, the connection between PAC-Bayes and the proposed informativeness measure (named PABI) is vague. Besides the presentation, I don't quite understand how PABI can be used as a practical measure for other applications. No PAC-Bayes bounds are fully optimized; PABI borrows from PAC-Bayes the sole idea of relying on the KL between distribution.
They also present a new metric called Sequential Search Work Ratio (SSWR) to evaluate the quality and efficiency of the search. Then, the authors design SSWR metric evaluate the efficiency and quality of the search process. At the end of section 3, after presentation of SSWR, it is not clear why we are minimizing for a search sequence generator G that is aggregated over database-query pairs (D,q) -- wouldn't we learn a data structure per database (as is done for data structures used for nearest-neighbor search)?
This paper presents a variation of the MONet model where an additional Region Proposal Network generates bounding boxes for various objects in the scene. It extends the previous MONet by introducing the region-based self-supervised training. Although the self-supervised loss is intuitive, incorporating it into MONet is non-trivial and it does outperform MONet. Despite that such self-supervised works are hard to work on real scenes, this paper does have some merits. An additional loss is introduced during training to make the segmentations produced by the MONet segmenter consistent with the proposed bounding boxes. The self-supervised idea is interesting that uses the segmentation mask to get the pseudo bounding box label for object detection, which could ensure the consistency of object mask and bounding box. MONet uses spatial attention to identify the most salient object one by one, which makes senses. In the original MONet, the spatial attention network is an RNN-like structure, they decompose the scene step-by-step. For example, in Figure 3, the visual performance of MONet and R-MONet(UNet) are quite similar. The author should compare their R-MONet(UNet) with the baseline of R-MONet(UNet) w/o the self-supervised loss, i.e. removing the object detection branch. [Paper Weakness] The self-supervision between segmentation masks and detection bounding boxes is the main contribution.
I don't see how the theory in Section 2 is crucial for the understanding of the impact of β. The choice of optimal β still remains unclear and left to the future work. This paper investigates how the inverse temperature parameter β in the softmax-cross-entropy loss impacts the learning and the generalization. In the theory part, this paper introduces the concepts of early learning timescale and nonlinear timescale, and shows how the learning dynamics depend on the parameter β. The results suggest that the optimal β is architecture sensitive. This paper analyses the how the dynamics of the training is effected by a non-unit scale β. Q) As far as I understood, from the ovservation of Sec 2.2 to 2.4, setting large β makes the times scales τz and τnl smaller, which means that  the early linear learning timescale and nonlinear timescale smaller.
This paper proposes a generalized additive model to learn joint intensity functions for multiple Poisson processes. So my first question is: i) Can you define the likelihood in a single Poisson process model in order to consider all the interactions you mentioned in section 2.2? For example, a Gaussian process is a stochastic process, but doesn't have anything to do with the processes considered in this work. For example, on page 2: Given a realization of timestamps t1,t2,…,tN with ti∈[0,T]D from an inhomogeneous (multi-dimensional) Poisson process with the intensity λ.
Pros: Presents a long range convolution layer, with an efficient implementation so that a neural network model can benefit from both short and long range interactions among data points. The long-range convolutional (LRC)-layer mollifies the point cloud to an adequately sized regular grid, computes its Fourier transform, multiplies the results by a set of trainable Fourier multipliers, computes the inverse Fourier transform, and finally interpolates the result back to the point cloud. The paper is clearly written, and presents an approach to efficiently utilize long range convolutions through a nonuniform FFT in for coulomb particle configurations. The paper proposes an efficient long-range convolution method for point clouds by using the non-uniform Fourier transform. Probably there is a regime where direct convolution (which does not need the approximation introduced by g_\\tau, as far as I understood), is better then the NUFFT approach introduced here. Concretely, I expect that for too large \\tau/L (presented in appendix, Equation 20), the precision of the long range kernels will be poor and error will be large;
The paper proposes to separate the dominant factors and the residuals,  train the original DNN on residuals with a much faster SGD learning rate,  and then recombine with a shallow small NN learned on the dominant factors trained using its own (slower) learning rate. If we were to remove the dominant factors, the residuals would have much weaker correlation structure and allow faster DNN convergence with SGD. 2 is very dense and difficult to follow, and only seems to motivate the method in the special cases of a very shallow linear regression model, where the gradient is easily related to eigenvalues of the input features X (and assuming a "strong factor structure", which has been shown to be reasonable for natural images.) The paper also claims that strong factor structure (with a small number of dominant components) is prevalent in modern ultra-large scale DNN applications -- I would like to see some references / supporting evidence beyond just computer vision. The paper describes a training scheme based on decomposing input features into two parts which have different training dynamics: a low rank "factor feature" computed using PCA on the raw features, and a high rank "residual". Can you give some references claiming strong factor structure in several DNN applications in ultra-high dimensions? The paper makes an observation that datasets used for training many deep neural nets exhibit a strong factor structure, i.e. have a small number of dominant principal components explaining most of the variance. Missing obvious baselines of training separate models on either the factor features or residual alone. Another criticism -- is that there is no discussion of how to do large-scale PCA / factor analysis for high-dimensional image data arising in say modern DNN image classification pipelines, and its computational cost,  as simple numpy.linalg.svd won't work. Perhaps this is the motivation for only processing "factor features" with an extremely shallow (albeit still nonlinear) network? Sec 3.3: "a standard SGD algorithm cannot be used to train a DNN model".
The paper can be positioned a bit better if the paper uses these models, and performs additional learning on top with the 'dataset-internal' contrastive learning approach in order to improve performance on long-tail zero and few-shot learning. The proposed, dataset-internal contrastive self-supervised learning benefits in zero- or/and few-shot learning settings. Another concern is the unclear description of how two binary labels are optimized in a contrastive way.
The paper proposed a learned variant of the well-known iterative Hessian sketch (IHS) method of Pilanci and Wainwright, for efficiently solving least-squares regression. For example in Figures 1, 2, 6, 7, that the learned IHS has the same linear convergence rate as the unlearn IHS, and actually only slightly faster overall (only 1 iteration faster if we view the plots horizontally), but the authors' claims are like "We can observe that the classical sketches have approximately the same order of error and the learned sketches reduce the error by a 5/6 to 7/8 factor in all iterations", "We observe that the classical sketches yield approximately the same order of error, while the learned sketches improve the error by at least 30% for the synthetic dataset and surprisingly by at least 95% for the Tunnel dataset. From the reviewer's point of view, the meaningful analysis for learned IHS should certainly be how the converge relates to  the statistics of the training data, to show the benefit of the learned sketch theoretically (i.e. to show how much better the learned sketch SA compare to the unlearned ones in terms of Z_1 and Z_2). Theorem 3.2: Given the error in Lemma 3.1, the bound in Theorem 3.2 needs to be updated accordingly. Overall, the idea of combining IHS with learned sparse sketch is interesting, but the reviewer believes that the current version needs significant amount of rework to be publishable in top conferences. In the experiments, the sketching matrix is learned in each iteration. Proof of Lemma 3.1: In Section A, 2nd sentence, you say "Since T is a subspace embedding of the column space of A...". What is the total running time of the learned sketch against the other baselines?
For the first objective, the data supports 6 clinical classification tasks (which were proposed before in a earlier MIMICIII benchmark) but also supports 4 different modalities: physiological time series (the standard modality that used in the prior benchmark), clinical notes, baseline data, and waveform (although not being used in the method/experiments). It collects a dataset called M3 as the benchmark for the multi-modal and multi-task benchmark in the clinical domain. Considering multi-modal multi-task settings in the clinical domain is useful for the development in this area.
Specifically, they propose two regularization terms to 1) capture the diversity of the tasks and 2) control the norm of the prediction layer, thereby satisfying the assumptions in meta-learning theory. To improve the practical performance of meta-learning algorithms, this paper proposes two regularization terms that are motivated by two common assumptions in some recent theoretical work on meta-learning, namely ########################################################################## Summary: The paper reviews common assumptions made by recent theoretical analysis of meta-learning and applies them to meta-learning methods as regularization. In some experimental results, the improvement due to the proposed regularization seems to be at the same level of the standard deviation, as well as the difference between the reproduced results of existing meta-learning algorithms and those reported in earlier papers. Numerical experiments show that the proposed regularization terms help achieve better performance of meta-learning in some tasks. Results show that these regularization terms improve over vanilla meta-learning. One main theoretical assumption in meta-learning theory is the task distribution. [2] HOW TO TRAIN YOUR MAML, *CONF* 2019 The main motivation of this paper is based on the theoretical results of meta-learning. As to regularizing the Frobenius norm, there exist a line of literature showing weight decay works for general settings apart from meta-learning. Following the previous point, I'm curious about one question: if the learning problem actually doesn't satisfy the two assumptions, then is it still helpful to add the proposed regularization terms to the loss function? Here are the main concerns of this paper: The proposed method in this paper is based on the meta-learning theory as stated in Section 2. The meta-learning loss in Eq. 4 is a bit different from the popular meta-learning objective. Summary: In this paper, the authors aim at bridging the gap between the practice and theory in meta-learning approaches.
compared to existing graph pooling methods, the authors think their methods are able to capture information from all nodes, collect second-order statistics, and leverage the ability of neural networks to learn relationships among node representations, making them more powerful. Weaknesses: My biggest concern is that the proposed approach lacks originality and novelty because it is a simplification and variant of SOPOOL from Second-Order Pooling for Graph Neural Networks (Ji and Wang, 2020) Wang and S. Ji. Second-order pooling for graph neural networks. Wang and S. Ji. Second-order pooling for graph neural networks. Both of them are flat pooling strategies, which try to obtain a graph representation directly from its node representations without coarsening graphs step by step. Based on the author's writing, it is unclear what is the second-order statistics for graph pooling, why it is important to have second-order pooling, and how the proposed method can capture the second-order statistics.
The optimal Bayes-adaptive policy explains when it is optimal to explore a new task and learn adaptive behaviors, depending on the horizon length and amount of exploration needed, and therefore, the main claim of the paper can be framed as observing that existing meta-RL agents can learn the Bayes-adaptive optimal policy in simple tasks. Summary. The authors investigate the question of when the optimal behavior for an agent is to learn from experience versus when the optimal behavior is to apply the same (memorized) policy in every scenario. If the research question is "How does the optimal policy depend on task parameters such as uncertainty and horizon?" I believe Bayes-adaptive work answers that question. Summary: This paper observes that in meta-RL (and evolutionary biology), sometimes it is advantageous to learn behaviors that adapt to the particular task, while other times not adapting to the task, and instead relying on a task-agnostic "hard-coded" behavior is sufficient. Specifically, this paper presents three main findings: (i) whether or not it is optimal to learn adaptive behavior strongly depends on the horizon of the task and complexity of learning such adaptive behaviors — if the horizon is too short, or if the adaptive behavior requires complex exploration, then exploring the new task to learn adaptive behaviors may not be worth it; Similarly, the grid world tasks in Section 4 also clearly illustrate how the behavior of meta-RL agents changes with the horizon of the task. (ii) existing meta-RL agents are capable of choosing not to learn adaptive behaviors, when it is optimal to do so; This is an interesting observation, but prior work [2] already shows that meta-RL is equivalent to learning the Bayes-adaptive optimal policy. Empirical evidence in two sets of experiments, on a 2-armed bandit toy task and a grid-world navigation task, show that hard-coded strategies can be a function of training task distribution and task complexity as well as task horizon. This section also convincingly shows that existing meta-RL agents roughly learn the Bayes-optimal policy in this case.
The authors claimed that, by adjusting α through training, the trained model m(⋅;θ^) with an optimal parameter θ^ is asymptotically consistent with the model trained on a dataset with clean labels, i.e.,  the trained model without α performs well on clean test data Adding experiments with synthetic datasets with different levels of noise can be helpful in understanding the advantages of AC1/AC2 over other methods for handling noisy labels. This work presents a theoretical model formulation to capture the biased effects of incorrect labels automatically Note that this trivial solution does not depend on the model m(⋅;θ).
When making identification arguments, assumptions play the role of eliminating equally plausible causal explanations of the observed data, until only the true one can remain (if the assumptions are true). its positioning in the standard causal inference framework which it claims to make a new contribution to and its causal model/underlying assumptions about confounding are not stated clearly enough. It provides theoretical results on the identifiability of the confounder and the treatment effects under some assumptions. For example, the statement "CEVAE assumes a specific causal graph where the covariates should be independent of the treatment given the confounder." seems to contradict the end of page 3 in Louizos et.
Besides, one thing should also be noticed: the speedup yielded by HRT seems less charming compared to recent NAT models, such as Levenshetain Transformer 3x-4x (Gu et al, NeurIPS 2019), and JM-NAT (k = 10) 5.73x (Guo et al, ACL 2020) where both models achieve similar translation quality to the AR baseline. This paper targets at alleviating this IR bottleneck by proposing hybrid-regressive translation (HRT), which combines AT and NAT in two stages. This paper proposes a hybrid-regressive machine translation (HRT) approach—combining autoregressive (AT) and non-autoregressive (NAT) translation paradigms: it first uses an AT model to generate a "gappy" sketch (every other token in a sentence), and then applies a NAT model to fill in the gaps with a single pass. Early on the paper claims that HRT can outperform AT in terms of translation quality, but this doesn't seem to be the case if one compares HRT against AT both trained with mixed distillation. Experiments on two WMT translation tasks (four translation directions) demonstrate the effectiveness of HRT, which consistently accelerates decoding by >50% compared to the AT baseline and yields comparable or even better translation quality.
Moreover, if the authors want to use the tree size as a mean to measure branching decision quality, they must also provide the optimal solution value to the solver at the beginning of the solving process, in order to deactivate side-effects from pruning. ############################################################################### Summary The paper presents a novel method for learning branching strategies within branch-and-bound solvers, which consists in a graph-convolutional network (GCN) combined with a novelty-search evolutionary strategy (NS-ES) for training, and a new representation of B&B trees for computing novelty scores. ############################################################################### Pros and cons Pros: the idea of learning branching strategies using reinforcement learning instead of imitation learning makes a lot of sense and seems like a promising direction to follow the proposed representation of B&B trees is original the presented results seem promising It is known that node selection and branching strategies do interact with each other, and all the evaluated branching methods were proposed in the context of a default, state-of-the-art solver. The paper proposes a method of choosing variables for branching in branch and bound approaches to MIP solving. ############################################################################### Questions to authors I would appreciate if the authors could clarify why they conduct some of their experiments in the "clean" setting, why they don't report results on the complete benchmark from Gasse et al., and also comment on the performance of SVM vs GCN, which contradicts what is reported in Gasse et al. ############################################################################### Final recommendation
The paper proposes a knowledge distillation method for face recognition, which inherits the teacher's classifier as the student's classifier and then optimizes the student model with advanced loss functions. Instead of minimizing the outputs of teacher and student models, ProxylessKD adopts a shared classifier for two models. Second, ProxylessKD can be interpreted as initializing classifier of student model by the classifier of teacher model. The paper demonstrates using an ensemble of teacher models can boost the performance of knowledge distillation. It needs sufficient analysis to justify the authors' choice of only applying the teacher model's classifier as distillation. ECCV2020 This paper proposes a new KD method to inherit classifier from teacher models and utilize it to train the student model feature representation, where previous KD methods are mostly focusing on the proxy task other than the target task itself. Since the optimization objective for student model is learning discriminative embeddings,  the face recognition performance is improved compared to the vanilla KL counterpart. The idea of using teacher model's classifier to directly reshape the student model's feature representation is somewhat novel.
Specifically, it proposes PeerPL to perform efficient policy learning from the available weak supervisions, which covers PeerRL (for RL with noisy rewards), PeerBC (for imitation learning from imperfect demonstration) and PeerCT (for hybrid setting). The relationship between the RL and BC solutions appears to be superficial, however, as PeerRL and PeerBC seem to address noisy supervision in very different ways. The decomposition of the noisy reward in lines (7) and (8) seems to assume that, even in the absence of noise, the expected reward signal will be zero, regardless of the policy being followed, or the current state. In RL, the issue is often not that the reward is noisy in a fixed state, but that it is sparse within the state space, such that the return under a random policy is noisy.
Overall, this paper is interesting in setting up a benchmark for unsupervised object representations which is a very important problem in computer vision, reinforcement learning, etc. The paper proposes a benchmark for the evaluation of unsupervised learning of object-centric representation. Paper Strengths Although I am not familiar with unsupervised learning of object-centric representation, I like the idea of proposing a common protocol for evaluation. [1] SPACE: Unsupervised Object-Oriented Scene Representation via Spatial Attention and Decomposition, *CONF* 2020 The paper presents an empirical evaluation of a number of recent models for unsupervised object-based video modelling. The benchmark consists of three datasets, multi-object tracking metrics and of the evaluation of four methods. The proposed dataset consists of three sets of video sequences, procedurally generated, which are either generated from slight variations of existing works (Sprites-MOT) or on the basis of existing datasets (dSpirites, Video Object Room). The paper highlights important weaknesses of unsupervised object models, such as the overreliance on color or the difficulties with handling occlusion. The benchmark is comprehensive, in that it contains both data and evaluation measures.
The paper proposes a federated learning framework using a mixture of experts to trade-off the local model and the global model in a federated learning setting. The mixed-use of global and local models (equation 6) is not a novel way of federated learning. Federated learning methods, in a context of privacy protection, aim at building a global model, using private data, without revealing anything about the private data. The paper proposed a novel personalized federated learning method using a mixture of global and local models. If equation (8) is applied, this means that local data is used to learn the global model through w_g, so there is a leak of local information which contradicts the strict respect of privacy. Isn't the best solution, in this case, simply to learn a standard classifier for each local problem, using both local and global data and using a loss function that weights the examples according to the class distributions?
Then, the paper proposes to use mean-field approximation to approximate the optimal joint policy (Equation (3)), which leads to a concrete algorithm that relies on passing the embedding of each agent's local policy around to neighbors. For the assumptions on rewards, Proposition 1 assumes that each agent's reward depends on its neighbors, while the derivation of Equation (3) (and thus the following algorithm) further assumes that the reward depends on pairwise actions. Paper Summary The paper considers the cooperative multiagent MARL setting where each agent's reward depends on the state and the actions of itself and its neighbors The paper has a theoretical claim that, for such reward structure, the optimal maximum entropy joint policy in the form that can be factored into potential functions, one for each agent. An agent encodes its policy and sends the "intention" to the neighboring agents with the assumption that only the closest agents would be the affected by it. Although the authors emphasize that they are communicating the intentions of agents, I think their method is quite similar to those communicating local observations, like NDQ (https://arxiv.org/abs/1910.05366), DGN, or CollaQ (https://arxiv.org/abs/2010.08531). In the experiments, it would be interesting to check if intention only helps the nearby agents. In particular, if the sum of all agents' rewards is a function on pairwise actions, those potential functions are one for each agent and one for each pair of actions (i.e. the equation after Proposition 1). This method is based on the loosely coupled reward structures among agents, which, as far as I am concerned, are generally held in complex multi-agent settings. Proposition 1: "The optimal policy has the form ... For the baselines used in the experiments, it seems that only IP and DGN allow communication/message passing during execution, which makes it unsurprising that the two methods outperform other baselines. The paper proposes a scalable approach via intention propagation to learn a multi-agent RL algorithm using communication in a structured environment. However, the exact assumptions are not clear, and the chain of issues discussed throughout section 4 seems to include discussion of approximation. However, the exact assumptions are not clear, and the chain of issues discussed throughout section 4 seems to include discussion of approximation. Major Comments/Questions Although the motivation has an interpretation of intention propagation, the resulting architecture (Figure 1b) and loss functions (Section 4.2) seems to be a standard messaging passing architecture with SAC loss functions that loses the intention semantics.
In more detail, the authors claim four proposed components: AdaQuant, Integer programming, Batch-norm tuning, and two pipelines for neural quantization. The methods include AdaQuant (which jointly optimizes quantization steps for weight and activation per output activation of each layer), Integer Programming (which determines bit-precision for all the layers), and the batchnorm tuning. In particular, the authors focus on sub-8 bit quantization and propose a novel integer linear programming formulation to find the optimal bit width for a given model size. Below are some of the errors that I caught: 3 OPTIMIZING QUANTIZATION PIPLINE -> PIPELINE Note that the impact of quantization in the earlier layers affect the quantization impact in the current layer. Also, it seems that the "per-channel" quantization method is utilized in this work, but the formulation in (2) seems to be for "per-layer" optimization. BOPS proposed by (https://arxiv.org/pdf/2005.07093.pdf) is a good metric to measure the total reduction in computations for mixed precision quantization. This is a good result but please note that other work in the literature (arxiv:2001.00281) reports 72.91% for INT8 quantization of MobileNetV2 (this comparison is actually missing from the paper). It is straightforward to think that the joint optimization of quantization step size for weight and activation would result in better quantization results. Note that the biggest merit of post-training quantization is its simplicity (cf., QAT incurs full-blown training epochs); thus increased cost for post-training quantization is not desirable. Page 8: For instance, on the extensively studies -> For instance, on the extensively studied This work presents a quite comprehensive multi-step scheme for post-training neural quantization that does not rely on large datasets or large computational resources. The paper introduces a series of techniques to quantize neural networks, and how to combine them: Layer by layer quantization where weights can change as needed (rather than to the nearest quantization error). Additional approaches are proposed to minimize accuracy degradation after quantization. The authors propose to use many techniques to push the limit of neural quantization, which shows reasonable improvements in some datasets. Summary: The paper studies the problem of Post-Training Quantization of NNs, where no fine-tuning is performed to quantize the model. Some spelling mistakes, e.g., "Optimizing Quantization Pipline" Baselines are barely discussed. Also, I am not very familiar with quantization papers, so I might have missed relevant baselines, but they seem hard to compare. I think section 4 can be titled simply "Quantization flow".
It proposes to use Hamiltonian Monte-Carlo (HMC) to sample the next states (instead of IID samples) and matrix completion to learn a low-rank Q matrix. In reference, McAllister and Rasmussen: ta-efficient -> data-efficient In this paper, a Hamiltonian Q-learning is proposed by combining Hamiltonian Monte Carlo with matrix completion. 2607-2613). AAAI Press. This paper studies appling Hamiltonian sampling to computing Bellman equation (thus Q-learning iterations) approximately. The so-called Hamiltonian Q-learning takes minimization optimization and essentially claims an equivalence of energy in physics model, which might not always make sense.
The authors present theory that backs up these claims and they propose a new generator training objective which re-weights the gradients of the generator objective to have the same average magnitude as NS-GAN but have the same relative magnitudes of the original GAN objective. This paper proposes an explanation for mode collapse in the original GAN with the log -D objective for the generator (dubbed the non-saturating GAN or NS-GAN for short). The GAN community quickly observed that when the discriminator outperforms the generator with this objective, the saturating nature of the sigmoid function causes the gradients to vanish for the generator's objective. Because the samples from modes that are already overrepresented are likely declared fake by the discriminator, the contribution to the generator gradient is dominated by these samples in NS-GAN. When GANs were originally proposed, most researchers saw the NS-GAN objective as a strict improvement over the MM-GAN objective and moved on. The paper takes the approach of comparing the gradient of the generator objective for the original GAN with cross-entropy loss (dubbed the minimax GAN or MM-GAN for short) and the log -D variant. The key observation is that the difference between the gradient of the generator objective of MM-GAN and NS-GAN is that MM-GAN has a factor of D_p(G(z,\\theta)), whereas NS-GAN has a factor of 1-D_p(G(z,\\theta)), where D_p(G(z,\\theta)) is the output of the discriminator on a sample generated from z. Summary: This work proposes that many common issues with GAN methods are based on the weighting of the samples given to the generator's objective function. It sheds light on the weaknesses of the log -D variant of GANs. Weaknesses: Paper title is broader than what the paper shows - the proposes explanation only applies to GANs with the log -D generator objective and does not apply to other GAN variants, e.g.: original GAN (with cross-entropy generator objective), WGAN, LSGAN etc. For this reason, a new objective (NS-GAN) was proposed which modifies the generator's objective to alleviate this gradient vanishing issue. Since a near 20-point increase in FID can be achieved with a few tweaks to the NS-GAN objective (SN-GAN), I am left wondering if the presented improvement from the MM-NSAT objective will vanish once those improvements are applied or if it will still hold. This work does a great job to demonstrate that NS-GAN is most definitely not "superior" to MM-GAN and may possibly be a worse choice of objective function. This paper reexamines the original (MM) and the non-saturating (NS) GAN objective. They focus on a study of the original GAN objective proposed in Goodfellow et al. where the generator's objective is the negative of the discriminators objective.
Their algorithmic contributions are essentially two-fold: -They propose RIDE-SimCLR, a modification of RIDE which replaces RIDE's intrinsic motivation (the l2-distance, between timesteps, of an embedding of observations) with a SimCLR-inspired contrastive learning dissimilarity measure. -They integrate a version of episodic memory with this, proposing EC-SimCLR, with an episodic RIDE ablation RIDE-MinAgg. Using procedurally-generated enviuronments from MiniGrid, they go on to show superior performance of EC-SimCLR over RIDE, RIDE-SimCLR, and RIDE-MinAgg, while they claim RIDE and RIDE-SimCLR to have comparable performance. This paper proposes RIDE-SimCLR, that uses self-supervised learning technique like SimCLR to learn a representation for RIDE to use. RIDE (i.e., rewarding impact-driven exploration) is an intrinsic reward signal (Eqn. They choose to focus on procedurally-generated MiniGrid environments, for which the RIDE paper demonstrates superiority over a number of other intrinsic motivation methods, and hence they exclude these other methods in evaluations.
And this extension is mainly based on the idea from algorithmic fairness literature which considers the worst-case environments and solves a bi-level optimization. The paper combines ideas from two prior papers from the domain generalization and fairness literature: (i) invariant  risk minimization for OOD generalization (Arjovsky et al.) and (ii) adversarially reweighting for fairness without protected groups (Lahoti et al.). The paper develops its own algorithm EIIL which extends the Invariant Risk Minimization (IRM) of domain generalization to work in the situation when the prior knowledge of environments is not available. The authors explore cases where e is known or not and come up with some algorithms that draw connections between recent work on domain generalization, specifically invariant risk minimization (Arjovsky et al 2019) and fairness. The high-level idea is that similarly to the way that fair algorithm are able to improve the worst-case accuracy of predictors across different groups without knowing the sensitive attributes, perhaps we can use these ideas to domain generalization when environment partitions are not known to the algorithm. (2) Similarly, the paper does not provide any theoretical analysis of EIIL for algorithmic fairness. Strength: (1) The connection between domain generalization and algorithmic fairness shown by the paper is interesting.
"This also provides direct evidence that existing segmentation models could be particularly weak at certain real-world generalization, which is not surprising because the 1,464 training images are deemed to be extremely sparsely distributed in the space of natural images." I am not sure I agree with the evidence part. Annotating images for training of segmentation models is time consuming and it can be difficult to annotate enough examples to ensure good performance on the rare difficult examples that often occur when methods are applied to real world data. Leveraging those counterexamples to improve the segmentation models' generalization performance on unseen images seems to be novel in this field. Weakly-supervised labeling is more practical for segmentation; and (2) extending to active training/tuning, leveraging the selected hard examples to improve the segmentation model for multiple rounds. The authors identify that scaling human annotation, in particular for image segmentation, can be cost prohibitive and propose a method to optimize this process. Detailed comments "First, segmentation benchmarks require pixel-level dense annotation", I do not believe this is necessarily true, and there is little need to state this. "Specifically, given the target model ft, we let it compete with a group of state-of-the-art segmentation models {g_j}^m_{j=1} by maximizing the discrepancy (Wang et al., 2020) between f_t and g_j on D." They are not really competing are they? That is, they consist of examples that the proposed segmentation model disagrees with the "competing" models the most on. The paper therefore proposes a measure based on the discrepancy of a group of segmentation models to identify more valuable images to annotate and add to the training data in a iterative fashion.
To solve this problem, the authors present a deep learning approach that utilises a combination of (i) deep ensemble training, (ii) post hoc model selection, and (iii) importance weighted parameter estimation. To test that, the authors should carry out an ablation study by varying the size of the ensemble, varying the number of M top performing models, and by adding or removing the sample reweighting from step (iii). Positives Flexible approach: The approach is separated into two stages: (1) Training a deep ensemble on a regression task, and (2) maximum likelihood estimation of distribution parameters under the ensemble and an unseen test set.
arXiv preprint arXiv:1906.05274 (2019). The paper presents a pre-training scheme (APT) for RL with two components: contrastive representation learning and particle based entropy maximization. DETAILED COMMENTS C1) The exploration component of APT has striking similarities with the method in [1], which also seeks the optimization of a k-NN estimate of the state distribution entropy in a reward-free context. Weakness: Some concerns about experiment results: For DMControl results, it seems APT only improves significantly over "From scratch" and "Count-based pre-training" schemes when reward is sparse? Strength: Pre-training good representations and policy initialization without reward is obviously an important direction in RL. C2) The paper does not present an explicit empirical evaluation of the reward-free phase, thus I am not sure on how APT is performing in entropy maximization.
This paper proposes a neural network optimized by MLM loss that has inductive bias to be useful for unsupervised constituency and dependency parsing. From a CNN on the input sentence, the model predicts a syntactic "height" for every word in the sentence and a syntactic "distance" for every inter-word position; these heights and distances determine the joint constituency & dependency parse of the sentence. The model is trained for masked language modeling (MLM) and evaluated via MLM on held-out data and its ability to induce constituency and dependency trees. The model shows improvements in MLM perplexity over a standard Transformer baseline, and the authors present it as offering competitive unsupervised parsing scores relative to other models. SUMMARY The paper extends PRPN with the syntactic height in Luo et al. (2019) to do dependency parsing simultaneously with constituency parsing. The results are better than trivial baselines for unsupervised constituency and dependency parsing but not as strong as related recent work. The dependency parsing performance is far behind the current SOTA (neural DMVs), even when we account for the fact that it does not use the POS information. The model outperforms ON-LSTM in constituency parsing and is competitive with classical NLP methods that use gold POS tags in dependency parsing. Table 3 doesn't seem consistent with Table 1b: none of the dependency results in Table 3 match the 41.0 result for StructFormer listed in 1b. The new masking scheme that explicitly enforces MLM to be sensitive to dependency structure is interesting. However, there are a number of unclarities in the paper that make it difficult to determine whether the results on unsupervised parsing are actually comparable to prior work.
while the exploration step estimates the sampling distribution according to the sampled data in exploitation step and rectifies it to sample all data possibly. To address the issue of optimizing high-dimensional sampling hyper-parameter in data sampling and release the requirement of prior knowledge from current methods, the authors introduce a searching-based method named AutoSampling. This work presents an interesting exploration of learning optimal data sampling probability. Advantages: l    The exploitation step and exploration step in AutoSampling is interesting, it is straightforward that this method can work well as the sampling strategy is updated dynamically according to the current state of model. This work is taking a novel aspect for effectively training neural networks, since rare effort has been devoted to make data sampling learnable. First I will comment on the listed contributions: • To our best knowledge, we are the first to propose to directly learn a robust sampling schedule from the data themselves without any human prior or condition on the dataset. To this end, authors propose to record the data frequencies through all previous steps and thereby generate the sampling policy for the next step. The exploitation step train multi child models with current sampling strategy and save the best model for next iteration. • We propose the AutoSampling method to handle the optimization difficulty due to the high dimension of sampling schedules, and efficiently learn a robust sampling schedule through shortened reward collection cycle and online update of the sampling schedule. l    The transferability of the gained optimal sampling schedule is discussed in Section 4.4, a simple experiment is recommended. The chosen baselines are essentially standard sampling scheme or variants of the proposed method.
Summary The paper introduces two simple modules, SelfNorm (SN) and CrossNorm (CN), that are highly modular and can theoretically be attached to different parts of the CNNs to control the balance between style and content cues for their recognition. In particular, this paper proposes to recalibrate style using SelfNorm motivated by the fact that attention help emphasize essential styles and suppress trivial ones and reduce texture bias using CrossNorm by swapping feature maps within one instance. Selfnorm recalibrates style in features to reduce texture sensitivity. Crossnorm performs style augmentation (swap channel-wise means and variances) to reduce texture bias. The paper presents two new methods to improve corruption robustness and domain generalization: SelfNorm, a way to adapt style information during inference, and CrossNorm, a simple data augmentation technique diversifying image style in feature space. In general the motivation is very high level drawing a lot on the concepts of style, texture and content but the method itself is rather down to earth modifying instance normalization parameters. It is difficult to agree that SN and CN, which are argued to control style and content for the benefit of the recognition task at hand, are really working as speculated. The authors argue that SN then learns to emphasize important styles and suppresses less important ones, with the underlying assumption that the first and second-order statistics are often sufficient and necessary representations for style. - The authors explains SelfNorm recalibrate feature style while  CrossNorm performs style augmentation. I find it difficult to follow the rationale behind key assumptions (e.g. that shape = content and texture = style) and the experimental section is, I have to admit, a bit chaotic. The opposite two processes forms a unity of using style to advance model robustness.
The benchmark is a combination of existing data sets and newly created ones, and covers a variety of applications and tasks, from small molecules to RNA or protein structures, and including classification, regression and ranking tasks. I would really refrain from using "atomistic learning" to describe what the community has been referring to as "learning from 3D molecular representations" for decades. Actually, the abstract (and, more generally, the paper) reads as if neural networks were the only kind of machine learning algorithms that could be applied to molecules and that very little work has been done in the past to incorporate 3D information in chemoinformatics. Swamidass, S. J., et al. "Kernels for small molecules and the prediction of mutagenicity, toxicity and anti-cancer activity." Bioinformatics 21.suppl_1 (2005): i359-i368. By creating a standardized set of prediction tasks and associated data sets, the authors have presented a resource that may help the community to compare 3D atomistic methods quickly and fairly. Mahé, P., et al. "Graph kernels for molecular structure− activity relationship analysis with support vector machines." Journal of chemical information and modeling 45.4 (2005): 939-951. Bioinformatics, 35(3), 470-477. This paper presents a large benchmark of machine learning tasks for molecules represented by the 3D coordinates of their atoms. Determining the multiple possible conformations of drug-like molecules is still an ongoing research topic (see for example the review of Hawkins (2017)), not to mention the determination of the 3D structure of proteins, which is indeed the topic of one of the data sets provided. The idea of using atomistic learning or at least 3D derived features have already been implemented or at least contemplated in many of the presented tasks (Gilmer et al [1], Wu et al [2], Townshend et al. [3]). [2] Liu et al: N-Gram Graph: Simple Unsupervised Representation for Graphs, with Applications to Molecules Some tasks like Ligand Binding Affinity (LBA) and Ligand Efficacy Prediction (LEP) requires modeling representations of both proteins and small molecules. Axen, Seth D., et al. "A simple representation of three-dimensional molecular structure." Journal of medicinal chemistry 60.17 (2017): 7393-7409.
Overall: This paper examines many experiments to improve the optimization of sparse networks, but I am not convinced that the approaches are the most effective (how important is EGF and comparing sparse to same size dense). Equipped with the proposed methods EGF and SC-SDF, the paper then analyzes several (standard) approaches like batch normalization etc as to determine which of these approaches are useful for training sparse models.
Using camera to select most important radar regions contradicts the stated advantage of radars to perform better in adverse weather conditions While it is generally a great idea to guide the selection of radar regions to be sampled at a higher rate the paper is very application-focused and lacks novelty in its method. On page 3, "The radar data is split into 8 equal regions, in azimuth and 37 equal regions in range." Should the block size be 8 x 37? Summary: In the paper, the author(s) propose 1) a method to dynamically adjust the sampling rate on radar data using 2D object detections (algo1) and previous image and radar data (algo2); 2) an end-to-end transformer-based 2D object detection model using both radar and image data. 2019. [2] Nobis, Felix, et al. "A Deep Learning-based Radar and Camera Sensor Fusion Architecture for Object Detection." 2019 Sensor Data Fusion: Trends, Solutions, Applications (SDF). The paper proposes a sensor fusion approach combining radar and camera to improve the detection of object in an automotive sensing scenario.
For example: Regeneration of low level concepts from high level concepts: what are we expecting from a network that moves from a "high in the hierarchy" concept to a "low in the hierarchy" concept? The cons include (In my opinion, the main weakness is the experiments): Some design of model (e.g., Traversing across the concept hierarchy, Observation, and Instruction Regeneration, etc) are not fully estimated in this section: are they really useful? Introducing a two-level hierarchy into concept learning is also not new. Code is available. Weaknesses: The concept of hierarchy is not well defined or well motivated. The paper introduces the solution of an important task: hierarchical concept learning(or temporal abstractions) from demonstration data. This paper addresses the problem of extracting a hierarchy of concepts in an unsupervised way from demonstration data. Especially jointly learning concepts from frames and instructions while considering the hierarchy. Specifically, this paper considers 1) unsupervised setting; 2) the hierarchy of concepts, and conducts experiments in two datasets. The cooking video dataset does not have such annotations so hypothetically the one can set an arbitrary number for the levels of hierarchy. For example in Figure 5, the results without commentary (A) look more reasonable for z^H_1 to contain all the steps for a higher-level concept (say, pan heating) but the model with commentary (B) split the concepts mainly according to the language descriptions. The two-level hierarchy has been seen a lot in relevant fields such as video recognition. As the network is unsupervised, the starting time and ending time of each concept is quite crucial and the problem was tackled by training using a soft-DTW loss.
Contributions: This paper analyzes the convergence of decentralized SGD with asynchronous updates, quantization and local updates, which is novel and challenging. Summary: This paper combines the existing scaling techniques to reduce the communication cost of distributed SGD among a large number of computing nodes. (3) In Theorem 4.1, the assumption that T>=n^4 (n^4 can be very large) is the disadvantage of this algorithm because the same convergence rate O(1/sqrt(T)) has been achieved without such assumption in some distributed settings, including plain distributed SGD, federated average, etc. Three extensions of the algorithm are also proposed to relax different constraints, while maintaining the convergence guarantees: synchronous updates and decentralized data: if the number of local gradient updates Hi before an edge update is constant, convergence guarantees hold for decentralized data, as long as partitions are i.i.d. from the original distribution; Remarks on theoretical analysis Theorem 4.1 shows that the average second moment of the loss gradient evaluated at the average model μt is bounded and decreases with T, proving that the model updates converge to a local minimum. Summary The paper proposes and analyses a distributed learning algorithm for training with Stochastic Gradient Descent a global model on a regular graph, that allows for local and asynchronous gradient updates. asynchronous updates: the number of local gradient updates Hi can vary between nodes and between every edge update; Nodes continuously update their local models Xi by gradient descent, while they communicate with their peers (a peer at a time) and update their local model with the pair model average Xi+Xj2. define T (global number of edge updates) and H (number of local updates in between edge communication); These techniques include asynchronization, local updates, and quantized communication. (6) On page 2, the authors said "SwarmSGD has a Θ(n) speedup in the non-convex case, matching results from previous work which considered decentralized dynamics but which synchronize upon every SGD step." What is the measure, is it the number of communications, local SGD iterations or gradient evaluations? What does the effect of quantization on the convergence rate and the communication cost? (4) The claim in the abstract that the new algorithm can converge to local minima is not supported, since the theorems only imply gradient convergence. However, if the analysis can not explain why more local updates can reduce communications, I would not recommend to accept. For example, if we optimize the convergence rate in Theorem 4.1 over H, the best choice is H=(λ22r2⋅f(μ0)−f∗L2M2)1/3. Still, it would help to clarify the following points from the beginning: what the authors mean by decentralized, later explained as decentralized model updates, but centralized/distributed data for the experiments; The authors claim that this is the first work to consider decentralization, local updates, asynchrony, and quantization in conjunction. These techniques include asynchronous, decentralized, or quantized communication. The benefit of local steps is not clear. This paper considers several techniques to minimize decentralized cost for training neural network models via stochastic gradient descent (SGD).
The methodological contribution of the paper is more or less an off-the-shelf use of LSH for negative sampling. The experiments are very convincing and show that CPU C++ implementation that uses new sampling can outperform GPU-based approach based on uniform sampling.
Without further assumptions on the class of games, I do not really see why the base policy would be having a good expected reward before adaptation (either in average over a broad class of opponents or against the optimal opponent), or even less why it would be hard to exploit (especially after adaptation). Then, given a base policy and a "hard-to-exploit" opponent, more diverse opponents are sequentially generated (in a procedure coined diverse-OSG) by optimizing their expected reward against the base policy while maximizing their "diversity" with the "hard-to-exploit" opponent and the already generated diverse opponents. Because L2E's base model is pre-trained using opponents generated by OSG, a more fair comparison is to pre-train the TRPO (possibly based on the random opponents) and then fine-tune against a new opponent. In particular, the authors propose Opponent Strategy Generation (OSG) that produces effective opponents for training automatically through adversarial training for eliminating its own strategy's weaknesses and diversity-regularized policy optimization to improve the generalization ability of L2E. First, given a base policy, an "hard-to-exploit" opponent is generated adversarially (in a procedure coined hard-OSG), to minimize the the reward that the base policy would get by adapting to it (using the few updates that are allowed to it in its adaptation step). Adding an algorithm in the Appendix when adapting to a new opponent after the base policy training (i.e., the meta-testing procedure) will be helpful. In fact the empirical results suggest that the base policy is breaking even against a random opponent (before adaptation) at Leduc poker, which seems rather weak to me. To do this they use L2E which can generate exploiter and diverse opponent agents and updates the base policy based on trajectories from exploiter base policies.
Recommendation Because of the lack of motivation for adversarial meta-learning, lacking intuition for ADML, and particularly the robustness evaluation against a very weak attack, the paper is a clear reject to me. The idea of combining meta-learning with adversarial defense has already been proposed and well studies, such as in "Adversarial Attacks on Graph Neural Networks via Meta Learning, *CONF* 2019". Summarization of the contribution: This paper presents a novel ADversarial Meta-Learner (ADML), the claimed first work that tackles adversarial samples in meta-learning. Meta-Learning Approach. 2020. This paper presents ADML (ADversarial Meta-Learner), a method for general meta-learning when adversarial samples are present. The paper propose a new meta-learning algorithm ADML that uses adversarial and clean examples during meta-training (both for train-train and train-test). In particular, ADML formulates two task optimisation problems (Eq. 2) for the same variable (the initialisation) - one is adaptation under clean data and one under adversarial data.
Introducing MACAW: an algorithm for offline meta-RL that has the desirable property of being consistent (i.e. converges to a good policy if enough time and data for the meta-test task are given, regardless of meta-training). To do so they rely on MAML (which provides consistency) and AWR (a simple, popular offline RL algorithm) and add a couple of changes: some hyper-network like parameterization to add capacity and adding an extra objective in the policy update to enrich the inner loop. However, in the offline setting the policy that generated the batch of data is of critical importance and should be part of the task definition. The offline meta-RL formulation should include behavior policy as part of the task definition. It would still be great to add an experiment where MACAW is adapted online at test time (entirely without offline data) like in C.2 but on in-distribution tasks. They argue that a naive application of MAML with advantage weighted regression (a recently proposed approach to offline RL) is insufficient for this setting and propose to make the policy more expressive by including an advantage head that regresses the advantage conditioned on the state and action. In experiments they show that this outperforms the offline metaRL method PEARL, and combining multi-task offline RL with AWR in a naive way. It could be online with the meta-train tasks remaining offline, but then we would have to meta-learn an exploration policy, which isn't done in this work. The authors propose a gradient-based meta learning method (MACAW) to approach this problem, which uses an actor-critic method combined with advantage weighting which is an offline RL method. My comments: While this paper touches on a very interesting and practical problem in meta-rl and batch-rl, I didn't find their setting is very realistic with respect to batch and offline RL setup. A main motivation for the offline RL setting is that you can use real-world data and train a metaRL agent using this. In the experiments on 4 MuJoCo benchmarks the proposed method outperforms two baselines, Offline PEARL and Offline MT+FT. Within offline meta-RL the experiments focus on the fully offline case, where the meta-test task is also offline. Here the agent is trained on a fixed offline dataset, which distinguishes it from most metaRL algorithms that interact with the environment during meta training. The paper also proposes a method for the fully offline meta-RL problem based on the MAML method.
The empirical analysis is systematic and the two main results are thought provoking and interesting: 1) Different sources of nondeterminism (such as random initialization, data augmentation, data shuffling, etc.) causes similar levels of variability (based on standard deviation and correlation metrics), and Pros: Show that different sources of nondeterminism give similar level of variability effect on the final accuracy and loss. Some questions: given such robustness in the level of variability across different sources of nondeterminism, is it possible to predict the level of variability from the data, architecture, etc.? Is there anything more fundamental about the particular value of variability that all the different sources of nondeterminism concentrate around? The paper would be strengthened by having experiments on popular benchmark state-of-the-art networks (maybe fewer runs and not as many nondeterminism sources as in Table 1). The authors establish an experimental protocol to understand the effect of optimization nondeterminism on model diversity, and study the independent effect of different sources of nondeterminism. The authors establish an experimental strategy to analyze the different sources of nondeterminism.
[1] https://arxiv.org/abs/1901.10430 This paper introduces the Attention Free Transformer (AFT), an alternative to multi-head attention (MHA) operation in Transformer. This paper proposes an efficient transformer variant by replacing softmax in the self-attention layer with a RELU activation and arranging the computation using element-wise products and global/local pooling. What are the benefits of AFT compared to recent established efficient transformers such as Sparse Transformer (Child et al., 2019), Reformer (Kitaev et al., 2019), or Linear transformer (Katharopoulos et al., 2020)? Under what circumstances we should expect AFT to reach vanilla transformer performance and still offer clear efficiency benefits when using the same setup?
Since the method operated only within attention layers (reduces only dimensions of queries and keys), in terms of efficiency/quality trade-off it should be compared to other methods, e.g. simple head pruning. If you look a deeper look into Transformers, in case of BERT, the attention block only takes around 25% of total parameters. The paper investigates the over-parameterization of attention heads in Transformer's multi-head attention. They propose a reparameterization of multi-head attention allowing the parameters of queries and keys to be shared between heads: this is called "collaborative attention". This paper analyzes the multi-head attention in transformers and suggests to use collaboration instead of concatenation of multiple heads. Namely, while the original definition of attention layers does not have biases in linear projections for q/k/v in attention, the authors claim that implementations contain the bias terms, and spend some time showing how to model the biases in key and query layers properly. Moreover, for pre-trained language models, no large-scale pretraining is required to convert the attention. The authors show that query-key projections are redundant because trained concatenated heads tend to compute their attention patterns on common features. Additionally, the part with the biases in attention implementation is misleading. (minor) As a side contribution, the authors claim to report the discrepancy between the theory and implementation of attention layers. With simple head pruning, about 50% of the heads can be removed without sacrificing quality: in addition to queries and keys, this also halves values and the output projection matrix. ========================== Overall review This paper challenges the widely adopted multi-head attention, asking the question whether the concatenation of multiple heads is the best way to fuse heads. This attention can be applied either instead of the standard attention during training, or as a drop-in replacement for an already trained model.
There exists a prior work that bridges a gap between the two exploration methods for linear policies, and this paper generalizes the prior work for various deep RL methods: on-policy (A2C, PPO) and off-policy (SAC). Experiments show that the proposed exploration strategy mostly helps A2C, PPO and SAC in three Mujoco environments. Summary of the paper The basis of this work is van Hoof et al., 2017; there, "Generalized Exploration" views policy parameters as being drawn from a per-trajectory Markov chain. The authors propose to perturb only the last(linear) layer of the policy for exploration, instead of perturbing all layers of the policy network. This paper presents a method to combine step-based exploration with trajectory-based exploration (in the form of action-space noise and parameters-space noise) in continuous MDPs, which is scalable to deep RL methods. *In section 5, apart from the comparison of the performance of the learned policy, the comparison of the complexity (which might be measured by wall time to learn the policy?) of different exploration strategies can also be interesting. But for the three bullet points in section 1, the first point of "Generalizing Step-based and Trajectory-based Exploration" should not be one of the main contributions of this paper, because this paper follows the formulation of policy in van Hoof et al. (2017) and the latter proposed the generalized exploration connecting step-based and trajectory-based exploration. I would like to thank the authors of "Deep Coherent Exploration For Continuous Control" for their valuable and interesting submission.
Motivated by the fact that NF are diffeomorphic  transformations from a simple space, ideally Euclidean, the author address the problem of modeling data which distribution is defined on more complex and unknow manifolds.The idea consists in inflating the data manifold with suitable noise (normal noise) in order to make it diffeomorphic to a simpler space where a NF can be estimated. In practice, Proposition 1 shows that the noise can be drawn from a Gaussian with variance depending from the curvature of the manifold. The denoising auto-encoder is famous as a manifold learning method that adds pseudo noise. This paper argues that Gaussian noise e is a good approximation of en when the manifold dimension d is sufficiently smaller than the higher-dimensional Euclidean space dimension D.
Examples include, this sentence from the introduction (yes, it's one sentence): "We first mathematically prove that, by greatly shrinking the graph of the search space, reducing the operators' complexity in magnitude, lowering the required searching period from 50 epochs to one iteration and significantly easing the Matthew effect, namely that the complex operators may never get the chance to be well tuned, thus the found architecture only contains very simple operators, and performs poorly on the testing set, FTSO reduces the required parameters by a factor of 0.53×108, decreases the FLOPs per iteration by a factor of 2×105 and significantly promotes the accuracy compared to the baseline, PC-DARTS." Strengths: The search time is reduced significantly from normal DARTS and PC-DARTS. The choice of skip connection as the operator for topology search is not well-justified.
The second theorem shows that the surprise, multiplied by a constant that depends on the policy, is an upper bound to EVB in the soft RL setting. The first theorem shows that the surprise (aka the temporal difference error) is an upper bound of the EVB in the Q-learning setting. The authors demonstrate that the proposed tighter bound on the EVB could yield improvements in the soft RL setting. Unfortunately, they are not sufficient to help the understanding, as the upper bound derived is just the TD error used in the prioritized experience replay.
Summary: This paper studies the detection and recovery problem in spiked tensor models in the form T = \\beta v0^\\otimes k + Z, where v0 is the underlying spike signal and Z is a Gaussian noise. Theorem 7 claims that Algorithm 1 and 2 run in linear time. The paper presents a pair of interesting algorithms using trace invariants to detect the signal in the signal-plus-noise tensor PCA framework. The authors claim that: 1) they "build tractable algorithms with polynomial complexity", "a detection algorithm linear in time"; 2) the algorithms are very suitable for parallel architectures; 3) an improvement of the state of the art for the symmetric tensor PCA experimentally. ########################################################################## Pros: The paper takes an interesting question about tensor PCA and proposes a promising approach to solve it based on the trace invariants. Page 8: "eg" should be "e.g." Summary: The paper provides an interesting algorithm for tensor PCA, which is based on trace invariants. It will be very hard to follow the proofs if the definitions are unclear. The algorithms function by considering cutting an edge in the graph representation of the trace invariant, yielding a matrix whose leading eigenvector provides a (up to a rotation) estimate of the signal vector v. The problem consists of recovering a (single-spike/multiple orthogonal spikes) tensor corrupted by a Gaussian noise tensor.
It would be interesting to see how the clustering techniques compare when the context set includes more than two protected categories, there is initial strong data imbalance between those groups, and the "invisible demographic" has relatively few data examples in the context set. #Summary This paper studies zero-shot fairness where the demographic information is partially unavailable, but assuming the existence of a context dataset that contains all labels x all demographics (including the invisible). #Pros Zero-shot fairness is a very important topic under many practical settings, where the demographic information can be (partially) missing due to sampling bias or privacy reasons. This paper introduces the problem of enforcing group-based fairness for "invisible demographics," which they define to be demographic categories that are not present in the training dataset. This paper tackles a fair classification problem with an invisible demographic, a situation where the records who have some specific target labels and sensitive attributes are missing. Summary This paper proposed a problem in algorithmic fairness where labeled examples for some demographic groups are completely missing in the training dataset and still the goal is to make predictions that satisfy parity-based fairness constraints. In addition, how does the distribution of the label x demographics on the context dataset affect clustering quality? This algorithm involves first applying clustering methods on the context set to "balance" it, followed by disentangled representation learning and on the "balanced" context set. #Cons The biggest concern I have is the clustering part of the context set into a perfect set.
(1) "Specifically, the number of independent affine layers in the SaBN equals to the total number of candidate operation paths of the connected previous layer." Are you sure it doesn't equals to the total number of candidate operations in next layer? It does not appear that the approach imposes any restrictions or regularization on the CCBN affine parameters; therefore the only difference between CCBN and SaBN seems to be a different parameterization. (2) I think it will be better to add the SaBN in Fig3, and point out the correspondence between the candidate operations and the conditional affine layer. Does the author use SaBN to search the architecture, and use BN to fine-tune the searched model? (2)What is the difference between the searched architectures by using BN, CCBN and SaBN?
Comments/questions: I do not feel like the way the abstract and introduction argue for a general framework "Joint Perception and Control as Inference" is productive. (2020). Sophisticated Inference. arXiv preprint arXiv:2006.04120. This paper features two complementary contributions: The perception and control as inference (PCI) framework, which describes the graphical model of a POMDP with auxiliary optimality variables, allowing for the derivation of an objective for optimizing both a perception model and policy jointly to maximize log⁡p(O,x∣a). I could imagine that the joint objective would cause OPC to perform a bit better than world models at convergence, but I am surprised that the perception is apparently so difficult that a separate modeling objective never gets off the ground at all. The authors propose a framework for joint perception and control as inference (PCI) to combine perception and control for the case of POMDPs. One of the major contributions in the paper is a well worked-out joint inference derivation, which performs amortized inference in this shared model utilizing an RNN. Finally, the novel aspect of the control objective isn't actually used in practice, as a simple TD error method is used to train the policy, which others have done without taking a "RL as inference" stance. Experimentally, the authors verify their joint model on a waterworld environment, which consists of simple object shapes with semantics. It seems that this sort of approach would be favored only when the perception problem is too difficult for a standard maximum likelihood objective, as the joint objective would (presumably) bias the perception model to be useful for control. Their main proposal is denoted as OPC, which stands for object based perception and control, which serves the purpose of automatically discovering objects from pixels while controlling the system.
This conditional network approach is illustrated for two standard convolutional neural network (CNN) architectures, U-Net and VGG, on two benchmark datasets suitable for OOD detection, the Inria Aerial Image Labeling Dataset and the Tumor Infiltrating Lymphocytes classification dataset. Strengths: Significance / Novelty: The conditional network uses a latent / intermediate representation z of an auxiliary network solving an auxiliary task T (as usually considered in the context of self-supervised learning) to learn parameters used to perform an affine transformation of feature map activations in the main neural network solving the main task Y. In 4.1 CONDITIONAL NETWORKS FOR SEMANTIC SEGMENTATION OF AERIAL IMAGES: You state "The transfer set also has variation in illumination, landscape, and time, making it well-suited to evaluate out-of distribution generalization." Can you clarify how those variations were obtained? The main stated goal is the evaluation of how conditional networks improve generalization in the presence of distributional shift.
