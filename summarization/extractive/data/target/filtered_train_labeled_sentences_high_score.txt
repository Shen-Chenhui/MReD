2018-1	This paper analyzes a problem with the convergence of Adam, and presents a solution.	abstract
2018-1	It identifies an error in the convergence proof of Adam (which also applies to related methods such as RMSProp) and gives a simple example where it fails to converge.	abstract
2018-1	The paper then repairs the algorithm in a way that guarantees convergence without introducing much computational or memory overhead.	abstract
2018-1	There ought to be a lot of interest in this paper: Adam is a widely used algorithm, but sometimes underperforms SGD on certain problems, and this could be part of the explanation.	strength
2018-1	The fix is both principled and practical.	strength
2018-1	Overall, this is a strong paper, and I recommend acceptance.	decision

2018-2	The pros and cons of this paper cited by the reviewers can be summarized below: <sep> Pros: <sep> * The paper is a first attempt to investigate an under-studied area in neural MT (and potentially other applications of sequence-to-sequence models as well) <sep>	strength
2018-2	* This area might have a large impact; existing models such as Google Translate fail badly on the inputs described here <sep>	strength
2018-2	* Experiments are very carefully designed and thorough <sep>	strength
2018-2	* Experiments on not only synthetic but also natural noise add significant reliability to the results <sep>	strength
2018-2	* Paper is well-written and easy to follow <sep>	strength
2018-2	Cons: <sep> * There may be better architectures for this problem than the ones proposed here <sep>	weakness
2018-2	* Even the natural noise is not entirely natural, eg artificially constrained to exist within words <sep>	weakness
2018-2	* Paper is not a perfect fit to *CONF* (although *CONF* is attempting to cast a wide net, so this alone is not a critical criticism of the paper) <sep>	decision
2018-2	This paper had uniformly positive reviews and has potential for large real-world impact.	rating_summary

2018-3	As stated by reviewer 3 "This paper introduces a new model to perform image classification with limited computational resources at test time.	abstract
2018-3	The model is based on a multi-scale convolutional neural network similar to the neural fabric (Saxena and Verbeek 2016), but with dense connections (Huang et al, 2017) and with a classifier at each layer." <sep>	abstract
2018-3	As stated by reviewer 2 "My only major concern is the degree of technical novelty with respect to the original DenseNet paper of Huang et al (2017).".	weakness
2018-3	The authors assert novelty in the sense that they provide a solution to improve computational efficiency and focus on this aspect of the problem.	rebuttal_process
2018-3	Overall, the technical innovation is not huge, but I think this could be a very useful idea in practice.	strength

2018-6	This work introduces a trainable signal representation for spherical signals (functions defined in the sphere) which are rotationally equivariant by design, by extending CNNs to the corresponding group SO(3).	abstract
2018-6	The method is implemented efficiently using fast Fourier transforms on the sphere and illustrated with compelling tasks such as 3d shape recognition and molecular energy prediction. <sep>	abstract
2018-6	Reviewers agreed this is a solid, well-written paper, which demonstrates the usefulness of group invariance/equivariance beyond the standard Euclidean translation group in real-world scenarios.	strength
2018-6	It will be a great addition to the conference.	decision

2018-7	this submission presents a novel way in which a neural machine reader could be improved.	abstract
2018-7	that is, by learning to reformulate a question specifically for the downstream machine reader.	abstract
2018-7	all the reviewers found it positive, and so do i.	rating_summary

2018-8	The reviewers unanimously recommended that this paper be accepted, as it contains an important theoretical result that there are problems for which heavy-ball momentum cannot outperform SGD.	rating_summary
2018-8	The theory is backed up by solid experimental results, and the writing is clear.	strength
2018-8	While the reviewers were originally concerned that the paper was missing a discussion of some related algorithms (ASVRG and ASDCA) that were handled in discussion.	weakness

2018-9	This paper attracted strong praise from the reviewers, who felt that it was of high quality and originality.	strength
2018-9	The broad problem that is being tackled is clearly of great importance. <sep>	strength
2018-9	This paper also attracted the attention of outside experts, who were more skeptical of the claims made by the paper.	weakness
2018-9	The technical merits do not seem to be in question, but rather, their interpretation/application.	weakness
2018-9	The perception by a community as to whether an important problem has been essentially solved can affect the choices made by other reviewers when they decide what work to pursue themselves, evaluate grants, etc.	suggestion
2018-9	It's important that claims be conservative and highlight the ways in which the present work does not fully address the broader problem of adversarial examples. <sep>	weakness
2018-9	Ultimately, it has been decided that the paper will be of great interest to the community.	strength
2018-9	The authors have also been entrusted with the responsibility to consider the issues raised by the outside expert (and then echoed by the AC) in their final revisions. <sep>	suggestion
2018-9	One final note: In their responses to the outside expert, the authors several times remark that the guarantees made in the paper are, in form, no different from standard learning-theoretic claims: "This criticism, however, applies to many learning-theoretic results (including those applied in deep learning)."	rebuttal_process
2018-9	I don't find any comfort in this statement.	weakness
2018-9	Learning theorists have often focused on the form of the bounds (sqrt(m) dependence and, say, independence from the # of weights) and then they resort to empirical observations of correlation to demonstrate that the value of the bound is predictive for generalization.	rebuttal_process
2018-9	because the bounds are often meaningless ("vacuous") when evaluated on real data sets.	rebuttal_process
2018-9	(There are some recent examples bucking this trend.)	rebuttal_process
2018-9	In a sense, learning theorists have gotten off easy.	rebuttal_process
2018-9	Adversarial examples, however, concern security, and so there is more at stake.	rebuttal_process
2018-9	The slack we might afford learning theorists is not appropriate in this new context.	rebuttal_process
2018-9	I would encourage the authors to clearly explain any remaining work that needs to be done to move from "good enough for learning theory" to "good enough for security".	suggestion
2018-9	The authors promise to outline important future work / open problems for the community.	rebuttal_process
2018-9	I definitely encourage this.	misc

2018-10	The reviewers are unanimous in finding the work in this paper highly novel and significant.	strength
2018-10	They have provided detailed discussions to back up this assessment.	misc
2018-10	The reviewer comments surprisingly included a critique that  "the scientific content of the work has critical conceptual flaws" (!)	weakness
2018-10	However, the author rebuttal persuaded the reviewers that the concerns were largely addressed.	rebuttal_process

2018-11	This paper proposes a new generative model that has the stability of variational autoencoders (VAE) while producing better samples.	abstract
2018-11	The authors clearly compare their work to previous efforts that combine VAEs and Generative Adversarial Networks with similar goals.	abstract
2018-11	Authors show that the proposed algorithm is a generalization of Adversarial Autoencoder (AAE) and minimizes Wasserstein distance between model and target distribution.	abstract
2018-11	The paper is well written with convincing results.	strength
2018-11	Reviewers agree that the algorithm is novel and practical; and close connections of the algorithm to related approaches are clearly discussed with useful insights.	strength
2018-11	Overall, the paper is strong and I recommend acceptance.	decision

2018-13	There was some debate between the authors and an anonymous commentator on this paper.	rebuttal_process
2018-13	The feeling of the commentator was that existing work (mostly from the PL community) was not compared to appropriately and, in fact, performs better than this approach.	rebuttal_process
2018-13	The authors point out that their evaluation is hard to compare directly but that they disagreed with the assessment.	rebuttal_process
2018-13	They modified their texts to accommodate some of the commentator's concerns; agreed to disagree on others; and promised a fuller comparison to other work in the future. <sep>	rebuttal_process
2018-13	I largely agree with the authors here and think this is a good and worthwhile paper for its approach. <sep>	strength
2018-13	PROS: <sep> 1. well written <sep>	strength
2018-13	2. good ablation study <sep>	strength
2018-13	3. good evaluation including real bugs identified in real software projects <sep>	strength
2018-13	4. practical for real world usage <sep>	strength
2018-13	CONS: <sep> 1. perhaps not well compared to existing PL literature or on existing datasets from that community <sep>	weakness
2018-13	2. the architecture (GGNN) is not a novel contribution	weakness

2018-14	The paper characterizes the latent space of adversarial examples and introduces the concept of local intrinsic dimenstionality (LID).	abstract
2018-14	LID  can be used to detect adversaries as well build better attacks as it characterizes the space in which DNNs might be vulnerable.	abstract
2018-14	The experiments strongly support their claim.	abstract

2018-15	Viewing language modeling as a matrix factorization problem, the authors argue that the low rank of word embeddings used by such models limits their expressivity and show that replacing the softmax in such models with a mixture of softmaxes provides an effective way of overcoming this bottleneck.	abstract
2018-15	This is an interesting and well-executed paper that provides potentially important insight.	strength
2018-15	It would be good to at least mention prior work related to the language modeling as matrix factorization perspective (eg Levy & Goldberg, 2014).	suggestion

2018-16	Looks like a great contribution to *CONF*.	strength
2018-16	Continuous adaptation in nonstationary (and competitive) environments is something that an intelligent agent acting in the real world would need to solve and this paper suggests that a meta-learning approach may be quite appropriate for this task.	abstract

2018-17	This paper proposes improvements to WaveNet by showing that increasing connectivity provides superior models to increasing network size.	abstract
2018-17	The reviewers found both the mathematical treatment of the topic and the experiments to be of higher quality that most papers they reviewed, and were unanimous in recommending it for acceptance in the conference.	rating_summary
2018-17	I see no reason not to give it my strongest recommendation as well.	decision

2018-18	This paper presents a novel and interesting sketch-based approach to conditional program generation.	abstract
2018-18	I will say upfront that it is worth of acceptance, based on its contribution and the positivity of the reviews.	decision
2018-18	I am annoyed to see that the review process has not called out the authors' lack of references to the decently body of existing work on generating structure on neural sketch programming and on generating under grammatical constraint.	weakness
2018-18	The authors' will need look no further than the proceedings of the *ACL conferences of the last few years to find papers such as: <sep> * Dyer, Chris, et al "Recurrent Neural Network Grammars."	suggestion
2018-18	Proceedings of NAACL-HLT (2016). <sep>	misc
2018-18	* Kuncoro, Adhiguna, et al "What Do Recurrent Neural Network Grammars Learn About Syntax?"	misc
2018-18	Proceedings of EACL (2016). <sep>	misc
2018-18	* Yin, Pengcheng, and Graham Neubig.	misc
2018-18	"A Syntactic Neural Model for General-Purpose Code Generation."	misc
2018-18	Proceedings of ACL (2017). <sep>	misc
2018-18	* Rabinovich, Maxim, Mitchell Stern, and Dan Klein.	misc
2018-18	"Abstract Syntax Networks for Code Generation and Semantic Parsing."	misc
2018-18	Proceedings of ACL (2017). <sep>	misc
2018-18	Or other work on neural program synthesis, with sketch based methods: <sep> * Gaunt, Alexander L., et al "Terpret: A probabilistic programming language for program induction."	suggestion
2018-18	arXiv preprint arXiv:1608.04428 (2016). <sep>	misc
2018-18	* Riedel, Sebastian, Matko Bosnjak, and Tim Rocktäschel.	misc
2018-18	"Programming with a differentiable forth interpreter."	misc
2018-18	CoRR, abs/1605.06640 (2016). <sep>	misc
2018-18	Likewise the references to the non-neural program synthesis and induction literature are thin, and the work is poorly situated as a result. <sep>	weakness
2018-18	It is a disappointing but mild failure of the scientific process underlying peer review for this conference that such comments were not made.	misc
2018-18	The authors are encouraged to take heed of these comments in preparing their final revision, but I will not object to the acceptance of the paper on these grounds, as the methods proposed therein are truly interesting and exciting.	suggestion

2018-21	The authors have proposed a method for imitating a given control trajectory even if it is sparsely sampled.	abstract
2018-21	The method relies on a parametrized skill function and uses a triplet loss for learning a stopping metric and for a dynamics consistency loss.	abstract
2018-21	The method is demonstrated with real robots on a navigation task and a knot-tying task.	abstract
2018-21	The reviewers agree that it is a novel and interesting alternative to pure RL which should inspire good discussion at the conference.	strength

2018-23	All three reviewers were positive about the paper, finding it to be on an interesting topic and with broad applicability.	rating_summary
2018-23	The results were compelling and thus the paper is accepted.	decision

2018-24	This paper presents a nice approach to domain adaptation that improves empirically upon previous work, while also simplifying tuning and learning.	abstract

2018-28	This work presents a RNN tailored to generate sketch drawings.	abstract
2018-28	The model has novel elements and advances specific to the considered task, and allows for free generation as well as generation with (partial) input.	abstract
2018-28	The results are very satisfactory.	strength
2018-28	Importantly, as part of this work a large dataset of sketch drawings is released.	strength
2018-28	The only negative aspect is the insufficient evaluation, as pointed out by R1 who points out the need for baselines and evaluation metrics.	weakness
2018-28	R1's concerns have been acknowledged by the authors but not really addressed in the revision.	rebuttal_process
2018-28	Still, this is a very interesting contribution.	strength

2018-29	In this paper the authors show how to allow deep neural network training on logged contextual bandit feedback.	abstract
2018-29	The newly introduced framework comprises a new kind of output layer and an associated training procedure.	abstract
2018-29	This is a solid piece of work and a significant contribution to the literature, opening up the way for applications of deep neural networks when losses based on manual feedback and labels is not possible.	strength

2018-30	This paper with the self-explanatory title was well received by the reviewers and, additionally, comes with available code.	rating_summary
2018-30	The paper builds on prior work (Sinkhorn operator) but shows additional, significant amount of work to enable its application and inference in neural networks.	strength
2018-30	There were no major criticisms by the reviewers, other than obvious directions for improvement which should have been already incorporated in the paper, issues with clarity and a little more experimentation.	weakness
2018-30	To some extent, the authors addressed the issues in the revised version.	rebuttal_process

2018-31	This is a paper introducing a hierarchical RL method which incorporates the learning of a latent space, which enables the sharing of learned skills. <sep>	abstract
2018-31	The reviewers unanimously rate this as a good paper.	rating_summary
2018-31	They suggest that it can be further improved by demonstrating the effectiveness through more experiments, especially since this is a rather generic framework.	suggestion
2018-31	To some extent, the authors have addressed this concern in the rebuttal.	rebuttal_process

2018-39	This paper introduces an algorithm for optimization of discrete hyperparameters based on compressed sensing, and compares against standard gradient-free optimization approaches. <sep>	abstract
2018-39	As the reviewers point out, the provable guarantees (as is usually the case) don't quite make it to the main results section, but are still refreshing to see in hyperparameter optimization. <sep>	strength
2018-39	The method itself is relatively simple compared to full-featured Bayesopt (spearmint), although not as widely applicable.	strength

2018-45	The paper proposes a new deep architecture based on polar transformation for improving rotational invariance.	abstract
2018-45	The proposed method is interesting and the experimental results strong classification performance on small/medium-scale datasets (eg, rotated MNIST and its variants with added translations and clutters, ModelNet40, etc.).	strength
2018-45	It will be more impressive and impactful if the proposed method can bring performance improvement on large-scale, real datasets with potentially cluttered scenes (eg, Imagenet, Pascal VOC, MS-COCO, etc.).	suggestion

2018-46	The reviewers agree that the method is simple, the results are quite good, and the paper is well written.	strength
2018-46	The issues the reviewers brought up have been adequately addressed.	rebuttal_process
2018-46	There is a slight concern about novelty, however the approach will likely be quite useful in practice.	strength

2018-50	This paper presents a learned inference architecture which generalizes HMC.	abstract
2018-50	It defines a parameterized family of MCMC transition operators which share the volume preserving structure of HMC updates, which allows the acceptance ratio to be computed efficiently.	abstract
2018-50	Experiments show that the learned operators are able to mix significantly faster on some simple toy examples, and evidence is presented that it can improve posterior inference for a deep latent variable model.	abstract
2018-50	This paper has not quite demonstrated usefulness of the method, but it is still a good proof of concept for adaptive extensions of HMC.	strength

2018-55	This paper clearly surveys a set of methods related to using generative models to produce samples with desired characteristics.	abstract
2018-55	It explores several approaches and extensions to the standard recipe to try to address some weaknesses.	abstract
2018-55	It also demonstrates a wide variety of tasks.	abstract
2018-55	The exposition and figures are well-done.	strength

2018-56	This paper makes progress on the open problem of text generation with GANs, by a sensible combination of novel approaches.	abstract
2018-56	The method was described clearly, and is somewhat original.	strength
2018-56	The only problem is the hand-engineering of the masking setup.	weakness

2018-67	This paper presents a nice set of results on a new RL algorithm.	abstract
2018-67	The main downside is the limitation to the Atari domain, but otherwise the ablation studies are nice and the results are strong.	strength

2018-75	An interesting paper, generally well-written.	strength
2018-75	Though it would be nice to see that the methods and observations generalize to other datasets, it is probably too much to ask as datasets with required properties do not seem to exist.	suggestion
2018-75	There is a clear consensus to accept the paper. <sep>	rating_summary
2018-75	+ an interesting extension of previous work on emergent communications (eg, referential games) <sep>	strength
2018-75	+ well written paper	strength

2018-76	Graph neural networks (incl.	abstract
2018-76	GCNs) have been shown effective on a large range of tasks.	abstract
2018-76	However, it has been so far hard (ie computationally expensive or requiring the use of heuristics) to apply them to large graphs.	abstract
2018-76	This paper aims to address this problem and the solution is clean and elegant.	strength
2018-76	The reviewers generally find it well written and interesting.	strength
2018-76	There were some concerns about the comparison to GraphSAGE (an alternative approach), but these have been addressed in a subsequent revision. <sep>	rebuttal_process
2018-76	+ an important problem <sep>	strength
2018-76	+ a simple approach <sep>	strength
2018-76	+ convincing results <sep>	strength
2018-76	+ clear and well written	strength

2018-83	The submission proposes a loss surrogate for top-k classification, as in the official imagenet evaluation.	abstract
2018-83	The approach is well motivated, and the paper is very well organized with thorough technical proofs in the appendix, and a well presented main text.	strength
2018-83	The main results are: 1) a theoretically motivated surrogate, 2) that gives up to a couple percent improvement over cross-entropy loss in the presence of label noise or smaller datasets. <sep>	abstract
2018-83	It is a bit disappointing that performance is limited in the ideal case and that it does not more gracefully degrade to epsilon better than cross entropy loss.	weakness
2018-83	Rather, it seems to give performance epsilon worse than cross-entropy loss in an ideal case with clean labels and lots of data.	weakness
2018-83	Nevertheless, it is a step in the right direction for optimizing the error measure to be used during evaluation.	strength
2018-83	The reviewers uniformly recommended acceptance.	rating_summary

2018-84	The submission proposes optimization with hard-threshold activations.	abstract
2018-84	This setting can lead to compressed networks, and is therefore an interesting setting if learning can be achieved feasibly.	abstract
2018-84	This leads to a combinatorial optimization problem due to the non-differentiability of the non-linearity.	abstract
2018-84	The submission proceeds to analyze the resulting problem and propose an algorithm for its optimization. <sep>	abstract
2018-84	Results show slight improvement over a recent variant of straight-through estimation (Hinton 2012, Bengio et al 2013), called saturated straight-through estimation (Hubara et al, 2016).	abstract
2018-84	Although the improvements are somewhat modest, the submission is interesting for its framing of an important problem and improvement over a popular setting.	strength

2018-86	The paper proposes to regularize via a family of structured sparsity norms on the weights of a deep network.	abstract
2018-86	A proximal algorithm is employed for optimization, and results are shown on synthetic data, MNIST, and CIFAR10. <sep>	abstract
2018-86	Pros: the regularization scheme is reasonably general, the optimization is principled, the presentation is reasonable, and all three reviewers recommend acceptance. <sep>	rating_summary
2018-86	Cons: the regularization is conceptually not terribly different from other kinds of regularization proposed in the literature.	weakness
2018-86	The experiments are limited to quite simple data sets.	weakness

2018-90	Thank you for submitting you paper to *CONF*.	misc
2018-90	The reviewers and authors have engaged well and the revision has improved the paper.	rebuttal_process
2018-90	The reviewers are all in agreement that the paper substantially expands the prior work in this area,  eg by Balle et al (2016, 2017), and is therefore suitable for publication.	rating_summary
2018-90	Although I understand that the authors have not optimised their compression method for runtime yet, a comment about this prospect in the main text would be a sensible addition.	suggestion

2018-94	This paper gives a scalable Laplace approximation which makes use of recently proposed Kronecker-factored approximations to the Gauss-Newton matrix.	abstract
2018-94	The approach seems sound and useful.	strength
2018-94	While it is a rather natural extension of existing methods, it is well executed, and the ideas seem worth putting out there.	strength

2018-96	Meta score: 7 <sep>	decision
2018-96	The paper combined low precision computation with different approaches to teacher-student knowledge distillation.	abstract
2018-96	The experimentation is good, with good experimental analysis.	strength
2018-96	Very clearly written.	strength
2018-96	The main contribution is in the different forms of teacher-student training combined with low precision. <sep>	abstract
2018-96	Pros: <sep> - good practical contribution <sep>	strength
2018-96	- good experiments <sep>	strength
2018-96	- good analysis <sep>	strength
2018-96	- well written <sep>	strength
2018-96	Cons: <sep> - limited originality	weakness

2018-97	meta score: 8 <sep>	decision
2018-97	The paper present a distributed architecture using prioritized experience replay for deep reinforcement learning.	abstract
2018-97	It is well-written and the experimentation is extremely strong.	strength
2018-97	The main issue is the originality - technically, it extends previous work in a limited way;  the main contribution is practical, and this is validated by the experiments.	weakness
2018-97	The experimental support is such that the paper has meaningful conclusions and will surely be of interest to people working in the field.	strength
2018-97	Thus I would say it is comfortably over the acceptance threshold. <sep>	decision
2018-97	Pros: <sep> - good motivation and literature review <sep>	strength
2018-97	- strong experimentation <sep>	strength
2018-97	- well-written and clearly presented <sep>	strength
2018-97	- details in the appendix are very helpful <sep>	strength
2018-97	Cons: <sep> - possibly limited originality in terms of modelling advances	weakness

2018-98	meta score: 8 <sep>	decision
2018-98	This is a good paper which augments the data by mixing sound classes, and then learns the  mixing ratio.	abstract
2018-98	Experiments performed on a number of sound classification results <sep>	abstract
2018-98	Pros <sep> - novel approach, clearly explained <sep>	strength
2018-98	- very good set of experimentation with excellent results <sep>	strength
2018-98	- good approach to mixing using perceptual criteria <sep>	strength
2018-98	Cons <sep> - discussion doesn't really generalise beyond sound recognition	weakness

2018-102	This paper uses known methods for learning a differentially private models and applies it to the task of learning a language model, and find they are able to maintain accuracy results on large datasets.	abstract
2018-102	Reviewers found the method convincing and original saying it was "interesting and very important to the machine learning ... community", and that in terms of results it was a "very strong empirical paper, with experiments comparable to industrial scale".	strength
2018-102	There were some complaints as to the clarity of the work, with requests for more clear explanations of the methods used.	weakness

2018-103	This paper provides a game-based interface to have Turkers compete to analyze data for a learning task over multiple rounds.	abstract
2018-103	Reviewers found the work interesting and clear written, saying "the paper is easy to follow and the evaluation is meaningful."	strength
2018-103	They also note that there is clear empirical benefit "the results seem to suggest that MTD provides an improvement over non-HITL methods."	strength
2018-103	They also like the task compared to synthetic grounding experiments.	strength
2018-103	There was some concern about the methodology of the experiments but the authors provide reasonable explanations and clarification. <sep>	rebuttal_process
2018-103	One final concern that I hope the readers take into account.	misc
2018-103	While the reviewers were convinced by the work and did not require it, I feel like the work does not engage enough with the literature of crowd-sourcing in other disciplines.	weakness
2018-103	While there are likely some unique aspects to ML use of crowdsourcing, there are many papers about encouraging crowd-workers to produce more useful data.	weakness

2018-105	This work presents some of the first results on unsupervised neural machine translation.	abstract
2018-105	The group of reviewers is highly knowledgeable in machine translation, and they were generally very impressed by the results and the think it warrants a whole new area of research noting "the fact that this is possible at all is remarkable.".	strength
2018-105	There were some concerns with the clarity of the details presented and how it might be reproduced, but it seems like much of this was cleared up in the discussion.	rebuttal_process
2018-105	The reviewers generally praise the thoroughness of the method, the experimental clarity, and use of ablations.	strength
2018-105	One reviewer was less impressed, and felt more comparison should be done.	weakness

2018-106	This work extends upon recent ideas to build a complete summarization system using clever attention, copying, and RL training.	abstract
2018-106	Reviewers like the work but have some criticisms.	misc
2018-106	Particularly in terms of its originality and potential significance  noting "It is a good incremental research, but the downside of this paper is lack of innovations since most of the methods proposed in this paper are not new to us.".	weakness
2018-106	Still reviewers note the experimental results are of high quality performing excellent on several datasets and building "a strong summarization model."	strength
2018-106	Furthermore the model is extensively tested including in "human readability and relevance assessments ".	strength
2018-106	The work itself is well written and clear.	strength

2018-107	This paper proposes an offline neural method using concrete/gumbel for learning a sparse codebook for use in NLP tasks such as sentiment analysis and MT.	abstract
2018-107	The method outperforms other methods using pruning and other sparse coding methods, and also produces somewhat interpretable codes.	abstract
2018-107	Reviewers found the paper to be simple, clear, and effective.	strength
2018-107	There was particular praise for the strength of the results and the practicality of application.	strength
2018-107	There were some issues, such as only being applicable to input layers, and not being able to be applied end-to-end.	weakness
2018-107	The author also did a very admirable job of responding to questions about analysis with clear and comprehensive additional experiments.	rebuttal_process

2018-118	The reviewers agree that the proposed method is theoretically interesting, but disagree on whether it has been properly experimentally validated.	strength
2018-118	My view is that the the theoretical contribution is interesting enough to warrant inclusion in the conference, and so I will err on the side of accepting.	decision

2018-132	Important problem (modular continual RL) and novel contributions.	strength
2018-132	The initial submission was judged to be a little dense and hard to read, but the authors have been responsive in responding and updating the paper.	rebuttal_process
2018-132	I support accepting this paper.	decision

2018-133	Simple idea (which is a positive) to regularize RNNs, broad applicability, well-written paper.	strength
2018-133	Initially, there were concerns about  comparisons, but he authors have provided additional experiments that have made the paper stronger.	rebuttal_process

2018-142	This paper constructs a variant of deep CNNs which is provably invertible, by replacing spatial pooling with multiple shifted spatial downsampling, and capitalizing on residual layers to define a simple, invertible representation.	abstract
2018-142	The authors show that the resulting representation is equally effective at large-scale object classification, opening up a number of interesting questions. <sep>	abstract
2018-142	Reviewers agreed this is an strong contribution, despite some comments about the significance of the result; ie, why is invertibility a "surprising" property for learnability, in the sense that F(x) = {x,  phi(x)}, where phi is a standard CNN satisfies both properties: invertible and linear measurements of F producing good classification.	rating_summary
2018-142	All in all, this will be a great contribution to the conference.	decision

2018-144	This paper implements Group convolutions on inputs defined over hexagonal lattices instead of square lattices, using the roto-translation group.	abstract
2018-144	The internal symmetries of the hexagonal grid allow for a larger discrete rotation group than when using square pixels, leading to improved performance on CIFAR and aerial datasets. <sep>	abstract
2018-144	The paper is well-written and the reviewers were positive about its results.	strength
2018-144	That said, the AC wonders what is the main contribution of this work relative to existing related works (such as Group Equivarant CNNS, Cohen & Welling'16, or steerable CNNs, Cohen & Welling'17).	weakness
2018-144	While it is true that extending GCNNs to hexagonal lattices is a non-trivial implementation task, the contribution lacks significance in the mathematical/learning fronts, which are perhaps the ones *CONF* audience will care more about.	weakness
2018-144	Besides, the numerical results, while improved versus their square lattice counterparts, are not a major improvement over the state-of-the-art. <sep>	weakness
2018-144	In summary, the AC believes this is a borderline paper.	misc
2018-144	The unanimous favorable reviews tilt the decision towards acceptance.	decision

2018-148	this submission proposes a novel extension of existing recurrent networks that focus on capturing long-term dependencies via tracking entities/their statesand tested it on a new task.	abstract
2018-148	there's a concern that the proposed approach is heavily engineered toward the proposed task and may not be applicable to other tasks, which i fully agree with.	weakness
2018-148	i however find the proposed approach and the authors' justification to be thorough enough, and for now, recommend it to be accepted.	decision

2018-151	this submission proposes a learning algorithm for resnets based on their interpreration of them as a discrete approximation to a continuous-time dynamical system.	abstract
2018-151	all the reviewers have found the submission to be clearly written, well motivated and have proposed an interesting and effective learning algorithm for resnets.	strength

2018-157	The paper presents a differentiable upper bound on the performance of classifier on an adversarially perturbed example (with small perturbation in the L-infinity sense).	abstract
2018-157	The paper presents novel ideas, is well-written, and appears technically sound.	strength
2018-157	It will likely be of interest to the *CONF* community. <sep>	decision
2018-157	The only downside of the paper is its limited empirical evaluation: there is evidence suggesting that defenses against adversarial examples that work well on MNIST/CIFAR do not necessarily transfer well to much higher-dimensional datasets, for instance, ImageNet.	weakness
2018-157	The paper could, therefore, would benefit from empirical evaluations of the defenses on a dataset like ImageNet.	suggestion

2018-172	The paper got mixed scores of 4 (R1), 6 (R3), 8 (R2).	rating_summary
2018-172	R1 initially gave up after a few pages of reading, due to clarity problems.	rebuttal_process
2018-172	But looking over the revised version was much happier, so raised their score to 7.	rebuttal_process
2018-172	R2, who is knowledge about the area, was very positive about the paper, feeling it is a very interesting idea.	rating_summary
2018-172	R3 was also cautiously positive.	rating_summary
2018-172	The authors have absorbed the comments by the reviewers to make significant changes to the paper.	rebuttal_process
2018-172	The AC feels the idea is interesting, even if the experimental results aren't that compelling, so feels the paper can be accepted.	decision

2018-178	All three reviewers recommend acceptance.	rating_summary
2018-178	The authors did a good job at the rebuttal which swayed the first reviewer to increase the final rating.	rebuttal_process
2018-178	This is a clear accept.	decision

2018-185	This work shows how activation patterns of units reminiscent of grid and border cells emerge in RNNs trained on navigation tasks.	abstract
2018-185	While the *CONF* audience is not mainly focused on neuroscience, the findings of the paper are quite intriguing, and grid cells are sufficiently well-known and "mainstream" that this may interest many people.	strength

2018-187	All reviewers agree that the proposed method is novel and experiments do a good job in establishing its value for few-shot learning.	strength
2018-187	Most the concerns raised by the reviewers on experimental protocols have been addressed in the author response and revised version.	rebuttal_process

2018-193	Pros <sep> -- A novel formulation for cross-task and cross-domain transfer learning. <sep>	strength
2018-193	-- Extensive evaluations. <sep>	strength
2018-193	Cons <sep> -- Presentation a bit confusing, please improve. <sep>	weakness
2018-193	The paper received positive reviews from reviewers.	rating_summary
2018-193	But the reviewers pointed out some issues with presentation and flow of the paper.	weakness
2018-193	Even though the revised version has improved, the AC feels that it can be improved further.	rebuttal_process
2018-193	For example, as pointed out by reviewers, different parts of the model are trained using different losses and / or are pre-trained.	rebuttal_process
2018-193	It would be worth clarifying that.	suggestion
2018-193	It might help if the authors include a pseudocode / algorithm block to the final version of the paper.	suggestion

2018-195	pros: <sep> * novel explanation: skip connections <--> singualrities <sep>	strength
2018-195	* thorough analysis <sep>	strength
2018-195	* significant topic in understanding deep nets <sep>	strength
2018-195	cons: <sep> * more rigorous theoretical analysis would be better <sep>	weakness
2018-195	overall, the committee feels this paper would be interesting to have at *CONF*.	strength

2018-196	The proposed Bi-BloSAN is a two-levels' block SAN, which has both parallelization efficiency and memory efficiency.	abstract
2018-196	The study is thoroughly conducted and well presented.	strength

2018-197	The proposed routing networks using RL to automatically learn the optimal network architecture is very interesting.	strength
2018-197	Solid experimental justification and comparisons.	strength
2018-197	The authors also addressed reviewers' concerns on presentation clarity in revisions.	rebuttal_process

2018-211	This is a high quality paper, clearly written, highly original, and clearly significant.	strength
2018-211	The paper gives a complete analysis of SGD in a two layer network where the second layer does not undergo training and the data are linearly separable.	abstract
2018-211	Experimental results confirm the theoretical suggestion that the second layer can be trained provided the weights don't change sign and remain bounded.	abstract
2018-211	The authors address the major concerns of the reviewers (namely, whether these results are indicative given the assumptions).	rebuttal_process
2018-211	This line of work seems very promising.	strength

2018-212	This is a strong paper presenting a very clean proof of a result that is similar, though now incomparable to one due to Bartlett et al These bounds (and Bartlett's) are among the most promising norm-based bounds for NNs. <sep>	strength
2018-212	I would simply add that the citation of Dziugaite and Roy (2017) could be improved.	weakness
2018-212	There work also connects sharpness (or flatness) with generalization via the PAC-Bayes framework, and moreover, there bounds are nonvacuous.	weakness
2018-212	Are the bounds in this paper nonvacuous, say, on MNIST for 60,000 training data, for the network learned by SGD?	weakness
2018-212	If not, how close do they get to 1.0?	weakness

2018-213	The paper contributes to a body of empirical work towards understanding generalization in deep learning.	abstract
2018-213	They do this  through a battery of experiments studying "single directions" or selectivity of small groups of neurons.	abstract
2018-213	The reviewers that have actively participated agree that the revision is of high quality, impact, originality, and significance.	strength
2018-213	The issue of a lack of prescriptiveness was raised by one reviewer.	weakness
2018-213	I agree with the majority that this is not necessary, but nevertheless, the revision makes some suggestions.	ac_disagreement
2018-213	I urge the authors to express the appropriate amount of uncertainty regarding any prescriptions that have not been as thoroughly vetted!	suggestion

2018-221	This is an interesting paper that provides modeling improvements over several strong baselines and presents SOTA on Squad.	abstract
2018-221	One criticism of the paper is that it evaluates only on Squad, which is somewhat of an artificial task, but we think for publication purposes at *CONF*, the paper has a reasonable set of components.	ac_disagreement

2018-237	This paper proposes an interesting machinery around Generative Adversarial Networks to enable sampling not only from conditional observational distributions but also from interven­tional distributions.	abstract
2018-237	This is an important contribution as this means that we can obtain samples with desired properties that may not be present in the training set; useful in applications such as ones involving fairness and also when data collection is expensive and biased.	abstract
2018-237	The main component called the causal controller models the label dependencies and drives the standard conditional GAN.	abstract
2018-237	As reviewers point out, the causal controller assumes the knowledge of the causal graph which is a limitation as this is not known a priori in many applications.	weakness
2018-237	Nevertheless, this is a strong paper that convincingly demonstrates a novel approach to incorporate causal structure into generative models.	strength
2018-237	This should be of great interest to the community and may lead to interesting applications that exploit causality.	strength
2018-237	I recommend acceptance.	decision

2018-238	The paper presents a practical approach to compute Wasserstein distance based image embeddings.	abstract
2018-238	The Euclidean distance in the embedded space approximates the true Wasserstein distance, thus reducing the high computation cost associated with the latter. <sep>	abstract
2018-238	Pros: <sep> - Reviewers agree that the proposed solution is novel, straightforward and well described. <sep>	strength
2018-238	- Experiment demonstrate the usefulness of such embeddings for data mining tasks such as fast computation of barycenters & geodesic analysis. <sep>	strength
2018-238	Cons: <sep> - Though the empirical analysis is convincing, the paper lacks theoretical analysis of the approximation quality.	weakness

2018-243	State-of-the-art results on Squad (at least at time of submission) with a nice model.	strength
2018-243	Authors have since applied the model to additional tasks (SNLI).	abstract
2018-243	Good discussion with reviewers, well written submission and all reviewers suggest acceptance.	rating_summary

2018-244	Nice language modeling paper with consistently high scores.	strength
2018-244	The model structure is neat and the results are solid.	strength
2018-244	Good *CONF*-type paper with contributions mostly on the ML side and experiments on a (simple) NLP task.	strength

2018-248	PROS: <sep> 1. good results; the authors made it work <sep>	strength
2018-248	2. paper is largely well written <sep>	strength
2018-248	CONS: <sep> 1. some found the writing to be unclear and sloppy in places <sep>	weakness
2018-248	2. the algorithm is complicated -- a chain of sub-algorithms <sep>	weakness
2018-248	A few small points: <sep> -I initially found Algorithm 1 to be confusing because it wasn't clear whether it was intended to be invoked for each task (making the training depend on all the datasets).	weakness
2018-248	I finally convinced myself that this was not the intention and that the inner loop of the algorithm is what is actually executed incrementally.	misc

2018-262	The paper shows that many of the current state-of-the-art interpretability methods are inaccurate even for linear models.	abstract
2018-262	Then based on their analysis of linear models they propose a technique that is thus accurate for them and also empirically provides good performance for non-linear models such as DNNs.	abstract

2018-263	The paper proposes a way of detecting statistical interactions in a dataset based on the weights learned by a DNN.	abstract
2018-263	The idea is interesting and quite useful as is showcased in the experiments.	strength
2018-263	The reviewers feel that the paper is also quite well written and easy to follow.	strength

2018-266	All reviewers gave "accept" ratings. <sep>	rating_summary
2018-266	it seems that everyone thinks this is interesting work. <sep>	strength
2018-266	The paper generated a large number of anonymous comments and these were addressed by the authors.	rebuttal_process

2018-270	This is an interesting and well-written paper introducing two unbiased gradient estimators for optimizing expectations of black box functions.	abstract
2018-270	LAX can handle functions of both continuous and discrete random variables, while RELAX is specialized to functions of discrete variables and can be seen as a version of the recently introduced REBAR with its concrete-relaxation-based control variate replaced by (or augmented with) a free-form function.	abstract
2018-270	The experimental section of the paper is adequate but not particularly strong.	weakness
2018-270	If Q-prop is the most similar existing RL approach, as is state in the paper, why not include it as a baseline?	weakness
2018-270	It would also be good to see how RELAX performs at optimizing discrete VAEs using just the free-form control variate (instead of combining it with the REBAR control variate).	suggestion

2018-277	An interesting analysis of the issue of short-horizon bias in meta-optimization that highlights a real problem in a number of existing setups.	abstract
2018-277	I concur with Reviewer 3 that it would be nice to provide a constructive solution to this issue: if something like K-FAC does indeed work well, it would be a great addition to a final version of this paper.	suggestion
2018-277	Nonetheless, I think the paper would be a interesting addition to *CONF* and recommend acceptance.	decision

2018-278	An interesting application of self-ensembling/temporal ensembling for visual domain adaptation that achieves state of the art on the visual domain adaptation challenge.	abstract
2018-278	Reviewers noted that the approach is quite engineering-heavy, but I am not sure it's really much worse than making a pixel-to-pixel approach work well for domain adaptation. <sep>	ac_disagreement
2018-278	I hope the authors follow through with their promise to add experiments to the final version (notably the minimal augmentation experiments to show just how much this domain adaptation technique is tailored towards imagenet-like things). <sep>	suggestion
2018-278	As it stands, this paper would be a good contribution to *CONF* as it shows an efficient and interesting way to solve a particular visual domain adaptation problem.	strength

2018-287	This paper addresses an important application in genomics, ie the prediction of chromatin structure from nucleotide sequences.	abstract
2018-287	The authors develop a novel method for converting the nucleotide sequences to a 2D structure that allows a CNN to detect interactions between distant parts of the sequence.	abstract
2018-287	The reviewers found the paper innovative, interesting and convincing.	strength
2018-287	Two reviewers gave a 7 and there was one 6.	rating_summary
2018-287	The 6, however, indicated during rather lengthy discussion that they were willing to raise their scores if their comments were addressed.	rating_summary
2018-287	Hopefully the authors will address these comments in the camera ready version.	suggestion
2018-287	Overall a solid application paper with novel insights and technical innovation.	strength

2018-292	The paper addresses the problem of learning a teacher model which selects the training samples for the next mini-batch used by the student model.	abstract
2018-292	The proposed solution is to learn the teacher model using policy gradient.	abstract
2018-292	It is an interesting training setting, and the evaluation demonstrates that the method outperforms the baseline.	strength
2018-292	However, it remains unclear how the method would scale to larger datasets, eg ImageNet.	weakness
2018-292	I would strongly encourage the authors to extend their evaluation to larger datasets and state-of-the-art models, as well as include better baselines, eg from Graves et al	suggestion

2018-293	The paper proposes a novel method for conditional image generation which is based on nearest neighbor matching for transferring high-frequency statistics.	abstract
2018-293	The evaluation is carried out on several image synthesis tasks, where the technique is shown to perform better than an adversarial baseline.	abstract

2018-297	This is an interesting piece of work that provides solid evidence on the topic of bootstrapping in deep reinforcement learning.	strength

2018-301	The reviewers unanimously agree that this paper is worth publication at *CONF*.	rating_summary
2018-301	Please address the feedback of the reviewers and discuss exactly how the potential speed up rates are computed in the appendix.	suggestion
2018-301	I speed up rates to be different for different devices.	suggestion

2018-312	This paper on automatic option discovery connects recent research on successor representations with eigenoptions.	abstract
2018-312	This is a solidly presented, conceptual paper with results in tabular and atari environments.	abstract

2018-313	Biological memory systems are grounded in spatial representation and spatial memory, so neural methods for spatial memory are highly interesting.	strength
2018-313	The proposed method is novel, well-designed and the empirical results are good on unseen environments, although the noise model may be too weak.	strength
2018-313	Moreover, it would have been great to evaluate this method on real data rather than in simulation.	suggestion

2018-314	The paper proposes a neural net based method for active localization in a known map using a learnt perception model (convnet) and a learnt control policy combined with a set belief state representation.	abstract
2018-314	The method compares well to baselines and has good accuracy in 2d and 3d envs.	strength
2018-314	All three reviewers are in favor of acceptance due to the novelty and competitive performance of the approach.	rating_summary

2018-327	The paper presents a variational Bayesian approach for quantising neural network weights and makes interesting and useful steps in this increasingly popular area of deep learning.	abstract

2019-109	The first reviewer summarizes the contribution well: This paper combines [a CNN that computes both a multi-scale feature pyramid and a depth prediction, which is expressed as a linear combination of "depth bases"].	abstract
2019-109	This is used to [define a dense re-projection error over the images, akin to that of dense or semi-dense methods].	abstract
2019-109	[Then, this error is optimized with respect to the camera parameters and depth linear combination coefficients using Levenberg-Marquardt (LM).	abstract
2019-109	By unrolling 5 iterations of LM and expressing the dampening parameter lambda as the output of a MLP, the optimization process is made differentiable, allowing back-propagation and thus learning of the networks' parameters.] <sep>	abstract
2019-109	Strengths: <sep> While combining deep learning methods with bundle adjustment is not new, reviewers generally agree that the particular way in which that is achieved in this paper is novel and interesting.	strength
2019-109	The authors accounted for reviewer feedback during the review cycle and improved the manuscript leading to an increased rating. <sep>	rebuttal_process
2019-109	Weaknesses: <sep> Weaknesses were addressed during the rebuttal including better evaluation of their predicted lambda and comparison with CodeSLAM. <sep>	rebuttal_process
2019-109	Contention: <sep> This paper was not particularly contentious, there was a score upgrade due to the efforts of the authors during the rebuttal period. <sep>	rebuttal_process
2019-109	Consensus: <sep> This paper addresses an interesting area of research at the intersection of geometric computer vision and deep learning and should be of considerable interest to many within the *CONF* community.	strength
2019-109	The discussion of the paper highlighted some important nuances of terminology regarding the characterization of different methods.	strength
2019-109	This paper was also rated the highest in my batch.	misc
2019-109	As such, I recommend this paper for an oral presentation.	decision

2019-110	The manuscript proposes deterministic approximations for Bayesian neural networks as an alternative to the standard Monte-Carlo approach.	abstract
2019-110	The results suggest that the deterministic approximation can be more accurate than previous methods.	abstract
2019-110	Some explicit contributions include efficient moment estimates and empirical Bayes procedures. <sep>	strength
2019-110	The reviewers and ACs note weakness in the breadth and complexity of models evaluated, particularly with regards to ablation studies.	weakness
2019-110	This issue seems to have been addressed to the reviewer's satisfaction by the rebuttal.	rebuttal_process
2019-110	The updated manuscript also improves references to related prior work. <sep>	rebuttal_process
2019-110	Overall, reviewers and AC agree that the general problem statement is timely and interesting, and well executed.	strength
2019-110	We recommend acceptance.	decision

2019-111	This paper presents a substantially new way of introducing a syntax-oriented inductive bias into sentence-level models for NLP without explicitly injecting linguistic knowledge.	abstract
2019-111	This is a major topic of research in representation learning for NLP, so to see something genuinely original work well is significant.	strength
2019-111	All three reviewers were impressed by the breadth of the experiments and by the results, and this will clearly be among the more ambitious papers presented at this conference. <sep>	strength
2019-111	In preparing a final version of this paper, though, I'd urge the authors to put serious further effort into the writing and presentation.	suggestion
2019-111	All three reviewers had concerns about confusing or misleading passages, including the title and the discussion of the performance of tree-structured models so far.	weakness

2019-112	The paper proposes a set of tricks leading to a new SOTA for sampling high resolution images.	abstract
2019-112	It is clearly written and the presented contribution will be of high interest for practitioners.	strength

2019-113	This paper proposes a new unsupervised learning approach based on maximizing the mutual information between the input and the representation.	abstract
2019-113	The results are strong across several image datasets.	abstract
2019-113	Essentially all of the reviewer's concerns were directly addressed in revisions of the paper, including additional experiments.	rebuttal_process
2019-113	The only weakness is that only image datasets were experimented with; however, the image-based experiments and comparisons are extensive.	weakness
2019-113	The reviewers and I all agree that the paper should be accepted, and I think it should be considered for an oral presentation.	decision

2019-114	The paper presents a novel strategy for statistically motivated feature selection ie aimed at controlling the false discovery rate.	abstract
2019-114	This is achieved by extending knockoffs to complex predictive models and complex distributions via (multiple) generative adversarial networks. <sep>	abstract
2019-114	The reviewers and ACs noted weakness in the original submission which seems to have been fixed after the rebuttal period -- primary related to missing experimental details.	rebuttal_process
2019-114	There was also some concern (as is common with inferential papers) that the claims are difficult to evaluate on real data, as the ground truth is unknown.	weakness
2019-114	To this end, the authors provide empirical results with simulated data that address this issue.	rebuttal_process
2019-114	There is also some concern that more complex predictive models are not evaluated. <sep>	weakness
2019-114	Overall the reviewers and AC have a positive opinion of this paper and recommend acceptance.	decision

2019-116	This paper proposes a hypothesis about the kinds of visual information for which popular neural networks are most selective.	abstract
2019-116	It then proposes a series of empirical experiments on synthetically modified training sets to test this and related hypotheses.	abstract
2019-116	The main conclusions of the paper are contained in the title, and the presentation was consistently rated as very clear.	strength
2019-116	As such, it is both interesting to a relatively wide audience and accessible. <sep>	strength
2019-116	Although the paper is comparatively limited in theoretical or algorithmic contribution, the empirical results and experimental design are of sufficient quality to inform design choices of future neural networks, and to better understand the reasons for their current behavior. <sep>	strength
2019-116	The reviewers were unanimous in their appreciation of the contributions, and all recommended that the paper be accepted.	rating_summary

2019-118	This paper analyzes random auto encoders in the infinite dimension limit with an assumption that the weights are tied in the encoder and decoder.	abstract
2019-118	In the limit the paper is able to show the random auto encoder transformation  as doing an approximate inference on data.	abstract
2019-118	The paper is able to obtain principled initialization strategies for training deep autoencoders using this analysis, showing the usefulness of their analysis.	abstract
2019-118	Even though there are limitations of paper such as studying only random models, and characterizing them only in the limit, all the reviewers agree that the analysis is novel and gives insights on an interesting problem.	strength

2019-119	The reviewers all agree that the idea is interesting, the writing clear and the experiments sufficient. <sep>	strength
2019-119	To improve the paper, the authors should consider better discussing their meta-objective and some of the algorithmic choices.	suggestion

2019-120	This paper proposes an approach for learning to transfer knowledge across multiple tasks.	abstract
2019-120	It develops a principled approach for an important problem in meta-learning (short horizon bias).	abstract
2019-120	Nearly all of the reviewer's concerns were addressed throughout the discussion phase.	rebuttal_process
2019-120	The main weakness is that the experimental settings are somewhat non-standard (ie the Omniglot protocol in the paper is not at all standard).	weakness
2019-120	I would encourage the authors to mention the discrepancies from more standard protocols in the paper, to inform the reader.	suggestion
2019-120	The results are strong nonetheless, evaluating in settings where typical meta-learning algorithms would struggle.	strength
2019-120	The reviewers and I all agree that the paper should be accepted, and I think it should be considered for an oral presentation.	decision

2019-123	The paper advocates neuroscience-based V1 models to adapt CNNs.	abstract
2019-123	The results of the simulations are convincing from a neuroscience-perspective.	strength
2019-123	The reviewers equivocally recommend publication.	rating_summary

2019-125	All reviewers agree that the presented audio data augmentation is very interesting, well presented, and clearly advancing the state of the art in the field.	strength
2019-125	The authors' rebuttal clarified the remaining questions by the reviewers.	rebuttal_process
2019-125	All reviewers recommend strong acceptance (oral presentation) at *CONF*.	rating_summary
2019-125	I would like to recommend this paper for oral presentation due to a number of reasons including the importance of the problem addressed (data augmentation is the only way forward in cases where we do not have enough of training data), the novelty and innovativeness of the model, and the clarity of the paper.	decision
2019-125	The work will be of interest to the widest audience beyond *CONF*.	misc

2019-127	The paper presents a new approach for domain generalization whereby the original supervised model is trained with an explicit objective to ignore so called superficial statistics present in the training set but which may not be present in future test sets.	abstract
2019-127	The paper proposes using a differentiable variant of gray-level co-occurrence matrix to capture the textural information and then experiments with two techniques for learning feature invariance.	abstract
2019-127	All reviewers agree the approach is novel, unique, and potentially high impact to the community. <sep>	strength
2019-127	The main issues center around reproducibility as well as the intended scope of problems this approach addresses.	weakness
2019-127	The authors have offered to include further discussions in the final version to address these points.	rebuttal_process
2019-127	Doing so will strengthen the paper and aid the community in building upon this work.	suggestion

2019-128	The authors propose a new method of securely evaluating neural networks. <sep>	abstract
2019-128	The reviewers were unanimous in their vote to accept.	rating_summary
2019-128	The paper is very well written, the idea is relatively simple, and so it is likely that this would make a nice presentation.	strength

2019-131	This paper proposes the use of recently propose neural ODEs in a flow-based generative model. <sep>	abstract
2019-131	As the paper shows, a big advantage of a neural ODE in a generative flow is that an unbiased estimator of the log-determinant of the mapping is straightforward to construct.	strength
2019-131	Another advantage, compared to earlier published flows, is that all variables can be updated in parallel, as the method does not require "chopping up" the variables into blocks.	strength
2019-131	The paper shows significant improvements on several benchmarks, and seems to be a promising venue for further research. <sep>	strength
2019-131	A disadvantage of the method is that the authors were unable to show that the method could produce results that were similar (of better than) the SOTA on the more challenging benchmark of CIFAR-10.	weakness
2019-131	Another downside is its computational cost.	weakness
2019-131	Since neural ODEs are relatively new, however, these problems might resolved with further refinements to the method.	suggestion

2019-132	Graph neural networks are an increasingly popular topic of research in machine learning, and this paper does a good job of studying the representational power of some newly proposed variants.	strength
2019-132	The framing of the problem in terms of the WL test, and the proposal of the GIN architecture is a valuable contribution.	strength
2019-132	Through the reviews and subsequent discussion, it looks like the issues surrounding Theorem 3 have been resolved, and therefore all of the reviewers now agree that this paper should be accepted.	rating_summary
2019-132	There may be some interesting followup work based on studying depth, as pointed out by reviewer 1, but this may not be an issue in GIN and is regardless a topic for future research.	ac_disagreement

2019-141	pros: <sep> - the paper is well-written and presents a nice framing of the composition problem <sep>	strength
2019-141	- good comparison to prior work <sep>	strength
2019-141	- very important research direction <sep>	strength
2019-141	cons: <sep> - from an architectural standpoint the paper is somewhat incremental over Routing Networks [Rosenbaum et al] <sep>	weakness
2019-141	- as Reviewers 2 and 3 point out, the experiments are a bit weak, relying on heuristics such as a window over 3 symbols in the multi-lingual arithmetic case, and a pre-determined set of operations (scaling, translation, rotation, identity) in the MNIST case. <sep>	weakness
2019-141	As the authors state, there are three core ideas in this paper (my paraphrase): <sep>	abstract
2019-141	(1) training on a set of compositional problems (with the right architecture/training procedure) can encourage the model to learn modules which can be composed to solve new problems, enabling better generalization. <sep>	abstract
2019-141	(2) treating the problem of selecting functions for composition as a sequential decision-making problem in an MDP <sep>	abstract
2019-141	(3) jointly learning the parameters of the functions and the (meta-level) composition policy. <sep>	abstract
2019-141	As discussed during the review period, these three ideas are already present in the Routing Networks (RN) architecture of Rosenbaum et al  However CRL offers insights and improvements over RN algorithmically in a several ways: <sep>	strength
2019-141	(1) CRL uses a curriculum learning strategy.	strength
2019-141	This seems to be key in achieving good results and makes a lot of sense for naturally compositional problems. <sep>	strength
2019-141	(2) The focus in RN was on using the architecture to solve multi-task problems in object recognition.	strength
2019-141	The solutions learned in image domains while "compositional" are less clearly interpretable.	strength
2019-141	In this paper (CRL) the focus is more squarely on interpretable compositional tasks like arithmetic and explores extrapolation. <sep>	strength
2019-141	(3) The RN architecture does support recursion (and there are some experiments in this mode) but it was not the main focus.	strength
2019-141	In this paper (CRL) recursion is given a clear, prominent role. <sep>	strength
2019-141	I appreciate that the authors' engagement in the discussion period.	misc
2019-141	My feeling is that  the paper offers nice improvements, a useful framing of the problem, a clear recursive formulation, and a more central focus on naturally compositional problems.	strength
2019-141	I am recommending the paper for acceptance but suggest that the authors remove or revise their contributions (3) and (4) on pg.	decision
2019-141	2 in light of the discussion on routing nets. <sep>	decision
2019-141	Routing Networks, Adaptive Selection of Non-Linear Functions for Multi-task Learning, *CONF* 2018	misc

2019-144	Pros: <sep> - novel method for continual learning <sep>	strength
2019-144	- clear, well written <sep>	strength
2019-144	- good results <sep>	strength
2019-144	- no need for identified tasks <sep>	strength
2019-144	- detailed rebuttal, new results in revision <sep>	strength
2019-144	Cons: <sep> - experiments could be on more realistic/challenging domains <sep>	weakness
2019-144	The reviewers agree that the paper should be accepted.	rating_summary

2019-145	While there has been lots of previous work on training dictionaries for sparse coding, this work tackles the problem of doing son in a purely local way.	strength
2019-145	While previous work suggests that the exact computation of gradient addressed in the paper is not necessarily critical, as noted by reviewers, all reviewers agree that the work still makes important contributions through both its theoretical analyses and presented experiments.	strength
2019-145	Authors are encouraged to work on improving clarity further and delineating their contribution more precisely with respect to previous results.	suggestion

2019-147	This is a well executed paper that makes clear contributions to the understanding of unrolled iterative optimization and soft thresholding for sparse signal recovery with neural networks.	strength

2019-150	This paper studies deep convolutional architectures to perform compressive sensing of natural images, demonstrating improved empirical performance with an efficient pipeline. <sep>	abstract
2019-150	Reviewers reached a consensus that this is an interesting contribution that advances data-driven methods for compressed sensing, despite some doubts about the experimental setup and the scope of the theoretical insights.	strength
2019-150	We thus recommend acceptance as poster.	decision

2019-151	This paper studies the task of learning a binary classifier from only unlabeled data.	abstract
2019-151	They first provide a negative result, ie, they show it is impossible to learn an unbiased estimator from a set of unlabeled data.	abstract
2019-151	Then they provide an empirical risk minimization method which works when given two sets of unlabeled data, as well as the class priors. <sep>	abstract
2019-151	The four submitted reviews were unanimous in their vote to accept.	rating_summary
2019-151	The results are impactful, and might make for an interesting oral presentation.	decision

2019-156	This is a solid paper that proposes and analyzes a sound approach to zero order optimization, covering a variants of a simple base algorithm.	abstract
2019-156	After resolving some issues during the response period, the reviewers concluded with a unanimous recommendation of acceptance.	rating_summary
2019-156	Some concerns regarding the necessity for such algorithms persisted, but the connection to adversarial examples provides an interesting motivation.	strength

2019-159	The authors have taken inspiration from recent publications that demonstrate transfer learning over sequential RL tasks and have proposed a method that trains individual learners from experts using layerwise connections, gradually forcing the features to distill into the student with a hard-coded annealing of coeffiecients.	abstract
2019-159	The authors have done thorough experiments and the value of the approach seems clear, especially compared against progressive nets and pathnets.	strength
2019-159	The paper is well-written and interesting, and the approach is novel.	strength
2019-159	The reviewers have discussed the paper in detail and agree, with the AC, that it should be accepted.	decision

2019-162	All reviewers (including those with substantial expertise in RL) were solid in their praise for this paper that is also tackling an interesting application that is much less well studied but deserves attention.	strength

2019-178	This paper is an analysis of the phenomenon of example forgetting in deep neural net training.	abstract
2019-178	The empirical study is the first of its kind and features convincing experiments with architectures that achieve near state-of-the-art results.	strength
2019-178	It shows that a portion of the training set can be seen as support examples.	abstract
2019-178	The reviewers noted weaknesses such as in the measurement of the forgetting itself and the training regiment.	weakness
2019-178	However, they agreed that their concerns we addressed by the rebuttal.	rebuttal_process
2019-178	They also noted that the paper is not forthcoming with insights, but found enough value in the systematic empirical study it provides.	strength

2019-180	This paper introduces a new graph neural network architecture designed to learn to solve Circuit SAT problems, a fundamental problem in computer science.	abstract
2019-180	The key innovation is the ability to to use the DAG structure as an input, as opposed to typical undirected (factor graph style) representations of SAT problems.	abstract
2019-180	The reviewers appreciated the novelty of the approach as well as the empirical results provided that demonstrate the effectiveness of the approach.	strength
2019-180	Writing is clear.	strength
2019-180	While the comparison with NeuroSAT is interesting and useful, there is no comparison with existing SAT solvers which are not based on learning methods.	weakness
2019-180	So it is not clear how big the gap with state-of-the-art is.	weakness
2019-180	Overall, I recommend acceptance, as the results are promising and this could inspire other researchers working on neural-symbolic approaches to search and optimization problems.	decision

2019-183	This paper proposes a novel dataset of bouncing balls and a way to learn the dynamics of the balls when colliding.	abstract
2019-183	The reviewers found the paper well-written, tackling an interesting and hard problem in a novel way.	strength
2019-183	The main concern (that I share with one of the reviewers) is about the fact that the paper proposes both a new dataset/environment *and* a solution for the problem.	weakness
2019-183	This made it difficult the for the authors to provide baselines to compare to.	weakness
2019-183	The ensuing back and forth had the authors relax some of the assumptions from the environment and made it possible to evaluate with interaction nets. <sep>	rebuttal_process
2019-183	The main weakness of the paper is the relatively contrived setup that the authors have come up with.	weakness
2019-183	I will summarize some of the discussion that happened as a result of this point: it is relatively difficult to see how this setup that the authors have and have studied (esp.	weakness
2019-183	knowing the groundtruth impact locations and the timing of the impact) can generalize outside of the proposed approach.	weakness
2019-183	There is some concern that the comparison with interaction nets was not entirely fair. <sep>	weakness
2019-183	I would recommend the authors redo the comparisons with interaction nets in a careful way, with the right ablations, and understand if the methods have access to the same input data (eg are interaction nets provided with the bounce location?). <sep>	suggestion
2019-183	Despite the relatively high average score, I think of this paper as quite borderline, specifically because of the issues related to the setup being too niche.	weakness
2019-183	Nonetheless, the work does have a lot of scientific value to it, in addition to a new simulation environment/dataset that other researchers can then use.	strength
2019-183	Assuming the baselines are done in a way that is trustworthy, the ablation experiments and discussion will be something interesting to the *CONF* community.	decision

2019-184	Reviewers largely agree that the proposed method for finetuning the deep neural networks is interesting and empirical results clearly show the benefits over finetuning only the last layer.	strength
2019-184	I recommend acceptance.	decision

2019-185	1. Describe the strengths of the paper.	misc
2019-185	As pointed out by the reviewers and based on your expert opinion. <sep>	misc
2019-185	- The problem is well-motivated and related work is thoroughly discussed <sep>	strength
2019-185	- The evaluation is compelling and extensive. <sep>	strength
2019-185	2. Describe the weaknesses of the paper.	misc
2019-185	As pointed out by the reviewers and based on your expert opinion.	misc
2019-185	Be sure to indicate which weaknesses are seen as salient for the decision (ie, potential critical flaws), as opposed to weaknesses that the authors can likely fix in a revision. <sep>	misc
2019-185	- Very dense.	weakness
2019-185	Clarity could be improved in some sections. <sep>	weakness
2019-185	3. Discuss any major points of contention.	misc
2019-185	As raised by the authors or reviewers in the discussion, and how these might have influenced the decision.	misc
2019-185	If the authors provide a rebuttal to a potential reviewer concern, it's a good idea to acknowledge this and note whether it influenced the final decision or not.	misc
2019-185	This makes sure that author responses are addressed adequately. <sep>	misc
2019-185	No major points of contention. <sep>	rebuttal_process
2019-185	4. If consensus was reached, say so.	misc
2019-185	Otherwise, explain what the source of reviewer disagreement was and why the decision on the paper aligns with one set of reviewers or another. <sep>	misc
2019-185	The reviewers reached a consensus that the paper should be accepted.	rating_summary

2019-188	This paper is on the problem of adversarial example generation in the setting where the predictor is only accessible via function evaluations with no gradients available.	abstract
2019-188	The associated problem can be cast as a blackbox optimization problem wherein finite difference and related gradient estimation techniques can be used.	abstract
2019-188	This setting appears to be pervasive.	abstract
2019-188	The reviewers agree that the paper is well written and the proposed bandit optimization-based algorithm provides a nice framework in which to integrate priors, resulting in impressive empirical improvements.	strength

2019-190	This paper presents an interesting method for code generation using a graph-based generative approach.	abstract
2019-190	Empirical evaluation shows that the method outperforms relevant baselines (PHOG). <sep>	abstract
2019-190	There is consensus among reviewers that the methods are novel and is worth acceptance to *CONF*.	rating_summary

2019-191	Irrespective of their taste for comparisons of neural networks to biological organisms, all reviewers agree that the empirical observations in this paper are quite interesting and well presented.	strength
2019-191	While some reviewers note that the paper is not making theoretical contributions, the empirical results in themselves are intriguing enough to be of interest to *CONF* audiences.	strength

2019-195	The paper generalizes the concept of "hindsight", ie the recycling of data from trajectories in a goal-based system based on the goal state actually achieved, to policy gradient methods. <sep>	abstract
2019-195	This was an interesting paper in that it scored quite highly despite all three reviewers mentioning incrementality or a relative lack of novelty.	rating_summary
2019-195	Although the authors naturally took some exception to this, AC personally believes that properly executed, contributions that seem quite straightforward in hindsight (pun partly intended) can be valuable in moving the field forward: a clean and didactic presentation of theory backed by well-designed and extensive empirical investigation (both of which are adjectives used by reviewers to describe the empirical work in this paper) can be as valuable, or moreso, than a poorly executed but higher-novelty works.	strength
2019-195	To quote AnonReviewer3, "HPG is almost certainly going to end up being a widely used addition to the RL toolbox". <sep>	strength
2019-195	Feedback from reviewers prompted extensive discussion and a direct comparison with Hindsight Experience Replay which reviewers agreed added significant value to the manuscript, earning it a post-rebuttal unanimous rating of 7.	rating_summary
2019-195	It is therefore my pleasure to recommend acceptance.	decision

2019-202	Reviewers are in a consensus and recommended to accept after engaging with the authors.	rating_summary
2019-202	Please take reviewers' comments into consideration to improve your submission for the camera ready.	suggestion

2019-209	This paper proposes a new solution for tackling domain adaptation across disjoint label spaces.	abstract
2019-209	Two of the reviewers agree that the main technical approach is interesting and novel.	strength
2019-209	The final reviewer asked for clarification of the problem setting which the authors have provided in their rebuttal.	rebuttal_process
2019-209	We encourage the authors to include this in the final version.	suggestion
2019-209	However, there is also a consensus that more experimental evaluation would improve the manuscript and complete experimental details are needed for reliable reproduction.	suggestion

2019-211	The reviewers of this paper agreed that it has done a stellar job of presenting a novel and principled approach to attention as a latent variable, providing a new and sound set of inference techniques to this end.	strength
2019-211	This builds on top of a discussion of the limitations of existing deterministic approaches to attention, and frames the contribution well in relation to other recurrent and stochastic approaches to attention.	strength
2019-211	While there are a few issues with clarity surrounding some aspects of the proposed method, which the authors are encouraged to fine-tune in their final version, paying careful attention to the review comments, this paper is more or less ready for publication with a few tweaks.	decision
2019-211	It makes a clear, significant, and well-evaluate contribution to the field of attention models in sequence to sequence architectures, and will be of great interest to many attendees at *CONF*.	strength

2019-216	This paper proposes factorized prior distributions for CNN weights by using explicit and implicit parameterization for the prior.	abstract
2019-216	The paper suggest a few tractable methods to learn the prior and the model jointly.	abstract
2019-216	The paper, overall, is interesting. <sep>	strength
2019-216	The reviewers have had some disagreement regarding the effectiveness of the method.	weakness
2019-216	The factorized prior may not be the most informative prior and using extra machinery to estimate it might deteriorates the performance.	weakness
2019-216	On the other hand, estimating a more informative prior might be difficult.	weakness
2019-216	It is extremely important to discuss this trade-off in the paper.	weakness
2019-216	I strongly recommend for the authors to discuss the pros and cons of using priors that are weakly informative vs strongly informative. <sep>	suggestion
2019-216	The idea of using a hierarchical model has been around, eg, see the paper on "Hierarchical variational models" and more recently "semi-implicit Variational Inference".	suggestion
2019-216	Please include a related work on such existing work.	suggestion
2019-216	Please discuss why your proposed method is better than these existing methods. <sep>	suggestion
2019-216	Conditioned on the two discussions added to the paper, we can accept it.	decision

2019-218	The authors derive and experiment with quaternion-based recurrent neural networks, and demonstrate their effectiveness on speech recognition tasks (TIMIT and WSJ), where the authors demonstrate that the proposed models can achieve the same accuracy with fewer parameters than conventional models.	abstract
2019-218	The reviewers were unanimous in recommending that the paper be accepted.	rating_summary

2019-221	This paper introduces an approach for improving the scalability of neural network models with large output spaces, where naive soft-max inference scales linearly with the vocabulary size.	abstract
2019-221	The proposed approach is based on a clustering step combined with per-cluster, smaller soft-maxes.	abstract
2019-221	It retains differentiability with the Gumbel softmax trick.	abstract
2019-221	The experimental results are impressive.	strength
2019-221	There are some minor flaws, however there's consensus among the reviewers the paper should be published.	rating_summary

2019-232	The paper gives an extension of scattering transform to non-Euclidean domains by introducing scattering transforms on graphs using diffusion wavelet representations, and presents a stability analysis of such a representation under deformation of the underlying graph metric defined in terms of graph diffusion. <sep>	abstract
2019-232	Concerns of the reviewers are primarily with what type of graphs is the primary consideration (small world social networks or point cloud submanifold samples) and experimental studies.	weakness
2019-232	Technical development like deformation in the proposed graph metric is motivated by sub-manifold scenarios in computer vision, and whether the development is well suitable to social networks in experiments still needs further investigations. <sep>	weakness
2019-232	The authors make satisfied answers to the reviewers' questions.	rebuttal_process
2019-232	The reviewers unanimously accept the paper for *CONF* publication.	rating_summary

2019-234	All of the reviewers agree that this is a well-written paper with the novel perspective of minimizing energy consumption in neural networks, as opposed to maximizing sparsity, which does not always correlate with energy cost.	strength
2019-234	There are a number of promised clarifications and additional results that have emerged from the discussion that should be put into the final draft.	rebuttal_process
2019-234	Namely, describing the overhead of converting from sparse to dense representations, adding the Imagenet sparsity results, and adding the time taken to run the projection step.	suggestion

2019-240	This paper is about unsupervised learning for ASR, by matching the acoustic distribution, learned unsupervisedly, with a prior phone-lm distribution.	abstract
2019-240	Overall, the results look good on TIMIT.	strength
2019-240	Reviewers agree that this is a well written paper and that it has interesting results. <sep>	strength
2019-240	Strengths <sep> - Novel formulation for unsupervised ASR, and a non-trivial extension to previously proposed unsupervised classification to segmental level. <sep>	strength
2019-240	- Well written, with strong results.	strength
2019-240	Improved results and analysis based on review feedback. <sep>	strength
2019-240	Weaknesses <sep> - Results are on TIMIT -- a small phone recognition task. <sep>	weakness
2019-240	- Unclear how it extends to large vocabulary ASR tasks, and tasks that have large scale training data, and RNNs that may learn implicit LMs.	weakness
2019-240	The authors propose to deal with this in future work. <sep>	suggestion
2019-240	Overall, the reviewers agree that this is an excellent contribution with strong results.	rating_summary
2019-240	Therefore, it is recommended that the paper be accepted.	decision

2019-244	The paper introduces a method for using information directed sampling, by taking advantage of recent advances in computing parametric uncertainty and variance estimates for returns.	abstract
2019-244	These estimates are used to estimate the information gain, based on a formula from (Kirschner & Krause, 2018) for the bandit setting.	abstract
2019-244	This paper takes these ideas and puts them together in a reasonably easy-to-use and understandable way for the reinforcement learning setting, which is both nontrivial and useful.	abstract
2019-244	The work then demonstrates some successes in Atari.	abstract
2019-244	Though it is of course laudable that the paper runs on 57 Atari games, it would make the paper even stronger if a simpler setting (some toy domain) was investigated to more systematically understand this approach and some choices in the approach.	suggestion

2019-254	This paper proposes a general-purpose continuous relaxation of the output of the sorting operator.	abstract
2019-254	This enables end-to-end training to enable more efficient stochastic optimization over the combinatorially large space of permutations. <sep>	abstract
2019-254	In the submitted versions, two of the reviewers had difficulty in understanding the writing.	weakness
2019-254	After the rebuttal and the revised version, one of the reviewers is satisfied.	rebuttal_process
2019-254	I personally went through the paper and found that it could be tricky to read certain parts of the paper.	weakness
2019-254	For example, I am personally very familiar with the Placket-Luce model but the writing in Section 2.1 does not do a good job in explaining the model (particularly Eq 1 is not very easy to read, same with eq 3 for the key identity used in the paper). <sep>	weakness
2019-254	I encourage authors to improve writing and make it a bit more intuitive to read. <sep>	suggestion
2019-254	Overall, this is a good paper and I recommend to accept it.	decision

2019-256	The submission proposes a model to generate images where one can control the fine-grained locations of objects.	abstract
2019-256	This is achieved by adding an "object pathway" to the GAN architecture.	abstract
2019-256	Experiments on a number of baselines are performed, including a number of reviewer-suggested metrics that were added post-rebuttal. <sep>	abstract
2019-256	The method needs bounding boxes of the objects to be placed (and labels).	abstract
2019-256	The proposed method is simple and likely novel and I like the evaluating done with Yolov3 to get a sense of the object detection performance on the generated images.	strength
2019-256	I find the results (qual & quant) and write-up compelling and I think that the method will be of practical relevance, especially in creative applications. <sep>	strength
2019-256	Because of this, I recommend acceptance.	decision

2019-262	This paper introduces a new graph convolutional neural network, called LGNN, and applied it to solve the community detection problem.	abstract
2019-262	The reviewers think LGNN yields a nice and useful extension of graph CNN, especially in using the line graph of edge adjacencies and a non-backtracking operator.	strength
2019-262	The empirical evaluation shows that the new method provides a useful tool for real datasets.	strength
2019-262	The reviewers raised some issues in writing and reference, for which the authors have provided clarification and modified the papers accordingly.	rebuttal_process

2019-268	This paper proposed a method that creates neural networks that can run under different resource constraints.	abstract
2019-268	The reviewers have consensus on accept.	rating_summary
2019-268	The pro is that the paper is novel and provides a practical approach to adjust model for different computation resource, and achieved performance improvement on object detection.	strength
2019-268	One concern from reviewer2 and another public reviewer is the inconsistent performance impact on classification/detection (performance improvement on detection, but performance degradation on classification).	weakness
2019-268	Besides, the numbers reported in Table 1 should be confirmed: MobileNet v1 on Google Pixel 1 should have less than 120ms latency [1], not 296 ms. <sep>	weakness
2019-268	[1] Table 4 of https://arxiv.org/pdf/1801.04381. pdf	misc

2019-270	This paper builds on the recent DCFNet (Decomposed Convolutional Filters) architecture to incorporate rotation equivariance while preserving stability.	abstract
2019-270	The core idea is to decompose the trainable filters into a steerable representation and learn over a subset of the coefficients of that representation. <sep>	abstract
2019-270	Reviewers all agreed that this is a solid contribution that advances research into group equivariant CNNs, bringing efficiency gains and stability guarantees, albeit these appear to be incremental with respect to the techniques developed in the DCFNet work.	strength
2019-270	In summary, the AC believes this to be a valuable contribution and therefore recommends acceptance.	decision

2019-272	This paper proposes a novel approach for network pruning in both training and inference.	abstract
2019-272	This paper received a consensus of acceptance.	rating_summary
2019-272	Compared with previous work that focus and model compression on training, this paper saves memory and accelerates both training and inference.	strength
2019-272	It is activation, rather than weight that dominates the training memory.	strength
2019-272	Reviewer1 posed a valid concern about the efficient implementation on GPUs, and authors agreed that practical speedup on GPU is difficult.	rebuttal_process
2019-272	It'll be great if the authors can give practical insights on how to achieve real speedup in the final draft.	suggestion

2019-275	Pros: <sep> - novel, general idea for hard exploration domains <sep>	strength
2019-275	- multiple additional tricks <sep>	strength
2019-275	- ablations, control experiments <sep>	strength
2019-275	- well-written paper <sep>	strength
2019-275	- excellent results on Montezuma <sep>	strength
2019-275	Cons: <sep> - low sample efficiency (2B+ frames) <sep>	weakness
2019-275	- unresolved questions (non-episodic intrinsic rewards) <sep>	weakness
2019-275	- could have done better apples-to-apples comparisons to baselines <sep>	weakness
2019-275	The reviewers did not reach consensus on whether to accept or reject the paper.	rating_summary
2019-275	In particular, after multiple rounds of discussion, reviewer 1 remains adamant that the downsides of the paper outweigh its good points.	rating_summary
2019-275	However, given that the other three reviewers argue strongly and credibly for acceptance, I think the paper should be accepted.	decision

2019-279	1. Describe the strengths of the paper.	misc
2019-279	As pointed out by the reviewers and based on your expert opinion. <sep>	misc
2019-279	- novel approach to audio synthesis <sep>	strength
2019-279	- strong qualitative and quantitative results <sep>	strength
2019-279	- extensive evaluation<sep>	strength
2019-279	2. Describe the weaknesses of the paper.	misc
2019-279	As pointed out by the reviewers and based on your expert opinion.	misc
2019-279	Be sure to indicate which weaknesses are seen as salient for the decision (ie, potential critical flaws), as opposed to weaknesses that the authors can likely fix in a revision. <sep>	misc
2019-279	- small grammatical issues (mostly resolved in the revision).<sep>	weakness
2019-279	3. Discuss any major points of contention.	misc
2019-279	As raised by the authors or reviewers in the discussion, and how these might have influenced the decision.	misc
2019-279	If the authors provide a rebuttal to a potential reviewer concern, it's a good idea to acknowledge this and note whether it influenced the final decision or not.	misc
2019-279	This makes sure that author responses are addressed adequately. <sep>	misc
2019-279	No major points of contention.<sep>	misc
2019-279	4. If consensus was reached, say so.	misc
2019-279	Otherwise, explain what the source of reviewer disagreement was and why the decision on the paper aligns with one set of reviewers or another. <sep>	misc
2019-279	The reviewers reached a consensus that the paper should be accepted.	rating_summary

2019-284	Strong points: <sep> -- Interesting, fairly systematic and novel analyses of recurrent NMT models, revealing individual neurons responsible for specific type of information (eg, verb tense or gender) <sep>	strength
2019-284	-- Interesting experiments showing how these neurons can be used to manipulate translations in specific ways (eg, specifying the gender for a pronoun when the source sentence does not reveal it) <sep>	strength
2019-284	-- The paper is well written <sep>	strength
2019-284	Weak points <sep> -- Nothing serious (eg, maybe interesting to test across multiple runs how stable these findings are). <sep>	weakness
2019-284	There is a consensus among the reviewers that this is a strong paper and should be accepted.	rating_summary

2019-295	The authors propose an approach for visual navigation that leverages a semantic knowledge graph to ground and inform the policy of an RL agent.	abstract
2019-295	The agent uses a graphnet to learn relationships and support the navigation.	abstract
2019-295	The empirical protocol is sound and uses best practices, and the authors have added additional experiments during the revision period, in response to the reviewers' requests.	rebuttal_process
2019-295	However, there were some significant problems with the submission - there were no comparisons to other semantic navigation methods, the approach is somewhat convoluted and will not survive the test of time, and the authors did not conclusively show the value of their approach.	rebuttal_process
2019-295	The reviewers uniformly support the publication of this paper, but with a low confidence.	rating_summary

2019-306	This paper provides a novel and non-trivial method for approximating the eigenvectors of the Laplacian, in large or continuous state environments.	abstract
2019-306	Eigenvectors of the Laplacian have been used for proto-value functions and eigenoptions, but it has remained an open problem to extend their use to the non-tabular case.	abstract
2019-306	This paper makes an important advance towards this goal, and will be of interest to many that would like to learn state representations based on the geometric information given by the Laplacian. <sep>	strength
2019-306	The paper could be made stronger by including a short discussion on why the limitations of this approach.	suggestion
2019-306	Its an important new direction, but there must still be open questions (eg, issues with the approach used to approximate the orthogonality constraint).	suggestion
2019-306	It will be beneficial to readers to understand these issues.	suggestion

2019-309	1. Describe the strengths of the paper.	misc
2019-309	As pointed out by the reviewers and based on your expert opinion. <sep>	misc
2019-309	- The problem is interesting and challenging <sep>	strength
2019-309	- The proposed approach is novel and performs well. <sep>	strength
2019-309	2. Describe the weaknesses of the paper.	misc
2019-309	As pointed out by the reviewers and based on your expert opinion.	misc
2019-309	Be sure to indicate which weaknesses are seen as salient for the decision (ie, potential critical flaws), as opposed to weaknesses that the authors can likely fix in a revision. <sep>	misc
2019-309	- The clarity could be improved <sep>	weakness
2019-309	3. Discuss any major points of contention.	misc
2019-309	As raised by the authors or reviewers in the discussion, and how these might have influenced the decision.	misc
2019-309	If the authors provide a rebuttal to a potential reviewer concern, it's a good idea to acknowledge this and note whether it influenced the final decision or not.	misc
2019-309	This makes sure that author responses are addressed adequately. <sep>	misc
2019-309	Many concerns were clarified during the discussion period.	rebuttal_process
2019-309	One major concern had been the experimental evaluation.	rebuttal_process
2019-309	In particular, some reviewers felt that experiments on real images (rather than in simulation) was needed. <sep>	rebuttal_process
2019-309	To strengthen this aspect, the authors added new qualitative and quantitative results on a real-world experiment with a robot arm, under 10 different scenarios, showing good performance on this challenging task.	rebuttal_process
2019-309	Still, one reviewer was left unconvinced that the experimental evaluation was sufficient. <sep>	weakness
2019-309	4. If consensus was reached, say so.	misc
2019-309	Otherwise, explain what the source of reviewer disagreement was and why the decision on the paper aligns with one set of reviewers or another. <sep>	misc
2019-309	Consensus was not reached.	misc
2019-309	The final decision is aligned with the positive reviews as the AC believes that the evaluation was adequate.	decision

2019-311	This paper proposes a novel framework for tractably learning non-eucliean embeddings that are product spaces formed by hyperbolic, spherical, and Euclidean components, providing a heterogenous mix of curvature properties.	abstract
2019-311	On several datasets, these product space embeddings outperform single Euclidean or hyperbolic spaces.	abstract
2019-311	The reviewers unanimously recommend acceptance.	rating_summary

2019-315	The reviewers have all recommended accepting this paper thus I am as well.	decision
2019-315	Based on the reviews and the selectivity of the single track for oral presentations, I am only recommending acceptance as a poster.	decision

2019-317	This work highlights the problem of biased rewards present in common adversarial imitation learning implementations, and proposes adding absorbing states to to fix the issue.	abstract
2019-317	This is combined with an off-policy training algorithm, yielding significantly improved sample efficiency, whose benefits are convincingly shown empirically.	abstract
2019-317	The paper is well written and clearly presents the contributions.	strength
2019-317	Questions were satisfactorily answered during discussion, and resulted in an improved submission, a paper that all reviewers now agree is worth presenting at *CONF*.	rebuttal_process

2019-320	The reviewers all feel that the paper should be accepted to the conference.	rating_summary
2019-320	The main strengths that they noted were the quality of writing, the wide applicability of the proposed method and the strength of the empirical evaluation.	strength
2019-320	It's nice to see experiments across a large number of problems (100), with corresponding code, where baselines were hyperparameter tuned as well.	strength
2019-320	This helps to give some assurance that the method will generalize to new problems and datasets.	strength
2019-320	Some weaknesses noted by the reviewers were computational cost (the method is significantly slower than the baselines) and they weren't entirely convinced that having more concise representations would directly lead to the claimed interpretability of the approach.	weakness
2019-320	Nevertheless, they found it would make for a solid contribution to the conference.	strength

2019-321	This paper presents methods to scale learning of embedding models estimated using neural networks.	abstract
2019-321	The main idea is to work with Gram matrices whose sizes depend on the length of the embedding.	abstract
2019-321	Building upon existing works like SAG algorithm, the paper proposes two new stochastic methods for learning using stochastic estimates of Gram matrices. <sep>	abstract
2019-321	Reviewers find the paper interesting and useful, although have given many suggestions to improve the presentation and experiments.	strength
2019-321	For this reason, I recommend to accept this paper. <sep>	decision
2019-321	A small note: SAG algorithm was originally proposed in 2013.	weakness
2019-321	The paper only cites the 2017 version.	weakness
2019-321	Please include the 2013 version as well.	suggestion

2019-330	This paper proposes a knowledge graph completion approach that represents relations as rotations in a complex space; an idea that the reviewers found quite interesting and novel.	abstract
2019-330	The authors provide analysis to show how this model can capture symmetry/assymmetry, inversions, and composition.	abstract
2019-330	The authors also introduce a separate contribution of self-adversarial negative sampling, which, combined with complex rotational embeddings, obtains state of the art results on the benchmarks for this task. <sep>	abstract
2019-330	The reviewers and the AC identified a number of potential weaknesses in the initial paper: (1) the evaluation only showed the final performance of the approach, and thus it was not clear how much benefit was obtained from adversarial sampling vs the scoring model, or further, how good the results would be for the baselines if the same sampling was used, (2) citation and comparison to a closely related approach (TorusE), and (3) a number of presentation issues early on in the paper. <sep>	weakness
2019-330	The reviewers appreciated the author's comments and the revision, which addressed all of the concerns by including (1) additional experiments to performance with and without self-adversarial sampling, and comparisons to TorusE, (2) improved presentation. <sep>	rebuttal_process
2019-330	With the revision, the reviewers agreed that this is a worthy paper to include in the conference.	rating_summary

2019-332	This paper proposes a new stochastic optimization scheme similar to Adam.	abstract
2019-332	The authors claim that Adam can be improved upon by decorrelating the second-moment estimate v_t from gradient estimates g_t.	abstract
2019-332	This is done through the temporal decorrelation scheme, as well as block-wise sharing of estimates v_t. <sep>	abstract
2019-332	The reviewers agree that the paper is sufficiently well-written, original and significant to be accepted for *CONF*, although some unclarity remains after the reviews.	rating_summary
2019-332	A disadvantage of the method is mainly an increased computational cost (linear in 'n', however this might be negligible when sharing v_t across blocks).	weakness

2019-338	The authors discuss an improved distillation scheme for parallel WaveNet using a Gaussian inverse autoregressive flow, which can be computed in closed-form, thus simplifying training.	abstract
2019-338	The work received favorable comments from the reviewers, along with a number of suggestions for improvement which have improved the draft considerably.	rating_summary
2019-338	The AC agrees with the reviewers that the work is a valuable contribution, particularly in the context of end-to-end neural text-to-speech systems.	strength

2019-342	The paper proposes a decision-theoretic framework for meta-learning.	abstract
2019-342	The ideas and analysis are interesting and well-motivated, and the experiments are thorough.	strength
2019-342	The primary concerns of the reviewers have been addressed in new revisions of the paper.	rebuttal_process
2019-342	The reviewers all agree that the paper should be accepted.	rating_summary
2019-342	Hence, I recommend acceptance.	decision

2019-344	This paper proposes an effective method to train neural networks with quantized reduced precision.	abstract
2019-344	It's fairly straight-forward idea and achieved good results and solid empirical work.	strength
2019-344	reviewers have a consensus on acceptance.	rating_summary

2019-349	pros: <sep> - Identification of several interesting problems with the original DNC model: masked attention, erasion of de-allocated elements, and sharpened temporal links <sep>	strength
2019-349	- An improved architecture which addresses the issues and shows improved performance on synthetic memory tasks and bAbI over the original model <sep>	strength
2019-349	- Clear writing <sep>	strength
2019-349	cons: <sep> - Does not really show this modified DNC can solve a task that the original DNC could not and the bAbI tasks are effectively solved anyway.	weakness
2019-349	It is still not clear whether the DNC even with these improvements will have much impact beyond these toy tasks. <sep>	weakness
2019-349	Overall the reviewers found this to be a solid paper with a useful analysis and I agree.	strength
2019-349	I recommend acceptance.	decision

2019-362	The paper introduces a version of approximate policy iteration (API), called Autodidactic Iteration (ADI), designed to overcome the problem of sparse rewards.	abstract
2019-362	In particular, the policy evaluation step of ADI is trained on a distribution of states that allows the reward to easily propagate from the goal state to states farther away.	abstract
2019-362	ADI is applied to successfully solve the Rubik's Cube (together with other existing techniques). <sep>	abstract
2019-362	This work is an interesting contribution where the ADI idea may be useful in other scenarios.	strength
2019-362	A limitation is that the whole empirical study is on the Rubik's Cube; a controlled experiment on other problems (even if simpler) can be useful to understand the pros & cons of ADI compared to others. <sep>	weakness
2019-362	Minor: please update the bib entry of Bottou (2011).	suggestion
2019-362	It's now published in MLJ 2014.	suggestion

2019-365	The paper proposes an interesting framework for visualizing and understanding GANs, that will be of clear help for understanding existing models and might provide insights for developing new ones.	strength

2019-382	This paper proposes a reinforcement learning approach that better handles sparse reward environments, by using previously-experienced roll-outs that achieve high reward.	abstract
2019-382	The approach is intuitive, and the results in the paper are convincing.	strength
2019-382	The authors addressed nearly all of the reviewer's concerns.	rebuttal_process
2019-382	The reviewers all agree that the paper should be accepted.	decision

2019-394	This paper introduces a very simple but effective method for the neural architecture search problem.	abstract
2019-394	The key idea of the method is a particular continuous relaxation of the architecture representation to enable gradient descent-like differentiable optimization.	abstract
2019-394	Results are quite good.	strength
2019-394	Source code is also available.	strength
2019-394	A concern of the approach is the (possibly large) integrality gap between the continuous solution and the discretized architecture.	weakness
2019-394	The solution provided in the paper is a heuristic without guarantees.	weakness
2019-394	Overall, this is a good paper.	strength
2019-394	I recommend acceptance.	decision

2019-398	This paper presents quasi-hyperbolic momentum, a generalization of Nesterov Accelerated Gradient.	abstract
2019-398	The method can be seen as adding an additional hyperparameter to NAG corresponding to the weighting of the direct gradient term in the update.	abstract
2019-398	The contribution is pretty simple, but the paper has good discussion of the relationships with other momentum methods, careful theoretical analysis, and fairly strong experimental results.	strength
2019-398	All the reviewers believe this is a strong paper and should be accepted, and I concur.	decision

2019-399	This paper analyzes local SGD optimization for strongly convex functions, and proves that local SGD enjoys a linear speedup (in the number of workers and minibatch size) over vanilla SGD, while also communicating less than distributed mini-batch SGD.	abstract
2019-399	A similar analysis is also provided for the asynchronous case, and limited empirical confirmation of the theory is provided.	abstract
2019-399	The main weakness of the current revision is that it does not yet properly relate this work to two prior publications: Dekel et al, 2012 (https://arxiv.org/pdf/1012.1367. pdf) and Jain et al, 2016 (https://arxiv.org/abs/1610.03774).	weakness
2019-399	It is critical that these references and suitable discussion be added in the camera-ready paper, since this issue was the subject of considerable discussion and the authors promised to include the references and discussion in the final paper.	suggestion

2019-406	This paper introduces a new way to estimate gradients of expectations of discrete random variables by introducing antithetic noise samples for use in a control variate. <sep>	abstract
2019-406	Quality:  The experiments are mostly appropriate, although I disagree with the choice to present validation and test-set results instead of training-time results.	strength
2019-406	If the goal of the method is to reduce variance, then checking whether optimization is improved (training loss) is the most direct measure.	suggestion
2019-406	However reasonable people can disagree about this. <sep>	misc
2019-406	I also think the toy experiment (copied from the REBAR and RELAX paper) is a bit too easy for this method, since it relies on taking two antithetic samples.	weakness
2019-406	I would have liked to see a categorical extension of the same experiment. <sep>	suggestion
2019-406	Clarity:  I think that this method will not have the impact it otherwise could because of the authors' fearless use of long equations and heavy notation throughout.	weakness
2019-406	This is unavoidable to some degree, but <sep> 1) The title of the paper isn't very descriptive <sep>	weakness
2019-406	2) Why not follow previous work and use \\theta instead of \\phi for the parameters being optimized? <sep>	suggestion
2019-406	The presentation has come a long way, but I fear that few besides our intrepid reviewers will have the stomach.	weakness
2019-406	I recommend providing more intuition throughout. <sep>	suggestion
2019-406	Originality:  The use of antithetic samples to reduce variance is old, but this seems like a well-thought-through and non-trivial application of the idea to this setting. <sep>	strength
2019-406	Significance:  Ultimately I think this is a new direction in gradient estimators for discrete RVs.	strength
2019-406	I don't think this is the last word in this direction but it's both an empirical improvement, and will inspire further work.	strength

2019-410	The paper addresses normalisation and conditioning of GANs.	abstract
2019-410	The authors propose to replace class-conditional batch norm with whitening and class-conditional coloring.	abstract
2019-410	Evaluation demonstrates that the method performs very well, and the ablation studies confirm the design choices.	strength
2019-410	After extensive discussion, all reviewers agreed that this is a solid contribution, and the paper should be accepted.	rating_summary

2019-414	* Strengths <sep> The paper addresses an important topic: how to bound the probability that a given "bad" event occurs for a neural network under some distribution of inputs.	strength
2019-414	This could be relevant, for instance, in autonomous robotics settings where there is some environment model and we would like to bound the probability of an adverse outcome (eg for an autonomous aircraft, the time to crash under a given turbulence model).	strength
2019-414	The desired failure probabilities are often low enough that direct Monte Carlo simulation is too expensive.	strength
2019-414	The present work provides some preliminary but meaningful progress towards better methods of estimating such low-probability events, and provides some evidence that the methods can scale up to larger networks.	strength
2019-414	It is well-written and of high technical quality. <sep>	strength
2019-414	* Weaknesses <sep> In the initial submission, one reviewer was concerned that the term "verification" was misleading, as the methods had no formal guarantees that the estimated probability was correct.	rebuttal_process
2019-414	The authors proposed to revise the paper to remove reference to verification in the title and the text, and afterwards all reviewers agreed the work should be accepted.	rebuttal_process
2019-414	The paper also may slightly overstate the generality of the method.	weakness
2019-414	For instance, the claim that this can be used to show that adversarial examples do not exist is probably wrong---adversarial examples often occupy a negligibly small portion of the input space.	weakness
2019-414	There was also concern that most comparisons were limited to naive Monte Carlo. <sep>	weakness
2019-414	* Discussion <sep> While there was initial disagreement among reviewers, after the discussion all reviewers agree the paper should be accepted.	rebuttal_process
2019-414	However, we remind the authors to implement the changes promised during the discussion period.	suggestion

2019-417	This paper addresses the issue of numerical rounding-off errors that can arise when using latent variable models for data compression,  eg, because of differences in floating point arithmetic across different platforms (sender and receiver).	abstract
2019-417	The authors propose using neural networks that perform integer arithmetic (integer networks) to mitigate this issue.	abstract
2019-417	The problem statement is well described, and the presentation is generally OK, although it could be improved in certain aspects as pointed out by the reviewers.	strength
2019-417	The experiments are properly carried out, and the experimental results are good. <sep>	strength
2019-417	Thank you for addressing the questions raised by the reviewers.	misc
2019-417	After taking into account the author's responds, there is consensus that the paper is worthy of publication.	rating_summary
2019-417	I therefore recommend acceptance.	decision

2019-422	This paper addresses a well motivated problem and provides new insight on the theoretical analysis of representational power in quantized networks.	abstract
2019-422	The results contribute towards a better understanding of quantized networks in a way that has not been treated in the past. <sep>	abstract
2019-422	The most moderate rating (marginally above acceptance threshold) explains that while the paper is technically quite simple, it gives an interesting study and blends well into recent literature on an important topic. <sep>	strength
2019-422	A criticism is that the approach uses modules to approximate the basic operations of non quantized networks.	weakness
2019-422	As such it not compatible with quantizing the weights of a given network structure, but rather with choosing the network structure under a given level of quantization.	weakness
2019-422	However, reviewers consider that this issue is discussed directly and clearly in the paper. <sep>	strength
2019-422	The reviewers report to be only fairly confident about their assessment, but they all give a positive or very positive evaluation of the paper.	rating_summary

2019-426	The paper proposes an improved method for uncertainty estimation in deep neural networks. <sep>	abstract
2019-426	Reviewer 2 and AC note that the paper is a bit isolated in terms of comparing the literature. <sep>	weakness
2019-426	However, as all of reviewers and AC found, the paper is well written and the proposed idea is clearly new/interesting.	strength

2019-428	A well-written paper that proposes an original approach for leaning a structured prior for VAEs, as a latent tree model whose structure and parameters are simultaneously learned.	abstract
2019-428	It describes a well-principled approach to learning a multifaceted clustering, and is shown empirically to be competitive with other unsupervised clustering models. <sep>	abstract
2019-428	Reviewers noted that the approach reached a worse log-likelihood than regular VAE (which it should be able to find as a special case), hinting towards potential optimization difficulties (local minimum?).	weakness
2019-428	This would benefit form a more in-depth analysis. <sep>	weakness
2019-428	But reviewers appreciated the gain in interpretability and insights from the model, and unanimously agreed that the paper was an interesting novel contribution worth publishing.	rating_summary

2019-430	Strengths: <sep> This paper develops a method for learning the structure of discrete latent variables in a VAE.	strength
2019-430	The overall approach is well-explained and reasonable. <sep>	strength
2019-430	Weaknesses: <sep> Ultimately, this is done using the usual style of discrete relaxations, which come with tradeoffs and inconsistencies. <sep>	weakness
2019-430	Consensus: <sep> The reviewers all agreed that the paper is above the bar.	rating_summary

2019-433	There is consensus among the reviewer that this is a good paper.	rating_summary
2019-433	It is a bit incremental compared to Gregor et al 2016.	weakness
2019-433	This paper show quite better empirical results.	strength

2019-434	The paper presents an interesting technique for constrained policy optimization, which is applicable to existing RL algorithms such as TRPO and PPO.	abstract
2019-434	All of the reviewers agree that the paper is above the bar and the authors have improved the exposition during the review process.	rebuttal_process
2019-434	I encourage the authors to address all of the comments in the final version.	suggestion

2019-435	pros: <sep> - the paper is well-written and precise <sep>	strength
2019-435	- the proposed method is novel <sep>	strength
2019-435	- valuable for real-world problems <sep>	strength
2019-435	cons: <sep> - Reviewer 2 expresses some concern about the organization of the paper and over-generality in the exposition <sep>	weakness
2019-435	- There could be more discussion of scalability	weakness

2019-436	The authors present a learnt scheduling mechanism for managing communications in bandwidth-constrained, contentious multi-agent RL domains.	abstract
2019-436	This is well-positioned in the rapidly advancing field of MARL and the contribution of the paper is both novel, interesting, and effective.	strength
2019-436	The agents learn how to schedule themselves, how to encode messages, and how to select actions.	strength
2019-436	The approach is evaluated against several other methods and achieves a good performance increase.	strength
2019-436	The reviewers had concerns regarding the difficulty of evaluating the overall performance and also about how it would fare in more real-world scenarios, but all agree that this paper should be accepted.	rating_summary

2019-439	Pros <sep> - Thorough analysis on a large number of diverse tasks <sep>	strength
2019-439	- Extending the probing technique typically applied to individual encoder states to testing for presence of certain (linguistic) information based on pairs of encoders states (corresponding to pairs of words) <sep>	strength
2019-439	- The comparison can be useful when deciding which representations to use for a given task <sep>	strength
2019-439	Cons <sep> - Nothing serious, it is solid and important empirical study <sep>	strength
2019-439	The reviewers are in consensus.	rating_summary

2019-443	although i (ac) believe the contribution is fairly limited (eg, (1) only looking at the word embedding which goes through many nonlinear layers, in which case it's not even clear whether how word vectors are distributed matters much, (2) only considering the case of tied embeddings, which is not necessarily the most common setting,..), all the reviewers found the execution of the submission (motivation, analysis and experimentation) to be done well, and i'll go with the reviewers' opinion.	decision

2019-446	This is a well written paper that contributes a clear advance to the understanding of how gradient descent behaves when training deep linear models.	strength
2019-446	Reviewers were unanimously supportive.	rating_summary

2019-451	The authors present a novel method for tackling exploration and exploitation that yields promising results on some hard navigation-like domains.	strength
2019-451	The reviewers were impressed by the contribution and had some suggestions for improvement that should be addressed in the camera ready version.	suggestion

2019-462	This paper is about representation learning for calcium imaging and thus a bit different in scope that most *CONF* submissions.	weakness
2019-462	But the paper is well-executed with good choices for the various parts of the model making it relevant for other similar domains.	strength

2019-466	This paper presents new generalized methods for representing sentences and measuring their similarities based on word vectors.	abstract
2019-466	More specifically, the paper presents Fuzzy Bag-of-Words (FBoW), a generalized approach to composing sentence embeddings by combining word embeddings with different degrees of membership, which generalize more commonly used average or max-pooled vector representations.	abstract
2019-466	In addition, the paper presents DynaMax, an unsupervised and non-parametric similarity measure that can dynamically extract and max-pool features from a sentence pair. <sep>	abstract
2019-466	Pros: <sep> The proposed methods are natural generalization of exiting average and max-pooled vectors.	strength
2019-466	The proposed methods are elegant, simple, easy to implement, and demonstrate strong performance on STS tasks. <sep>	strength
2019-466	Cons: <sep> The paper is solid, no significant con other than that the proposed methods are not groundbreaking innovations per say. <sep>	weakness
2019-466	Verdict: <sep> The simplicity is what makes the proposed methods elegant.	strength
2019-466	The empirical results are strong.	strength
2019-466	The paper is worthy of acceptance.	decision

2019-476	This paper introduces an approach for reducing the dimensionality of training data examples in a way that preserves information about soft target probabilistic representations provided by a teacher model, with applications such as zero-shot learning and distillation.	abstract
2019-476	The authors provide an extensive theoretical and empirical analysis, showing performance improvements in zero shot learning and finite sample error upper bounds.	abstract
2019-476	The reviewers generally agree this is a good paper that should be published.	rating_summary

2019-477	The reviewers and authors had a productive conversation, leading to an improvement in the paper quality.	rebuttal_process
2019-477	The strengths of the paper highlighted by reviewers are a novel learning set-up and new loss functions that seem to help in the task of protein contact prediction and protein structural similarity prediction.	strength
2019-477	The reviewers characterize the work as constituting an advance in an exciting application space, as well as containing a new configuration of methods to address the problem. <sep>	strength
2019-477	Overall, it is clear the paper should be accepted, based on reviewer comments, which unanimously agreed on the quality of the work.	decision

2019-481	This paper presents an RL agent which progressively synthesis programs according to syntactic constraints, and can learn to solve problems with different DSLs, demonstrating some degree of transfer across program synthesis problems.	abstract
2019-481	Reviewers agreed that this was an exciting and important development in program synthesis and meta-learning (if that word still has any meaning to it), and were impressed with both the clarity of the paper and its evaluation.	strength
2019-481	There were some concerns about missing baselines and benchmarks, some of which were resolved during the discussion period, although it would still be good to compare to out-of-the-box MCTS. <sep>	rebuttal_process
2019-481	Overall, everyone agrees this is a strong paper and that it belongs in the conference, so I have no hesitation in recommending it.	decision

2019-482	The paper aims to encourage deep networks to have stable derivatives over larger regions under networks with piecewise linear activation functions. <sep>	abstract
2019-482	All reviewers and AC note the significance of the paper.	strength
2019-482	AC also thinks this is also a very timely work and potentially of broader interest of *CONF* audience.	strength

2019-483	This paper proposes a new measure to quantify the contribution of an individual neuron within a deep neural network.	abstract
2019-483	Interpretability and better understanding of the inner workings of neural networks are important questions, and all reviewers agree that this work is contributing an interesting approach and results.	strength

2019-486	The authors have presented a simple yet elegant model to learn grid-like responses to encode spatial position, relying only on relative Euclidean distances to train the model, and achieving a good path integration accuracy.	abstract
2019-486	The model is simpler than recent related work and uses a structure of 'disentangled blocks' to achieve multi-scale grids rather than requiring dropout or injected noise.	abstract
2019-486	The paper is clearly written and it is intriguing to get down to the fundamentals of the grid code.	strength
2019-486	On the negative side, the section on planning does not hold up as well and makes unverifiable claims, and one reviewer suggests that this section be replaced altogether by additional analysis of the grid model.	weakness
2019-486	Another reviewer points out that the authors have missed an opportunity to give a theoretical perspective on their model.	weakness
2019-486	Although there are aspects of the work which could be improved, the AC and all reviewers are in favor of acceptance of this paper.	decision

2019-488	The paper provides a comprehensive study and generalisations of previous results on linear permutation invariant and equivariant operators / layers for the case of hypergraph data on multiple node sets.	abstract
2019-488	Reviewers indicate that the paper makes a particularly interesting and important contribution, with applications to graphs and hyper-graphs, as demonstrated in experiments. <sep>	strength
2019-488	A concern was raised that the paper could be overstating its scope.	weakness
2019-488	A point is that the model might not actually give a complete characterization, since the analysis considers permutation action only.	weakness
2019-488	The authors have rephrased the claim.	rebuttal_process
2019-488	Following comments of the reviewer, the authors have also revised the paper to include a discussion of how the model is capable of approximating message passing networks. <sep>	rebuttal_process
2019-488	Two referees give the paper a strong support.	rating_summary
2019-488	One referee considers the paper ok, but not good enough.	rating_summary
2019-488	The authors have made convincing efforts to improve issues and address the concerns.	rebuttal_process

2019-489	This paper provides interesting discussions on the trade-off between model accuracy and robustness to adversarial examples.	abstract
2019-489	All reviewers found that both empirical studies and theoretical results are solid.	strength
2019-489	The paper is very well written.	strength
2019-489	The visualization results are very intuitive.	strength
2019-489	I recommend acceptance.	decision

2019-492	This paper proposes a new optimization method for ReLU networks that optimizes in a scale-invariant vector space in the hopes of facilitating learning.	abstract
2019-492	The proposed method is novel and is validated by some experiments on CIFAR-10 and CIFAR-100.	strength
2019-492	The reviewers find the analysis of the invariance group informative but have raised questions about the computational cost of the method.	weakness
2019-492	These concerns were addressed by the authors in the revision.	rebuttal_process
2019-492	The method could be of practical interest to the community and so acceptance is recommended.	decision

2019-496	The paper introduces a new and convincing method for video frame prediction, by adding prediction uncertainty through VAEs.	abstract
2019-496	The results are convincing, and the reviewers are convinced. <sep>	strength
2019-496	It's unfortunate however that the method is only evaluated on simulated data.	weakness
2019-496	Letting it loose on real data would cement the results and merit oral representation; in the current form, poster presentation is recommended.	decision

2019-498	The authors have described a navigation method that uses co-grounding between language and vision as well as an explicit self-assessment of progress.	abstract
2019-498	The method is used for room 2 room navigation and is tested in unseen environments.	abstract
2019-498	On the positive side, the approach is well-analyzed, with multiple ablations and baseline comparisons.	strength
2019-498	The method is interesting and could be a good starting point for a more ambitious grounded language-vision agent.	strength
2019-498	The approach seems to work well and achieves a high score using the metric of successful goal acquisition.	strength
2019-498	On the negative side, the method relies on beam search, which is certainly unrealistic for real-world navigation, the evaluation metric is very simple and may be misleading, and the architecture is quite complex, may not scale or survive the test of time, and has little relevance for the greater ML community.	weakness
2019-498	There was a long discussion between the authors and the reviewers and other members of the public that resolved many of these points, with the authors being extremely responsive in giving additional results and details, and the reviewers' conclusion is that the paper should be accepted.	rating_summary

2019-501	The reviewers all argued for acceptance citing the novelty and potential of the work as strengths.	rating_summary
2019-501	They all found the experiments a little underwhelming and asked for more exciting empirical evaluation.	weakness
2019-501	The authors have addressed this somewhat by including multi-modal experiments in the discussion period.	rebuttal_process
2019-501	The paper would be more impactful if the authors could demonstrate significant improvements on really challenging problems where MCMC is currently prohibitively expensive, such as improving over HMC for highly parameterized deep neural networks.	suggestion
2019-501	Overall, however, this is a very nice paper and warrants acceptance to the conference.	decision

2019-502	The paper focuses on hybrid pipelines that contain black-boxes and neural networks, making it difficult to train the neural components due to non-differentiability.	abstract
2019-502	As a solution, this paper proposes to replace black-box functions with neural modules that approximate them during training, so that end-to-end training can be used, but at test time use the original black box modules.	abstract
2019-502	The authors propose a number of variations: offline, online, and hybrid of the two, to train the intermediate auxiliary networks.	abstract
2019-502	The proposed model is shown to be effective on a number of synthetic datasets. <sep>	abstract
2019-502	The reviewers and AC note the following potential weaknesses: (1) the reviewers found some of the experiment details to be scattered, (2) It was unclear what happens if there is a mismatch between the auxiliary network and the black box function it is approximating, especially if the function is one, like sorting, that is difficult for neural models to approximate, and (3) the text lacked description of real-world tasks for which such a hybrid pipeline would be useful. <sep>	weakness
2019-502	The authors provide comments and a revision to address these concerns.	rebuttal_process
2019-502	They added a section that described the experiment setup to aid reproducibility, and incorporated more details in the results and related work, as suggested by the reviewers.	rebuttal_process
2019-502	Although these changes go a long way, some of the concerns, especially regarding the mismatch between neural and black box function, still remain. <sep>	rebuttal_process
2019-502	Overall, the reviewers agreed that the issues had been addressed to a sufficient degree, and the paper should be accepted.	rating_summary

2019-506	Dear authors, <sep>	misc
2019-506	All reviewers agreed that your work sheds new light on a popular class of algorithms and should thus be presented at *CONF*. <sep>	rating_summary
2019-506	Please make sure to implement all their comments in the final version.	suggestion

2019-511	The paper proposes a new dataset for studying knowledge grounded conversations, that would be very useful in advancing this field.	abstract
2019-511	In addition to the details of the dataset and its collection, the paper also includes a framework for advancing the research in this area, that includes evaluation methods and baselines with a relatively new approach. <sep>	abstract
2019-511	The proposed approach for dialogue generation however is a simple extension of previous work by (Zhang et al) to user transformers, hence is not very interesting.	weakness
2019-511	The proposed approach is also not compared to many previous studies in the experimental results. <sep>	weakness
2019-511	One of the reviewers highlighted the weakness of the human evaluation performed in the paper.	weakness
2019-511	Moving on, it would be useful if further approaches are considered and included in the task evaluation. <sep>	weakness
2019-511	A poster presentation of the work would enable participants to ask detailed questions about the proposed dataset and evaluation, and hence may be more appropriate.	decision

2019-512	There's precious little work asking existential questions about adversarial examples, and so this work is most welcome.	strength
2019-512	The work connects with deep results in probability to make simple and transparent claims about the inevitability of adversarial examples under some assumptions.	abstract
2019-512	The authors have addressed the key criticisms of the authors around clarity.	rebuttal_process

2019-517	The paper proposes a new distributed DQN algorithm that combines recurrent neural networks with distributed prioritized replay memory.	abstract
2019-517	The authors systematically compare three types of initialization strategies for training the recurrent models.	abstract
2019-517	The thorough investigation is cited as a valuable contribution by all reviewers, with reviewer 1 noting that the study would be of interest to "anyone using recurrent networks on RL tasks".	strength
2019-517	Empirical results on Atari and DMLab are impressive. <sep>	strength
2019-517	The reviewers noted several weaknesses in their original reviews.	rebuttal_process
2019-517	These included issues of clarity, a need for more detailed ablation studies, and need to more carefully document the empirical setup.	weakness
2019-517	A further question was raised on whether the empirical results could be complemented with theoretical or conceptual insights. <sep>	weakness
2019-517	The authors carefully addressed all concerns raised during the reviewing and rebuttal period.	rebuttal_process
2019-517	They took exceptional care to clarify their writing, document experiment details, and ran a large set of additional experiments as suggested by the reviewers.	rebuttal_process
2019-517	The AC feels that the review period for the paper was particularly productive and would like to thank the reviewers and authors. <sep>	rebuttal_process
2019-517	The reviewers and AC agree that the paper makes a significant contribution to the field and should be accepted.	decision

2019-519	This work presents a method to model embeddings as distributions, instead of points, to better quantify uncertainty.	abstract
2019-519	Evaluations are carried out on a new dataset created from mixtures of MNIST digits, including noise (certain probability of occlusions), that introduce ambiguity, using a small "toy" neural network that is incapable of perfectly fitting the data, because authors mention that performance difference lessens when the network is complex enough to almost perfectly fit the data. <sep>	abstract
2019-519	Reviewer assessment is unanimously accept, with the following points: <sep>	rating_summary
2019-519	Pros: <sep> + "The topic of injecting uncertainty in neural networks should be of broad interest to the *CONF* community." <sep>	strength
2019-519	+ "The paper is generally clear." <sep>	strength
2019-519	+ "The qualitative evaluation provides intuitive results." <sep>	strength
2019-519	Cons: <sep> - Requirement of drawing samples may add complexity.	weakness
2019-519	Authors reply that alternatives should be studied in future work. <sep>	rebuttal_process
2019-519	- No comparison to other uncertainty methods, such as dropout.	weakness
2019-519	Authors reply that dropout represents model uncertainty and not data uncertainty, but do not carry out an experiment to compare (ie sample from model leaving dropout activated during evaluation). <sep>	rebuttal_process
2019-519	- No evaluation in larger scale/dimensionality datasets.	weakness
2019-519	Authors mention method scales linearly, but how practical or effective this method is to use on, say, face recognition datasets, is unclear. <sep>	rebuttal_process
2019-519	As the general reviewer consensus is accept, Area Chair is recommending Accept; However, Area Chair has strong reservations because the method is evaluated on a very limited dataset, with a toy model designed to exaggerate differences between techniques.	decision
2019-519	Essentially, the toy evaluation was designed to get the results the authors were looking for.	weakness
2019-519	A more thorough investigation would use more realistic sized network models on true datasets.	suggestion

2019-524	All the reviewers agree that the paper has an interesting idea on regularizing the spectral norm of the weight matrices in GANs, and a generalization bound has been shown.	strength
2019-524	The empirical result shows that indeed regularization improves the performance of the GANs.	abstract
2019-524	Based on these the AC suggested acceptance.	decision

2019-525	The authors have extended previous publications on curiosity driven, intrinsically motivated RL with this broad empirical study on the effectiveness of the curiosity algorithm on many game environments, the merits of different feature sets, and limitations of the approach.	abstract
2019-525	The paper is well-written and should be of interest to the community.	strength
2019-525	The experiments are well conceived and seem to validate the general effectiveness of curiosity.	strength
2019-525	However, the paper does not actually have any novel contribution compared against prior work, and there are no great insights or takeaways from the empirical study.	weakness
2019-525	Therefore, the reviewers were somewhat divided on how confident they were that the paper should be accepted.	rating_summary
2019-525	Overall, the AC agrees that it is a valuable paper that should be accepted even though it does not deliver any algorithmic novelty.	decision

2019-533	The paper presents an interesting theoretical analysis by deriving polynomial sample complexity bounds for the training of GANs that depend on the approximator properties of the discriminator. <sep>	abstract
2019-533	Even if it is not clear if the theory will help to pick suitable discriminators in practice, it provides new and interesting theoretical insights on the properties of GAN training.	strength

2019-537	This paper makes a substantial contribution to the understanding of the approximation ability of deep networks in comparison to classical approximation classes, such as polynomials.	abstract
2019-537	Strong results are given that show fundamental advantages for neural network function approximators in the presence of a natural form of latent structure.	abstract
2019-537	The analysis techniques required to achieve these results are novel and worth reporting to the community.	strength
2019-537	The reviewers are uniformly supportive.	rating_summary

2019-538	This paper proposed an interesting approach to weight sharing among CNN layers via shared weight templates to save parameters.	abstract
2019-538	It's well written with convincing results.	strength
2019-538	Reviewers have a consensus on accept.	rating_summary

2019-539	The paper proposes a particle based framework for learning object dynamics.	abstract
2019-539	A scene is represented by a hierarchical graph over particles, edges between particles are established dynamically based on Euclidean distance.	abstract
2019-539	The model is used for model predictive control, and there is also one experiment with a particle graph built from a real scene as opposed to simulation. <sep>	abstract
2019-539	All reviewers agree that the architectural changes over previous relational networks  are worthwhile and merit publication.	rating_summary
2019-539	They also suggest to tone down the ``dynamic" part of the graph construction by stating that edges are determined based on a radius.	suggestion
2019-539	In particular, previous works also consider similar addition of edges during collisions, quoting Mrowca et al "Collisions between objects are handled by dynamically defining pairwise collision relations ... between leaf particles..." which suggests that comparison against a baseline for Mrowca et al that uses a static graph is not entirely fair.	suggestion
2019-539	The authors are encouraged to repeat the experiment without disabling such dynamic addition of edges.	suggestion

2019-553	Strengths: <sep> Well written paper on a new kind of spherical convolution for use in spherical CNNs. <sep>	strength
2019-553	Evaluated on rigid and non-rigid 3D shape recognition and retrieval problems. <sep>	strength
2019-553	Paper provides solid strategy for efficient GPU implementation. <sep>	strength
2019-553	Weaknesses: There was some misunderstanding about the properties of the alt-az convolution detected by one of the reviewers along with some points needing clarifications.	weakness
2019-553	However, discussion of these issues appears to have led to a resolution of the issues. <sep>	rebuttal_process
2019-553	Contention: The weaknesses above were discussed in some detail, but the procedure was not particularly contentious and the discussion unfolded well. <sep>	rebuttal_process
2019-553	All reviewers rate the paper as accept, the paper clearly provides value to the community and therefore should be accepted.	decision

2019-554	This is an interesting paper that develops new techniques for analyzing the loss surface of deep networks, allowing the existence of spurious local minima to be established under fairly general conditions.	abstract
2019-554	The reviewers responded with uniformly positive opinions.	rating_summary

2019-565	Pros: <sep> - novel idea of endowing RL agents with recursive reasoning <sep>	strength
2019-565	- clear, well presented paper <sep>	strength
2019-565	- thorough rebuttal and revision with new results <sep>	strength
2019-565	Cons: <sep> - small-scale experiments <sep>	weakness
2019-565	The reviewers agree that the paper should be accepted.	rating_summary

2019-566	Quality: The overall quality of the work is high.	strength
2019-566	The main idea and technical choices are well-motivated, and the method is about as simple as it could be while achieving its stated objectives. <sep>	strength
2019-566	Clarity:  The writing is clear, with the exception of using alternative scripts for some letters in definitions. <sep>	strength
2019-566	Originality:  The biggest weakness of this work is originality, in that there is a lot of closely related work, and similar ideas without convergence guarantees have begun to be explored.	weakness
2019-566	For example, the (very natural) U-net architecture was explored in previous work. <sep>	weakness
2019-566	Significance:  This seems like an example of work that will be of interest both to the machine learning community, and also the numerics community, because it also achieves the properties that the numerics community has historically cared about.	strength
2019-566	It is significant on its own as an improved method, but also as a demonstration that using deep learning doesn't require scrapping existing frameworks but can instead augment them.	strength

2019-569	This paper conducted theoretical analysis of the effect of batch normalisation to auto rate-tuning.	abstract
2019-569	It provides an explanation for the empirical success of BN.	abstract
2019-569	The assumptions for the analysis is also closer to the common practice of batch normalization compared to a related work of Wu et al 2018. <sep>	abstract
2019-569	One of the concerns raised by the reviewer is that the analysis does not immediately apply to practical uses of BN, but the authors already discussed how to fill the gap with a slight change of the activation function.	rebuttal_process
2019-569	Another concern is about the lack of empirical evaluation of the theory, and the authors provide additional experiments in the revision.	rebuttal_process
2019-569	R1 also points out a few weaknesses in the theoretical analysis, which I think would help improve the paper further if the authors could clarify and provide discussion in their revision. <sep>	weakness
2019-569	Overall, it is a good paper that will help improve our theoretical understanding about the power tool of batch normalization.	strength

2019-572	The authors have proposed a new method for exploration that is related to parameter noise, but instead uses Gaussian dropout across entire episodes, thus allowing for temporally consistent exploration.	abstract
2019-572	The method is evaluated in sparsely rewarded continuous control domains such as half-cheetah and humanoid, and compared against PPO and other variants.	abstract
2019-572	The method is novel and does seem to work stably across the tested tasks, and simple exploration methods are important for the RL field.	strength
2019-572	However, the paper is poorly and confusingly written and really really needs to be thoroughly edited before the camera ready deadline.	weakness
2019-572	There are many approaches which are referred to without any summary or description, which makes it difficult to read the paper.	weakness
2019-572	The three reviewers all had low confidence in their understanding of the paper, which makes this a very borderline submission even though the reviewers gave relatively high scores.	rating_summary

2019-575	This paper formulates a method for training deep networks to produce high-resolution semantic segmentation output using only low-resolution ground-truth labels.	abstract
2019-575	Reviewers agree that this is a useful contribution, but with the limitation that joint distribution between low- and high-resolution labels must be known.	weakness
2019-575	Experimental results are convincing.	strength
2019-575	The technique introduced by the paper could be applicable to many semantic segmentation problems and is likely to be of general interest.	strength

2019-580	The paper shows limitations on the types of functions that can be represented by deep skinny networks for certain classes of activation functions, independently of the number of layers.	abstract
2019-580	With many other works discussing capabilities but not limitations, the paper contributes to a relatively underexplored topic. <sep>	strength
2019-580	The settings capture a large family of activation functions, but exclude others, such as polynomial activations, for which the considered type of obstructions would not apply.	weakness
2019-580	Also a concern is raised about it not being clear how this theoretical result can shed insight on the empirical study of neural networks. <sep>	weakness
2019-580	The authors have responded to some of the comments of the reviewers, but not to all comments, in particular comments of reviewer 1, who's positive review is conditional on the authors addressing some points. <sep>	rebuttal_process
2019-580	The reviewers are all confident and are moderately positive, positive, or very positive about this paper.	rating_summary

2019-592	The authors provide a new analysis of generalization in deep linear networks, provide new insight through the role of "task structure".	abstract
2019-592	Empirical findings are used to cast light on the general case.	abstract
2019-592	This work seems interesting and worthy of publication.	decision

2019-593	This paper proposes Switchable Normalization (SN) that leans how to combine three existing normalization techniques for improved performance.	abstract
2019-593	There is a general consensus that that the paper has good quality and clarity, is well motivated, is sufficiently novel, makes clear contributions for training deep neural networks, and provides convincing experimental results to show the advantages of the proposed SN.	strength

2019-597	Strengths: The paper tackles a novel, well-motivated problem related to options & HRL. <sep>	strength
2019-597	The problem is that of learning transition policies, and the paper proposes a novel and simple solution to that problem, using learned proximity predictors and transition policies that can leverage those.	strength
2019-597	Solid evaluations are done on simulated locomotion and manipulation tasks.	strength
2019-597	The paper is well written. <sep>	strength
2019-597	Weaknesses: Limitations were not originally discussed in any depth. <sep>	weakness
2019-597	There is related work related to sub-goal generation in HRL. <sep>	weakness
2019-597	AC: The physics of the 2D walker simulations looks to be unrealistic; <sep>	weakness
2019-597	the character seems to move in a low-gravity environment, and can lean forwards at extreme angles without falling.	weakness
2019-597	It would be good to see this explained. <sep>	weakness
2019-597	There is a consensus among reviewers and AC that the paper would make an excellent *CONF* contribution. <sep>	misc
2019-597	AC: I suggest a poster presentation; it could also be considered for oral presentation based on the very positive reception by reviewers.	decision

2019-600	This paper introduces a method that aims to solve the problem of 'posterior collapse' in variational autoencoders (VAEs).	abstract
2019-600	The problem of posterior collapse is well-documented in the VAE literature, and various solutions have been proposed.	abstract
2019-600	Existing proposed solutions, however, aim to solve the problem by either changing the objective function (eg beta-VAE) or by changing the prior and/or approximate posterior models.	abstract
2019-600	The proposed method, in contrast, aims to solve the problem by bringing the VAE optimization procedure closer to the EM optimization procedure.	abstract
2019-600	Every iteration in optimization consists of SGD updates to the inference model (E-step), performed until the approximate posterior converges.	abstract
2019-600	This is followed by a single SGD update of the generative model.	abstract
2019-600	The multi-update E-step makes sure that the M-step optimizes something closer to the marginal log-likelihood, compared to what we would normaly do in VAEs (joint optimization of both inference model and generative model). <sep>	abstract
2019-600	The experiments are relatively small-scale, but convincing. <sep>	strength
2019-600	The reviewers agree that the method is clearly described, and that the proposed technique is well supported by the experiments.	strength
2019-600	We think that this work will probably be of high interest to the *CONF* community.	decision

2019-601	The reviewers all agreed that this paper makes a strong contribution to *CONF* by providing the first asynchronous analysis of a Nesterov-accelerated coordinate descent method.	rating_summary

2019-603	In this work, the authors propose a simple, under parameterized network architecture which can fit natural images well, when fed with a fixed random input signal.	abstract
2019-603	This allows the model to be used for a number of tasks without requiring that the model be trained on a dataset.	abstract
2019-603	Further, unlike a recently proposed related method (DIP; [Ulyanov et al, 18]), the method does not require regularization such as early-stopping as with DIP. <sep>	abstract
2019-603	The reviewers noted the simplicity and experimental validation, and were unanimous in recommending acceptance.	rating_summary

2019-607	Interesting paper applying memory networks that encode external knowledge (represented in the form of triples) and conversation context for task oriented dialogues.	abstract
2019-607	Experiments demonstrate improvements over the state of the art on two public datasets. <sep>	abstract
2019-607	Notation and presentation in the first version of the paper were not very clear, hence many question and answers were exchanged during the reviews.	rebuttal_process

2019-608	This paper addresses a promising method for unpaired cross-domain image-to-image translation that can accommodate multi-instance images.	abstract
2019-608	It extends the previously proposed CycleGAN model by taking into account per-instance segmentation masks.	abstract
2019-608	All three reviewers and AC agree that performing such transformation in general is a hard problem when significant changes in shape or appearance of the object have to be made, and that the proposed approach is sound and shows promising results.	strength
2019-608	As rightly acknowledged by R1 'The formulation is intuitive and well done!' <sep>	strength
2019-608	There are several potential weaknesses and suggestions to further strengthen this work: <sep> (1) R1 and R2 raised important concerns about the absence of baselines such as crop & attach simple baseline and CycleGAN+Seg Pleased to report that the authors showed and discussed in their response some preliminary qualitative results regarding these baselines.	rebuttal_process
2019-608	In considering the author response and reviewer comments, the AC decided that the paper could be accepted given the comparison in the revised version, but the authors are strongly urged to include more results and evaluations on crop & attach baseline in the final revision if possible. <sep>	decision
2019-608	(2) more quantitative results are needed for assessing the benefits of this approach (R3).	weakness
2019-608	The authors discussed in their response to R3 that more quantitative results such as the segmentation accuracy of the synthesized images are not possible since no ground-truth segmentation labels are available.	rebuttal_process
2019-608	This is true in general for unpaired image-to-image translation, however collecting annotations and performing such quantitative evaluation could have a substantial impact for assessing the significance of this work and can be seen as a recommendation for further improvement. <sep>	rebuttal_process
2019-608	(3) the proposed model performs translation for a pair of domains; extending the work to multi-domain translation like StarGAN by Choi et al 2018 or GANimation by Pumarola 2018 would strengthen the significance of the work.	suggestion
2019-608	The authors discussed in their response to R3 that this is indeed possible.	rebuttal_process

2019-609	This paper relates deep learning to convex optimization by showing that the forward pass though a dropout layer, linear layer (either convolutional or fully connected), and a nonlinear activation function is equivalent to taking one τ-nice proximal gradient descent step on a a convex optimization objective.	abstract
2019-609	The paper shows (1) how different activation functions correspond to different proximal operators, (2) that replacing Bernoulli dropout with additive dropout corresponds to replacing the τ-nice proximal gradient descent method with a variance-reduced proximal method, and (3) how to compute the Lipschitz constant required to set the optimal step size in the proximal step.	abstract
2019-609	The practical value of this perspective is illustrated in experiments that replace various layers in ConvNet architectures with proximal solvers, leading to performance improvements on CIFAR-10 and CIFAR-100.	abstract
2019-609	The reviewers felt that most of their concerns were adequately addressed in the discussion and revision, and that the paper should be accepted.	rating_summary

2019-748	This paper proposes a simple modification of the Adam optimizer, introducing a hyper-parameter 'p' (with value in the range [0,1/2]) parameterizing the parameter update: <sep> theta_new = theta_old + m/v^p <sep> where p=1/2 falls back to the standard Adam/Amsgrad optimizer, and p=0 falls back to a variant of SGD with momentum. <sep>	abstract
2019-748	The authors motivate the method by pointing out that: <sep> - Through the value of 'p', one can interpolate between SGD with momentum and Adam/Amsgrad.	abstract
2019-748	By choosing a value of 'p' smaller than 0.5, one can therefore use perform optimization that is 'partially adaptive'. <sep>	abstract
2019-748	- The method shows good empirical performance.<sep>	abstract
2019-748	The paper contains an inaccuracy, which we hope will be solved before the final version.	weakness
2019-748	The authors argue that the 1/sqrt(v) term in Adam results in a lower learning rate, and the authors argue that the effective learning rate "easily explodes" (section 3) because of this term, and that a "more aggressive" learning rate is more appropriate.	weakness
2019-748	This last point is false; the value of 1/sqrt(v) can be smaller or larger than 1 depending on the value of 'v', and that a decrease in value of 'p' can result in either an increase or decrease in effective learning rate, depending on the value of v. The value of 'v' is a function of the scale of loss function, which can really be arbitrary.	weakness
2019-748	(In case of very high-dimensional predictions, for example, the scale of the loss function is often proportional with the dimensionality of variable to be modeled, which can be arbitrarily large, eg in image or video modeling the loss function tends to be of a much larger scale than with classification.) <sep>	weakness
2019-748	The authors promise to include a comparison to AdamW [Loshchilov, 2017] that includes tuning of the weight decay parameter.	rebuttal_process
2019-748	The lack of this experiments makes it more difficult to make a conclusion regarding the performance relative to AdamW.	weakness
2019-748	However, the methods offer potentially orthogonal (and combinable) advantages. <sep>	strength
2019-748	[Loshchilov, 2017] https://arxiv.org/pdf/1711.05101. pdf	misc

2019-1677	This paper proposes a latent variable approach to the neural module networks of Andreas et al, whereby the program determining the structure of a module network is a structured discrete latent variable.	abstract
2019-1677	The authors explore inference mechanisms over such programs and evaluate them on SHAPES. <sep>	abstract
2019-1677	This paper may seem acceptable on the basis of its scores, but R1 (in particular) and R3 did a shambolic job of reviewing: their reviews are extremely short, and offer no substance to justify their scores.	ac_disagreement
2019-1677	R2 has admirably engaged in discussion and upped their score to 6, but continue to find the paper fairly borderline, as do I.	rating_summary
2019-1677	Weighing the reviews by the confidence I have in the reviewers based on their engagement, I would have to concur with R2 that this paper is very borderline.	misc
2019-1677	I like the core idea, but agree that the presentation of the inference techniques for V-NMN is complex and its presentation could stand to be significantly improved.	weakness
2019-1677	I appreciate that the authors have made some updates on the basis of R2's feedback, but unfortunately due to the competitive nature of this year's *CONF* and the number of acceptable paper, I cannot fully recommend acceptance at this time. <sep>	decision
2019-1677	As a complete side note, it is surprising not to see the Kingma & Welling (2013) VAE paper cited here, given the topic.	weakness

2020-34	The paper presents a new architecture that achieves the advantages of both Bi-encoder and Cross-encoder architectures.	abstract
2020-34	The proposed idea is reasonable and well-motivated, and the paper is clearly written.	strength
2020-34	The experimental results on retrieval and dialog tasks are strong, achieving high accuracy while the computational efficiency is orders of magnitude smaller than Cross-encoder.	strength
2020-34	All reviewers recommend acceptance of the paper and this AC concurs.	decision

2020-39	This paper exams the role of mutual information (MI) estimation in representation learning.	abstract
2020-39	Through experiments, they show that the large MI is not predictive of downstream performance, and the empirical success of  methods like InfoMax may be more attributed to the inductive bias in  the choice of architectures of discriminators, rather than accurate MI estimation.	abstract
2020-39	The work is well appreciated by the reviewers.	rating_summary
2020-39	It forms a strong contribution and may motivate subsequent works in the field.	strength

2020-40	The paper proposes a way to analyze overfitting to non-relevant parts of the state space in RL and proposes a framework to measure this type of generalization error.	abstract
2020-40	All reviewers agree that the formulation is interesting and useful for practical RL.	strength

2020-52	Overall, this paper got strong scores from the reviewers (2 accepts and 1 weak accept).	rating_summary
2020-52	The paper proposes to address the responsibility problem, enabling encoding and decoding sets without worrying about permutations.	abstract
2020-52	This is achieved using permutation-equivariant set autoencoders and an 'inverse' operation that undoes the sorting in the decoder.	abstract
2020-52	The reviewers all agreed that the paper makes a meaningful contribution and should be accepted.	rating_summary
2020-52	Some concerns regarding clarity of exposition were initially raised but were addressed during the rebuttal period.	rebuttal_process
2020-52	I recommend that the paper be accepted.	decision

2020-92	The paper proposes a neural network architecture to address the problem of estimating a sparse precision matrix from data, which can be used for inferring conditional independence if the random variables are gaussian.	abstract
2020-92	The authors propose an Alternating Minimisation procedure for solving the l1 regularized maximum likelihood which can be unrolled and parameterized.	abstract
2020-92	This method is shown to converge faster at inference time than other methods and it is also far more effective in terms of training time compared to an existing data driven method. <sep>	abstract
2020-92	Reviewers had good initial impressions of this paper, pointing out the significance of the idea and the soundness of the setup.	strength
2020-92	After a productive rebuttal phase the authors significantly improved the readibility and successfully clarified the remaining concerns of the reviewers.	rebuttal_process
2020-92	This AC thus recommends acceptance.	decision

2020-98	This paper presents neural architecture search for semantic segmentation, with search space that integrates multi-resolution branches.	abstract
2020-98	The method also uses a regularization to overcome the issue of learned networks collapsing to low-latency but poor accuracy models.	abstract
2020-98	Another interesting contribution is a collaborative search procedure to simultaneously search for student and teacher networks in a single run.	strength
2020-98	All reviewers agree that the proposed method is well-motivated and shows promising empirical results.	strength
2020-98	Author response satisfactorily addressed most of the points raised by the reviewers.	rebuttal_process
2020-98	I recommend acceptance.	decision

2020-112	After the rebuttal period the ratings on this paper increased and it now has a strong assessment across reviewers.	rebuttal_process
2020-112	The AC recommends acceptance.	decision

2020-140	The submission applies architecture search to find effective architectures for video classification.	abstract
2020-140	The work is not terribly innovative, but the results are good.	strength
2020-140	All reviewers recommend accepting the paper.	rating_summary

2020-153	The paper considers the problem of knowledge-grounded dialogue generation with low resources.	abstract
2020-153	The authors propose to disentangle the model into three components that can be trained on separate data, and achieve SOTA on three datasets. <sep>	abstract
2020-153	The reviewers agree that this is a well-written paper with a good idea, and strong empirical results, and I happily recommend acceptance.	decision

2020-171	Main content: <sep> Blind review #1 summarizes it well: <sep> Recently many language GAN papers have been published to overcome the so called exposure bias, and demonstrated improvements in natural language generation in terms of sample quality, some works propose to assess the generation in terms of diversity, however, quality and diversity are two conflicting measures that are hard to meet.	abstract
2020-171	This paper is a groundbreaking work that proposes receiver operating curve or Pareto optimality for quality and diversity measures, and shows that simple temperature sweeping in MLE generates the best quality-diversity curves than all language GAN models through comprehensive experiments.	abstract
2020-171	It points out a good target that language GANs should aims at. <sep>	abstract
2020-171	-- <sep>	misc
2020-171	Discussion: <sep> The main reservation was the originality of the idea of using temperature sweep in the softmax.	rebuttal_process
2020-171	However, it turns out this idea came from the authors in the first place, which they have not been able to state directly due to the anonymity requirement.	rebuttal_process
2020-171	Per the program chair's instruction to direct this to the area chair, I think this has been handled correctly. <sep>	rebuttal_process
2020-171	-- <sep>	misc
2020-171	Recommendation and justification: <sep> This paper should be accepted.	decision
2020-171	It provides readers with insight in that it illuminates a misconception of how important exposure bias has been assumed to be, and provides a less expensive MLE based way to train than GAN counterparts.	strength

2020-174	All three reviewers agree that the paper provide an interesting study on the ability of generative adversarial networks to model geometric transformations and a simple practical approach to how such ability can be improved.	strength
2020-174	Acceptance as a poster is recommended.	decision

2020-213	This paper proposes to build an 'imitative model' to improve the performance for imitation learning.	abstract
2020-213	The main idea is to combine the model-based RL type of work to the imitation learning approach.	abstract
2020-213	The model is trained using a probabilistic method and can help the agent imitate goals that were previously not easy to achieve with previous works. <sep>	abstract
2020-213	Reviewers 2 and 3 strongly agree that the paper should be accepted.	rating_summary
2020-213	R3 has increased their score after the rebuttal, and the authors' response helped in this case.	rebuttal_process
2020-213	Based on reviewers score, I recommend to accept this paper.	decision

2020-227	Two knowledgable reviewers recommend accepting the paper, and the less familiar reviewer is also positive.	rating_summary
2020-227	The final decision is to accept the paper.	decision
2020-227	It's an interesting and timely topic with insightful results.	strength

2020-268	The authors introduce a new associative inference task from cognitive psychology, show shortcomings of current memory-augmented architectures, and introduce a new memory architecture that performs better with respect to the task.	abstract
2020-268	The reviewers like the motivation and thought the experimental results were strong, although they also initially had several questions and pointed to areas of the paper which lacked clarity.	rebuttal_process
2020-268	The authors updated the paper in response to the reviewer's questions and increased the clarity of the paper.	rebuttal_process
2020-268	The reviewers are satisfied and believe the paper should be accepted.	rating_summary

2020-294	This paper proposes an approach to type inference in dynamically typed languages using graph neural networks.	abstract
2020-294	The reviewers (and the area chair) love this novel and useful application of GNNs to a practical problem, the presentation, the results.	strength
2020-294	Clear accept.	decision

2020-299	Reviewer worries include: whether the approach scales to distant language pairs, overselling of the paper as a "framework", a few citations and comparisons missing.	weakness
2020-299	I agree and encourage the authors not to use the word "framework" here.	suggestion
2020-299	I would also encourage the authors to evaluate on more interesting language pairs, and analyze what vocabularies are relocated, as well as what their method is better at compared to previous work.	suggestion

2020-312	This paper studies how much overparameterization is required to achieve zero training error via gradient descent in one hidden layer neural nets.	abstract
2020-312	In particular the paper studies the effect of margin in data on the required amount of overparameterization.	abstract
2020-312	While the paper does not improve in the worse case in the presence of margin the paper shows that sometimes even logarithmic width is sufficient.	weakness
2020-312	The reviewers all seem to agree that this is a nice paper but had a few mostly technical concerns.	weakness
2020-312	These concerns were sufficiently addressed in the response.	rebuttal_process
2020-312	Based on my own reading I also find the paper to be interesting, well written with clever proofs.	strength
2020-312	So I recommend acceptance.	decision
2020-312	I would like to make a suggestion that the authors do clarify in the abstract intro that this improvement can not be achieved in the worst case as a shallow reading of the manuscript may cause some confusion (that logarithmic width suffices in general).	suggestion

2020-313	All reviewers agree that this research is novel and well carried out, so this is a clear accept.	decision
2020-313	Please ensure that the final version reflect the reviewer comments and the new information provided during the rebuttal	suggestion

2020-326	This paper proposes a novel method for learning Hamiltonian dynamics from data.	abstract
2020-326	The data is obtained from systems subjected to an external control signal.	abstract
2020-326	The authors show the utility of their method for subsequent improved control in a reinforcement learning setting.	abstract
2020-326	The paper is well written, the method is derived from first principles, and the experimental validation is solid.	strength
2020-326	The authors were also able to take into account the reviewers' feedback and further improve their paper during the discussion period.	rebuttal_process
2020-326	Overall all of the reviewers agree that this is a great contribution to the field and hence I am happy to recommend acceptance.	decision

2020-327	Main content: <sep> Blind review #3 summarizes it well: <sep> This paper presents results on Dictionary Learning through l4 maximization.	abstract
2020-327	The authors base this paper heavily off of the formulation and algorithm in Zhai et al (2019) "Complete dictionary learning via l4-norm maximization over the orthogonal group".	abstract
2020-327	The paper draws connections between complete dictionary learning, PCA, and ICA by pointing out similarities between the objectives functions that are optimized as well as the algorithms used.	abstract
2020-327	The paper further presents results on dictionary learning in the presence of different types of noise (AWGN, sparse corruptions, outliers) and show that the l4 objective is robust to different types of noise.	abstract
2020-327	Finally the authors apply different types of noise to synthetic and real images and show that the dictionaries that they learn are robust to the noise applied. <sep>	abstract
2020-327	-- <sep>	misc
2020-327	Discussion: <sep> Reviews agree about the interesting work, including the connections of complete dictionary learning with classic PCA and ICA (after further clarification during the rebuttal period).	rebuttal_process
2020-327	Additional empirical strengthening during the rebuttal period also addressed a reviewer concern. <sep>	rebuttal_process
2020-327	-- <sep>	misc
2020-327	Recommendation and justification: <sep> As review #3 wrote, "Overall this paper makes significant contributions by extending the work in [Zhai et al's (2019) "Complete dictionary learning via l4-norm maximization over the orthogonal group"] to noisy dictionary learning settings".	strength

2020-335	Double coúnterfactual regret minimization is an extension of neural counterfactual regret minimization that uses separate policy and regret networks (reminiscent of similar extensions of the basic RL formula in reinforcement learning).	abstract
2020-335	Several new algorithmic modifications are added to improve the performance. <sep>	abstract
2020-335	The reviewers agree that this paper is novel, sound, and interesting.	strength
2020-335	One of the reviewers had a set of questions that the authors responded to, seemingly satisfactorily.	rebuttal_process
2020-335	Given that this seems to be a high-quality paper with no obvious issues, it should be accepted.	decision

2020-360	This paper investigates the tasks used to pretrain language models.	abstract
2020-360	The paper proposes not using a generative tasks ('filling in' masked tokens), but instead a discriminative tasked (recognising corrupted tokens).	abstract
2020-360	The authors empirically show that the proposed method leads to improved performance, especially in the "limited compute" regime. <sep>	abstract
2020-360	Initially, the reviewers had quite split opinions on the paper, but after the rebuttal and discussion phases all reviewers agreed on an "accept" recommendation.	rating_summary
2020-360	I am happy to agree with this recommendation based on the following observations: <sep>	decision
2020-360	- The authors provide strong empirical results including relevant ablations.	strength
2020-360	Reviews initially suggested a limitation to classification tasks and a lack of empirical analysis, but those issues have been addressed in the updated version. <sep>	rebuttal_process
2020-360	- The problem of pre-training language model is relevant for the ML and NLP communities, and it should be especially relevant for *CONF*.	strength
2020-360	The resulting method significantly outperforms existing methods, especially in the low compute regime. <sep>	strength
2020-360	- The idea is quite simple, but at the same time it seems to be a quite novel idea.	strength

2020-396	This work introduces a new neural network model that can represent hyperedges of variable size, which is experimentally shown to improve or match the state of the art on several problems. <sep>	abstract
2020-396	Both reviewers were in favor of acceptance given the method's strong performance, and had their concerns resolved by the rebuttals and the discussion.	rating_summary
2020-396	I am therefore recommending acceptance.	decision

2020-398	The paper shows the relationship between node embeddings and structural graph representations.	abstract
2020-398	By careful definition of what structural node representation means, and what node embedding means, using the permutation group, the authors show in Theorem 2 that node embeddings cannot represent any extra information that is not already in the structural representation.	abstract
2020-398	The paper then provide empirical experiments on three tasks, and show in a fourth task an illustration of the theoretical results. <sep>	abstract
2020-398	The reviewers of the paper scored the paper highly, but with low confidence.	rating_summary
2020-398	I read the paper myself (unfortunately not with a lot of time), with the aim of increasing the confidence of the resulting decision.	misc
2020-398	The main gap in the paper is between the phrases "structural node representation" and "node embedding", and their theoretical definitions.	weakness
2020-398	The analogy of distribution and its samples follows unsurprisingly from the definitions (8 and 12), but the interpretation of those definitions as the corresponding English phrases is not obvious by only looking at the definitions.	weakness
2020-398	There also seems to be a sleight of hand going on with the most expressive representations (Definitions 9 and 11), which is used to make the conditional independence statement of Theorem 2.	weakness
2020-398	The authors should clarify in the final version whether the existence of such a representation can be shown, or even better a constructive way to get it from data. <sep>	suggestion
2020-398	Given the significance of the theoretical results, the authors should improve the introduction of the two main concepts by: <sep>	suggestion
2020-398	- relating them to prior work (one way is to move Section 5 towards the front) <sep>	suggestion
2020-398	- explaining in greater detail why Definitions 8 and 12 correspond to the two concepts.	suggestion
2020-398	For example expanding the part of the proof of Corollary 1 about SVD, to make clear what Definition 12 means. <sep>	suggestion
2020-398	- a corresponding simple example of Definition 8 to relate to a classical method. <sep>	suggestion
2020-398	The paper provides a nice connection between two disparate concepts.	strength
2020-398	Unfortunately, the connection uses graph invariance and equivariance, which is unfamiliar to many of the *CONF* audience.	weakness
2020-398	On balance, I believe that the authors can improve the presentation such that a reader can understand the implications of the connection without being an expert in graph isomorphism.	suggestion
2020-398	As such, I am recommending an accept.	decision

2020-406	This paper extends the information bottleneck method to the unsupervised representation learning under the multi-view assumption.	abstract
2020-406	The work couples the multi-view InfoMax principle with the information bottleneck principle to derive an objective which encourages the representations to contain only the information shared by both views and thus eliminate the effect of independent factors of variations.	abstract
2020-406	Recent advances in estimating lower-bounds on mutual information are applied to perform approximate optimisation in practice.	abstract
2020-406	The authors empirically validate the proposed approach in two standard multi-view settings. <sep>	abstract
2020-406	Overall, the reviewers found the presentation clear, and the paper well written and well motivated.	strength
2020-406	The issues raised by the reviewers were addressed in the rebuttal and we feel that the work is well suited for *CONF*.	decision
2020-406	We ask the authors to carefully integrate the detailed comments from the reviewers into the manuscript.	suggestion
2020-406	Finally, the work should investigate and briefly establish a connection to [1]. <sep>	suggestion
2020-406	[1] Wang et al "Deep Multi-view Information Bottleneck".	misc
2020-406	International Conference on Data Mining 2019 (https://epubs.siam.org/doi/pdf/10.1137/1.9781611975673.5)	misc

2020-442	This paper proposes a new self-supervised pre-trained speech model that improves speech recognition performance. <sep>	abstract
2020-442	The idea combines an earlier pre-training approach (wav2vec) with discretization followed by BERT-style masked reconstruction.	abstract
2020-442	The result is a fairly complex approach, with not too much novelty but with a good amount of engineering and analysis, and ultimately very good performance.	strength
2020-442	The reviewers agree that the work deserves publication at *CONF*, and the authors have addressed some of the reviewer concerns in their revision.	rating_summary
2020-442	The complexity of the approach may mean that it is not immediately widely adopted by others, but it is a good proof of concept and may well inspire other related work.	strength
2020-442	I believe the *CONF* community will find this work interesting.	strength

2020-465	The paper discusses smooth market games and demonstrate the merit of the approach.	abstract
2020-465	The reviewers agree on the quality of the paper, and the comments have been addressed well by the authors.	rebuttal_process

2020-481	This paper addresses the problem of many-to-many cross-domain mapping tasks with a double variational auto-encoder architecture, making use of the normalizing flow-based priors. <sep>	abstract
2020-481	Reviewers and AC unanimously agree that it is a well written paper with a solid approach to a complicated real problem supported by good experimental results.	strength
2020-481	There are still some concerns with confusing notations, and with human study to further validate their approach, which should be addressed in a future version. <sep>	weakness
2020-481	I recommend acceptance.	decision

2020-489	The submission presents a differentiable take on classic active contour methods, which used to be popular in computer vision.	abstract
2020-489	The method is sensible and the results are strong.	strength
2020-489	After the revision, all reviewers recommend accepting the paper.	rating_summary

2020-491	The article studies the role of the activation function in learning of 2 layer overparaemtrized networks, presenting results on the minimum eigenvalues of the Gram matrix that appears in this type of analysis and which controls the rate of convergence.	abstract
2020-491	The article makes numerous observations contributing to the development of principles for the design of activation functions and a better understanding of an active area of investigation as is convergence in overparametrized nets.	abstract
2020-491	The reviewers were generally positive about this article.	rating_summary

2020-495	The authors introduce an RL algorithm / architecture for partially observable environments. <sep>	abstract
2020-495	At the heart of it is a filtering algorithm based on a differentiable version of sequential Monte Carlo inference. <sep>	abstract
2020-495	The inferred particles are fed into a policy head and the whole architecture is trained by RL. <sep>	abstract
2020-495	The proposed methods was evaluated on multiple environments and ablations establish that all moving parts are necessary for the observed performance.<sep>	abstract
2020-495	All reviewers agree that this is an interesting contribution for addressing the important problem of acting in POMDPs.<sep>	strength
2020-495	I think this paper is well above acceptance threshold.	decision
2020-495	However, I have a few points that I would quibble with: <sep> 1) I don't see how the proposed trampling is fully differentiable; as far as I understand it, no credit is assigned to the discrete decision which particle to reuse.	weakness
2020-495	Adding a uniform component to the resampling distribution does not make it fully differentiable, see eg [Filtering Variational Objectives.	weakness
2020-495	Maddison et al].	misc
2020-495	I think the authors might use a form of straight-through gradient approximation. <sep>	suggestion
2020-495	2) Just stating that unsupervised losses might incentivise the filter to learn the wrong things, and just going back to plain RL loss is not in itself a novel contribution; in extremely sparse reward settings, this will not be satisfactory.	weakness

2020-497	The paper presents a method that unifies classification-based approaches for outlier detection and (one-class) anomaly detection.	abstract
2020-497	The paper also extends the applicability to non-image data. <sep>	abstract
2020-497	In the end, all the reviewers agreed that the paper makes a valuable contribution and I'm happy to recommend acceptance.	decision

2020-503	The paper considers representational aspects of neural tangent kernels (NTKs).	abstract
2020-503	More precisely, recent literature on overparametrized neural networks has identified NTKs as a way to characterize the behavior of gradient descent on wide neural networks as fitting these types of kernels.	abstract
2020-503	This paper focuses on the representational aspect: namely that functions of appropriate "complexity" can be written as an NTK with parameters close to initialization (comparably close to what results on gradient descent get). <sep>	abstract
2020-503	The reviewers agree this content is of general interest to the community and with the proposed revisions there is general agreement that the paper has merits to recommend acceptance.	rating_summary

2020-528	This paper describes a new method for creating word embeddings that can operate on corpora from more than one language.	abstract
2020-528	The algorithm is simple, but rivals more complex approaches. <sep>	abstract
2020-528	The reviewers were happy with this paper.	misc
2020-528	They were also impressed that the authors ran the requested multi-lingual BERT experiments, even though they did not show positive results.	strength
2020-528	One reviewer did think that non-contextual word embeddings were of less interest to the NLP community, but thought your arguments for the computational efficiency were convincing.	strength

2020-534	The paper extends LISTA by introducing gain gates and overshoot gates, which respectively address underestimation of code components and compensation of small step size of LISTA.	abstract
2020-534	The authors theoretically analyze these extensions and backup the effectiveness of their proposed algorithm with encouraging empirical results.	abstract
2020-534	All reviewers are highly positive on the contributions of this paper, and appreciate the rigorous theory which is further supported by convincing experiments.	strength
2020-534	All three reviewers recommended accept.	rating_summary

2020-535	The paper provides a theoretical analysis of graph neural networks, as the number of layers goes to infinity.	abstract
2020-535	For the graph convolutional network, they relate the expressive power of the network with the graph spectra.	abstract
2020-535	In particular for Erdos-Renyi graphs, they show that very deep graphs lose information, and propose a new weight normalization scheme based on this insight. <sep>	abstract
2020-535	The authors responded well to reviewer comments.	rebuttal_process
2020-535	It is nice to see that the open review nature has also resulted in a new connection.	rebuttal_process
2020-535	Unfortunately one of the reviewers did not engage further in the discussion with respect to the author rebuttals. <sep>	rebuttal_process
2020-535	Overall, the paper provides a nice theoretical analysis of a widely used graph neural network architecture, and characterises its behaviour on a popular class of graphs.	strength
2020-535	The fact that the theory provides a new approach for weight normalization is a bonus.	strength

2020-549	This is a mostly theoretical paper concerning online and stochastic optimization for convex loss functions that are not Lipschitz continuous.	abstract
2020-549	The authors propose a method for replacing the Lipschitz continuity condition with a more general Riemann-Lipschitz continuity condition, under which they are able to provide regret bounds for the online mirror descent algorithm, as well as extending to the stochastic setting.	abstract
2020-549	They follow up by evaluating their algorithm on Poisson inverse problems. <sep>	abstract
2020-549	The reviewers all agree that this is a well-written paper that makes a clear contribution.	strength
2020-549	To the best of our knowledge, the theory and derivations are correct, and the authors were highly responsive to reviewers' (minor) comments.	rebuttal_process
2020-549	I'm therefore happy to recommend acceptance.	decision

2020-550	This paper presents new non-linearity function which specially affects regions of the model which are densely valued.	abstract
2020-550	The non-linearity is simple: it retains only top-k highest units from the input, while truncating the rest to zero.	abstract
2020-550	This also makes the models more robust to adversarial defense which depend on the gradients.	abstract
2020-550	The non-linearity function is shown to have better adversarial robustness on CIFAR-10 and SVHN datasets.	abstract
2020-550	The paper also presents theoretical analysis for why the non-linearity is a good function. <sep>	abstract
2020-550	The authors have already incorporated major suggestions by the reviewers and the paper can make significant impact on the community.	rebuttal_process
2020-550	Thus, I recommend its acceptance.	decision

2020-551	This paper describes a new language model that captures both the position of words, and their order relationships.	abstract
2020-551	This redefines word embeddings (previously thought of as fixed and independent vectors) to be functions of position.	abstract
2020-551	This idea is implemented in several models (CNN, RNN and Transformer NNs) to show improvements on multiple tasks and datasets. <sep>	abstract
2020-551	One reviewer asked for additional experiments, which the authors provided, and which still supported their methodology.	rebuttal_process
2020-551	In the end, the reviewers agreed this paper should be accepted.	rating_summary

2020-552	This paper proposes a novel differentiable digital signal processing in audio synthesis.	abstract
2020-552	The application is novel and interesting.	strength
2020-552	All the reivewers agree to accept it.	rating_summary
2020-552	The authors are encouraged to consider the reviewer's suggestions to revise the paper.	suggestion

2020-554	The paper studies theoretical properties of ridge regression, and in particular how to correct for the bias of the estimator. <sep>	abstract
2020-554	The reviewers appreciated the contribution and the fact that you updated the manuscript to make it clearer. <sep>	rebuttal_process
2020-554	I however advise the authors to think about the best way to maximize impact for the *CONF* audience, perhaps by providing relevant examples from the ML literature.	suggestion

2020-555	This paper aims to study the mean and variance of the neural tangent kernel (NTK) in a randomly initialized ReLU network.	abstract
2020-555	The purpose is to understand the regime where the width and depth go to infinity together with a fixed ratio.	abstract
2020-555	The paper does not have a lot of numerical experiments to test the mathematical conclusions.	weakness
2020-555	In the discussion the reviewers concurred that the paper is interesting and has nice results but raised important points regarding the fact that only the diagonal elements are studied.	weakness
2020-555	This I think is the major limitation of this paper.	misc
2020-555	Another issue raised was lack of experimental work validating the theory.	weakness
2020-555	Despite the limitations discussed above, overall I think this is an interesting and important area as it sheds light on how to move beyond the NTK regime.	strength
2020-555	I also think studying this limit is very important to better understanding of neural network training.	strength
2020-555	I recommend acceptance to *CONF*.	decision

2020-556	The paper introduces the concept of overfitting in meta learning and proposes some solutions to address this problem.	abstract
2020-556	Overall, this is a good paper.	strength
2020-556	It would be good if the authors could relate this work to meta learning approaches, which are based on hierarchical (Bayesian) modeling for learning a task embedding. <sep>	suggestion
2020-556	[1] Hausman et al (*CONF* 2018): Learning an Embedding Space for Transferable Robot Skills <sep>	misc
2020-556	https://openreview.net/pdf?id=rk07ZXZRb <sep>	misc
2020-556	[2] Saemundsson et al (UAI 2018): Meta Reinforcement Learning with Latent Variable Gaussian Processes <sep>	misc
2020-556	http://auai.org/uai2018/proceedings/papers/235. pdf	misc

2020-559	The paper addresses an important problem (preventing catastrophic forgetting in continual learning) through a novel approach based on the sliced Kramer distance.	abstract
2020-559	The paper provides a novel and interesting conceptual contribution and is well written.	strength
2020-559	Experiments could have been more extensive but this is very nice work and deserves publication.	decision

2020-560	This paper analyzes the weights associated with filters in CNNs and finds that they encode positional information (ie near the edges of the image).	abstract
2020-560	A detailed discussion and analysis is performed, which shows where this positional information comes from. <sep>	abstract
2020-560	The reviewers were happy with your paper and found it to be quite interesting.	strength
2020-560	The reviewers felt your paper addressed an important (and surprising!)	strength
2020-560	issue not previously recognized in CNNs.	strength

2020-573	The paper focuses on characterizing the expressiveness of graph neural networks.	abstract
2020-573	The reviewers were satisfied that the authors answered their questions suffciiently and uniformly agree that this is a strong paper that should be accepted.	rating_summary

2020-575	This paper proposes three modifications of BERT type models two of which is concerned with parameter sharing and one with a new auxiliary loss.	abstract
2020-575	New SOTA on downstream tasks are demonstrated. <sep>	abstract
2020-575	All reviewers liked the paper and so did a lot of comments. <sep>	misc
2020-575	Acceptance is recommended.	decision

2020-582	The method presented, the simplified action decoder, is a clever way of addressing the influence of exploratory actions in multi-agent RL.	abstract
2020-582	It's shown to enable state of the art performance in Hanabi, an interesting and relatively novel cooperative AI challenge.	strength
2020-582	It seems, however, that the method has wider applicability than that. <sep>	strength
2020-582	All reviewers agree that this is good and interesting work.	strength
2020-582	Reviewer 2 had some issues with the presentation of the results and certain assumptions, but the authors responded so as to alleviate any concerns. <sep>	rebuttal_process
2020-582	This paper should definitely be accepted, if possible as oral.	decision

2020-583	This paper presents a feature normalization method for CNNs by decorrelating channel-wise and spatial correlation simultaneously.	abstract
2020-583	Overall all reviewers are positive to the acceptance and I support their opinions.	decision
2020-583	The idea and implementation is relatively straightforward but well-motivated and reasonable.	strength
2020-583	Experiments are well-organized and intensive, providing enough evidence to convince its effectiveness in terms of final accuracy and convergence speed.	strength
2020-583	Also, it's analogy to biological center-surrounded structure is thought provoking.	strength
2020-583	The novelty of the method seems somewhat incremental considering that there already exists a channel-wise decorrelation method, but I think the findings of the paper are interesting and valuable enough for *CONF* community and would like to recommend acceptance. <sep>	decision
2020-583	Minor comments: I recommend authors to mention about zero-component analysis (ZCA) normalization, which has been a standard input normalization method for CIFAR datasets.	suggestion
2020-583	I guess it is quite similar to the proposed method considering 1x1 convolution.	suggestion
2020-583	Also, comparison with other recent normalization methods (eg, Group Norm) would be useful.	suggestion

2020-586	This paper presents an approach to model-based reinforcement learning in high-dimensional tasks.	abstract
2020-586	The approach involves learning a latent dynamics model, and performing rollouts thereof with an actor-critic model to learn behaviours.	abstract
2020-586	This is extensively evaluated on 20 visual control tasks. <sep>	abstract
2020-586	This paper was favourably received, but there were concerns around it being incremental (relative to PlaNet and SVG).	weakness
2020-586	The authors highlighted the differences in the rebuttal, clarifying the novelty of this work. <sep>	rebuttal_process
2020-586	Given the interesting ideas presented, and the convincing results, this paper should be accepted.	decision

2020-589	This paper presents a new benchmark for architecture search.	abstract
2020-589	Reviewers put this paper in the top tier.	rating_summary
2020-589	I encourage the authors to also cite https://openreview.net/forum?id=SJx9ngStPH in their final version.	suggestion

2020-594	This paper addresses to compress the network weights by quantizing their values to some fixed codeword vectors.	abstract
2020-594	The paper is well written, and is overall easy to follow.	strength
2020-594	The proposed algorithm is well-motivated, and easy to apply.	strength
2020-594	The method can be expected to perform well empirically, which the experiments verify, and to have potential impact.	strength
2020-594	On the other hand, the novelty is not very high, though this paper uses these existing techniques in a different setting.	weakness

2020-598	This paper presents an approach for interactive imitation learning while avoiding an adversarial optimization by using ensembles.	abstract
2020-598	The reviewers agreed that the contributions were significant and the results were compelling.	strength
2020-598	Hence, the paper should be accepted.	decision

2020-599	The authors propose stable rank normalization, which minimizes the stable rank of a linear operator and apply this to neural network training.	abstract
2020-599	The authors present techniques for performing the normalization efficiently and evaluate it empirically in a range of situations.	abstract
2020-599	The only issues raised by reviewers related to the empirical evaluation.	weakness
2020-599	The authors addressed these in their revisions.	rebuttal_process

2020-600	Main content: <sep> This paper provides a unified way to provide robust statistics in evaluating the reliability of RL algorithms, especially deep RL algorithms.	abstract
2020-600	Though the metrics are not particularly novel, the investigation should be useful to the broader community as it compares seven specific evaluation metrics, including 'Dispersion across Time (DT): IQR across Time', 'Short-term Risk across Time (SRT): CVaR on Differences', 'Long-term Risk across Time (LRT): CVaR on Drawdown', 'Dispersion across Runs (DR): IQR across Runs', 'Risk across Runs (RR): CVaR across Runs', 'Dispersion across Fixed-Policy Rollouts (DF): IQR across Rollouts' and 'Risk across Fixed-Policy Rollouts (RF): CVaR across Rollouts'.	abstract
2020-600	The paper further proposed ranking and also confidence intervals based on bootstrapped samples, and compared against continuous control and discrete actions algorithms on Atari and OpenAI Gym. <sep>	abstract
2020-600	-- <sep>	misc
2020-600	Discussion: <sep> The reviews clearly agree on accepting the paper, with a weak accept coming from a reviewer who does not know much about this subarea.	rating_summary
2020-600	Comments are mostly just directed at clarifications and completeness of description, which the authors have addressed. <sep>	rebuttal_process
2020-600	-- <sep>	misc
2020-600	Recommendation and justification: <sep> This paper should be accepted due to its useful contributions toward doing a better job of measuring performance of RL.	decision

2020-601	This paper proposes a sequential latent variable model for the knowledge selection task for knowledge grounded dialogues.	abstract
2020-601	Experimental results demonstrate improvements over the previous SOTA in the WoW, knowledge grounded dialogue dataset, through both automated and human evaluation.	abstract
2020-601	All reviewers scored the paper highly, but they also made several suggestions for improving the presentation.	rating_summary
2020-601	Authors responded positively to all these suggestions and provided updated results and other stats.	rebuttal_process
2020-601	The paper will be a good contribution to *CONF*.	misc

2020-605	This paper carries out extensive experiments on Neural Tangent Kernel (NTK) --kernel methods based on infinitely wide neural nets on small-data tasks.	abstract
2020-605	I recommend acceptance.	decision

2020-606	This paper proposes a method for efficiently training neural networks combined with blackbox implementations of exact combinatorial solvers. <sep>	abstract
2020-606	Reviewers and AC agree that it is a well written paper with a novel idea supported by good experimental results.	strength
2020-606	Experimental results are of small scale and can be further improved, but the authors acknowledged this aspect well. <sep>	rebuttal_process
2020-606	Hence, I recommend acceptance.	decision

2020-607	This paper presents an approach for scalable autoregressive video generation based on a three-dimensional self-attention mechanism.	abstract
2020-607	As rightly pointed out by R3, the proposed approach 'is individually close to ideas proposed elsewhere before in other forms ... but this paper does the important engineering work of selecting and combining these ideas in this specific video synthesis problem setting.' <sep>	strength
2020-607	The proposed method is relevant and well-motivated, and the experimental results are strong.	strength
2020-607	All reviewers agree that experiments on the Kinetics dataset are particularly appealing.	strength
2020-607	In the initial evaluation, the reviewers have raised several concerns such as performance metrics, ablation study, training time comparison, empirical evaluation of the baseline methods on Kinetics, that were addressed by the authors in the rebuttal. <sep>	rebuttal_process
2020-607	In conclusion, all three reviewers were convinced by the author's rebuttal, and AC recommends acceptance of this paper – congratulations to the authors!	decision

2020-608	This is a very interesting paper which discusses practical issues and solutions around deploying RL on real physical robotic systems, specifically involving questions on the use of raw sensory data, crafting reward functions, and not having resets at the end of episodes. <sep>	abstract
2020-608	Many of the issues raised in the reviews and discussion were concerned with experimental details and settings, as well as relation to different areas of related work.	rebuttal_process
2020-608	These were all sufficiently handled in the rebuttal, and all reviewers were in favour of acceptance.	rating_summary

2020-609	This paper explores the idea of using meta-learning for acquisition functions.	abstract
2020-609	It is an interesting and novel research direction with promising results. <sep>	strength
2020-609	The paper could be strengthened by adding more insights about the new acquisition function and performing more comparisons eg to Chen et al 2017.	suggestion
2020-609	But in any case, the current form of the paper should already be of high interest to the community	strength

2020-614	This paper proposes a framework which qualifies how well given neural architectures can perform on reasoning tasks.	abstract
2020-614	From this, they show a number of interesting empirical results, including the ability of graph neural network architectures for learn dynamic programming. <sep>	abstract
2020-614	This substantial theoretical and empirical study impressed the reviewers, who strongly lean towards acceptance.	rating_summary
2020-614	My view is that this is exactly the sort of work we should be show-casing at the conference, both in terms of focus, and of quality.	decision
2020-614	I am happy to recommend this for acceptance.	decision

2020-617	This paper studies Graph Neural Networks for quantum chemistry by incorporating a number of physics-informed innovations into the architecture.	abstract
2020-617	In particular, it considers directional edge information while preserving equivariance. <sep>	abstract
2020-617	Reviewers were in agreement that this is an excellent paper with strong empirical results, great empirical evaluation and clear exposition.	strength
2020-617	Despite some concerns about the limited novelty in terms of GNN methodology ( for instance, directional message passing has appeared in previous GNN papers, see eg https://openreview.net/forum?id=H1g0Z3A9Fm , in a different context).	weakness
2020-617	Ultimately, the AC believes this is a strong, high quality work that will be of broad interest, and thus recommends acceptance.	decision

2020-618	This paper presents an idea for interpolating between two points in the decision-space of a black-box classifier in the image-space, while producing plausible images along the interpolation path.	abstract
2020-618	The presentation is clear and the experiments support the premise of the model. <sep>	strength
2020-618	While the proposed technique can be used to help understanding how a classifier works, I have strong reservations in calling the generated samples "explanations".	weakness
2020-618	In particular, there is no reason for the true explanation of how the classifier works to lie in the manifold of plausible images.	weakness
2020-618	This constraint is more of a feature to please humans rather than to explain the geometry of the decision boundary. <sep>	weakness
2020-618	I believe this paper will be well-received and I suggested acceptance, but I believe it will be of limited usefulness for robust understanding of the decision boundary of classifiers.	decision

2020-620	The paper considers the problem of training neural networks asynchronously, and the gap in generalization due to different local minima being accessible with different delays.	abstract
2020-620	The authors derive a theoretical model for the delayed gradients, which provide prescriptions for setting the learning rate and momentum. <sep>	abstract
2020-620	All reviewers agreed that this a nice paper with valuable theoretical and empirical contributions.	strength

2020-625	The authors take a closer look at widely held beliefs about neural networks.	abstract
2020-625	Using a mix of analysis and experiment, they shed some light on the ways these assumptions break down.	abstract
2020-625	The paper contributes to our understanding of various phenomena and their connection to generalization, and should be a useful paper for theoreticians searching for predictive theories.	strength

2020-628	The paper proposes a new way to train latent variable models.	abstract
2020-628	The standard way of training using the ELBO produces biased estimates for many quantities of interest.	abstract
2020-628	The authors introduce an unbiased estimate for the log marginal probability and its derivative to address this.	abstract
2020-628	The new estimator is based on the importance weighted autoencoder, correcting the remaining bias using russian roulette sampling.	abstract
2020-628	The model is empirically shown to give better test set likelihood, and can be used in tasks where unbiased estimates are needed. <sep>	abstract
2020-628	All reviewers are positive about the paper.	rating_summary
2020-628	Support for the main claims is provided through empirical and theoretical results.	strength
2020-628	The reviewers had some minor comments, especially about the theory, which the authors have addressed with additional clarification, which was appreciated by the reviewers. <sep>	rebuttal_process
2020-628	The paper was deemed to be well organized.	strength
2020-628	There were some unclarities about variance issues and bias from gradient clipping, which have been addressed by the authors in additional explanation as well as an additional plot. <sep>	rebuttal_process
2020-628	The approach is novel and addresses a very relevant problem for the *CONF* community: optimizing latent variable models, especially in situations where unbiased estimates are required.	strength
2020-628	The method results in marginally better optimization compared to IWAE with much smaller average number of samples.	strength
2020-628	The method was deemed by the reviewers to open up new possibilities such as entropy minimization.	strength

2020-629	The paper presents a deep learning approach for tasks such as symbolic integration and solving differential equations. <sep>	abstract
2020-629	The reviewers were positive and the paper has had extensive discussion, which we hope has been positive for the authors. <sep>	rating_summary
2020-629	We look forward to seeing the engagement with this work at the conference.	decision

2020-631	Main content: <sep> Blind review #1 summarizes it well: <sep> The paper proposes an algorithmic improvement that significantly simplifies training of energy-based models, such as the Restricted Boltzmann Machine.	abstract
2020-631	The key issue in training such models is computing the gradient of the log partition function, which can be framed as computing the expected value of f(x) = dE(x; theta) / d theta over the model distribution p(x).	abstract
2020-631	The canonical algorithm for this problem is Contrastive Divergence which approximates x ~ p(x) with k steps of Gibbs sampling, resulting in biased gradients.	abstract
2020-631	In this paper, the authors apply the recently introduced unbiased MCMC framework of Jacob et al to completely remove the bias.	abstract
2020-631	The key idea is to (1) rewrite the expectation as a limit of a telescopic sum: E f(x_0) + \\sum_t E f(x_t) - E f(x_{t-1}); (2) run two coupled MCMC chains, one for the "positive" part of the telescopic sum and one for the "negative" part until they converge.	abstract
2020-631	After convergence, all remaining terms of the sum are zero and we can stop iterating.	abstract
2020-631	However, the number of time steps until convergence is now random. <sep>	abstract
2020-631	Other contributions of the paper are: <sep> 1. Proof that Bernoulli RBMs and other models satisfying certain conditions have finite expected number of steps and finite variance of the unbiased gradient estimator. <sep>	strength
2020-631	2. A shared random variables method for the coupled Gibbs chains that should result in faster convergence of the chains. <sep>	strength
2020-631	3. Verification of the proposed method on two synthetic datasets and a subset of MNIST, demonstrating more stable training compared to contrastive divergence and persistent contrastive divergence. <sep>	strength
2020-631	-- <sep>	misc
2020-631	Discussion: <sep> The main objection in reviews was to have meaningful empirical validation of the strong theoretical aspect of the paper, which the authors did during the rebuttal period to the satisfaction of reviewers. <sep>	rebuttal_process
2020-631	-- <sep>	misc
2020-631	Recommendation and justification: <sep> As review #1 said, "I am very excited about this paper and strongly support its acceptance, since the proposed method should revitalize research in energy-based models.	rating_summary

2020-632	This paper explores several embedding models (Skip-gram, BERT, XLNet) and describes a framework for comparing, and in the end, unifying them.	abstract
2020-632	The framework is such that it actually suggests new ways of creating embeddings, and draws connections to methodology from computer vision. <sep>	abstract
2020-632	One of the reviewers had several questions about the derivations in your paper and was worried about the paper's clarity.	weakness
2020-632	But all of the reviewers appreciated the contributions of the paper, which joins multiple seemingly disparite models under into one theoretical framework. <sep>	strength
2020-632	The reviewers were positive about the paper, and in particular were happy to see the active response of authors to their questions and willingness to update the paper with their suggested improvements.	rebuttal_process

2020-634	The article is concerned with depth width tradeoffs in the representation of functions with neural networks.	abstract
2020-634	The article presents connections between expressivity of neural networks and dynamical systems, and obtains lower bounds on the width to represent periodic functions as a function of the depth.	abstract
2020-634	These are relevant advances and new perspectives for the theoretical study of neural networks.	abstract
2020-634	The reviewers were very positive about this article.	rating_summary
2020-634	The authors' responses also addressed comments from the initial reviews.	rebuttal_process

2020-635	This paper focuses on studying the double descent phenomenon in a one layer neural network training in an asymptotic regime where various dimensions go to infinity together with fixed ratios.	abstract
2020-635	The authors provide precise asymptotic characterization of the risk and use it to study various phenomena.	abstract
2020-635	In particular they characterize the role of various scales of the initialization and their effects.	abstract
2020-635	The reviewers all agree that this is an interesting paper with nice contributions.	strength
2020-635	I concur with this assessment.	strength
2020-635	I think this is a solid paper with very precise and concise theory.	strength
2020-635	I recommend acceptance.	decision

2020-636	The paper introduces a generative approach to reconstruct 3D images for cryo-electron microscopy (cryo-EM). <sep>	abstract
2020-636	All reviewers really liked the paper, appreciate the challenging problem tackled and the proposed solution. <sep>	strength
2020-636	Acceptance is therefore recommended.	decision

2020-637	This paper proposes a novel way to learn hierarchical disentangled latent representations by building on the previously published Variational Ladder AutoEncoder (VLAE) work.	abstract
2020-637	The proposed extension involves learning disentangled representations in a progressive manner, from the most abstract to the more detailed.	abstract
2020-637	While at first the reviewers expressed some concerns about the paper, in terms of its main focus (whether it was the disentanglement or the hierarchical aspect of the learnt representation), connections to past work, and experimental results, these concerns were fully alleviated during the discussion period.	rebuttal_process
2020-637	All of the reviewers now agree that this is a valuable contribution to the field and should be accepted to *CONF*.	rating_summary
2020-637	Hence, I am happy to recommend this paper for acceptance as an oral.	decision

2020-641	The paper efficiently computes quantities, such as variance estimates of the gradient or various Hessian approximations, jointly with the gradient, and the paper also provides a software package for this.	abstract
2020-641	All reviewers agree that this is a very good paper and should be accepted.	rating_summary

2020-643	All the reviewers agreed that this was a sensible application of mostly existing ideas from standard neural net initialization to the setting of hypernetworks.	strength
2020-643	The main criticism was that this method was used to improve existing applications of hypernets, instead of extending their limits of applicability.	weakness

2020-644	This manuscript analyzes the convergence of federated learning wit hstragellers, and provides convergence rates.	abstract
2020-644	The proof techniques involve bounding the effects of the non-identical distribution due to stragglers and related issues.	abstract
2020-644	The manuscript also includes a thorough empirical evaluation.	abstract
2020-644	Overall, the reviewers were quite positive about the manuscript, with a few details that should be improved.	rating_summary

2020-645	This paper addresses the problem of poor generation quality in models for text generation that results from the use of the maximum likelihood (ML) loss, in particular the fact that the ML loss does not differentiate between different "incorrect" generated outputs (ones that do not match the corresponding training sequence).	abstract
2020-645	The authors propose to train text generation models with an additional loss term that measures the distance from the ground truth via a Gaussian distribution based on embeddings of the ground-truth tokens.	abstract
2020-645	This is not the first attempt to address drawbacks of ML training for text generation, but it is simple and intuitive, and produces improvements over the state of the art on a range of tasks.	strength
2020-645	The reviewers are all quite positive, and are in agreement that the author responses and revisions have improved the paper quality and addressed initial concerns.	rebuttal_process
2020-645	I think this work will be broadly appreciated by the *CONF* audience.	strength
2020-645	One negative point is that the writing quality still needs improvement.	weakness

2020-646	This paper presents an approach to learn state representations of the scene as well as their action-conditioned transition model, applying contrastive learning on top of a graph neural network.	abstract
2020-646	The reviewers unanimously agree that this paper contains a solid research contribution and the authors' response to the reviews further clarified their concerns.	rebuttal_process

2020-647	The authors develop a strategy to learn branching strategies for branch-and-bound based neural network verification algorithms, based on GNNs that imitate strong branching.	abstract
2020-647	This allows the authors to obtain significant speedups in branch and bound based neural network verification algorithms relative to strong baselines considered in prior work. <sep>	abstract
2020-647	The reviewers were in consensus and the quality of the paper and minor concerns raised in the initial reviews were adequately addressed in the rebuttal phase. <sep>	rebuttal_process
2020-647	Therefore, I strongly recommend acceptance.	decision

2020-648	Gradient clipping is increasingly popular and it's nice to see a paper theoretically exploring its nice performance.	strength
2020-648	All reviewers appreciated the work and the results. <sep>	strength
2020-648	Please make sure to incorporate all of their comments for the final version.	suggestion

2020-650	This paper presents a new twist on the typical LSTM that applies several rounds of gating on the history and input, with the end result that the LSTM's transition function is effectively context-dependent.	abstract
2020-650	The performance of the model is illustrated on several datasets. <sep>	abstract
2020-650	In general, the reviews were positive, with one score being upgraded during the rebuttal period.	rating_summary
2020-650	One of the reviewers complained that the baselines were not adequate, but in the end conceded that the results were still worthy of publication. <sep>	rating_summary
2020-650	One reviewer argued very hard for the acceptance of this paper "Papers that are as clear and informative as this one are few and far between.	rating_summary
2020-650	.. As such, I vehemently argue in favor of this paper being accepted to *CONF*.	decision

2020-651	The paper is extremely well-written with a clear motivation (Section 1).	strength
2020-651	The approach is novel.	strength
2020-651	But I think the paper's biggest strength is in its very thorough experimental investigation.	strength
2020-651	Their approach is compared to other very recent speech discretization methods on the same data using the same (ABX) evaluation metric.	abstract
2020-651	But the work goes further in that it systematically attempts to actually understand what types of structures are captured in the intermediate discrete layers, and it is able to answer this question convincingly.	strength
2020-651	Finally, very good results on standard benchmarks are achieved. <sep>	strength
2020-651	To authors: Please do include the additional discussions and results in the final paper.	suggestion

2020-652	This paper proposes a novel method for considering translations in both directions within the framework of generative neural machine translation, significantly improving accuracy. <sep>	abstract
2020-652	All three reviewers appreciated the paper, although they noted that the gains were somewhat small for the increased complexity of the model.	weakness
2020-652	Nonetheless, the baselines presented are already quite competitive, so improvements on these datasets are likely to never be extremely large. <sep>	strength
2020-652	Overall, I found this to be a quite nice paper, and strongly recommend acceptance, perhaps as an oral presentation.	decision

2020-655	This is a very interesting paper on unsupervised skill learning based on the predictability of skill effects, with the incorporation of these ideas into model-based RL. <sep>	abstract
2020-655	This is a clear accept, based on the clarity of the ideas presented and the writing, as well as the thorough and convincing experiments.	decision

2020-658	The paper shows empirical evidence that the the optimal action-value function Q* often has a low-rank structure.	abstract
2020-658	It uses ideas from the matrix estimation/completion literature to provide a modification of value iteration that benefits from such a low-rank structure. <sep>	abstract
2020-658	The reviewers are all positive about this paper.	rating_summary
2020-658	They find the idea novel and the writing clear. <sep>	strength
2020-658	There have been some questions about the relation of this concept of rank to other definitions and usage of rank in the RL literature. <sep>	weakness
2020-658	The authors' rebuttal seem to be satisfactory to the reviewers.	rebuttal_process
2020-658	Given these, I recommend acceptance of this paper.	decision

2020-660	This paper's contribution is twofold: 1) it proposes a new meta-RL method that leverages off-policy meta-learning by importance weighting, and 2) it demonstrates that current popular meta-RL benchmarks don't necessarily require meta-learning, as a simple non-meta-learning algorithm (TD3) conditioned on a context variable of the trajectory is competitive with SoTA meta-learning approaches. <sep>	abstract
2020-660	The reviewers all agreed that the approach is interesting and the contributions are significant.	strength
2020-660	I'd like to thank the reviewers for engaging in a spirited discussion about this paper, both with each other and with the authors.	misc
2020-660	There was also a disagreement about the semantics of whether the approach can be classified as "meta-learning", but in my opinion this argument is orthogonal to the practical contributions.	rebuttal_process
2020-660	After the revisions and rebuttal, reviewers agreed that the paper was improved and increased their ratings as a result, with all recommending accept. <sep>	rating_summary
2020-660	There's a good chance this work will make an impactful contribution to the field of meta-reinforcement learning and therefore I recommend it for an oral presentation.	decision

2020-661	This paper was very well received by the reviewers with solid Accept ratings across the board. <sep>	rating_summary
2020-661	The subject matter is quite interesting -  mathematical reasoning in latent space, and it was suggested by a reviewer that this could be a good candidate for an oral.	strength
2020-661	The AC agrees and recommends acceptance as an oral.	decision
2020-661	Some of the intuitions of what is being done in this paper could be better visualized and presented and I encourage the authors to think carefully about how to present this work if an oral presentation is granted by the PCs.	suggestion

2020-663	This paper investigates the use non-convex optimization for two dictionary learning problems, ie, over-complete dictionary learning and convolutional dictionary learning.	abstract
2020-663	The paper provides theoretical results, associated with empirical experiments, about the fact that, that when formulating the problem as an l4 optimization, gives rise to a landscape with strict saddle points and as such, they can be escaped with negative curvature.	abstract
2020-663	As a result, descent methods can be used for learning with provable guarantees.	abstract
2020-663	All reviews found the work extremely interesting, highlighting the importance of the results that constitute "a solid improvement over the prior understandings on over-complete DL" and "extends our understanding of provable methods for dictionary learning".	strength
2020-663	This is an interesting submission on non-convex optimization, and as such of interest to the ML community of *CONF* .	strength
2020-663	I'm recommending this work for acceptance.	decision

2020-664	The paper provides a simple method of active learning for classification using deep nets.	abstract
2020-664	The method is motivated by choosing examples based on an embedding computed that represents the last layer gradients, which is shown to have a connection to a lower bound of model change if labeled.	abstract
2020-664	The algorithm is simple and easy to implement.	strength
2020-664	The method is justified by convincing experiments. <sep>	strength
2020-664	The reviewers agree that the rebuttal and revisions cleared up any misunderstandings. <sep>	rebuttal_process
2020-664	This is a solid empirical work on an active learning technique that seems to have a lot of promise.	strength
2020-664	Accept.	decision

2020-665	This paper studies the properties of Differentiable Architecture Search, and in particular when it fails, and then proposes modifications that improve its performance for several tasks.	abstract
2020-665	The reviews were all very supportive with three Accept opinions, and authors have addressed their comments and suggestions.	rebuttal_process
2020-665	Given the unanimous reviews, this appears to be a clear Accept.	decision

2020-666	The paper empirically studies the behaviour of deep policy gradient algorithms, and reveals several unexpected observations that are not explained by the current theory.	abstract
2020-666	All three reviewers are excited about this work and recommend acceptance.	rating_summary

2020-667	This paper provides a careful and well-executed evaluation of the code-level details of two leading policy search algorithms, which are typically considered implementation details and therefore often unstated or brushed aside in papers.	abstract
2020-667	These are revealed to have major implications for the performance of both algorithms. <sep>	abstract
2020-667	The reviewers are all in agreement that this paper has important reproducibility and evaluation implications for the field, and adds substantially to our body of knowledge on policy gradient algorithms.	strength
2020-667	I therefore recommend it be accepted. <sep>	decision
2020-667	However, a serious limitation is that only 3 random seeds were used to get average performance in the first, key experiment.	weakness
2020-667	Experiments are expensive, but that result is not meaningful without more runs, and arguably could be misleading rather than informative.	weakness
2020-667	The authors should increase the number of runs as much as possible, at least to 10 but ideally more.	suggestion

2020-668	This work uses a variational autoencoder-based approach to combine the benefits of recent methods that learn policies with behavioral diversity with the advantages of successor representations, addressing the generalization and slow inference problems of competing methods such as DIAYN.	abstract
2020-668	After discussion of the author rebuttal, the reviewers all agreed on the significant contribution of the paper and that concerns about clarity were sufficiently addressed.	rebuttal_process
2020-668	Thus, I recommend this paper for acceptance.	decision

2020-669	The reviewers generally agreed that the paper presents a compelling method that addresses an important problem.	strength
2020-669	This paper should clearly be accepted, and I would suggest for it to be considered for an oral presentation. <sep>	decision
2020-669	I would encourage the authors to take into account the reviewers' suggestions (many of which were already addressed in the rebuttal period) and my own suggestion. <sep>	suggestion
2020-669	The main suggestion I would have in regard to improving the paper is to position it a bit more carefully in regard to prior work on Bayesian meta-learning.	suggestion
2020-669	This is an active research field, with quite a number of papers.	misc
2020-669	There are two that are especially close to the VI method that the authors are proposing: Gordon et al and Finn et al (2018).	weakness
2020-669	For example, the graphical model in Figure 2 looks nearly identical to the ones presented in these two prior papers, as does the variational inference procedure.	weakness
2020-669	There is nothing wrong with that, but it would be appropriate for the authors to discuss this prior work a bit more diligently -- currently the relationship to these prior works is not at all apparent from their discussion in the related work section.	weakness
2020-669	A more appropriate way to present this would be to begin Section 3.2 by stating that this framework follows prior work -- there is nothing wrong with building on prior work, and the significant and important contribution of this paper is no way diminished by being up-front about which parts are inspired by previous papers.	suggestion

2020-671	The paper investigates parallelizing MCTS. <sep>	abstract
2020-671	The authors propose a simple method based on only updating the exploration bonus in (P)-UCT by taking into account the number of currently ongoing / unfinished simulations. <sep>	abstract
2020-671	The approach is extensively tested on a variety of environments, notably including ATARI games.<sep>	abstract
2020-671	This is a good paper. <sep>	strength
2020-671	The approach is simple, well motivated and effective. <sep>	strength
2020-671	The experimental results are convincing and the authors made a great effort to further improve the paper during the rebuttal period. <sep>	rebuttal_process
2020-671	I recommend an oral presentation of this work, as MCTS has become a core method in RL and planning, and therefore I expect a lot of interest in the community for this work.	decision

2020-672	The paper presents a general view of supervised learning models that are jointly trained with a model for embedding the labels (targets), which the authors dub target-embedding autoencoders (TEAs).	abstract
2020-672	Similar models have been studied before, but this paper unifies the idea and studies more carefully various components of it.	abstract
2020-672	It provides a proof for the specific case of linear models and a set of experiments on disease trajectory prediction tasks.	abstract
2020-672	The reviewer concerns were addressed well by the authors and I believe the paper is now strong.	rebuttal_process
2020-672	It would be even stronger if it included more tasks (and in particular some "typical" tasks that more of the community is focusing on), and the theoretical part is to my mind not a major contribution, or at least not as large as the paper implies, because it analyzes a much simpler model than anyone is likely to use TEAs for.	suggestion

2020-675	This paper proposes an RL-based structure search method for causal discovery.	abstract
2020-675	The reviewers and AC think that the idea of applying reinforcement learning to causal structure discovery is novel and intriguing.	strength
2020-675	While there were initially some concerns regarding presentation of the results, these have been taken care of during the discussion period.	rebuttal_process
2020-675	The reviewers agree that this is a very good submission, which merits acceptance to *CONF*-2020.	rating_summary

2020-678	This paper combine recent ideas from capsule networks and group-equivariant neural networks to form equivariant capsules, which is a great idea.	abstract
2020-678	The exposition is clear and the experiments provide a very interesting analysis and results.	strength
2020-678	I believe this work will be very well received by the *CONF* community.	decision

2020-679	This paper analyzes and extends learning methods based on Policy-Spaced Response Oracles (PSRO) through the application of alpha-rank.	abstract
2020-679	In doing so, the paper explores connections with Nash equilibria, establishes convergence guarantees in multiple settings, and presents promising empirical results on (among other things) 3-to-5 player poker games. <sep>	abstract
2020-679	Although this paper originally received mixed scores, after the rebuttal period all reviewers converged to a consensus.	rebuttal_process
2020-679	A revised version also includes new experiments from the MuJoCo soccer domain, and new poker results as well.	rebuttal_process
2020-679	Overall, this paper provides a nice balance of theoretical support and practical relevance that should be of high impact to the RL community.	strength

2020-680	The authors design a GAN-based text-to-speech synthesis model that performs competitively with state-of-the-art synthesizers.	abstract
2020-680	The reviewers and I agree that this appears to be the first really successful effort at GAN-based synthesis.	strength
2020-680	Additional positives are that the model is designed to be highly parallelisable, and that the authors also propose several automatic measures of performance in addition to reporting human mean opinion scores.	strength
2020-680	The automatic measures correlate well (though far from perfectly) with human judgments, and in any case are a nice contribution to the area of evaluation of generative models.	strength
2020-680	It would be even more convincing if the authors presented human A/B forced-choice test results (in addition to the mean opinion scores), which are often included in speech synthesis evaluation, but this is a minor quibble.	suggestion

2020-681	The paper presents a framework for scalable Deep-RL on really large-scale architecture, which addresses several problems on multi-machine training of such systems with many actors and learners running.	abstract
2020-681	Large-scale experiments and impovements over IMPALA are presented, leading to new SOTA results.	abstract
2020-681	The reviewers are very positive over this work, and I think this is an important contribution to the overall learning / RL community.	rating_summary

2020-683	This paper presents Convolutional Conditional Neural Process (ConvCNP), a new member of the neural process family that models translation equivariance.	abstract
2020-683	Current models must learn translation equivariance from the data, and the authors show that ConvCNP can learn this as part of the model, which is much more generalisable and efficient.	abstract
2020-683	They evaluate the ConvCNP on several benchmarks, including an astronomical time-series modelling experiment, a sim2real experiment, and several image completion experiments and show excellent results.	abstract
2020-683	The authors wrote extensive responses the the reviewers, uploading a revised version of the paper, and there was some further discussion.	rebuttal_process
2020-683	This is a strong paper worthy of inclusion in *CONF* and could have a large impact on many fields in ML/AI.	decision

2020-684	This paper studies the implicit regularization of the gradient descent in homogeneous and shows that when the training loss falls below a threshold, then the smoothed.	abstract
2020-684	This study generalizes some of the earlier related works by relying on weaker assumptions.	abstract
2020-684	Experiments on MNIST and CIFAR-10 are provided to backup the theoretical findings of the paper. <sep>	abstract
2020-684	R2 had some concern about one of the assumptions in this work (A4).	weakness
2020-684	While authors admitted that (A4) may not hold for all neural networks and all datasets, they stressed that this assumptions is reasonable when the network is overparameterized and can perfectly fit the training data.	rebuttal_process
2020-684	Overall, all reviewers are very positive about this submission and find a valuable step toward understanding implicit regularization.	rating_summary

2020-685	The reviewers develop a novel technique for training neural networks that are provably robust to adversarial attacks, by combining provable defenses using convex relaxations with latent adversarial attacks that lie in the gap between the convex relaxation and the true realizable set of activations at a layer of the network.	abstract
2020-685	The authors show that the resulting procedure is computationally efficient and able to train neural networks to attain SOTA provable robustness to adversarial attacks. <sep>	abstract
2020-685	The paper is well written and clearly explains an interesting idea, backed by thorough experiments.	strength
2020-685	The reviewers were in consensus on acceptance and relatively minor concerns were clearly addressed in the rebuttal phase. <sep>	rating_summary
2020-685	Hence, I strongly recommend acceptance.	decision

2020-686	This paper proposes a novel architecture for question-answering, which is trained in an end-to-end fashion. <sep>	abstract
2020-686	The reviewers were unanimous in their vote to accept.	rating_summary
2020-686	Authors are encouraged to revise addressing reviewer comments.	suggestion

2020-1127	This paper proposes an abstractive text summarization model that takes advantage of lead bias for pretraining on unlabeled corpora and a combination of reconstruction and theme modeling loss for finetuning.	abstract
2020-1127	Experiments on NYT, CNN/DM, and Gigaword datasets demonstrate the benefit of the proposed approach. <sep>	abstract
2020-1127	I think this is an interesting paper and the results are reasonably convincing.	strength
2020-1127	My only concern is regarding a parallel submission that contains a significant overlap in terms contributions, as originally pointed out by R2 (https://openreview.net/forum?id=ryxAY34YwB).	weakness
2020-1127	All of us had an internal discussion regarding this submission and agree that if the lead bias is considered a contribution of another paper this paper is not strong enough. <sep>	weakness
2020-1127	Due to space constraint and the above concern, along with the issue that the two submissions contain a significant overlap in terms of authors as well, I recommend to reject this paper.	decision

2021-2	The paper looks into theoretical analysis of self-training beyond the existing linear case and considers deep networks under additional assumption on data.	abstract
2021-2	namely: expansion and minimal overlap in the neighborhood of examples in different classes.	abstract
2021-2	The results shed some light on self-training algorithms that use input consistency regularizers. <sep>	abstract
2021-2	Although the assumptions are very hard to check for all input distributions, the authors make an attempt by considering output of BigGAN generator.	abstract
2021-2	In summary, the paper is a great first step in understanding self-training for deep networks. <sep>	strength
2021-2	The paper is overall clearly written.	strength
2021-2	please add the explanation of  Assumption 4.1 as requested by Reviewer 4. <sep>	suggestion
2021-2	Pros: - given the extensive use of self-training the paper is of great importance to the community <sep>	strength
2021-2	-extending the analysis of self-training to deep networks <sep>	strength
2021-2	-the paper is clearly written and easy to follow <sep>	strength
2021-2	cons: -the assumptions are very hard to validate on all datasets	weakness

2021-3	The paper leverages concepts coming from hindsight relabelling methods to define a novel "iterated" supervised learning procedure to learn policies to reach different goals.	abstract
2021-3	The algorithmic solution is well supported in terms of intuition, preliminary theoretical guarantees, as well as strong empirical validation. <sep>	strength
2021-3	There is a general consensus among the reviewers that this is a strong submission and the rebuttal helped in clarifying some aspects of the paper (eg, the comparison with Go-Explore) and reinforced the empirical analysis.	rating_summary
2021-3	This is a clear accept.	decision

2021-4	This paper proposes an approach of generating mathematical expressions with a recurrent neural network, which is trained with risk-seeking policy gradient to maximize the quality of best examples rather than average examples.	abstract
2021-4	The proposed approach also enables easily incorporating domain knowledge or constraints to avoid illegal or redundant expressions.	abstract
2021-4	In extensive experiments, the proposed method is shown to significantly outperform strong baselines, including commercial software.	abstract
2021-4	All of the reviewers find the work interesting and relevant, and there are no major concerns or issues after discussion.	strength
2021-4	The topic is also of interest to a wide range of audience in the *CONF* community.	decision

2021-5	The paper presents some exciting results on the convergence of averaged SGD for overparameterized two-layer neural networks.	abstract
2021-5	The AC and reviewers all agree that the contributions are significant and well presented, and appreciate the author feedback to the reviews.	strength
2021-5	The corresponding revisions on assumptions and references, and the added simplified proposition in the introduction have nicely improved the manuscript.	rebuttal_process

2021-6	This paper proposes a novel and powerful data augmentation strategy for few-shot learning, producing convincing improvements over current approaches.	abstract
2021-6	The request by the reviewers to include additional ablations, more backbones, and an additional dataset have been satisfactorily  resolved, with the results remaining strong.	rebuttal_process
2021-6	The reviewers are all unanimous in their recommendation that the paper be accepted for publication.	rating_summary

2021-8	The reviewers agree that this is an interesting and original paper that will be of interest to the *CONF* community, and is likely to lead to follow up work.	strength

2021-9	Motivated by the importance of gameplay in the development of critical skills for humans and other biological species, this work aims to explore representation learning via gameplay in a realistic, high fidelity environment.	abstract
2021-9	Inspired by childhood psychology, they propose a variant of hide-and-seek game called "Cache" built on top of AI2-THOR, where one agent must place an object in a room such that another agent cannot find it, and demonstrate that the adversarial nature of the game helps the agents learn useful representations of the environment.	abstract
2021-9	They examine the difference in representations learned via such a dynamic, interactive adversarial gameplay approach, vs other more passive approaches involving static images. <sep>	abstract
2021-9	The paper is well written and motivated, and easy to follow.	strength
2021-9	All reviewers agree that the paper will be a great contribution to the *CONF* community.	misc
2021-9	I believe this is an important work, because not only does it challenge the traditional way of training many components of our systems passively (via static image recognition models), it synthesizes ideas from various disciplines (psychology, embodiment, ML) and provides an excellent framework for future research.	strength
2021-9	For these reasons I'm recommending we accept this work as an Oral presentation.	decision

2021-10	This paper provides a global convergence guarantee for feedforward three-layer networks trained with SGD in the MF regime.	abstract
2021-10	By introducing the novel concept of neuronal embedding of a random initialization procedure, SGD trajectories of large-width networks  are shown to be well approximated by the MF limit, a continuous-time infinite-width limit (Theorem 3).	abstract
2021-10	Furthermore, under some additional assumptions the MF limit is shown to converge to the global optimum when the loss is convex (Theorem 8, case 1) and for a generic loss when y=y(x) is a deterministic function of input x (Theorem 8, case 2).	abstract
2021-10	The global convergence guarantee presented in this paper is based on less restrictive assumptions compared with existing studies.	abstract
2021-10	All the reviewers rated this paper quite positively, with less confidence however, seemingly because of mathematical thickness of the proofs.	rating_summary
2021-10	Although the reviewers did not manage to check every detail of the proofs, they agreed that the reasoning seems mathematically sound as far as they can tell.	strength
2021-10	The authors response adequately addressed minor concerns raised by the reviewers.	rebuttal_process
2021-10	I am thus glad to recommend acceptance of this paper. <sep>	decision
2021-10	Pros: <sep> Introduces the idea of a neuronal embedding, which allows establishing relation between SGD on large-width three-layer networks and its MF limit in a quantitative way with a less restrictive setting. <sep> 	strength
2021-10	Provides a global convergence guarantee under the iid initialization, in the sense that if the MF limit converges it attains the global optimum. <sep>	strength
2021-10	Shows that the global convergence guarantee does not require convexity of the loss when a deterministic function is to be learned.<sep>	strength
2021-10	In particular, the uniform approximation property, rather than the convexity of the loss, plays a crucial role in proving the  global convergence guarantee (it allows translation of the vanishing gradient in expectation at convergence into the almost-sure vanishing gradient), which is a quite original contribution of this paper.	strength

2021-12	This is a solid paper that proposes a new method for approximating softmax attention in transformer architectures that scales linearly with the size of the sequence.	abstract
2021-12	Even though linear architectures have been proposed before using a similar idea (Katharopoulos et al 2020), this paper provides a better solution along with theoretical analysis and makes a rigorous empirical comparison against other methods.	abstract
2021-12	All reviewers agree that this is a strong paper that should be accepted.	rating_summary
2021-12	I suggest citing the recent paper https://arxiv.org/abs/2011.04006 (Long Range Arena, mentioned in the discussion) which provides further comparisons on long-range benchmarks, including the method presented in this paper and Katharopoulos et al 2020, along with a detailed discussion of the differences between the two methods.	suggestion

2021-14	This nice paper gives a better understanding of how Curriculum Learning (CL) affects image classification.	abstract
2021-14	In particular, it gives insight into cases such as noisy training data and limited training time.	abstract
2021-14	It shows that examples can be rated by difficulty to some extent, in that the order in which examples are learned seems to be consistent across runs.	abstract
2021-14	The paper is thorough and well-written.	strength

2021-15	The paper introduces a new federated learning algorithm that ensures that the objective function optimized on each device is asymptotically consistent with the global loss function.	abstract
2021-15	Both theoretical analysis and empirical results, evaluating communication efficiency, demonstrate the advantages of the proposed FedDyn method over the baselines. <sep>	abstract
2021-15	All the reviewers recommend accepting the paper.	rating_summary
2021-15	To summarize the discussion: <sep> R1 mentioned a very recent (NeurIPS 20) related paper and asks several questions.	rebuttal_process
2021-15	I believe that the authors nicely answered the questions and discussed the relation to the previous paper in detail.<sep>	rebuttal_process
2021-15	R2 mentioned that the paper focuses solely on minimizing communication costs, ignoring costs of local computations.	rebuttal_process
2021-15	The authors argued that the local computation costs are comparable to those of the baselines, and, in general, communication costs are the main source of computation energy costs (pointing to previous work), and, thus, are a natural objective to optimize.	rebuttal_process
2021-15	I believe that this adequately addressed this (and other) reviewer's concerns and the reviewer kept their score unchanged.<sep>	rebuttal_process
2021-15	R3 had several concerns, which according to the reviewer were addressed in the rebuttal (they increased the score).<sep>	rebuttal_process
2021-15	R4 points out several limitations of the method and theoretical analysis and believes that the rebuttal did not quite address the concerns.	rebuttal_process
2021-15	Nevertheless, remains positive about the paper, and believes that the shortcomings can be addressed in follow-up work.<sep>	rebuttal_process
2021-15	We share the reviewers' sentiment: it is a very nice and interesting paper, and should be accepted.	decision

2021-16	The paper proposes an insightful study on the robustness and accuracy of the model.	abstract
2021-16	It was hard to simultaneously keep the robustness and accuracy.	abstract
2021-16	A few works tried to improve accuracy while maintaining the robustness by investigating more data, early stopping or dropout.	abstract
2021-16	From a different perspective, this paper aims to improve robustness while maintaining accuracy. <sep>	abstract
2021-16	There are some interesting findings in this paper, which could deepen our understanding of adversarial training.	abstract
2021-16	For example, the authors conducted experiments with different sizes of the network in standard training and adversarial training.	abstract
2021-16	The capacity of an overparameterized network can be sufficient for standard training, but it may be far from enough to fit adversarial data, because of the smoothing effect.	abstract
2021-16	Hence given the limited model capacity, adversarial data all have unequal importance.	abstract
2021-16	Though this technique is simple and widely studied in traditional ML, it is an interesting attempt in adversarial ML and the authors provide extensive experimental results to justify its effectiveness. <sep>	strength
2021-16	In the authors' responses, the concerns raised by the reviewers have been well addressed.	rebuttal_process
2021-16	The new version becomes more complete by including more results on different PGD steps and the insights on designing weight assignment function.	rebuttal_process
2021-16	Also, the authors gave an interesting discussion on enough model size for the adversarial training, though it is still kind of an open question.	rebuttal_process
2021-16	I would thus like to recommend the acceptance of this paper.	decision

2021-18	All of the reviewers agree that this paper is well-written, and provides sound theoretical analyses and comprehensive empirical evaluations.	strength
2021-18	Overall, this paper makes a useful contribution in the direction of individual fairness.	strength
2021-18	The authors have also addressed the concerns raised by the reviewers in their response.	rebuttal_process

2021-20	The paper introduces a novel dataset condensation technique that generates synthetic samples (images) by matching model gradients with those obtained on the original input samples (images).	abstract
2021-20	The authors also show that these synthetic images  are not architecture dependent and can be used to train different deep neural networks.	abstract
2021-20	The approach is validated on several smaller datasets like MNIST, SVHN and CIFAR10.	abstract
2021-20	This work is well-motivated and the methodological contributions convincing.	strength
2021-20	All reviewers were enthusiastic and indicated that there were no flaws in this work.	strength
2021-20	The rebuttal clarified outstanding questions and made the paper stronger.	rebuttal_process

2021-21	This paper proposes a new selection paradigm for selecting the optimal architecture in neural architecture search (NAS), in particular for methods that involve a one-shot model and that deploy gradient-based methods for the search.	abstract
2021-21	Basically, the paper focuses on examining the max selection very closely and found the magnitude of architecture weights are misleading.	abstract
2021-21	Instead, the paper proposes much more intuitive finalization step, pick the operator that has the largest drop in validation if the edge is removed.	abstract
2021-21	All reviewers agreed that the idea is interesting, the paper is well-written, and the results found in the paper are interesting.	strength
2021-21	In addition, author response satisfactorily addressed most of the points raised by the reviewers, and most of them increased their original score.	rebuttal_process
2021-21	Therefore, I recommend acceptance.	decision

2021-23	The paper proposes a new solution for cross-domain correspondence in control, which combines GANs and cycle-consistency, and separates shifts in observation space and in action space.	abstract
2021-23	The paper targets unpaired data / simulations, and discovers alignment of state by enforcing that domains are mappable. <sep>	abstract
2021-23	The paper was received well by reviewers, who pointed out several strengths: a strong contribution on a fundamental problem, and an interesting formulation; a well written and well positioned paper; This compensates minor weaknesses, in particular the fact that transfer has been tested between two different simulated environments. <sep>	strength
2021-23	The reviewers unanimously suggested acceptance, the AC concurs.	decision

2021-24	All reviewers agree that this paper is very solid work, that presents great progress in no-press diplomacy.	strength
2021-24	The method and presented experiments are of very good quality and the work merits to be presented at *CONF*.	decision

2021-25	This paper presents an elegant and effective approach to knowledge transfer in RL by learning a policy prior from expert data.	abstract
2021-25	The paper is generally well structured and well written. <sep>	strength
2021-25	Generally, all the reviewers were favourable about this paper, with its simple idea and convincing results. <sep>	strength
2021-25	It was thought that the paper would benefit from the addition of more discussion around related work, and more experimental results, but it remains a strong paper.	suggestion

2021-26	This paper proposed using the state bisimulation metric to learn invariant representations for reinforcement learning.	abstract
2021-26	The method is generic, effective, and is supported by both theoretical and experimental results.	strength
2021-26	All reviewers and I think this is a strong contribution to the area.	decision

2021-27	The paper proposes to use pre-trained 2D (ie, image) GANs as a mechanism for recovering 3D shape from a single 2D image.	abstract
2021-27	The work demonstrates impressive results on not only human and cat faces, but also cars and buildings.	abstract
2021-27	The method is demonstrated with qualitative results and quantitative results on multiple datasets and tasks. <sep>	abstract
2021-27	The reviewers were persuaded by the novelty and "neatness" of the idea (and the AC is in agreement) as well as the results.	strength
2021-27	At submission time, there were some concerns with experimental details.	weakness
2021-27	For instance, there was a question of how carefully the settings have to be tuned (always a concern with unsupervised methods) as well as an overarching concern about the initialization and whether the method will work on less clean data.	weakness
2021-27	The reviewers (and the AC) seem to think that these have been sorted out in discussion. <sep>	rebuttal_process
2021-27	All three reviewers were in favor of acceptance and the area chair is inclined to agree with the reviewers.	decision
2021-27	In particular, the AC finds the work interesting and compelling.	strength
2021-27	While there is an updated version already uploaded during the discussion, the AC encourages the reviewers to double check all the questions from the reviewers and include the answers from the discussion into the camera ready (even these results are in the appendix).	suggestion

2021-29	This paper studies why input gradients can give meaningful feature attributions even though they can be changed arbitrarily without affecting the prediction.	abstract
2021-29	The claim in this paper is that "the learned logits in fact represent class conditional probabilities and hence input gradients given meaningful feature attributions".	abstract
2021-29	The main concern is that this claim is verified very indirectly, by adding a regularization term that promotes logits learning class conditional probabilities and observing that input gradient quality also improves.	weakness
2021-29	Nevertheless, there are interesting insights in the paper and the questions it asks are very timely and important, and overall, it could have a significant impact on further research in this area.	strength

2021-30	Interesting method for binaural synthesis from moving mono-audio <sep>	strength
2021-30	Nice insight into why l2 isn't the best loss for binaural reconstructions. <sep>	strength
2021-30	Interesting architectural choice with nice results. <sep>	strength
2021-30	Nicely motivated and clearly presented idea -- especially after addressing the reviewers comments.<sep>	strength
2021-30	I agree with the idea of a title change.	suggestion
2021-30	While I think its implied that the source is probably single source, making it explicit would make it clearer for those not working in a closely related topic.	suggestion
2021-30	Hence, "Neural Synthesis of Binaural Speech from Mono Audio" as suggested in the review process sounds quite reasonable.	suggestion

2021-31	I join all five reviewers in recommending acceptance. <sep>	decision
2021-31	There was some discussion about a comparison with WaveGrad (Chen et al, 2020), a contemporaneous work that explores a similar modelling approach for speech generation.	misc
2021-31	While I agree that such a comparison is a useful addition to the manuscript, I do not think it is reasonable to request anything beyond an acknowledgement and citation of the work from the authors as a condition for acceptance.	ac_disagreement
2021-31	Further discussion and comparison experiments could be valuable, but I believe that should not factor into the final decision.	ac_disagreement
2021-31	My position is most similar to Reviewer 4's in this sense.	misc
2021-31	The current version of the manuscript briefly discusses the differences between WaveGrad and DiffWave, which I think is more than sufficient.	ac_disagreement
2021-31	(As an aside, another difference potentially worth discussing is that the "noise schedule" for WaveGrad can be adapted at inference time, enabling a trade-off between inference speed and sample quality, which I believe is not possible for DiffWave in its current form.) <sep>	ac_disagreement
2021-31	There was some debate about the weakly conditioned generation results; I believe they are a nice addition to the paper, although it would have been suitable for publication without them.	rebuttal_process
2021-31	They certainly do not detract from it, and might inspire further work in weakly conditioned audio generation (eg music).	rebuttal_process
2021-31	There were also concerns about the clarity of writing, which I believe the authors have addressed in the current version of the manuscript. <sep>	rebuttal_process
2021-31	This work stands out because it applies a relatively fresh idea in generative modelling to a domain of great practical importance, which has long been dominated by traditional likelihood-based models, with compelling results.	strength
2021-31	While this implies a limited degree of technical novelty, I do not think that is grounds for rejection, and in fact I would argue that making new ideas work well for practical problems is just as important.	strength

2021-32	This paper has generated a lot of great discussion and it presents a very different way of doing image recognition at scale compared to current state of the art practices.	strength
2021-32	All reviewers rated this paper as an accept. <sep>	rating_summary
2021-32	This work is interesting enough that in my view it really deservers further exposure and discussion and an oral presentation at *CONF* would be a good way to achieve that.	decision

2021-35	This paper proposes a meta-learning algorithm for reinforcement learning.	abstract
2021-35	The work is very interesting for the RL community, it is clear and well-organized.	strength
2021-35	The work is impressive and it contributes to the state-of-the-art.	strength

2021-37	Accept.	decision
2021-37	The paper proposes Deformable DETR that builds on DETR and solves the slow convergence and limited spatial resolution problem while getting impressive results. <sep>	abstract
2021-37	The authors should think about comparing with other linear attention mechanisms to show the applicability of the method.	suggestion

2021-39	The authors propose a method for modeling dynamical systems that balances theoretically derived models, which may be grounded in domain knowledge but subject to overly strict assumptions, with neural networks that can pick up the slack.	abstract
2021-39	All reviewers were enthusiastic about this work, appreciating its balance of mathematical rigor and experimental assessment.	strength
2021-39	One concern was that this paper follows on decades of related work, which was difficult to adequately summarize.	weakness
2021-39	However, changes made throughout discussion phase did address these concerns.	rebuttal_process

2021-40	The reviewers unanimously agree that this paper is a strong accept; it makes important progress in developing our ability to query relational embedding  models.	rating_summary

2021-41	This paper proposes a model for learning disentangled representations by assuming the slowness prior over transitions between two frames.	abstract
2021-41	The model is well justified theoretically, and evaluated extensively experimentally.	abstract
2021-41	The results are good, and all reviewers agree that this paper is among the top papers they have reviewed.	rating_summary
2021-41	For this reason, I am pleased to recommend this paper for an Oral.	decision

2021-42	The paper introduces an approach to self-train a source domain classifier on unlabeled data from the target domain, considering the few-shot learning setting when there is significant discrepancy between the source and target domains.	abstract
2021-42	While the reviewers pointed out a few weaknesses, such as somewhat limited methodological novelty  and lack of comparisons with other methods, they all recommend acceptance as final decision.	rating_summary
2021-42	The paper is beautifully written.	strength
2021-42	The proposed method is very simple, but yields excellent results in a very practical problem, which should be of wide interest to the *CONF* community.	strength
2021-42	The experimental evaluation is rigorous and the ablation studies are convincing.	strength
2021-42	The AC agrees with the decision made by the reviewers and recommends acceptance.	decision

2021-44	This paper proposes a conditional language-specific routing (CLSR)  mechanism for multilingual NMT, which also considers the trade-off between language specificity and generality. <sep>	abstract
2021-44	All of the reviewers think this paper is interesting for both idea and empirical findings.	strength
2021-44	Therefore, it is a clear acceptance.	decision

2021-45	The paper proposes to bring together a GAN, a differentiable renderer, and an inverse graphics model.	abstract
2021-45	This combined model learns 3D-aware image analysis and synthesis with very limited annotation effort (order of minutes).	abstract
2021-45	The results look impressive, even compared to training on a labeled dataset annotation of which took several orders of magnitude more time. <sep>	abstract
2021-45	The reviewers point out the novelty of the proposed system and the very high quality of the results.	strength
2021-45	On the downside, R2 mentions that the model appears over-engineered and some important experimental results are missing.	weakness
2021-45	The authors' response addresses these concerns quite well. <sep>	rebuttal_process
2021-45	Overall, this is a really strong work with compelling results, taking an important step towards employing generative models and neural renderers "in the wild".	strength
2021-45	I think it can make for a good oral.	decision

2021-46	This paper studies how (two layer) neural nets extrapolates.	abstract
2021-46	The paper is beautifully written and the authors very successfully answered all the questions.	rebuttal_process
2021-46	They managed to update the paper, clarify the assumptions and add additional experiments.	rebuttal_process

2021-48	All reviewers recommend acceptance.	rating_summary
2021-48	Some concerns were raised about the precision of theorem 2 (now renamed to proposition 1), as well as the analysis of hyperparameter choices and quantitative evaluation, which I believe the authors have adequately addressed.	rebuttal_process
2021-48	Based on a suggestion of reviewer 1, experiments with flow-based models were also added, which demonstrates that the method is not strictly tied to autoregressive models.	rebuttal_process
2021-48	Personally, I was also curious about the connection between noise injection and quantisation, which the authors responded to by adding a paragraph discussing this connection in the manuscript. <sep>	rebuttal_process
2021-48	I would recommend that the authors also add the kernel inception distance (KID) results reported in the comments to the manuscript. <sep>	suggestion
2021-48	This work stands out to me in that it combines a relatively simple, easy to understand idea with nice results, which is a trait of many impactful papers.	strength
2021-48	I will therefore join the reviewers in recommending acceptance.	decision

2021-49	Thanks for your submission to *CONF*. <sep>	misc
2021-49	When the initial reviews were written, three of the four reviewers were positive about the paper.	rebuttal_process
2021-49	Everyone felt it was overall a solid contribution, but there were some concerns about the clarity and presentation, as well as some suggestions for additional experiments.	rebuttal_process
2021-49	During the rebuttal/response period, the authors did a very nice job in responding to the concerns of the reviewers.	rebuttal_process
2021-49	Ultimately, all of the reviews were in agreement after discussion that the paper is strong and ready for publication.	rating_summary
2021-49	I also like this paper a lot, and find it to be a nice way to combine LSH with NN training.	strength
2021-49	I am happy to recommend this paper for publication.	decision

2021-50	The paper proposes a new approach to continual learning with known task boundaries that is scalable and highly performant, while preserving data privacy.	abstract
2021-50	To mitigate forgetting the proposed approach restricts gradient updates to fall in the orthogonal direction to the gradient space that are important for the past tasks.	abstract
2021-50	The main novelty of the approach is to estimate these subspaces by analysing the activations for the inputs linked for each given task. <sep>	abstract
2021-50	All reviewers give accepting scores.	rating_summary
2021-50	R2, R3 and R4 strongly recommend accepting the paper, while R1 considers it borderline. <sep>	rating_summary
2021-50	The authors provided an extensive response carefully considering all reviewers' comments.	rebuttal_process
2021-50	New experiments were introduced (training time analysis and comparisons with expansion-based methods), and several clarifications were added. <sep>	rebuttal_process
2021-50	All reviewers agree that the paper is well written and its literature review adequate. <sep>	strength
2021-50	The main concern of R1 was the similarities with OGD (Farajtabar et al 2020).	weakness
2021-50	R1 considered the authors' response acceptable.	rebuttal_process
2021-50	R2, R3 and R4 consider the contribution well motivated and significant and highlight its simplicity.	strength
2021-50	The AC agrees with this assessment. <sep>	strength
2021-50	The empirical evaluation covers most of the typical benchmarks in CL.	strength
2021-50	Very strong results are reported on a variety of tasks both in terms of performance and memory efficiency, as agreed by R2, R3 and R4. <sep>	strength
2021-50	Overall the paper makes a strong contribution to the field of CL.	strength

2021-51	The paper analyzes the sample complexity of convolutional architectures, proving a gap between it and that of fully connected (fc) networks.	abstract
2021-51	The approach builds on certain invariances of fc nets.	abstract
2021-51	The reviewers appreciated the technical content and its contribution to understanding the relative advantages of different architecture, as well as the role of invariance.	strength

2021-52	This paper presents an original perspective on how to learn layouts and modules of neural module networks jointly, in a way that encourages the emergence of compositional solutions.	abstract
2021-52	In particular, layouts are treated as messages from an emergent language, and iterated learning is used to encourage the emergence of structure.	abstract
2021-52	The paper shows good performance in inducing compositional structure in two datasets. <sep>	strength
2021-52	Summarizing the reviewers' doubts, one is that the idea is tested on relatively toyish data sets, and it is not clear how it would scale up.	weakness
2021-52	The second, coming from one reviewer, concerns a lack of originality that, honestly, I do not see.	ac_disagreement
2021-52	If anything, this is probably the most original paper in my pool. <sep>	misc
2021-52	Concerning the first point, that is a fair objection, but I think that getting good results on program learning on datasets such as CLEVER is more than encouraging for a paper that is introducing quite a novel idea for the first time. <sep>	ac_disagreement
2021-52	Finally, the authors added new text and new experiments strenghtening their conclusion during the discussion. <sep>	rebuttal_process
2021-52	I am strongly in favour of accepting this paper.	decision

2021-53	A novel second order nonlinear oscillator RNN architecture is proposed, analyzed, and evaluated in this paper.	abstract
2021-53	The results are solid and impactful.	strength
2021-53	Authors and expert reviewers showed exemplary interactions with each other, improving the manuscript in significant ways.	rebuttal_process
2021-53	All four reviewers overwhelmingly recommended accept.	rating_summary
2021-53	I recommend that this paper be selected as an oral presentation.	decision

2021-55	This paper presents an interesting idea for task-free continual learning, which makes use of random graphs to represent relational structures among contextual and target samples.	abstract
2021-55	The reviewers agreed that the technical idea is novel, the experiments are extensive and the presentation is good.	strength
2021-55	The authors addressed the reviewers' concerns in the rebuttal.	rebuttal_process
2021-55	I recommend to accept.	decision

2021-58	All of the reviewers are impressed by this paper's empirical results and they agree that this is a good paper and should be accepted.	rating_summary
2021-58	Some questions about the theoretical justification of the proposed method and its potential practical impact remain open, but the empirical results are impressive and can result in more research in understanding Cyclic Precision Training (CPT) and improving quantized training of neural nets.	strength
2021-58	I suggest acceptance as a spotlight presentation.	decision

2021-59	This paper considers the problem of learning a k-dimensional latent simplices given perturbations of data points in the simplex: this problem is of wide relevance in machine learning as it encompasses many latent variable models including Latent Dirichlet allocation and Stochastic Block Models.	abstract
2021-59	It presents a modification of the recent algorithm of Bhattacharya & Kannan (SODA, 2020) which takes time O(k * nnz(A)), where A is the matrix of perturbed data points.	abstract
2021-59	The modified algorithm works with a low-dimensional sketch of the matrix A instead of A, and thereby avoids the dependence on k in the running time of the original algorithm, which used k passes over the data set.	abstract
2021-59	The main result of the paper is thus that the latent simplex problem can be solved in O(nnz(A)) time for instances of the problem that satisfy the "Spectrally Bounded Perturbation" property introduced by the authors. <sep>	abstract
2021-59	The main questions of the reviewers concerned the question of whether the assumptions needed for the analysis of the novel algorithm to apply hold on real data sets.	weakness
2021-59	The authors point out that the assumptions may be stronger than are needed in practice, and suggest that the assumptions could be weakened to assuming that a spectrally-accurate sketch could be used--- this would increase the run-time dependence from just nnz(A), but weakens the assumptions needed.	rebuttal_process
2021-59	It was also observed that, in addition to a faster runtime, the method outperforms the benchmark method. <sep>	strength
2021-59	This paper should be accepted, due to its theoretical and practical contributions to the problem of latent simplex recovery: it presents an algorithm that provably runs in true input sparsity time given an amenable instance, and practically this algorithms performs well relative to the baseline, verifying the theoretical claims.	decision

2021-60	This paper is concerned with the ongoing research program of mapping the approximation power of different GNN architectures.	abstract
2021-60	It provides significant advances in the study of equivariant GNNs and nice extensions in the invariant case by closing existing gaps between distinct GNN families. <sep>	abstract
2021-60	All reviewers agreed that this is a strong submission with substantial new theoretical results.	strength
2021-60	The AC recommends a strong acceptance.	decision

2021-61	All reviewers are positive or very positive about this work.	rating_summary
2021-61	The authors successfully addressed all questions.	rebuttal_process
2021-61	I believe this paper should be accepted.	decision

2021-62	The rebuttal (revisions, and released code) very successfully addressed all the major concerns the reviewers had. <sep>	rebuttal_process
2021-62	Pros: The dynamics distance function is a very neat, simple (which is good in this case) idea that is theoretically sound, has proven to perform well in thorough experimental results, and that can be broadly applied. <sep>	strength
2021-62	Cons: None	misc

2021-66	This is a solid paper providing the first theoretical convergence result for NAS and showing promising empirical results. <sep>	strength
2021-66	The authors' proposed GAEA method can be combined with different types of weight-sharing algorithms (DARTS, PC-DARTS, etc) and is likely to reduce the impact of the architecture discretization step due to finding sparser solutions. <sep>	strength
2021-66	I clearly recommend acceptance and would expect this to make a nice spotlight.	decision

2021-67	The paper gives a learning-augmented algorithm for estimating the support size of a discrete distribution.	abstract
2021-67	The proposed algorithm is evaluated experimentally, showing significant improvements in the estimation accuracy.	abstract
2021-67	The reviewers unanimously agreed that the contributions are strong and relevant.	strength
2021-67	I recommend accept.	decision

2021-69	This is a novel, simple, and experimentally well-supported new idea for entity linking.	strength
2021-69	The key insight is to perform entity linking by producing meaningful entity names with seq2seq approaches, and the big surprise is how well this works experimentally (at least for wikipedia-style entities).	strength
2021-69	Very nice paper!	misc

2021-70	All reviewers seems in favour of accepting this paper, witht he majority voting for marginally above acceptance threshold. <sep>	rating_summary
2021-70	The authors have taken special heed of the suggestions and improved the clarity of the paper. <sep>	rebuttal_process
2021-70	From examination of the reviews, the paper achieves enough to warrant publication. <sep>	decision
2021-70	My recommendation is therefore to accept the manuscript.	decision

2021-71	This paper proposes a method to improve the convergence time of PSRO.	abstract
2021-71	The paper was well received by all reviewers and is likely to be of interest to a similar sub-community within *CONF*, but may be of less relevance to the wider community not focused on multi-agent learning. <sep>	weakness
2021-71	A number of issues were raised by reviewers regarding the clarity of the originally submitted version of the paper.	weakness
2021-71	I encourage the authors to consider all constructive feedback given and revise the paper to maximise its impact.	suggestion
2021-71	This will be of particular help in reaching a wider audience than those with pre-existing experience with the methods this work builds on.	suggestion

2021-72	The paper studies the effect of importance weighting schemes on the implicit bias of gradient descent in deep learning models.	abstract
2021-72	It provides several theoretical results which give important insights on the effect of the importance weighting scheme on the limit of the convergence, as well as convergence rates.	abstract
2021-72	Results are presented for linear separators and deep learning models.	abstract
2021-72	A covariate shift setting is also studied.	abstract
2021-72	The theoretical results are supported with empirical demonstrations, and also lead to useful insights regarding which weighting schemes are expected to be more helpful.	abstract
2021-72	They also explain some previously observed empirical phenomena. <sep>	abstract
2021-72	Pros: <sep> New theoretical results which provide important insights on an important topic <sep> 	strength
2021-72	Empirical demonstrations which support the theoretical results<sep>	strength
2021-72	Cons: <sep> No significant issues.	misc

2021-73	This paper got 3 acceptance and 1 marginally below the threshold.	rating_summary
2021-73	After the rebuttal, the rating was raised to above the threshold.	rating_summary
2021-73	All the reviewers are positive about this submission.	rating_summary
2021-73	They agree that the method proposed in the submission is novel, the experiments are comprehensive and convincing.	strength
2021-73	AC agrees and recommend acceptance.	decision

2021-74	Reviewers agreed that connecting neural networks with dynamical systems to create a new kind of optimizer is an interesting idea.	strength
2021-74	After the authors' improvements, this is a strong submission of wide interest.	rebuttal_process

2021-75	This paper studies the problem of learning from data that have been corrupted by label noise.	abstract
2021-75	The authors define a natural data-dependent noise condition, that allows the noise rate to be large close to the decision boundary, and provide a simple iterative method that eventually converges to the Bayes optimal classifier.	abstract
2021-75	The method is evaluated on both synthetic a real datasets.	abstract
2021-75	There was a consensus among the reviewers that this is an interesting contribution and I propose acceptance.	decision

2021-77	The paper introduces MUSIC, a method for unsupervised learning of control policies, which partitions state variables into exogenous and endogenous collections and maximizes mutual information between them.	abstract
2021-77	Reviewers were uniformly positive, agreeing that the  approach was interesting and well-motivated, and the experiments convincing.	rating_summary
2021-77	Some concerns were raised as to clarity, which were addressed through several revisions of the manuscript.	rebuttal_process
2021-77	I am happy to recommend acceptance.	decision

2021-78	This paper analyzes deep networks optimized using non-convex noisy gradient descent.	abstract
2021-78	The main result shows that in a teacher-student setting, the excess risk converges in a fast-rate and is stronger than any linear estimators (which include kernel methods).	abstract
2021-78	The paper also gives a convergence rate result that depends on some spectral gaps (which can be very small) but not on dimension.	abstract
2021-78	Overall the paper is interesting.	strength
2021-78	It should probably emphasize that the dependency on spectral gaps (and the fact that they could be exponentially small) on the convergence as the current abstract suggests efficient convergence.	suggestion

2021-80	This paper proposed to defend against model stealing attacks by dataset inference.	abstract
2021-80	The paper received unanimous rating of "Good paper" and "accept".	rating_summary
2021-80	The reviewers praise this paper insightful and well written.	strength
2021-80	There are active discussion between the reviewers and authors, which further clarify some of the issues.	rebuttal_process
2021-80	Given the positive review and overall rating, the AC recommends it to be an spotlight paper.	decision

2021-81	The paper provides a method to train boosted decision trees to satisfy individual fairness.	abstract
2021-81	All of the reviews suggest that this paper is well-written and gives novel techniques for solving an interesting problem.	strength
2021-81	The authors have addressed most of the concerns raised by the reviewers during their response.	rebuttal_process
2021-81	However, the authors should follow a suggestion in the reviews and include the running time in the empirical evaluation.	suggestion

2021-83	This paper describes a method for adapting an RL policy in a deployment environment that does not provide a reward signal.	abstract
2021-83	This concern arises commonly when a task reward is available in a robot simulator but not on the physical robot where the policy is eventually deployed.	abstract
2021-83	The proposed solution is to learn an inverse dynamics model as an auxiliary prediction task on an internal state embedding that is shared with the policy.	abstract
2021-83	The policy is adapted during deployment by modifying the state embedding using this auxiliary task (with the assumption that the main objective remains unchanged).	abstract
2021-83	The proposed method is tested with transfer between simulated domains and also on transfer from a simulator to a physical robot.	abstract
2021-83	The experiments showed the method had consistently higher performance than alternatives. <sep>	abstract
2021-83	The reviewers found many positive contributions in the presented paper.	strength
2021-83	These include the problem's importance (R1, R2,R4), extensive experiments (R1, R2, R3), clear writing (R1,R4), simplicity and effectiveness in comparison to ablations (R3, R4).	strength
2021-83	The reviewers saw a weakness in the method's limitation to perceptual adaptation instead of dynamics adaptation (R1-4) and the lack of novelty (R4).	weakness
2021-83	The author response addressed both concerns.	rebuttal_process
2021-83	They stated that the method is novel for adapting to continuously changing environments in a self-supervised fashion without rewards.	rebuttal_process
2021-83	The authors modified the paper to clarify how the method demonstrates robustness to changes in the system dynamics.	rebuttal_process
2021-83	The reviewers found the author response addressed their major concerns. <sep>	rebuttal_process
2021-83	Four reviewers indicate accept for the contributions stated above and expressed no remaining concerns.	rating_summary
2021-83	The paper is therefore accepted.	decision

2021-84	The paper proposes to minimize the loss while regularizing its sharpness: so that the minimum will lie in a region with uniformly low loss. <sep>	abstract
2021-84	The reviewers uniformly appreciated the paper.	rating_summary
2021-84	They have made a number of suggestion for improving the paper, which the authors should consider incorporating in their final version.	suggestion

2021-86	The paper posits that VAEs, if made sufficiently deep, are able to implement autoregressive models, and could possibly outperform them.	abstract
2021-86	Experimentally, the authors attempt make VAEs sufficiently deep so that they are able to outperform autoregressive models on image generation.	abstract
2021-86	The authors use a variety of tricks to scale the depth of the model to up to 78 stochastic layers, and achieve SOTA, or near-SOTA NLLs on a number of datasets.	abstract
2021-86	Furthermore, in comparison to other models (in particular the recently proposed Nouveau VAE), the models achieve these scores using far fewer parameters. <sep>	abstract
2021-86	Although the tricks are a bit ad-hoc and the novelty is a bit weak, the experimental results are quite strong and would be of interest to anyone working on VAE research.	strength
2021-86	Moreover, one of the weakness of the paper, a lack of ablations, was addressed during the rebuttal.	rebuttal_process
2021-86	All reviewers believed that the paper should be accepted, and I see nothing in the paper or the reviews to suggest otherwise.	decision

2021-89	All the reviewers are positive about the paper; R2 and R3 voted for clear accept.	rating_summary
2021-89	Overall, all the reviewers feel that evolution is comprehensive and the results are decent.	strength
2021-89	There is a novel objective formulation that controls for motion diversity, disentanglement and content matching, outperforming existing methods across multiple datasets.	strength
2021-89	High-res videos at 1024x1024 are generated and there is cross-domain video generation.	strength
2021-89	Many good questions were raised by the reviewers, and they were addressed in details in the rebuttal.	rebuttal_process
2021-89	In particular, the question about subtle motion and short video sequences was raised (which was the concern that the AC had).	rebuttal_process
2021-89	The AC agrees with the reviewers that the paper warrants a publication.	decision
2021-89	Please address the questions raised by the reviewers in the final version.	suggestion

2021-90	Reviewers all agree on acceptance for this paper.	rating_summary
2021-90	The initial issues with clarity seem to have been addressed by the authors. <sep>	rebuttal_process
2021-90	The paper introduces a new transformer-based architecture for MARL that enables variable input and output sizes, which is used to train the agent in a more general setting and on more diverse tasks for multi-task training.	abstract
2021-90	The method also produces more interpretable agents. <sep>	abstract
2021-90	The paper shows results on the Starcraft multi-agent challenge (not the full game of Starcraft, but still a recognised and widely used multi-agent benchmark).	abstract
2021-90	The method produces solid results both in terms of final training performance and zero-shot generalisation. <sep>	abstract
2021-90	Although reviewers are generally supportive of this paper, they mention that the Starcraft challenge used is somewhat simple (only few units used), and that the transformer-based architecture may not be applied to domain which lack the proper structure.	weakness

2021-91	The reviewers have supported the acceptance of this paper (R3 and R5 were particularly excited) so I recommend to accept this paper.	decision

2021-92	This paper focuses on two new characteristics of adversarial examples from the channel-wise activation perspective, namely the activation magnitudes and the activated channels.	abstract
2021-92	The philosophy behind sounds quite interesting to me, namely, suppressing redundant activations from being activated by adversarial perturbations.	abstract
2021-92	This philosophy leads to a novel algorithm design I have never seen, ie, Channel-wise Activation Suppressing (CAS) training strategy. <sep>	strength
2021-92	The clarity and novelty are clearly above the bar of *CONF*.	strength
2021-92	While the reviewers had some concerns on the significance, the authors did a particularly good job in their rebuttal.	rebuttal_process
2021-92	Thus, all of us have agreed to accept this paper for publication!	decision
2021-92	Please carefully address all comments in the final version.	suggestion

2021-93	The paper shows hardness results for batch reinforcement learning.	abstract
2021-93	Authors show that even if all value functions are linear in a given set of features and the exploration data covers all directions, evaluating any policy might require a sample size that is exponentially large in the problem horizon.	abstract
2021-93	This is an interesting and somewhat surprising result, and I believe it would be of interest to the wider RL community.	strength
2021-93	I recommend acceptance of this paper.	decision

2021-94	The paper proposed a novel methodology for protecting personal data from unauthorized exploitation for training commercial models.	abstract
2021-94	The proposal is conceptually intuitive and technically motivated.	abstract
2021-94	It goes to the opposite direction of adversarial training: by adding certain error-minimizing noise (rather than error-maximizing noise) to the data, the model is fooled and believes there is nothing to learn from the data, and thus this can protect the data from being used for training.	abstract
2021-94	The paper is of not only high quality but also broad interest given the current social concerns about personal data privacy.	strength
2021-94	I think its potential impact should get it a spotlight presentation.	decision

2021-95	The paper develops a methodology for using graph neural networks for mesh-based physics simulation.	abstract
2021-95	This extends recent work that focused on grids or particles to mesh-based domains, which are challenging due to irregular (and possibly changing) connectivity.	abstract
2021-95	The reviewers had some concerns but recognized that this is an important work that will be of broad interest and may have significant impact.	strength

2021-96	The paper proposed locally free weight sharing strategy (CafeNet) for searching optimal network width.	abstract
2021-96	The proposal is a nice tradeoff between manually fixed weight sharing pattern (too small search space) and completely free weight sharing pattern (too large search space).	strength
2021-96	The originality and significance are clearly above the bar.	decision
2021-96	The paper is related to the general interests of deep learning research and its applicability deserves a spotlight presentation. <sep>	decision
2021-96	It seems the clarity can still be improved, so please carefully revise the paper following the reviews.	weakness
2021-96	BTW, I am very curious, why "locally free weight sharing strategy" goes to a short name CafeNet?	misc
2021-96	I went over the paper but I didn't find the answer.	misc
2021-96	Perhaps the name of the proposal should also be explained...	suggestion

2021-97	All reviewers expressed consistent enthusiasm on this submission during the review process.	rating_summary
2021-97	No reviewers expressed concerns and objections to accept this submission during discussion.	rating_summary
2021-97	It is quite clear this is a strong submission and deserves accept.	decision

2021-101	The paper is proposing a test time adaptation method without modifying the training.	abstract
2021-101	The proposed idea is simple and effective, adapting the normalization layers using the entropy of the model predictions as a loss function.	abstract
2021-101	The paper presents an extensive empirical study.	abstract
2021-101	Paper received unanimously accept scores.	rating_summary
2021-101	It also has potential to be impactful as it is easy to apply without any strong assumption/requirement.	strength
2021-101	A clear accept!	decision

2021-102	The paper gives an elegant and efficient closed form solution for steering directions in the latent space of a pretrained GAN to to produce transformations in the image domain such as scaling and rotation etc,  this also extended to attribute transfer.	abstract
2021-102	The new method leads to "speed up, analytical transformation end points, and better disentanglement"  w.r.t to competitive methods.	abstract
2021-102	All reviewers agreed on the merits of this work, and the good qualitative and quantitative results .	strength
2021-102	The rebuttal addressed reviewers questions and concerns regarding the structure of the paper and its coherence.	rebuttal_process
2021-102	Accept	decision

2021-103	This paper reveals a novel interpretation of the well-established CD for energy-based model training as an adversarial game through conditional NCE.	abstract
2021-103	The paper could be potential impactful for the community of EBMs. <sep>	misc
2021-103	There are several points should be addressed in final version: <sep> 1, Based on such an interpretation, the number of steps becomes a tunable parameters, rather than in vanilla understaning in CD-family (the larger, the better in terms of approximation, by with more computation cost). <sep>	weakness
2021-103	2, It is okay to stop the gradient when solving an adversarial game as the paper discussed.	weakness
2021-103	However, propagating the gradient through the component is also another choice, which leads to the algorithm proposed in [1]. <sep>	weakness
2021-103	It will be interesting to discuss these in the paper. <sep>	suggestion
2021-103	[1] Sohl-Dickstein, Jascha, Peter Battaglino, and Michael R. DeWeese.	misc
2021-103	"Minimum probability flow learning."	misc
2021-103	arXiv preprint arXiv:0906.4779 (2009).	misc

2021-106	The reviewers unanimously agree that the paper is timely, well motivated and correct, with potential to significantly impact digital contact tracing.	strength

2021-108	Two knowledgeable reviewers and one fairly confident reviewer were positive (7) about this submission.	rating_summary
2021-108	The authors' response clarified a few questions and comments from the initial reviews.	rebuttal_process
2021-108	The paper provides exact bounds that close the gap between lower and upper bounds, and that helps us understand these networks better.	strength
2021-108	With the unanimously positive feedback, I am recommending the paper to be accepted.	decision

2021-112	The paper seeks to understand how training over-parametrized models (eg, those based on neural networks) to zero training accuracy even when the test error is small (ie, benign overfitting) can introduce vulnerabilities in the form of adversarial examples and how to remedy the situation.	abstract
2021-112	The paper implicates label noise as one of the causes of adversarial robustness, and suboptimal representations learned as part of the training as another.	abstract
2021-112	The claims are supported both theoretically and empirically.	strength
2021-112	A good paper overall, accept!	decision

2021-116	This paper proposes a new differentiable physics benchmark for soft-body manipulation.	abstract
2021-116	The proposed benchmark is based on the  DiffTaichi system.	abstract
2021-116	Several existing reinforcement learning algorithms are evaluated on this benchmark.	abstract
2021-116	The paper identify a set of key challenges that are posed by this specific benchmark to RL algorithms.	abstract
2021-116	Short horizon tasks are shown to be feasible by optimizing the physics parameters via gradient descent.	abstract
2021-116	The reviewers agree that this paper is very well-written, the  problem tackled in it is quite interesting and challenging, and the use of differentiable physics in RL for manipulating soft objects quite intriguing.	strength

2021-117	This is a clear accept.	decision
2021-117	Solid and timely work extending normalizing flows to implicitly defined mappings.	strength
2021-117	Convincing presentation.	strength
2021-117	Supported by all four reviewers.	rating_summary
2021-117	Best paper in my batch.	misc
2021-117	Has the potential to spark further developments in the field.	strength
2021-117	I recommend to feature this paper as a spotlight.	decision

2021-118	This paper studies the problem of learning better video-text representation learning with an application to video-text retrieval.	abstract
2021-118	It proposes a key innovation: it uses a new generative task of cross-captioning that addresses issues with contrastive learning by learning to reconstruct a sample's text representation as a weighted combination of a video support set, using a novel objective function using video-set bottlenecks.	abstract
2021-118	It uses pre-training based on YouTube video-ASR pairs, and shows empirical results where the proposed method outperforms multiple SOTA methods. <sep>	abstract
2021-118	The authors have addressed the feedback of the reviewers, especially with the following improvements: <sep> 	rebuttal_process
2021-118	Experiments were run on more datasets <sep>	rebuttal_process
2021-118	Relevant work pointed out by the reviewers were added <sep>	rebuttal_process
2021-118	Concerns regarding technical details were clarified	rebuttal_process

2021-119	With reviewer scores of (7, 7, 9, 7), and with only one low-confidence score (R5's score of 7 with confidence of 2) it is obvious that the paper should be accepted.	decision

2021-122	The paper investigates the capacity for neural language models to perform fast-mapping word acquisition using a proposed multimodal external memory architecture.	abstract
2021-122	Much work exists that shows that neural models are capable of following instructions whose meaning persists across episodes (ie, slow-learning), however much less attention has been paid to instruction-following in a one-shot learning context.	abstract
2021-122	Using a simulated 3D navigation/manipulation domain, the paper shows that the proposed multimodal memory network is capable of both slow and one-shot word learning when trained via standard RL. <sep>	abstract
2021-122	The submission was reviewed by four knowledgable referees, who read the author feedback and engaged in discussion with the authors.	rebuttal_process
2021-122	The paper is topical---one-shot language learning for instruction-following using neural models is of significant interest of-late.	strength
2021-122	The reviewers agree that the proposed multimodal memory architecture is both interesting and technically solid.	strength
2021-122	The reviewers raised concerns about the experimental evaluation and the role of embodiment.	weakness
2021-122	The author feedback together with discussion with reviewers were helpful in resolving some of these issues.	rebuttal_process
2021-122	However, the authors are encouraged to ensure that the paper clearly motivates the importance of embodiment to slow learning and fast-mapping, particularly given the large body of work in language acquisition in robotics, a truly embodied domain, which is notably missing from the related work discussion.	suggestion

2021-123	The paper addresses the important problem of classification with unbalanced semantic classes.	abstract
2021-123	The key idea is a two-stage process that first learns a representation under various distributions (experts), then assign a cascade of experts to hard samples, whose predictions are combined.	abstract
2021-123	The approach can be added on top of various backbone networks.	abstract
2021-123	Experiments are systematic and extensive, showing improvement on three standard benchmarks for this task. <sep>	abstract
2021-123	All four reviewers recommended accept.	rating_summary
2021-123	The paper extends a recent research direction of learning "balanced" representations, followed by distribution-aware experts.	abstract
2021-123	This general approach could have wider impact on architectures designed for out-of-distribution and low-shot learning.	strength
2021-123	The authors should update the final paper based on their answers and on reviewer feedback.	suggestion
2021-123	We also encourage authors to make their code available.	suggestion

2021-125	Reviewers all agreed that this submission has an interesting new idea for learning object/keypoint representations: parts of a visual scene that are not easily predictable from their neighborhoods are good object candidates.	strength
2021-125	Experimental gains on various Atari games are convincing.	strength
2021-125	The main drawback at this point is that the evaluation is limited to visually rather simple settings, and it is unclear how the approach will scale to more realistic scenes.	weakness

2021-129	All reviewers find the idea of self-supervised learning on mathematical reasoning with the proposed skip-tree training interesting and gave the firmly positive scores.	rating_summary
2021-129	The paper is clearly written, and the experiments and the analysis are well-organized, particularly the ability of free-form conjecturing is quite thought-provoking.	strength
2021-129	Also, the reviewers' initial concerns have been properly addressed during the discussion phrase. <sep>	rebuttal_process
2021-129	I think this is a good paper from which people can learn a lot, and should be broadly presented at the conference either as an oral or a spotlight presentation.	decision

2021-130	The authors propose a new parameterization which (across multiple architectures) generalized hypercomplex multiplication and provides for small low dimensions strong performance at substantial parameter savings.	abstract
2021-130	All reviewers are happy with the theoretical contributions of the work, but would appreciate additional empirical evidence.	strength

2021-131	In this paper, the authors propose a new max-sliced Wasserstein distance.	abstract
2021-131	Specifically, the proposed method is a multiple sliced variants of the existing max-sliced Wasserstein distance.	abstract
2021-131	Compared to the subspace Robust Wasserstein distance, the proposed method can be efficiently computed. <sep>	abstract
2021-131	Overall, the proposed method is a good extension of the max-sliced Wasserstein and can be used in various applications.	strength
2021-131	All authors agree to accept the paper, so, I also vote for acceptance.	decision

2021-132	The reviewers agree that this paper overcomes a number of difficult algorithmic and technical challenges in parallelizing the RED method for image reconstruction.	strength

2021-134	This work uses a graph representation of the protein backbone and a GNN for model quality assessment (MQA) and protein design.	abstract
2021-134	The proposed GNN has the property that the vector and scalar outputs are equivariant and invariant with respect to composition of 3D rotations and reflections.	abstract
2021-134	Overall speaking, the reviewers like this paper very much (especially its technical novelty), and provide quite positive comments.	rating_summary
2021-134	On the other hand, there are also some concerns being mentioned: <sep> The datasets used in the experiments are a little old – experiments on CASP 13 are preferred. <sep> 	weakness
2021-134	Some technical details are not very clear and the paper writing needs improvements <sep>	weakness
2021-134	Experimental comparison with some recent baselines is missing.<sep>	weakness
2021-134	The authors did a good job in their rebuttal and paper revision.	rebuttal_process
2021-134	Most of the above concerns have been addressed.	rebuttal_process
2021-134	Therefore, we think the current version of the paper is clearly beyond the bar of *CONF*.	decision

2021-136	The paper argued some viewpoint about knowledge distillation quite interesting to me: the technically good KD might surprisingly be socially bad in helping outsiders "stealing" commercial models, even if the models are released as black boxes.	abstract
2021-136	Then the paper proposed a way called self-undermining KD in order to turn a well trained model into a "nasty teacher" (ie, an undistillable model), and by this way the commercial models and the corresponding intellectual properties for training them from insiders can be nicely protected. <sep>	abstract
2021-136	Overall, the quality is quite high.	strength
2021-136	The argument is very conceptually novel and the method is still technically novel.	strength
2021-136	The idea of the method is simple but works for the purpose --- that's great!	strength
2021-136	Although the experimental significance seems not too impressive, the paper opens a door to a new world concerning model privacy instead of data privacy, and hence it is of social significance.	strength
2021-136	In my opinion, the paper should have a potentially huge social impact to DL practitioners (and company owners), because KD is being used almost everywhere in the Internet industry to provide the standalone mode of Apps without clouds on personal devices.	strength
2021-136	Based on the quality and the impact, I recommend to accept the paper as a spotlight presentation.	decision

2021-137	This paper proposes an approach to probabilistic time series forecasting based on combining autoregressive deep learning models with normalizing flows.	abstract
2021-137	In terms of strengths, time series forecasting is a fundamental problem.	strength
2021-137	The proposed approach is a reasonable combination of existing model components that provides a flexible, end-to-end trainable framework for multivariate probabilistic forecasting.	strength
2021-137	The experiments are well-conducted and the results outperform recently published methods.	strength
2021-137	While the reviewers raised a number of questions, all of the reviewers agree that their questions have be answered satisfactorily by the authors during the discussion and the paper should be accepted.	rating_summary
2021-137	The authors should be sure to incorporate the reviewer suggestions and author responses into the final paper.	suggestion

2021-140	This paper introduces a method for approximating real-time recurrent learning (RTRL) in a more computationally efficient manner.	abstract
2021-140	Using a sparse approximation of the Jacobian, the authors show how they can reduce the computational costs of RTRL applied to sparse recurrent networks in a manner that introduces some bias, but which manages to preserve good performance on a variety of tasks. <sep>	abstract
2021-140	The reviewers all agreed that the paper was interesting, and all four reviewers provided very thorough reviews with constructive criticisms.	rebuttal_process
2021-140	The authors made a very strong effort to attend to all of the reviewers' comments, and as a result, some scores were adjusted upward.	rebuttal_process
2021-140	By the end, all reviewers had provided scores above the acceptance threshold. <sep>	rebuttal_process
2021-140	In the AC's opinion, this paper is of real interest to the community and may help to develop new approaches to training RNNs at large-scale.	strength
2021-140	As such, the AC believes that it should be accepted and considered for a spotlight.	decision

2021-142	This paper proposes a broad framework for unifying various pruning approaches and performs detailed analyses to make recommendations about the settings in which various approaches may be most useful.	abstract
2021-142	Reviewers were generally excited by the framework and analyses, but had some concerns regarding scale and the paper's focus on structured pruning.	weakness
2021-142	The authors included new experiments however, which mostly addressed reviewer concerns.	rebuttal_process
2021-142	Overall, I think is a strong paper which will likely be provide needed grounding for pruning frameworks and recommend acceptance.	decision

2021-143	This paper proposes a novel recurrent network called RIMs for improving generalization and robustness to localized changes.	abstract
2021-143	The network consists of largely independent recurrent modules that are sparsely activated and interact through soft attention.	abstract
2021-143	The experiments on a range of diverse tasks show that RIMs generalizes better than LSTMs. <sep>	abstract
2021-143	The overall feedback from reviewers is positive: the paper is well written, the idea is interesting, and the experiments cover a wide range of diverse tasks. <sep>	strength
2021-143	The main concerns of most reviewers are the fairness of comparison, the limited novelty, and lacking details on how and why the system works.	rebuttal_process
2021-143	The authors pointed out that RIMs are not a straightforward combination of attention and RNN, and it has fewer parameters than LSTMs.	rebuttal_process
2021-143	They also conducted ablation study to demonstrate the benefits of RIMs and provided the missing details in the revised version. <sep>	rebuttal_process
2021-143	In summary, this paper presents an important research direction for systematic generalization using modularized network.	strength
2021-143	The paper is well written, the idea is novel and interesting, and the experiments cover a wide range of diverse tasks.	strength
2021-143	Hence, it makes a worthwhile contribution to *CONF* and I am recommending acceptance of this paper.	decision

2021-144	There was a consensus among reviewers that this paper should be accepted as the authors addressed reviewers' concerns in the discussion phase.	rating_summary
2021-144	This paper is well-written and easy to read.	strength
2021-144	It provides a coherent story and investigation on two important hypotheses: that natural images have a lower intrinsic dimension than the extrinsic dimension (eg the number of pixels) and that a lower intrinsic dimension lowers the sample complexity of learning.	abstract
2021-144	These results appear to be novel and significant for the *CONF* community as it provides justifications for numerous work on understanding and designing convolutional neural networks based on low-dimensional assumptions.	strength

2021-145	The reviewers all agreed that the paper is a solid contribution. <sep>	strength
2021-145	Pros: <sep> A simple and reasonable extension to adaptive prediction sets that performs well empirically. <sep> 	strength
2021-145	The procedure presented is versatile (ie can be applied to general scores or be used to improve base conformal prediction methods). <sep>	strength
2021-145	A very thorough experimental analysis, including large datasets (ie Imagenet) and a wide range of model architectures including ResNet-152. <sep>	strength
2021-145	Some formal theoretical guarantees are provided for the procedure, although they appear to be straightforward.<sep>	strength
2021-145	Cons: <sep> Limited technical novelty. <sep> 	weakness
2021-145	Overall, I recommend a spotlight because the reviewers felt that the topic of predictive uncertainty is of interest to the broader ML and computer vision community, and the paper can have a potentially large impact in popularizing conformal methods as a viable uncertainty estimation method.	decision

2021-147	This paper presents an approach for learning disentangled static and dynamic latent variables for sequence data.	abstract
2021-147	In terms of learning objective, the paper extends Wasserstein autoencoder to sequential data, and this approach is novel and well-motivated; the aggregated posterior for static variables comes out naturally and plays an important role for regularization (this appears to be new for sequence data).	abstract
2021-147	The authors also studies how to model additional categorical variables for weakly supervised learning in real scenarios.	abstract
2021-147	The main steps (generation and inference) were illustrated by graphical models with clarity, and rigorous statements are provided to back them up.	abstract
2021-147	Experimental results demonstrate the advantages of proposed method, in terms of disentanglement performance and generation quality. <sep>	abstract
2021-147	The reviewers think this paper makes nice contributions to the sequential generative model community.	strength

2021-151	Very good paper: it proposes a novel parameterization of orthogonal convolutions that uses the Cayley transform in the Fourier domain.	abstract
2021-151	The paper discusses several aspects of the proposed parameterization, including limitations and computational considerations, and showcases it in the important application of adversarial robustness, achieving good results.	abstract
2021-151	The reviews are all very positive, so I'm happy to recommend acceptance. <sep>	decision
2021-151	Also, a big shout-out to the reviewers and to the authors for being outstanding during the discussion period.	misc
2021-151	The reviewers engaged with the paper to a great depth, and the authors improved the paper considerably as a response.	rebuttal_process
2021-151	Well done to all of you.	misc

2021-153	This submission explores how certain common padding choices can induce spatial biases in convolutional networks.	abstract
2021-153	It looks into alternative padding schemes which mitigate these issues and demonstrates significant performance improvements in widely used convnets.	abstract
2021-153	Reviewers generally agreed that this is an important point that should be more widely understood in the community, and that the proposed changes are relatively simple to adopt, so this work is likely to be impactful.	strength
2021-153	Most reviewers thought the paper was well-written, describing the problem well, and the analysis well-executed.	strength
2021-153	Most reviewers acknowledged that most of the weaknesses described in their initial reviews were well-addressed by the authors' responses and manuscript updates.	rebuttal_process
2021-153	Given the strength of the analysis and the impact for many practitioners, I recommend the submission be accepted with a spotlight presentation.	decision

2021-154	This paper addresses a method for unsupervised meta-learning where a VAE with Gaussian mixture prior is used and set-level inference, taking episode-specific dataset as input, is performed to calculate its posterior.	abstract
2021-154	In the meta-testing phase, semi-supervised learning with the learned VAE is used to fast adapt to few-show learning.	abstract
2021-154	Reviewers are satisfied with the author responses, agreeing that the method is a principled way to tackle unsupervised meta-learning.	rebuttal_process

2021-156	All reviewers are for accepting the paper: in particular, R1 and R3 found the rebuttal sufficiently convincing to increase their scores from their initial assessment leaning towards rejection. <sep>	rating_summary
2021-156	Strengths: <sep> Clarity <sep> 	strength
2021-156	Simplicity of the proposed approach <sep>	strength
2021-156	Convincing experiments outperforming reasonable baselines across all problem instances<sep>	strength
2021-156	Weaknesses: <sep> Scale (as noted by R2 and R3) to larger problem sizes, beyond the setting of less than a dozen. <sep> 	weakness
2021-156	I agree with some hesitation that the paper is narrow in scope (both in interest from the community and scale---and ultimately whether it would interest the overall quantum computing audience).	weakness
2021-156	However, I think the paper makes significant advances toward the area of adiabatic quantum computation.	strength

2021-157	All four reviewers unanimously recommended for an acceptance (four 7s).	rating_summary
2021-157	They generally appreciated that the proposed idea is novel and experiments are convincing.	strength
2021-157	I think the paper tackles an important problem of evaluating GANs, and the idea of using self-supervised representations, as opposed to the conventional ImageNet-based representations, would lead to interesting discussions and follow-ups.	strength

2021-158	This paper proposes an interesting method for combining retrieval-based models and graph neural networks for source code summarization.	abstract
2021-158	Finding new ways of bringing in additional context for graph-based models is an important research direction in this space, and the paper presents a novel and effective approach.	strength
2021-158	The initial submission was missing experiments on existing benchmarks, but new experiments presented in the discussion phase are enough to resolve that concern.	rebuttal_process
2021-158	Reviewers are unanimously in support of acceptance.	rating_summary

2021-159	This paper proposes a self supervised learning algorithm to compute object-centric representations for efficient RL in the context of robot manipulation tasks. <sep>	abstract
2021-159	The key idea is to learn an object-centric representation (using prior work on SCALOR) and use this to intrinsically generate goals for a SAC policy to achieve.	abstract
2021-159	The policy is a goal-conditioned attention policy.	abstract
2021-159	The evaluation metric is a set of tasks to manipulate objects for a visual rearrangement task. <sep>	abstract
2021-159	Pros: <sep> The baselines are reasonable and consist of other unsupervised RL algorithms in recent literature. <sep> 	strength
2021-159	Object-oriented RL is a growing area of interest and this paper proposes a reasonably novel and validated set of ideas in this domain.	strength
2021-159	I believe it will be of significant interest and potentially make an impact on research in robotics and deep reinforcement learning.<sep>	strength
2021-159	The goal-conditioned attention policy can handle realistic scenarios, namely -- multi-object manipulation tasks<sep>	strength
2021-159	The attention mechanism also provides a reasonable solution to mitigate combinatorial hardness in multi-object environments<sep>	strength
2021-159	Cons: <sep> Some of the reviewers felt that the experimental results from pixel inputs could have been pushed further.	weakness
2021-159	However, since the setup and algorithm is relatively novel, there are already many moving parts and this paper seems like a step in that direction<sep>	weakness
2021-159	Experiments with larger set of objects would have been interesting to investigate and report.	weakness

2021-160	This paper describes a clever new class of piecewise-linear RNNs that contains a long-time scale memory subsystem.	abstract
2021-160	The reviewers found the paper interesting and valuable, and I agree.	strength
2021-160	The four submitted reviews were unanimous in their vote to accept.	rating_summary
2021-160	The theoretical insights and empirical results are impactful and would be suitable for spotlight presentation.	decision

2021-161	The reviewers unanimously agreed that this is an interesting paper that belongs at *CONF*.	rating_summary
2021-161	The use of optimal transport in neural topic models is novel and the paper is well-written. <sep>	strength
2021-161	A common theme among the reviewers was that they would like to see more intuition and justification.	weakness
2021-161	I suggest you bear this in mind while editing the final version of the paper.	suggestion
2021-161	I also believe that R3 brings up valid points about evaluating perplexity -- I don't think the lack of perplexity results are a reason to reject the paper, but I believe they can be calculated here (see eg the reference R3 provided) and they would give a clearer view of the model's performance.	weakness

2021-162	The reviewers all agree that Monet proposed in the paper which optimizes for both local and global memory saving in Deep learning models is theoretically sound and experimentally convincing. <sep>	strength
2021-162	Accept!	decision

2021-163	The paper proposes to use a regularization term for stabilizing the perturbation trajectories in generating adversarial examples for medical image tasks.	abstract
2021-163	The authors tested the effectiveness of their proposal on different medical image datasets obtained by different modalities, and the experimental results are generally encouraging. <sep>	abstract
2021-163	All the reviewers see the value of the paper and give positive comments.	rating_summary
2021-163	At the same time, they also point out some aspects for further improvement, including<sep> The datasets used are relatively small <sep>	weakness
2021-163	The title is a little misleading since the paper only tackles the image attacks (but the title is stabilized medical attacks). <sep>	weakness
2021-163	Case studies and visualization are needed to help people better understand the paper<sep>	weakness
2021-163	The authors have done a good job in their rebuttal and paper revision, by adding experiments on larger datasets, changing the title to "stabilized medical image attacks", and adding some geometric figures for better illustration.	rebuttal_process
2021-163	These have largely addressed the concerns of the reviewers, and we see no problem with accepting the paper.	decision

2021-164	The proposed approach for evaluating reward functions is theoretically grounded while having several properties appealing to practical RL tasks.	abstract
2021-164	This novel approach fills a gap in the literature.	strength
2021-164	All reviewers agree that this paper has a place at *CONF*.	rating_summary

2021-167	The paper shows convergence results for RMSprop in certain regimes.	abstract
2021-167	The reviews are uniformly positive about this paper and I recommend acceptance.	decision

2021-172	This paper introduces a novel pruning algorithm for neural networks, gently regularizing the weights away (through weight decay) and using Hessian information instead of simple magnitude.	abstract
2021-172	All in all an idea that is simple and effective, and could be of interest to a large audience. <sep>	strength
2021-172	AC	misc

2021-203	Summary: The authors propose to approximate operations on graphs, roughly speaking by approximating the graph locally around a collection of vertices by a collection of trees.	abstract
2021-203	The method is presented as a meta-algorithm that can be applied to a range of problems in the context of learning graph representations. <sep>	abstract
2021-203	Discussion: The reviews are overall positive, though they point out a number of weaknesses.	rating_summary
2021-203	One was unconvincing experimental validation.	weakness
2021-203	Another, more conceptual one was that this is a 'unifying framework' rather than a novel method.	weakness
2021-203	Additionally, there were a number of minor points that were not clear.	weakness
2021-203	However, the authors have provided additional experiments that the reviewers consider convincing, and were able to provide sufficient clarification. <sep>	rebuttal_process
2021-203	Recommendation: <sep> The reviewer's verdict post-discussion favors publication, and I agree.	decision
2021-203	The authors have convincingly addressed the main concerns in discussion, and novelty is not a necessity: Unifying frameworks often seem an end in themselves, but this one is potentially useful and compellingly simple.	rebuttal_process

2021-205	This paper introduces Signatory, a library for computing functionality related to the signature and logsignature transforms.	abstract
2021-205	Although a large body of the initial literature on the signature in ML focuses on using it as a feature extractor, more recent works have incorporated within modern deep learning architectures and therefore, the importance of having GPU-capable libraries (with automatic differentiation) that implement these transforms.	abstract
2021-205	Several algorithmic improvements are incorporated into the library.	abstract
2021-205	Some of the computational benefits of this library wrt to previous ones are demonstrated empirically. <sep>	abstract
2021-205	There were some concerns from the reviewers about accepting library papers at *CONF*.	rating_summary
2021-205	Library papers clearly fall into the *CONF* CFP and, therefore, library, frameworks and platform papers that can be relevant and impactful are welcome contributions to the community.	ac_disagreement
2021-205	Additionally, more signature-related papers are appearing are mainstream ML venues, hence, despite the poor scalability wrt input dimensions, this paper is definitely relevant. <sep>	ac_disagreement
2021-205	Perhaps one the drawbacks of this paper is the lack of a more rigorous empirical evaluation.	weakness
2021-205	The authors have added a deep learning benchmark, which is welcome but only on a toy dataset.	rebuttal_process
2021-205	There are still some concerns about the wide applicability of the signature (and its relatives) given its exponential scaling.	weakness
2021-205	That's why applications on more realistic problems will be welcome.	suggestion
2021-205	At the very least, It will be good if the authors incorporate a separate section discussing the limitations of the signature transform (and the library), especially in terms of computations and scalability.	suggestion

2021-231	This paper proposes a new method for post-training quantization, achieving very good results.	abstract
2021-231	After the author's response, all the reviewers were positive.	rebuttal_process
2021-231	There were some issues regarding clarity, and about explaining why the methods work better than just optimizing the loss, but I think the reviewers were eventually satisfied.	rebuttal_process
2021-231	Following some info after the author's response phase, I'll just ask the authors to verify their published code works with publicly available PyTorch packages, so their method could be easily used.	suggestion

2021-240	The paper proposes an approach to meta-learning symmetries.	abstract
2021-240	While several approaches have recently emerged with similar goals, and sometimes greater convenience and empirical performance, the proposed approach has some interesting characteristics, such as changing properties of the architecture to extrapolate these symmetries.	strength
2021-240	There was a quite a spread of opinions about the paper, the empirical results were not strong, and updates to the paper focused on helpful text additions, but did not substantively improve the evaluation or experiments.	rebuttal_process
2021-240	Notwithstanding, the paper is conceptually interesting, there are no major flaws, and there is sufficient support for it.	strength

2021-271	This paper analyzes the expressive power of NTK corresponding to deep neural network.	abstract
2021-271	It is shown that the depth hardly affects the behavior of the spectrum of the corresponding integral operator, which indicates that depth separation does not occur as long as NTK is considered. <sep>	abstract
2021-271	The analysis is novel and gives a significant insight to the NTK research literature.	strength
2021-271	The theoretical framework considered in this paper is considerably broad and potentially can be applied to several types of activation functions (while only ReLU is analyzed as a concrete example in the paper).	strength
2021-271	Moreover, some numerical experiments are conducted that support the validity of the theoretical analysis. <sep>	strength
2021-271	All reviewers are positive on this paper.	rating_summary
2021-271	I agree with their evaluations.	misc
2021-271	For these reasons, I think this paper is worth acceptance.	decision

2021-289	Summary: <sep> The authors propose a Bayesian approach to data cleaning, implemented via a variational auto-encoder.	abstract
2021-289	They argue that a common problem in this context are posteriors that overfit by concentrating on a low-dimensional subset and introduce an optimization target intended to discourage that behavior. <sep>	abstract
2021-289	Discussion: <sep> Arguably the main concern brought up in the reviews was how much novelty there is in addressing latent variable posterior collapse, solutions for which have been proposed.	weakness
2021-289	The authors were able to clarify that this was due to a misunderstanding (the collapse they address is not in latent space), <sep>	rebuttal_process
2021-289	and the reviewer considers the matter resolved. <sep>	rebuttal_process
2021-289	Recommendation: <sep> I recommend publication.	decision
2021-289	The reviewers are all positive, agree that the method is interesting, and seems novel.	rating_summary
2021-289	The writing is clear, and remaining doubts have been addressed in the discussion.	rebuttal_process

2021-303	This is a well written paper addressing a challenging problem with an original approach.	strength
2021-303	While one reviewer claims there is not a strong call for calibration of regression tasks, this may well be because methods don't exist.	weakness
2021-303	Certainly, calibration is a critical tool for classification. <sep>	misc
2021-303	The major failing of the paper, however, is the empirical evaluation.	weakness
2021-303	Given that no prior work exists, it is arguably OK to not do this, but one could easily reject the paper on this issue alone, as AnonReviewer4 was inclined to do.	weakness
2021-303	One reviewer, however, thought highly of the paper, which bumped up its average score, more than I think it should have got (due to the poor experimental work). <sep>	rating_summary
2021-303	The abstract could be improved by mentioning the use of kernels, the nature of this solution is a substantial part of the paper.	suggestion

2021-306	The paper addresses generalization to compositions of rare and unseen sequences.	abstract
2021-306	It proposes an unstructured data augmentation, that achieves comparable generalization to structured approaches (eg using grammars).	abstract
2021-306	The idea is based on recombining prototypes and oversampling  in the tail. <sep>	abstract
2021-306	The paper provides a novel approach to an important problem.	strength
2021-306	All four reviewers recommended accept.	rating_summary

2021-307	The authors carefully study a class of unsupervised learning models called self-expressive deep subspace clustering (SEDSC) models,  which involve clustering data arising from mixtures of complex nonlinear manifolds.	abstract
2021-307	The main contribution is to show that the SEDSC formulation itself suffers from fundamental degeneracies, and that the experimental gains reported in the literature may be due to ad-hoc preprocessing. <sep>	abstract
2021-307	The contributions are compelling, and all reviewers appreciated the paper.	strength
2021-307	Despite the paper being of somewhat narrow focus, my belief is that negative results of this nature are useful and timely.	strength
2021-307	I recommend an accept.	decision

2021-316	The paper studies nonconvex-strongly concave min-max optimization using  proximal gradient descent-ascent (GDA), assuming Kurdyka-Łojasiewicz (KŁ) condition holds.	abstract
2021-316	The main contribution is a novel Lyapunov function, which leads to a clean analysis.	strength
2021-316	The main downsides of the paper as discussed by the reviewers are the lack of experiments and somewhat stringent assumptions needed in the analysis.	weakness
2021-316	Nevertheless, the paper was overall viewed favorably by the reviewers, who considered it a worthwhile contribution to the area min-max optimization.	rating_summary

2021-322	This paper proposes and investigates an approach for audiovisual synthesis based on the so-called exemplar autoencoders.	abstract
2021-322	The proposed approach is shown to be able to convert an audio input to audiovisual outputs using only very small amount of training data.	abstract
2021-322	All reviewers consider the paper interesting with a lot of potentials in a variety of applications and appreciate the novelty of the work in this domain.	strength
2021-322	But there are also concerns on the technical presentation and the quality of the samples in the demo.	weakness
2021-322	The authors addressed most of the concerns in the rebuttal but agreed that the quality of the results still had room for further improvements.	rebuttal_process
2021-322	Overall, the work presented is interesting.	strength
2021-322	The paper can be accepted.	decision

2021-331	This work makes the observation that gradients in neural network training are approximately distributed according to a log-normal distribution.	abstract
2021-331	This observation is then used to compress and sparsify the gradients, which can be useful in distributed optimization of neural nets.	abstract
2021-331	The reviewers indicate that this contribution is novel and useful and they do not find any major issues with the presented work.	strength
2021-331	I recommend accepting the paper for a poster presentation.	decision

2021-332	The paper proposes a two-level hierarchical algorithm for efficient and scalable multi-agent learning where the high-level policy decides a reduced space for low-level to explore in.	abstract
2021-332	All the reviewers liked the premise and the experimental evaluation.	strength
2021-332	Reviewers had some clarification questions which were answered in the authors' rebuttal.	rebuttal_process
2021-332	After discussing the rebuttal, AC as well as reviewers believe that the paper provides insights that will be useful for the multi-agent learning community and recommend acceptance.	decision

2021-350	Most reviewers agree that the paper makes valuable contribution in analyzing single-timescale actor-critic algorithms.	strength
2021-350	There were some doubts on the theoretical advantage over two-timescale algorithms and the realizability assumptions, but the authors made satisfactory clarifications. <sep>	rebuttal_process
2021-350	Therefore, acceptance is recommended, though I strongly suggest the authors to explicitly state key assumptions required to ensure global optimality in the abstract and introduction to avoid confusion.	decision

2021-362	This paper proposed a new semi-supervised object detection approach using Unbiased Teacher to jointly address the pseudo-labeling bias and overfitting issues.	abstract
2021-362	Significant improvements over SOTA were reported on COCO and VOC.	abstract
2021-362	Reviewers agree that the proposed method is simple and effective, and the experimental results are solid and convincing.	strength
2021-362	While the novelty of technical contributions for individual components may not be very significant, the idea is simple and well executed with strong results and good presentation.	strength
2021-362	Overall, the paper is recommended for acceptance (poster).	decision

2021-366	The paper proposes a model and a training mechanism for multimodal generation.	abstract
2021-366	The reviews are generally positive: they praise the generality of the method, the extensive experimental evaluation, and the good empirical results.	strength
2021-366	Overall, no major concerns were raised, and all reviewers recommend acceptance. <sep>	rating_summary
2021-366	A couple of concerns remain, in my view: <sep> The method is generally heuristic, and intuitively rather than theoretically motivated.	weakness
2021-366	This is compensated of course by the empirical evaluation, which is thorough. <sep>	weakness
2021-366	The paper could be better written.	weakness
2021-366	The reviewers suggested some minor improvements which were implemented in the updated version, but I believe there is room for further improvement.<sep>	rebuttal_process
2021-366	Due to the above concerns, I consider the rating of reviewer #3 (10: Top 5% of accepted papers, seminal paper) to be unjustifiably high.	ac_disagreement
2021-366	On balance, however, I'm happy to recommend acceptance. <sep>	decision
2021-366	Message to the authors: <sep> In the abstract you write: "a simple generic model that can beat highly engineered pipelines".	weakness
2021-366	Please be aware that the word "beat" evokes competition, winners and losers, so it's not appropriate in the context of scientific evaluation.	weakness
2021-366	Please consider replacing it with something neutral, such as "a simple generic model that can perform better than ...".	suggestion

2021-382	This work describes a system for collaborative learning in which several agents holding data want to improve their models by asking other agents to label their points.	abstract
2021-382	The system preserves confidentiality of queries using MPC and also throws in differentially private aggregation of labels (taken from the PATE framework).	abstract
2021-382	It provides expriments showing computational feasibility of the system.	abstract
2021-382	The techniques use active learning to improve the models. <sep>	abstract
2021-382	Overall the ingredients are fairly standard but are put together in a new (to the best of my , admittedly limited, knowledge of this area).	strength
2021-382	This seems like a solid attempt to explore approaches for learning in a federated setting with strong limitations on data sharing.	strength

2021-434	A good paper with significant contribution on XAI and the on- vs off- data manifold explainability. <sep>	strength
2021-434	Reviewers have appreciated authors' feedback and update of the paper (R1, R2, R4).	rebuttal_process
2021-434	I would like to personally thank the authors for a smooth, extensive and focused interaction w/ updates.	misc

2021-441	The paper offers novel insights about memorization, the process by which deep neural networks are able to learn examples with incorrect labels.	abstract
2021-441	The core insight is that late layers are responsible for memorization.	abstract
2021-441	The paper presents a thorough examination of this claim from different angles.	strength
2021-441	The experiments involving rewinding late layers are especially innovative. <sep>	strength
2021-441	The reviewers found the insights valuable and voted unanimously for accepting the paper.	rating_summary
2021-441	The sentiment is well summarized by R2: "The findings of the paper are interesting.	strength
2021-441	It shows the heterogeneity in layers and training stage of the neural net". <sep>	strength
2021-441	I would like to bring to your attention the Coherent Gradients paper (see also R1 comment).	suggestion
2021-441	This and other related papers already discusses the effect of label permutation on the gradient norm.	suggestion
2021-441	Please make sure you discuss this related work.	suggestion
2021-441	As a minor comment, please improve the resolution of all figures in the paper. <sep>	suggestion
2021-441	In summary, it is my pleasure to recommend the acceptance of the paper.	decision
2021-441	Thank you for submitting your work to *CONF*, and please make sure you address all remarks of the reviewers in the camera-ready version.	suggestion

2021-448	This paper presents an approach for mitigating subgroup performance gap in images in cases when a classifier relies on subgroup specific features.	abstract
2021-448	The authors propose a data augmentation approach, where synthetically produced examples (by GANs) act as instantiations of the real samples in all possible subgroups.	abstract
2021-448	By matching the predictions of original and augmented examples, the prediction model is forced to ignore subgroup differences encouraging invariance.	abstract
2021-448	The proposed method of 'controlled data augmentations' (as precisely called by R4) is relevant and well-motivated, the theoretical justifications support the main claims, and the experimental results are diverse and demonstrate merits of the proposed approach.	strength
2021-448	As rightly pointed out by R3, 'The appendices are also very thorough, and the code is organized well'. <sep>	strength
2021-448	In the initial evaluation, the reviewers have raised (in unison) concerns regarding overlapping subgroups per class, and an imbalance problem in the subgroups when training GANs.	rebuttal_process
2021-448	There were also questions reg theoretical justifications, and empirical evaluations of the baseline methods.	rebuttal_process
2021-448	The authors have addressed all major concerns in the rebuttal.	rebuttal_process
2021-448	Pleased to report that based on the author respond with extra experiments and explanations, R2 has raised the score from 6 to 7.	rebuttal_process
2021-448	In conclusion, all four reviewers were convinced by the author's rebuttal, and AC recommends acceptance of this paper – congratulations to the authors! <sep>	decision
2021-448	There is a colossal effort in the community addressing a goal similar to this work – learning invariant representations wrt sensitive features by means of algorithmic fairness methods.	misc
2021-448	(R1 and R3 relate to it).	misc
2021-448	When preparing the final version, the authors are encouraged to elaborate more on the discussion/comparison to fairness-based methods, ideally including empirical evidence where possible (where subgroups overlap, eg CelebA).	suggestion
2021-448	The AC believes this will strengthen the final revision and will have an even broader impact in the community.	misc

2021-449	The paper introduces new tighter non-asymptotic confidence intervals for off-policy evaluation, and all reviewers generally liked the results.	strength
2021-449	I recommend acceptance of this paper.	decision
2021-449	Some concerns of Reviewer2 and Reviewer3 are not fully addressed in your rebuttal.	rebuttal_process
2021-449	Please make sure to address all remaining issues.	suggestion

2021-452	Protein molecule structure analysis is an important problem in biology that has recently become of increasing interest in the ML field.	abstract
2021-452	The paper proposes a new architecture using a new type of convolution and pooling both on Euclidean as well as intrinsic representations of the proteins, and applies it to several standard tasks in the field. <sep>	abstract
2021-452	Overall the reviews were strong, with the reviewers commending the authors for an important result on the intersection of biology and ML.	strength
2021-452	The reviewers raised the points of: <sep> weak baselines (The authors responded with adding suggested comparison, which were not completely satisfactory) <sep> 	rebuttal_process
2021-452	focus mostly on recent protein literature <sep>	weakness
2021-452	the reliance of the method on the 3D structure.	weakness
2021-452	The AC however does not find this as a weakness, as there are multiple problems that rely on 3D structure, which with recent methods can be predicted computationally rather than experimentally.<sep>	ac_disagreement
2021-452	We believe this to be an important paper and thus our recommendation is Accept.	decision
2021-452	As the AC happens to have expertise in both 3D geometric ML and structural biology, he/she would strongly encourage the authors to better do their homework as there have been multiple recent works on convolutional operators on point clouds, as well as intrinsic representation-based ML methods for proteins.	suggestion

2021-461	The paper shows that under a very restrictive assumption on the data, ReLU networks with one hidden layer and zero bias trained by gradient flow converge two a meaningful predictor provided that the network weights are randomly initialized with sufficiently small variances.	abstract
2021-461	While there is some overlap with a paper by Lyu & Li (2020), the paper under review establishes its results for networks with arbitrary widths whereas using the results of Lyu & Li (2020) works, at least so far, only for sufficiently wide networks.	abstract
2021-461	The assumption on the data is anything than realistic and actually any "simple, conventional" learning algorithm can easily learn in this regime.	weakness
2021-461	Nonetheless, getting meaningful results for neural networks is still a notoriously difficult task and for this reason, the paper deserves publication.	decision

2021-462	This is an interesting, controversial paper that contributes to an ongoing debate in Bayesian deep learning. <sep>	strength
2021-462	Bayesian inference with artificially "cooled" posteriors (eg, trained with Langevin dynamics with down-weighted noise) was recently found to outperform over both point estimation and fully-Bayesian treatments (Wenzel et al, 2020).	abstract
2021-462	This paper proposes a new explanation for these observed phenomena in terms of a data curation mechanism that popular benchmark data sets such as CIFAR underwent.	abstract
2021-462	The analysis boils down to an evidence overcounting/undercounting argument and takes into account that curated data sets only contain data points for which all labelers agreed on a label.	abstract
2021-462	The authors claim that, when modeling the true generative process of the data, the cold posterior effect (partially) vanishes. <sep>	abstract
2021-462	The paper is well-written and provides a consistent analysis by modeling the data curation mechanism in terms of an underlying probabilistic graphical model of the labeling mechanism.	strength
2021-462	Unfortunately, several observed phenomena of (Wenzel et al, 2020) remain unexplained by the theoretical arguments, eg, the fact that "very cold" (T --> 0) posteriors don't hurt performance, or the observation that the optimal temperature seems to depend on the model capacity.	weakness
2021-462	While the proposed explanation doesn't capture the full picture (upon which both authors and reviewers agree), the paper's focus on the data curation process, supported extensive experiments, gives a partial explanation and provides an interesting perspective that will spur further discussion and should be of broad interest to the Bayesian deep learning community.	strength

2021-481	This work proposes a method, inspired by Cellular Automata, to generate 3D objects in voxel space.	abstract
2021-481	By only using local update rule for each location, the method can probabilistic generate high resolution models of everyday objects in the dataset.	abstract
2021-481	Due to the ability to incrementally generate details, the quality of the samples are seemingly higher than tradition approaches using Voxel-based GANs. <sep>	abstract
2021-481	Most reviewers and myself agree this is a strong and interesting paper that will spark good discussion in the *CONF* community.	strength
2021-481	It is also well written and ideas are clearly explained.	strength
2021-481	During the review process, the authors improved the work by conducting additional experiments to analyze the sensitivity of hyper parameters and took in and incorporated various suggestions from the reviewers.	rebuttal_process
2021-481	After the revision, I believe the work to be in good shape to be accepted at *CONF*2021, and I will recommend that this paper be accepted (Poster).	decision

2021-489	This work proposes a stochastic process variant that extends existing work on neural ODEs.	abstract
2021-489	The resulting method allows for a fast data-adaptive method that can work well fit to sparser time series settings, without retraining.	abstract
2021-489	The methodology is backed up empirically, and after the response period, the reviewers' concerns are sufficiently addressed and reviewers are in agreement that the contributions are clear and correct.	rebuttal_process

2021-509	The paper addresses the problem of improving generalization when few annotated data is available by leveraging available auxiliary information.	abstract
2021-509	The authors consider the respective merits of two alternatives: using auxiliary information as complementary inputs or as additional outputs in a multi-task or transfer setting.	abstract
2021-509	For linear regression, they show theoretically that the former can help improve in distribution error but may hurt OOD error, while the latter may help improve OOD error.	abstract
2021-509	They propose a framework for combining the two alternatives and show empirically that it does so on three different datasets. <sep>	abstract
2021-509	All the reviewers agree on the novelty, interest and impact of the method.	strength
2021-509	The rebuttal clarified the reviewers' questions.	rebuttal_process
2021-509	I propose an accept.	decision

2021-551	This paper provides an interesting analysis on the research on Domain Generalization with main principles and limitations.	abstract
2021-551	The authors provide a strong rebuttal to address some comments pointed by reviewers.	rebuttal_process
2021-551	All the reviews are very positive. <sep>	rating_summary
2021-551	Hence, I recommend acceptance.	decision

2021-577	The authors introduce an approach for designing pseudo-labels in semi-supervised segmentation. <sep>	abstract
2021-577	The approach combines the idea a refining pseudo-labels with self-attention grad-CAM (SGC) and a calibrated prediction fusion, and consistency training by enforcing pseudo labels to be robust to strongly-augmented data. <sep>	abstract
2021-577	The reviewers overall like idea and point out the good level of performance obtained by the method in the challenging semi-supervised context.	strength
2021-577	However, they also point out the limited novelty of the approach, and the need for a better positioning with respect to related works.	weakness
2021-577	After rebuttal, reviewers were satisfied with authors' answers and paper modifications, and all recommend acceptance.	rating_summary
2021-577	The AC considers that the submission is a nice combination of existing techniques and likes the simplicity of the one-stage approach, which reaches good performances.	strength
2021-577	Therefore, the AC recommends acceptance.	decision

2021-587	The paper proposes a method for training GANs in few-shot setting.	abstract
2021-587	Two key components of the method are: a skip-layer channel-wise excitation (SLE) module that encourages gradient flow across resolutions, and a self-supervised loss of autoencoding to regularize the discriminator.	abstract
2021-587	The results presented in the paper are indeed impressive in the few-shot setting.	strength
2021-587	Reviewers had some concerns about training set memorization which have been addressed by the authors with additional evaluations using LPIPS metric.	rebuttal_process
2021-587	Overall, the paper tackles an important problem of few-shot learning of GANs and will be a good addition to the *CONF* program.	decision

2021-588	This paper introduces a clever new problem that may prove useful in the advancement of Automatic Theorem Proving -- finding intermediate steps in a proof.	abstract
2021-588	A non-synthetic benchmark is created based on a large human-created dataset of proofs.	abstract
2021-588	Neural models were shown to have non-trivial performance.	abstract
2021-588	Reviewers were convinced that this is ultimately a useful benchmark.	strength

2021-596	This paper presents a zero-shot generation approach by disentangling representations into swappable components (each component corresponding to an attribute) and then conditioning on any desired combination of attributes to do zero-shot synthesis of samples containing those attributes. <sep>	abstract
2021-596	There were some concerns raised in the original reviews which the authors have addressed in the rebuttal and the revised submission.	rebuttal_process
2021-596	Post the discussion phase, all reviewers see merit in the proposed ideas and unanimously recommend acceptance.	rating_summary
2021-596	Based on my own reading of the paper and the reviews/author responses, I agree with the assessment.	decision

2021-612	This paper presents a broad exploratory analysis of the geometry of token representations in large language models, with a focus on isotropy and manifold structure, and reveals some surprising findings that help explain past observations. <sep>	abstract
2021-612	Pros: <sep> Clear and surprising analytical findings concerning a broad and widely-used family of models. <sep> 	strength
2021-612	Cons: <sep> The paper is a fairly broad exploratory analysis, with no single precise claim that ties together every piece of the work. <sep> 	weakness
2021-612	I thank both the authors and reviewers for an unusually productive discussion.	misc

2021-613	This paper studies extensions of the Scattering Graph Transform to spatio-temporal domains.	abstract
2021-613	By exploring several design choices for spatio-temporal wavelet filters, the authors provide a solid and broad study of such predefined represenatations, including stability analysis as well as extensive empirical evaluations. <sep>	abstract
2021-613	Reviewers were generally favorable, and highlighted the importance of this method as providing a simple yet powerful baseline for spatio-temporal graph prediction tasks that requires no training.	strength
2021-613	Despite some concerns about lack of analysis of the empirical results, the AC believes this work will provide a valuable baseline for future research and therefore recommends acceptance as a poster.	decision

2021-615	This paper presents a framework for joint differentiable simulation of physics and image formation for inverse problems.	abstract
2021-615	It brings together ideas from differentiable physics and differentiable rendering in a compelling framework.	strength

2021-618	This work is likely to lead to more connections between machine learning and neuroscience at a fine-grained level where ML methods can help explain and understand neural circuits. <sep>	abstract
2021-618	To encourage this, it would be helpful if authors described the biology of the PN-KC-APL network and the known constraints over possible formalizations of that network.	suggestion
2021-618	The authors present one formalization, but little discussion is given toward the design space for such models.	weakness
2021-618	Are there other possible ways to describe the PN-KC-APL network?	suggestion
2021-618	Are all alternate ways to do so equivalent to the model presented here?	suggestion
2021-618	What properties are unknown and how could they affect the formalization presented here? <sep>	suggestion
2021-618	Overall, reviewers agree this is a good submission.	rating_summary

2021-627	The paper proposes matching the distribution of biases for an LSTM to estimates of long range mutual information from analyzing the statistics of languages.	abstract
2021-627	The authors shows empirical evidence that LSTMs seem indeed to be following such a distribution, using natural language and Dyck-2 grammar.	abstract
2021-627	They show that explicitly enforcing the distribution of biases in learning can actually help LSTM language models. <sep>	abstract
2021-627	The reviewers had slight concern about some of the baseline numbers reported, but the authors took the time to address those concerns.	rebuttal_process
2021-627	Overall, it was an interesting and thought provoking paper that can provide a useful angle to consider when building recurrent models for a problem -- namely that of matching the properties / inductive bias of the model to that of the data.	strength

2021-628	This work demonstrates that autoregressive (AR) models for machine translation can can be competitive with their non-autoregressive (NAR) counterparts in terms of practicality.	abstract
2021-628	This is a timely observation, given the flurry of recent work on NAR models, whose primary benefit is often cited to be fast inference. <sep>	strength
2021-628	It was argued that the results are not surprising -- if this is the case, I still think this work merits acceptance because its thesis runs counter to the direction the field as a whole seems to be moving in, and the results are convincing.	decision
2021-628	That said, I agree with the authors that the observation that some encoder and decoder layers are interchangeable, is not self-evident (ie it is surprising).	weakness
2021-628	This is of course subjective to some degree, so I am making a judgement call here.	misc
2021-628	The work also has value in that it draws attention to some practices regarding evaluation in NAR machine translation literature that could be improved and made more fair (specifically regarding comparison with AR models). <sep>	strength
2021-628	There were some concerns about whether these models should be evaluated in the small-batch or large-batch setting.	weakness
2021-628	The authors have updated their manuscript in response, and it now explicitly discusses both settings.	rebuttal_process
2021-628	The authors have also run more experiments and added several additional results requested by reviewers to the manuscript. <sep>	rebuttal_process
2021-628	All things considered, I am inclined to follow the majority and recommend acceptance.	decision

2021-630	The paper studies the features extracted by the pre-trained language model and how fine-tuning makes use of these features.	abstract
2021-630	The paper is well-motivated by two lines of research in the NLP area -- 1) probing approaches for understanding the features extracted in the pre-training model, 2) model behavior analysis that shows models take shortcuts for making predictions.	abstract
2021-630	The paper provides a comprehensive study to bridge the gaps between these two lines of discussion. <sep>	abstract
2021-630	All the reviewers agree the paper has strong merits and concerns have been addressed.	rebuttal_process

2021-640	This paper presents a theoretical characterization of the impact of noise in causal and non-causal features on model generalization, through the lens of counterfactual data augmentations with toy data and models, and demonstrates that the predictions of this characterization bear out in several experiments on language counterfactually-augmented language data with substantial models. <sep>	abstract
2021-640	Pros: <sep> Spurious features and their relationship out-of-domain generalization are a practically issue in modern applied ML, and this work helps to coalesce our understanding of this area. <sep> 	strength
2021-640	Fairly extensive experimental work.<sep>	strength
2021-640	Cons: <sep> Reviewers didn't find the connection between the theoretical analysis, which focused on a simplified setting, and the experimental work, to be especially clear.	weakness
2021-640	In particular, reviewers worried that the predictions that were tested experimentally were fairly intuitive ones that could reasonably be derived from a number of starting assumptions, so it's not clear that they offer strong support for the specific account given here. <sep>	weakness
2021-640	Reviewers found the presentation, especially of the empirical work, confusing.<sep>	weakness
2021-640	Overall, this paper makes a legitimate and sound contribution to an important research area.	strength
2021-640	That contribution is small, and somewhat easy to misinterpret, but after some discussion, reviewers agreed that the paper should still be a worthwhile net positive for the field.	strength

2021-681	This paper gives a new theoretical tool to connect the gap between the spectral perspective and spatial perspective of graph neural networks.	abstract
2021-681	The frame-work is considerably broad and can deal with several existing methods.	abstract
2021-681	From this view point, the connection between the spatial and spectral perspectives are made explicit while they are noticed in an informal way by existing researches.	abstract
2021-681	The frequency response of several methods are analyzed through theories with support by some numerical experiments. <sep>	abstract
2021-681	The idea of connecting spatial and spectral perspective would not be entirely new, but the main novelty of this paper is to make it explicit and analyzed the frequency response of well-known methods concretely.	strength
2021-681	This is informative to the literature and extends some known results to more general settings.	strength
2021-681	The numerical experiments well justify the plausibility of the theory.	strength
2021-681	For reasons mentioned above, I think this paper is worth publishing in *CONF*2021.	decision

2021-689	This paper presents a novel method for general-purpose supervised domain transfer that trains both generator and discriminator to compete in a minimax game in order to reconstruct data.	abstract
2021-689	This setup is meant to address a common issue in conditional GAN setups: they often ignore conditioning information.	abstract
2021-689	Results are positive and span two very different tasks: image-to-image translation and silent-video-to-speech reconstruction.	abstract
2021-689	Overall reviewers were quite positive about this paper: they found the method to be novel and well-motivated, and after rebuttal, found experimental results to be sufficiently convincing.	rating_summary
2021-689	Several concerns were brought up: (a) lack of emphasis that the approach is in fact supervised, (b) need for comparisons with stronger or task-specific baselines, (c) lack of description of experimental details for reproducibility, and (d) lack of discussion of ethical implications.	weakness
2021-689	All of these concerns were satisfactorily addressed by authors in rebuttal and reviewers unanimously vote for acceptance.	rating_summary
2021-689	I agree, and recommend this paper be accepted.	decision

2021-700	The reviewers and AC liked the basic idea of how this paper improves on ALISTA, and the initial scores were high.	rating_summary
2021-700	Because the contributions rely quite a lot on empirical demonstrations, the reviewers asked for more experiments, changes to experiments, and timing results.	rebuttal_process
2021-700	The revision and rebuttal addressed most of these requests.	rebuttal_process
2021-700	The multipath channel estimation problem was interesting though outside the scope of the AC and reviewer's expertise, so it is hard to evaluate how helpful the method is in that particular setting.	weakness

2021-706	The paper proves new rates of convergence for stochastic subgradient under an interpolation condition.	abstract
2021-706	The analysis is rather simple but it produces better rates than previously known, which all reviewers agree is interesting.	strength
2021-706	As pointed out by the reviewers, this work has the potential to help the community better understand optimization with over-parametrized neural networks (where convexity or other related assumptions play a role). <sep>	strength
2021-706	To the authors, please add a citation to Pegasos as requested by the reviewers.	suggestion

2021-717	The paper proposes multiplicative filter networks (GaborNet and FourierNet) as functional approximations of deepnets.	abstract
2021-717	The proposed networks are a sequence of multiplications linear functions of sinusoidal or Gabor filters.	abstract
2021-717	The authors show that in some cases the performance of proposed networks outperforms the existing deepnets using ReLu activations.	abstract
2021-717	This representation is notably simpler as well.	strength
2021-717	Moreover, compared to classical Fourier approach, the proposed method scales to higher dimensions in practice as well. <sep>	strength
2021-717	The downside of the paper is that it is not clear how to empirically use exponentially many Fourier functions.	weakness
2021-717	Moreover, proposed methods have more parameters, and the additional parameters are linear in size of the hidden layer. <sep>	weakness
2021-717	The paper is clearly written and the authors improved the quality of the paper and added additional experiments to support their claim through the review process and I appreciate that.	rebuttal_process

2021-720	This paper provides a privacy-preserving method to boost the sample quality after training a GAN.	abstract
2021-720	The reviewers were unanimous that this paper should be presented at *CONF*, with an important contribution to privacy-preserving GANs.	rating_summary

2021-732	This paper introduces a few variants of neural ODE architectures to improve their expressivity.	abstract
2021-732	The motivation and method make sense, but are fairly incremental.	weakness
2021-732	The tasks are also fairly low dimensional and as one reviewer pointed out, reconstruction isn't a good benchmark task. <sep>	weakness
2021-732	However, the paper seems well-executed, and the rebuttals answered the expert rewiewers' concerns.	rebuttal_process

2021-736	Congratulations!	decision
2021-736	The reviewers unanimously viewed this work positively and were in favor of acceptance to *CONF*. <sep>	rating_summary
2021-736	While the current revision already addresses many reviewer concerns, it may be worth adding some of the datasets pointed out by R3 or comparing to some of the papers suggested by R1.	suggestion

2021-762	This paper extends an earlier work with scalar output to vector output.	abstract
2021-762	It establish a relationship of two-layer ReLu network and convex program.	abstract
2021-762	The result can be used to design training algorithms for ReLu networks with provably computational complexity.	abstract
2021-762	Overall, this is an interesting idea, leading to better theoretical insights to computational issues of two-layer ReLu networks.	strength

2021-776	The paper proposes a hybrid VAE-normalizing-flow for extracting local and global representations of images.	abstract
2021-776	While the reviewers found the model itself to be "conceptually simple" and "straightforward", all were convinced by the empirical evaluation that, indeed, interesting representation learning is going on, resulting in a unanimous vote to accept.	rating_summary

2021-788	The paper looks into performance of a single network vs ensemble CNN networks of similar no.	abstract
2021-788	of parameters, through lens of accuracy, training time, memory used, inference time. <sep>	abstract
2021-788	the authors show that after some threshold, the ensemble model starts to outperform a single model and make better use of its capacity. <sep>	abstract
2021-788	although this is not the first paper to look into this question and there are two other earlier results from this year, the current paper looks into more measures and not just accuracy. <sep>	abstract
2021-788	Although initially the paper only looked at over-parameterized regime, the authors added experiments on under-parametrized case as well.	rebuttal_process
2021-788	moreover, the authors address the issue of only looking into small and medium sized datasets by adding two more ImageNet experiments. <sep>	rebuttal_process
2021-788	I thank the authors for engaging with the reviewers, addressing their comments and updating the paper accordingly. <sep>	misc
2021-788	It's of interest for follow up work to consider large data regime and transformer style models as well.	suggestion

2021-824	The paper presents a Bayesian approach for classification able  to  adapt  to  novel  classes  given  only  a  few  labeled  examples.	abstract
2021-824	The models combines a one-vs-each approximation of the likelihood combined with a Gaussian process.	abstract
2021-824	This allows to resort to a data-augmentation scheme based on Polya-gamma random variables. <sep>	abstract
2021-824	The paper is clearly written and combines existing techniques in a convincing manner; the experiments demonstrate better accuracy and uncertainty quantification on benchmark datasets. <sep>	strength
2021-824	I recommend acceptance.	decision

2021-826	The paper introduces new methods and building blocks to improve hyperbolic neural networks, including a tighter parameterization of fully connected layers, convolution, and concatenate/split operations to define a version of hyperbolic multi-head attention.	abstract
2021-826	The paper is well written and relevant to the *CONF* community.	strength
2021-826	The proposed methods offer solid improvements over previous approaches in various aspects of constructing hyperbolic neural networks and also extends their applicability.	strength
2021-826	As such, the paper provides valuable contributions to advance research in learning non-Euclidean representations and HNNs.	strength
2021-826	All reviewers and the AC support acceptance for the paper's contributions.	decision
2021-826	Please consider revising your paper to take feedback from reviewers after reubttal into account.	suggestion

2021-832	The paper presents a new algorithm, BOIL, on the importance of representation change vs reuse in MAML.	abstract
2021-832	All reviewers found the paper insightful, with some proposing a few changes to make the paper even stronger.	strength
2021-832	Like them, I recommend accepting the paper.	decision

2021-847	This paper introduces an alternative to self-attention, based on matrix factorization, and apply it to computer vision problems such as semantic segmentation.	abstract
2021-847	The method is simple and novel and obtains competitive results compared to existing approaches.	strength
2021-847	The reviewers found the paper well written and easy to understand.	strength
2021-847	For these reasons, I recommend to accept the paper.	decision

2021-851	This paper focuses on adversarial robustness with unlabeled data.	abstract
2021-851	The philosophy behind sounds quite interesting to me, namely, utilizing unlabeled data to enforce labeling consistency while reducing adversarial transferability among the networks via diversity regularizers.	abstract
2021-851	This philosophy leads to a novel algorithm design I have never seen, ie, ARMOURED, an adversarially robust training method based on semi-supervised learning. <sep>	strength
2021-851	The clarity and novelty are clearly above the bar of *CONF*.	decision
2021-851	While the reviewers had some concerns on the significance, the authors did a particularly good job in their rebuttal.	rebuttal_process
2021-851	Thus, all of us have agreed to accept this paper for publication!	decision
2021-851	Please carefully address all comments in the final version.	suggestion

2021-860	The paper questions the use of cross-entropy loss for classification tasks and shows that using squared error loss can work just as well for deep neural networks.	abstract
2021-860	The authors conduct extensive experiments across ASR, NLP, and CV tasks.	abstract
2021-860	Comparing cross-entropy to squared error loss is certainly not novel, but the conclusions of the paper, backed by a lot of experimental evidence, are certainly thought-provoking. <sep>	strength
2021-860	I would have liked to see a bit more analysis into the results of the paper, and perhaps a bit more theoretical justification.	suggestion
2021-860	That said, the paper will be of interest to the community, given the ubiquity of classification tasks.	strength

2021-2124	The decision for this paper is quite difficult: the methodological ideas are interesting, such as learning the generators directly, but the experimental results are relatively weak.	weakness
2021-2124	Perhaps learning the generators is too unconstrained.	weakness
2021-2124	Moreover, the proposed 'L-conv' builds heavily on prior work such as the 'LieConv' model of Finzi et al, which is not made very transparent in the current narrative.	weakness
2021-2124	And LieConv does provide better performance than L-Conv --- this should also be made clear in the text.	weakness
2021-2124	This was a very difficult call, and after extensive discussion, the decision is intended to be in the long term best interests of the paper.	misc
2021-2124	The ideas are interesting and warmly appreciated, the reviewers appreciated aspects of the response, and the project is sufficiently promising that it was felt that their impact would be much greater if the experimental execution were strengthened, such that the project largely terminating at this point would do it a disservice in the long run.	strength
2021-2124	There was a general feeling that this is still a 'work in a progress' and was somewhat rush-written.	weakness
2021-2124	The authors are strongly encouraged to continue pursuing this work, strengthening the experiments and narrative,  as above.	suggestion

