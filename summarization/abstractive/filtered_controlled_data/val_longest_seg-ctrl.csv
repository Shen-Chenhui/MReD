text,summary
"abstract | strength | rebuttal_process | strength | decision  ==> This paper borrows the classic idea of spectral regularization, recently applied to deep learning by Yoshida and Miyato (2017) and use it to normalize GAN objectives. The ensuing GAN, coined SN-GAN, essentially ensures the Lipschitz property of the discriminator. This Lipschitz property has already been proposed by recent methods and has showed some success. However,  the authors here argue that spectral normalization is more powerful; it allows for models of higher rank (more non-zero singular values) which implies a more powerful discriminator and eventually more accurate generator. This is demonstrated in comparison to weight normalization in Figure 4. The experimental results are very good and give strong support for the proposed normalization. <sep> While the main idea is not new to machine learning (or deep learning), to the best of my knowledge it has not been applied on GANs. The paper is overall well written (though check Comment 3 below), it covers the related work well and it includes an insightful discussion about the importance of high rank models. I am recommending acceptance, though I anticipate to see a more rounded evaluation of the exact mechanism under which SN improves over the state of the art. More details in the comments below. <sep> Comments: <sep> 1. One concern about this paper is that it doesn't fully answer the reasons why this normalization works better. I found the discussion about rank to be very intuitive, however this intuition is not fully tested.  Figure 4 reports layer spectra for SN and WN. The authors claim that other methods, like (Arjovsky et al. 2017) also suffer from the same rank deficiency. I would like to see the same spectra included. <sep> 2. Continuing on the previous point: maybe there is another mechanism at play beyond just rank that give SN its apparent edge? One way to test the rank hypothesis and better explain this method is to run a couple of truncated-SN experiments. What happens if you run your SN but truncate its spectrum after every iteration in order to make it comparable to the rank of WN? Do you get comparable inception scores? Or does SN still win? <sep> 3. Section 4 needs some careful editing for language and grammar.","This paper presents impressive results on scaling GANs to ILSVRC2012 dataset containing a large number of classes. To achieve this, the authors propose ""spectral normalization"" to normalize weights and stabilize training which turns out to help in overcoming mode collapse issues. The presented methodology is principled and well written. The authors did a good job in addressing reviewer's comments and added more comparative results on related approaches to demonstrate the superiority of the proposed methodology. The reviewers agree that this is a great step towards improving the training of GANs. I recommend acceptance."
"abstract | strength | rebuttal_process  ==> [Edit: After revisions, the authors have made a good-faith effort to improve the clarity and presentation of their paper: figures have been revised, key descriptions have been added, and (perhaps most critically) a couple of small sections outlining the contributions and significance of this work have been written. In light of these changes, I've updated my score.] <sep> Summary: <sep> The authors aim to overcome one of the central limitations of intrinsically motivated goal exploration algorithms by learning a representation without relying on a ""designer"" to manually specify the space of possible goals. This work is significant as it would allow one to learn a policy in complex environments even in the absence of a such a designer or even a clear notion of what would constitute a ""good"" distribution of goal states. <sep> However, even after multiple reads, much of the remainder of the paper remains unclear. Many important details, including the metrics by which the authors evaluate performance of their work, can only be found in the appendix; this makes the paper very difficult to follow. <sep> There are too many metrics and too few conclusions for this paper. The authors introduce a handful of metrics for evaluating the performance of their approach; I am unfamiliar with a couple of these metrics and there is not much exposition justifying their significance and inclusion in the paper. Furthermore, there are myriad plots showing the performance of the different algorithms, but very little explanation of the importance of the results. For instance, in the middle of page 9, it is noted that some of the techniques ""yield almost as low performance as"" the randomized baseline, yet no attempt is made to explain why this might be the case or what implications it has for the authors' approach. This problem pervades the paper: many metrics are introduced for how we might want to evaluate these techniques, yet there is no provided reason to prefer one over another (or even why we might want to prefer them over the classical techniques). <sep> Other comments: <sep> - There remain open questions about the quality of the MSE numbers; there are a number of instances in which the authors cite that the ""Meta-Policy MSE is not a simple to interpret"" (The remainder of this sentence is incomplete in the paper), yet little is done to further justify why it was used here, or why many of the deep representation techniques do not perform very well. <sep> - The authors do not list how many observations they are given before the deep representations are learned. Why is this? Additionally, is it possible that not enough data was provided? <sep> - The authors assert that 10 dimensions was chosen arbitrarily for the size of the latent space, but this seems like a hugely important choice of parameter. What would happen if a dimension of 2 were chosen? Would the performance of the deep representation models improve? Would their performance rival that of RGE-FI? <sep> - The authors should motivate the algorithm on page 6 in words before simply inserting it into the body of the text. It would improve the clarity of the paper. <sep> - The authors need to be clearer about their notation in a number of places. For instance, they use \\gamma to represent the distribution of goals, yet it does not appear on page 7, in the experimental setup. <sep> - It is never explicitly mentioned exactly how the deep representation learning methods will be used. It is pretty clear to those who are familiar with the techniques that the latent space is what will be used, but a few equations would be instructive (and would make the paper more self-contained). <sep> In short, the paper has some interesting ideas, yet lacks a clear takeaway message. Instead, it contains a large number of metrics and computes them for a host of different possible variations of the proposed techniques, and does not include significant explanation for the results. Even given my lack of expertise in this subject, the paper has some clear flaws that need addressing. <sep> Pros: <sep> - A clear, well-written abstract and introduction <sep> - While I am not experienced enough in the field to really comment on the originality, it does seem that the approach the authors have taken is original, and applies deep learning techniques to avoid having to custom-design a ""feature space"" for their particular family of problems. <sep> Cons: <sep> - The figure captions are all very ""matter-of-fact"" and, while they explain what each figure shows, provide no explanation of the results. The figure captions should be as self-contained as possible (I should be able to understand the figures and the implications of the results from the captions alone). <sep> - There is not much significance in the current form of the paper, owing to the lack of clear message. While the overarching problem is potentially interesting, the authors seem to make very little effort to draw conclusions from their results. I.e. it is difficult for me to easily visualize all of the ""moving parts"" of this work: a figure showing the relationship bet <sep> - Too many individual ideas are presented in the paper, hurting clarity. As a result, the paper feels scattered. The authors do not have a clear message that neatly ties the results together.","This paper aims to improve on the intrinsically motivated goal exploration framework by additionally incorporating representation learning for the space of goals. The paper is well motivated and follows a significant direction of research, as agreed by all reviewers. In particular, it provides a means for learning in complex environments, where manually designed goal spaces would not be available in practice. There had been significant concerns over the presentation of the paper, but the authors put great effort in improving the manuscript according to the reviewers' suggestions, raising the average rating by 2 points after the rebuttal."
"strength | weakness  ==> ==Update== <sep> I appreciate the response, and continue to recommend acceptance. The evaluation metric used in this paper (SentEval) represents an important open problem in NLP—learning reusable sentence representations—and one of the problems in NLP best suited to presentation at IC*LR*. Because of this, I'm willing to excuse the fact that the paper is only moderately novel, in light of the impressive reported results. <sep> While I would appreciate a direct (same codebase, same data) comparison with some outside baselines, this paper meets or exceeds the standards for rigor that were established by previous published work in the area, and the existing results are sufficient to support some substantial conclusions. <sep> ========== <sep> This paper proposes an alternative formulation of Kiros's SkipThought objective for training general-purpose sentence encoder RNNs on unlabeled data. This formulation replaces the decoder in that model with a second encoder, and yields substantial improvements to both speed and model performance (as measured on downstream transfer tasks). The resulting model is, for the first time, reasonably competitive even with models that are trained end-to-end on labeled data for the downstream tasks (despite the requirement, imposed by the evaluation procedure, that only the top layer classifier be trained for the downstream tasks here), and is also competitive with models trained on large labeled datasets like SNLI. The idea is reasonable, the topic is important, and the results are quite strong. I recommend acceptance, with some caveats that I hope can be addressed. <sep> Concerns: <sep> A nearly identical idea to the core idea of this paper was proposed in an arXiv paper this spring, as a commenter below pointed out. That work has been out for long enough that I'd urge you to cite it, but it was not published and it reports results that are far less impressive than yours, so that omission isn't a major problem. <sep> I'd like to see more discussion of how you performed your evaluation on the downstream tasks. Did you use the SentEval tool from Conneau et al., as several related recent papers have? If not, does your evaluation procedure differ from theirs or Kiros's in any meaningful way? <sep> I'm also a bit uncomfortable that the paper doesn't directly compare with any baselines that use the exact same codebase, word representations, hyperparameter tuning procedure, etc.. I would be more comfortable with the results if, for example, the authors compared a low-dimensional version of their model with a low-dimensional version of SkipThought, trained in the *exact* same way, or if they implemented the core of their model within the SkipThought codebase and showed strong results there. <sep> Minor points: <sep> The headers in Table 1 don't make it all that clear which additions (vectors, UMBC) are cumulative with what other additions. This should be an easy fix. <sep> The use of the check-mark as an output in Figure 1 doesn't make much sense, since the task is not binary classification. <sep> ""Instead of training a model to reconstruct the surface form of the input sentence or its neighbors, our formulation attempts to focus on the semantic aspects of sentences. The meaning of a sentence is the property that creates bonds between a sequence of sentences and makes it logically flow."" – It's hard to pin down exactly what this means, but it sounds like you're making an empirical claim here: semantic information is more important than non-semantic sources of variation (syntactic/lexical/morphological factors) in predicting the flow of a text. Provide some evidence for this, or cut it. <sep> You make a similar claim later in the same section: ""In figure 1(a) however, the reconstruction loss forces the model to predict local structural information about target sentences that may be irrelevant to its meaning (e.g., is governed by grammar rules)."" This is a testable prediction: Are purely grammatical (non-semantic) variations in sentence form helpful for your task? I'd suspect that they are, at least in some cases, as they might give you clues as to style, dialect, or framing choices that the author made when writing that specific passage. <sep> ""Our best BookCorpus model (MC-QT) trains in just under 11hrs, compared to skip-thought model's training time of 2 weeks."" –  If you say this, you need to offer evidence that your model is faster. If you don't use the same hardware and low-level software (i.e., CuDNN), this comparison tells us nearly nothing. The small-scale replication of SkipThought described above should address this issue, if performed.","Though the approach is not terribly novel, it is quite effective (as confirmed on a wide range of evaluation tasks). The approach is simple and likely to be useful in applications. The paper is well written. <sep> + simple and efficient <sep> + high quality evaluation <sep> + strong results <sep> - novelty is somewhat limited"
"strength | abstract | rebuttal_process | strength  ==> The authors present an interesting application of Graph Neural Networks to learning policies for controlling ""centipede"" robots of different lengths. They leverage the non-parametric nature of graph neural networks to show that their approach is capable of transferring policies to different robots more quickly than other approaches. The significance of this work is in its application of GNNs to a potentially practical problem in the robotics domain. The paper suffers from some clarity/presentation issues that will need to be improved. Ultimately, the contribution of this paper is rather specific, yet the authors show the clear advantage of their technique for improved performance and transfer learning on some agent types within this domain. <sep> Some comments: <sep> - Significant: A brief statement of the paper's ""contributions"" is also needed; it is unclear at first glance what portions of the work are the authors' own contributions versus prior work, particularly in the section describing the GNN theory. <sep> - Abstract: I take issue with the phrase ""are significantly better than policies learned by other models"", since this is not universally true. While there is a clear benefit to their technique for the centipede and snake models, the performance on the other agents is mostly comparable, rather than ""significantly better""; this should be reflected in the abstract. <sep> - Figure 1 is instructive, but another figure is needed to better illustrate the algorithm (including how the state of the world is mapped to the graph state h, how these ""message"" are passed between nodes, and how the final graph states are used to develop a policy). This would greatly help clarity, particularly for those who have not seen GNNs before, and would make the paper more self-contained and easier to follow. The figure could also include some annotated examples of the input spaces of the different joints, etc. Relatedly, Sec. 2.2.2 is rather difficult to follow because of the lack of a figure or concrete example (an example might help the reader understand the procedure without having to develop an intuition for GNNs). <sep> - There is almost certainly a typo in Eq. (4), since it does not contain the aggregated message \\bar{m}_u^t. <sep> Smaller issues / typos: <sep> - Abstract: please spell out spell out multi-layer perceptrons (MLP). <sep> - Sec 2.2: ""servers"" should be ""serves"" <sep> - ""performance By"" on page 4 is missing a ""."" <sep> Pros: <sep> - The paper presents an interesting application of GNNs to the space of reinforcement learning and clearly show the benefits of their approach for the specific task of transfer learning. <sep> - To the best of my knowledge, the paper presents an original result and presents a good-faith effort to compare to existing, alternative systems (showing that they outperform on the tasks of interest). <sep> Cons: <sep> - The contributions of the paper should be more clearly stated (see comment above). <sep> - The section describing their approach is not ""self contained"" and is difficult for an unlearned reader to follow. <sep> - The problem the authors have chosen to tackle is perhaps a bit ""specific"", since the performance of their approach is only really shown to exceed the performance on agents, like centipedes or snakes, which have this ""modular"" quality. <sep> I certainly hope the authors improve the quality of the theory section; the poor presentation here brings down the rest of the paper, which is otherwise an easy read.","An interesting application of graph neural networks to robotics. The body of a robot is represented as a graph, and the agent's policy is defined using a graph neural network (GNNs/GCNs) over the graph structure. <sep> The GNN-based policy network perform on par with best methods on traditional benchmarks, but shown to be very effective for transfer scenarios: changing robot size or disabling its components. I believe that the reviewers' concern that the original experiments focused solely on centepedes and snakes were (at least partially) addressed in the author response: they showed that their GNN-based model outperforms MLPs on a dataset of 2D walkers. <sep> Overall: <sep> -- an interesting application <sep> -- modeling robot morphology is an under-explored direction <sep> -- the paper is well written <sep> -- experiments are sufficiently convincing (esp. after addressing the concerns re diversity and robustness)."
"abstract | strength  ==> = Quality = <sep> Overall, the authors do a good job of placing their work in the context of related research, and employ a variety of non-trivial technical details to get their methods to work well. <sep> = Clarity = <sep> Overall, the exposition regarding the method is good. I found the setup for the sequence tagging experiments confusing, tough. See more comments below. <sep> = Originality / Significance = <sep> The paper presents a clever idea that could help make SPENs more practical. The paper's results also suggest that we should be thinking more broadly about how to using complicated structured distributions as teachers for model compression. <sep> = Major Comment = <sep> I'm concerned by the quality of your results and the overall setup of your experiments. In particular, the principal contribution of the sequence tagging experiments seems top be different than what is advertised earlier on in the paper. <sep> Most of your empirical success is obtained by taking a pretrained CRF energy function and using this as a teacher model to train a feed-forward inference network. You have have very few experiments using a SPEN energy function parametrization that doesn't correspond to a CRF, even though you could have used an arbitrary convnet, RNN, etc. The one exception is when you use the tag language model. This is a good idea, but it is pretrained, not trained using the saddle-point objective you introduce. In fact, you don't have any results demonstrating that the saddle-point approach is better than simpler alternatives. <sep> It seems that you could have written a very different paper about model compression with CRFs that would have been very interesting and you could've have used many of the same experiments. It's unclear why SPENs are so important. The idea of amortizing inference is perhaps more general. My recommendation is that you either rebrand the paper to be more about general methods for amortizing structured prediction inference using model compression or do more fine-grained experiments with SPENs that demonstrate empirical gains that leverage their flexible deep-network-based energy functions. <sep> = Minor Comments = <sep> * You should mention 'Energy Based GANs"" <sep> * I don't understand ""This approach performs backpropagation through each step of gradient descent, permitting more stable training but also evidently more overfitting."" Why would it overfit more? Simply because training was more stable? Couldn't you prevent overfitting by regularizing more? <sep> * You spend too much space talking about specific hyperparameter ranges, etc. This should be moved to the appendix. You should also add a short summary of the TLM architecture to the main paper body. <sep> * Regarding your footnote discussing using a positive vs. negative sign on the entropy regularization term, I recommend checking out ""Regularizing neural networks by penalizing confident output distributions."" <sep> * You should add citations for the statement ""In these and related settings, gradient descent has started to be replaced by inference networks."" <sep> * I didn't find Table 1 particularly illuminating. All of the approaches seem to perform about the same. What conclusions should I make from it? <sep> * Why not use KL divergence as your \\Delta function? <sep> * Why are the results in Table 5 on the dev data? <sep> * I was confused by Table 4. First of all, it took me a very long time to figure out that the middle block of results corresponds to taking a pretrained CRF energy and amortizing inference by training an inference network. This idea of training with a standard loss (conditional log lik.) and then amortizing inference post-hoc was not explicitly introduced as an alternative to the saddle point objective you put forth earlier in the paper. Second, I was very surprised that the inference network outperformed Viterbi (89.7 vs. 89.1 for the same CRF energy). Why is this? <sep> * I'm confused by the difference between Table 6 and Table 4? Why not just include the TLM results in Table 4?","The submission modifies the SPEN framework for structured prediction by adding an inference network in place of the usual combinatorial optimization based inference. The resulting architecture has some similarity to a GAN, and significantly increases the speed of inference. <sep> The submission provides links between two seemingly different frameworks: SPENs and GANs. By replacing inference with a network output, the connection is made, but importantly, this massively speeds up inference and may mark an important step forward in structured prediction with deep learning."
"misc | abstract | rating_summary | strength | rebuttal_process | strength  ==> This paper presents a variational inference algorithm for models that contain deep neural network components and probabilistic graphical model (PGM) <sep> components. <sep> The algorithm implements natural-gradient message-passing where the messages automatically reduce to stochastic gradients for the non-conjugate neural network components. The authors demonstrate the algorithm on a Gaussian mixture model and linear dynamical system where they show that the proposed algorithm outperforms previous algorithms. Overall, I think that the paper proposes some interesting ideas, however, in its current form I do not think that the novelty of the contributions are clearly presented and that they are not thoroughly evaluated in the experiments. <sep> The authors propose a new variational inference algorithm that handles models with deep neural networks and PGM components. However, it appears that the authors rely heavily on the work of (Khan & Lin, 2017) that actually provides the algorithm. As far as I can tell this paper fits inference networks into the algorithm proposed in (Khan & Lin, 2017) which boils down to i) using an inference network to generate potentials for a conditionally-conjugate distribution and ii) introducing new PGM parameters to decouple the inference network from the model parameters. These ideas are a clever solution to work inference networks into the message-passing algorithm of (Khan & Lin, 2017), <sep> but I think the authors may be overselling these ideas as a brand new algorithm. <sep> I think if the authors sold the paper as an alternative to (Johnson, et al., 2016) <sep> that doesn't suffer from the implicit gradient problem the paper would fit into the existing literature better. <sep> Another concern that I have is that there are a lot of conditiona-conjugacy assumptions baked into the algorithm that the authors only mention at the end of the presentation of their algorithm. Additionally, the authors briefly state that they can handle non-conjugate distributions in the model by just using conjugate distributions in the variational approximation. Though one could do this, the authors do not adequately show that one should, or that one can do this without suffering a lot of error in the posterior approximation. I think that without an experiment the small section on non-conjugacy should be removed. <sep> Finally, I found the experimental evaluation to not thoroughly demonstrate the advantages and disadvantages of the proposed algorithm. The algorithm was applied to the two models originally considered in (Johnson, et al., 2016) and the proposed algorithm was shown to attain lower mean-square errors for the two models. The experiments do not however demonstrate why the algorithm is performing better. For instance, is the (Johnson, et al., 2016) algorithm suffering from the implicit gradient? It also would have been great to have considered a model that the (Johnson, et. al., 2016) algorithm would not work well on or could not be applied to show the added applicability of the proposed algorithm. <sep> I also have some minor comments on the paper: <sep> - There are a lot of typos. <sep> - The first two sentences of the abstract do not really contribute anything to the paper. What is a powerful model? What is a powerful algorithm? <sep> - DNN was used in Section 2 without being defined. <sep> - Using p() as an approximate distribution in Section 3 is confusing notation because p() was used for the distributions in the model. <sep> - How is the covariance matrix parameterized that the inference network produces? <sep> - The phrases ""first term of the inference network"" are not clear. Just use The <sep> DNN term and the PGM term of the inference networks, and better still throw in a reference to Eq. (4). <sep> - The term ""deterministic parameters"" was used and never introduced. <sep> - At the bottom of page 5 the extension to the non-conjugate case should be presented somewhere (probably the appendix) since the fact that you can do this is a part of your algorithm that's important.","Thank you for submitting you paper to *CONF*. The paper presents a general approach for handling inference in probabilistic graphical models that employ deep neural networks. The framework extends Jonhson et al. (2016) and Khan & Lin (2017). The reviewers are all in agreement that the paper is suitable for publication. The paper is well written and the use of examples to illustrate the applicability of the methods brings great clarity. The experiments are not the strongest suit of the paper and, although the revision has improved this aspect, I would encourage a more comprehensive evaluation of the proposed methods. Nevertheless, this is a strong paper."
"misc | strength | decision  ==> The paper proposes action-dependent baselines for reducing variance in policy gradient, through the derivation based on Stein's identity and control functionals. The method relates closely to prior work on action-dependent baselines, but explores in particular on-policy fitting and a few other design choices that empirically improve the performance. <sep> A criticism of the paper is that it does not require Stein's identity/control functionals literature to derive Eq. 8, since it can be derived similarly to linear control variate and it has also previously been discussed in IPG [Gu et. al., 2017] as reparameterizable control variate. The derivation through Stein's identity does not seem to provide additional insights/algorithm designs beyond direct derivation through reparameterization trick. <sep> The empirical results appear promising, and in particular in comparison with Q-Prop, which fits Q-function using off-policy TD learning. However, the discussion on the causes of the difference should be elaborated much more, as it appears there are substantial differences besides on-policy/off-policy fitting of the Q, such as: <sep> -FitLinear fits linear Q (through parameterization based on linearization of Q) using on-policy learning, rather than fitting nonlinear Q and then at application time linearize around the mean action. A closer comparison would be to use same locally linear Q function for off-policy learning in Q-Prop. <sep> -The use of on-policy fitted value baseline within Q-function parameterization during on-policy fitting is nice. Similar comparison should be done with off-policy fitting in Q-Prop. <sep> I wonder if on-policy fitting of Q can be elaborated more. Specifically, on-policy fitting of V seems to require a few design details to have best performance [GAE, Schulman et. al., 2016]: fitting on previous batch instead of current batch to avoid overfitting  (this is expected for your method as well, since by fitting to current batch the control variate then depends nontrivially on samples that are being applied), and possible use of trust-region regularization to prevent V from changing too much across iterations. <sep> The paper presents promising results with direct on-policy fitting of action-dependent baseline, which is promising since it does not require long training iterations as in off-policy fitting in Q-Prop. As discussed above, it is encouraged to elaborate other potential causes that led to performance differences. The experimental results are presented well for a range of Mujoco tasks. <sep> Pros: <sep> -Simple, effective method that appears readily available to be incorporated to any on-policy PG methods without significantly increase in computational time <sep> -Good empirical evaluation <sep> Cons: <sep> -The name Stein control variate seems misleading since the algorithm/method does not rely on derivation through Stein's identity etc. and does not inherit novel insights due to this derivation.",Thank you for submitting you paper to *CONF*. The reviewers agree that the paper's development of action-dependent baselines for reducing variance in policy gradient is a strong contribution and that the use of Stein's identity to provide a principled way to think about control variates is sensible. The revision clarified an number of the reviewers' questions and the resulting paper is suitable for publication in *CONF*.
"decision | strength | weakness | decision  ==> [ =========================== REVISION ===============================================================] <sep> I am satisfied with the answers to my questions. The paper still needs some work on clarity, and authors defer the changes to the next version (but as I understood, they did no changes for this paper as of now), which is a bit frustrating. However I am fine accepting it. <sep> [ ============================== END OF REVISION =====================================================] <sep> This paper concerns with addressing the issue of SGD not converging to the optimal parameters on one hidden layer network for a particular type of data and label (gaussian features, label generated using a particular function that should be learnable with neural net). Authors demonstrate empirically that this particular learning problem is hard for SGD with l2 loss (due to apparently bad local optima) and suggest two ways of addressing it, on top of the known way of dealing with this problem (which is overparameterization). First is to use a new activation function, the second is by designing a new objective function that has only global optima and which can be efficiently learnt with SGD <sep> Overall the paper is well written. The authors first introduce their suggested loss function and then go into details about what inspired its creation. I do find interesting the formulation of population risk in terms of tensor decomposition, this is insightful <sep> My issues with the paper are as follows: <sep> - The loss function designed seems overly complicated. On top of that authors notice that to learn with this loss efficiently, much larger batches had to be used. I wonder how applicable this in practice - I frankly didn't see insights here that I can apply to other problems that don't fit into this particular narrowly defined framework <sep> - I do find it somewhat strange that no insight to the actual problem is provided (e.g. it is known empirically but there is no explanation of what actually happens and there is a idea that it is due to local optima), but authors are concerned with developing new loss function that has provable properties about global optima. Since it is all empirical, the first fix (activation function) seems sufficient to me and new loss is very far-fetched. <sep> - It seems that changing activation function from relu to their proposed one fixes the problem without their new loss, so i wonder whether it is a problem with relu itself and may be other activations funcs, like sigmoids will not suffer from the same problem <sep> - No comparison with overparameterization in experiments results is given, which makes me wonder why their method is better. <sep> Minor: fix margins in formula 2.7.","I recommend acceptance based on the reviews. The paper makes novel contributions to learning one-hidden layer neural networks and designing new objective function with no bad local optima. <sep> There is one point that the paper is missing. It only mentions Janzamin et al in the passing. Janzamin et al propose using score function framework for designing alternative objective function. For the case of Gaussian input that this paper considers, the score function reduces to Hermite polynomials. Lack of discussion about this connection is weird. There should be proper acknowledgement of prior work. Also missing are some of the key papers on tensor decomposition and its analysis <sep> I think there are enough contributions in the paper for acceptance irrespective of the above aspect."
"strength | rebuttal_process | suggestion  ==> This is a nice paper proposing a simple but effective heuristic for generating adversarial examples from class labels with no gradient information or class probabilities. Highly relevant prior work was overlooked and there is no theoretical analysis, but I think this paper still makes a valuable contribution worth sharing with a broader audience. <sep> What this paper does well: <sep> - Suggests a type of attack that hasn't been applied to image classifiers <sep> - Proposes a simple heuristic method for performing this attack <sep> - Evaluates the attack on both benchmark neural networks and a commercial system <sep> Problems and limitations: <sep> 1. No theoretical analysis. Under what conditions does the boundary attack succeed or fail? What geometry of the classification boundaries is necessary? How likely are those conditions to hold? Can we measure how well they hold on particular networks? <sep> Since there is no theoretically analysis, the evidence for effectiveness is entirely empirical. That weakens the paper and suggests an important area of future work, but I think the empirical evidence is sufficient to show that there's something interesting going. Not a fatal flaw. <sep> 2. Poor framing. The paper frames the problem in terms of ""machine learning models"" in general (beginning with the first line of the abstract), but it only investigates image classification. There's no particular reason to believe that all machine learning algorithms will behave like convolutional neural network image classifiers. Thus, there's an implicit claim of generality that is not supported. <sep> This is a presentation issue that is easily fixed. I suggest changing the title to reflect this, or at least revising the abstract and introduction to make the scope clearer. <sep> A minor presentation quibble/suggestion: ""adversarial"" is used in this paper to refer to any class that differs from the true class of the instance to be disguised. But an image of a dalmation that's labeled as a dalmation isn't adversarial -- it's just a different image that's labeled correctly. The adversarial process is about constructing something that will be mislabeled, exploiting some kind of weakness that doesn't show up on a natural distribution of inputs. I suggest rewording some of the mentions of adversarial. <sep> 3. Ignorance of prior work. Finding deceptive inputs using only the classifier output has been done by Lowd and Meek (KDD 2005) for linear classifiers and Nelson et al. (AISTATS 2010, JMLR 2012) for convex-inducing classifiers. Both works include theoretical bounds on the number of queries required for near-optimal adversarial examples. Biggio et al. (ECML 2013) further propose training a surrogate classifier on similar training data, using the predictions of the target classifier to relabel the training data. In this way, decision information from the target model is used to help train a more similar surrogate, and then attacks can be transferred from the surrogate to the target. <sep> Thus, ""decision-based attacks"" are not new, although the algorithm and experiments in this paper are. <sep> Overall, I think this paper makes a worthwhile contribution, but needs to revise the claims to match what's done in the paper and what's been done before.","The reviewers all agree this is a well written and interesting paper describing a novel black box adversarial attack. There were missing relevant references in the original submission, but these have been added. I would suggest the authors follow the reviewer suggestions on claims of generality beyond CNN; although there may not be anything obvious stopping this method from working more generally, it hasn't been tested in this work. Even if you keep the title you might be more careful to frame the body in the context of CNN's."
"decision | strength  ==> The paper summarizes and compares some of the current explanation techniques for deep neural networks that rely on the redistribution of relevance / contribution values from the output to the input space. <sep> The main contributions are the introduction of a unified framework that expresses 4 common attribution techniques (Gradient * Input, Integrated Gradient, eps-LRP and DeepLIFT) in a similar way as modified gradient functions and the definition of a new evaluation measure ('sensitivity n') that generalizes the earlier defined properties of 'completeness' and 'summation to delta'. <sep> The unified framework is very helpful since it points out equivalences between the methods and makes the implementation of eps-LRP and DeepLIFT substantially more easy on modern frameworks. However, as correctly stated by the authors some of the unification (e.g. relation between LRP and Gradient*Input) has been already mentioned in prior work. <sep> Sensitivity-n as a measure tries to tackle the difficulty of estimating the importance of features that can be seen either separately or in combination. While the measure shows interesting trends towards a linear behaviour for simpler methods, it does not persuade me as a measure of how well the relevance attribution method mimics the decision making process and does not really point out substantial differences between the different methods. Furthermore, The authors could comment on the relation between sensitivity-n and region perturbation techniques (Samek et al., IEEE TNNLS, 2017). Sensitivtiy-n seems to be an extension of the region perturbation idea to me. <sep> It would be interesting to see the relation between the ""unified"" gradient-based explanation methods and approaches (e.g. Saliency maps, alpha-beta LRP, Deep Taylor, Deconvolution Networks, Grad-CAM, Guided Backprop ...) which do not fit into the unification framework. It's good that the author mention these works, still it would be great to see more discussion on the advantages/disadvantages, because these methods may have some nice theoretically properties (see e.g. the discussion on gradient vs. decompositiion techniques in Montavon et al., Digital Signal Processing, 2017) which can not be incorporated into the unified framework.","With scores of 7-7-6 and the justification below the AC recommends acceptance. <sep> One of the reviewers summarizes why this is a good paper as follows: <sep> ""This paper discusses several gradient based attribution methods, which have been popular for the fast computation of saliency maps for interpreting deep neural networks. The paper provides several advances: <sep> - This gives a more unified way of understanding, and implementing the methods. <sep> - The paper points out situations when the methods are equivalent <sep> - The paper analyses the methods' sensitivity to identifying single and joint regions of sensitivity <sep> - The paper proposes a new objective function to measure joint sensitivity"""
"abstract | rebuttal_process | strength  ==> Summary <sep> ======== <sep> The authors present CLEVER, an algorithm which consists in evaluating the (local) Lipschitz constant of a trained network around a data point. This is used to compute a lower-bound on the minimal perturbation of the data point needed to fool the network. <sep> The method proposed in the paper already exists for classical function, they only transpose it to neural networks. Moreover, the lower bound comes from basic results in the analysis of Lipschitz continuous functions. <sep> Clarity <sep> ===== <sep> The paper is clear and well-written. <sep> Originality <sep> ========= <sep> This idea is not new: if we search for ""Lipschitz constant estimation"" in google scholar, we get for example <sep> Wood, G. R., and B. P. Zhang. ""Estimation of the Lipschitz constant of a function."" (1996) <sep> which presents a similar algorithm (i.e., estimation of the maximum slope with reverse Weibull). <sep> Technical quality <sep> ============== <sep> The main theoretical result in the paper is the analysis of the lower-bound on \\delta, the smallest perturbation to apply on a data point to fool the network. This result is obtained almost directly by writing the bound on Lipschitz-continuous function <sep> | f(y)-f(x) | < L || y-x || <sep> where x = x_0 and y = x_0 + \\delta. <sep> Comments: <sep> - Lemma 3.1: why citing Paulavicius and Zilinskas for the definition of Lipschitz continuity? Moreover, a Lipschitz-continuous function does not need to be differentiable at all (e.g. |x| is Lipschitz with constant 1 but sharp at x=0). Indeed, this constant can be easier obtained if the gradient exists, but this is not a requirement. <sep> - (Flaw?) Theorem 3.2 : This theorem works for fixed target-class since g = f_c - f_j for fixed g. However, once g = min_j f_c - f_j, this theorem is not clear with the constant Lq. Indeed, the function g should be g(x) = min_{k \\neq c} f_c(x) - f_k(x). <sep> Thus its Lipschitz constant is different, potentially equal to <sep> L_q = max_{k} \\| L_q^k \\|, <sep> where L_q^k is the Lipschitz constant of f_c-f_k. If the theorem remains unchanged after this modification, you should clarify the proof. Otherwise, the theorem will work with the maximum over all Lipschitz constants but the theoretical result will be weakened. <sep> - Theorem 4.1: I do not see the purpose of this result in this paper. This should be better motivated. <sep> Numerical experiments <sep> ==================== <sep> Globally, the numerical experiments are in favor of the presented method. The authors should also add information about the time it takes to compute the bound, the evolution of the bound in function of the number of samples and the distribution of the relative gap between the lower-bound and the best adversarial example. <sep> Moreover, the numerical experiments look to be realized in the context of targeted attack. To show the real effectiveness of the approach, the authors should also show the effectiveness of the lower-bound in the context of non-targeted attack. <sep> ####################################################### <sep> Post-rebuttal review <sep> --------------------------- <sep> Given the details the authors provided to my review, I decided to adjust my score. The method is simple and shows to be extremely effective/accurate in practice. <sep> Detailed answers: <sep> 1) Indeed, I was not aware that the paper only focuses on one dimensional functions. However, they still work with less assumption, i.e., with no differential functions. I was pointing out the similarities between their approach and your: the two algorithms (CLEVER and Slope) are basically the same, and using a limit you can go from ""slope"" to ""gradient norm"". <sep> In any case, I have read the revision and the additional numerical experiment to compare Clever with their method is a good point. <sep> 2) "" Overall, our analysis is simple and more intuitive, and we further facilitate numerical calculation of the bound by applying the extreme value theory in this work. "" <sep> This is right. I am just surprised is has not been done before, since it requires only few lines of derivation. I searched a bit but it is not possible to find any kind of similar results. Moreover, this leads to good performances, so there is no needs to have something more complex. <sep> 3) ""The usual Lipschitz continuity is defined in terms of L2 norm and the extension to an arbitrary Lp norm is not straightforward"" <sep> Indeed, people usually use the Lipschitz continuity using the L2norm, but the original definition is wider. <sep> Quickly, if you have a differential, scalar function from a space E -> R, then the gradient is a function from space E to E*, the dual of the space E. <sep> Let || . || the norm of space E. Then, || . ||* is the dual norm of ||.||, and also the norm of E*. <sep> In that case, Lipschitz continuity writes f(x)-f(y) <= L || x-y ||, with L >= max_{x in E*} || f'(x) ||* <sep> In the case where || . || is an \\ell-p norm, then || . ||* is an \\ell-q norm; with 1/p+1/q = 1. <sep> If you are interested, there is a clear and concise explanation in the introduction of this paper: Accelerating the cubic regularization of Newton's method on convex problems, by Yurii Nesterov. <sep> I have no additional remarks for 4) -> 9), since everything is fixed in the new version of the paper.","This paper proposes a new metric to evaluate the robustness of neural networks to adversarial attacks. This metric comes with theoretical guarantees and can be efficiently computed on large-scale neural networks. <sep> Reviewers were generally positive about the strengths of the paper, especially after major revisions during the rebuttal process. The AC believes this paper will contribute to the growing body of literature in robust training of neural networks."
"abstract | strength | weakness | suggestion  ==> The authors propose to use a generative model of images to detect and defend against adverarial examples. White-box attacks against standard models for image recognition (Resnet and VGG) are considered, and a generative model (a PixelCNN) is trained on the same data as the classifiers. The authors first show that adversarial examples created by the white-box attacks correspond to low likelihood region (according to the pixelCNN), which first gives a classification rule for detecting adversarial examples. <sep> Then, to turn the genrative model into a defensive algorithm, the authors propose to preprocess test images by approximately maximizing the likelihood under similar constraints as the attacker of images, to ""project"" adversarial examples back to high-density regions (as estimated by the generative model). As a heuristic method, the authors propose to greedily maximize the likelihood of the incoming images pixel-by-pixel, which is possible because of the specific form of the PixelCNN likelihood in the context of l-infty attacks. An ""adaptive"" version of the algorithm, in which the preprocessing is used only when the likelihood of an example is below a certain threshold, is also proposed. <sep> Experiments are carried out on Fashion MNIST and CIFAR-10. At a high level, the message is that projecting the image into a high density region is sufficient to correct for a significant portions of the mistakes made on adversarial examples. The main result is that this approach based on generative models seems to work even on against the strongest attacks. <sep> Overall, the idea proposed in the paper, using a generative model to detect and filter out spurious patterns that can appear in adversarial examples, is rather intuitive. The experimental result that adversarial examples can somehow be corrected by a generative model is also interesting. The design choice of PixelCNN, which allows for a greedy optimization seems reasonable in that setting. <sep> Whereas the paper is an interesting step forward, the paper still doesn't provide definitive arguments in favor of using such approaches in practice. There is a significant loss in accuracy on clean examples (2% on CIFAR-10 for a resnet), and more generally against weaker opponents such as the fast gradient sign. Thus, in reality, the experiments show that the pipeline generative model + classifier is robust against the strongest white box methods for this classifier, but on the other hand these methods do not transfer well to new models. This somewhat weakens the result, since robustness against these methods that do not transfer well is achieved by changing the model.","The paper studies the use of PixelCNN density models for the detection of adversarial images, which tend to lie in low-probability parts of image space. The work is novel, relevant to the *CONF* community, and appears to be technically sound. <sep> A downside of the paper is its limited empirical evaluation: there evidence suggesting that defenses against adversarial examples that work well on MNIST/CIFAR do not necessarily transfer well to much higher-dimensional datasets, for instance, ImageNet. The paper could, therefore, would benefit from empirical evaluations of the defense on a dataset like ImageNet."
"strength | weakness  ==>  ==> tl;dr: <sep> - The paper has a really cool theoretical contribution. <sep> - The experiments do not directly test whether the theoretical insight holds in practice, but instead a derivate method is tested on various benchmarks. <sep> I must say that this paper has cleared up quite a few things for me. I have always been a skeptic wrt LSTM, since I myself did not fully understand when to prefer them over vanilla RNNs for reasons other than ""they empirically work much better in many domains."" and ""they are less prone to vanishing gradients"". <sep> Section 1 is a bliss: it provides a very useful candidate explanation under which conditions vanilla RNNs fail (or at least, do not efficiently generalise) in contrast to gated cells. I am sincerely happy about the write up and will point many people to it. <sep> The major problem with the paper, in my eyes, is the lack of experiments specific to test the hypothesis. Obviously, quite a bit of effort has gone into the experimental section. The focus however is comparison to the state of the art in terms of raw performance. <sep> That leaves me asking: are gated RNNs superior to vanilla RNNs if the data is warped? <sep> Well, I don't know now. I only can say that there is reason to believe so. <sep> I *really* do encourage the authors to go back to the experiments and see if they can come up with an experiment to test the main hypothesis of the paper. E.g. one could make synthetic warpings, apply it to any data set and test if things work out as expected. Such a result would in my opinion be of much more use than the tiny increment in performance that is the main output of the paper as of now, and which will be stomped by some other trick in the months to come. It would be a shame if such a nice theoretical insight got under the carpet because of that. E.g. today we hold [Pascanu 2013] dear not because of the proposed method, but because of the theoretical analysis. <sep> Some minor points. <sep> - The authors could make use of less footnotes, and try to incorporate them into the text or appendix. <sep> - A table of results would be nice. <sep> - Some choices of the experimental section seem arbitrary, e.g. the use of optimiser and to not use clipping of gradients. In general, the evaluation of the hyper parameters is not rigorous. <sep> - ""abruplty"" -> ""abruptly"" on page 5, 2nd paragraph <sep> ### References <sep> [Pascanu 2013] Pascanu, Razvan, Tomas Mikolov, and Yoshua Bengio. ""On the difficulty of training recurrent neural networks."" International Conference on Machine Learning. 2013.","All the reviews like the theoretical result presented in the paper which relates the gating mechanism of LSTMS (and GRUs) to time invariance / warping. The theoretical result is great and is used to propose a heuristic for setting biases when time invariance scales are known. The experiments are not mind-boggling, but none of the reviewers seem to think that's a show stopper."
"strength | rating_summary | decision  ==> As there are many kinds of domain adaptation problems, the need to mix several learning strategies to improve the existing approaches is obvious. However, this task is not necessarily easy to succeed. The authors proposed a sound approach to learn a proper representation (in an adversarial way) and comply the cluster assumption. <sep> The experiments show that this Virtual Adversarial Domain Adaptation network (VADA) achieves great results when compared to existing learning algorithms. Moreover, we also see the learned model is consistently improved using the proposed ""Decision-boundary Iterative Refinement Training with a Teacher"" (DIRT-T) approach. <sep> The proposed methodology relies on multiple choices that could sometimes be better studied and/or explained. Namely, I would like to empirically see which role of the locally-Lipschitz regularization term (Equation 7). Also, I wonder why this term is tuned by an hyperparameter (lamda_s) for the source, while a single hyperparamer (lambda_t) is used for the sum of the two target quantity. <sep> On the theoretical side, the discussion could be improved. Namely, Section 3 about ""limitation of domain adversarial training"" correctly explained that ""domain adversarial training may not be sufficient for domain adaptation if the feature extraction function has high-capacity"". It would be interesting to explain whether this observation is consistent with Theorem 1 of the paper (due to Ben-David et al., 2010), on which several domain adversarial approaches are based. The need to consider supplementary assumptions (such as ) to achieve good adaptation can also be studied through the lens of more recent Ben-David's work, e.g. Ben-David and Urner (2014). In the latter, the notion of ""Probabilistic Lipschitzness"", which is a relaxation of the ""cluster assumption"" seems very related to the actual work. <sep> Reference: <sep> Ben-David and Urner. Domain adaptation-can quantity compensate for quality?, Ann. Math. Artif. Intell., 2014 <sep> Pros: <sep> - Propose a sound approach to mix two complementary strategies for domain adaptation. <sep> - Great empirical results. <sep> Cons: <sep> - Some choices leading to the optimization problem are not sufficiently explained. <sep> - The theoretical discussion could be improved. <sep> Typos: <sep> - Equation 14: In the first term (target loss), theta should have an index t (I think). <sep> - Bottom of page 6: ""... and that as our validation set"" (missing word).","Well motivated and well written, with extensive results. The paper also received positive comments from all reviewers. The AC recommends that the paper be accepted."
"abstract | rebuttal_process  ==> The paper proposes an extension to a previous monotonic attention model (Raffel et al 2017) to attend to a fixed-sized window up to the alignment position. Both the soft attention approximation used for training the monotonic attention model, and the online decoding algorithm is extended to the chunkwise model. In terms of the model this is a relatively small extention of Raffel et al 2017. <sep> Results show that for online speech recognition the model matches the performance of an offline soft attention baseline, doing significantly better than the monotonic attention model. Is the offline attention baseline unidirectional or bidirectional? In case it is unidirectional it cannot really be claimed that the proposed model's performance is competitive with an offline model. <sep> My concern with the statement that all hyper-parameters are kept the same as the monotonic model is that the improvement might partly be due to the increase in total number of parameters in the model. Especially given that w=2 works best for speech recognition, it not clear that the model extension is actually helping. My other concern is that in speech recognition the time-scale of the encoding is somewhat arbitrary, so possibly a similar effect could be obtained by doubling the time frame through the convolutional layer. While the empirical result is strong it is not clear that the proposed model is the best way to obtain the improvement. <sep> For document summarization the paper presents a strong result for an online model, but the fact that it is still less accurate than the soft attention baseline makes it hard to see the real significance of this. If the contribution is in terms of speed (as shown with the synthetic benchmark in appendix B) more emphesis should be placed on this in the paper. <sep> Sentence summarization tasks do exhibit mostly monotonic alignment, and most previous models with monotonic structure were evaluated on that, so why not test that here? <sep> I like the fact that the model is truely online, but that contribution was made by Raffel et al 2017, and this paper at best proposes a slightly better way to train and apply that model. <sep> --- <sep> The additional experiments in the new version gives stronger support in favour of the proposed model architecture (vs the effect of hyperparameter choices). While I'm still on the fence on whether this paper is strong enough to be accepted for *CONF*, this version is certainly improves the quality of the paper.","This clearly written paper describes a simple extension to hard monotonic attention -- the addition of a soft attention mechanism that operates over a fixed length window of inputs that ends at the point selected by the hard attention mechanism. Experiments on speech recognition (WSJ) and on a document summarization task demonstrate that the new attention mechanism improves significantly over the hard monotonic mechanism. About the only ""con"" the reviewers noted is that the paper is a minor extension over Raffel et al., 2017, but the authors successfully argue that the strong empirical results render this simplicity a ""pro."""
"strength | weakness  ==> ## Review Summary <sep> Overall, the paper's paper core claim, that increasing batch sizes at a linear rate during training is as effective as decaying learning rates, is interesting but doesn't seem to be too surprising given other recent work in this space. The most useful part of the paper is the empirical evidence to backup this claim, which I can't easily find in previous literature. I wish the paper had explored a wider variety of dataset tasks and models to better show how well this claim generalizes, better situated the practical benefits of the approach (how much wallclock time is actually saved? how well can it be integrated into a distributed workflow?), and included some comparisons with other recent recommended ways to increase batch size over time. <sep> ## Pros / Strengths <sep> + effort to assess momentum / Adam / other modern methods <sep> + effort to compare to previous experimental setups <sep> ## Cons / Limitations <sep> - lack of wallclock measurements in experiments <sep> - only ~2 models / datasets examined, so difficult to assess generalization <sep> - lack of discussion about distributed/asynchronous SGD <sep> ## Significance <sep> Many recent previous efforts have looked at the importance of batch sizes during training, so topic is relevant to the community. Smith and Le (2017) <sep> present a differential equation model for the scale of gradients in SGD, <sep> finding a linear scaling rule proportional to eps N/B, where eps = learning rate, N = training set size, and B = batch size. Goyal et al (2017) show how to train deep models on ImageNet effectively with large (but fixed) batch sizes by using a linear scaling rule. <sep> A few recent works have directly tested increasing batch sizes during training. De et al (AISTATS 2017) have a method for gradually increasing batch sizes, as do Friedlander and Schmidt (2012). Thus, it is already reasonable to practitioners that the proposed linear scaling of batch sizes during training would be effective. <sep> While increasing batch size at the proposed linear scale is simple and seems to be effective, a careful reader will be curious how much more could be gained from the backtracking line search method proposed in De et al. <sep> ## Quality <sep> Overall, only single training runs from a random initialization are used. It would be better to take the best of many runs or to somehow show error bars, <sep> to avoid the reader wondering whether gains are due to changes in algorithm or to poor exploration due to bad initialization. This happens a lot in Sec. 5.2. <sep> Some of the experimental setting seem a bit haphazard and not very systematic. <sep> In Sec. 5.2, only two learning rate scales are tested (0.1 and 0.5). Why not examine a more thorough range of values? <sep> Why not report actual wallclock times? Of course having reduced number of parameter updates is useful, but it's difficult to tell how big of a win this could be. <sep> What about distributed SGD or asyncronous SGD (hogwild)? Small batch sizes sometimes make it easier for many machines to be working simultaneously. If we scale up to batch sizes of ~ N/10, we can only get 10x speedups in parallelization (in terms of number of parameter updates). I think there is some subtle but important discussion needed on how this framework fits into modern distributed systems for SGD. <sep> ## Clarity <sep> Overall the paper reads reasonably well. <sep> Offering a related work ""feature matrix"" that helps readers keep track of how previous efforts scale learning rates or minibatch sizes for specific experiments could be valueable. Right now, lots of this information is just provided in text, so it's not easy to make head-to-head comparisons. <sep> Several figure captions should be updated to clarify which model and dataset are studied. For example, when skimming Fig. 3's caption there is no such information. <sep> ## Paper Summary <sep> The paper examines the influence of batch size on the behavior of stochastic gradient descent to minimize cost functions. The central thesis is that instead of the ""conventional wisdom"" to fix the batch size during training and decay the learning rate, it is equally effective (in terms of training/test error reached) to gradually increase batch size during training while fixing the learning rate. These two strategies are thus ""equivalent"". Furthermore, <sep> using larger batches means fewer parameter updates per epoch, so training is potentially much faster. <sep> Section 2 motivates the suggested linear scaling using previous SGD analysis from Smith and Le (2017). Section 3 makes connections to previous work on finding optimal batch sizes to close the generaization gap. Section 4 extends analysis to include SGD methods with momentum. <sep> In Section 5.1, experiments training a 16-4 ResNet on CIFAR-10 compare three possible SGD schedules: * increasing batch size * decaying learning rate * <sep> hybrid (increasing batch size and decaying learning rate) Fig. 2, 3 and 4 show that across a range of SGD variants (+/- momentum, etc) these three schedules have similar error vs. epoch curves. This is the core claimed contribution: <sep> empirical evidence that these strategies are ""equivalent"". <sep> In Section 5.3, experiments look at Inception-ResNet-V2 on ImageNet, showing the proposed approach can reach comparable accuracies to previous work at even fewer parameter updates (2500 here, vs. ∼14000 for Goyal et al 2007)","Pros: <sep> + Nice demonstration of the equivalence between scaling the learning rate and increasing the batch size in SGD optimization. <sep> Cons: <sep> - While reporting convergence as a function of number of parameter updates is consistent, the paper would be more compelling if wall-clock times were given in some cases, as that will help to illustrate the utility of the approach. <sep> - The paper would be stronger if additional experimental results, which the authors appear to have at hand (based on their comments in the discussion) were included as supplemental material. <sep> - The results are not all that surprising in light of other recent papers on the subject."
"abstract | strength  ==>  ==> This is a well-written paper that proposes regularization and optimization strategies for word-based language modeling tasks.   The authors propose the use of DropConnect  on the hidden-hidden connections as a regularization method, in order to take advantage of high-speed LSTM implementations via the cuDNN LSTM libraries from NVIDIA.  The  focus of this work is on the prevention of overfitting on the recurrent connections of the LSTM.  The authors explore a variant of Average-SGD (NT-ASGD) as an optimization strategy which eliminates the need for tuning the average trigger and uses a constant learning rate.  Averaging is triggered when the validation loss worsens or stagnates for a few cycles, leading to two new hyper parameters: logging interval and non-monotone interval.  Other forms of well-know regularization methods were applied to the non-recurrent connections, input, output and embedding matrices. <sep> As the authors point out, all the methods used in this paper have been proposed before and theoretical convergence explained. The novelty of this work lies in its successful application to the language modeling task achieving state-of-the-art results. <sep> On the PTB task, the proposed AWD-LSTM achieves a perplexity of 57.3 vs 58.3 (Melis et al 2017) and almost the same perplexity as Melis et el. on the Wiki-Text2 task (65.8 vs 65.9).  The addition of a cache model provides significant gains on both tasks. <sep> It would be useful, if authors had explored the behavior of the  AWD-LSTM algorithm with respect to various hyper parameters  and provided a few insights towards their choices for other large vocabulary language modeling tasks (1 million vocabulary sizes). <sep> Similarly, the choice of the average trigger and number of cycles seem arbitrary -  it would have been good to see a graph over a range of values, showing their impact on the model's performance. <sep> A 3-layer LSTM has been used for the experiments  - how was this choice made?  What is the impact of this algorithm if the net was a 2-layer net as is typical in most large-scale LMs? <sep> Table 3 is interesting to see how the cache model helps with rare words  and as such has applications in key word spotting tasks. Were the hyper parameters of the cache tuned to perform better on rare words?  More details on the design of the cache model would have been useful. <sep> You state that the gains obtained using the cache model were far less than what was obtained in Graves et al 2016 - what do you attribute this to? <sep> Ablation analysis in Table 4 is very useful - in particular it shows how lack of regularization of the recurrent connections can lead to maximum degradation in performance. <sep> Most of the results in this paper have been based on one choice of various model parameters. Given the emperical nature of this work, it would have made the paper even clearer if an analysis of their choices were presented.  Overall, it would be beneficial to the MLP community to see this paper accepted in the conference.",This paper presents a simple yet effective method for weight dropping for an LSTM that requires no modification of an RNN cell's formulation. Experimental results shows good perplexity results on benchmarks compared to many baselines. All reviewers agree that the paper will bring good contribution to the conference.
"misc | strength | decision  ==> The paper presents a method for learning a curriculum for reinforcement learning tasks.The approach revolves around splitting the personality of the agent into two parts. The first personality learns to generate goals for other personality for which the second agent is just barely capable--much in the same way a teacher always pushes just past the frontier of a student's ability. The second personality attempts to achieve the objectives set by the first as well as achieve the original RL task. <sep> The novelty of the proposed method is introduction of a teacher that learns to generate a curriculum for the agent.The formulation is simple and elegant as the teacher is incentivised to widen the gap between bob but pays a price for the time it takes which balances the adversarial behavior. <sep> Prior and concurrent work on learning curriculum and intrinsic motivation in RL rely on GANs (e.g., automatic goal generation by Held et al.), adversarial agents (e.g., RARL by Pinto et al.), or algorithmic/heuristic methods (e.g., reverse curriculum by Florensa et al. and HER Andrychowicz et al.).  In the context of this work, the contribution is the insight that an agent can be learned to explore the immediate reachable space but that is just within the capabilities of the agent. HER and goal generation share the core insight on training to reach goals. However, HER does generate goals beyond the reachable it instead relies on training on existing reached states or explicitly consider the capabilities of the agent on reaching a goal. Goal generation while learning to sample from the achievable frontier does not ensure the goal is reachable and may not be as stable to train. <sep> As noted by the authors the above mentioned prior work is closely related to the proposed approach. However, the paper only briefly mentions this corpus of work. A more thorough comparison with these techniques should be provided even if somewhat concurrent with the proposed method. The authors should consider additional experiments on the same domains of this prior work to contrast performance. <sep> Questions: <sep> Do the plots track the combined iterations that both Alice and Bob are in control of the environment or just for Bob?","I fully agree with strong positive statements in the reviews. All reviewers agree that the paper introduces a novel and elegant twist on standard RL, wherein one agent proposes a sequence of diverse tasks to a second agent so as to accelerate the second agent's learning models of the environment. I also concur that the empirical testing of this method is quite good. There are strong and/or promising results in five different domains (Hallway, LightKey, MountainCar, Swimmer Gather and TrainingMarines in StartCraft). This paper would make for a strong poster at *CONF*."
"abstract | rebuttal_process  ==> This paper identifies and proposes a fix for a shortcoming of the Deep Information Bottleneck approach, namely that the induced representation is not invariant to monotonic transform of the marginal distributions (as opposed to the mutual information on which it is based). The authors address this shortcoming by applying the DIB to a transformation of the data, obtained by a copula transform. This explicit approach is shown on synthetic experiments to preserve more information about the target, yield better reconstruction and converge faster than the baseline. The authors further develop a sparse extension to this Deep Copula Information Bottleneck (DCIB), which yields improved representations (in terms of disentangling and sparsity) on a UCI dataset. <sep> (significance) This is a promising idea. This paper builds on the information theoretic perspective of representation learning, and makes progress towards characterizing what makes for a good representation. Invariance to transforms of the marginal distributions is clearly a useful property, and the proposed method seems effective in this regard. <sep> Unfortunately, I do not believe the paper is ready for publication as it stands, as it suffers from lack of clarity and the experimentation is limited in scope. <sep> (clarity) While Section 3.3 clearly defines the explicit form of the algorithm (where data and labels are essentially pre-processed via a copula transform), details regarding the ""implicit form"" are very scarce. From Section 3.4, it seems as though the authors are optimizing the form of the gaussian information bottleneck I(x,t), in the hopes of recovering an encoder fβ(x) which gaussianizes the input (thus emulating the explicit transform) ? Could the authors clarify whether this interpretation is correct, or alternatively provide additional clarifying details ? There are also many missing details in the experimental section: how were the number of ""active"" components selected ? Which versions of the algorithm (explicit/implicit) were used for which experiments ? I believe explicit was used for Section 4.1, and implicit for 4.2 but again this needs to be spelled out more clearly. I would also like to see a discussion (and perhaps experimental comparison) to standard preprocessing techniques, such as PCA-whitening. <sep> (quality) The experiments are interesting and seem well executed. Unfortunately, I do not think their scope (single synthetic, plus a single UCI dataset) is sufficient. While the gap in performance is significant on the synthetic task, this gap appears to shrink significantly when moving to the UCI dataset. How does this method perform for more realistic data, even e.g. MNIST ? I think it is crucial to highlight that the deficiencies of DIB matter in practice, and are not simply a theoretical consideration. Similarly, the representation analyzed in Figure 7 is promising, but again the authors could have targeted other common datasets for disentangling, e.g. the simple sprites dataset used in the beta-VAE paper. I would have also liked to see a more direct and systemic validation of the claims made in the paper. For example, the shortcomings of DIB identified in Section 3.1, 3.2 could have been verified more directly by plotting I(y,t) for various monotonic transformations of x. A direct comparison of the explicit and implicit forms of the algorithms would also also make for a stronger paper in my opinion. <sep> Pros: <sep> * Theoretically well motivated <sep> * Promising results on synthetic task <sep> * Potential for impact <sep> Cons: <sep> * Paper suffers from lack of clarity (method and experimental section) <sep> * Lack of ablative / introspective experiments <sep> * Weak empirical results (small or toy datasets only).","Observing that in contrast to classical information bottleneck, the deep variational information bottleneck (DVIB) model is not invariant to monotonic transformations of input and output marginals, the authors show how to incorporate this invariance along with sparsity in DVIB using the copula transform. The revised version of the paper addressed some of the reviewer concerns about clarity as well as the strength of the experimental section, but the authors are encouraged to improve these aspects of the paper further."
"strength | decision  ==> SUMMARY: <sep> The authors reinvent a 20 years old technique for adapting a global or component-wise learning rate for gradient descent. The technique can be derived as a gradient step for the learning rate hyperparameter, or it can be understood as a simple and efficient adaptation technique. <sep> GENERAL IMPRESSION: <sep> One central problem of the paper is missing novelty. The authors are well aware of this. They still manage to provide added value. <sep> Despite its limited novelty, this is a very interesting and potentially impactful paper. I like in particular the detailed discussion of related work, which includes some frequently overlooked precursors of modern methods. <sep> CRITICISM: <sep> The experimental evaluation is rather solid, but not perfect. It considers three different problems: logistic regression (a convex problem), and dense as well as convolutional networks. That's a solid spectrum. However, it is not clear why the method is tested only on a single data set: MNIST. Since it is entirely general, I would rather expect a test on a dozen different data sets. That would also tell us more about a possible sensitivity w.r.t. the hyperparameters \\alpha_0 and \\beta. <sep> The extensions in section 5 don't seem to be very useful. In particular, I cannot get rid of the impression that section 5.1 exists for the sole purpose of introducing a convergence theorem. Analyzing the actual adaptive algorithm would be very interesting. In contrast, the present result is trivial and of no interest at all, since it requires knowing a good parameter setting, which defeats a large part of the value of the method. <sep> MINOR POINTS: <sep> page 4, bottom: use \\citep for Duchi et al. (2011). <sep> None of the figures is legible on a grayscale printout of the paper. Please do not use color as the only cue to identify a curve. <sep> In figure 2, top row, please display the learning rate on a log scale. <sep> page 8, line 7 in section 4.3: ""the the"" (unintended repetition) <sep> End of section 4: an increase from 0.001 to 0.001002 is hardly worth reporting - or am I missing something?","All reviewers agreed that, despite the lack of novelty, the proposed method is sound and correctly linked to existing work. As the topic of automatically learning the stepsize is of great practical interest, I am glad to have this paper presented as a poster at *CONF*."
"abstract | strength | ac_disagreement  ==> Summary: <sep> The manuscript introduces a principled way of network to network compression, which uses policy gradients for optimizing two policies which compress a strong teacher into a strong but smaller student model. The first policy, specialized on architecture selection, iteratively removes layers, starting with architecture of the teacher model. After the first policy is finished, the second policy reduces the size of each layer by iteratively outputting shrinkage ratios for hyperparameters such as kernel size or padding. This organization of the action space, together with a smart reward design achieves impressive compression results, given that this approach automates tedious architecture selection. The reward design favors low compression/high accuracy over high compression/low performance while the reward still monotonically increases with both compression and accuracy. As a bonus, the authors also demonstrate how to include hard constraints such as parameter count limitations into the reward model and show that policies trained on small teachers generalize to larger teacher models. <sep> Review: <sep> The manuscript describes the proposed algorithm in great detail and the description is easy to follow. The experimental analysis of the approach is very convincing and confirms the author's claims. <sep> Using the teacher network as starting point for the architecture search is a good choice, as initialization strategies are a critical component in knowledge distillation. I am looking forward to seeing work on the research goals outlined in the Future Directions section. <sep> A few questions/comments: <sep> 1) I understand that L_{1,2} in Algorithm 1 correspond to the number of layers in the network, but what do N_{1,2} correspond to? Are these multiple rollouts of the policies? If so, shouldn't the parameter update theta_{{shrink,remove},i} be outside the loop over N and apply the average over rollouts according to Equation (2)? I think I might have missed something here. <sep> 2) Minor: some of the citations are a bit awkward, e.g. on page 7: ""algorithm from Williams Williams (1992). I would use the \\citet command from natbib for such citations and \\citep for parenthesized citations, e.g. ""... incorporate dark knowledge (Hinton et al., 2015)"" or ""The MNIST (LeCun et al., 1998) dataset..."" <sep> 3) In Section 4.6 (the transfer learning experiment), it would be interesting to compare the performance measures for different numbers of policy update iterations. <sep> 4) Appendix: Section 8 states ""Below are the results"", but the figure landed on the next page. I would either try to force the figures to be output at that position (not in or after Section 9) or write ""Figures X-Y show the results"". Also in Section 11, Figure 13 should be referenced with the \\ref command <sep> 5) Just to get a rough idea of training time: Could you share how long some of the experiments took with the setup you described (using 4 TitanX GPUs)? <sep> 6) Did you use data augmentation for both teacher and student models in the CIFAR10/100 and Caltech256 experiments? <sep> 7) What is the threshold you used to decide if the size of the FC layer input yields a degenerate solution? <sep> Overall, this manuscript is a submission of exceptional quality and if minor details of the experimental setup are added to the manuscript, I would consider giving it the full score.","This is a meta-learning approach to model compression which trains 2 policies using RL to reduce the capacity (computational cost) of a trained network while maintaining performance, such that it can be effectively transferred to a smaller student network. The approach has similarities to recently proposed methods for architecture search, but is significantly different. The paper is well written and the experiments are clear and convincing. One of the reviews was unacceptable; I am not considering it (R1)."
"abstract | strength | weakness | rebuttal_process | weakness | strength  ==>  ==> The authors propose to compare three different memory architecture for recurrent neural network language models: <sep> vanilla LSTM, random access based on attention and continuous stack. The second main contribution of the paper is to propose an extension of continuous stacks, which allows to perform multiple pop operations at a single time step. <sep> The way to do that is to use a similar mechanism as the adaptive computation time from Graves (2016): all the pop operations are performed, and the final state of the continuous stack is weighted average of all the intermediate states. The different memory models are evaluated on two standard language modeling tasks: PTB and WikiText-2, as well as on the verb number prediction dataset from Linzen et al (2016). On the language modeling tasks, the stack model performs slightly better than the attention models (0-2 ppl points) which performs slightly better than the plain LSTM (2-3 ppl). On the verb number prediction tasks, the stack model tends to outperforms the two other models (which get similar results) for hard examples (2 or more attractors). <sep> Overall, I enjoy reading this paper: it is clearly written, and contains interesting analysis of different memory architecture for recurrent neural networks. As far as I know, it is the first thorough comparison of the different memory architecture for recurrent neural network applied to language modeling. The experiments on the Linzen et al. (2016) dataset is also interesting, as it shows that for hard examples, the different models do have different behavior (even when the difference are not noticeable on the whole test set). <sep> One small negative aspect of the paper is that the substance might be a bit limited. The only technical contribution is to merge the ideas from the continuous stack with the adaptive computation time to obtain the ""multi-pop"" model. In the experimental section, which I believe is the main contribution of the paper, I would have liked to see more ""in-depth"" analysis of the different models. I found the experiments performed on the Linzen et al. (2016) dataset (Table 2) to be quite interesting, and would have liked more analysis like that. On the other hand, I found Figures 2 or 3 not very informative, as it is (would like to see more). For example, from Fig. 2, it would be interesting to get a better understanding of what errors are made by the different models (instead of just the distribution). <sep> Finally, I have a few questions for the authors: <sep> - In Figure 1. shouldn't there be an arrow from h_{t-1} to m_t instead of x_{t-1} to m_t? <sep> - What are the equations to update the stack? I assume something similar to Joulin & Mikolov (2015)? <sep> - Do you have any ideas why there is a sharp jump between 4 and 5 attractors (Table 2)? <sep> - Why no ""pop"" operations in Figure 3 and 4? <sep> pros/cons: <sep> + clear and easy to read <sep> + interesting analysis <sep> - not very original <sep> Overall, while not groundbreaking, this is a serious paper with interesting analysis. Hence, I am weakly recommending to accept this paper.","This paper provides a comparison of different types of a memory augmented models and extends some of them to beyond their simple form. Reviewers found the paper to be clearly written, saying it ""nice introduction to the topic"" and noting that they ""enjoyed reading this paper"". In general though there was a feeling that the ""substance of the work is limited"". One reviewer complained that experiments were limited to small English datasets PTB and Wikitext-2 and asked why they didn't try ""machine translation or speech recognition"". (The author's note that they did try the Linzen dataset, and while the reviewers found the experiments impressive, the task itself felt artificial) . Another felt that the ""multipop model"" alone was not too large a contribution. The actual experiments in the work are well done, although given the fact that the models are known there was expectation of ""more ""in-depth"" analysis of the different models"". Overall this is a good empirical study, which shows the limited gains achieved by these models, a nevertheless useful piece of information for those working in this area."
"strength | weakness | rebuttal_process | misc | weakness | rating_summary | decision  ==>  ==> This paper tackles the overfitting problem when training neural networks based on regularization technique. More precisely, the authors propose new regularization terms that are related to the underlying virtual geometrical transformations (shift, rotation and scale) of the input data (signal, image and video). By formalizing the geometrical transformation process of a given image, the authors deduce constraints on the objective function which depend on the magnitude of the applied transformation. The proposed method is compared to three methods: one baseline and two methods of the literature (AT and VAT). The comparison is done on three datasets (synthetic data, MNIST and CIFAR10) in terms of test errors (for classification problems) and running time. <sep> The paper is well formalized and the idea is interesting. The regularization approach is novel compared to the methods of the literature. <sep> Main concerns: <sep> 1) The experimental validation of the proposed approach is not consistent: <sep> The description of the baseline method is not detailed in the paper. <sep> A priori, the baseline should naturally be the method without your regularization terms. <sep> But, this seems to be contrary with what you displayed in Figure 3. <sep> Indeed, in Figure 3, there is three different graphs for the baseline method (i.e., one for each regularization term). It seems that the baseline method depends on the different kinds of regularization term, why? Same question for AT and VAT methods. <sep> In practice, what is the magnitude of the perturbations? <sep> Please, explain the axis of all the figures. <sep> Please, explain how do you mix your different regularization terms in your method that you call VMT-all? <sep> All the following points are related to the experiment for which you presented the results in Table 2: <sep> Please, provide the results of all your methods on the synthetic dataset (only VMT-shift is provided). What is VMF? Do you mean VMT? <sep> For the evaluations, it would be more rigorous to re-implement also the state-of-the-art methods for which you only give the results that they report in their paper. Especially, because you re-implemented AT with L-2 constraint, so, it seems straightforward to re-implement also AT with L-infinite constraint. Same remark for the dropout regularization technique, which is easy to re-implement on the dense layers of your neural networks, within the Tensorflow framework. <sep> As you mentioned, your main contribution is related to running time, thus, you should give the running time in all experiments. <sep> 2) The method seems to be a tradeoff between accuracy and running time: <sep> The VAT method performs better than all your methods in all the datasets. <sep> The baseline method is faster than all the methods (Table 3). <sep> This being said, the proposed method should be clearly presented in the paper as a tradeoff between accuracy and running time. <sep> 3) The positioning of the proposed approach is not so clear: <sep> As mentioned above, your method is a tradeoff between accuracy and running time. But you also mentioned (top of page 2) that the contribution of your paper is also related to the interpretability in terms of ''Human perception''. Indeed, you clearly mentioned that the methods of the literature lacks interpretability. You also mentioned that your method is more ''geometrically'' interpretable than methods of the literature. The link between interpretability in terms of ""human perception"" and ""geometry"" is not obvious. Anyway, the interpretability point is not sufficiently demonstrated, or at least, discussed in the paper. <sep> 4) Many typos in the paper : <sep> Section 1: ""farward-backward"" <sep> Section 2.1: ""we define the movement field V of as a n+1…"" <sep> Section 2.2: ""lable"" - ""the another"" - ""of how it are generated"" – Sentence ""Since V is normalized."" seems incomplete… - \\mathcal{L} not defined - Please, precise the simplifications like \\mathcal{L}_{\\theta} to \\mathcal{L} <sep> Section 3: ""DISCUSSTION"" <sep> Section 4.1: ""negtive"" <sep> Figure 2: ""negetive"" <sep> Table 2: ""VMF"" <sep> Section 4.2: ""Tab 2.3"" does not exist <sep> Section 4.3: ""consists 9 convolutional"" – ""nerual networks""… <sep> Please, always use the \\eqref latex command to refer to equations. <sep> There is many others typos in the paper, so, please proofread the paper…","R1 thought the proposed method was novel and the idea interesting. However, he/she raised concerns with consistency in the experimental validation, the trade-off between accuracy and running time, and the positioning/motivation, specifically the claim about interpretability. The authors responded to these concerns, and R1 upgraded their score. R2 didn't raise major concerns or strengths. R3 questioned the novelty of the work and the experimental validations. All reviewers raised concerns with the writing. Though I think the work is interesting, issues raised about experiments and writing make me hesitant to go against the overall recommendation of the reviewers, which is just below the bar. I think this is a paper that could make a good workshop contribution."
"abstract | rating_summary | decision  ==>  ==> The authors introduce a set of very simple tasks that are meant to illustrate the challenges of learning visual relations.  They then evaluate several existing network architectures on these tasks, and show that results are not as impressive as others might have assumed they would be.   They show that while recent approaches (e.g. relational networks) can generalize reasonably well on some tasks, these results do not generalize as well to held-out-object scenarios as might have been assumed. <sep> Clarity:  The paper is fairly clearly written.   I think I mostly followed it. <sep> Quality:  I'm intrigued by but a little uncomfortable with the generalization metrics that the authors use.   The authors estimate the performance of algorithms by how well they generalize to new image scenarios when trained on other image conditions.   The authors state that "". . . the effectiveness of an architecture to learn visual-relation problems should be measured in terms of generalization over multiple variants of the same problem, not over multiple splits of the same dataset.""  Taken literally, this would rule out a lot of modern machine learning, even obviously very good work. On the other hand, it's clear that at some point, generalization needs to occur in testing ability to understand relationships.  I'm a little worried that it's ""in the eye of the beholder"" whether a given generalization should be expected to work or not. <sep> There are essentially three scenarios of generalization discussed in the paper: <sep> (a) various generalizations of image parameters in the PSVRT dataset <sep> (b) various hold-outs of the image parameters in the sort-of-CLEVR dataset <sep> (c) from sort-of-CLEVR ""objects"" to PSVRT bit patterns <sep> The result that existing architectures didn't do very well at these generalizations (especially b and c) *may* be important -- or it may not.    Perhaps if CNN+RN were trained on a quite rich real-world training set with a variety of real-world three-D objects beyond those shown in sort-of-CLEVR, it would generalize to most other situations that might be encountered.    After all, when we humans generalize to understanding relationships, exactly what variability is present in our ""training sets"" as compared to our ""testing"" situations?   How do the authors know that humans are effectively generalizing rather than just ""interpolating"" within their (very rich) training set?  It's not totally clear to me that if totally naive humans (who had never seen spatial relationships before) were evaluated on exactly the training/testing scenarios described above, that they would generalize particularly well either.   I don't think it can just be assumed a priori that humans would be super good this form of generalization. <sep> So how should authors handle this criticism?  What would be useful would either be some form of positive control.  Either human training data showing very effective generalization (if one could somehow make ""novel"" relationships unfamiliar to humans), or a different network architecture that was obviously superior in generalization to CNN+RN. If such were present, I'd rate this paper significantly higher. <sep> Also, I can't tell if I really fully believe the results of this paper.  I don't doubt that the authors saw the results they report.  However, I think there's some chance that if the same tasks were in the hands of people who *wanted* CNNs or CNN+RN to work well, the results might have been different.   I can't point to exactly what would have to be different to make things ""work"", because it's really hard to do that ahead of actually trying to do the work.   However, this suspicion on my part is actually a reason I think it might be *good* for this paper to be published at *CONF*.  This will give the people working on (e.g.) CNN+RN somewhat more incentive to try out the current paper's benchmarks and either improve their architecture or show that the the existing one would have totally worked if only tried correctly.  I myself am very curious about what would happen and would love to see this exchange catalyzed. <sep> Originality and Significance:  The area of relation extraction seems to me to be very important and probably a bit less intensively worked on that it should be.  However, as the authors here note, there's been some recent work (e.g. Santoro 2017) in the area.   I think that the introduction of baselines  benchmark challenge datasets such as the ones the authors describe here is very useful, and is a somewhat novel contribution.","This paper studies an important problem (visual relationship detection and generalization capabilities existing networks for this task). Unfortunately, all reviewers raise concerns (e.g. limited relations studied) and are largely on the fence about this paper. While this paper does not propose solutions, it does present interesting ""negative results"" that should get some visibility in the workshop track."
"abstract | weakness | strength | suggestion  ==> * In the ""flat vs sharp"" dilemma, the experiments display that the dilemma, if any, is subtle. Table 1 does not necessarily contradict this view. It would be a good idea to put the test results directly on Fig. 4 as it does not ease reading currently (and postpone ResNet-56 in the appendix). <sep> How was Figure 5 computed ? It is said that *a* random direction was used from each minimiser to plot the loss, so how the 2D directions obtained ? <sep> * On the convexity vs non-convexity (Sec. 6), it is interesting to see how pushing the Id through the net changes the look of the loss for deep nets. The difference VGG - ResNets is also interesting, but it would have been interesting to see how this affects the current state of the art in understanding deep learning, something that was done for the ""flat vs sharp"" dilemma, but is lacking here. For example, does this observation that the local curvature of the loss around minima is different for ResNets and VGG allows to interpret the difference in their performances ? <sep> * On optimisation paths, the choice of PCA directions is wise compared to random projections, and results are nice as plotted. There is however a phenomenon I would have liked to be discussed, the fact that the leading eigenvector captures so much variability, which perhaps signals that optimisation happens in a very low dimensional subspace for the experiments carried, and could be useful for optimisation algorithms (you trade dimension d for a much smaller ""effective"" d', you only have to figure out a generating system for this subspace and carry out optimisation inside). Can this be related to the ""flat vs sharp"" dilemma ? I would suppose that flatness tends to increase the variability captured by leading eigenvectors ? <sep> Typoes: <sep> Legend of Figure 2: red lines are error -> red lines are accuracy <sep> Table 1: test accuracy -> test error <sep> Before 6.2: architecture effects -> architecture affects","This work proposes an improved visualisation techniques for ReLU networks that compensates for filter scale symmetries/invariances, thus allowing a more meaningful comparison of low-dimensional projected optimization landscapes between different network architectures. <sep> - the visualisation techniques are a small variation over previous works <sep> + extensive experiments provide nice visualisations and yield a clearer visual picture of some properties of the optimization landscape of various architectural variants <sep> A promising research direction, which could be further improved by providing more extensive and convincing support for the significance of its contribution in comparison to prior techniques, and to its empirically derived observations, findings and claims."
"abstract | weakness | decision  ==> Summary of paper - The paper introduces the Gradient Scale Coefficient and uses it to demonstrate issues with the current understanding of where and why exploding gradients occur. <sep> Review - The paper attempts to contribute to the discussion about the exploding gradient problem by both introducing a metric for discussing this issue and by showing that current understanding of the exploding gradient problem may be incorrect. It is admirable that the authors are seeking to add to the understanding about theory of neural nets instead of contributing a new architecture with better error rates but without understanding why said error rates are lower. While the authors list 7 contributions, the current version of the text is a challenge to read and makes it challenging to distill an overarching theme or narrative to these contributions. <sep> The authors do mention experiments on page 8, but confess that some of the results are somewhat underwhelming. Unfortunately, all tables with the experimental results are left to the appendix. As this is a mostly theoretical paper, pushing experimental results to the appendix does make sense, but the repeated references to these tables suggest that these experimental results are crucial for the authors' overall points. <sep> While the authors do attempt to accomplish a lot in these nearly 16 pages of text, the authors' main points and overall narrative gets lost due to the writing that is a bit jumbled at times and that relies heavily on the supplement. There are several places where it is not immediately clear why a certain block of text is included (i.e. the proof outlines on pages 8 and 10). At other points the authors default to an chronological narrative that can be useful at times (i.e. page 9), but here seems to distract from their overall narrative. <sep> This paper has a lot of content, but not all of it appears to be relevant to the authors' central points. Furthermore, the paper is nearly double the recommended page length and has a nearly 30 page supplement. My biggest recommendations for this paper are for the authors to 1) articulate one theme and then 2) look at each part (whether that be section, paragraph, or sentence) and ask what does that part contribute to that theme. <sep> Pros - <sep> * This paper attempts to add the understanding of neural nets instead of only contributing better error rates on benchmark datasets. <sep> * At several points, the authors seek to make the work accessible by offering lay explanations for their more technical points. <sep> * The practical suggestions on page 16 are a true highlight and could provide an outline for possible revisions. <sep> Cons - <sep> * The main narrative is lost in the text, leaving a reader unsure of the authors main points and contributions as they read. For example, the authors' first contribution is hidden among the text presentation of section 2. <sep> * The paper relies heavily on the supplement to make their central points. <sep> * It is nearly double the recommended page length with a nearly 30 page supplement <sep> Minor issues - <sep> * Use one style for introducing and defining terms either use italics or single quotes. The latter is not recommended since the authors use double quotes in the abstract to express that the exploding gradient problem is not solved. <sep> * The citation style of Authors (YEAR) at times leads to awkward sentence parsing. <sep> * Given that many figures have several subfigures, the authors should consider using a package that will denote subfigures with letters. <sep> * The block quotes in the introduction may be quite important for points later in the paper, but summarizing the points of these quotes may be a better use of space. The authors more successfully did this in paragraph 2 of the introduction. <sep> * All long descriptions of the appendix should be carefully revisited and possibly removed due to page length considerations. <sep> * In the text, figure 4 (which is in the supplement) is referenced before figure 3 (which is in the text). <sep> =-=-=-= Response to the authors <sep> During the initial reviewing period, I was unable to distill the significance of the authors' contributions from the current literature in large part due to the nature of the writing style. After reading the authors responses and consulting the differences between the versions of the paper, my review remains the same. It should be noted that all three reviewers pointed out the length of the paper as a weakness of the paper, and that in the most recent draft, the authors made the main text of the paper longer. <sep> Consulting the differences between the paper revisions, I was initially intrigued with the volume of differences that shown in the summary bar. Upon closer inspection, I read a much stronger introduction and appreciated the summaries at the ends of sections 4.4 and 6. However, I did notice that the majority of these changes were superficial re-orderings of the original text. Given the limited substantive changes to the main text, I did not deeply re-read the text of the paper beyond the introduction.","The paper sets out to analyze the problem of exploding gradients in deep nets which is of fundamental importance. Reviewers largely acknowledge the novelty of the main ideas in the paper towards this goal, however it is also strongly felt that the writing/presentation of the paper needs significant improvement to make it into a coherent and clean story before it can be published. There are also some concerns on networks used in the experiments not being close to practice. <sep> I recommend invitation to the workshop track as it has novel ideas and will likely generate interesting discussion."
"abstract | weakness | decision  ==>  ==> This paper proposes a conditional Generative Adversarial Networks that is used for data augmentation. In order to evaluate the performance of the proposed model, they use Omniglot, EMNIST, and VGG-Faces datasets and uses in the meta-learning task and standard classification task in the low-data regime. The paper is well-written and consistent. <sep> Even though this paper learns to do data-augmentation (which is very interesting ) rather than just simply applies some standard data augmentation techniques and shows improvements in some tasks, I am not convinced about novelty and originality of this paper, especially on the model side. To be more specific, the paper uses the previously proposed conditional GAN as the main component of their model. And for the one-shot learning tasks, it only trains the previously proposed models with these newly augmented data. <sep> In addition, there are some other works that used GAN as a method for some version of data augmentation: <sep> - RenderGAN: Generating Realistic Labeled Data https://arxiv.org/abs/1611.01331 <sep> -Data Augmentation in Emotion Classification Using Generative Adversarial Networks https://arxiv.org/abs/1711.00648 <sep> It is fair to say that their model shows improvement on the above tasks but this improvement comes with a cost of training of GAN network. <sep> In summary, the idea of the paper is very interesting to learn data-augmentation but yet I am not convinced the current paper has enough novelty and contribution and see the contribution of paper as on more the application side rather than on model and problem side. That said I'd be happy to hear the argument of the author about my comments.","The paper based on cGAN developed a data augmentation GAN to deal with unseen classes of data. The paper developed new modifications to each component and designed network structure using ideas from state-of-the-art nets. As pointed out by reviewer 1 & 2, the technical contribution is not sufficient. We hence recommend it to workshop publication."
"weakness | decision  ==> The main contribution of this work is just a combination of LSH schemes and SGD updates. Since hashing schemes essentially reduce the dimension, LSH brings computational benefits to the SGD operation. The targeted issue is fundamentally important, and the proposed approach (exploiting LSH schemes) seems to be sound.  Specifically, LSH schemes fit into the SGD schemes since they hash two vectors to the same bucket with probability in proportional to their distance (here, inner product or Cosine similarity). <sep> Strengths:  a sound approach; a simple and straightforward idea that is shown to work well in evaluations. <sep> Weaknesses: <sep> 1. The phrase of ""computational chicken-and-egg loop"" in the title and also in the main body is misleading and not accurate. The so-called ""chicken-and-egg"" issue concerns the causality dilemma: two causally related things, which comes the first. In the paper, the authors concerned ""more accurate gradients"" and ""faster convergence""; their causality is very clear (the first leads to the second), and there is no causality dilemma. Even from a computational perspective, ""SDG schemes aim for computational efficiency"" and ""stochastic makes the convergence slow down"" are not a causality dilemma.  The reason behind is that the latter is the cost of the first one, just the old saying that ""there is no such thing as a free lunch"". Therefore, this disordered logic makes the title very misleading, and all the corresponding descriptions in the main body are obscured by ""twisted"" and unnatural logics. <sep> 2. The depth is so limited. Besides a good observation that LSH fits well into SDG, there are no more in-depth results provided. The theorems (Theorems 1~3) are trivial, with loose relations with LSH. <sep> 3. The LSH schemes are not correctly referred to. Since the similarity metric is inner-product, the authors are expected to refer to Cosine similarity and inner-product based LSHs, which were published recently in NIPS. It is not in depth to assume ""any known LSH scheme"" in Alg. 2. Accordingly again, Theorems 1~3 are unrelated with this specific kind of similarity metric (Cosine similarity). <sep> 4. As the authors tried hard to stick to the unnecessary (a bit bragging) phrase ""computational chicken-and-egg loop"", the organization and presentation of the whole manuscript are poor. <sep> 5. Occasionally, there are typos, and it is not good to use words in formulas. Please proof-read carefully.","The reviewers think that the theoretical contribution is not significant on its own. The reviewers find the empirical aspect of the paper interesting, but more analysis of the empirical behavior is required, especially for large datasets. Even for small datasets with input augmentation (e.g. random crops in CIFAR-10) the pre-processing can become prohibitive. I recommend improving the manuscript for a re-submission to another venue and an *CONF* workshop presentation."
"misc | weakness | decision  ==> This paper presents an analysis of convolutional neural networks from the perspective of how the rank of the features is affected by the kinds of layers found in the most popular networks. Their analysis leads to the formulation of a certain theorem about the global minima with respect to parameters in the latter portion of the network. <sep> The authors ask important questions, but I am not sure that they obtain important answers. On the plus side, I'm glad that people are trying to further our understanding our neural networks, and I think that their investigation is worthy of being published. <sep> They present a collection of assumptions, lemmas, and theorems. They have no choice but to have assumptions, because they want to abstract away the ""data"" part of the analysis while still being able to use certain properties about the rank of the features at certain layers. <sep> Most of my doubts about this paper come from the feeling that equivalent results could be obtained with a more elegant argument about perturbation theory, instead of something like the proof of Lemma A1. That being said, it's easy to voice such concerns, and I'm willing to believe that there might not exist a simple way to derive the same results with an approach more along the line of ""whatever your data, pick whatever small epsilon, and you can always have the desired properties by perturbing your data by that small epsilon in a random direction"". Have the authors tried this ? <sep> I'm not sure if the authors were the first to present this approach of analyzing the effects of convolutions from a ""patch perspective"", but I think this is a clever approach. It simplifies the statement of some of their results. I also like the idea of factoring the argument along the concept of some critical ""wide layer"". <sep> Good review of the literature. <sep> I wished the paper was easier to read. Some of the concepts could have been illustrated to give the reader some way to visualize the intuitive notions. For example, maybe it would have been interesting to plot the rank of features a every layer for LeNet+MNIST ? <sep> At the end of the day, if a friend asked me to summarize the paper, I would tell them : <sep> ""Features are basically full rank. Then they use a square loss and end up with an over-parametrized system, so they can achieve loss zero (i.e. global minimum) with a multitude of parameters values."" <sep> Nitpicking : <sep> ""This paper is one of the first ones, which studies CNNs."" <sep> This sentence is strange to read, but I can understand what the authors mean. <sep> ""This is true even if the bottom layers (from input to the wide layer) and chosen randomly with probability one."" <sep> There's a certain meaning to ""with probability one"" when it comes to measure theory. The authors are using it correctly in the rest of the paper, but in this sentence I think they simply mean that something holds if ""all"" the bottom layers have random features.","Dear authors, <sep> While I appreciate the result that a convolutional layer can have full rank output, this allowing a dataset to be classified perfectly under mild conditions, the fact that all reviewers expressed concern about the statement is an indication that the presentation sill needs quite a bit of work. <sep> Thus, I recommend it as an *CONF* workshop paper."
"rebuttal_process | strength  ==> The paper includes the terms first-order proximity (""the concept that connected nodes in a graph should have similar properties"") and second-order proximity (""the concept that nodes with similar neighborhoods should have common characteristics""). These are called homophily in social network analysis. It is also known as assortativity in network science literature. The paper states on Page 4: ""A trade-off between first and second order proximity can be achieved by changing the parameter k, which simultaneously controls both the sizes of sentences generated and the size of the wind used in the SkipGram algorithm."" It is not readily clear why this statement should hold. Also the paper does not include a discussion on how the amount of homophily in the graph affects the results. There are various ways of measuring the level of homophily in a graph. There is simple local consistency, which is % of edges connecting nodes that have the same characteristics at each endpoint. Neville & Jensen's JMLR 2007 paper describes relational auto-correlation, which is Pearson contingency coefficient on the characteristics of endpoints of edges. Park & Barabasi's PNAS 2007 paper describes dyadicity and heterophilicity, which measures connections of nodes with the same characteristics compared to a random model and the connections of nodes with different characteristics compared to a random model. <sep> k (""which simultaneously controls both the sizes of sentences generated and the size of the wind used in the SkipGram algorithm"") is a free-parameter in the proposed algorithm. The paper needs an in-depth discussion of the role of k in the results. Currently, no discussion is provided on k except that it was set to 5 for the experiments.  From a network science perspective, it makes sense to have k vary per node. <sep> It is also not clear why d = 128 was chosen as the size of the embedding. <sep> From the description of the experimental setup for link prediction, it is not clear if a stratified sample of the entries of the adjacency matrix (i.e., both 0 and 1 entries) where selected. <sep> For the node classification experiments, information on class distribution and homophily levels would be helpful. <sep> In Section 5.1, the paper states: ""For highly connected graphs, larger numbers of permutations should be chosen (n in [10, 1000]) to better represent distributions, while for sparser graphs, smaller values can be used (n in [1, 10])."" How high is highly connected graphs? How spare is a sparser graph? In general, the paper lacks an in-depth analysis of when the approach works and when it does not.  I recommend running experiments on synthetic graphs (such as Barabasi-Albert, Watts-Strogatz, Forest Fire, Kronecker, and/or BTER graphs), systematically changing various characteristics of the graph, and reporting the results. <sep> The faster runtime is interesting but not surprising given the ego-centric nature of the approach.","The authors addressed the reviewers concerns but the scores remain somewhat low. <sep> The method is not super novel, but it is an incremental improvement over existing approaches."
"abstract | strength | decision  ==> Building rich 3D environments where to run simulations is a very interesting area of research. <sep> Strengths: <sep> 1. The authors propose a virtual environment of indoor scenes having a much larger scale compared to similar interactive environments and access to multiple visual modalities. They also show how the number of available scenes greatly impacts generalization in navigation based tasks. <sep> 2. The authors provide a thorough analysis on the contribution of different feature types (Mask, Depth, RGB) towards the success rate of the goal task. The improvements and generalization brought by the segmentation and depth masks give interesting insights towards building new navigation paradigms for real-world robotics. <sep> Weaknesses: <sep> 1. The authors claim that the proposed environment allows for multiple applications and interactions, however from the description in section 3, the capacities of the simulator beyond navigation are unclear. <sep> The dataset proposed, Home3D, adds a number of functionalities over the SUNCG dataset. The SUNCG dataset provides a large number of 3D scanned houses.  The most important contributions with respect to SUNCG are: <sep> - An efficient renderer: an important aspect. <sep> - Introducing physics: this is very interesting, unfortunately the contribution here is very small. Although I am sure the authors are planing to move beyond the current state of their implementation, the only physical constraint currently implemented is an occupancy rule and collision detection. This is not technically challenging. <sep> Therefore, the added novelty with respect to SUNCG is very limited. <sep> 2. The paper presents the proposed task as navigation from high level task description, but given that the instructions are fixed for a given target, there are only 5 possible instructions which are encoded as one-hot vectors. Given this setting, it is unclear the need for a gated attention mechanism. While this limited setting allows for a clear generalization analysis, it would have been good to study a setting with more complex instructions, allowing to evaluate instructions not seen during training. <sep> 3. While the authors make a good point showing generalization towards unseen scenes, it would have been good to also show generalization towards real scenarios, demonstrating the realistic nature of House3D and the advantages of using non-RGB features. <sep> 4. It would have been good to report an analysis on the number of steps performed by the agent before reaching its goal on the success cases. It seems to me that the continuous policy would be justified in this setting. <sep> Comments <sep> - It is unclear to me how the reward shaping addition helps generalize to unseen houses at test time, as suggested by the authors. <sep> - I miss a reference to (https://arxiv.org/pdf/1609.05143.pdf) beyond the AI-THOR environment, given that they also approach target driven navigation using an actor-critic approach. <sep> The paper proposes a new realistic indoor virtual environment, having a much larger number of scenes than similar environments. From the experiments shown, it seems that the scale increase, together with the availability of features such as Segmentation and Depth improve generalization in navigation tasks, which makes it a promising framework for future work on this direction. However, the task proposed seems too simple considering the power of this environment, and the models used to solve the task don't seem to bring  relevant novelties from previous approaches. (https://arxiv.org/pdf/1706.07230.pdf)","The authors present an environment for semantic navigation that is based on an existing dataset, SUNCG. Datasets/environments are important for deep RL research, and the contribution of this paper is welcome. However, this paper does not offer enough novelty in terms of approach/method and its claims are somewhat misleading, so it would probably be a better fit to publish it at a workshop."
"strength | weakness  ==>  ==> _________________________________________________________________________________________________________ <sep> I raise my rating on the condition that the authors will also address the minor concerns in the final version, please see details below. <sep> _________________________________________________________________________________________________________ <sep> This paper proposes to perform Adversarially Learned Inference (ALI) in a layer-wise manner. The idea is interesting, and the authors did a good job to describe high-level idea, and demonstrate one advantage of hierarchy: providing different levels reconstructions. However, the advantage of better reconstruction could be better demonstrated.  Some major concerns should be clarified before publishing: <sep> (1) How did the authors implement p(x|z) and q(z|x), or p(z_l | z_{l+1}) and q(z_{l+1} | z_l )? Please provide the details, as this is key to the reconstruction issues of ALI. <sep> (2) Could the authors provide the pseudocode procedure of the proposed algorithm? In the current form of the writing, it is not clear what the HALI procedure is, whether (1) one discriminator is used to distinguish the concatenation of (x, z_1, ..., z_L), or (2) L discriminators are used to distinguish the concatenation of (z_l, z_{l+1}) at each layer, respectively? <sep> The above two points are important. If not correctly constructed, it might reveal potential flaws of the proposed technique. <sep> Since one of the major claims for HALI is to provide better reconstruction with higher fidelity than ALI. Could the authors provide quantitative results on MNIST and CIFAR to demonstrate this? The reconstruction issues have first been highlighted and theoretically analyzed in ALICE [*], and some remedy has been proposed to alleviate the issue.  Quantitative comparison on MNIST and CIFAR are also conducted. Could the authors report numbers to compare with them (ALI and ALICE)? <sep> The 3rd paragraph in Introduction should be adjusted to correctly clarify details of algorithms, and reflect up-to-date literature. ""One interesting feature highlighted in the original ALI work (Dumoulin et al., 2016) is that ... never explicitly trained to perform reconstruction, this can nevertheless be easily done..."". Note that ALI can only perform reconstruction when the deterministic mapping is used, while ALI itself adopted the stochastic mapping. Further, the deterministic mapping is the major difference of BiGAN from ALI. Therefore, more rigorous way to phrase is that ""the original ALI work with deterministic mappings"", or ""BiGAN"" never explicitly trained to perform reconstruction, this can nevertheless be easily done... This tiny difference between deterministic/stochastic mappings makes major difference for the quality of reconstruction, as theoretically analyzed and experimentally compared in ALICE. In ALICE, the authors confirmed further source of poor reconstructions of ALI in practice. It would be better to reflect the non-identifiability issues raised by ALICE in Introduction, rather than hiding it in Future Work as ""Although recent work designed to improve the stability of training in ALI does show some promise (Chunyuan Li, 2017), more work is needed on this front."" <sep> Also, please fix the typo in reference as: <sep> [*] Chunyuan Li, Hao Liu, Changyou Chen, Yunchen Pu, Liqun Chen, Ricardo Henao and Lawrence Carin. ALICE: Towards understanding adversarial learning for joint distribution matching. In Advances in Neural Information Processing Systems (NIPS), 2017.",Pros: <sep> - The paper proposes to use a hierarchical structure to address reconstruction issues with ALI model. <sep> - Obtaining multiple latent representations that individually achieve a different level of reconstructions is interesting. <sep> - Paper is well written and the authors made a reasonable attempt to improve the paper during the rebuttal period. <sep> Cons: <sep> - Reviewers agree that the approach lacks novelty as similar hierarchical approaches have been proposed before. <sep> - The main goal of the paper to achieve better reconstruction in comparison to ALI without changing the latter's objective seems narrow. More analysis is needed to demonstrate that the approach out-performs other approaches that directly tackle this problem in ALI. <sep> - The paper does not provide strong arguments as to why hierarchy works (limited to 2 levels in the empirical analysis presented in the paper). <sep> - Semi-supervised learning as a down-stream task is impressive but limited to MNSIT.
"abstract | weakness  ==> Paper summary: <sep> Existing works on multi-task neural networks typically use hand-tuned weights for weighing losses across different tasks. This work proposes a dynamic weight update scheme that updates weights for different task losses during training time by making use of the loss ratios of different tasks. Experiments on two different network indicate that the proposed scheme is better than using hand-tuned weights for multi-task neural networks. <sep> Paper Strengths: <sep> - The proposed technique seems simple yet effective for multi-task learning. <sep> - Experiments on two different network architectures showcasing the generality of the proposed method. <sep> Major Weaknesses: <sep> - The main weakness of this work is the unclear exposition of the proposed technique. Entire technique is explained in a short section-3.1 with many important details missing. There is no clear basis for the main equations 1 and 2. How does equation-2 follow from equation-1? Where is the expectation coming from? What exactly does 'F' refer to? There is dependency of 'F' on only one of sides in equations 1 and 2? More importantly, how does the gradient normalization relate to loss weight update? It is very difficult to decipher these details from the short descriptions given in the paper. <sep> - Also, several details are missing in toy experiments. What is the task here? What are input and output distributions and what is the relation between input and output? Are they just random noises? If so, is the network learning to overfit to the data as there is no relationship between input and output? <sep> Minor Weaknesses: <sep> - There are no training time comparisons between the proposed technique and the standard fixed loss learning. <sep> - Authors claim that they operate directly on the gradients inside the network. But, as far as I understood, the authors only update loss weights in this paper. Did authors also experiment with gradient normalization in the intermediate CNN layers? <sep> - No comparison with state-of-the-art techniques on the experimented tasks and datasets. <sep> Clarifications: <sep> - See the above mentioned issues with the exposition of the technique. <sep> - In the experiments, why are the input images downsampled to 320x320? <sep> - What does it mean by 'unofficial dataset' (page-4). Any references here? <sep> - Why is 'task normalized' test-time loss as good measure for comparison between models in the toy example (Section 4)? The loss ratios depend on initial loss, which is not important for the final performance of the system. <sep> Suggestions: <sep> - I strongly suggest the authors to clearly explain the proposed technique to get this into a publishable state. <sep> - The term 'GradNorm' seem to be not defined anywhere in the paper. <sep> Review Summary: <sep> Despite promising results, the proposed technique is quite unclear from the paper. With its poor exposition of the technique, it is difficult to recommend this paper for publication.","This paper proposes a way to automatically weight different tasks in a multi-task setting. The problem is a bit niche, and the paper had a lot of problems with clarity, as well as the motivation for the experimental setup and evaluation."
"misc | strength | weakness | decision | suggestion  ==> This paper baffles me. It appears to be a stochastic RNN with skip connections (so it's conditioned on the last two states rather than last one) trained by an adversarial objective (which is no small feat to make work for sequential tasks) with results shown on the firetruck category of the QuickDraw dataset. Yet the authors claim significantly more importance for the work than I think it merits. <sep> First, there is nothing variational about their variational RNN. They seem to use the term to be equivalent to ""stochastic"", ""probabilistic"" or ""noisy"" rather than having anything to do with optimizing a variational bound. To strike the right balance between pretension and accuracy, I would suggest substituting the word ""stochastic""  everywhere ""variational"" is used. <sep> Second, there is nothing self-improving or collaborative about their self-improving collaborative GAN. Once the architecture is chosen to share the weights between the weak and strong generator, the only difference between the two is that the weak generator has greater noise at the output. In this sense the architecture should really be seen as a single model with different noise levels at alternating steps. In this sense, I am not entirely clear on what the difference is between the SIC-GAN and their noisy GAN baseline - presumably the only difference is that the noisy GAN is conditioned on a single timestep instead of two at a time? The claim that these models are somehow ""self-improving"" baffles me as well - all machine learning models are self-improving, that is the point of learning. The authors make a comparison to AlphaGo Zero's use of self-play, but here the weak and strong generators are on the same side of the game, and because there are no game rules provided beyond ""reproduce the training set"", there is no possibility of discovery beyond what is human-provided, contrary to the authors' claim. <sep> Third, the total absence of mathematical notation made it hard in places to follow exactly what the models were doing. While there are plenty of papers explaining the GAN framework to a novice, at least some clear description of the baseline architectures would be appreciated (for instance, a clearer explanation of how the SIC-GAN differs from the noisy GAN). Also the description of the soft ℓ1 loss (which the authors call the ""1-loss"" for some reason) would benefit from a clearer mathematical exposition. <sep> Fourth, the experiments seem too focused on the firetruck category of the QuickDraw dataset. As it was the only example shown, it's difficult to evaluate their claim that this is a general method for improving variety without sacrificing quality. Their chosen metrics for variety and detail are somewhat subjective, as they depend on the fact that some categories in the QuickDraw dataset resemble firetrucks in the fine detail while others resemble firetrucks in outline. This is not a generalizable metric. Human evaluation of the relative quality and variety would likely suffice. <sep> Lastly, the entire section on the strong-weak collaborative GAN seems to add nothing. They describe an entire training regiment for the model, yet never provide any actual experimental results using that model, so the entire section seems only to motivate the SIC-GAN which, again, seems like a fairly ordinary architectural extension to GANs with RNN generators. <sep> The results presented on QuickDraw do seem nice, and to the best of my knowledge it is the first (or at least best) applications of GANs to QuickDraw - if they refocused the paper on GAN architectures for sketching and provided more generalizable metrics of quality and variety it could be made into a good paper.","Pros and cons of the paper can be summarized as follows: <sep> Pros: <sep> * The underlying idea may be interesting <sep> * Results are reasonably strong on the test set used <sep> Cons: <sep> * Testing on the single dataset indicates that the model may be of limited applicability <sep> * As noted by reviewer 2, core parts of the paper are extremely difficult to understand, and the author response did little to assuage these concerns <sep> * There is little mathematical notation, which compounds the problems of clarity <sep> After reading the method section of the paper, I agree with reviewer 2: there are serious clarity issues here. As a result, I do cannot recommend that this paper be accepted to *CONF* in its current form. I would suggest the authors define their method precisely in mathematical notation in a future submission."
"misc | strength | weakness | decision  ==> This paper proposes a model for solving the WikiSQL dataset that was released recently. <sep> The main issues with the paper is that its contributions are not new. <sep> * The first claimed contribution is to use typing at decoding time (they don't say why but this helps search and learning). Restricting the type of the decoded tokens based on the programming language has already been done by the Neural Symbolic Machines of Liang et al. 2017. Then Krishnamurthy et al. expanded that in EMNLP 2017 and used typing in a grammar at decoding time. I don't really see why the authors say their approach is simpler, it is only simpler because the sub-language of sql used in wikisql makes doing this in an encoder-decoder framework very simple, but in general sql is not regular. Of course even for CFG this is possible using post-fix notation or fixed-arity pre-fix notation of the language as has been done by Guu et al. 2017 for the SCONE dataset, and more recently for CNLVR by Goldman et al., 2017. <sep> So at least 4 papers have done that in the last year on 4 different datasets, and it is now close to being common practice so I don't really see this as a contribution. <sep> * The authors explain that they use a novel loss function that is better than an RL based function used by Zhong et al., 2017. If I understand correctly they did not implement Zhong et al. only compared to their numbers which is a problem because it is hard to judge the role of optimization in the results. <sep> Moreover, it seems that the problem they are trying to address is standard - they would like to use cross-entropy loss when there are multiple tokens that could be gold. the standard solution to this is to just have uniform distribution over all gold tokens and minimize the cross-entropy between the predicted distribution and the gold distribution which is uniform over all tokens. The authors re-invent this and find it works better than randomly choosing a gold token or taking the max. But again, this is something that has been done already in the context of pointer networks and other work like See  et al. 2017 for summarization and Jia et al., 2016 for semantic parsing. <sep> * As for the good results - the data is new, so it is probable that numbers are not very fine-tuned yet so it is hard to say what is important and what not for final performance. In general I tend to agree that using RL for this task is probably unnecessary when you have the full program as supervision.","The pros and cons of the paper can be summarized below: <sep> Pro: <sep> * The improvements afforded by the method are significant over baselines, although these baselines are very preliminary baselines on a new dataset. <sep> Con <sep> * There is already a significant amount of work in using grammars to guide semantic parsing or code generation, as rightfully noted by the authors, and thus the approach in the paper is not extremely novel. <sep> * Because there is no empirical comparison with these methods, the relative utility of the proposed method is not clear. <sep> As a result, I recommend that the paper not be accepted at this time."
"abstract | weakness | rating_summary | strength | weakness  ==> The paper introduced recurrent relational network (RRNs), an enhanced version of the existing relational network, that can be added to any neural networks to add relational reasoning capacity. RRNs are illustrated on sudoku puzzles and textual QA. <sep> Overall the paper is well written and structured. It also addresses an important research question: combining relational reasoning and neural networks is currently receiving a lot of attention, in particular when generally considering the question of bridging sub-symbolic and symbolic methods. Unfortunately, it is current form, the paper has two major downsides. First of all,  the sudoku example does not illustrate ""complex relational reasoning"" as claimed in the title. The problem is encoded at a positional level where messages encoded as MLPs and LSTMs implement the constraints for sudoko. Indeed, <sep> this allows to realise end-to-end learning but does not illustrate complex reasoning. <sep> This is also reflected in the considered QA task, which is essentially coded as a positional problem. Consequently, the claim of the conclusions, namely that ""we have proposed a general relational reasoning model"" is not validated, unfortunately. Such a module that can be connected to any existing neural network would be great. However, <sep> for that one should show capabilities of relational logic. Some standard (noisy) <sep> reasoning capabilities such as modus ponens. This also leads me to the second downside. <sep> Unfortunately, the paper falls short on discussion related work. First of all, <sep> there is the large field of statistical relational learning, see <sep> Luc De Raedt, Kristian Kersting, Sriraam Natarajan, David Poole: <sep> Statistical Relational Artificial Intelligence: Logic, Probability, and Computation. Synthesis Lectures on Artificial Intelligence and Machine Learning, Morgan & Claypool Publishers 2016 <sep> for a recent overview. As it has the very same goals, while not using a neural architecture for implementation, it is very much related and has to be discussed. That one can also use a neural implementation can be seen in <sep> Ivan Donadello, Luciano Serafini, Artur S. d'Avila Garcez: <sep> Logic Tensor Networks for Semantic Image Interpretation. IJCAI 2017: 1596-1602 <sep> Matko Bosnjak, Tim Rocktäschel, Jason Naradowsky, Sebastian Riedel: <sep> Programming with a Differentiable Forth Interpreter. ICML 2017: 547-556 <sep> Luciano Serafini, Artur S. d'Avila Garcez: <sep> Learning and Reasoning with Logic Tensor Networks. AI*IA 2016: 334-348 <sep> Gustav Sourek, Vojtech Aschenbrenner, Filip Zelezný, Ondrej Kuzelka: <sep> Lifted Relational Neural Networks. CoCo@NIPS 2015 <sep> Tim Rocktäschel, Sebastian Riedel: <sep> End-to-end Differentiable Proving. CoRR abs/1705.11040 (2017) <sep> William W. Cohen, Fan Yang, Kathryn Mazaitis: <sep> TensorLog: Deep Learning Meets Probabilistic DBs. CoRR abs/1707.05390 (2017) <sep> to list just some approaches. There are also (deep) probabilistic programming approaches such as Edward that should be mentioned as CPS like problems (Sudoku) can definitely be implement there. Moreover, there is a number of papers that discuss embeddings of relational data and rules such as <sep> William Yang Wang, William W. Cohen: <sep> Learning First-Order Logic Embeddings via Matrix Factorization. IJCAI 2016: 2132-2138 <sep> Thomas Demeester, Tim Rocktäschel, Sebastian Riedel: <sep> Lifted Rule Injection for Relation Embeddings. EMNLP 2016: 1389-1399 <sep> and even neural-symbolic approaches with a long publication history. Unfortunately, <sep> non of these approaches has been cited, giving the wrong impression that this is the first paper that tackles the long lasting question of merging sub-symbolic and symbolic reasoning. BTW, there have been also other deep networks for optimisation, see e.g. <sep> Brandon Amos, J. Zico Kolter: <sep> OptNet: Differentiable Optimization as a Layer in Neural Networks. <sep> ICML 2017: 136-145 <sep> that have also considered Sudoku. To summarise, I like very much the direction of the paper but it seems to be too early to be published.","The proposed relational reasoning algorithm is basically a fairly standard graph neural network, with a few modifications (e.g., the prediction loss at each layer - also not a new idea per se). <sep> The claim that previously reasoning has not been considered in previous applications of graph neural networks (see discussion) is questionable. It is not even clear what is meant here by 'reasoning' as many applications of graph neural networks may be regarded as performing some kind of inference on graphs (e.g., matrix completion tasks by Berg, Kipf and Welling; statistical relational learning by Schlichtkrull et al). <sep> So the contribution seems a bit over-stated. Rather than introduces a new model, the work basically proposes an application of largely known model to two (not-so-hard) tasks which have not been studied in the context of GNNs. The claim that the approach is a general framework for dealing with complex reasoning problems is not well supported as both problems are (arguably) not complex reasoning problems (see R2). <sep> There is a general consensus between reviewers that the paper, in its current form, does not quite meet acceptance criteria. <sep> Pros: <sep> -- an interesting direction <sep> -- clarity <sep> Cons: <sep> -- the claim of generality is not well supported <sep> -- the approach is not so novel <sep> -- the approach should be better grounded in previous work"
"abstract | misc | strength | weakness  ==> The paper introduces a neural tree decoder architecture for binary trees that conditions the next node prediction on representations of its ascendants (encoded with an LSTM recurrent net) and left sibling subtree (encoded with a binary LSTM recursive net) for right sibling nodes. <sep> To perform tree to tree transduction the input tree is encoded as a vector with a Tree LSTM; correspondences between input and output subtrees are not modelled directly (using e.g. attention) as is done in traditional tree transducers. <sep> While the term context-sensitive should be used with caution, I do accept the claim here, although the notation used does not make the exposition clear. <sep> Experimental results show that the architecture performs better at synthetic tree transduction tasks (relabeling, reordering, deletion) than sequence-to-sequence baselines. <sep> While neural approches to tree-to-tree transduction is an understudied problem, the contributions of this paper are very narrow and it is not shown that the proposed approach will generalize to more expressive models or real-world applications of tree-to-tree transduction. <sep> Existing neural tree decoders, such as Dong and Lapata or Alvarex-Melis and Jaakkola, could be combined with tree LSTM encoders without any technical innovations and could possibly do as well as the proposed model for the transduction tasks tested - no experiments are performed with existing tree-based decoder architectures. <sep> Specific comments per section: <sep> 1. Unclear what is meant be ""equivalent"" in first paragraph. <sep> 2. The model does not assign an explicit probability to the tree structure - rather it seems to rely on the distinction between terminal and non-terimal symbols and the restriction to binary trees to know when closing brackets are implied - this is not made clear, and a general model should not have this restriction, as there are many cases where we want to generate non-binary trees. <sep> The production rule notation used is incorrect and confusing, mixing sets with non-terminals and terminal symbols: <sep> A better notation for the rules in 2.1.1 would be something like S -> P | v | \\epsilon; P -> Q R | Q u | u Q | u w, where P, Q, R \\in O and u, w \\in v. <sep> 2.1.2. Splitting production rules as ->_left, ->_right is not standard notation. Rather introduce intermediate non-terminals in the grammar: <sep> O -> O_L O_R; O_L -> a | Q, O_R -> b | Q. <sep> 2.1.3 The context-sensitively here arise when conditioning on the entire left sibling subtree (not just the top non-terimal). <sep> The rules should have a format such as O -> O_L O_R; O_L -> a | Q; \\alpha O_R -> \\alpha a | \\alpha Q, where \\alpha is an entire subtree rooted at O_L. <sep> 2.1.4 Should be g(x|.) = exp( ), the softmax function includes the normalization which is done in the equation below. <sep> 3. Note that is is possible to restrict the decoder to produce tree structures while keeping a sequential neural architecture. For some tasks sequential decoders do actually produce mostly well-formed trees, given enough training data. <sep> RNNG encodes completed subtrees recursively, and the stack LSTM encodes the entire partially-produced tree, so it does produce and condition on trees not just sequences. The model in this paper is not more expressive than RNNG, it just encodes somewhat different structural biases, which might or might not be suited for real tasks. <sep> 4. In the examples given, the same set of symbols are used as both terminals and non-terminals. How is the tree structure then predicted by the decoder? <sep> Details about the training setup are missing: How is the training data generated, what is the size of the trees during training (compared to testing)? <sep> 4.2 The steep drop in performance between depth 5 and 6 indicates that model is very sensitive to its memorization capacity and might not be generalizing over the given training data. <sep> For real tree-to-tree applications involving these operations, there is good reason to believe that some kind of attention mechanism will be needed over the input tree during decoding. <sep> Reference should generally be to published proceedings rather than to arxiv where available - e.g. Aharoni and Goldberg, Dong and Lapata, Erguchi et al, Rush et al. For Graehl and Knight there is a published journal paper in Computational Linguistics.",The proposed neural tree transduction framework is basically a combination of tree encoding and tree decoding. The tree encoding component is simply reused from previous work (TreeLSTM) whereas the decoding components is somewhat different from the previous work. They key problems (acknowledge also by at least 2 reviewers): <sep> Pros: <sep> -- generating trees input under-explored direction (note that it is more general than parsing as nodes may not directly correspond to input symbols) <sep> Cons: <sep> -- no comparison with previous tree-decoding work <sep> -- only artificial experiments <sep> -- the paper is hard too read (confusing) / mathematical notation and terminology is confusing and seems sometimes inaccurate (see R3)
"strength | weakness | suggestion  ==> The below review addresses the first revision of the paper. The revised version does address my concerns. The fact that the paper does not come with substantial theoretical contributions/justification still stands out. <sep> --- <sep> The authors present a variant of the adversarial feature learning (AFL) approach by Edwards & Storkey. AFL aims to find a data representation that allows to construct a predictive model for target variable Y, and at the same time prevents to build a predictor for sensitive variable S. The key idea is to solve a minimax problem where the log-likelihood of a model predicting Y is maximized, and the log-likelihood of an adversarial model predicting S is minimized. The authors suggest the use of multiple adversarial models, which can be interpreted as using an ensemble model instead of a single model. <sep> The way the log-likelihoods of the multiple adversarial models are aggregated does not yield a probability distribution as stated in Eq. 2. While there is no requirement to have a distribution here - a simple loss term is sufficient - the scale of this term differs compared to calibrated log-likelihoods coming from a single adversary. Hence, lambda in Eq. 3 may need to be chosen differently depending on the adversarial model. Without tuning lambda for each method, the empirical experiments seem unfair. This may also explain why, for example, the baseline method with one adversary effectively fails for Opp-L. A better comparison would be to plot the performance of the predictor of S against the performance of Y for varying lambdas. The area under this curve allows much better to compare the various methods. <sep> There are little theoretical contributions. Basically, instead of a single adversarial model - e.g., a single-layer NN or a multi-layer NN - the authors propose to train multiple adversarial models on different views of the data. An alternative interpretation is to use an ensemble learner where each learner is trained on a different (overlapping) feature set. Though, there is no theoretical justification why ensemble learning is expected to better trade-off model capacity and robustness against an adversary. Tuning the architecture of the single multi-layer NN adversary might be as good? <sep> In short, in the current experiments, the trade-off of the predictive performance and the effectiveness of obtaining anonymized representations effectively differs between the compared methods. This renders the comparison unfair. Given that there is also no theoretical argument why an ensemble approach is expected to perform better, I recommend to reject the paper.","The reviewers tend to agree that the empirical results in this paper are good compared to the baselines. However, the paper in its current form is considered a bit too incremental. Some reviewers also suggested additional theory could help strengthen the paper."
"weakness | suggestion  ==>  ==> This paper proposes an importance-weighted estimator of the MMD, in order to estimate the MMD between distributions based on samples biased according to a known scheme. It then discusses how to estimate the scheme when it is unknown, and further proposes using it in either the MMD-based generative models of Y. Li et al. (2015) / Dziugaite et al. (2015), or in the MMD GAN of C.-L. Li et al. (2017). <sep> The estimator itself is natural (and relatively obvious), though it has some drawbacks that aren't fully discussed (below). <sep> The application to GAN-type learning is reasonable, and topical. The first, univariate, experiment shows that the scheme is at least plausible. But the second experiment, involving a simple T ratio based on whether an MNIST digit is a 0 or a 1, doesn't even really work! (The best model only gets the underrepresented class from 20% up to less than 40%, rather than the desired 50%, and the ""more realistic"" setting only to 33%.) <sep> It would be helpful to debug whether this is due to the classifier being incorrect, estimator inaccuracies, or what. In particular, I would try using T based on a pretrained convnet independent of the autoencoder representation in the MMD GAN, to help diagnose where the failure mode comes from. <sep> Without at least a working should-be-easy example like this, and with the rest of the paper's technical contribution so small, I just don't think this paper is ready for *CONF*. <sep> It's also worth noting that the equivalent algorithm for either vanilla GANs or Wasserstein GANs would be equally obvious. <sep> Estimator: <sep> In the discussion about (2): where does the 1/m bias come from? This doesn't seem to be in Robert and Casella section 3.3.2, which is the part of the book that I assume you're referring to (incidentally, you should specify that rather than just citing a 600-page textbook). <sep> Moreover, it is worth noting that Robert and Cassela emphasize that if E[1 / \\tilde T] is infinite, the importance sampling estimator can be quite bad (for example, the estimator may have infinite variance). This happens when \\tilde T puts mass in a neighborhood around 0, i.e. when the thinned distribution doesn't have support at any place that P does. In the biased-observations case, this is in some sense unsurprising: if we don't see *any* data in a particular class of inputs, then our estimates can be quite bad (since we know nothing about a group of inputs that might strongly affect the results). In the modulating case, the equivalent situation is when F(x) lacks a mean, which seems less likely. Thus although this is probably not a huge problem for your case, it's worth at least mentioning. (See also the following relevant blog posts:  https://radfordneal.wordpress.com/2008/08/17/the-harmonic-mean-of-the-likelihood-worst-monte-carlo-method-ever/ and https://xianblog.wordpress.com/2012/03/12/is-vs-self-normalised-is/ .) <sep> The paper might be improved by stating (and proving) a theorem with expressions for the rate of convergence of the estimator, and how they depend on T. <sep> Minor: <sep> Another piece of somewhat-related work is Xiong and Schneider, Learning from Point Sets with Observational Bias, UAI 2014. <sep> Sutherland et al. 2016 and 2017, often referenced in the same block of citations, are the same paper. <sep> On page 3, above (1): ""Since we have projected the distributons into an infinite-dimensional space, the distance between the two distributions is zero if and only if all their moments are the same."" An infinite-dimensional space isn't enough; the kernel must further be characteristic, as you mention. See e.g. Sriperumbuder et al. (AISTATS 2010) for more details. <sep> Figure 1(b) seems to be plotting only the first term of \\tilde T, without the + 0.5.","The reviewers agree that the problem being addressed is interesting, however there are concerns with novelty and with the experimental results. An experiment beyond dealing with class imbalance would help strengthen this paper, as would experiments with other kinds of GANs."
"misc | strength | rating_summary | suggestion  ==> - Good work on developing VAEs for few-shot learning. <sep> - Most of the results are qualitative and I reckon the paper was written in haste. <sep> - The rest of the comments are below: <sep> - 3.1: I got a bit confused over what X actually is: <sep> -- ""We would like to learn a generative model for **sets X** of the form"". <sep> --""... to refer to the **class X_i** ..."". <sep> -- ""we can lower bound the log-likelihood of each **dataset X** ..."" <sep> - 3.2: ""In general, if we wish to learn a model for X in which each latent variable ci affects some arbitrary subset Xi of the data (**where the Xi may overlap**), ..."": Which is just like learning a Z for a labeled X but learning it in an unsupervised manner, i.e. the normal VAE, isn't it? If not, could you please elaborate on what is different (in the case of 3.2 only, I mean)? i.e. Could you please elaborate on what's different (in terms of learning) between 3.2 and a normal latent Z that is definitely allowed to affect different classes of the data without knowing the classes? <sep> - Figure 1 is helpful to clarify the main idea of a VHE. <sep> - ""In a VHE, this recognition network takes only small subsets of a class as input, which additionally ..."": And that also clearly leads to loss of information that could have been used in learning. So there is a possibility for potential regularization but there is definitely a big loss in estimation power. This is obviously possible with any regularization technique, but I think it is more of an issue here since parts of the data are not even used in learning. <sep> - ""Table 4.1 compares these log likelihoods, with VHE achieving state-of-the-art. To"": Where is Table 4.1?? <sep> - This is a minor point and did not have any impact on the evaluation but VAE --> VHE, reparameterization trick --> resampling trick. Maybe providing rather original headings is better? It's a style issue that is up to tastes anyway so, again, it is minor. <sep> - ""However, sharing latent variables across an entire class reduces the encoding cost per element is significantly"": typo. <sep> - ""Figure ?? illustrates"".","Thank you for submitting your paper to *CONF*. The reviewers agree that the idea of sharing the approximating distribution across sets of variables is an interesting one and that the Omniglot experiments are thorough. However, although the authors make the nice addition of some simple examples during the revision period and a new table of quantitative results on Omniglot, the consensus is that the experimental results are not quite persuasive enough for publication. Adding a second dataset, such as mini-imagenet or the youtube faces dataset, would make the paper very strong."
"misc | decision  ==> In this paper, the authors propose a type of Normalizing Flows (Rezende and Mohamed, 2015) for Variational Autoencoders (Kingma and Welling, 2014; Rezende et al., 2014) they call Convolutional Normalizing Flows. <sep> More particularly, it aims at extending on the Planar Flow scheme proposed in Rezende and Mohamed (2015). The authors notice an improvement through their method over Normalizing Flows, IWAE with diagonal gaussian approximation, and standard Variational Autoencoders. <sep> As noted by AnonReviewer3, several baselines are missing. But the authors partly address that issue in the comment section for the MNIST dataset. <sep> The requirement of h being bijective seems wrong. For example, if h was a rectifier nonlinearity in the zero-derivative regime, the Jacobian determinant of the ConvFlow would be 1. <sep> More importantly, the main issue is that this paper might need to highlight the fundamental difference between their proposed method and Inverse Autoregressive Flow (Kingma et al., 2016). The proposed connectivity pattern proposed for the convolution in order to make the Jacobian determinant computation is exactly the same as Inverse Autoregressive Flow and the authors seems to be aware of the order dependence of their architecture which is every similar to autoregressive models. This presentation of the paper can be misleading concerning the true innovation in the model trained. Proposing ConvFlow as a type of Inverse Autoregressive Flow would be more accurate and would allow to highlight better the innovation of the work. <sep> Since this work does not offer additional significant insight over Inverse Autoregressive Flow, its value should be on demonstrating the efficiency of the proposed method. MNIST and Omniglot seems insufficient for that purpose given currently published work. <sep> In the current state, I can't recommend the paper for acceptance. <sep> Danilo Jimenez Rezende, Shakir Mohamed: Variational Inference with Normalizing Flows. ICML 2015 <sep> Danilo Jimenez Rezende, Shakir Mohamed, Daan Wierstra: Stochastic Back-propagation and Variational Inference in Deep Latent Gaussian Models. ICML 2014 <sep> Diederik P. Kingma, Max Welling: Auto-Encoding Variational Bayes. *CONF* 2014 <sep> Diederik P. Kingma, Tim Salimans, Rafal Józefowicz, Xi Chen, Ilya Sutskever, Max Welling: Improving Variational Autoencoders with Inverse Autoregressive Flow. NIPS 2016","Thank you for submitting you paper to *CONF*. *CONF*. Although there revision has improved the paper, the consensus from the reviewers is that this is not quite ready for publication."
"abstract | weakness | decision  ==>  ==> The paper proposes, under the GAN setting, mapping real data points back to the latent space via the ""generator reversal"" procedure on a sample-by-sample basis (hence without the need of a shared recognition network) and then using this induced empirical distribution as the ""ideal"" prior targeting which yet another GAN network might be trained to produce a better prior for the original GAN. <sep> I find this idea potentially interesting but am more concerned with the poorly explained motivation as well as some technical issues in how this idea is implemented, as detailed below. <sep> 1. Actually I find the entire notion of an ""ideal"" prior under the GAN setting a bit strange. To start with, GAN is already training the generator G to match the induced P_G(x) (from P(z)) with P_d(x), and hence by definition, under the generator G, there should be no better prior than P(z) itself (because any change of P(z) would then induce a different P_G(x) and hence only move away from the learning target). <sep> I get it that maybe under different P(z) the difficulty of learning a good generator G can be different, and therefore one may wish to iterate between updating G (under the current P(z)) and updating P(z) (under the current G), and hopefully this process might converge to a better solution. But I feel this sounds like a new angle and not the one that is adopted by the authors in this paper. <sep> 2. I think the discussions around Eq. (1) are not well grounded. Just as you said right before presenting Eq. (1), typically the goal of learning a DGM is just to match Q_x with the true data distrubution P_x. It is **not** however to match Q(x,z) with P(x,z). And btw, don't you need to put E_z[ ... ] around the 2nd term on the r.h.s. ? <sep> 3. I find the paper mingles notions from GAN and VAE sometimes and misrepresents some of the key differences between the two. <sep> E.g. in the beginning of the 2nd paragraph in Introduction, the authors write ""Generative models like GANs, VAEs and others typically define a generative model via a deterministic generative mechanism or generator ..."". While I think the use of a **deterministic** generator is probably one of the unique features of GAN, and that is certainly not the case with VAE, where typically people still need to specify an explicit probabilistic generative model. <sep> And for this same reason, I find the multiple references of ""a generative model P(x|z)"" in this paper inaccurate and a bit misleading. <sep> 4. I'm not sure whether it makes good sense to apply an SVD decomposition to the \\hat{z} vectors. It seems to me the variances \\nu^2_i shall be directly estimated from \\hat{z} as is. Otherwise, the reference ""ideal"" distribution would be modeling a **rotated** version of the \\hat{z} samples, which imo only introduces unnecessary discrepancies. <sep> 5. I don't quite agree with the asserted ""multi-modal structure"" in Figure 2. Let's assume a 2d latent space, where each quadrant represents one MNIST digit (e.g. 1,2,3,4). You may observe a similar structure in this latent space yet still learn a good generator under even a standard 2d Gaussian prior. I guess my point is, a seemingly well-partitioned latent space doesn't bear an obvious correlation with a multi-modal distribution in it. <sep> 6. The generator reversal procedure needs to be carried out once for each data point separately, and also when the generator has been updated, which seems to be introducing a potentially significant bottleneck into the training process.","This paper presents a method for learning more flexible prior distributions for GANs by learning another distribution on top of the latent codes for training examples. It's reminiscent of layerwise training of deep generative models. This seems like a reasonable thing to do, but it's probably not a substantial enough contribution given that similar things have been done for various other generative models. Experiments show improvement in samples compared with a regular GAN, but don't compare against various other techniques that have been proposed for fixing mode dropping. For these reasons, as well as various issues pointed out by the reviewers, I don't recommend acceptance."
"weakness | suggestion | weakness | suggestion  ==> The author proposed: <sep> 1. A data augmentation technique for asynchronous time series data. <sep> 2. A convolutional 'Significance' weighting neural network that assigns normalised weights to the outputs of a fully-connected autoregressive 'Offset' neural network, such that the output is a weighted average of the 'Offset' neural net. <sep> 3. An 'auxiliary' loss function. <sep> The experiments showed that: <sep> 1. The proposed method beat VAR/CNN/ResNet/LSTM 2 synthetic asynchronous data sets, 1 real electricity meter data set and 1 real financial bid/ask data set. It's not immediately clear how hyper-parameters for the benchmark models were chosen. <sep> 2. The author observed from the experiments that the depth of the offset network has negligible effect, and concluded that the 'Significance' network has crucial impact. (I don't see how this conclusion can be made.) <sep> 3. The proposed auxiliary loss is not useful. <sep> 4. The proposed architecture is more robust to noise in the synthetic data set compared to the benchmarks, and together with LSTM, are least prone to overfitting. <sep> Pros <sep> - Proposed a useful way of augmenting asynchronous multivariate time series for fitting autoregressive models <sep> - The convolutional Significance/weighting networks appears to reduce test errors (not entirely clear) <sep> Cons <sep> - The novelties aren't very well-justified. The 'Significance' network was described as critical to the performance, but there is no experimental result to show the sensitivity of the model's performance with respect to the architecture of the 'Significance' network. At the very least, I'd like to see what happens if the weighting was forced to be uniform while keeping the 'Offset' network and loss unchanged. <sep> - It's entirely unclear how the train and test data was split. This may be quite important in the case of the financial data set. <sep> - It's also unclear if model training was done on a rolling basis, which is common for time series forecasting. <sep> - The auxiliary loss function does not appear to be very helpful, but was described as a key component in the paper. <sep> Quality: The quality of the paper was okay. More details of the experiments should be included in the main text to help interpret the significance of the experimental results. The experiment also did not really probe the significance of the 'Significance' network even though it's claimed to be important. <sep> Clarity: Above average. <sep> Originality: Mediocre. Nothing really shines. Weighted average-type architecture has been proposed many times in neural networks (e.g., attention mechanisms). <sep> Significance: Low. It's unclear how useful the architecture really is.","The reviewers feel that the novelties in the model are not significant. Furthermore, they suggest that empirical results could be improved by <sep> 1: analyses showing how the significance network functions and directly measuring its impact <sep> 2: More reproducible experiments. In particular, this is really an applications paper, and the experiments on the main application are not reproducible because the data is proprietary. <sep> 3: baselines that make assumptions more in line with the authors' problem setup"
"weakness | rebuttal_process | decision  ==> The paper studies the effect of different network structures (plain CNN, ResNet and DenseNet). This is an interesting line of research to pursue, however, it gives an impression that a large amount of recent work in this direction has not been considered by the authors. The paper contains ONLY 4 references. <sep> Some references that might be useful to consider in the paper: <sep> - K. Greff et. al. Highway and Residual Networks learn Unrolled Iterative Estimation. <sep> - C. Zang et. al. UNDERSTANDING DEEP LEARNING REQUIRES RETHINKING GENERALIZATION <sep> - Q. Liao el. al. Bridging the Gaps Between Residual Learning, Recurrent Neural Networks and Visual Cortex <sep> - A. Veit et. al. Residual Networks Behave Like Ensembles of Relatively Shallow Networks <sep> - K. He at. Al Identity Mappings in Deep Residual Networks <sep> The writing and the structure of the paper could be significantly improved. From the paper, it is difficult to understand the contributions. From the ones listed in Section 1, it seems that most of the contributions were shown in the original ResNet and DenseNet papers. Given, questionable contribution and a lack of relevant citations, it is difficult to recommend for acceptance of the paper. <sep> Other issues: <sep> Section 2: ""Skip connection …. overcome the overfitting"", could the authors comment on this a bit more or point to relevant citation? <sep> Section 2: ""We increase the number of skip connections from 0 to 28"", it is not clear to me how this is done. <sep> Section 3.1.1 ""deep Linear model"", what the authors mean with this? Multiple layers without a nonlinearity? Is it the same as Cascade Net? <sep> Section 3.2 From the data description, it is not clear how the training data was obtained. Could the authors provide more details on this? <sep> Section 3.2 ""…, only 3 of them are chosen to be displayed…"", how the selection process was done? <sep> Section 3.2 ""Instead of showing every layer's output we exhibit the 3th, 5th, 7th, 9th, 11th, 13th and the final layer's output"", according to the description in Fig. 7 we should be able to see 7 columns, this description does not correspond to Fig. 7. <sep> Section 4 ""This paper investigates how skip connections works in vision tasks…"" I do not find experiments with vision datasets in the paper. In order to claim this, I would encourage the authors to run tests on a CV benchmark dataset (e. g. ImageNet)","The paper appears unfinished in many ways: the experiments are preliminary, the paper completely ignored a large body of prior work on the subject, and the presentation needs substantial improvements. The authors did not provide a rebuttal. <sep> I encourage the authors to refrain from submitting unfinished papers such as this one in the future, as it unnecessarily increases the load on a review system that is already strained."
"abstract | weakness  ==> This paper studies the impact of angle bias on learning deep neural networks, where angle bias is defined to be the expected value of the inner product of a random vectors (e.g., an activation vector) and a given vector (e.g., a weight vector).  The angle bias is non-zero as long as the random vector is non-zero in expectation and the given vector is non-zero.  This suggests that the some of the units in a deep neural network have large values (either positive or negative) regardless of the input, which in turn suggests vanishing gradient.  The proposed solution to angle bias is to place a linear constraint such that the sum of the weight becomes zero.  Although this does not rule out angle bias in general, it does so for the very special case where the expected value of the random vector is a vector consisting of a common value.  Nevertheless, numerical experiments suggest that the proposed approach can effectively reduce angle bias and improves the accuracy for training data in the CIFAR-10 task.  Test accuracy is not improved, however. <sep> Overall, this paper introduces an interesting phenomenon that is worth studying to gain insights into how to train deep neural networks, but the results are rather preliminary both on theory and experiments. <sep> On the theoretical side, the linearly constrained weights are only shown to work for a very special case.  There can be many other approaches to mitigate the impact of angle bias.  For example, how about scaling each variable in a way that the mean becomes zero, instead of scaling it into [-1,+1] as is done in the experiments?  When the mean of input is zero, there is no angle bias in the first layer.  Also, what about if we include the bias term so that b + w a is the preactivation value? <sep> On the experimental side, it has been shown that linearly constrained weights can mitigate the impact of angle bias on vanishing gradient and can reduce the training error, but the test error is unfortunately increased for the particular task with the particular dataset in the experiments.  It would be desirable to identify specific tasks and datasets for which the proposed approach outperforms baselines.  It is intuitively expected that the proposed approach has some merit in some domains, but it is unclear exactly when and where it is. <sep> Minor comments: <sep> In Section 2.2, is Layer 1 the input layer or the next?","The paper identifies an interesting problem in sigmoid deep nets, addressed diffferently by batchnorm, and proposes a different simple fix. It shows empirically that constraining neuron's weights to sum to zero improves training of a 100 layers sigmoid MLP. <sep> The work is currenlty limited in its theoretical contribution, and regarding the showcased practical interest of the method compared to batchnorm (it's not appplicable to RELUs and shows positive effect on optimization but not generalization)."
"rating_summary | weakness | rebuttal_process | weakness | decision  ==>  ==> 1. Paper Summary <sep> This paper adds a separate network at every layer of a residual network that performs classification. They minimize the loss of every classifier using two proposed weighting schemes. They also ensemble this model. <sep> 2. High level paper <sep> The organization of this paper is a bit confusing. Two weighing schemes are introduced in Section 3.1, then the ensemble model is described in Section 3.2, then the weighing schemes are justified in Section 4.1. <sep> Overall this method is essentially an cascade where each cascade classifier is a residual block. Every input is passed through as many stages as possible until the budget is reached. While this model is likely quite useful in industrial settings, I don't think the model itself is wholly original. <sep> The authors have done extensive experiments evaluating their method in different settings. I would have liked to see a comparison with at least one other anytime method. I think it is slightly unfair to say that you are comparing with Xie & Tu, 2015 and Huang et al., 2017 just because they use the CONSTANT weighing schemes. <sep> 3. High level technical <sep> I have a few concerns: <sep> - Why does AANN+LINEAR nearly match the accuracy of EANN+SIEVE near 3e9 FLOPS in Figure 4b but EANN+LINEAR does not in Figure 4a? Shouldn't EANN+LINEAR be strictly better than AANN+LINEAR? <sep> - Why do the authors choose these specific weighing schemes? Section 4.1 is devoted to explaining this but it is still unclear to me. They talk about there being correlation between the predictors near the end of the model so they don't want to distribute weight near the final predictors but this general observation doesn't obviously lead to these weighing schemes, they still seem a bit adhoc. <sep> A few other comments: <sep> - Figure 3b seems to contain strictly less information than Figure 4a, I would remove Figure 3b and draw lines showing the speedup you get for one or two accuracy levels. <sep> Questions: <sep> - Section 3.1: ""Such an ideal θ* does not exist in general and often does not exist in practice."" Why is this the case? <sep> - Section 3.1: "" In particular, spreading weights evenly as in (Lee et al., 2015) keeps all i away from their possible respective minimum"" Why is this true? <sep> - Section 3.1: ""Since we will evaluate near depth b3L/4e, and it is the center of L/2 low-weight layers, we increase it weight by 1/8."" I am completely lost here, why do you do this? <sep> 4. Review summary <sep> Ultimately because the model itself resembles previous cascade models, the selected weighings have little justification, and there isn't a comparison with another anytime method, I think this paper isn't yet ready for acceptance at *CONF*.","The paper received mixed reviews with scores of 5 (R1), 5 (R2), 7 (R3). All three reviewers raise concerns about the lack of comparisons to other methods. The rebuttal is not compelling on this point. There are quite a few methods that could be used for this application available (often with source code) and should be compared to, e.g. DenseNets (Huang et al.). Given that the proposed method isn't in of itself hugely novel, a thorough experimental evaluation is crucial to the justifying the approach. The AC has closely looked at the rebuttal and the paper and feels that it cannot be accepted for this reason at this time."
"rating_summary | weakness | decision  ==>  ==> The paper proposes a method for adapting a pre-trained network, trained on a fixed number of classes, to incorporate novel classes for doing classification, especially when the novel classes only have a few training examples available. They propose to do a `hard' distillation, i.e. they introduce new nodes and parameters to the network to add the new classes, but only fine-tune the new networks without modifying the original parameters. This ensures that, in the new expanded and fine-tuned network, the class confusions will only be between the old and new classes and not between the old classes, thus avoiding catastrophic forgetting. In addition they use GMMs trained on the old classes during the fine-tuning process, thus avoiding saving all the original training data. <sep> They show experiments on public benchmarks with three different scenarios, i.e.  base and novel classes from different domains, base and novel classes from the same domain and novel classes have similarities among themselves, and base and novel classes from the same domain and each novel class has similarities with at least one of the base class. <sep> - The paper is generally well written and it is clear what is being done <sep> - The idea is simple and novel; to the best of my knowledge it has not been tested before <sep> - The method is compared with Nearest Class Means (NCM) and Prototype-kNN with soft distillation <sep> (iCARL; where all weights are fine-tuned). The proposed method performs better in low-shot settings and comparably when large number of training examples of the novel classes are available <sep> - My main criticism will be the limited dataset size on which the method is validated. The ILSVRC12 <sep> subset contains 5 base and 5 novel classes and the UT-Zappos50K subset also has 10 classes. The idea is simple and novel, which is good, but the validation is limited and far from any realistic use. Having only O(10) classes is not convincing, especially when the datasets used do have large number of classes. I agree that this will not allow or will takes some involved manual effort to curate subsets for the settings proposed, but it is necessary for being convincing.","Two reviewers recommended rejection, and one is slightly more positive. The main concern is that the experiments are not convincing (ie, the number of base and added classes is very small). Furthermore, while the paper introduces several interesting ideas, the AC agrees with the second reviewer that each of these could be explored in more detail. This work seems preliminary. The authors are encouraged to resubmit to a future conference."
"strength | weakness | misc | decision  ==> This paper proposes a technique to improve the output of GANs by maximising a separate score that aims to mimic human interactions. <sep> Summary: <sep> The goal of the technique to involve human interaction in generative processes is interesting. The proposed addition of a new loss function for this purpose is an obvious choice, not particularly involved. It is unclear to me whether the paper has value in its current form, that is without experimental results for the task it achieves. It feels to premature for publication. <sep> More comments: <sep> The main problem with this paper is that the proposed systems is designed for a human interaction setting but no such experiment is done or presented. The title is misleading, this may be the direction where the authors of the submission want to go, but the title  "".. with human interactions"" is clearly misleading. ""Model of human interactions"" may be more appropriate. <sep> The technical idea of this paper is to introduce a separate score in the GAN training process. This modifies the generator objective.  Besides ""fooling"" the discriminator, the generator objective is to maximise user interaction with the generated batch of images. This is an interesting objective but since no interactive experiments presented in this paper, the rest of the experiments hinges on the definition of ""PIR"" (positive interaction rate)using a model of human interaction. Instead of real interactions, the submission proposes to maximise the activations of hidden units in a separate neural network. By choosing the hierarchy level and type of filter the results of the GAN differ. <sep> I could not appreciate the results in Figure 2 since I was missing the definition of PIR, how it is drawn in the training setup. Further I found it not surprising that the PIR changes when a highly parameterised model is trained for this task. The PIR value comes from a separate network not directly accessible during training time, nonetheless I would have been surprised to not see an increase. Please comment in the rebuttal and I would appreciate if the details of the synthetic PIR values on the training set could be explained. <sep> - Technically it was a bit unclear to me how the objective is defined. There is a PIR per level and filter (as defined in C4) but in the setup the L_{PIR} was mentioned to be a scalar function, how are the values then summarized? There is a PIR per level and feature defined in C4. <sep> - What does the PIR with the model in Section 3 stand for? Shouldn't be something like ""uniqueness"", that is how unique is an image in a batch of images be a better indicator? Besides, the intent of what possibly interesting PIR examples will be was unclear. <sep> E.g., the statement at the end of 2.1 is unclear at that point in the document. How is the PIR drawn exactly? What does it represent? Is there a PIR per image? It becomes clear later, but I suggest to revisit this description in a new version. <sep> - Also I suggest to move more details from Section C4 into the main text in Section 3. The high level description in Section 3.","The reviewers agree that the idea of incorporating humans in the training of generative adversarial networks is interesting and worthwhile exploring. However, they felt that the paper fell short in providing strong support for their approach. The AC agrees. The authors are encouraged to strengthen their work and resubmit to a future venue."
"abstract | weakness | rebuttal_process  ==> The authors use a distant supervision technique to add dialogue act tags as a conditioning factor for generating responses in open-domain dialogues. In their evaluations, this approach, and one that additionally uses policy gradient RL with discourse-level objectives to fine-tune the dialogue act predictions, outperform past models for human-scored response quality and conversation engagement. <sep> While this is a fairly straightforward idea with a long history, the authors claim to be the first to use dialogue act prediction for open-domain (rather than task-driven) dialogue. If that claim to originality is not contested, and the authors provide additional assurances to confirm the correctness of the implementations used for baseline models, this article fills an important gap in open-domain dialogue research and suggests a fruitful future for structured prediction in deep learning-based dialogue systems. <sep> Some points: <sep> 1. The introduction uses ""scalability"" throughout to mean something closer to ""ability to generalize."" Consider revising the wording here. <sep> 2. The dialogue act tag set used in the paper is not original to Ivanovic (2005) but derives, with modifications, from the tag set constructed for the DAMSL project (Jurafsky et al., 1997; Stolcke et al., 2000). It's probably worth citing some of this early work that pioneered the use of dialogue acts in NLP, since they discuss motivations for building DA corpora. <sep> 3. In Section 2.1, the authors don't explicitly mention existing DA-annotated corpora or discuss specifically why they are not sufficient (is there e.g. a dataset that would be ideal for the purposes of this paper except that it isn't large enough?) <sep> 3. The authors appear to consider only one option (selecting the top predicted dialogue act, then conditioning the response generator on this DA) among many for inference-time search over the joint DA-response space. A more comprehensive search strategy (e.g. selecting the top K dialogue acts, then evaluating several responses for each DA) might lead to higher response diversity. <sep> 4. The description of the RL approach in Section 3.2 was fairly terse and included a number of ad-hoc choices. If these choices (like the dialogue termination conditions) are motivated by previous work, they should be cited. Examples (perhaps in the appendix) might also be helpful for the reader to understand that the chosen termination conditions or relevance metrics are reasonable. <sep> 5. The comparison against previous work is missing some assurances I'd like to see. While directly citing the codebases you used or built off of is fantastic, it's also important to give the reader confidence that the implementations you're comparing to are the same as those used in the original papers, such as by mentioning that you can replicate or confirm quantitative results from the papers you're comparing to. Without that there could always be the chance that something is missing from the implementation of e.g. RL-S2S that you're using for comparison. <sep> 6. Table 5 is not described in the main text, so it isn't clear what the different potential outputs of e.g. the RL-DAGM system result from (my guess: conditioning the response generation on the top 3 predicted dialogue acts?) <sep> 7. A simple way to improve the paper's clarity for readers would be to break up some of the very long paragraphs, especially in later sections. It's fine if that pushes the paper somewhat over the 8th page. <sep> 8. A consistent focus on human evaluation, as found in this paper, is probably the right approach for contemporary dialogue research. <sep> 9. The examples provided in the appendix are great. It would be helpful to have confirmation that they were selected randomly (rather than cherry-picked).","This work takes dialogue acts into account to generate responses in a human-machine conversation. However, incorporating dialogue acts into open-domain dialogue was already the focus of Zhao et al's ACL 2017 paper, Learning Discourse-level Diversity for Neural Dialog Models using Conditional Variational Autoencoders, and using dialogue acts in a policy for human-machine conversation was also an idea that already appeared in Serban et al 2017, A Deep Reinforcement Learning Chatbot. Despite the authors' response that tries to adjust their claims and incorporate a more thorough overview, I encourage the authors to re-work their research with a much more careful and reliable examination of previous work and how their effort should be understood in that more comprehensive context."
"abstract | decision  ==>  ==> This paper proposes integrating information from a semantic resource that quantifies the affect of different words into a text-based word embedding algorithm. <sep> The affect lexical seems to be a very interesting resource (although I'm not sure what it means to call it 'state of the art'), and definitely support the endeavour to make language models more reflective of complex semantic and pragmatic phenomena such as affect and sentiment. <sep> The justification for why we might want to do this with word embeddings in the manner proposed seems a little unconvincing to me: <sep> - The statement that 'delighted' and 'disappointed' will have similar contexts is not evident to me at least (other then them both being participle / adjectives). <sep> - Affect in language seems to me to be a very contextual phenomenon. Only a tiny subset of words have intrinsic and context-free affect. Most affect seems to me to come from the use of words in (phrasal, and extra-linguistic) contexts, so a more context-dependent model, in which affect is computed over phrases or sentences, would seem to be more appropriate. Consider words like 'expensive', 'wicked', 'elimination'... <sep> The model proposes several applications (sentiment prediction, predicting email tone, word similarity) where the affect-based embeddings yield small improvements. However, in different cases, taking different flavours of affect information (V, A or D) produces the best score, so it is not clear what to conclude about what sort of information is most useful. <sep> It is not surprising to me that an algorithm that uses both WordNet and running text to compute word similarity scores improves over one that uses just running text. It also not surprising that adding information about affect improves the ability to predict sentiment and the tone of emails. <sep> To understand the importance of the proposed algorithm (rather than just the addition of additional data), I would like to see comparison with various different post-processing techniques using WordNet and the affect lexicon (i.e. not just Bollelaga et al.) including some much simpler baselines. For instance, what about averaging WordNet path-based distance metrics and distance in word embedding space (for word similarity), and other ways of applying the affect data to email tone prediction?","This work attempts to incorporate affect information from additional resources into word embeddings. This is a valuable goal, but the methods used are very similar to existing ones, and the experimental results are not quite convincing enough to make a strong enough case for accepting the paper."
"strength | weakness | rating_summary | rebuttal_process  ==>  ==> The main emphasis of this paper is how to add background knowledge so as to improve the performance of NLU (specifically QA and NLI) systems. They adopt the sensible perspective that background knowledge might most easily be added by providing it in text format. However, in this paper, the way it is added is simply by updating word representations based on this extra text. This seems too simple to really be the right way to add background knowledge. <sep> In practice, the biggest win of this paper turns out to be that you can get quite a lot of value by sharing contextualized word representations between all words with the same lemma (done by linguistic preprocessing; the paper never says exactly how, not even if you read the supplementary material). This seems a useful observation which it would be easy to apply everywhere and which shows fairly large utility from a bit of linguistically sensitive matching!  As the paper notes, this type of sharing is the main delta in this paper from simply using a standard deep LSTM (which the paper claims to not work on these data sets, though I'm not quite sure couldn't be made to work with more tuning). <sep> pp. 6-7: The main thing of note seems to be that sharing of representations between words with the same lemma (which the tables refer to as ""reading"" is worth a lot (3.5-6.0%), in every case rather more than use of background knowledge (typically 0.3-1.5%). A note on the QA results: The QA results are certainly good enough to be in the range of ""good systems"", but none of the results really push the SOTA. The best SQuAD (devset) results are shown as several percent below the SOTA. In the table the TriviaQA results are shown as beating the SOTA, and that's fair wrt published work at the time of submission, but other submissions show that all of these results are below what you get by running the DrQA (Chen et al. 2017) system off-the-shelf on TriviaQA, so the real picture is perhaps similar to SQuAD, especially since DrQA is itself now considerably below the SOTA on SQUAD. Similar remarks perhaps apply to the NLI results. <sep> p.7 In the additional NLI results, it is interesting and valuable to note that the lemmatization and knowledge help much more when amounts of data (and the covarying dimensionality of the word vectors) is much smaller, but the fact that the ideas of this paper have quite little (or even negative) effects when run on the full data with full word vectors on top of the ESIM model again draws into question whether enough value is being achieved from the world knowledge. <sep> Biggest question: <sep> - Are word embeddings powerful enough as a form of memory to store the kind of relational facts that you are accessing as background knowledge? <sep> Minor notes: <sep> - The paper was very well written/edited. The only real copyediting I noticed was in the conclusion: and be used ➔ and can be used; that rely on ➔ that relies on. <sep> - Should reference to (Manning et al. 1999) better be to (Manning et al. 2008) since the context here appears to be IR systems? <sep> - On p.3 above sec 3.1: What is u? Was that meant to be z? <sep> - On p.8, I'm a bit suspicious of the ""Is additional knowledge used?"" experiment which trains with knowledge and then tests without knowledge. It's not surprising that this mismatch might hurt performance, even if the knowledge provided no incremental value over what could be gained from standard word vectors alone. <sep> - In the supplementary material the paper notes that the numbers are from the best result from 3 runs. This seems to me a little less good experimental practice than reporting an average of k runs, preferably for k a bit bigger than 3.","Pros: <sep> + The paper is very clearly written. <sep> + The proposed re-embedding approach is easily implemented and can be integrated into fancier architectures. <sep> Cons: <sep> - A lot of the gains reported come from lemmatization, and the gains from background knowledge become marginal when used on a stronger baseline (e.g., ESIM with full training data and full word vectors). <sep> This paper is rather close to the decision boundary. The authors had reasonable answers for some of the reviewers' concerns, but in the end the reviewers were not completely convinced."
"weakness | rebuttal_process | strength | weakness  ==> The paper proposes a generalization of an algorithm by Yin et al. (2017), which performs SGD with adaptive batch sizes. The present paper generalizes the algorithm to SGD with momentum. Since the original algorithm was already formulated with a general utility function, the proposed algorithm is similar in structure but replaces the utility function so that it takes momentum into account. Experiments on an image classification task show improvements in the training loss. However, no test accuracies are reported and the learning curves have suspicious artifacts, see below. Experiments on a relation extraction task show little improvement over SGD with momentum and constant batch size. <sep> COMMENTS: <sep> The paper discusses a relevant issue. While adaptive learning algorithms are popular in deep learning, most algorithms adapt the learning rate or the momentum coefficient, but not the batch size. It appears to me that the main idea and the overall structure of the proposed algorithm is the same as in the one published by Yin et al. (2017), and that only few changes were necessary to include momentum. Given the incremental process, I find the presentation unnecessarily involved, and experiments not convincing enough. <sep> Concerning the presentation, the paper dedicates two full pages on a review of the algorithm by Yin et al. (2017). The first page of this review states that, for large enough batch sizes, the change of the objective function in SGD is normal distributed with a variance that is inversely proportional the batch size. It seems to me that this is a direct consequence of the central limit theorem. The derivation, however, is quite technical and introduces some quantities that are never used (e.g., ξ→j is never used individually, only the combined term ϵt defined below Eq. 12 is). The second page of the review seems to discuss the main part of the algorithm, but I could not follow it. First, a ""state"" st (also written as S) is introduced, which, according to the text, is ""the objective value"", which was earlier denoted by F. Nevertheless, the change of st, Eq. 5, appears to obey a different probability distribution than the change of F. The paper provides a verbal explanation for this discrepancy, saying that it is possible that S is first reduced to the minimum S∗ of the objective and then increased again. However, in my understanding, the minimum of the objective is only realized at a singular point in parameter space. Crossing this point in an update step should have zero probability as long as the model has more than one parameter. The explanation also does not make it clear why the argument should apply to S (or s) but not to F. <sep> Page 5 provides pseudocode for the proposed algorithm. However, I couldn't find an explanation of the code. The code suggests that, for each update step, one gradually increases the batch size until it becomes larger or equal than a running estimate of the optimal batch size. While this may be a plausible strategy in practice, it seems to have a bias that is not addressed in the paper: the algorithm recalculates a noisy estimate of the optimal batch size after each increase of the batch size, and it terminates as soon as the noisy estimate happens to be small enough, resulting in a bias towards a smaller than optimal batch size. A probably more important issue is that the algorithm is sequential and hard to parallelize, where parallelization is usually the main motivation to use larger batch sizes. As the gradient noise scales inversely proportional to the batch size, I don't see why increasing the batch size should be preferred over decreasing the learning rate unless optimizations with a larger batch size can be parallelized. The experiments don't compare the two alternatives. <sep> Concerning the experiments, it seems peculiar that the learning curves in Figure 1 remain at a constant value for a long time at the beginning of the optimization before they begin to drop. Do the authors understand this behavior? It could indicate that the magnitude of the random initialization was chosen too small. I.e., the parameters might have been initialized too close to zero, where the loss is stationary due to symmetries. Also, absolute values of the training loss can be deceptive since there is often no natural scale. A better indicator of convergence would be the test accuracy. The identification of the ""batch size boom"" is interesting.","The reviewers generally thought the proposed algorithm was a straightforward extension of Yin et al., 2017, and not enough for a new paper. They also objected to a lack of test results (to show generalization), but the authors did provide these in their revision. <sep> Pros: <sep> + Adaptive batch sizing is useful, especially if the larger batches license parallelization. <sep> Cons: <sep> - Small, incremental change to the algorithm from Yin et al., 2017 <sep> - Test performance did not improve over well-tuned momentum optimization, which limits the appeal of the method."
"rebuttal_process | abstract | weakness | abstract | misc | suggestion | rebuttal_process  ==> I personally warmly welcome any theoretically grounded methods to perform deep learning. I read the paper with interest, but I have two concerns about the main theoretical result (Theorem 1, lifelong learning PAC-Bayes bound). <sep> * Firstly, the bound is valid for a [0,1]-valued loss, which does not comply with the losses used in the experiments (Euclidean distance and cross-entropy). This is not a big issue, as I accept that the authors are mainly interested in the learning strategy promoted by the bound. However, this should clearly appear in the theorem statement. <sep> * Secondly, and more importantly, I doubt that the uaw of the meta-posterior as a distribution over priors for each task is valid. In Proposition 1 (the classical single-task PAC-Bayes bound), the bound is valid with probability 1-delta for one specific choice of prior P, and this choice must be independent of the learning sample S. However, it appears that the bound should be valid uniformly for all P in order to be used in Theorem 1 proof (see Equation 18). From a learning point of view, it seems counterintuitive that the prior used in the KL term to learn from a task relies on the training samples (i.e., the same training samples are used to learn the meta-posterior over priors, and the task specific posterior). <sep> A note about the experiments: <sep> I am slightly disappointed that the authors compared their algorithm solely with methods learning from fewer tasks. I would like to see the results obtained by another method using five tasks. A simple idea would be to learn a network independently for each of the five tasks, and consider as a meta-prior an isotropic Gaussian distribution centered on the mean of the five learned weight vectors. <sep> Typos and minor comments: <sep> - Equation 1: \\ell is never explicitly defined. <sep> - Equation 4: Please explicitly define m in this context (size of the learning sample drawn from tau). <sep> - Page 4, before Equation 5: A dot is missing between Q and ""This"". <sep> - Page 7, line 3: Missing parentheses around equation number 12. <sep> - Section 5.1.1, line 5: ""The hypothesis class is a the set of..."" <sep> - Equation 17: Q_1, ... Q_n are irrelevant. <sep> === UPDATE === <sep> I increased my score after author's rebuttal. See my other post.","The author's revisions addressed clarity issues and some experimental issues (e.g., including MAML results in the comparison). The work takes an original path to an important problem (transfer learning, essentially). There is a question of significance, and this is due to the fact that the empirical comparisons are still very limited. The task is an artificial one derived from MNIST. I would call this ""toy"" as well. On this toy task, the approach isn't that much different from MAML, which is not in of itself a problem, but it would be interested to have a less superficial discussion of the differences. <sep> The authors mention that they didn't have time for a larger empirical study. I think one is necessary in this case because the work is purposing a new learning algorithm/framework, and the question of its potential impact/significance is an empirical one."
"abstract | weakness | rebuttal_process | decision  ==>  ==> Summary: <sep> This paper proposes an adversarial learning framework for machine comprehension task. Specifically, authors consider a reader network which learns to answer the question by reading the passage and a narrator network which learns to obfuscate the passage so that the reader can fail in its task. Authors report results in 3 different reading comprehension datasets and the proposed learning framework results in improving the performance of GMemN2N. <sep> My Comments: <sep> This paper is a direct application of adversarial learning to the task of reading comprehension. It is a reasonable idea and authors indeed show that it works. <sep> 1. The paper needs a lot of editing. Please check the minor comments. <sep> 2. Why is the adversary called narrator network? It is bit confusing because the job of that network is to obfuscate the passage. <sep> 3. Why do you motivate the learning method using self-play? This is just using the idea of adversarial learning (like GAN) and it is not related to self-play. <sep> 4. In section 2, first paragraph, authors mention that the narrator prevents catastrophic forgetting. How is this happening? Can you elaborate more? <sep> 5. The learning framework is not explained in a precise way. What do you mean by re-initializing and retraining the narrator? Isn't it costly to reinitialize the network and retrain it for every turn? How many such epochs are done? You say that test set also contains obfuscated documents. Is it only for the validation set? Can you please explain if you use obfuscation when you report the final test performance too? It would be more clear if you can provide a complete pseudo-code of the learning procedure. <sep> 6. How does the narrator choose which word to obfuscate? Do you run the narrator model with all possible obfuscations and pick the best choice? <sep> 7. Why don't you treat number of hops as a hyper-parameter and choose it based on validation set? I would like to see the results in Table 1 where you choose number of hops for each of the three models based on validation set. <sep> 8. In figure 2, how are rounds constructed? Does the model sees the same document again and again for 100 times or each time it sees a random document and you sample documents with replacement? This will be clear if you provide the pseudo-code for learning. <sep> 9. I do not understand author's' justification for figure-3. Is it the case that the model learns to attend to last sentences for all the questions? Or where it attends varies across examples? <sep> 10. Are you willing to release the code for reproducing the results? <sep> Minor comments: <sep> Page 1, ""exploit his own decision"" should be ""exploit its own decision"" <sep> In page 2, section 2.1, sentence starting with ""Indeed, a too low percentage …"" needs to be fixed. <sep> Page 3, ""forgetting is compensate"" should be ""forgetting is compensated"". <sep> Page 4, ""for one sentences"" needs to be fixed. <sep> Page 4, ""unknow"" should be ""unknown"". <sep> Page 4, ""??"" needs to be fixed. <sep> Page 5, ""for the two first datasets"" needs to be fixed. <sep> Table 1, ""GMenN2N"" should be ""GMemN2N"". In caption, is it mean accuracy or maximum accuracy? <sep> Page 6, ""dataset was achieves"" needs to be fixed. <sep> Page 7, ""document by obfuscated this word"" needs to be fixed. <sep> Page 7, ""overall aspect of the two first readers"" needs to be fixed. <sep> Page 8, last para, references needs to be fixed. <sep> Page 9, first sentence, please check grammar. <sep> Section 6.2, last sentence is irrelevant.","The paper presents an adversarial learning framework for reading comprehension. Although the idea is interesting and presents an approach that ideally would make reading comprehension approaches more robust, the results are not substantially solid (see reviewer 3's comments) compared to other baselines to warrant acceptance. Comments from reviewer 2 are also noteworthy where they mention that adversarial perturbations to a context around an answer can alter the facts in the context, thus destroying the actual information present there, and the rebuttal does not seem to satisfy the concern. Addressing these issues will strengthen the paper for a potential future venue."
"abstract | weakness | suggestion | weakness  ==> The paper describes an approach to use LSTM's for finger classification based on ECOG. and a transfer learning extension of which two variations exists. From the presented results, the LSTM model is not an improvement over a basic linear model. The transfer learning models performs better than subject specific models on a subset of the subjects. Overall, I think the problem Is interesting but the technical description and the evaluation can be improved. I am not confident in the analysis of the model. Additionally, the citations are not always correct and some related work is not referenced at all. For the reasons above, I am not willing to recommend the paper for acceptance at his point. <sep> The paper tackles a problem that is challenging and interesting. Unfortunately, the dataset size is limited. <sep> This is common for brain data and makes evaluation much more difficult. <sep> The paper states that all hyper-parameters were optimized on 75% of subject B data. <sep> The actual model training was done using cross-validation. <sep> So far this approach seems more or less correct but in this case I would argue that subject B should not be considered for evaluation since its data is heavily used for hyper-parameter optimization and the results obtained on this subject are at risk of being biased. <sep> Omitting subject B from the analysis, each non-transfer learning method  performs best on one of the remaining subjects. <sep> Therefore it is not clear that an LSTM model is an improvement. <sep> For transfer learning (ignoring B again) only C and D are improved but it is unclear what the variance is. <sep> In the BCI community there are many approaches that use transfer learning with linear models. I think that it would be interesting how linear model transfer learning would fare in this task. <sep> A second issue that might inflate the results is the fact that the data is shuffled randomly. While this is common practice for most machine learning tasks, it is dangerous when working with brain data due to changes in the signal over time. As a result, selecting random samples might inflate the accuracy compared to having a proper train and test set that are separated in time. Ideally the cross-validation should be done using contiguous folds. <sep> I am not quite sure whether it should be possible to have an accuracy above chance level half a second before movement onset? How long does motor preparation take? I am not familiar with this specific subject, but a quick search gave me a reaction time for sprinters of .15 seconds. Is it possible that cue processing activity was used to obtain the classification result? Please discuss this effect because I am do not understand why it should be possible to get above chance level accuracy half a second before movement onset. <sep> There are also several technical aspects that are not clear to me. I am confident that I am unable to re-implement the proposed method and their baseline given the information provided. <sep> LDA baseline: <sep> ————————— <sep> For the LDA baseline, how is the varying sequence length treated? <sep> Ledoit wolf analytic  regularization is used, but it isn not referenced. If you use that method, cite the paper. <sep> The claim that LDA works for structured experimental tasks but not in naturalistic scenarios and will not generalize when electrode count and trial duration increases is a statement that might be true. However, it is never empirically verified.  Therefore this statement should not be in the paper. <sep> HMM baseline <sep> ————————— <sep> How are the 1 and the 2 state HMM used w.r.t. the 5 classes? It is unclear to me how they are used exactly. Is there a single HMM per class? Please be specific. <sep> LSTM Model <sep> ————— <sep> What is the random and language model initialization scheme? I can only find the sequence auto-encoder in the Dai and Le paper. <sep> Model analysis <sep> ——————————- <sep> It is widely accepted in the neuroimaging community that linear weight vectors should not be interpreted directly. It is actually impossible to do this.  Therefore this section should be completely re-done. Please read the following paper on this subject. <sep> Haufe, Stefan, et al. ""On the interpretation of weight vectors of linear models in multivariate neuroimaging."" Neuroimage 87 (2014): 96-110. <sep> References <sep> ———— <sep> Ledoit wolf regularization is used but not cited. Fix this. <sep> There is no citation for the random/language model initialization of the LSTM model. I have no clue how to do this without proper citation. <sep> Le at al (2011) are referenced for auto-encoders. This is definitely not the right citation. <sep> Rumelhart, Hinton, & Williams, 1986a; Bourlard & Kamp, 1988; Hinton & Zemel, 1994 and Bengio, Lamblin, Popovici, & Larochelle, 2007; Ranzato, Poultney, Chopra, & LeCun, 2007 are probably all more relevant. <sep> Please cite the relevant work on affine transformations for transfer learning especially the work by morioka et al who also learn an input transferm. <sep> Morioka, Hiroshi, et al. ""Learning a common dictionary for subject-transfer decoding with resting calibration."" NeuroImage 111 (2015): 167-178.","This paper tries to establish that LSTMs are suitable for modeling neural signals from the brain. However, the committee and most reviewers find that results are inconclusive. Results are mixed across subjects. We think it would have been far more interesting to compare other types of sequence models for this task other than the few simple baselines implemented here. It is also unclear what is the LSTM learning extra in contrast with the other models presented in the paper."
"weakness | rebuttal_process | weakness | suggestion  ==>  ==> The paper proposes to augment (traditional) text-based sentence generation/dialogue approaches by incorporating visual information.  The idea is that associating visual information with input text, and using that associated visual information as additional input will produce better output text than using only the original input text. <sep> The basic idea is to collect a bunch of data consisting of both text and associated images or video.  Here, this was done using Japanese news programs.  The text+image/video is used to train a model that requires both as input and that encodes both as context vectors, which are then combined and decoded into output text.  Next, the image inputs are eliminated, with the encoded image context vector being instead associatively predicted directly from the encoded text context vector (why not also use the input text to help predict the visual context?), which is still obtained from the text input, as before.  The result is a model that can make use of the text-visual associations without needing visual stimuli.  This is a nice idea. <sep> Actually, based on the brief discussion in Section 2.2.2, it occurs to me that the model  might not really be learning visual context vectors associatively, or, that this doesn't really have meaning in some sense.  Does it make sense to say that what it is really doing is just learning to associate other concepts/words with the input text, and that it is using the augmenting visual information in the training data to provide those associations?  Is this worth talking about? <sep> Unfortunately, while the idea has merit, and I'd like to see it pursued, the paper suffers from a fatal lack of validation/evaluation, which is very curious, given the amount of data that was collected, the fact that the authors have both a training and a test set, and that there are several natural ways such an evaluation might be performed.  The two examples of Fig 3 and the additional four examples in the appendix are nice for demonstrating some specific successes or weaknesses of the model, but they are in no way sufficient for evaluation of the system, to demonstrate its accuracy or value in general. <sep> Perhaps the most obvious thing that should be done is to report the model's accuracy for reproducing the news dialogue, that is, how accurately is the next sentence predicted by the baseline and ACM models over the training instances and over the test data?  How does this compare with other state-of-the-art models for dialogue generation trained on this data (perhaps trained only on the textual part of the data in some cases)? <sep> Second, some measure of accuracy for recall of the associative image context vector should be reported; for example, on average, how close (cosine similarity or some other appropriate measure) is the associatively recalled image context vector to the target image context vector?  On average?  Best case?  Worst case?  How often is this associative vector closer to a confounding image vector than an appropriate one? <sep> A third natural kind of validation would be some form of study employing human subjects to test it's quality as a generator of dialogue. <sep> One thing to note, the example of learning to associate the snowy image with the text about university entrance exams demonstrates that the model is memorizing rather than generalizing.  In general, this is a false association (that is, in general, there is no reason that snow should be associated with exams on the 14th and 15th—the month is not mentioned, which might justify such an association.) <sep> Another thought: did you try not retraining the decoder and attention mechanisms for step 3?  In theory, if step 2 is successful, the retraining should not be necessary.  To the extent that it is necessary, step 2 has failed to accurately predict visual context from text.  This seems like an interesting avenue to explore (and is obviously related to the second type of validation suggested above).  Also, in addition to the baseline model, it seems like it would be good to compare a model that uses actual visual input and the model of step 1 against the model of step 3 (possibly bot retrained and not retrained) to see the effect on the outputs generated—how well do each of these do at predicting the next sentence on both training and test sets? <sep> Other concerns: <sep> 1. The paper is too long by almost a page in main content. <sep> 2. The paper exhibits significant English grammar and usage issues and should be carefully proofed by a native speaker. <sep> 3. There are lots of undefined variables in the Eqs. (s, W_s, W_c, b_s, e_t,i, etc.)  Given the context and associated discussion, it is almost possible to sort out what all of them mean, but brief careful definitions should be given for clarity. <sep> 4. Using news broadcasts as a substitute for true dialogue data seems kind of problematic, though I see why it was done.","None of the reviewers are enthusiastic about the paper, primarily due to lack of proper evaluation. The response of the authors towards this criticism is also not sufficient. The final results are mixed which does not show very clearly that the presented associative model performs better than the sole seq2seq baseline that the authors use for comparison. We think that addressing these immediate concerns would improve the quality of this paper."
"strength | weakness | rebuttal_process  ==> Summary: <sep> The paper presents a generic dynamic architecture for CLEVR VQA and Reverse Polish notation problems. Experiments on CLEVR show that the proposed model DDRprog outperforms existing models, but it requires explicit program supervision. The proposed architecture for RPN, called DDRstack outperforms an LSTM baseline. <sep> Strengths: <sep> — For CLEVR VQA task, the proposed model outperforms the state-of-the-art with significantly less number of parameters. <sep> — For RPN task, the proposed model outperforms baseline LSTM model by a large margin. <sep> Weaknesses: <sep> — The paper doesn't describe the model clearly. After reading the paper, it's not clear to me what the components of the model are, what each of them take as input and produce as output, what these modules do and how they are combined. I would recommend to restructure the paper to clearly mention each of the components, describe them individually and then explain how they are combined for both cases - DDRprog and DDRstack. <sep> — Is the ""fork"" module the main contribution of the paper? If so, at least this should be described in detail. So, if no fork module is required for a question, the model architecture is effectively same as IEP? <sep> — Machine accuracy is already par with human accuracy on CLEVR and very close to 100%. Why is this problem still important? <sep> — Given that the performance of state-on-art on CLEVR dataset is already very high ( <5% error) and the performance numbers of the proposed model are not very far from the previous models, it is very important to report the variance in accuracies along with the mean accuracies to determine if the performance of the proposed model is statistically significantly better than the previous models or not. <sep> — In Figure 4, why are the LSTM32/128 curves different for Length 10 and Length 30 till subproblem index 10? They are both trained on the same training data, only test data is of different length and ideally both models should achieve similar accuracy for the first 10 subproblems (same trend as DDRstack). <sep> — Why is DDRstack not compared to StackRNN? <sep> — Can the authors provide training time comparison of their model and other/baseline models? Because that is more important than the number of epochs required in training. <sep> — There are only 3 curves (instead of 4) in Figure 3. <sep> — In a number of places, the authors are referring to left and right program branches. What are they? These names have not being defined formally in the paper. <sep> Overall: <sep> I think the research work in the paper is interesting and significant, but given the current presentation and level of detail in the paper, I don't think it will be helpful for the research community. By proper restructuring of paper and adding more details, the paper can be converted to a solid submission.","The reviewers generally agree that the DDRprog method is both novel and interesting, while also seeing merit in outperformance of related methods in the empirical results. However, There were a lot of complaints about the writing quality, the clarity of the exposition, and unclear motivation of some of the work. The reviewers also noted insufficient comparisons and discussions regarding relevant prior art, including recursive NNs, Tree RNNs, IEP, etc. While the authors have made substantial revisions to the manuscript, with several additional pages of exposition, reviewers have not raised their scores or confidence in response."
"abstract | weakness  ==>  ==> Summary: <sep> The paper proposes a model to estimate a non-linear transform of data with labels, trying to increase the discriminative power of the transformation while preserving information. <sep> Quality: <sep> The quality is potentially good but I misunderstood too many things (see below) to emit a confident judgement. <sep> Clarity: <sep> Clarity is poor. The paper is overall difficult to follow (at least for me) for different reasons. First, with 17 pages + references + appendix, the length of the paper is way above the « strongly suggested limit of 8 pages ». Second, there are a number of small typos and the citations are not well formatted (use \\citep instead of \\citet). Third, and more importantly, several concepts are only vaguely formulated (or wrong), and I must say I certainly misunderstood some parts of the manuscript. <sep> A few examples of things that should be clarified: <sep> - p4: « per class c there exists an unknown nonlinear function […] that separates the data samples from different classes in the transformed domain ». What does « separate » mean here? There exists as many functions as there are classes, do you mean that each function separates all classes? Or that when you apply each class function to the elements of its own class, you get a separation between the classes? (which would not be very useful) <sep> - p5: what do you mean exactly by « non-linear thresholding function »? <sep> - p5: « The goal of learning a nonlinear transform (2) is to estimate… »: not sure what you mean by « accurate approximation » in this sentence. <sep> - p5 equation 3: if I understand correctly, you not only want to estimate a single vector of thresholds for all classes, but also want it to be constant. Is there a reason for the second constraint? <sep> - p5 After equation 4, you define different « priors ». The one on z_k seems to be Gaussian; however in equation 4, z_k seems to be the difference between the input and the output of the nonlinear threshold operator. If this is correct, then the nonlinear threshold operator is just the addition of a Gaussian random noise, which is really not a thresholding operator. I suppose I misunderstood something here, some clarification is probably needed. <sep> - p5 before equation 5, you define a conditional distribution of \\tau_c given y_{c,k} ; however if you define a prior on \\tau_c given each point (i.e., each k), how do you define the law of \\tau_c given all points? <sep> - p5 Equation 6: I dont understand what the word « approximation » refers to, and how equation 6 is derived. <sep> -p6 equation 7 : missing exponent in the Gaussian distribution <sep> - p6-7: to combine equations 7-8-9 and obtain 10, I suppose in equation 7 the distributions should be conditioned on A (at least the first one), and in equation 9 I suppose the second line should be removed and the third line should be conditioned on A; otherwise more explanations are needed. <sep> - Lemma 1 is just unreadable. Please at least split equations in several lines. <sep> Originality: <sep> As far as I can tell, the approach is quite original and the results proved are new. <sep> Significance: <sep> The method provides a new way to learn discriminative features; as such it is a variant of several existing methods, which could have some impact if clarity is improved and a public code is provided.","This paper proposes an approach for learning a sparsifying transform via a set of nonlinear transforms at learning time. The presentation needs a lot of work. The original paper was 17 pages long and very difficult to understand. The revised paper is 12 pages long, which is still too long for the content. The paper needs to better distinguish between the major and minor points. It is still too difficult to judge the contribution."
"abstract | weakness  ==> Overall, the writing is very confusing at points and needs some attention to make the paper clearer. I'm not entirely sure the authors understand the material particularly well, as I found some of the arguments and narrative confusing or just incorrect. I don't really see any significant contribution here except ""we had this idea for this model, and it works"". There's no interesting questions being asked about missing modes (and no answers through good experimentation), no insight that might contribute to our understanding of the problem, and no comparison to other models. My guess is this submission was rushed (and perhaps they were just looking for feedback). I like the idea, don't get me wrong: a model that is trainable across multiple GPUs and that distributes generative work is pretty cool, and I want to see this work succeed (after a *lot* more work). But the paper really lacks what I'd consider good science, and I don't see it publishable without significant improvement. <sep> Personally I think you should change the angle from missing modes to parallel training. I don't see any strong guarantees that the model will do what you say it will, especially as beta goes to zero. <sep> Detailed comments <sep> P1 <sep> "", that explicitly approximate data distribution, the approximation of GAN is implicit"" <sep> The wording of this is pretty strange: by ""implicit"", we mean that we only have *samples* from the distribution(s) of interest, but what does it mean for an approximation to be ""implicit""? <sep> From the intro, it doesn't sound like the approach is meant for the ""mode collapse"" problem, but for dealing with missing modes. These are different types of failures for GANs, and while there are many theories for why these happen, to my knowledge there's no such consensus that these issues are the same. <sep> For instance, what is keeping each of the generators from collapsing onto a single value? We often see the model collapse on several different values: why couldn't each of your generators do this? <sep> P2: No, it is incorrect that the KL is what is causing mode collapse, and I think actually you mean ""missing modes"". Arjovsky et al addresses the mode collapse problem, which is just another word for a type of instability in GANs. But this isn't because of ""vanishing gradients"", as the ""proxy loss"" (which you call ""heuristic loss"", this isn't a common term, fyi), which is what GANs are trained on in practice don't vanish, but show some other sorts of instabilities (Arjovsky 2016). That said, other GAN variants without regularization also show collapse *and* missing modes, such as LSGAN and all the f-GAN variants (even the auto encoder variants). <sep> You should also probably cite Che et al 2016 as another model that addressed missing modes. Also, what about ALI, BiGAN, and ALiCE? These also address missing modes (at least they claim to). <sep> I don't understand why you're comparing f-GAN and WGAN convergences: they are addressing different things with GANs: one shows insight into what exactly traditional GANs are doing (solving a dual problem of minimizing an f-divergence) versus addressing stability through using an IPM (though also a dual formulation of the wasserstein). f-GANs ensure neither stability nor non-vanishing gradients. <sep> P3: I like the breakdown of how the memory is organized. <sep> This is for multi-GPU, correct? This needs to be explicitly stated. <sep> P6: <sep> There's a sign error in proof 1 (both in the definition of the reverse KL and when the loss is written out). <sep> Also, the gradient w.r.t. theta magically appears in the second half. <sep> This is a pretty round-about way to arrive at that you're minimizing the reverse KL: I'm pretty sure this can be shown by formulating the second term in f-gan (the one where you sample from the generator), that is f*(T), where f* is the convex conjugate of f = -log <sep> Mixture of Gaussians: common *missing modes* experiment. <sep> So my general comments about the experiments <sep> You need to compare to other models that address missing modes. Overall, many people have shown success with experiments similar to your simple mixture of Gaussians experiments, so in order to show something significant here, you will need to have a more challenging experiments and show a comparison to other models. <sep> The real-world experiments are fairly unconvincing, as you only show MNIST and CIFAR-10 (and MNIST doesn't look very good). Overall, the good inception scores aren't too surprising given the model has several generators for each mode, but I think we need to see a demonstration on better datasets.",The paper aims to address the mode collapse issue in GANs by training multiple generators and forcing them to be diverse. <sep> Reviewers agree that the proposed solution is not novel and has disadvantages such as increased parameters due to multiple generator models. The authors do not provide convincing arguments as to why the proposed approach should work well. The experiments presented also fail to demonstrate this. The results are limited to smaller MNIST and CIFAR10 datasets. Comparisons with approaches that directly address the mode collapse problem are missing.
"abstract | weakness  ==> Summary of paper and review: <sep> The paper presents the instability issue of training GANs for semi-supervised learning. Then, they propose to essentially utilize a wgan for semi-supervised learning. <sep> The novelty of the paper is minor, since similar approaches have been done before. The analysis is poor, the text seems to contain mistakes, and the results don't seem to indicate any advantage or promise of the proposed algorithm. <sep> Detailed comments: <sep> - Unless I'm grossly mistaken the loss function (2) is clearly wrong. There is a cross-entropy term used by Salimans et al. clearly missing. <sep> - As well, if equation (4) is referring to feature matching, the expectation should be inside the norm and not outside (this amounts to matching random specific random fake examples to specific random real examples, an imbalanced form of MMD). <sep> - Theorem 2.1 is an almost literal rewrite of Theorem 2.4 of [1], without proper attribution. Furthermore, Theorem 2.1 is not sufficient to demonstrate existence of this issues. This is why [1] provides an extensive batch of targeted experiments to verify this assumptions. Analogous experiments are clearly missing. A detailed analysis of these assumptions and its implications are missing. <sep> - In section 3, the authors propose a minor variation of the Improved GAN approach by using a wgan on the unsupervised part of the loss. Remarkably similar algorithms (where the two discriminators are two separate heads) to this have been done before (see for example, [2], but other approaches exist after that, see for examples papers citing [2]). <sep> - Theorem 3.1 is a trivial consequence of Theorem 3 from WGAN. <sep> - The experiments leave much to be desired. It is widely known that MNIST is a bad benchmark at this point, and that no signal can be established from a minor success in this dataset. Furthermore, the results in CIFAR don't seem to bring any advantage, considering the .1% difference in accuracy is 1/100 of chance in this dataset. <sep> [1]: Arjovsky & Bottou, Towards Principled Methods for Training Generative Adversarial Networks, *CONF* 2017 <sep> [2]: Mroueh & Sercu, Goel, McGan: Mean and Covariance Feature Matching GAN, ICML 2017",The paper aims to combine Wasserstein GAN with Improved GAN framework for semi-supervised learning. <sep> The reviewers unanimously agree that: <sep> - the paper lacks novelty and such approaches have been tried before. <sep> - the approach does not make sufficient gains over the baselines and stronger baselines are missing. <sep> - the paper is not well written and experimental results are not satisfactory.
"weakness | rebuttal_process | strength | weakness  ==>  ==> This paper presents a method for choosing a subset of examples on which to run a constraint solver in order to solve program synthesis problems. This problem is basically active learning for programming by example, but the considerations are slightly different than in standard active learning. The assumption here is that labels (aka outputs) are easily available for all possible inputs, but we don't want to give a constraint solver all the input-output examples, because it will slow down the solver's execution. <sep> The main baseline technique CEGIS (counterexample-guided inductive synthesis) addresses this problem by starting with a small set of examples, solving a constraint problem to get a hypothesis program, <sep> then looking for ""counterexamples"" where the hypothesis program is incorrect. <sep> This paper instead proposes to learn a surrogate function for choosing which examples to select. The paper isn't presented in exactly these terms, but the idea is to consider a uniform distribution over programs and a zero-one likelihood for input-output examples (so observations of I/O examples just eliminate inconsistent programs). We can then compute a posterior distribution over programs and form a predictive distribution over the output for all the remaining possible inputs. The paper suggests always adding the I/O example that is least likely under this predictive distribution <sep> (i.e., the one that is most ""surprising""). <sep> Forming the predictive distribution explicitly is intractable, so the paper suggests training a neural net to map from a subset of inputs to the predictive distribution over outputs.  Results show that the approach is a bit faster than CEGIS in a synthetic drawing domain. <sep> The paper starts off strong. There is a start at an interesting idea here, and I appreciate the thorough treatment of the background, including CEGIS and submodularity as a motivation for doing greedy active learning, although I'd also appreciate a discussion of relationships between this approach and what is done in the active learning literature.Once getting into the details of the proposed approach, <sep> the quality takes a downturn, unfortunately. <sep> Main issues: <sep> - It's not generally scalable to build a neural network whose size scales with the number of possible inputs. I can't see how this approach would be tractable in more standard program synthesis domains where inputs might be lists of arrays or strings, for example.  It seems that this approach only works due to the peculiarities of the formulation of the only task that is considered, <sep> in which the program maps a pixel location in 32x32 images to a binary value. <sep> - It's odd to write ""we do not suggest a specific neural network architecture for the middle layers, one should seelect whichever architecture that is appropriate for the domain at hand."" Not only is it impossible to reproduce a paper without any architectural details, but the result is then that Fig 3 essentially says inputs -> ""magic"" -> outputs. Given that I don't even think the representation of inputs and outputs is practical in general, I don't see what the contribution is here. <sep> - This paper is poor in the reproducibility category. The architecture is never described, <sep> it is light on details of the training objective, it's not entirely clear what the DSL used in the experiments is (is Figure 1 the DSL used in experiments), and it's not totally clear how the random images were generated (I assume values for the holes in Figure 1 were sampled from some distribution, and then the program was executed to generate the data?). <sep> - Experiments are only presented in one domain, and it has some peculiarities relative to more standard program synthesis tasks (e.g., it's tractable to enumerate all possible inputs).  It'd be stronger if the approach could also be demonstrated in another domain. <sep> - Technical point: it's not clear to me that the training procedure as described is consistent with the desired objective in sec 3.3. Question for the authors: in the limit of infinite training data and model capacity, will the neural network training lead to a model that will reproduce the probabilities in 3.3? <sep> Typos: <sep> - The paper needs a cleanup pass for grammar, typos, and remnants like ""Figure blah shows our neural network architecture"" on page 5. <sep> Overall: There's the start of an interesting idea here, but I don't think the quality is high enough to warrant publication at this time.","The reviewers were largely agreed that the paper presented an interesting idea and has potential but needs a better empirical evaluation. It seems that the authors largely agree and are working to improve it. <sep> PROS: <sep> 1. Improving the speed of program synthesis is a useful problem <sep> 2. Good treatment of related work, e.g. CEGIS <sep> CONS: <sep> 1. The approach likely does not scale <sep> 2. The architecture is underspecified making it hard to reproduce <sep> 3. Only 1 domain for evaluation"
"weakness | decision  ==>  ==> This paper shows that an idealized version of stochastic gradient descent converges when learning autoencoders with ReLu non-linearities under strong sparsity assumptions. Convergence rates are also determined. The result is another one in the emerging line of proving convergence guarantees for non-convex optimization problems arising in machine learning, and aims to explain certain phenomena experienced in practice. <sep> The paper is generally nicely written, providing intuitions, but there are several typos (both in the text and in the math, e.g., missing indices), which should also be corrected. <sep> On the negative side, while the proof technique in general looks plausible, there seem to be some mistakes in the derivations, which must be corrected before the paper can be accepted. Also, the assumptions in the in the paper seem quite restrictive, and their implications are not discussed thoroughly. <sep> The assumptions are the following: <sep> 1. The input data is coming from a mixture distribution, in the form x=w_I + eps, where {w_1,...,w_k} is a collection of unit vectors, I is uniform in {1,...,K}, eps is some noise (independent for each sample). <sep> 2. The maximum norm of the noise is O(1/k). <sep> 3. The number n of hidden neurons in the autoencoder is Omega(k) (this is not explicitly assumed but is necessary to make the probability of ""incorrect"" initialization small as well as the results to hold). <sep> Under these assumptions it is shown that the weights of the autoencoder converge to the centers {w_1,...,w_k} (i.e., for any i the autoencoder has at least one weight converging to w_i). The rate of convergence depends on the coherence of the vectors w_i: the less coherent they are the faster the convergence is. <sep> First notice that some assumptions are missing from the main statement, as the error probability delta is certainly connected to the probability of incorrect initialization: when n=1<k, the convergence result clearly cannot hold. This comes from the mistake that in Theorem 3 you state the bound for the probability P(F^\\infty) instead of the conditional probability P(F^\\infty|E_o) (this is present everywhere in the proof). Theorem 3 should also depend on delta_o, which is used in the definition of F^\\infty. <sep> Theorem 2 also seems incorrect. Intuitively, the question is why it cannot happen that two neurons contribute to reproducing a given w_i, and so neither of their weights converge to w_i: E.g., assuming that {w_1,...,w_k,w_1',...,w_k'} form an orthogonal system and the noise is 0, the weight matrix of size n=2k defined as W_{2i-1,*}^T = 1/sqrt{2}(w_i + w'_i) and W_{2i,*}^T=1/sqrt{2}(w_i - w'_i), i \\in [k], with 0 bias can exactly recover any x=w_i (indeed, W_{2j-1,*} x= W_{2j,*} x = 1/sqrt{2}, while the other products are 0, and so W^T W x = W^T W w_j = 1/sqrt{2}(W_{2j-1,*}+W_{2j,*})^T = w_j). Then SGD does not change the weights and hence cannot recover the original weights {w_i }, in particular, it cannot increase the coherence in any step, contradicting Theorem 2. This counterexample can be extended even to the situation when k>d, as--in fact--we only need that the existence of a single j such that w_j and w'_j are orthogonal and also orthogonal to the other basis vectors. <sep> The assumptions are also very strange in the sense that the norm of the noise is bounded by O(1/k), thus the more modes the input distribution has the more separable they become. What motivates this scaling? Furthermore, the parameters of the algorithm for which the convergence is claimed heavily depend on the problem parameters, which are not known. How can you instantiate the algorithm then (accepting the ideal definition of b)? What are the consequences? <sep> Given the above, at this point I cannot recommend the paper for acceptance. However, if the above problems are resolved, I would be very happy to see the paper at the conference. <sep> Other comments <sep> ----------------------- <sep> - Add a short derivation why the weights of the autoencoder should converge to the w_i. <sep> - Definition 3: C_j is not defined in the main text. <sep> - While it is mentioned multiple times that the interesting regime is d<n, this is actually never used, nor needed (personally, I have never seen such an autoencoder--please give some references). What is really needed is n>k, which is natural if one wants to preserve the information, and also k>d for a rich family of distributions. <sep> - The area of the spherical cap is well understood (up to multiplicative constants), and better bounds than yours are readily available: with a cap of height 1-t, for sqrt{2/d}<t<1, the relative surface of the cap is between P/6 and P/2 where <sep> P=1/(t \\sqrt{d}) (1-t^2)^{(d-1)/2}; see, e.g., A. Brieden, P. Gritzmann, R. Kannan, V. Klee, L. Lovasz, and M. Simonovits. Deterministic and randomized polynomial-time approximation of radii. Mathematika. A Journal of Pure and Applied Mathematics, 48(1-2):63–105, 2001. <sep> - The notation section should be brought forward (or referred the fist time the notation is actually used). <sep> - Instead of unit spherical Gaussian you could simply say uniform distribution on the unit sphere <sep> - While Algorithm 1 is called ""norm-controlled SGD training,"" it does not control the norm at all.","The reviewers have unanimously expressed strong concerns about the technical correctness of the theoretical results in the paper. The paper should be carefully revised and checked for technical errors. In its current form, the paper is not suitable for acceptance at *CONF* 2018."
"rating_summary | weakness  ==> 1. Paper summary <sep> This paper describes a technique using 3 neural networks to privatize data and make predictions: a feature extraction network, an image classification network, and an image reconstruction network. The idea is to learn a feature extraction network so that the image classification network performs well and the image reconstruction network performs poorly. <sep> 2. High level paper - subjective <sep> I think the presentation of the paper is somewhat scattered: In section 2 the authors introduce their network and their metric for utility and privacy and then immediately do a sensitivity analysis. Section 3 continues with a sensitivity analysis now considering performance and storage of the method. Then 2.5 pages are spent on channel pruning. <sep> I would have liked if the authors spent more time justifying why we should trust their method as a privacy preserving technique (described in detail below). <sep> The authors clearly performed an impressive amount of sensitivity experiments. Assuming the privacy claims are reasonable (which I have some doubts about below) then this paper is clearly useful to any company wanting to do privacy preserving classification. At the same time I think the paper does not have a significant amount of machine learning novelty in it. <sep> 3. High level technical <sep> I have a few doubts about this method as a privacy-preserving technique: <sep> - Nearly every privacy-preserving technique gives a guarantee, e.g., differential privacy guarantees a statistical notion of privacy and cryptographic methods guarantee a computational notion of privacy. In this work the authors provide a way to measure privacy but there is no guarantee that if someone uses this method their data will be private, by some definition, even under certain assumptions. <sep> - Another nice thing about differential privacy and cryptography is that they are impervious to different algorithms because it is statistically hard or computationally hard to reveal sensitive information. Here there could be a better image reconstruction network that does a better job of reconstructing images than the ones used in the paper. <sep> - It's not clear to my why PSNR is a useful way to measure privacy loss. I understand that it is a metric to compare two images that is based on the mean-squared error so a very private image should have a low PSNR while a not private image should have a high PSNR, but I have no intuition about how small the PSNR should be to afford a useful amount of privacy. For instances, in nearly all of the images of Figures 21 and 22 I think it would be quite easy to guess the original images. <sep> 4. 1/2 sentence summary <sep> While the authors did an extensive job evaluating different settings of their technique I have serious doubts about it as a privacy-preserving method.",Reviews are marginal. <sep> I concur with the two less-favorable reviews that the metrics for privacy protection are not sufficiently strong for preserving privacy.
"abstract | weakness | rebuttal_process  ==> The ambition of this paper is to address multi-view object recognition and the associated navigation as a unified reinforcement learning problem using a deep CNN to represent the policy. <sep> Multi-view recognition and active viewpoint selection have been studied for more than 30 years, but this paper ignores most of this history.  The discussion of related work as well as the empirical evaluation are limited to very recent methods using neural networks.  I encourage the authors to look e.g. at Paletta and Pinz [1] (who solve a very similar and arguably harder problem in related ways) and at Bowyer & Dyer [2] as well as the references contained in these papers for history and context.  Active vision goes back to Bajcsy, Aloimonos, and Ballard; these should be cited instead of Ammirato et al.  Conversely, the related work cites a handful of papers (e.g. in the context of Atari 2600 games) that are unrelated to this work. <sep> The navigation aspect is limited to fixed-size left or right displacements (at least for ModelNet40 task which is the only one to be evaluated and discussed).  This is strictly weaker than active viewpoint selection.  Adding this to the disregard of prior work, it is (at best) misleading to claim that this is ""the first framework to combine learning of navigation and object recognition"". <sep> Calling this ""multi-task"" learning is also misleading.  There is only one ultimate objective (object recognition), while the agent has two types of actions available (moving or terminating with a classification decision). <sep> There are other misleading, vague, or inaccurate statements in the paper, for example: <sep> - ""With the introduction of deep learning to reinforcement learning, there has been ... advancements in understanding ... how humans navigate"": I don't think such a link exists; if it does, a citation needs to be provided. <sep> - ""inductive bias like image pairs"": Image pairs do not constitute inductive bias.  Either the term is misused or the wording must be clarified; likewise for other occurrences of ""inductive bias"". <sep> - ""a single softmax layer is biased towards tasks with larger number of actions"": I think I understand what this is intended to say, but a ""softmax layer"" cannot be ""biased towards tasks"" as there is only one, given, task. <sep> - I do not understand what the stated contribution of ""extrapolation of the action space to a higher dimension for multi-task learning"" is meant to be. <sep> - ""Our method performs better ... than state-of-the-art in training for navigation to the object"": The method does not involve ""navigation to the object"", at least not for the ModelNet40 dataset, the only for which results are given. <sep> It is not clear what objective function the system is intended to optimize.  Since the stated task is object recognition and from Table 2 I was expecting it to be the misclassification rate, but this is clearly not the case, as the system is not set up to minimize it.  What ""biases"" the system towards classification actions (p. 5)?  Why is it bad if the agent shows ""minimal movement actions"" as long as the misclassification rate is minimized? No results are given to show whether this is the case or not.  The text then claims that the ""hierarchical method gives superior results"", but this is not shown either. <sep> Table 3 reveals that the system fails to learn much of interest at all.  Much of the time the agent chooses not to move and performs relatively poorly; taking more steps improves the results; often all 12 views are collected before a classification decision is made.  Two of the most important questions remain open: (1) What would be the misclassification rate if all views are always used? (2) What would be the misclassification rate under a random baseline policy not involving navigation learning (e.g., taking a random number of steps in the same direction)? <sep> Experiments using the THOR dataset are announced but are left underspecified (e.g., the movement actions), but no results or discussion are given. <sep> SUMMARY <sep> Quality: lacking in may ways; see above. <sep> Clarity: Most of the paper is clear enough, but there are confusions and missing information about THOR and problems with phrasing and terminology.  Moreover, there are many grammatical and typographical glitches. <sep> Originality: Harder tasks have been looked at before (using methods other than CNN).  Solving a simpler version using CNN I do not consider original unless there is a compelling pay-off, which this paper does not provide. <sep> Significance: Low. <sep> Pros: The problem would be very interesting and relevant if it was formulated in a more ambitious way (e.g., a more elaborate action space than that used for ModelNet40) with a clear objective function, <sep> Cons: See above. <sep> [1] Lucas Paletta and Axel Pinz, Active object recognition by view integration and reinforcement learning, Robotics and Autonomous Systems 31, 71-86, 2000 <sep> [2] Bowyer, K. W. and Dyer, C. R. (1990), Aspect graphs: An introduction and survey of recent results. Int. J. Imaging Syst. Technol., 2: 315–328. doi:10.1002/ima.1850020407","This paper describes active vision for object recognition learned in an RL framework. <sep> Reviewers think the paper is not of sufficient quality: Insufficient detail, and insufficient evaluation. <sep> While the authors have provided a lengthy rebuttal, the shortcomings have not yet been addressed in the paper."
"rating_summary | weakness | rebuttal_process  ==> This paper introduces a machine learning adaptation of the active inference framework proposed by Friston (2010), and applies it to the task of image classification on MNIST through a foveated inspection of images. It describes a cognitive architecture for the same, and provide analyses in terms of processing compression and ""confirmation biases"" in the model. <sep> – Active perception, and more specifically recognition through saccades (or viewpoint selection) is an interesting biologically-inspired approach and seems like an intuitive and promising way to improve efficiency. The problem and its potential applications are well motivated. <sep> – The perception-driven control formulation is well-detailed and simple to follow. <sep> – The achieved compression rates are significant and impressive, though additional demonstration of performance on more challenging datasets would have been more compelling <sep> Questions and comments: <sep> – While an 85% compression rate is significant, 88% accuracy on MNIST seems poor. A plot demonstrating the tradeoff of accuracy for compression (by varying Href or other parameters) would provide a more complete picture of performance. Knowing baseline performance (without active inference) would help put numbers in perspective by providing a performance bound due to modeling choices. <sep> – What does the distribution of number of saccades required per recognition (for a given threshold) look like over the entire dataset, i.e. how many are dead-easy vs difficult? <sep> – Steady state assumption: How can this be relaxed to further generalize to non-static scenes? <sep> – Figure 3 is low resolution and difficult to read. <sep> Post-rebuttal comments: <sep> I have revised my score after considering comments from other reviewers and the revised paper. While the revised version contains more experimental details, the paper in its present form lacks comparisons to other gaze selection and saliency models which are required to put results in context. The paper also contains grammatical errors and is somewhat difficult to understand. Finally, while it proposes an interesting formulation of a well-studied problem, more comparisons and analysis are required to validate the approach.","All 3 reviewers consider the paper insufficiently good, including a post-rebuttal updated score. <sep> All reviewers + anonymous comment find that the paper isn't well-enough situated with the appropriate literature. <sep> Two reviewers cite poor presentation - spelling /grammar errors making hte paper hard to read. <sep> Authors have revised the paper and promise further revisions for final version."
"abstract | weakness | decision  ==> Summary <sep> The authors propose a hierarchical generative model with both continuous and discrete latent variables. The authors empirically demonstrate that the latent space of their model separates well healthy vs pathological cells in a dataset for Chronic lymphocytic leukemia (CLL) diagnostics. <sep> Main <sep> Overall the paper is reasonably well written. There are a few clarity issues detailed below. <sep> The results seem very promising as the model clearly separates the two types of cells. But more baseline experiments are needed to assess the robustness of the results. <sep> Novelty <sep> The model introduced is a variant of a deep latent Gaussian model, where the top-most layer is a discrete random variable. Furthermore, the authors employ the Gumbel-trick to avoid having to explicitly marginalize the discrete latent variables. <sep> Given the extensive literature on combining discrete and continuous latent variables in VAEs, the novelty factor of the proposed model is quite weak. <sep> The authors use the Gumbel-trick in order to avoid explicit marginalization over the discrete variables. However, the number of categories in their problem is small (n=2), so the computational overhead of an explicit marginalization would be negligible. The result would be equivalent to replacing the top of the model p(y) p(z_L|y) by a GMM p_{GMM}(z_L) with two Gaussian components only. <sep> Give these observations, it seems that this is an unnecessary complication added to the model as an effort to increase novelty. <sep> It would be very informative to compare both approaches. <sep> I would perhaps recommend this paper for an applied workshop, but not for publication in a main conference. <sep> Details: <sep> 1) Variable h was not defined before it appeared in Eq. (5). From the text/equations we can deduce h = (y, z_1, …, z_L), but this should be more clearly stated. <sep> 2) It is counter-intuitive to define the inference model before having defined the generative model structure, perhaps the authors should consider changing the presentation order. <sep> 3) Was the VAE in VAE+SVM also trained with lambda-annealing? <sep> 4) How does a simple MLP classifier compares to the models on Table 1 and 2? <sep> 5) It seems that, what is called beta-VAE here is the same model HCDVAE but trained with a lambda that anneals to a value different than one (the value of beta). In this case what is the value it terminates? How was that value chosen? <sep> 6) The authors used 3 stochastic layers, how was that decided? Is there a substantial difference in performance compared to 1 and 2 stochastic layers? <sep> 7) How do the different models behave in terms train vs test set likelihoods. Was there overfitting detected for some settings? How does the choice of the MCC threshold affects train/test likelihoods? <sep> 8) Have the authors compared explicit marginalizing y with using the Gumbel-trick? <sep> Other related work: <sep> A few other papers that have explored discrete latent variables as a way to build more structured VAEs are worth mentioning/referring to: <sep> [1] Dilokthanakul N, Mediano PA, Garnelo M, Lee MC, Salimbeni H, Arulkumaran K, Shanahan M. Deep unsupervised clustering with gaussian mixture variational autoencoders. arXiv preprint arXiv:1611.02648. 2016 Nov 8. <sep> [2] Goyal P, Hu Z, Liang X, Wang C, Xing E. Nonparametric Variational Auto-encoders for Hierarchical Representation Learning. arXiv preprint arXiv:1703.07027. 2017 Mar 21.","The authors propose a hierarchical VAE model with a discrete latent variable in the top-most layer for unsupervised learning of discriminative representations. While the reported results on the two flow cytometry datasets are encouraging, they are insufficient to draw strong conclusions about the general effectiveness of the proposed architecture. Also, as two of the reviewers stated the proposed model is very similar to several VAE models in the literature. This paper seems better suited for a more applied venue than *CONF*."
"abstract | weakness | misc | decision  ==> The paper proposes a novel way of causal inference in situations where in causal SEM notation the outcome Y = f(T,X) is a function of a treatment T and covariates X. The goal is to infer the treatment effect E(Y|T=1,X=x) - E(Y|T=0,X=x) for binary treatments at every location x. If the treatment effect can be learned, then forecasts of Y under new policies that assign treatment conditional on X will still ""work"" and the distribution of X can also change without affecting the accuracy of the predictions. <sep> What is proposed seems to be twofold: <sep> - instead of using a standard inverse probability weighting, the authors construct a bound for the prediction performance under new distributions of X and new policies and learn the weights by optimizing this bound. The goal is to avoid issues that arise if the ratio between source and target densities become very large or small and the weights in a standard approach would become very sparse, thus leading to a small effective sample size. <sep> - as an additional ingredient the authors also propose ""representation learning"" by mapping x to some representation Phi(x). <sep> The goal is to learn the mapping Phi (and its inverse) and the weighting function simultaneously by optimizing the derived bound on the prediction performance. <sep> Pros: <sep> - The problem is relevant and also appears in similar form in domain adaptation and transfer learning. <sep> - The derived bounds and procedures are interesting and nontrivial, even if there is some overlap with earlier work of Shalit et al. <sep> Cons: <sep> - I am not sure if *CONF* is the optimal venue for this manuscript but will leave this decision to others. <sep> - The manuscript is written in a very compact style and I wish some passages would have been explained in more depth and detail. Especially the second half of page 5 is at times very hard to understand as it is so dense. <sep> - The implications of the assumptions in Theorem 1 are not easy to understand, especially relating to the quantities B_\\Phi, C^\\mathcal{F}_{n,\\delta} and D^{\\Phi,\\mathcal{H}}_\\delta. Why would we expect these quantities to be small or bounded? How does that compare to the assumptions needed for standard inverse probability weighting? <sep> - I appreciate that it is difficult to find good test datasets for evaluating causal estimator.  The experiment on the semi-synthetic IHDP dataset is ok, even though there is very little information about its structure in the manuscript (even basic information like number of instances or dimensions seems missing?). The example does not provide much insight into the main ideas and when we would expect the procedure to work more generally.","The submission provides an interesting way to tackle the so-called distributional shift problem in machine learning. One familiar example is unsupervised domain adaptation. The main contribution of this work is deriving a bound on the generalization error/risk for a target domain as a combo of re-weighted empirical risk on the source domain and some discrepancy between the re-weighted source domain and the target domain. The authors then use this to formulate an objective function. <sep> The reviewers generally liked the paper for its theoretical results, but found the empirical evaluation somewhat lacking, as do I. Especially the unsupervised domain adaptation results are very toy-ish in nature (synthetic data), whereas the literature in this field, cited by the authors, does significantly larger scale experiments. I am unsure as to how much I value I can place in the IHDP results since I am not familiar with the benchmark (and hence my lower confidence in the recommendation). <sep> Finally, I am not very convinced that this is the appropriate venue for this work, despite containing some interesting results."
"abstract | weakness | suggestion  ==> Summary: <sep> This paper proposes to use a GAN to generate goals to implement a form of curriculum learning. A goal is defined as a subset of the state space. The authors claim that this model can discover all ""goals"" in the environment and their 'difficulty', which can be measured by the success rate / reward of the policy. Hence the goal network could learn a form of curriculum, where a goal is 'good' if it is a state that the policy can reach after a (small) improvement of the current policy. <sep> Training the goal GAN is done via labels, which are states together with the achieved reward by the policy that is being learned. <sep> The benchmark problems are whether the GAN generates goals that allow the agent to reach the end of a U-maze, and a point-mass task. <sep> Authors compare GAN goal generation vs uniformly choosing a goal and 2 other methods. <sep> My overall impression is that this work addresses an interesting question, but the experimental setup / results are not clearly worked out. More broadly, the paper does not address how one can combine RL and training a goal GAN in a stable way. <sep> Pro: <sep> - Developing hierarchical learning methods to improve the sample complexity of RL is an important problem. <sep> - The paper shows that the U-maze can be 'solved' using a variety of methods that generate goals in a non-uniform way. <sep> Con: <sep> - It is not clear to me how the asymmetric self-play and SAGG-RIAC are implemented and why they are natural baselines. <sep> - It is not clear to me what the 'goals' are in the point mass experiment. This entire experiment should be explained much more clearly (+image). <sep> - It is not clear how this method compares qualitatively vs baselines (differences in goals etc). <sep> - This method doesn't seem to always outperform the asymm-selfplay baseline. The text mentions that baseline is less efficient, but this doesn't make the graph very interpretable. <sep> - The curriculum in the maze-case consists of regions that just progress along the maze, and hence is a 1-dimensional space. Hence using a manually defined set of goals should work quite well. It would be better to include such a baseline as well. <sep> - The experimental maze-setting and point-mass have a simple state / goal structure. How can this method generalize to harder problems? <sep> -- The entire method is quite complicated (e.g. training GANs can be highly unstable). How do we stabilize / balance training the GAN vs the RL problem? <sep> -- I don't see how this method could generalize to problems where the goals / subregions of space do not have a simple distribution as in the maze problem, e.g. if there are multiple ways of navigating a maze towards some final goal state. In that case, to discover a good solution, the generated goals should focus on one alternative and hence the GAN should have a unimodal distribution. How do you force the GAN in a principled way to focus on one goal in this case? How could you combine RL and training the GAN stably in that case? <sep> Detailed: <sep> - (2) is a bit strange: shouldn't the indicator say: 1( \\exists t: s_t \\in S^g )? Surely not all states in the rollout (s_0 ... s_t) are in the goal subspace: the indicator does not factorize over the union. Same for other formulas that use \\union. <sep> - Are goals overlapping or non-overlapping subsets of the state space? <sep> Definition around (1) basically says it's non-overlapping, yet the goal GAN seems to predict goals in a 2d space, hence the predicted goals are overlapping? <sep> - What are the goals that the non-uniform baselines predict? Does the GAN produce better goals? <sep> - Generating goal labels is <sep> - Paper should discuss literature on hierarchical methods that use goals learned from data and via variational methods: <sep> 1. Strategic Attentive Writer (STRAW), V. Mnih et al, NIPS 2016 <sep> 2. Generating Long-term Trajectories Using Deep Hierarchical Networks. S. <sep> Zheng et al, NIPS 2016","In principle, the idea behind the submission is sound: use a generative model (GANs in this case) to learn to generate desirable ""goals"" (subsets of the state space) and use that instead of uniform sampling for goals. Overall I tend to agree with Reviewer 3 in that the current set of results is not convincing in terms of it being able to generate goals in a high-dimensional state space, which seems to be be whole raison d'etre of GANs in this proposed method. The coverage experiment in Figure 5 seems like a good *illustration* of the method, but for this work to be convincing, I think we would need a more diverse set of experiments (a la Figure 2) showing how this method performs on complicated tasks. <sep> I encourage the authors to sharpen the definitions, as suggested by reviewers, and, if possible, provide experiments where the Assumptions being made in Section 3.3 are *violated* somehow (to actually test how the method fails in those cases)."
"abstract | weakness | decision  ==> The paper presents a new parameterization of linear maps for use in neural networks, based on the Multiscale Entanglement Renormalization Ansatz (MERA). The basic idea is to use a hierarchical factorization of the linear map, that greatly reduces the number of parameters while still allowing for relatively complex interactions between variables to be modelled. A limited number of experiments on CIFAR10 suggests that the method may work a bit better than related factorizations. <sep> The paper contains interesting new ideas and is generally well written. However, a few things are not fully explained, and the experiments are too limited to be convincing. <sep> Exposition <sep> On a first reading, it is initially unclear why we are talking about higher order tensors at all. Usually, fully connected layers are written as matrix-vector multiplications. It is only on the bottom of page 3 that it is explained that we will reshape the input to a rank-k (k=12) tensor before applying the MERA factored map. It would be helpful to state this sooner. It would also be nice to state that (in the absense of any factorization of the weight tensor) a linear contraction of such a high-rank tensor is no less general than a matrix-vector multiplication. <sep> Most ML researchers will not know Haar measure. It would be more reader friendly to say something like ""uniform distribution over orthogonal matrices (i.e. Haar measure)"" or something like that. Explaining how to sample orthogonal matrices / tensors (e.g. by SVD) would be helpful as well. <sep> The article does not explain what ""disentanglers"" are. It is very important to explain this, because it will not be generally known by the machine learning audience, and is the main thing that distinguishes this work form earlier tree-based factorizations. <sep> On page 5 it is explained that the computational complexity of the proposed method is N^{log_2 D}. For D=2, this is better than a fully connected layer. Although this theoretical speedup may not currently have been realized, it perhaps could be achieved by a custom GPU kernel. It would be nice to highlight this potential benefit in the introduction. <sep> Theoretical motivation <sep> Although I find the theoretical motivation for the method somewhat compelling, some questions remain that the authors may want to address. For one thing, the paper talks about exploiting ""hierarchical / multiscale structure"", but this does not refer to the spatial multi-scale structure that is naturally present in images. Instead, the dimensions of a hidden activation vector are arbitrarily ordered, partitioned into pairs, and reshaped into a (2, 2, ..., 2) shape tensor. The pairing of dimensions determines the kinds of interactions the MERA layer can express. Although the earlier layers could learn to produce a representation that can be effectively analyzed by the MERA layer, one is left to wonder if the method could be made to exploit the spatial multi-scale structure that we know is actually present in image data. <sep> Another point is that although from a classical statistics perspective it would seem that reducing the number of parameters should be generally beneficial, it has been observed many times that in deep learning, highly overparameterized models are easier to optimize and do not necessarily overfit. Thus at this point it is not clear whether starting with a highly constrained parameterization would allow us to obtain state of the art accuracy levels, or whether it is better to start with an overparameterized model and gradually constrain it or perform a post-training compression step. <sep> Experiments <sep> In the introduction it is claimed that the method of Liu et al. cannot capture correlations on different length scales because it lacks disentanglers. Although this may be theoretically correct, the paper does not experimentally verify that the proposed factorization with disentanglers outperforms a similar approach without disentanglers. In my opinion this is a critical omission, because the addition of disentanglers seems to be the main or perhaps only difference to previous work. <sep> The experiments show that MERA can drastically reduce the number of parameters of fully connected layers with only a modest drop in accuracy, for a particular ConvNet trained on CIFAR10. Unfortunately this ConvNet is far from state of the art, so it is not clear if the method would also work for better architectures. Furthermore, training deep nets can be tricky, and so the poor performance makes it impossible to tell if the baseline is (unintentionally) crippled. <sep> Comparing MERA-2 to TT-3 or MERA-3 to TT-5 (which have an approximately equal number of parameters), the difference in accuracy appears to be less than 1 percentage point. Since only a handful of specific MERA / TT architectures were compared on a single dataset, it is not at all clear that we can expect MERA to outperform TT in many situations. In fact, it is not even clear that the small difference observed is stable under random retraining. <sep> Summary <sep> An interesting paper with novel theoretical ideas, but insufficient experimental validation. Some expository issues need to be fixed.","This paper proposes a tree-structured tensor factorisation method for parameter reduction. The reviewers felt the paper was somewhat interesting, but agreed that more detail was needed in the method description, and that the experiments were on the whole uninformative. This seems like a promising research direction which needs more empirical work, but is not ready for publication as is."
"abstract | weakness | strength | weakness | strength | weakness  ==> Many black-box optimization problems are ""multi-fidelity"", in which it is possible to acquire data with different levels of cost and associated uncertainty.  The training of machine learning models is a common example, in which more data and/or more training may lead to more precise measurements of the quality of a hyperparameter configuration.  This has previously been referred to as a special case of ""multi-task"" Bayesian optimization, in which the tasks can be constructed to reflect different fidelities.  The present paper examines this construction with three twists: using the knowledge gradient acquisition function, using batched function evaluations, and incorporating derivative observations.  Broadly speaking, the idea is to allow fidelity to be represented as a point in a hypercube and then include this hypercube as a covariate in the Gaussian process.  The knowledge gradient acquisition function then becomes ""knowledge gradient per unit cost"" the KG equivalent to the ""expected improvement per unit cost"" discussed in Snoek et al (2012), although that paper did not consider treating fidelity separately. <sep> I don't understand the claim that this is ""the first multi-fidelity algorithm that can leverage gradients"".  Can't any Gaussian process model use gradient observations trivially, as discussed in the <sep> Rasmussen and Williams book?  Why can't any EI or entropy search method also use gradient observations?  This doesn't usually come up in hyperparameter optimization, but it seems like a grandiose claim. <sep> Similarly, although I don't know of a paper that explicitly does ""A + <sep> B"" for multi-fidelity BO and parallel BO, it is an incremental contribution to combine them, not least because no other parallel BO <sep> methods get evaluated as baselines. <sep> Figure 1 does not make sense to me.  How can the batched algorithm outperform the sequential algorithm on total cost?  The sequential cfKG algorithm should always be able to make better decisions with its remaining budget than 8-cfKG.  Is the answer that ""cost"" here means <sep> ""wall-clock time when parallelism is available""?  If that's the case, <sep> then it is necessary to include plots of parallelized EI, entropy search, and KG.  The same is true for Figure 2; other parallel BO <sep> algorithms need to appear.","This paper combines multiple existing ideas in Bayesian optimization (continuous-fidelity, use of gradient information and knowledge gradient) to develop their proposed cfKG method. While the methodology seems neat and effective, the reviewers (and AC) found that the presented approach was not quite novel enough in light of existing work to justify acceptance to *CONF*. Continuous fidelity Bayesian optimization is well studied and knowledge gradient + derivative information was presented at NIPS. The combination of these things seems quite sensible but not sufficiently novel (unless the empirical results were *really* compelling). <sep> Pros: <sep> - The paper is clear and writing is of high quality <sep> - Bayesian optimization is interesting to the community and compelling methods are potentially practically impactful <sep> - Outperforms existing methods on the chosen benchmarks <sep> Cons: <sep> - Is an incremental combination of existing methods <sep> - The paper claims too much"
"abstract | strength | weakness | strength | weakness  ==> The RNN transition function is: h_t+1 = f(h_t,x_t) <sep> This paper proposes using a stochastic transition function instead of a deterministic one. <sep> i.e h_{t+1} \\sim expfam(mean = f(h_t,x_t), gamma) where expfam denotes a distribution from the exponential family. <sep> The experimental results consider text modeling (evaluating on perplexity) on Penn Treebank and Wikitext-2. The method of regularization is compared to a reimplementation of Variational Dropout and no regularization. <sep> The work is written clearly and easy to follow. <sep> Overall, the core idea in this work is interesting but underexplored. <sep> * As of when I read this paper, all results on this work used 200 hidden units realizing results that were well off from the state of the art results on Penn Tree Bank (as pointed out by the external reader). <sep> The authors responded by stating that this was done to achieve a relative comparison.  A more interesting comparison, in addition to the ones presented, would be to see how well each method performs while not controlling for hidden layer size. Then, it might be that restricting the number of hidden dimensions is required for the RNN without any regularization but for both Variational Dropout and Noisin, one obtains better results with a larger the hidden dimension. <sep> * The current experimental setup makes it difficult to assess when the proposed regularization is useful. Table 2 suggests the answer is sometimes and Table 3 suggests its marginally useful when the RNN size is restricted. <sep> * How does the proposed method's peformance compare to Zoneout https://arxiv.org/pdf/1606.01305.pdf? <sep> * Clarifying the role of variational inference: I could be missing something but I don't see a good reason why the prior (even if learned) should be close to the true posterior under the model. I fear the bound in Section (3) [please include equation numbers in the paper] could be quite loose. <sep> * What is the rationale for not comparing to the model proposed in [Chung et. al] where there is a stochastic and deterministic component to the transition function? In what situations do we expect the fully stochastic transition here to work better than a model that has both? Presumably, some aspect of the latent variable + RNN model could be expressed by having a small variance for a subset of the dimensions and large one for the others but since gamma is the same across all dimensions of the model, I'm not sure this feature can be incorporated into the current approach. Such a comparison would also empirically verify what happens when learning with the prior versus doing inference with an approximate posterior helps. <sep> * The regularization is motivated from the point of view of sampling the hidden states to be from the exponential family, but all the experiments provided seem to use a Gaussian distribution. This paper would be strengthened by a discussion and experimentation with other kinds of distributions in the exponential family.","This paper proposes a regularizer for recurrent neural networks, based on injecting random noise into the hidden unit activations. In general the reviewers thought that the paper was well written and easy to understand. However, the major concern among the reviewers was a lack of empirical evidence that the method works consistently. Essentially, the reviewers were not compelled by the presented experiments and demanded more rigorous empirical validation of the approach. <sep> Pros: <sep> - Well written and easy to follow <sep> - An interesting idea <sep> - Regularizing RNNs is an interesting and active area of research in the community <sep> Cons: <sep> - The experiments are not compelling and are questioned by all the reviewers <sep> - The writing does not cite relevant related work <sep> - The work seems underexplored (empirically and methodologically)"
"abstract | weakness  ==>  ==> This paper presents an analysis of LSTMS showing that they have a from where the memory cell contents at each step is a weighted combination of the ""content update"" values computed at each time step. The weightings are defined in terms of an exponential decay on each dimension at each time step (given by the forget gate), which lets the cell be computed sequentially in linear time rather than in the exhaustive quadratic time that would apparently be necessary for this definition. Second, the paper offers a simplification of LSTMs that compute the value by which the memory cell at each time step in terms of a deterministic function of the input rather than a function of the input and the current context. This reduced form of the LSTM is shown to perform comparably to ""full"" LSTMs. <sep> The decomposition of the LSTM in terms of these weights is useful, and suggests new strategies for comparing existing quadratic time attention-based extensions to RNNs. The proposed model variations (which replaces the ""content update"" that has a recurrent network in terms of context-independent update) and their evaluations seem rather more arbitrary. First, there are two RNNs present in the LSTM- one controls the gates, one controls the content update. You get rid of one, not the other. You can make an argument for why the one that was ablated was ""more interesting"", but really this is an obvious empirical question that should be addressed. The second problem of what tasks to evaluate on is a general problem with comparing RNNs. One non-language task (e.g., some RL agent with an LSTM, or learning to execute or something) and one synthetic task (copying or something) might be sensible. Although I don't think this is the responsibility of this paper (although something that should be considered). <sep> Finally, there are many further simplifications of LSTMs that could have been explored in the literature: coupled input-forget gates (Greff et al, 2015), diagonal matrices for gates, GRUs. When proposing yet another simplification, some sense for how these different reductions is useful, so I would recommend comparison to those. <sep> Notes on clarity: <sep> Before Eq 1 it's hard to know what the antecedent of ""which"" is without reading ahead. <sep> For componentwise multiplication, you have been using \\circ, but then for the iterated component wise product, \\prod is used. To be consistent, notation like \\odot and \\bigodot might be a bit clearer. <sep> The discussion of dynamic programming: the dynamic program is also only available because the attention pattern is limited in a way that self attention is not. This might be worth mentioning. <sep> When presenting Eq 11, the definition of w_j^t elides a lot of complexity. Indeed, w_j^t is only ever implicitly defined in Eq 8, whereas things like the input and forget gates are defined multiple times in the text. Since w_j^t can be defined iteratively and recursively (as a dynamic program), it's probably worth writing both out, for expository clarity. <sep> Eq 11 might be clearer if you show that Eq 8 can also be rewritten in the same wheat, provided, you make h_{t-1} an argument to output and content. <sep> Table 4 is unclear. In a language model, the figure looks like it is attending to the word that is being generated, which is clearly not what you want to convey since language models don't condition on the word they are predicting. Presumably the strong diagonal attention is attending to the previous word when computing the representation to generate the subsequent word? In any case, this figure should be corrected to reflect this. This objection also concerns the right hand figure, and the semantics of the meaning of the upper vs lower triangles should be clarified in the caption (rather than just in the text).","The paper performs an ablation analysis on LSTM, showing that the gating component is the most important. There is little novelty in the analysis, and in its current form, its impact is rather limited."
"strength | weakness  ==> This paper paper uses an ensemble of networks to represent the uncertainty in deep reinforcement learning. <sep> The algorithm then chooses optimistically over the distribution induced by the ensemble. <sep> This leads to improved learning / exploration, notably better than the similar approach bootstrapped DQN. <sep> There are several things to like about this paper: <sep> - It is a clear paper, with a simple message and experiments that back up the claims. <sep> - The proposed algorithm is simple and could be practical in a lot of settings and even non-DQN variants. <sep> - It is interesting that Bootstrapped DQN gets such poor performance, this suggests that it is very important in the original paper https://arxiv.org/abs/1602.04621 that ""ensemble voting"" is applied to the test evaluation... (why do you think this is by the way, do you think it has something to do with the data being *more* off-policy / diverse under a TS vs UCB scheme?) <sep> On the other hand: <sep> - The novelty/scope of this work is somewhat limited... this is more likely (valuable) incremental work than a game-changer. <sep> - Something feels wrong/hacky/incomplete about just doing ""ensemble"" for uncertainty without bootstrapping/randomization... if we had access to more powerful optimization techniques then this certainly wouldn't be sensible - I think that you should mention that you are heavily reliant on ""random initialization + SGD/Adam + specific network architecture"" to maintain this idea of uncertainty. For example, this wouldn't work for linear value functions! <sep> - I think the original bootstrapped DQN used ""ensemble voting"" at test time, so maybe you should change the labels or the way this is introduced/discussed. It's definitely very interesting that *essentially* the learning benefit is coming from ensembling (rather than ""raw"" bootstrapped DQN) and UCB still looks like it does better. <sep> - I'm not convinced that page 4 and the ""Bayesian"" derivation really add too much value to this paper... alternatively, maybe you could introduce the actual algorithm first (train K models in parallel) and then say ""this is similar to particle filter"" and add the mathematical derivation after, rather than as if it was some complex formula derived. If you want to reference some justification/theory for ensemble-based uncertainty approximation you might consider https://arxiv.org/pdf/1705.07347.pdf instead. <sep> - I think this paper might miss the point of the ""bigger"" problem of efficient exploration in RL... or even how to get ""deep"" exploration with deep RL. Yes this algorithm sees improvements across Atari, but it's not clear why/if this is a step change versus simply increasing the amount of replay or tuning the learning rate.  (Actually I do believe this algorithm can demonstrate deep exploration... but it looks like we're not seeing the big improvements on the ""sub-human"" games you might hope.) <sep> Overall I do think this is a pretty good short paper/evaluation of UCB-ensembles on Atari. <sep> The scope/insight of the paper isn't groundbreaking, but I think it delivers a clear short message on the Atari benchmark. <sep> Perhaps this will encourage people to dig deeper into some of these issues... I vote accept.","The idea studied here is interesting, if incremental. The empirical results are not particularly stellar, but it's clear that the authors have done their best to provide reproducible and defensible results. A few sticking points: a) The use of the term 'UCB', as mentioned in an anonymous comment, is somewhat misleading. ""Approximate Confidence Interval"" might be less controversial; b) there are a number of recent research results on exploration that are worth paying attention to (Plappert et al, O'Donoghue et al.) and worth comparing to, and c) the theoretical results are not always justified or useful (e.g. Equation 9: the bound is trivial, posterior >= 0 or 1)."
"weakness  ==> This paper proposes a new variant of DQN where the DQN targets are computed on a full episode by a « backward » update (i.e. from end to start of episode). The targets' update rule is similar to a regular tabular Q-learning update with high learning rate beta: this allows faster propagation of rewards obtained at the end of the episode (while beta=0 corresponds to regular DQN with no such reward propagation). This mechanism is shown to improve on Q-learning in a toy 2D maze environment (with MNIST-based pixel states providing cell coordinates) with beta=1, and on DQN and its optimality tightening variant on Atari games with beta=0.5. <sep> The intuition behind the algorithm (that one should try to speed up the propagation of rewards across multiple steps) is not new, in fact it has inspired other approaches like n-step Q-learning, eligibility traces or more recently Retrace(lambda) in deep RL. Actually the idea of replaying experiences in backward order can be traced back to the origins of experience replay («  Programming Robots Using Reinforcement Learning and Teaching », Lin, 1991), something that is not mentioned here. That being said, to the best of my knowledge the specific algorithm proposed in this submission (Alg. 2) is novel, even if Alg. 1 is not (Alg. 1 can be seen as a specific instance of Lin's algorithm with a very high learning rate, and clearly only makes sense in toy deterministic environments). <sep> In the absence of any theoretical analysis of the proposed approach, I would have expected an in-depth empirical validation. Unfortunately this is not the case here. In the toy environment (4.1) I am surprised by the really poor quality of the results (paths 5-10 times longer than the shortest path on average): have algorithms been run for a long enough time? Or maybe the average is a bad performance measure due to outliers? I would have also appreciated a comparison to Retrace(lambda), which is a more principled way to use multi-step rewards than n-step Q-learning (which is technically an on-policy method). Similar remarks can be made on the Atari experiments (4.2), where 10M frames is really low (the original DQN paper had results on 50M frames, and Rainbow reports 200M frames in only ~2x the training time reported here). The comparison also should have included prioritized experience replay, which has been shown to provide a significant boost in DQN, but may be tricky to combine with the proposed algorithm. Overall comparing only to vanilla DQN and its optimality tightening variant is too limited when there have been so many other meaningful improvements over DQN. This makes it really hard to tell whether the proposed algorithm would actually help when combined with a state-of-the-art method like Rainbow for instance. <sep> A few additional small remarks and questions: <sep> - « Second, there is no point in updating a one-step transition unless the future transitions have not been updated yet. »: should « unless » be replaced by « if »? <sep> - In 4.1 is there a maximum number of steps per episode and can you please confirm that training is done independently for each maze? <sep> - Typo in eq. 3: the - in the max should be a comma <sep> - There is a good amount of typos and grammar errors, though they do not harm the readability of the paper <sep> - Citations for « Deep Reinforcement Learning with Double Q-learning » and « Dueling Network Architectures for Deep Reinforcement Learning » could refer to their conference versions <sep> - « epsilon starts from 1 and is annealed to 0 at 200,000 steps in a quadratic manner »: please specify the exact formula <sep> - Fig. 7 is really confusing, there seem to be typos and it is not clear why the beta updates appear in these specific cells, please revise it if you want to keep it","The reviewers agree the proposed idea is relatively incremental, and the paper itself does not do an exemplary job in other areas to make up for this."
"misc | weakness | ac_disagreement | weakness | suggestion  ==> The paper investigates how the learning rate and mini-batch size in SGD impacts the optima that the SGD algorithm finds. <sep> Empirically, the authors argue that it was observed that larger learning rates converge to minima which are more wide, <sep> and that smaller learning rates more often lead to convergence to minima which are narrower, i.e. where the Hessian has large Eigenvalues. In this paper, the authors derive an analytical theory that aims at explaining this phenomenon. <sep> Point of departure is an analytical theory proposed by Mandt et al., where SGD is analyzed in a continuous-time stochastic formalism. In more detail, a stochastic differential equation is derived which mimicks the behavior of SGD. The advantage of this theory is that under specific assumptions, analytic stationary distributions can be derived. While Mandt et al. focused on the vicinity of a local optima, the authors of the present paper assumed white diagonal gradient noise, which allows to derive an analytic, *global* stationary distribution (this is similar as in Langevin dynamics). <sep> Then, the authors focus again on individual local optima and ""integrate out"" the stationary distribution around a local optimum, using again a Gaussian assumption. As a result, the authors obtain un-normalized probabilities of getting trapped in a given local optimum. This un-normalized probability depends on the strength of the value of the loss function in the vicinity of the optimum, the gradient noise, and the width of the optima. In the end, these un-normalized probabilities are taken as probabilities that the SGD algorithm will be trapped around the given optimum in finite time. <sep> Overall assessment: <sep> I find the analytical results of the paper very original and interesting. The experimental part has some weaknesses. The paper could be drastically improved when focusing on the experimental part. <sep> Detailed comments: <sep> Regarding the analytical part, I think this is all very nice and original. However, I have some comments/requests: <sep> 1. Since the authors focus around Gaussian regions around the local minima, perhaps the diagonal white noise assumption could be weakened. This is again the multivariate Ornstein-Uhlenbeck setup examined in Mandt et al., and probably possesses an analytical solution for the un-normalized probabilities (even if the noise is multivariate Gaussian). Would the authors to consider generalizing the proof for the camera-ready version perhaps? <sep> 2. It would be nice to sketch the proof of theorem 2 in the main paper, rather than to just refer to the appendix. In my opinion, the theorem results from a beautiful and instructive calculation that should provide the reader with some intuition. <sep> 3. Would the authors comment on the underlying theoretical assumptions a bit more? In particular, the stationary distribution predicted by the Ornstein-Uhlenbeck formalism is never reached in practice. When using SGD in practice, one is in the initial mode-seeking phase. So, why is it a reasonable assumption to still use results obtained from the stationary (equilibrated) distribution which is never reached? <sep> Regarding the experiments: here I see a few problems. First, the writing style drops in quality. Second, figures 2 and 3 are cryptic. Why do the authors focus on two manually selected optima? In which sense is this statistically significant? How often were the experiments repeated? The figures are furthermore hard to read. I would recommend overhauling the entire experiments section. <sep> Details: <sep> - Typo in Figure 2: ""with different with different"". <sep> - ""the endpoint of SGD with a learning rate schedule η → η/a, for some a > 0, and a constant batch size S, should be the same as the endpoint of SGD with a constant learning rate and a batch size schedule S → aS."" This is clearly wrong as there are many local minima, and running teh algorithm twice results in different local optima.  Maybe add something that this only true on average, like ""the characteristics of these minima ... should be the same"".","Dear authors, <sep> The reviewers agreed that the theoretical part lacked novelty and that the paper should focus on its experimental part which at the moment is not strong enough to warrant publication. <sep> Regarding the theoretical part, here are the main concerns: <sep> - Even though it is used in previous works, the continuous time approximation of stochastic gradient overlooks its practical behaviour, especially since a good rule of thumb is to use as large as stepsize as possible (without reaching divergence), as for instance mentioned in The Marginal Value of Adaptive Gradient Methods in Machine Learning by Wilson et al. <sep> - The isotropic approximation is very strong and I don't know settings where this would hold. Since it seems central to your statements, I wonder what can be deduced from the obtained results. <sep> - I do not think the Gaussian assumption is unreasonable and I am fine with it. Though there are clearly cases where this will not be true, it will probably be OK most of the time. <sep> I encourage the authors to focus on the experimental part in a resubmission."
"misc | weakness | decision  ==>  ==> The authors argue that the spectral dimensionality reduction techniques are too slow, due to the complexity of computing the eigenvalue decomposition, and that they are not suitable for out-of-sample extension. They also note the limitation of neural networks, which require huge amounts of data to properly learn the data structure. The authors therefore propose to first sub-sample the data and afterwards to learn an MDS-like cost function directly with a neural network, resulting in a parametric framework. <sep> The paper should be checked for grammatical errors, such as e.g. consistent use of (no) hyphen in low-dimensional (or low dimensional). <sep> The abbreviations should be written out on the first use, e.g. MLP, MDS, LLE, etc. <sep> In the introduction the authors claim that the complexity of parametric techniques does not depend on the number of data points, or that moving to parametric techniques would reduce memory and computational complexities. This is in general not true. Even if the number of parameters is small, learning them might require complex computations on the whole data set. On the other hand, even if the number of parameters is equal to the number of data points, the computations could be trivial, thus resulting in a complexity of O(N). <sep> In section 2.1, the authors claim ""Spectral techniques are non-parametric in nature""; this is wrong again. E.g. PCA can be formulated as MDS (thus spectral), but can be seen as a parametric mapping which can be used to project new words. <sep> In section 2.2, it says ""observation that the double centering..."". Can you provide a citation for this? <sep> In section 3, the authors propose they technique, which should be faster and require less data than the previous methods, but to support their claim, they do not perform an analysis of computational complexity. It is not quite clear from the text what the resulting complexity would be. With N as number of data points and M as number of landmarks, from the description on page 4 it seems the complexity would be O(N + M^2), but the steps 1 and 2 on page 5 suggest it would be O(N^2 + M^2). Unfortunately, it is also not clear what the complexity of previous techniques, e.g DrLim, is. <sep> Figure 3, contrary to text, does not provide a visualisation to the sampling mechanism. <sep> In the experiments section, can you provide a citation for ADAM and explain how the parameters were selected? Also, it is not meaningful to measure the quality of a visualisation via the MDS fit. There are more useful approaches to this task, such as the quality framework [*]. <sep> In figure 4a, x-axis should be ""number of landmarks"". <sep> It is not clear why the equation 6 holds. Citation? <sep> It is also not clear how exactly the equation 7 is evaluated. It says ""By varying the number of layers and the number of nodes..."", but the nodes and layer are not a part of the equation. <sep> The notation for equation 8 is not explained. <sep> Figure 6a shows visualisations by different techniques and is evaluated ""by looking at it"". Again, use [*]. <sep> [*] Lee, John Aldo ; Verleysen, Michel. Scale-independent quality criteria for dimensionality reduction. In: Pattern Recognition Letters, Vol. 31, no. 14, p. 2248-2257 (2010). doi:10.1016/j.patrec.2010.04.013.","Dear authors, <sep> Thank you for your submission to *CONF*. Sadly, the reviewers were not convinced by the novelty of your approach nor by its experimental results. Thus, your paper cannot be accepted to *CONF*."
"abstract | misc | weakness  ==>  ==> Summary: This paper proposes to use deep Q-learning to learn how to reconstruct a given tower of blocks, where DQN is also parameterized by the desired goal state in addition to the current observed state. <sep> Pros: <sep> - Impressive results on a difficult block-stacking task. <sep> Cons: <sep> - The idea of parameterizing an RL algorithm by goals is not particularly novel. <sep> Quality and Clarity: <sep> The paper is extremely well-written, easy to follow, and largely technically correct, though I am somewhat concerned about how the results were obtained as it does not seem like the vanilla DQN agent could do so well, even on the 2-block scenes. Even just including stable scenes, I estimated based on Figure 5 that there must be about 70 different configurations that are stable (and this is likely an underestimate). So, if each of these scenes occurs equally often and the vanilla DQN agent does not receive any information about the target goal and just acts based on an ""average"" policy, I would expect it to only achieve success about 1/70th of the time. Am I missing something here? <sep> Another thing that was unclear to me is how the rotation of the blocks is chosen: is the agent given the next block with the correct rotation, or can it also choose to rotate the block? In the text it is implied that the only actions are {left, right, down}, which seems to simplify the task immensely. It would be interesting to include results where the agent additionally has to choose from actions of {rotate left by 90 degrees, rotate right by 90 degrees}. <sep> Also: are the scenes used during testing separate from those used during training? If not, it's not obvious that the agent isn't just learning to memorize the solution (which somewhat defeats the idea behind parameterizing the Q-network with new goals every time). <sep> Originality and Significance: <sep> The block-stacking task is very cool and is more complex than many other physics-based RL tasks in the literature, which often involve just stacking square blocks in a single tower. I think it is a useful contribution to introduce this task and the GDQN agent as a baseline. However, the notion of parameterizing the policy by the goal state is not particularly novel. While it is true that many RL papers do train to optimize just a single reward function for a single goal, it is also very straightforward to modify the state space to include a goal and indeed [1-4] are just a few examples of recent papers that have done this. In general, any time there is a procedurally generated environment (e.g. Sokoban, as in [5]) the goal necessarily is included as part of the state space---so the idea of GDQN isn't really that new. <sep> [1] Oh, J., Singh, S., Lee, H., & Kohli, P. (2017). Zero-Shot Task Generalization with Multi-Task Deep Reinforcement Learning. arXiv Preprint arXiv:1706.05064. <sep> [2] Dosovitskiy, A., & Koltun, V. (2017). Learning to act by predicting the future. Proceedings of the 5th International Conference on Learning Representations (*CONF* 2017). <sep> [3] Hamrick, J. B., Ballard, A. J., Pascanu, R., Vinyals, O., Heess, N., & Battaglia, P. W. (2017). Metacontrol for adaptive imagination-based optimization. Proceedings of the 5th International Conference on Learning Representations (*CONF* 2017). <sep> [4] Pascanu, R., Li, Y., Vinyals, O., Heess, N., Buesing, L., Racanière, S., … Battaglia, P. (2017). Learning model-based planning from scratch. arXiv Preprint arXiv: 1707.06170. Retrieved from https://arxiv.org/abs/1707.06170 <sep> [5] Weber, T., Racanière, S., Reichert, D. P., Buesing, L., Guez, A., Rezende, D. J., … Wierstra, D. (2017). Imagination-Augmented Agents for Deep Reinforcement Learning. arXiv Preprint arXiv: 1707.06203. Retrieved from http://arxiv.org/abs/1707.06203","The authors present a toy stacking task where the goal is to stack blocks to match a given configuration, and a method that is a slightly modified DQN algorithm where the target configuration is observed by the network as well as the current state. There are a few problems with this paper. First, the method lacks novelty - it is very similar to DQN. Second, the claims of learning physical intuitions is not borne out by the method or experimental results. Third, the tasks are very simple and there is no held-out test set of target configurations."
"rebuttal_process  ==>  ==> *Summary* <sep> The paper proposes to use hyper-networks [Ha et al. 2016] for the tuning of hyper-parameters, along the lines of [Brock et al. 2017]. The core idea is to have a side neural network sufficiently expressive to learn the (large-scale, matrix-valued) mapping from a given configuration of hyper-parameters to the weights of the model we wish to tune. <sep> The paper gives a theoretical justification of its approach, and then describes several variants of its core algorithm which mix the training of the hyper-networks together with the optimization of the hyper-parameters themselves. Finally, experiments based on MNIST illustrate the properties of the proposed approach. <sep> While the core idea may appear as appealing, the paper suffers from several flaws (as further detailed afterwards): <sep> -Insufficient related work <sep> -Correctness/rigor of Theorem 2.1 <sep> -Clarity of the paper (e.g., Sec. 2.4) <sep> -Experiments look somewhat artificial <sep> -How scalable is the proposed approach in the perspective of tuning models way larger/more complex than those treated in the experiments? <sep> *Detailed comments* <sep> -""...and training the model to completion."" and ""This is wasteful, since it trains the model from scratch each time..."" (and similar statement in Sec. 2.1): Those statements are quite debatable. There are lines of work, e.g., in Bayesian optimization, to model early stopping/learning curves (e.g., Domhan2014, Klein2017 and references therein) and where training procedures are explicitly resumed (e.g., Swersky2014, Li2016). The paper should reformulate its statements in the light of this literature. <sep> -""Uncertainty could conceivably be incorporated into the hypernet..."". This seems indeed an important point, but it does not appear as clear how to proceed (e.g., uncertainty on w_phi(lambda) which later needs to propagated to L_val); could the authors perhaps further elaborate? <sep> -I am concerned about the rigor/correctness of Theorem 2.1; for instance, how is the continuity of the best-response exploited? Also, throughout the paper, the argmin is defined as if it was a singleton while in practice it is rather a set-valued mapping (except if there is a unique minimizer for L_train(., lambda), which is unlikely to be the case given the nature of the considered neural-net model). In the same vein, Jensen's inequality states that Expectation[g(X)] >= g(Expectation[X]) for some convex function g and random variable X; how does it precisely translate into the paper's setting (convexity, which function g, etc.)? <sep> -Specify in Alg. 1 that ""hyperopt"" refers to a generic hyper-parameter procedure. <sep> -More details should be provided to better understand Sec. 2.4. At the moment, it is difficult to figure out (and potentially reproduce) the model which is proposed. <sep> -The training procedure in Sec. 4.2 seems quite ad hoc; how sensitive was the overall performance with respect to the optimization strategy? For instance, in 4.2 and 4.3, different optimization parameters are chosen. <sep> -typo: ""weight decay is applied the..."" --> ""weight decay is applied to the..."" <sep> -""a standard Bayesian optimization implementation from sklearn"": Could more details be provided? (there does not seem to be implementation there http://scikit-learn.org/stable/model_selection.html to the best of my knowledge) <sep> -The experimental set up looks a bit far-fetched and unrealistic: first scalar, than diagonal and finally matrix-weighted regularization schemes. While the first two may be used in practice, the third scheme is not used in practice to the best of my knowledge. <sep> -typo: ""fit a hypernet same dataset."" --> ""fit a hypernet on the same dataset."" <sep> -(Franceschi2017) could be added to the related work section. <sep> *References* <sep> (Domhan2014) Domhan, T.; Springenberg, T. & Hutter, F. Extrapolating learning curves of deep neural networks ICML 2014 AutoML Workshop, 2014 <sep> (Franceschi2017) Franceschi, L.; Donini, M.; Frasconi, P. & Pontil, M. Forward and Reverse Gradient-Based Hyperparameter Optimization preprint arXiv:1703.01785, 2017 <sep> (Klein2017) Klein, A.; Falkner, S.; Springenberg, J. T. & Hutter, F. Learning curve prediction with Bayesian neural networks International Conference on Learning Representations (*CONF*), 2017, 17 <sep> (Li2016) Li, L.; Jamieson, K.; DeSalvo, G.; Rostamizadeh, A. & Talwalkar, A. Hyperband: A Novel Bandit-Based Approach to Hyperparameter Optimization preprint arXiv:1603.06560, 2016 <sep> (Swersky2014) Swersky, K.; Snoek, J. & Adams, R. P. Freeze-Thaw Bayesian Optimization preprint arXiv:1406.3896, 2014 <sep> ********* <sep> Update post rebuttal <sep> ********* <sep> I acknowledge the fact that I read the rebuttal of the authors, whom I thank for their detailed answers. <sep> My minor concerns have been clarified. Regarding the correctness of the proof, I am still unsure about the applicability of Jensen inequality; provided it is true, then it is important to see that the results seem to hold only for particular hyperparameters, namely regularization parameters (as explained in the new updated proof). This limitation should be exposed transparently upfront in the paper/abstract. <sep> Together with the new experiments and comparisons, I have therefore updated my rating from 5 to 6.","The paper is interesting, and the update to the paper and additional experiments has already improved it in many ways, but the paper still does still not have as much impact as it could, by further strengthening the comparisons and usefulness in many of situations of current practice."
"decision | misc  ==> The authors propose the use of bootstrapping the data (random sampling entries with replacement) to form surrogate data for which they can evaluate the singular value spectrum of the SVD of the matrix to the singular values of the bootstrapped data, thereby determining the number of latent dimensions in PCA by the point in which the singular values are no greater than the bootstrapped sampled values.  The procedure is contrasted to some existing methods for determining the number of latent components and found to perform similarly to another procedure based on bootstrapping correlation matrices, the PA procedure. <sep> Pros: <sep> Determining the number of components is an important problem that the authors here address. <sep> Cons: <sep> I find the paper poorly written and the methodology not sufficiently rooted in the existing literature. There are many approaches to determining the number of latent components in PCA that needs to be discussed and constrasted including: <sep> Cross-validation: <sep> http://scholar.google.dk/scholar_url?url=http%3A%2F%2Fwww.academia.edu%2Fdownload%2F43416804%2FGeneralizable_Patterns_in_Neuroimaging_H20160306-9605-1xf9c9h.pdf&hl=da&sa=T&oi=gga&ct=gga&cd=0&ei=rjkXWrzKKImMmAH-xo7gBw&scisig=AAGBfm2iRQhmI2EHEO7Cl6UZoRbfAxDRng&nossl=1&ws=1728x1023 <sep> Variational Bayesian PCA: <sep> https://www.microsoft.com/en-us/research/publication/variational-principal-components/ <sep> Furthermore, the idea of bootstrapping for the SVD has been discussed in prior publications and the present work need to be related to these prior works. This includes: <sep> Milan, Luis, and Joe Whittaker. ""Application of the Parametric Bootstrap to Models That Incorporate a Singular Value Decomposition."" Journal of the Royal Statistical Society. Series C (Applied Statistics), vol. 44, no. 1, 1995, pp. 31–49. JSTOR, JSTOR, www.jstor.org/stable/2986193. <sep> Fisher A, Caffo B, Schwartz B, Zipunnikov V. Fast, Exact Bootstrap Principal Component Analysis for p > 1 million. Journal of the American Statistical Association. 2016;111(514):846-860. doi:10.1080/01621459.2015.1062383. <sep> Including the following package in R for performing bootstrapped SVD: https://cran.r-project.org/web/packages/bootSVD/bootSVD.pdf <sep> The novelty of the present approach is therefore unclear given prior works on bootstrapping SVD/PCA. <sep> Furthermore, for sparse data with missing entries there are specialized algorithms handling sparsity either using imputation or marginalization, which would be more principled to estimate the PCA parameters. <sep> Finaly, the performance appears almost identical with the PA procedure. In fact, it seems bootstrapping the correlation matrix has a very similar effect as the proposed bootstrapping procedure. Thus, it seems the proposed procedure which is very similar in spirit to PA does not have much benefit over this procedure. <sep> Minor comments: <sep> Explain what SW abbreviates when introduced first. <sep> We will see that it PA a close relationship with BSVD-> We will see that PA is closely related to BSVD <sep> more effective than SVD under certain conditions (?). – please provide reference instead of ? <sep> But table 4 that shows -> But table 4 shows that <sep> We can sum up with that the result seems ->To summarize, the result seems","The paper addresses the important question of determining the intrinsic dimensionality, but there remain several issue, which make the paper not ready at this point: unclear exposition, lack of contextualisation of existing work and seemingly limited insights. The reviewers have provided many suggestions to improve the paper which we hope will be useful to improve the paper."
"abstract | misc | weakness  ==> Paper summary: <sep> This paper proposes a novel unsupervised embedding for time-series. Its architecture mainly consists in a series of dilated causal convolutions, followed by a temporal averaging to obtain a representation which is independent on the length of the time-series. The authors propose a triplet loss with negative mining to train the embedding, which is novel for real-valued time-series. This method is experimentally validated on a classification and a regression task. <sep> General opinion: <sep> * Pros: <sep> * Good writing <sep> * Detailed appendix with experimental hyperparameters, so that the results are pretty reproducible. <sep> * For classification and regression, the proposed method reaches results close to the state-of-the-art. <sep> * Cons: <sep> * The authors state that the embedding is unsupervised, but in Appendix C they acknowledge that it is trained with early-stopping based on the final classification accuracy, thereby relying on an implicit supervision. <sep> * This method does not improve over the state-of-the-art on time-series classification, even though it is its natural purpose <sep> * The experimental validation of the proposed method is weak (cf detailed method). <sep> * I have some concerns at the conceptual level (cf detailed questions). <sep> Taking into consideration these aspects, I tend to vote for a weak rejection. <sep> Detailed questions: <sep> - On a conceptual level, the ideas underlying the use of a triplet loss explained in the 3rd paragraph of section 2 seems a bit incomplete to me. On the one hand, the authors state that the embedding of a sub-series should be close to the embedding of the series. On the other hand, they also state that this embedding should be far from the embedding of a randomly sampled sub-series, possibly in the same long time-series. This seems contradictory, because if they belong to the same global time-series, they are both sub-series of the global time-series and therefore should be close. Also, the fact that no scale is taken into account when defining sub-series seems quite irrealistic. <sep> - Why is using a *causal* embedding important for classification purposes? <sep> - Experimentally, what are the results when the number of negative samples, K, varies? Experiments have been performed with this parameter varying as an ensemble is taken. It is a pity that the importance of this value is not reported, as it would have provided an intuition on its importance. <sep> - The runtimes reported in Table 1 are a bit strange. Why does the runtime of the raw values vary so much (x30) when moving from daily to quarterly predictions, while the runtime of the representations diminishes (/3)?","The paper presents some reasonable experiments and approaches for unsupervised time series. However as mentioned by R2 there is several issues. The paper also overclaims a bit some of the novelty. As noted by R1 and the metareviewer there is other works using triplet loss for timeseries, a relatively common approach in temporal dataset (e.g. video, audio), the popular causal convolution structure from wavenet is also quite well known, contributions should be more clear."
"rating_summary | decision  ==> Summary: <sep> This paper addresses an important problem in density estimation which is to scale the generation to high fidelity images. Till now, there have been no good density modeling results on large images when taken into account large datasets like Imagenet (there have been encouraging results like with Glow, but on 5-bit color intensities and simpler datasets like CelebA). This paper is the first to successfully show convincing Imagenet samples with 128x128 resolution for a likelihood density model, which is hard even for a GAN (only one GAN paper (SAGAN) prior to this conference has managed to show unconditional 128x128 Imagenet samples). The ideas in this paper to pick an ordering scheme at subsampled slices uniformly interleaved in the image and condition slice generation in an autoregressive way is very likely to be adopted/adapted to more high fidelity density modeling like videos. Another important idea in this paper is to do depth upscaling, focusing on salient color intensity bits first (first 3 bits per color channel) before generating the remaining bits. The color intensity dependency structure is also neat: The non-salient bits per channel are conditioned on all previously generated color bits (for all spatial locations). Overall, I think this paper is a huge advance in density modeling, deserves an oral presentation and deserves as much credit as BigGAN, probably more, given that it is doing unconditional generation. <sep> Details: <sep> Major: <sep> -1. Can you point out the total number of parameters in the models? Also would be good to know what hardware accelerators were used. The batch sizes mentioned in the Appendix (2048 for 256x256 Imagenet) are too big and needs TPUs? If TPU pods, which version (how many cores)? If not, I am curious to know how many GPUs were used. <sep> 0. I would really like to know the sampling times. The model still generates the image pixel by pixel. Would be good to have a number for future papers to reference this. <sep> 1. Any reason why 256x256 Imagenet samples are not included in the paper? Given that you did show 256x256 CelebA samples, sampling time can't be an issue for you to not show Imagenet 256x256. So, it would be nice to include them. I don't think any paper so far has shown good 256x256 unconditional samples. So showing this will make the paper even stronger. <sep> 2. Until now I have seen no good 64x64 Imagenet samples from a density model. PixelRNN samples are funky (colorful but no global structure). So I am curious if this model can get that. It may be the case that it doesn't, given that subscale ordering didn't really help on 32x32.  It would be nice to see both 5-bit and 8-bit, and for 8-bit, both the versions: with and without depth upscaling. <sep> 3. I didn't quite understand the architecture in slice encoding (Sec 3.2).  Especially the part about using a residual block convnet to encode the previous slices with padding, and to preserve relative meta-position of the slices. The part I get is that you concatenate the 32x32 slices along the channel dimension, with padded slices. I also get that padding is necessary to have the same channel dimension for any intermediate slice. Not sure if I see the whole point of preserving ordering. Isn't it just normal padding -> space to depth in a structured block-wise fashion? <sep> 4. Can you clarify how you condition the self-attention + Gated PixelCNN block on the previous slice embedding you get out of the above convnet? There are two embeddings passed in if I understand correctly: (1) All previous slices, (2) Tiled meta-position of current slice.  It is not clear to me how the conditioning is done for the transformer pixelcnn on this auxiliary embedding. The way you condition matters a lot for good performance, so it would be helpful for people to replicate your results if you provide all details. <sep> 5. I also don't understand the depth upscaling architecture completely. Could you provide a diagram clarifying how the conditioning is done there given that you have access to all pixels' salient bits now and not just meta-positions prior to this slice? <sep> 6. It is really cool that you don't lose out in bits/dim after depth upscaling that much. If you take Grayscale PixelCNN (pointed out in the anonymous comment), the bits/dim isn't as good as PixelCNN though samples are more structured. There is 0.04 b.p.d  difference in 256x256, but no difference in 128x128. Would be nice to explain this when you add the citation. <sep> 7. The architecture in the Appendix can be improved. It is hard to understand the notations. What are residual channels, attention channels, attention ffn layer, ""parameter attention"", conv channels? <sep> Minor: <sep> Typo: unpredented --> unprecedented","All reviewers recommend acceptance, with two reviewers in agreement that the results represent a significant advance for autoregressive generative models. The AC concurs."
"abstract | suggestion | rebuttal_process | suggestion | rebuttal_process  ==> This paper deals with Memory Augmnted Neural Networks (MANN) and introduces an algorithm which allows full writes to the dense memory to be only exectued every L timesteps. The controller produces a hidden output at most timestps, whih is appended to a cache. Every L steps, soft attention is used to combine this cache of N hidden states to a single one, and then this is used as the input hidden state for the controller, with the outputs performing a write in the full memory M, along with clearing the cache. <sep> The authors first derive ""Uniform Writing"" (UW) which updates the memory at regular intervals instead of every timestep. The derivation is based on the ""contribution"" which is norm of the gradient of some input timestep to some hidden state (potentially at a different timestep). I am not clear on whether this terminology for the quantity is novel, if this is the case maybe the authors should state this more clearly. UW says that if all timesteps are equally important, and only D writes can be made in a sequence of length T, then writes should be done every T/(D+1) steps. I have not checked the proof in detail but this seems reasonable that it would maximise the contribution quantity introduced. I am less clear on whether this is obviously the right thing to do - sometimes this value is referred to in relation to information, but that term does not strictly seem to be being used in the information theory sense (no mention of bits or nats anywhere). Regardless, as the authors point out, in real problems there are obviously timesteps which have less or no useful information, and clearly UW is mostly defined in order to build towards CUW. <sep> CUW expands on UW by adding the cache of different hidden states, and using soft attention over them. This feels like a reasonable step, although I would presume there are times when the L hidden states were collected over timesteps with no information, and so the resulting write is not that useful, and times when all of hte L timesteps contain different useful information. In these circumstances it seems like the problem of getting the *useful* information into the memory is still present, as the single write done with the averaged hidden state will need to contain lots of information, which may be more ideal written with several timesteps. <sep> The experiments are well described and overall the paper seems reproducable. The standard toy datasets of copy / reverse / sinusoid are used. The results are interesting - regular DNC with memory size 50 performs surprisingly badly on clean Sinusoid, my guess would be that with hyperparameter tuning this could be improved upon. I'm not sure that using exactly the same hyperparameters for a wide variety of models is appropriate - even with optimizers like Adam and RMSProp, I would want to see at least some sweeping for the best hyperparams, and then graphs like figure 3 should show error bars averaged across multiple runs with the best per-model hyperparameters. However, The DNC with CUW seems to perform well across all synthetic tasks. <sep> There is no mention of Adaptive Computation Time/ACT (Graves, https://arxiv.org/abs/1603.08983) throughout the paper, which is surprising considering Alex Graves' models form two of the baselines used throughout the paper. ACT aims to execute an RNN a variable number of times, usually to do >1 timestep of processing for a single timestep of input. In the context of this paper, I believe it could be adapted to do either zero or one steps of computation per timestep, and that would yield a very comparable network where the LSTM controller always executes, and writes to the memory only happen sometimes. Given that it allows a learned process to decide whether to write, as opposed to having a fixed L which separates full writes, this should have the potential to outperform CUW, as it could learn that at certain times, writes must happen at every step. In my view ACT is attempting to solve essentially the same problem as this paper, so it should either be included as a baseline, or the manuscript should be updated to explain why this is not an appropriate comparison. <sep> I think this is an interesting paper, trying to make progress on an important problem. The results look good, but I can only give a borderline score due to missing ACT numbers, and a few other unclear points. The addition of ACT experiments, and error bars on certain results, would change my mind here. <sep> Notes: <sep> ""No solution has been proposed to help MANNs handle ultra long sequence"" - (Rae et al 2016) is an attempt to do this, by improving the complexity of reads / writes. This allows bigger memory and longer sequences to be processed. <sep> ""Current MANNS only support dense writing"" - presumably this means dense as in 'every timestep', but this terminology is overloaded - you could consider NTM / DNC as doing dense writing, and then work of Rae et al 2016 doing sparse writing. <sep> In my experience training these kind of RNNs can have reasonably high variance across seeds - figures 2 & 3 should have error bars, and especially Table 4 as that contains the most important results. Getting 99 percent accuracy when previous SOTA is only 0.1% lower is only really meaningful if the standard deviation across seeds is very small. <sep> Appendix A: the 'by induction' result - I believe there is an error, it should be: <sep> h_t = \\sigma_{i=1}^t U_{t-i}W x_i + C <sep> As W is applied to inputs, before the repeated applications of U? I believe the rest of the derivation still holds the same, after the correction.","Well-written paper that motivates through theoretical analysis new memory writing methods in memory augmented neural networks. Extensive experimental analysis support and demonstrate the advantages of the new solutions over other recurrent architectures. <sep> Reviewers suggested extension and clarification of the analysis presented in the paper, for example, for different memory sizes. The paper was revised accordingly. Another important suggestion was considering ACT as a baseline. Authors explained clearly why it wasn't considered as a baseline, and updated the paper to include references and explanations in the paper as well."
"abstract | weakness | strength  ==> The authors propose an extension of cycle-consistent adversarial adaptation methods in order to tackle domain adaptation in settings where a limited amount of supervised target data is available (though they also validate their model in the standard unsupervised setting as well). The method appears to be a natural generalization/extension of CycleGAN/CyCADA. It uses the ideas of the semantic consistency loss and training on adapted data from CyCADA, but ""fills out"" the model by applying these techniques in both directions (whereas CyCADA only applied them in the source-to-target direction). <sep> The writing in this paper is a little awkward at times (many omitted articles such as ""the"" or ""a'), but, with a few exceptions, it is generally easy to understand what the authors are saying. They provide experiments in a variety of settings in order to validate their model, including both visual domain adaptation and speech domain adaptation. The experiments show that their model is effective both in low-resource supervised adaptation settings as well as high-resource unsupervised adaptation settings. An ablation study, provided in Section 4.1, helps to understand how well the various instantiations of the authors' model perform, indicating that enforcing consistency in both methods is crucial to achieving performance beyond the simple baselines. <sep> It's a little hard to understand how this method stands in comparison to existing work. Table 3 helps to show that the model can scale up to the high-resource setting, but it would also be nice to see the reverse: comparisons against existing work run in the limited data setting, to better understand how much limited data negatively impacts the performance of models that weren't designed with this setting in mind. <sep> I would've also liked to see more comparisons against the simple baseline of a classifier trained exclusively on the available supervised target data, or with the source and target data together—in my experience, these baselines can prove to be surprisingly strong, and would give a better sense of how effective this paper's contributions are. This corresponds to rows 2 and 3 of Table 1, and inspection of the numbers in that table shows that the baseline performance is quite strong even relative to the proposed method, so it would be nice to see these numbers in Table 2 as well, since that table is intended to demonstrate the model's effectiveness across a variety of different domain shifts. <sep> While it's nice that the model is experimentally validated on the speech domain, the experiment itself is not explained well. The speech experiments are hard to understand—it's unclear what the various training sets are, such as ""Adapted Male"" or ""All Data,"" making it hard to understand exactly what numbers should be compared. Why is there no CycleGAN result for ""Female + Adapted Male,"" or ""All Data + Adapted Male,"" for example? The paper would greatly benefit from a more careful explanation and analysis of this experimental setting. <sep> Ultimately, I think the idea is a nice generalization of previous work, and the experiments seem to indicate that the model is effective, but the limited scope of the experiments prevent me from being entirely convinced. The inclusion of additional baselines and a great deal of clarification on the speech experiments would improve the quality of this paper enormously. <sep> --- <sep> Update: After looking over the additional revisions and experiments, I'm bumping this to a weak accept. I agree with reviewer 3 that novelty is not the greatest, but there is a useful contribution here, and the demonstration of its effectiveness on low resource settings is valuable, since in a practical setting it is usually feasible to manually label a few examples. <sep> I'm still not convinced by the TIMIT experiments, now that I better understand them, since the F+M baseline is quite strong and very simple to run. It simply doesn't seem worthwhile to introduce all of this extra machinery for such a marginal improvement, but the experiment does serve the job of at least demonstrating an improvement over existing methods.","The authors propose a method for low-resource domain adaptation where the number of examples available in the target domain are limited. The proposed method modifies the basic approach in a CycleGAN by augmenting it with a ""content"" (task-specific) loss, instead of the standard reconstruction error. The authors also demonstrate experimentally that it is important to enforce the loss in both directions (target → source and source --> target). Experiments are conducted on both supervised as well as unsupervised settings. <sep> The main concern expressed by the reviewers relates to the novelty of the approach since it is a relatively straightforward extension of CycleGAN/CyCADA, but in the view of a majority of reviewers the work serves a useful contribution as a practical method for developing systems in low-resource conditions where it is feasible to label a few new instances. Although the reviewers were not unanimous in their recommendations, on balance in the view of the AC the work is a useful contribution with clear and detailed experiments in the revised version."
"abstract | strength  ==> This paper studies variance neural networks, which approximate the posterior of Bayesian neural networks with zero-mean Gaussian distributions. The inference results are surprisingly well though there is no information in the mean of the posterior. It further shows that the several variational dropout methods are closed related to the proposed method. The experiment indicates that the ELBO can actually better optimized with this restricted form of variational distribution. <sep> The paper is clearly written and easy to follow. The technique in the paper is solid. <sep> However, the authors might need to clarify a few questions below. <sep> Q1:  if every transformation is antisymmetric non-linear, then it seems that the expected distribution of t in (2) is zero. Is this true or not? In another word, class information has to be read out from the encoding of instances in Fig 1. It seems antisymmetric operators cannot do so, as it will only get symmetric distributions from symmetric distributions. <sep> Q2: it is not straightforward to see why KL term needs to go zero. In my understanding, the posterior aims to fit two objectives: maximizing data likelihood and minimizing KL term. When the signal from the data is strong (e.g. large amount of data), the first objective becomes more important. Then q does not really try to make KL zero, and alpha has no reason to go infinity. Can you explain more? <sep> Q3: Is the claimed benefit from the optimization procedure or the special structure of the variance layer? Is it possible to test the hypothesis by 1) initializing a q distribution with learnable mean by the solution of variance neural network and then 2) optimizing q? Then the optimization procedure should continue to increase ELBO. Then compare the learned q against the variance neural network. If the learned q is better than the variance network -- it means the network structure is better for optimization, but the structure itself might not be so special. If the learned q is worse than the variance network, then the structure is interesting. <sep> A few detailed comments: <sep> 1. logU used without definition. <sep> 2. if the paper has a few sentence explaining ""Gaussian dropout approximate posterior"", section 4 will be smoother to read.",The authors describe a very counterintuitive type of layer: one with mean zero Gaussian weights. They show that various Bayesian deep learning algorithms tend to converge to layers of this variety. This work represents a step forward in our understanding of bayesian deep learning methods and potentially may shine light on how to improve those methods.
"abstract | strength | weakness | decision  ==>  ==> This paper presents an improvement on the local/derivative-free learning algorithm equilibrium propagation. Specifically, it trains a feedforward network to initialize the iterative optimization process in equilibrium prop, leading to greater stability and computational efficiency, and providing a network that can later be used for fast feedforward predictions on test data. Non-local gradient terms are dropped when training the feedforward network, so that the entire system still doesn't require backprop. There is a neat theoretical result showing that, in the neighborhood of the optimum, the dropped non-local gradient terms will be correlated with the retained gradient terms. <sep> My biggest concern with this paper is the lack of significant literature review, and that it is not placed in the context of previous work. There are only 12 references, 5 of which come from a single lab, and almost all of which are to extremely recent papers. Before acceptance, I would ask the authors to perform a literature search, update their paper to include citations to and discussion of previous work, and better motivate the novelty of their paper relative to previous work. Luckily, this is a concern that is addressable during the rebuttal process! If the authors perform a literature search, and update their paper appropriately, I will raise my score as high as 7. <sep> Here are a few related topic areas which are currently not discussed in the paper. *I am including these as a starting point only! It is your job to do a careful literature search. I am completely sure there are obvious connections I'm missing, but these should provide some entry points into the citation web.* <sep> - The ""method of auxiliary coordinates"" introduces soft (often quadratic) couplings between post- and pre- activations in adjacent layers which, like your distributed quadratic penalty, eliminate backprop across the couplings. I believe researchers have also done similar things with augmented Lagrangian methods. A similar layer-local quadratic penalty also appears in ladder networks. <sep> - Positive/negative phase (clamped / unclamped phase) training is ubiquitous in energy based models. Note though that it isn't used in classical Hopfield networks. You might want to include references to other work in energy based models for both this and other reasons. e.g., there may be some similarities between this approach and continuous-valued Boltzmann machines? <sep> - In addition to feedback alignment, there are other approaches to training deep neural networks without standard backprop. examples include: synthetic gradients, meta-learned local update rules, direct feedback alignment, deep Boltzmann machines, ... <sep> - There is extensive literature on biologically plausible learning rules -- it is a field of study in its own right. As the paper is motivated in terms of biological plausibility, it would be good to include more general context on the different approaches taken to biological plausibility. <sep> More detailed comments follow: <sep> Thank you for including the glossary of symbols! <sep> ""Continuous Hopfield Network"" use lowercase for this (unless introducing acronym) <sep> ""is the set non-input"" -> ""is the set of non-input"" <sep> ""α=... ... αj⊂..."" I could not make sense of the set notation here. <sep> would recommend using something other than rho for nonlinearity. rho is rarely used as a function, so the prior of many readers will be to interpret this as a scalar. phi( ) or f( ) or h( ) are often used as NN nonlinearities. <sep> inline equation after ""clamping factor"" -- believe this should just be C, rather than \\partial C / \\partial s. <sep> Move definition of \\mathcal O up to where the symbol is first used. <sep> text before eq. 7 -- why train to approximate s- rather than s+? It seems like s+ would lead to higher accuracy when this is eventually used for inference. <sep> eq. 10 -- doesn't the regularization term also decrease the expressivity of the Hopfield network? e.g. it can no longer engage in ""explaining away"" or enforce top-down consistency, both of which are powerful positive attributes of iterative estimation procedures. <sep> notation nit: it's confusing to use a dot to indicate matrix multiplication. It is commonly used in ML to indicate an inner product between two vectors of the same shape/orientation. Typically matrix multiplication is implied whenever an operator isn't specified (eg x w_1 is matrix multiplication). <sep> eq. 12 -- is f' supposed to be h'? And wasn't the nonlinearity earlier introduced as rho? Should settle on one symbol for the nonlinearity. <sep> This result is very cool. It only holds in the neighborhood of the optimum though. At initialization, I believe the expected correlation is zero by symmetry arguments (eg, d L_2 / d s_2 is equally likely to have either sign). Should include an explicit discussion of when this relationship is expected to hold. <sep> ""proportional to"" -> ""correlated with"" (it's not proportional to) <sep> sec. 3 -- describe nonlinearity as ""hard sigmoid"" <sep> beta is drawn from uniform distribution including negative numbers? beta was earlier defined to be positive only. <sep> Figure 2 -- how does the final achieved test error change with the number of negative-phase steps? ie, is the final classification test error better even for init eq prop in the bottom row than it is in the top? <sep> The idea of initializing an iterative settling process with a forward pass goes back much farther than this. A couple contexts being deep Boltzmann machines, and the use of variational inference to initialize Monte Carlo chains sect 4.3 -- ""the the"" -> ""to the","The paper investigates a novel initialisation method to improve Equilibrium Propagation. In particular, the results are convincing, but the reviewers remain with small issues here and there. <sep> An issue with the paper is the biological plausibility of the approach. Nonetheless publication is recommended."
"strength | weakness | rebuttal_process | rating_summary | decision  ==> # 1. Summary <sep> This paper presents a model for future video prediction, which integrates 3D convolutions into RNNs. The internal operations of the RNN are modified by adding historical records controlled via a gate-controlled self-attention module. The authors show that the model is effective also for other tasks such as early activity recognition. <sep> Strengths: <sep> * Nice extensive experimentation on video prediction and early activity recognition tasks and comparison with recent papers <sep> * Each choice in the model definition are motivated, although some clarity is still missing (see below) <sep> Weaknesses: <sep> * Novelty: the proposed model is a small extension of a previous work (Wang et al., 2017) <sep> # 2. Clarity and Motivation <sep> In general, the paper is clear and general motivation makes sense, however some points need to be improved with further discussion and motivation: <sep> A) Page 2 ""Unlike the conventional memory transition function, it learns the size of temporal interactions. For longer sequences, this allows attending to distant states containing salient information"": This is not obvious. Can the authors add more details and motivate these two sentences? How is long-term relations are learned given Eq. 1? <sep> B) Page 5 ""These two terms are respectively designed for short-term and long-term video modeling"": How do you make sure that Recall(.) does not focus on the short-term modeling instead? Not clear why this should model long-term relations. <sep> C) Page 5 and Eq 1: motivation why layer norm is required when defining C_t^k is not clear <sep> D) What if the Recall is instead modeled as attention? The idea is to consider only C_{1:t-1}^k (not consider R_t) and have an attentional model that learn what to recall based only on C. Also, why does Recall need to depend on R_t? <sep> E) Page 5 ""to minimize the l1 + l2 loss over every pixel in the frame"": this sentence is not clear. How does it relate to Eq. 2? <sep> # 3. Novelty <sep> Novelty is the major concern of this paper. Although the introduced new concepts and ideas are interesting, the work seems to be an extension of ST-LSTM and PredRNN where Eq 1 is slightly modified by introducing Recall. <sep> In addition the existing relation between the proposed model and ST-LSTM is not clearly state. Page 2, first paragraph: here the authors should state that model is and extension of ST-LSTM and highlight what are the difference and advantage of the new model. <sep> # 4. Significance of the work <sep> This paper deals with an interesting and challenging topic (video prediction) as well as it shows some results on the early activity recognition task. These are definitively nice problem which are far to be solved. From the application perspective this work is significant, however from the methodological perspective it lacks a bit of significance because of the novelty issues highlighted above. <sep> # 5. Experimentation <sep> The experiments are robust with nice comparisons with recent methods and ablation study motivating the different components of the model (Table 1 and 2). Some suggested improvements: <sep> A) Page 7 ""Seq 1 and Seq 2 are completely irrelevant, and ahead of them, another sub-sequence called prior context is given as the input, which is exactly the same as Seq 2"": The COPY task is a bit unclear and need to be better explained. Why are Seq. 1 and 2 irrelevant? I would suggest to rephrase this part. <sep> B) Sec. 4.2, ""Dataset and setup"": which architecture has been used here? <sep> C) Sec. 4.3, ""Hyper-parameters and Baselines"": the something-something dataset is more realising that the other two ""toy"" dataset. Why did the authors choose to train a 2 layers 3D-CNN encoders, instead of using existing pretrained 3D CNNs? I would suspect that the results can improve quite a bit. <sep> # 6. Others <sep> * The term ""self-supervised auxiliary learning"" is introduced in the abstract, but at this point it's meaning is not clear. I'd suggest to either remove it or explain its meaning. <sep> * Figure 1(a): inconsistent notation with 2b. Also add citation (Wang et al., 2017) since it ie the same model of that paper <sep> ------- <sep> # Post-discussion <sep> I increased my rating: even if novelty is not high, the results support the incremental ideas proposed by the authors.","Strengths: Strong results on future frame video prediction using a 3D convolutional network. Use of future video prediction to jointly learn auxiliary tasks shown to to increase performance. Good ablation study. <sep> Weaknesses: Comparisons with older action recognition methods. Some concerns about novelty, the main contribution is the E3D-LSTM architecture, which R1 characterized as an LSTM with an extra gate and attention mechanism. <sep> Contention: Authors point to novelty in 3D convolutions inside the RNN. <sep> Consensus: All reviewers give a final score of 7- well done experiments helped address concerns around novelty. Easy to recommend acceptance given the agreement."
"abstract | strength | weakness | strength | rebuttal_process | decision | rebuttal_process | weakness  ==> The authors state nicely and clearly the main contributions they see in their work (Intro, last paragraph). Specifically the state the paper: 1) present a new and important machine learning task, 2) present a family of models that capture the structure of edits and compute efficient representations, 3) create a new source code edit dataset, 4) perform a set of experiments on the learned edit representations and present promising empirical evidence that the models succeed in capturing the semantics of edits. <sep> We decided to organize this review by commenting on the above-stated contributions one at a time: <sep> ""A new and important machine learning task"" <sep> Regarding ""new task"": <sep> PRO: We are unfamiliar with past work which presents this precise task; the task is new. Section 5 makes a good case for the novelty of this work. <sep> CON: None. <sep> Regarding ""important task"": <sep> PRO: The authors motivate the task with tantalizing prospective applications-- automatically editing text and code, e.g. for grammar, clarity, and style. Conceptualizing edits as NLP objects of interest that can be concretely represented, clustered, and used for prediction is an advance. <sep> CON: Many text editors, office suites, and coding IDEs already include features which automatically suggest or apply edits for grammar, clarity, and style. The authors do not describe shortcomings in existing tools that might be better addressed using distributed representations of edits. Consequently, the significance of the proposed contribution is unclear. <sep> ""A family of models that capture the structure of edits and compute efficient representations"" <sep> Regarding ""a family of models"": <sep> PRO: The family of models presented by the authors clearly generalizes: such models may be utilized for computational experiments on datasets and edit types beyond those specifically utilized in this evaluation. The authors apply well-utilized neural network architectures that may be trained and applied to large datasets. The architecture of the neural editor permits evaluation of the degree to which the editor successfully predicts the correct edit given a pre-edit input and a known representation of a similar edit. <sep> CON: The authors do not propose any scheme under which edit representations might be utilized for automatically editing text or code when an edit very similar to the desired edit is not already known and its representation available as input. Hence, we find the authors do not sufficiently motivate the input scheme of their neural editor. The input scheme of the neural editor makes trivial the case in which no edit is needed, as the editor would learn during training that the output x+ should be the same as the input x- when the representation of the ""zero edit"" is given as input. While the authors discuss the importance of ""bottlenecking"" the edit encoder so that it does not simply learn to encode the desired output x+, they do not concretely demonstrate that the edit encoder has done otherwise in the final experiments. Related to that: If the authors aimed to actually solve automated edits in text/code then it seems crucial their data contained ""negative examples"" i.e. segments which require no edits. In such an evaluation one would test also when the algorithm introduces unnecessary/erroneous edits. <sep> Regarding ""capture structure of edits"": <sep> PRO: The authors present evidence that edit encoders tightly cluster relatively simple edits which involve adding or removing common tokens. The authors present evidence that relatively simple edits completed automatically by a ""fixer"" often cluster together, i.e. a known signal is retained in clustering. The authors present evidence that the nearest neighbors of edits in an edit-representation space often are semantically or structurally similar, as judged by human annotators. Section 4.3 includes interesting observations comparing edit patterns better captured by the graph or seq edit encoders. <sep> CON: The details of the human annotation tasks which generated the numerical results in Tables 1 and 2 are unclear: were unbiased third parties utilized? Were the edits stripped of their source-encoder label when evaluated? Objectively, what separates an ""unrelated"" from a ""similar"" edit, and what separates a ""similar"" from a ""same"" edit? Did multiple human annotators undertake this task in parallel, and what was their overall concordance (e.g. ""intercoder reliability"")? Without concrete answers to these questions, the validity and significance of the DCG/NDCG results reported in Tables 1 and 2 are unclear. It is not clear from the two examples given in Table 1 that the three nearest neighbors embedded by the Seq encoder are ""better"", i.e. overall more semantically and/or syntactically similar to the example edit, than those embedded by the Bag of Words model. It is unclear which specific aspects of ""edit structure"" are better captured by the Seq encoder than the Bag of Words model. The overall structure of Tables 1 and 2 is awkward, with concrete numerical results dominated by a spatially large section containing a small number of examples. <sep> ""create a new source code edit dataset"" <sep> PRO: The authors create a new source code edit dataset, an important contribution to the study of this new task. <sep> CON: Minor: is the provided dataset large enough to do more than simple experiments? See note below on sample size. <sep> ""present promising empirical evidence that the models succeed in capturing the semantics of edits"" <sep> PRO: The experiment results show how frequently the end-to-end system successfully predicted the correct edit given a pre-edit input and a known representation of a similar edit. Gold standard accuracies of more than 70%, and averaged transfer learning accuracies of more than 30%, suggest that this system shows promise for capturing the semantics of edits. <sep> CON: Due to concerns expressed above about the model design and evaluation of the edit representations, it remains unclear to what degree the models succeed in capturing the semantics of edits. Table 11 shows dramatic variation in success levels across fixer ID in the transfer learning task, yet the authors do not propose ways their end-to-end system might be adjusted to address areas of weak performance. The authors do not discuss the impact of training set size on their evaluation metrics. The authors do not discuss the degree to which their model training task would scale to larger language datasets such as those needed for the motivating applications. <sep> ############## <sep> Based on the authors' response, revisions, and disucssions we have updated the review and the score.","This paper investigates learning to represent edit operations for two domains: text and source code. The primary contributions of the paper are in the specific task formulation and the new dataset (for source code edits). The technical novelty is relatively weak. <sep> Pros: <sep> The paper introduces a new dataset for source code edits. <sep> Cons: <sep> Reviewers raised various concerns about human evaluation and many other experimental details, most of which the rebuttal have successfully addressed. As a result, R3 updated their score from 4 to 6. <sep> Verdict: <sep> Possible weak accept. None of the remaining issues after the rebuttal is a serious deal breaker (e.g., task simplification by assuming the knowledge of when and where the edit must be applied, simplifying the real-world application of the automatic edits). However, the overall impact and novelty of the paper is relatively weak."
"abstract | weakness | rating_summary  ==> This paper proposed a variational autoencoder-based method for semi-supervised dependency parsing. Given an input sentence s, an LSTM-based encoder generates a sentence embedding z, and a NN of Kiperwasser & Goldberg (2016) generates a dependency structure T. Gradients over the tree encoder are approximated by (1) adding a perturbation matrix over the weight matrix and (2) relax dynamic programming-based parsing algorithm to a differentiable format. The decoder combines standard LSTM and Graph Convolutional Network to generate the input sentence from z and T. The authors evaluated the proposed method on three languages, using 10% of the original training data as labeled and the rest as unlabeled data. <sep> Pros <sep> 1. I like the idea of this sentence->tree->sentence autoencoder for semi-supervised parsing. The authors proposed a novel and nice way to tackle key challenges in gradient computation. VAE involves marginalization over all possible dependency trees, which is computationally infeasible, and the proposed method used a Gumbel-Max trick to approximate it. The tree inference procedure involves non-differentiable structured prediction, and the authors used a peaked-softmax method to address the issue. The whole model is fully differentiable and can be thus trained end to end. <sep> 2. The direction of semi-supervised parsing is useful and promising, not only for resource-poor languages, but also for popular languages like English. A successful research on this direction could be potentially helpful for lots of future work. <sep> Cons, and suggestions on experiments <sep> My main concerns are around experiments. Overall I think they are not strong enough to demonstrate that this paper has sufficient contribution to semi-supervised parsing. Below are details. <sep> 1. The current version only used 10% of original training data as labeled and the rest as unlabeled data. This makes the reported numbers way below existing state-of-the-art performance. For example, the SOTA UAS on English PTB has been >95%. Ideally, the authors should be able to train a competitive supervised parser on full training data (English or other languages), and get huge amount of unlabeled data from other sources (e.g. News) to further push up the performance. The current setting makes it hard to justify how useful the proposed method could be in practice. <sep> 2. The best numbers from the proposed model is lower than baseline (Kipperwasser & Goldberg) on English, and only marginally better on Swedish. This probably means the supervised baseline is weak, and it's hard to tell if the gains from VAE will retain if applied to a stronger supervised. <sep> 3. A performance curve with different amount of labeled and unlabeled data would be useful to better understand the impact of semi-supervised learning. <sep> 4. What's the impact of perturbation? One could simply use T=Eisner(W) as approximation. Did you observe any significant benefits from sampling? <sep> Other questions <sep> 1. What's the impact of keeping the tree constraint on dependencies during backpropagation?  Have you tried removing the tree constraint like previous work? <sep> 2. Are sentence embedding and trees generated from two separate LSTM encoders? Are there any parameter sharing between the two?","This paper proposes a method for unsupervised learning that uses a latent variable generative model for semi-supervised dependency parsing. The key learning method consists of making perturbations to the logits going into a parsing algorithm, to make it possible to sample within the variational auto-encoder framework. Significant gains are found through semi-supervised learning. <sep> The largest reviewer concern was that the baselines were potentially not strong enough, as significantly better numbers have been reported in previous work, which may have a result of over-stating the perceived utility. <sep> Overall though it seems that the reviewers appreciated the novel solution to an important problem, and in general would like to see the paper accepted."
"rebuttal_process | rating_summary | strength | suggestion  ==> REVISION AFTER REBUTTAL <sep> While the revision does not address all of my concerns about clarity, it is much better. I still think that the introduction is overly long and the subsequent sections repeat information; if this were shortened there could be room for some of the figures that are currently in appendix. I appreciate the new figures; I think that it would be great if especially figure 10 were included in the main paper. <sep> I agree with the other two reviewers that the work is somewhat incremental, but the differences are well explained, the experimental results are interesting (particularly the differences of parameter vs representation-based sparsity, and the plots in appendix showing neuron importance over tasks), and the progression from SNI to SLNID is well-presented.  I think overall that this paper is a good contribution and I recommend acceptance. I have updated my review to a 7. <sep> =============== <sep> ""Activations"" ""Representation"" and ""Outputs"" are used somewhat interchangably throughout the work; for anyone not familiar it might be worth mentioning something about this in the intro. <sep> Problem setting is similar to open set learning (classification); could be worth mentioning algorithms for this in the related work which attempt to set aside capacity for later tasks. <sep> Results are presented and discussed in the introduction, and overall the intro is a bit long, resulting in parts later being repetitive. <sep> Worth discussing sparsity vs. distributed representations in the intro, and how/where we want sparsity while still having a distributed representation. <sep> Should be made clear that this is inspired by one kind of inhibition, and there are many others (i.e. inhibition in the brain is not always about penalizing neurons which are active at the same time, as far as I know) <sep> Changes in verb tense throughout the paper make it hard to follow sometimes. Be consistent about explaining equations before or after presenting them, and make sure all terms in the equation are defined (e.g. SNI with a hat is used before definition). Improper or useless ""However"" or ""On the other hand"" to start a lot of sentences. <sep> Figure captions could use a lot more experimental insight and explanation - e.g. what am I supposed to take away from Figure 10 (in appendix B4), other than that the importance seems pretty sparse? It looks to me like there is a lot of overlap in which neurons are important or which tasks, which seems like the opposite of what the regularizer was trying to achieve. This is a somewhat important point to me; I think this interesting and I'm glad you show it, but it seems to contradict the aim of the regularizer. <sep> How does multi-task joint training differ from ""normal"" classification? The accuracies especially for CIFAR seem very low. <sep> Quality: 7/10 interesting and thoughtful proposed regularizer and experiments; I would be happy to increase this rating if the insights from experiments, especially in the appendix, are a bit better explained <sep> Clarity:  6/10 things are mostly clearly explained although frequently repetitive, making them seem more confusing than they are. If the paper is reorganized and the writing cleaned up I would be happy to increase my rating because I think the work is good. <sep> Originality: 8/10 to my knowledge the proposed regularizer is novel, and I think think identifying the approach of ""selfless"" sequential learning is valuable (although I don't like the name) <sep> Significance: 7/10 I am biased because I'm interested in LLL, but I think these problems should receive more attention. <sep> Pros: <sep> - proposed regularizer is well-explained and seems to work well, ablation study is helpful <sep> Cons: <sep> - the intro section is almost completely repetitive of section 3 and could be significantly shortened, and make more room for some of the experimental results to be moved from the appendix to main text <sep> - some wording choices and wordiness make some sentences unclear, and overall the organization and writing could use some work <sep> Specific comments / nits: (in reading order) <sep> 1. I think the name ""selfless sequential learning"" is a bit misleading and sounds like something to do with multiagent cooperative RL; I think ""forethinking"" or something like that that is an actual word would be better, but I can't think of a good word... maybe frugal? <sep> 2.  Mention continual/lifelong learning in the abstract <sep> 3. ""penalize changes"" maybe ""reduce changes"" would be better? <sep> 4. ""in analogy to parameter importance"" cite and explain parameter importance <sep> 5. ""advocate to focus on selfless SL"" focus what? For everyone doing continual learning to focus on methods which achieve that through leaving capacity for later tasks? This seems like one potentially good approach, but I can imagine other good ones (e.g. having a task model) <sep> 6. LLL for lifelong learning is defined near the end of the intro, should be at the beginning when first mentioned <sep> 7. ""lies at the heart of lifelong learning"" I would say it is an ""approach to lifelong learning"" <sep> 8. ""fixed model capacity"" worth being specific that you mean (I assume) fixed architecture and number of parameters <sep> 9. ""those parameters by new tasks"" cite this at the end of the sentence, otherwise it is unclear what explanation goes with which citation <sep> 10.  ""hard attention masks, and stored in an embedding"" unclear what is stored in the embedding. It would be more helpful to explain how this method relates to yours rather than just describing what they do. <sep> 11. I find the hat notation unclear; I think it would be better just to have acronyms for each setting and write out the acronyms in the caption <sep> 12.""richer representation is needed and few active neurons can be tolerated"" should this be ""more active neurons""? <sep> 13. Comparison with state of the art section is repetitive of the results sections","Two of the reviewers raised their scores during the discussion phase noting that the revised version was clearer and addressed some of their concerns. As a result, all the reviewers ultimately recommended acceptance. They particularly enjoyed the insights that the authors shared from their experiments and appreciated that the experiments were quite thorough. All the reviewers mentioned that the work seemed somewhat incremental, but given the results, insights and empirical evaluation decided that it would still be a valuable contribution to the conference. One reviewer added feedback about how to improve the writing and clarity of the paper for the camera ready version."
"decision | strength  ==> This paper introduced a new architecture for input embeddings of neural language models: adaptive input representation (ADP). ADP allowed a model builder to define a set of bands of input words with different frequency where frequent words have larger embedding size than the others. The embeddings of each band are then projected into the same size. This resulted in lowering the number of parameters. <sep> Extensive experiments with the Transformer LM on WikiText-103 and Billion Word corpus showed that ADP achieved competitive perplexities. While tying weight with the output did not benefit the perplexity, it lowered the runtime significantly on Billion Word corpus. Further analyses showed that ADP gained performance across all word frequency ranges. <sep> Overall, the paper was well-written and the experiments supported the claim. The paper was very clear on its contribution. The variable-size input of this paper was novel as far as I know. However, the method, particularly on the weight sharing, lacked a bit of important background on adaptive softmax. The weight sharing was also needed further investigation and experimental data on sharing different parts. <sep> The experiments compared several models with different input levels (characters, BPE, and words). The perplexities of the proposed approach were competitive with the character model with an advantage on the training time. However, the runtimes were a bit strange. For example, ADP and ADP-T runtimes were very close on WikiText-103 dataset but very different on Billion Word corpus (Table 3 and 4). The runtime of ADP seemed to lose in term of scaling as well to BPE. Perhaps, the training time was an artifact of multi-GPU training. <sep> Questions: <sep> 1. I am curious about what would you get if you use ADP on BPE vocab set? <sep> 2. How much of the perplexity reduction of 8.7 actually come from ADP instead of the transformer and optimization?","There is a clear consensus among the reviews to accept this submission thus I am recommending acceptance. The paper makes a clear, if modest, contribution to language modeling that is likely to be valuable to many other researchers."
"abstract | rebuttal_process | strength | decision | rebuttal_process | strength  ==>  ==> This is a well-motived paper that considers bridging the gap in discrete-time continuous-state/action optimal control by approximating the system dynamics with a convex model class. <sep> The convex model class has more representational power than linear model classes while likely being more tractable and stable than non-convex model classes. <sep> They show empirical results in Mujoco continuous-control environments and in an HVAC example. <sep> I think this setup is a promising direction but I have significant concerns with some of the details and claims in this work: <sep> 1. Proposition 2 is wrong and the proposed input-convex recurrent neural network architecture not input-convex. <sep> To fix this, the D1 parameters should also be non-negative. <sep> To show why the proposition is wrong, consider the convexity of y2 <sep> with respect to x1, using g to denote the activation function: <sep> z1 = g(U x1 + ...) <sep> y2 = g(D1 z1 + ...) <sep> Thus making y2 = g(D1 g(U x1 + ...) + ...) <sep> y2 is *not* necessarily convex with respect to x1 because D1 takes an unrestricted weighted sum of the convex functions g(U x1 + ...) <sep> With the ICRNN architecture as described in the paper not being input-convex, I do not know how to interpret the empirical findings in Section 4.2 that use this architecture. <sep> 2. I think a stronger and more formal argument should be used to show that Equation (5) is a convex optimization problem as claimed. <sep> It has arbitrary convex functions on the equality constraints that are composed with each other and then used in the objective. <sep> Even with parts of the objective being convex and non-decreasing as the text mentions, it's not clear that this is sufficient when combined with the composed functions in the constraints. <sep> 3. I have similar concerns with the convexity of Equation (6). <sep> Consider the convexity of x3 with respect to u1, where g is now an input-convex neural network (that is not recurrent): <sep> x3 = g(g(x1, u1), u2) <sep> This composes two convex functions that do *not* have non-decreasing properties and therefore introduces an equality constraint that is not necessarily even convex, almost certainly making the domain of this problem non-convex. I think a similar argument can be used to show why Equation (5) is not convex. <sep> In addition to these significant concerns, I have a few other minor comments. <sep> 1. Figure 1 hides too much information. It would be useful to know, <sep> for example, that the ICNN portion at the bottom right is solving a control optimization problem with an ICNN as part of the constraints. <sep> 2. The theoretical results in Section 3 seem slightly out-of-place within the broader context of this paper but are perhaps of standalone interest. <sep> Due to my concerns above I did not go into the details in this portion. <sep> 3. I think more information should be added to the last paragraph of <sep> Section 1 as it's claimed that the representational power of <sep> ICNNs and ""a nice mathematical property"" help improve the computational time of the method, but it's not clear why this is and this connection is not made anywhere else in the paper. <sep> 4. What method are you using to solve the control problems in <sep> Eq (5) and (6)? <sep> 5. The empirical setup and tasks seems identical to [Nagabandi et al.]. <sep> Figure 3 directly compares to the K=100 case of their method. <sep> Why does Fig 6 of [Nagabandi et al.] have significantly higher rewards for their method, even in the K=5 case? <sep> 6. In Figure 5, f_NN seems surprisingly bad in the red region of the data on the left side. Is this because the model is not using many parameters? What are the sizes of the networks used?","The paper makes progress on a problem that is still largely unexplored, presents promising results, and builds bridges with <sep> prior work on optimal control. It designs input convex recurrent neural networks to capture temporal behavior of <sep> dynamical systems; this then allows optimal controllers to be computed by solving a convex model predictive control problem. <sep> There were initial critiques regarding some of the claims. These have now been clarified. <sep> Also, there is in the end a compromise between the (necessary) approximations of the input-convex model and the true dynamics, and being able to compute an optimal result. <sep> Overall, all reviewers and the AC are in agreement to see this paper accepted. <sep> There was extensive and productive interaction between the reviewers and authors. <sep> It makes contributions that will be of interest to many, and builds interesting bridges with known control methods."
"abstract | strength | decision  ==> This paper proposes guiding program synthesis with information from partial/incomplete program execution. The idea is that by executing partial programs, synthesizers can obtain the information of the state the (partial) program ended in and can, therefore, condition the next step on that (intermediate) state. The paper also mentions ensembling synthesizers to achieve a higher score, and by doing that it outperforms the current state-of-the-art on the Karel dataset program synthesis task. <sep> In general, I like the idea of guiding synthesis with intermediate executions, and the evaluation in the paper shows this does make sense, and it outperforms the SOTA. The idea is original and the evaluation shows it is significant (enough). However, I have two major concerns with the paper, its presented contribution, and the clarity. <sep> First, I cannot accept ensembling as a contribution to this paper. There is nothing novel about the ensemble proposed, and ensembling, as a standard method that pushes models that extra few percentage points, is present in a lot of other research. I have nothing against achieving SOTA results with it, while at the same time showing that the best performing model outperforms previous SOTA, which this paper orderly does. However, I cannot accept non-novel ensembling as a contribution of the paper. <sep> Second, the clarity of the paper should be substantially improved: <sep> - my main issue is that it is not clear how the Exec algorithm (see next point too) is trained. From what I understand Exec is trained on supervised data via MLE. What is the supervised data here?  Given the generality claims and the formulation in Algorithm 1/2, and possible ways one could use the execution information, as well as the fact that the model should be end-to-end trainable via MLE, it seems to me that the model is trained on prefixes (defined by Algorithm 1/2) of programs. Whether this is correct or not, please provide full details on how one can train Exec without using RL. <sep> - By looking at Table 3, it seems that the generalization boost coming from Exec (I'm ignoring ensembling) is higher enough, and that's great. However, it's obvious that the exact match gain by Exec is minute, implying that the proposed algorithm albeit great on the generalization metric, does not improve the exact match at all. Do you have any idea why is that? Is that because Exec is trained via MLE and the Exec algorithm doesn't add anything new to the training procedure? <sep> - how do algorithm 1 and 2 exactly relate? I guess there is a meaning of ellipses in Lines 1 and 13, however, that is not mentioned anywhere. Is the mixture of algorithm 1 and 2 (and a non-presented algorithm for while loops) the Exec algorithm? How exactly are these algorithms joined, i.e what is the final algorithm? <sep> - while on one side, I find some formalizations (problem definitions, definition 1, semantic rules in table 2) nicely done, I do not see their necessity nor big gains from them. In my opinion, the understanding of the rest of the paper does not depend on them, and they are well-described in the text. <sep> - the paper says that the algorithm ""helps boost the performance of different existing training algorithms"", however, it does so only on the Bunel et al model (and the MLE baseline in it), and albeit there's mention of the generality, it has not been shown on anything other than those two models and the Karel dataset. <sep> - do lines 6-7 in Algorithm 2 recurse? Does the model support arbitrarily nested loops/if statements? <sep> - The claim that the shortest principle is most effective is supported by 2 data points, without any information on the variance of the prediction/dependence on the seed. Did you observe this for #models > 10 too? Up to what number? <sep> - In table 3, is Exec on MLE? Could you please, for completeness, present the results of Exec + RL + ensemble in the table too? <sep> - summarization, point 3 - what are the different modules mentioned here? Exec/RL/ensemble? <sep> Minor issues, remarks, typos: <sep> - table 1 position is very unfortunate <sep> - figure 1 is not self-explanatory - it takes quite a lot of space to explain the network architecture, yet it fails to deliver meaning to parts of it (e.g. what is h_t^x, why is it max-pooled, what is g_t, etc) <sep> - abstract & introduction - ""Reducing error rate around 60%"" absolute percentage points seem like a better evaluation measure (that the paper does use). Why is the error rate reduction necessary here? <sep> - figure 2 - why is the marker in one of the corners, and not in the cell itself? <sep> - Algorithm 1, step 4, is this here just as initialization, so S is non-empty to start with? <sep> - Table 2 rule names are unclear (e.g. S-Seq-Bot ?) <sep> - Table 3 mentions what Exec indicates twice","This paper presents a system which exploits semantic information of partial programs during program synthesis, and ensembling of synthesisers. The idea is general, and admirably simple. The explanation is clear, and the results are impressive. The reviewers, some after significant discussion, agree that this paper makes an import contribution and is one of the stronger papers in the conference. While some possible improvements to the method and experiment were discussed with the reviewers, it seems these are more suitable for future research, and that the paper is clearly publishable in its current form."
"abstract | strength | rebuttal_process | decision  ==>  ==> Summary: <sep> A method is presented for initialization and normalization of deep residual networks. The method is based on interesting observations regarding forward and backward explosion in such networks with the standard Xavier or (He, 2015) initializations. Experiments with the new method show that it is able to learn with very deep networks, and that its performance is on a par with the best results obtained by other networks with more explicit normalization. <sep> Advantages: <sep> - The paper includes interesting observations, resulting in two theorems,  which show the sensitivity of traditional initializations in residual networks <sep> - The method presented seems to work comparable to other state of the art initialization + normalization methods, providing overall strong empirical results. <sep> Disadvantages: <sep> - The authors claim to suggest a method without normalization, but the claim is misleading: the network has additive and multiplicative normalization nodes, and their function and placement is at least as 'mysterious'  as the role of normalization in methods like batch and layer normalization. <sep> o This significantly limits the novelty of the method: it is not 'an intialization' method, but a combination of initialization and normalization, which differ from previous ones in some details. <sep> - The method includes 3 components, of which only one is justified in a principled manner. The other components are not justified neither by an argument, nor by experiments. Without such experiments, it is not clear what actually works in this method, and what is not important. <sep> - The argument for the 'justified' component is not entirely clear to me. The main gist is fine, but important details are not explained so I could not get the entire argument step-by-step. This may be a clarity problem, or maybe indicate deeper problem of arbitrary decisions made without justification – I am not entirely sure. Such lack of clear argumentation occurs in several places <sep> - Experiments isolating the contribution of the method with respect to traditional initializations are missing (for example: experiments on Cifar10 and SVHN showing the result of traditional initializations with all the bells and whistles (cutout, mixup) as the zeroInit gets. <sep> More detailed comments: <sep> Page 3: <sep> - While I could follow the general argument before eq. 2, leading to the conclusion that the initial variance in a resnet explodes exponentially, I could not understand eq. 2. What is its justification and how is it related to the discussion before it? I think it requires some argumentation. <sep> Page 4: <sep> - I did not understand example 2) for a p.h. set. I think an argument, reminder of the details of resnet, or a figure are required. <sep> - I could not follow the details of the argument leading to the zeroInit method: <sep> o How is the second design principle ""Var[F_l(x_l)] = O( 1/L) justified? <sep> As far as I can see, having Var[F_l(x_l)] = 1/L will lead to output variance of (1+1/L)^L =~e, which is indeed O(1). Is this the argument? Is yes, why wasn't it stated? Also: why not smaller than O(1/L)? <sep> o Following this design principle several unclear sentences are stated: <sep>  We strive to make Var[F_l(x_l)] = 1/L, yet we set the last convolutional layer in the branch to 0 weights. Does not it set Var[F_l(x_l)] = 0, in contradiction to the 1/L requirement? <sep>   ""Assuming the error signal passing to the branch is O(1),"" – what does the term ""error signal"" refer to? How is it defined? Do you refer to the branch's input? <sep>  I understand why the input to the m-th layer in the branch is O(\\Lambda^m-1) if the branch input is O(1) but why is it claimed that ""the overall scaling of the residual branch after update is O(\\lambda^(2m-2))""? what is 'the overall scaling after update' (definition) and why is it the square of forward scaling? <sep> - The zero Init procedure step 3 is not justified by any argument in the proceeding discussion. Is there any reason for this policy? Or was it found by trial and error and is currently unjustified theoretically (justified empirically instead). This issue should be clearly elaborated in the text. Note that the addition of trainable additive and multiplicative elements is inserting the normalization back, while it was claimed to be eliminated. If I understand correctly, the 'zeroInit' method is hence not based on initialization (or at least: not only on initialization), but on another form of normalization, which is not more justified than its competitors (in fact it is even more mysterious: what should we need an additive bias before every element in the network?) <sep> Page 5: <sep> - What is \\sqrt(1/2) scaling? It should be defined or given a reference. <sep> Page 6: <sep> - It is not stated on what data set figure 2 was generated. <sep> - In table 2, for Cifar-10 the comparison between Xavier init and zeroInit shows only a small advantage for the latter. For SVHN such an experiment is completely missing, and should be added. <sep> o It raises the suspect the the good results obtained with zeroInit in this table are only due to the CutOut and mixup used, that is: maybe such results could be obtained with CutOut+Mixup without zero init, using plain Xavier init? experiments clarifying this point are also missing. <sep> Additional missing experiments: <sep> - It seems that  ZeroInit includes 3 ingredients (according to the box in page 4), among which only one (number 2) is roughly justified from the discussion.  Step 1) of zeroing the last layer in each branch is not justified –why are we zeroing the last layer and not the first, for example? Step 3 is not even discussed in the text – it appear without any argumentation. For such steps, empirical evidence should be brought, and experiments doing this are missing. Specifically experiments of interest are: <sep> o Using zero init without its step 3: does it work? The theory says it should. <sep> o Using only step 3 without steps 1,2. Maybe only the normalization is doing the magic? <sep> The paper is longer than 8 pages. <sep> I have read the rebuttal. <sep> Regarding normalization: I think that there are at least two reasonable meanings to the word 'normalziation': in the wider sense is just means mechanism for reducing a global constant (additive normalization) and dividing by a global constant (multiplicative normalization). In this sense the constant parameters can be learnt in any way. In the narrow sense the constants have to be statistics of the data. I agree with the authors that their method is not normalization in sense 2, only in sense 1. Note that keeping the normalization in sense 1 is not trivial (why do we need these normalization operations? at least for the multiplicative ones, the network has the same expressive power without them).  I think the meaning of normalization  should be clearly explained in the claim for 'no  normalization'. <sep> Regarding additional mathematical and empirical justifications required: I think such justifications are missing in the current paper version and are not minor or easy to add. I believe the work should be re-judged after re-submission of a version addressing the problems.","The paper explores the effect of normalization and initialization in residual networks, motivated by the need to avoid exploding and vanishing activations and gradients. Based on some theoretical analysis of stepsizes in SGD, the authors propose a sensible but effective way of initializing a network that greatly increases training stability. In a nutshell, the method comes down to initializing the residual layers such that a single step of SGD results in a change in activations that is invariant to the depth of the network. The experiments in the paper provide supporting evidence for the benefits; the authors were able to train networks of up to 10,000 layers deep. The experiments have sufficient depth to support the claims. Overall, the method seems to be a simple but effective technique for learning very deep residual networks. <sep> While some aspects of the network have been used in earlier work, such as initializing residual branches to output zeros, these earlier methods lacked the rescaling aspect, which seems crucial to the performance of this network. <sep> The reviewers agree that the papers provides interesting ideas and significant theoretical and empirical contributions. The main concerns by the reviewers were addressed by the author responses. The AC finds that the remaining concerns raised by the reviewers are minor and insufficient for rejection of the paper."
"abstract | rating_summary  ==>  ==> The paper proposes a multi-domain music translation method. The model presents a Wavenet auto-encoder setting with a single (domain independent) encoder and multiple (domain specific) decoders.  From the model perspective, the paper builds up on several exciting ideas such as Wavenet and autoencoder based translation models that can perform the domain conversion without relying on parallel datasets. The two main modifications are the use of data augmentation, the use of multiple decoders (rather a single decoder conditioned on the output domain identity) and the use of a domain confusion loss to prevent the latent space to encode domain specific information. This last idea has been also used on prior work. <sep> Up to my knowledge, this is the first autoencoder-based music translation method. While this problem is very similar to that of speaker conversion, modeling musical audio signal (with many instruments) is clearly more challenging. <sep> Summarizing, I think that the contributions in terms of methods are limited, but the results are very interesting. The paper gives an affirmative answer to the question of whether existing models could be adapted to handle the case of music translation, which is of value. The paper would be stronger in my view, if stronger baselines would be included. This would show that the technical contributions are better than alternative methods. Please read bellow some further comments and questions. <sep> The authors perform two ablation studies: eliminating data augmentation and the domain confusion network. In both cases, the model without this add on fails to train. However, it seems to me that different studies are important. <sep> The paper seems to be missing baselines. The authors could compare their work with that of VQ-VAE. The authors claim that they could not make VQ-VAE work on this problem. The cited work by Dieleman et al provides some improvements to adapt VQ-VAE to be better suited to the music domain. Did you evaluate also autoregressive discrete autoencoders? <sep> The proposed method uses an individual decoder per domain. This is unlike other conversion methods (such as the speech conversion studied in VQ-VAE). This modification is very costly and provides a very large capacity. Have you tried having a single decoder which is also conditioned on a one-hot vector indicating the domain? Is it reasonable to expect some transfer between domains or are they too different? Maybe this is the motivation behind using many decoders. It would be good to clarify. <sep> I understand that the emphasis of this work is on music translation, however, the model doesn't have anything specific to music. In that regard, maybe a way to compare to VQ-VAE is to run the proposed method to the voice conversion of the VQ-VAE. <sep> Have you tried producing samples using the decoder in an unconditional setting? <sep> The authors claim that the learned representation is disentangled. Why is this the case? Normally a representation is said to be disentangled if different properties are represented in different (disjoint) coordinates. I might not be understanding what is meant here. <sep> The loss used by the authors, encourages the latent representation to not have domain specific information. The authors should cite the work [A], which has very similar motivation. It would be interesting to report the classification accuracy of the classifier to see how much of the domain information is left in the latent codes. Is it reduced to chance? <sep> In Section 3.1 the authors describe some modifications to nv-wavenet. I imagine that this is because it leads to better performance or faster training. It would be good to give some more information. Did you perform ablation studies for these? <sep> In the human lineup experiment (Figure 2 b,c and d). While the listeners fail to select the correct source, many of the domains are never chosen. This could suggest that some translations are consistently poorer than others or the translations themselves are poor. This cannot be deduced from this experiment. Have you evaluated this?  Maybe it would be better to present pairs of audios with reconstruction and a translation. <sep> While I consider the results quite good, I tend to agree with the posted public comment. It is very hard to claim that the model is effectively transferring styles. A perceptual test should include the question: is this piece on this given style? As the authors mentioned, it is clearly very difficult to evaluate generative models. But maybe the claims could be toned down. <sep> [A] Louizos, Christos, et al. ""The variational fair autoencoder."" arXiv preprint arXiv:1511.00830 (2015).","The paper describes a method which, given a music waveform, generates another recording of the same music which should sound as if it was performed by different instruments. The model is an auto-encoder with a WaveNet-like domain-specific decoder and a shared encoder, trained with an adversarial ""domain confusion loss"". Even though the method is constructed mostly from existing components, the reviewers found the results interesting and convincing, and recommended the paper for acceptance."
"abstract | strength | suggestion | strength | rating_summary  ==> This paper presents the NeuroSAT architecture, which uses a deep, message passing neural net for predicting the satisfiability of CNF instances. The architecture is also able to predict a satisfiable assignment in the SAT case, and the literals involved in some minimal conflicting set of clauses (i.e. core) in the UNSAT case. The NeuroSAT architecture is based on a vector space embedding of literals and clauses, which exploits (with message passing) some important symmetries of SAT instances (permutation invariance and negation invariance). This architecture is tested on various classes of random SAT instances, involving both unstructured (RS) problems, and structured ones (e.g. graph colorings, vertex covers, dominating sets, etc.). <sep> Overall the paper is well-motivated, and the experimental results are quite convincing. Arguably, the salient characteristic of NeuroSAT is to iteratively refine the confidence of literals voting for the SAT - or UNSAT - output, using a voting scheme on the last iteration of the literal matrix. This is very interesting, and NeuroSAT might be used to help existing solvers in choosing variable orderings for tackling hard instances, or hard queries (e.g. find a core). <sep> On the other hand, the technical description of the architecture (sec. 3) is perhaps a little vague for having a clear intuition of how the classification task - for SAT instances - is handled in the NeuroSAT architecture. Namely, a brief description of the initial matrices (which encode the literal en clause embeddings) would be nice. Some comments on the role played by the multilayer perceptron units and the normalization units would also be welcome. The two update rules in Page 3 could be explained in more detail. For the sake of clarity, I would suggest to provide a figure for depicting a transition (from iteration t-1 to iteration t) in the architecture. As a minor comment, it would be nice (in Section 2) to define the main parameters n, m, and d used in the rest of the paper. <sep> Concerning the experimental part of the paper, Sections 4 & 5 are well-explained but, in Section 6,  the solution decoding method, inspired from PCA is a bit confusing. Specifically, we don't know how a satisfying assignment is extracted from the last layer, and this should be explained in detail. According to Figure 4 and the comments above, it seems that a clustering method (with two centroids) is advocated, but this is not clear. In Table 1, the correlation between the accuracy on SAT instances, and the percent of SAT instances solved is not clear. Is the ratio of 70% measured on the CNF instances which have been predicted to be satisfiable? Or, is this ratio measured on the whole set of test instances? Finally, for the results established in Table 1, how many training instances and test instances have been used? <sep> In Section 7, some important aspects related to experiments, are missing. In Sec 7.1, for SR(200) tasks, was NeuroSAT tested on the same conditions as those for SR(40) tasks? Notably, what is the input dimension d of the embedding space here? (I guess that d=128 is too small for such large instances). Also, how many training and test instances have been used to plot the curves in Figure 5? For the 4,888 satisfiable instances generated in Sec. 7.2, which solver have been used to determine the satisfiability of those instances (I guess it is Minisat, but this should be mentioned somewhere). <sep> In Section 8, I found interesting the the ability of NeuroSAT in predicting the literals that participate in an UNSAT core. Indeed the problem of finding an UNSAT core in CNF instances is computationally harder than determining the satisfiability of this instance. So, NeuroSAT might be used here to help a solver in finding a core. But the notion of ""confidence"" should be explained in more detail in this section, and more generally, in the whole paper. Namely, it seems that in the last layer of each iteration, literals are voting for SAT (red colors) with some confidence (say δ)  and voting for UNSAT (blue colors) with some confidence (say δ′). Are δ and δ′ correlated in the neural architecture? And, how confidences for UNSAT votes are updated? <sep> Finally, I found that the different benchmarks where relevant, but I would also suggest (for future work, or in the appendix) to additionally perform experiments on the well-known random 3-SAT instances (k is fixed to 3). Here, it is well-known that a phase transition (on the instances, not the solver/learner) occurs at 4.26 for the clause/variable ratio. A plot displaying the performance of NeuroSAT (accuracy in predicting the label of the instance) versus the clause/variable ratio would be very helpful in assessing the robustness of NeuroSAT on the so-called ""hard"" instances (which are close to 4.26). By extension, there have been a lot of recent work in generating ""pseudo-industrial"" random SAT instances, which incorporate some structure (e.g. communities) in order to mimic real-world structured SAT instances. To this point, it would be interesting to analyze the performance of NeuroSAT on such pseudo-industrial instances.","The submission proposes a machine learning approach to directly train a prediction system for whether a boolean sentence is satisfiable. The strengths of the paper seem to be largely in proposing an architecture for SAT problems and the analysis of the generalization performance of the resulting classifier on classes of problems not directly seen during training. <sep> Although the resulting system cannot be claimed to be a state of the art system, and it does not have a correctness guarantee like DPLL based approaches, the paper is a nice re-introduction of SAT in a machine learning context using deep networks. It may be nice to mention e.g. (W. Ruml. Adaptive Tree Search. PhD thesis, Harvard University, 2002) which applied reinforcement learning techniques to SAT problems. The empirical validation on variable sized problems, etc. is a nice contribution showing interesting generalization properties of the proposed approach. <sep> The reviewers were unanimous in their recommendation that the paper be accepted, and the review process attracted a number of additional comments showing the broader interest of the setting."
"abstract | strength  ==> Summary: <sep> This paper studies the properties of applying gradient flow and gradient descent to deep linear networks on linearly separable data. For strictly decreasing loss like the logistic loss, this paper shows 1) the loss goes to 0, 2) for every layer the normalized weight matrix converges to a rank-1 matrix 3) these rank-1 matrices are aligned. For the logistic loss, this paper further shows the linear function is the maximum margin solution. <sep> Comments: <sep> This paper discovers some interesting properties of deep linear networks, namely asymptotic rank-1, and the adjacent matrix alignment effect. These discoveries are very interesting and may be useful to guide future findings for deep non-linear networks. The analysis relies on many previous results in Du et al. 2018, Arora et al. 2018 and Soudry et al. 2017  authors did a good job in combing them and developed some techniques to give very interesting results. <sep> There are two weaknesses. First, there is no convergence rate. Second, the step size assumption (Assumption 5) is unnatural. If the step size is set proportional to 1/t or 1/t^2  does this setup satisfies this assumption? <sep> Overall I think there are some interesting findings for deep linear networks and some new analysis presented, so I think this paper is above the bar. <sep> However, I don't think this is a strong theory people due to the two weakness I mentioned.","This paper studies the behavior of weight parameters for linear networks when trained on separable data with strictly decreasing loss functions. For this setting the paper shows that the gradient descent solution converges to max margin solution and each layer converges to a rank 1 matrix with consequent layers aligned. All reviewers agree that the paper provides novel results for understanding implicit regularization effects of gradient descent for linear networks. Despite the limitations of this paper such as studying networks with linear activation, studying gradient descent not with practical step sizes, assuming data is linearly separable, reviewers find the results useful and a good addition to existing literature."
"strength | weakness | decision  ==> Given an additively decomposable function F(X, Q) = sum_over_x_in_X cost(x, Q), one can approximate it using either random sampling of x in X (unbiased, possibly high variance), or using importance sampling and replace the sum_over_x with a sum_over_coreset importance_of_a_point * cost(x, Q) which if properly defined can be both unbiased and have low variance [1]. In this work the authors consider the weighted sum of activations as F and suggest that for each neuron we can subsample the incoming edges. To construct the importance sampling strategy the authors adapt the classic notion of sensitivity from the coreset literature. Then, one has to carefully balance the approximation quality from one layer to the next and essentially union bound the results over all layers and all sampled points. The performed analysis is sound (up to my knowledge). <sep> Pro: <sep> - I commend the authors for a clean and polished writeup. <sep> - The analysis seems to be sound (apart from the issues discussed below) <sep> - The experimental results look promising, at least in the limited setup. <sep> Con: <sep> - There exists competing work with rigorous guarantees, for example [2]. <sep> - The analysis hinges on two assumptions which, in my opinion, make the problem feasible: having (sub) exponential tails allows for strong concentration results, and with proper analysis (as done by the authors), the fact that the additively decomposable function can be approximated given well-behaving summands is not surprising. The analysis is definitely non-trivial and I commend the authors for a clean writeup. <sep> - While rigorous guarantees are lacking for some previous work, previously introduced techniques were shown to be extremely effective in practice and across a spectrum of tasks. As the guarantees arguably stem from the assumptions 1 and 2, I feel that it's unfair to not compare to those results empirically. Hence, failing to compare to results of at least [2, 3] is a major drawback of this work. <sep> - The result holds for n points drawn from P. However, in practice the network might receive essentially arbitrary input from P at inference time. Given that we need to decide on the number of edges to preserve apriori, what are the implications? <sep> - The presented bounds should be discussed on an intuitive level (i.e. the number of non zero entries is approximately cubic in L). <sep> I consider this to be a well-executed paper which brings together the main ideas from the coreset literature and shows one avenue of establishing provable results. However, given that no comparison to the state-of-the-art techniques is given I'm not confident that the community will apply these techniques in practice. On the other hand, the main strength -- the theoretical guarantees -- hinge on the introduced assumptions. As such, without additional empirical results demonstrating the utility with respect to the state-of-the-art methods (for the same capacity in terms of NNZ) I cannot recommend acceptance. <sep> [1] https://arxiv.org/abs/1601.00617 <sep> [2] papers.nips.cc/paper/6910-net-trim-convex-pruning-of-deep-neural-networks-with-performance-guarantee <sep> [3] https://arxiv.org/abs/1510.00149 <sep> ======== <sep> Thank you for the detailed responses. Given the additional experimental results and connections to existing work, I have updated my score from 5 to 6.","The reviewers and AC note that the strength of the paper includes a) an interesting compression algorithm of neural networks with provable guarantees (under some assumptions), b) solid experimental comparison with the existing *matrix sparsification* algorithms. The AC's main concern of the experimental part of the paper is that it doesn't outperform or match the performance of the ""vanilla"" neural network compression algorithms such as Han et al'15. The AC decided to suggest acceptance for the paper but also strongly encourage the paper to clarify the algorithms in comparison don't include state-of-the-art compression algorithms."
"abstract | strength  ==> In this paper the authors present mutual posterior divergence regularization, a data-dependent regularization for the ELBO that enforces diversity and smoothness of the variational posteriors. The experiments show the effectiveness of the model for density estimation and representation learning. <sep> This is an interesting paper dealing with the important issues of fully exploiting the stochastic part of VAE models and avoiding inactive latent units in the presence of very expressive decoders. The paper reads well and is well motivated. <sep> The authors claim that their method is ""encouraging the learned variational posteriors to be diverse"". While it is important to have models that can use well the latent space, the constraints that are encoded seem too strong. If two data points are very similar, why should there be a term encouraging their posterior approximation to be different? In this case, their true posteriors will be in fact be similar, so it seems counter-intuitive to force their approximations to be different. <sep> The numerical results seem promising, but I think they could be further improved and made more convincing. <sep> - For the density estimation experiments, while there is an improvement in terms of NLL thanks to the new regularizer, it is not clear which is the additional computational burden. How much longer does it takes to train the model when computing all the regularization terms in the experiments with batch size 100? <sep> - I am not completely convinced by the claims on the ability of the regularizer to improve the learned representations. K-means implicitly assumes that the data manifold is Euclidean. However, as shown for example by [Arvanitidis et al. Latent space oddity: on the curvature of deep generative models, *CONF* 2018] and other authors, the latent manifold of VAEs is not Euclidean, and curved riemannian manifolds should be used when computing distances and performing clustering. Applying k-means in the high dimensional latent spaces of ResNet VAE and VLAE does not seem therefore a good idea. <sep> One possible reason why your MAE model may perform better in the unsupervised clustering of table 2 is that the terms added to the elbo by the regularizer may force the space to be more Euclidean (e.g. the squared difference term in the Gaussian KL) and therefore more suitable for k-means. <sep> - The semi-supervised classification experiment is definitely better to assess the representation learning capabilities, but KNN suffers with the same issues with the Euclidean distance as in the k-means experiments, and the linear classifier may not be flexible enough for non-euclidean and non-linear manifolds. Have you tried any other non-linear classifiers? <sep> - Comparisons with other methods that aim at making the model learn better representation (such as the kl-annealing of the beta-vae) would be useful. <sep> - The lack of improvements on the natural image task is a bit concerning for the generalizability of the results. <sep> Typos and minor comments: <sep> - devergence -> divergence in introduction <sep> - assistant -> assistance in 2.3 <sep> - the items (1) and (2) in 3.1 are not very clear <sep> - set -> sets in 3.2 <sep> - achieving -> achieve below theorem 1 <sep> - cluatering -> clustering in table 2","This paper proposes a solution for the well-known problem of posterior collapse in VAEs: a phenomenon where the posteriors fail to diverge from the prior, which tends to happen in situations where the decoder is overly flexible. <sep> A downside of the proposed method is the introduction of hyper-parameters controlling the degree of regularization. The empirical results show improvements on various baselines. <sep> The paper proposes the addition of a regularization term that penalizes pairwise similarity of posteriors in latent space. The reviewers agree that the paper is clearly written and that the method is reasonably motivated. The experiments are also sufficiently convincing."
"abstract | weakness | misc | weakness | suggestion  ==>  ==> In this paper the authors focus on the problem of weakly-supervised action localization. The authors state that a problem with weakly-supervised attention based methods is that they tend to focus on only the most salient regions and propose a solution to this which reduces the difference between the responses for the most salient regions and other regions. They do this by employing marginalized average aggregation to averaging a sample a subset of features in relation to their latent discriminative probability then calculating the expectation over all possible subsets to produce a final aggregation. <sep> The problem is interesting, especially noting that current attention methods suffer from paying attention to the most salient regions therefore missing many action segments in action localization. The authors build upon an existing weakly-supervised action localization framework, having identified a weakness of it and propose a solution. The work also pays attention to the algorithm's speed which is practically useful. The experiments also compare to several other potential feature aggregators. <sep> However, there are several weakness of the current version of the paper: <sep> - In parts the paper feels overly complicated, particularly in the method (section 2). It would be good to see more intuitive explanations of the concepts introduce here. For instance, the author's state that c_i captures the contextual information from other video snippets, it would be good to see a figure with an example video and the behaviour of p_i and c_i as opposed to lamba_i. I found it difficult to map p_i, c_i to z and lambda used elsewhere. <sep> - The experimental evidence does not show where the improvement comes from. The authors manage to acheieve a 4-5% improvement over STPN through their re-implemenation of the algorithm, however only have a ~2% improve with their marginalized average attention on THUMOS. I would like to know the cause in the increase over the original STPN results: is it a case of not being able to replicate the results of STPN or do the different parameter choices, such as use of leakly RELU, 20 snippets instead of 400 and only rejecting classes whose video-level probabilities are below 0.01 instead of 0.1, cause this big of an increase in results? There is also little evidence that the actual proposal (contextual information) is the reason for the reported improvement. <sep> - There seems to be several gaps in the review of current literature. Firstly, the authors refer to Wei et al. 2017 and Zhang et al. 2018b as works which erase the most salient regions to be able to explore regions other than the most salient. The authors state that the problem with these methods is that they are not end-to-end trainable, however Li et al. 2018 'Tell Me Where to Look': Guided Attention Inference Network' proposes a method which erases regions which is trainable end-to-end. Secondly, the authors do not mention the recent work W-TALC which performs weakly-supervised action localization and outperforms STPN. It would be good to have a baseline against this method. <sep> - The qualitative results in this paper are confusing and not convincing. It is true that the MAAN's activation sequence shows peaks which correspond to groundtruth and are not present in other methods. However, the MAAN activation sequence also shows several extra peaks not present in other methods and also not present in the groundtruth, therefore it looks like it is keener to predict the presence of the action causing more true positives, but also more false positives. It would be good to see some discussion of these failure cases and/or more qualitative results. The current figure could be easily compressed by only showing one instance of the ground-truth instead of one next to each method. <sep> I like the idea of the paper however I am currently unconvinced by the results that this is the correct method to solve the problem.","The paper proposes a new attentional pooling mechanism that potentially addresses the issues of simple attention-based weighted averaging (where discriminative parts/frames might get disportionately high attentions). A nice contribution of the paper is to propose an alternative mechanism with theoretical proofs, and it also presents a method for fast recurrent computation. The experimental results show that the proposed attention mechanism improves over prior methods (e.g., STPN) on THUMOS14 and ActivityNet1.3 datasets. In terms of weaknesses: (1) the computational cost may be quite significant. (2) the proposed method should be evaluated over several tasks beyond activity recognition, but it's unclear how it would work. <sep> The authors provided positive proof-of-concept results on weakly supervised object localization task, improving over CAM-based methods. However, CAM baseline is a reasonable but not the strongest method and the weakly-supervised object recognition/segmentation domains are much more competitive domains, so it's unclear if the proposed method would achieve the state-of-the-art by simply replacing the weighted-averaging-attentional-pooling with the proposed attention mechanism. In addition, the description on how to perform attentional pooling over images is not clearly described (it's not clear how the 1D sequence-based recurrent attention method can be extended to 2-D cases). However, this would not be a reason to reject the paper. <sep> Finally, the paper's presentation would need improvement. I would suggest that the authors give more intuitive explanations and rationale before going into technical details. The paper starts with Figure 1 which is not really well motivated/explained, so it could be moved to a later part. Overall, there are interesting technical contributions with positive results, but there are issues to be addressed."
"abstract | weakness | strength | weakness  ==>  ==> This paper presents a generative sequence model based on the dilated CNN <sep> popularized in models such as WaveNet. Inference is done via a hierarchical variational approach based on the Variational Autoencoder (VAE). While VAE <sep> approach has previously been applied to sequence modeling (I believe the earliest being the VRNN of Chung et al (2015)), the innovation where is the integration of a causal, dilated CNN in place of the more typical recurrent neural network. <sep> The potential advantages of the use of the CNN in place of <sep> RNN is (1) faster training (through exploitation of parallel computing across time-steps), and (2) potentially (arguably) better model performance. This second point is argued from the empirical results shown in the literature. The disadvantage of the CNN approach presented here is that these models still need to generate one sample at a time and since they are typically much deeper than the RNNs, sample generation can be quite a bit slower. <sep> Novelty / Impact: This paper takes an existing model architecture (the causal, dilated CNN) and applies it in the context of a variational approach to sequence modeling. It's not clear to me that there are any significant challenges that the authors overcame in reaching the proposed method. That said, it certainly useful for the community to know how the model performs. <sep> Writing: Overall the writing is fairly good though I felt that the model description could be made more clear by some streamlining -- with a single pass through the generative model, inference model and learning. <sep> Experiments: The experiments demonstrate some evidence of the superiority of this model structure over existing causal, RNN-based models. One point that can be drawn from the results is that a dense architecture that uses multiple levels of the latent variable hierarchy directly to compute the data likelihood is quite effective. This observation doesn't really bear on the central message of the paper regarding the use of causal, dilated CNNs. <sep> The evidence lower-bound of the STCN-dense model on MNIST is so good (low) <sep> that it is rather suspicious. There are many ways to get a deceptively good result in this task, and I wonder if all due care what taken. In particular, was the binarization of the MNIST training samples fixed in advance (as is standard) or were they re-binarized throughout training? <sep> Detailed comments: <sep> - The authors state ""In contrast to related architectures (e.g. (Gulrajani et al, 2016; Sonderby et al. 2016)), the latent variables at the upper layers capture information at long-range time scales"" I believe that this is incorrect in that the model proposed in at least Gulrajani et al also <sep> - It also seems that there is an error in Figure 1 (left). I don't think there should be an arrow between z^{2}_{t,q} and z^{1}_{t,p}. The presence of this link implies that the prior at time t would depend -- through higher layers -- on the observation at t. This would no longer be a prior at that point. By extension you would also have a chain of dependencies from future observations to past observations. It seems like this issue is isolated to this figure as the equations and the model descriptions are consistent with an interpretation of the model without this arrow (and including an arrow between z^{2}_{t,p} and z^{1}_{t,p}. <sep> - The term ""kla"" appears in table 1, but it seems that it is otherwise not defined. I think this is the same term and meaning that appears in Goyal et al. (2017), but it should obviously be defined here.","The paper presents a generative model of sequences based on the VAE framework, where the generative model is given by CNN with causal and dilated connections. <sep> Novelty of the method is limited; it mainly consists of bringing together the idea of causal and dilated convolutions and the VAE framework. However, knowing how well this performs is valuable the community. <sep> The proposed method appears to have significant benefits, as shown in experiments. The result on MNIST is, however, so strong that it seems incorrect; more digging into this result, or sourcecode, would have been better."
"abstract | rebuttal_process  ==> ======== <sep> Summary <sep> ======== <sep> The paper deals with the training of neural networks for classification or sequence generation tasks, using a cross-entropy loss. Minimizing the cross-entropy means maximizing the predicted probabilities of the ground-truth classes (averaged over the samples). The authors introduce a ""complementary entropy"" loss with the goal of minimizing the predicted probabilities of the complementary (incorrect) classes. To do that, they use the average of sample-wise entropy over the complement classes. By maximizing this entropy, the predicted complementary probabilities are encouraged to be equal and therefore, the authors claim that it neutralizes them as the number of classes grows large. The proposed training procedure, named COT, consists of alternating between the optimization of the two losses. <sep> The procedure is tested on image classification tasks with different datasets (CIFAR-10, CIFAR-100, Street View House Numbers, Tiny ImageNet and ImageNet), machine translation (training using IWSLT dataset, validation and test using TED tst2012/2013 datasets), and speech recognition (Gooogle Commands dataset). In the experiments, COT outperforms state-of-the-art models for each task/dataset. <sep> Adversarial attacks are also considered for the classification of images of CIFAR-10: using the Fast Gradient Sign and Basic Iterative Fast Gradient Sign methods on different models, adversarial examples specifically designed for each model, are generated. Then results of these models are compared to COT on these examples. The authors admit that the results are biased since the adversarial attacks only target part of the COT objective, hence more accurate comparisons should be done in future work. <sep> =========================== <sep> Main comments and questions <sep> =========================== <sep> End of page 1: ""the model behavior for classes other than the ground  truth stays unharnessed and not well-defined"". The probabilities  should still sum up to 1, so if the ground truth one is maximized,  the others are actually implicitly minimized. No? <sep> Page 3, sec 2.1: ""optimizing on the complement entropy drives ŷ_ij to 1/(K − 1)"". I believe that it drives each term ŷ_ij /(1 − ŷ_ig ) to be equal to 1/(K-1). Therefore, it drives ŷ_ij to (1 − ŷ_ig)/(K-1) for j!=g. <sep> This indeed flattens the ŷ_ij for j!=g, but the effect on ŷ_ig is not controlled. In particular this latter can decrease. Then in the next step of the algorithm, ŷ_ig will be maximized, but with no explicit control over the complementary probabilities. There are two objectives that are optimized over the same variable theta. So the question is, are we sure that this procedure will converge? What prevents situations where the probabilities will alternate between two values? <sep> For example, with 4 classes, we look at the predicted probabilities of a given sample of class 1: <sep> Suppose after step 1 of Algo 1, the predicted probabilities are:  0.5 0.3 0.1 0.1 <sep> After step 2:  0.1 0.3 0.3 0.3 <sep> Then step 1: 0.5 0.3 0.1 0.1 <sep> Then step 2: 0.1 0.3 0.3 0.3 <sep> And so on... Can this happen? Or why not? Did the algorithm have trouble converging in any of the experiments? <sep> Sec 3.1: <sep> ""additional efforts for tuning hyper-parameters might be required for optimizers to achieve the best performance"": Which hyper-parameters are considered here? If it is the learning rate, why not use a different one, tuned for each objective? <sep> Sec 3.2: <sep> The additional optimization makes each training iteration more costly. How much more? How do the total running times of COT compare to the ones of the baselines? I think this should be mentioned in the paper. <sep> Sec 3.4: <sep> As the authors mention, the results are biased and so the comparison is not fair here. Therefore I wonder about the  relevance of this section. Isn't there an easy way to adapt the attacks to the two objectives to be able to illustrate the conjectured robustness of COT? For example, naively having a two steps perturbation of the input: one based on the gradient of the primary objective and then perturb the result using the gradient of the complementary objective? <sep> =========================== <sep> Secondary comments and typos <sep> =========================== <sep> Page 3, sec 2.1: ""...the proposed COT also optimizes the complement objective for neutralizing the predicted probabilities..."", using maximizes instead of optimizes would be clearer. <sep> In the definition of the complement entropy, equation (2), C takes as parameter only y^hat_Cbar but then in the formula, ŷ_ig appears. Shouldn't C take all \\hat_y as an argument in this case? <sep> Algorithm 1 page 4: I find it confusing that the (artificial) variable that appears in the argmin (resp. argmax) is theta_{t-1} <sep> (resp. theta'_t) which is the previous parameter. Is there a reason for this choice? <sep> Sec 3: <sep> ""We perform extensive experiments to evaluate COT on the tasks"" --> COT on tasks <sep> ""compare it with the baseline algorithms that achieve state-of-the-art in the respective domain."" --> domainS <sep> ""to evaluate the model's robustness trained by COT when attacked"" needs reformulation. <sep> ""we select a state- of-the-art model that has the open-source implementation"" --> an open-source implementation <sep> Sec 3.2: <sep> Figure 4: why is the median reported and not the mean (as in Figure 3, Tables 2 and 3)? <sep> Table 3 and 4: why is it the validation error that is reported and not the test error? <sep> Sec 3.3: <sep> ""Neural machine translation (NMT) has populated the use of neural sequence models"": populated has not the intended meaning. <sep> ""We apply the same pre-processing steps as shown in the model"" --> in the paper? <sep> Sec 3.4: <sep> ""We believe that the models trained using COT are generalized better"" --> ""..using COT generalize better"" <sep> ""using both FGSM and I-FGSM method"" --> methodS <sep> ""The baseline models are the same as Section 3.2."" --> as in Section 3.2. <sep> ""the number of iteration is set at 10."" --> to 10 <sep> ""using complement objective may help defend adversarial attacks."" --> defend against <sep> ""Studying on COT and adversarial attacks.."" --> could be better formulated <sep> References: there are some inconsistencies (e.g.: initials versus first name) <sep> Pros <sep> ==== <sep> - Paper is clear and well-written <sep> - It seems to me that it is a new original idea <sep> - Wide applicability <sep> - Extensive convincing experimental results <sep> Cons <sep> ==== <sep> - No theoretical guarantee that the procedure should converge <sep> - The training time may be twice longer (to clarify) <sep> - The adversarial section, as it is,  does not seem relevant for me","This paper proposes adding a second objective to the training of neural network classifiers that aims to make the distribution over incorrect labels as flat as possible for each training sample. The authors describe this as ""maximizing the complement entropy."" Rather than adding the cross-entropy objective and the (negative) complement entropy term (since the complement entropy should be maximized while the cross-entropy is minimized), this paper proposes an alternating optimization framework in which first a step is taken to reduce the cross-entropy, then a step is taken to maximize the complement entropy. Extensive experiments on image classification (CIFAR-10, CIFAR-100, SVHN, Tiny Imagenet, and Imagenet), neural machine translation (IWSLT 2015 English-Vietnamese task), and small-vocabulary isolated-word recognition (Google Commands), show that the proposed two-objective approach outperforms training only to minimize cross-entropy. Experiments on CIFAR-10 also show that models trained in this framework have somewhat better resistance to single-step adversarial attacks. Concerns about the presentation of the adversarial attack experiments were raised by anonymous commenters and one of the reviewers, but these concerns were addressed in the revision and discussion. The primary remaining concern is a lack of any theoretical guarantees that the alternating optimization converges, but the strong empirical results compensate for this problem."
"abstract | strength  ==> This paper explores the task of finding discrete adversarial examples for (current) dialog models in a post hoc manner (i.e., once models are trained). In particular, the authors propose an optimization procedure for crafting inputs (utterances) that trigger trained dialog models to respond in an egregious manner. <sep> This line of research is interesting as it relates to real-world problems that our models face before they can be safely deployed. The paper is easy to read, nicely written, and the proposed optimization method seems reasonable. The study also seems clear and the results are fairly robust across three datasets. It was also interesting to study datasets which, a priori, seem like they would not contain much egregious content (e.g., Ubuntu ""help desk"" conversations). <sep> My main question is that after reading the paper, I'm not sure that one has an answer to the question that the authors set out to answer. In particular, are our current seq2seq models for dialogs prone to generating egregious responses? On one hand, it seems like models can assign higher-than-average probability to egregious responses. On the other, it is unclear what this means. For example, it seems like the possibility that such a model outputs such an answer in a conversation might still be very small. Quantifying this would be worthwhile. <sep> Further, one would imagine that a complete dialog system pipeline would contain a collection of different models including a seq2seq model but also others. In that context, is it clear that it's the role of the seq2seq model to limit egregious responses? <sep> A related aspect is that it would have been interesting to explore a bit more the reasons that cause the generation of such egregious responses. It is unclear how representative is the example that is detailed (""I will kill you"" in Section 5.3). Are other examples using words in other contexts? Also, it seems reasonable that if one wants to avoid such answers, countermeasures (e.g., in designing the loss or in adding common sense knowledge) have to be considered. <sep> Other comments: <sep> - I am not sure of the value of Section 3. In particular, it seems like the presentation of the paper would be as effective if this section was summarized in a short paragraph (and perhaps detailed in an appendix). <sep> - Section 3.1, ""continuous relaxation of the input embedding"", what does that mean since the embedding already lives in continuous space? <sep> - I understand that your study only considers (when optimizing for egregious responses)) dialogs that are 1-turn long. I wonder if you could increase hit rates by crafting multiple inputs at once. <sep> - In Section 4.3, you fix G (size of the word search space) to 100. Have you tried different values? Do you know if larger Gs could have an impact of reported hit metrics? <sep> - In Table 3, results from the first column (normal, o-greedy) seem interesting. Wouldn't one expect that the model can actually generate (almost) all normal responses? Your results indicate that for Ubuntu models can only generate between 65% and 82% of actual (test) responses. Do you know what in the Ubuntu corpus leads to such a result? <sep> - In Section 5.3, you seem to say that the lack of diversity of greedy-decoded sentences is related to the low performance of the ""o-greedy"" metric. Could this result simply be explained because the model is unlikely to generate sentences that it has never seen before? <sep> You could try changing the temperature of the decoding distribution, that should improve diversity and you could then check whether or not that also increases the hit rate of the o-greedy metric. <sep> - Perhaps tailoring the mal lists to each specific dataset would make sense (I understand that there is already some differences in between the mal lists of the different datasets but perhaps building the lists with a particular dataset in mind would yield ""better"" results).","This work examines how to craft adversarial examples that will lead trained seq2seq models to generate undesired outputs (here defined as, assigning higher-than-average probability to undesired outputs). Making a model safe for deployment is an important unsolved problem and this work is looking at it from an interesting angle, and all reviewers agree that the paper is clear, well-presented, and offering useful observations. While the paper does not provide ways to fix the problem of egregious outputs being probable, as pointed out by reviewers, it is still a valuable study of the behavior of trained models and an interesting way to ""probe"" them, that would likely be of high interest to many people at *CONF*."
"abstract | strength | weakness | decision  ==> This paper addresses the problem of architecture search, and specifically seeks to do this without having to train on ""proxy"" tasks where the problem is simplified through more limited optimization, architectural complexity, or dataset size. The paper puts together a set of existing complementary methods towards this end, specifically 1) Training ""cumbersome"" networks as in One Shot and DARTS, 2) Path binarization to address memory requirements (optimized using ideas in BinaryConnect), and 3) optimizing a non-differentiable architecture using REINFORCE. The end result is that this method is able to find efficient architectures that achieve state of art performance with fewer parameters, can be optimized for non-differentiable objectives such as latency, and can do so with smaller amounts of GPU memory and computation. <sep> Strengths <sep> + The paper is in general well-written and provides a clear description of the methods. <sep> + Different choices made are well-justified in terms of the challenge they seek to address (e.g. non-differentiable objectives, etc.) <sep> + The results achieve state of art while being able to trade off other objectives such as latency <sep> + There are some interesting findings such as the need for specialized blocks rather than repeating blocks, comparison of architectures for CPUs vs. GPUs, etc. <sep> Weaknesses <sep> - In the end, the method is really a combination of existing methods (One Shot/DART, BinaryConnect, use of RL/REINFORCE, etc.). One novel aspect seems to be factorizing the choice out of N candidates by making it a binary selection. In general, it would be good for the paper to make clear which aspects were already done by other approaches (or if it's a modification what exactly was modified/added in comparison) and highlight the novel elements. <sep> - The comparison with One Shot and DARTS seems strange, as there are limitations place on those methods (e.g. cell structure settings) that the authors state they chose ""to save time"". While that consideration has some validity, the authors should explicitly state why they think these differences don't unfairly bias the experiments towards the proposed approach. <sep> - It's not clear that the REINFORCE aspect is adding much; it achieves slightly higher parameters when compared against Proxyless-G, and while I understand the motivation to optimize a non-differentiable function in this case the latency example (on ImageNet) is never compared to Proxyless-G. It could be that optimized the normal differentiable objective achieves similar latency with the smaller number of parameters. Please show results for Proxyless-G in Table 4. <sep> - There were several typos throughout the paper (""great impact BY automatically designing"", ""Fo example"", ""is build upon"", etc.) <sep> In summary, the paper presents work on an interesting topic. The set of methods seem to be largely pulled from work that already exists, but is able to achieve good results in a manner that uses less GPU memory and compute, while supporting non-differentiable objectives. Some of the methodological issues mentioned above should be addressed though in order to strengthen the argument that all parts of the the method (especially REINFORCE) are necessary.","This paper integrates a bunch of existing approaches for neural architecture search, including OneShot/DARTS, BinaryConnect, REINFORCE, etc. Although the novelty of the paper may be limited, empirical performance seems impressive. The source code is not available. I think this is a borderline paper but maybe good enough for acceptance."
"abstract | strength | decision  ==>  ==> Summary: <sep> The authors propose to apply the Deep Variational Information Bottleneck (VIB) method of [1] on discriminator networks in various adversarial-learning-based scenarios. They propose a way to adaptively update the value for the bêta hyper-parameter to respect the constraint on I(X,Z). Their technique is shown to stabilize/allow training when P_g and P_data do not overlap, similarly to WGAN and gradient-penalty based approaches, by essentially pushing their representation distributions (p_z) to overlap with the mutual information bottleneck. It can also be considered as an adaptive version of instance noise, which serves the same goal. The method is evaluated on different adversarial learning setup (imitation learning, inverse reinforcement learning and GANs), where it compares positively to most related methods. Best results for 'classical' adversarial learning for image generation are however obtained when combining the proposed VIB with gradient penalty (which outperforms by itself the VGAN in this case). <sep> Pros : <sep> - This paper brings a good amount of evidence of the benefits to use the VIB formulation to adversarial learning by first showing the effect of such approach on a toy example, and then applying it to more complex scenarios, where it also boosts performance. The numerous experiments and analyses have great value and are a necessity as this paper mostly applies the VIB to new learning challenges. <sep> - The proposition of a principled way of adaptively varying the value of Bêta to actually respect more closely the constraint I(X,Z) < I_c, which to my knowledge [1] does not perform, is definitely appealing and seems to work better than fixed Bêtas and does also bring the KL divergence to the desired I_c. <sep> - The technique is fairly simple to implement and can be combined with other stabilization techniques such as gradient penalties on the discriminator. <sep> Cons: <sep> - In my view, the novelty of the approach is somewhat limited, as it seems like a straightforward application of the VIB from [1] for discriminators in adversarial learning, with the difference of using an adaptive Bêta. <sep> - I think the Bêta-VAE [2] paper is definitely related to this paper and to the paper on which it is based [1] and should thus be cited as the authors use a similar regularization technique, albeit from a different perspective, that restricts I(X,Z) in an auto-encoding task. <sep> - I think the content of batches used to regularize E(z|x) w.r.t. to the KL divergence should be clarified, as the description of p^tilde ""being a mixture of the target distribution and the generator"" (Section 4) can let the implementation details be ambiguous. I think batches containing samples from both distributions can cause problems as the expectation of the KL divergence on a batch can be low even if the samples from both distributions are projected into different parts of the manifold. This makes me think batches are separated? Either way, this should be more clearly stated in the text. <sep> - The last results for  the 'traditional' GAN+VIB show that in this case, gradient penalty (GP) alone outperforms the proposed VGAN, and that both can be combined for best results. I thus wonder if the results in all other experiments could show similar trends if GP had been tested in these cases as well. In the imitation learning task, authors compare with instance noise, but not with GP, which for me are both related to VIB in what they try to accomplish. Was GP tested in Imitation Learning/Inverse RL ? Was it better? Could it still be combined with VIB for better results? <sep> - In the saliency map of Figure 5, I'm unclear as to what the colors represent (especially on the GAIL side). I doubt that this is simply due to the colormap used, but this colormap should be presented. <sep> Overall, I think this is an interesting and relevant paper that I am very likely to suggest to peers working on adversarial learning, and should therefore be presented. I think the limited novelty is counterbalanced by the quality of empirical analysis. Some clarity issues and missing citations should be easy to correct. I appreciate the comparison and combination with a competitive method (Gradient Penalty) in Section 5.3, but I wish similar results were present in the other experiments, in order to inform readers if, in these cases as well, combining VIB with GP leads to the best performance. <sep> [1] Deep Variational Information Bottleneck, (Alemi et al. 2017) <sep> [2] beta-VAE: Learning Basic Visual Concepts with a Constrained Variational Framework (Higgins et al. 2017)","The paper proposes a simple and general technique based on the information bottleneck to constrain the information flow in the discriminator of adversarial models. It helps to train by maintaining informative gradients. While the information bottleneck is not novel, its application in adversarial learning to my knowledge is, and the empirical evaluation demonstrates impressive performance on a broad range of applications. Therefore, the paper should clearly be accepted."
"abstract | weakness | suggestion  ==>  ==> This work addresses the problem of online adapting dynamics models in the context of model-based RL. Learning globally accurate dynamics model is impossible if we consider that environments are dynamic and we can't observe every possible environment state at initial training time. Thus learning dynamics models that can be adapted online fast, to deal with unexpected und never seen before events is an important research problem. <sep> This paper proposes to use meta-learning to train an update policy that can update the dynamics model at test time in a sample efficient manner. Two methods are proposed <sep> - GrBAL: this method uses MAML for meta-learning <sep> - ReBAL: this method trains a recurrent network during meta-training such that it can update the dynamics effectively at test time when the dynamics  change <sep> Both methods are evaluated on several simulation environments, which show that GrBAL outperforms ReBAL (on average). GrBAL is then evaluated on a real system. <sep> The strengths of this paper are: <sep> - this work addresses an important problem and is well motivated <sep> - experiments on both simulated and on a real system are performed <sep> The weaknesses: <sep> - the related work section is biased towards the ML community. There is a ton of work on adapting (inverse) dynamics models in the robotics community. This line of work is almost entirely ignored in this paper. Furthermore some important recent references for model-based RL are not provided in the related work section (PETS [3] and MPPI [2]), although MPPI is the controller that is used in this work as a framework for model-based RL. Additionally, existing work on model-based RL with meta-learning [1] has not been cited. This is unacceptable. <sep> - There is no significant technical contribution - the ""contribution"" is that existing meta-learning methods have been applied to the model-based RL setting. Even if no-one has had that idea before - it would be a minor contribution, but given that there is prior work on meta-learning in the context of model-based RL, this idea itself is not novel anymore. <sep> - Two methods are provided, without much analysis. Often authors refer to ""our approach"" - but it's actually not clear what they mean by our approach. The authors can't claim ""model-based meta RL"" as their approach. <sep> - While I commend the authors for performing both simulation and real-world experiments, I find the that experiments lack a principled evaluation. More details below. <sep> Feedback on experiments: <sep> Section 6.2 (sample efficiency) <sep> You compare apples to oranges here. I have no idea whether your improvements in terms of sample-efficiency are due to using a model-based RL approach or because your deploying meta-learning. It is well known that model-based RL is more sample efficient, but often cannot achieve the same asymptotic performance as model-free RL. Since MPPI is your choice of model-based RL framework, you would have to include an evaluation that shows results on MPPI with model bootstrapping (as presented in [2]) to give us an idea of how much more sample-efficient your approach is. <sep> Section 6.3 (fast adaptation and generalization) <sep> While in theory one can choose the meta-learning approach independently from the choice of model-based controller, in practice the choice of the MPC method is very important. MPPI can handle model inaccuracies very well - almost to the point where sometimes adaptation is not necessary. You CANNOT evaluate MPPI with online adaptation to another MPC approach with another model-learning approach. This does not give me any information of how your meta-learning improves model-adaptation. In essence these comparisons are meaningless. To make your results more meaningful you need to use the same controller setup (let's say MPPI) and then compare the following: <sep> 1. MPPI with your meta-trained online adaptation <sep> 2. MPPI results with a fixed learned dynamics model - this shows us whether online adaptation helps <sep> 3. results of MPPI with the initial dynamics model (trained in the meta-training phase) -without online adaptation. This will tell us whether the meta-training phase provides a dynamics model that generalizes better (even without online adaptation) <sep> 4. MPPI with model bootstrapping (as presented in [2]). This will show whether your meta-trained online adaptation actually outperforms simple online model bootstrapping in terms of sample-efficiency <sep> The key here is that you need to use the same model-based control setup (whether its MPPI or some other method). Otherwise you cannot detangle the effect of controller choice from your meta-learned online adaptation. <sep> 6.4 Real-world: same comments as above, comparisons are not meaningful <sep> [1] Meta Reinforcement Learning with Latent Variable Gaussian Processes, UAI 2018 <sep> [2] MPPI with model-bootstrapping: Information Theoretic MPC for Model-Based Reinforcement Learning , ICRA 2017 <sep> [3] Deep Reinforcement Learning in a Handful of Trials using Probabilistic Dynamics Models, NIPS 2018","The authors consider the use of MAML with model based RL and applied this to robotics tasks with very encouraging results. There was definite interest in the paper, but also some concerns over how the results were situated, particularly with respect to the related research in the robotics community. The authors are strongly encouraged to carefully consider this feedback, as they have been doing in their responses, and address this as well as possible in the final version."
"abstract | strength | weakness | strength  ==>  ==> This paper shows that the problem of defending MNIST is still unsuccessful. It hereby proposes a model that is robust by design specifically for the MNIST classification task. Unlike conventional classifiers, the proposal learns a class-dependent data distribution using VAEs, and conducts variational inference by optimizing over the latent space to estimate the classification logits. <sep> Some extensive experiments verify the model robustness with respect to different distance measure, with most state-of-the-art attacking schemes, and compared against several baselines. The added experiments with rotation and translation further consolidate the value of the work. <sep> Overall I think this is a nice paper. Although being lack of some good intuition, the proposed model indeed show superior robustness to previous defending approaches. Also, the model has some other benefits that are shown in Figure 3 and 4. The results show that the model has indeed learned the data distribution rather than roughly determining the decision boundary of the input space as most existing models do. <sep> However, I have the following comments that might help to improve the paper: <sep> 1. It would be more interesting to add more intuition on why the proposed model is already robust by design. <sep> 2. Although the paper is designed for MNIST specifically, the proposed scheme should apply to other classification tasks. Have you tried the models on other datasets like CIFAR10/100? It would be interesting to see whether the proposal would work for more complicated tasks. When the training data for each label is unbalanced, namely, some class has very few samples, would you expect the model to fail? <sep> 3. Equation (8) is complicated and still model-dependent. Without further relaxation and simplification, it's not easy to see if this value is small or large, or to understand what kind of message this section is trying to pass. <sep> 4. Although the main contribution of the paper is to propose a model that is robust without further defending, the proposed model could still benefit from adversarial training. Have you tried to retrain your model using the adversarial examples you have got and see if it helps?","The paper presents a technique of training robust classification models that uses the input distribution within each class to achieve high accuracy and robustness against adversarial perturbations. <sep> Strengths: <sep> - The resulting model offers good robustness guarantees for a wide range of norm-bounded perturbations <sep> - The authors put a lot of care into the robustness evaluation <sep> Weaknesses: <sep> - Some of the ""shortcomings"" attributed to the previous work seem confusing, as the reported vulnerability corresponds to threat models that the previous work did not made claims about <sep> Overall, this looks like a valuable and interesting contribution."
"abstract | strength | suggestion | decision  ==> This paper proposes a method to learn a quantization of both observations and hidden states in an RNN. Its findings suggest that many problems can be reduced to relatively simple Moore Machines, even for complex environments such as Atari games. <sep> The method works by pretraing an RNN to learn a policy (e.g. through the A3C algorithm), and then training pairs of encoder/decoder networks with a quantizing forward pass and a straight-through backpropagation. The learned quantizations can then be used to build a Moore Machine, which itself can be reduced with FSM reduction algorithms, yielding a discrete, symbolic approximation of the inner workings of RNNs, that could in principle be interpreted more easily than latent embedding spaces. <sep> One downside of this paper is that it promises an exciting method to analyse the inner workings of RNNs, but then postpones this analysis to later work. Understandably, the synthetic experiments take some space and shows that the proposed method works as expected when the problem is amenable to discretization; maybe some parts of this could be in the appendix? <sep> Another downside is that there is little indication of the computational implications of the method. The method was evaluated on a fairly small set of hyperparameters, and there are no indication of how long the optimization and finetuning takes. Presumably, minimizing a Moore Machine has been studied for decades, but how long does minimizing the 1000s of states in Atari games take? A second or an hour? <sep> The paper is fairly well written and easy to understand. The method seems well grounded, although I'm not familiar enough with the quantization literature to detect if something important is missing. I think this is a great tool that hopefully will be used to try to understand the memory mechanisms of RNNs. <sep> I think the proposed method (and the fact that it works in simple cases) warrants acceptance, but I think more experimental work would make this a great contribution. Since there is no reason for quantization to improve performance if it is done after training, then more emphasis should be put on the interpretability of the discretization; yet it is lacking in the current work. Some Atari games are known to require various amounts of memory, this could be analysed. Some other Atari games are known to be hard to solve, what happens to the RNN when the agent fails to achieve an optimal policy might also show up in the subsequent discretization and be interesting to analyse. <sep> Comments: <sep> - In atari, you can have access to the RAM and from it, using exactly the same mechanisms and maybe a bit of tabular MDPs, you should be able to recover the optimal MM. <sep> - It is good that the authors report their failure to train MMNs from scratch; IMO this says something about the straight through estimators' limits. Measuring how sensible these things are to change in their target distribution and comparing to previous uses of ST in quantization works could be interesting. <sep> - in Section 8 (appendix) ""Grammer"" should be ""Grammar"" <sep> - All the (PO)MDPs that you analyse arguably have finite state spaces, and you set the ALE to be deterministic. What happens in continuous stochastic environments? <sep> - Do you think a similar technique could be used to recover a (possibly stochastic) MDP instead of a Moore Machine? It would be interesting to see MDP reduction methods applied to a learned MDP.",The paper addresses the problem of interpreting recurrent neural networks by quantizing their states an mapping them onto a Moore Machine. The paper presents some interesting results on reinforcement learning and other tasks. I believe the experiments could have been more informative if the proposed technique was compared against a simple quantization baseline (e.g. based on k-means) so that one can get a better understanding of the difficulty of these task. <sep> This paper is clearly above the acceptance threshold at *CONF*.
"abstract | strength | weakness | rebuttal_process | strength | suggestion | decision  ==>  ==> Summary: The paper proposes a GAN-based approach for dealing with adversarial instances, with the training of a robust discriminator that is able to identify adversaries from clean samples, and a generator that produces adversarial noise for its given input clean image in order to mislead the discriminator. In contrast to the state-of-the-art ""ensemble adversarial training"" approach, which relies on several pre-trained neural networks for generating adversarial examples, the authors introduce a way for dynamically generating adversarial examples on-the-fly by using a generator, which they along with their clean counterparts are then consumed for training the discriminator. <sep> Quality: The paper is relatively well-written, although a little sketchy, and its motivations are clear. The authors compare their proposed approach with a good of variety of strong defenses such as ""ensemble adversarial training"" and ""PGD adversarial training"", supporting with convincing experiments their approach. <sep> Originality: Xioa et al. (2018) used very similar technique for generating new adversarial examples (generator attack), then used for training a robust discriminator. Likewise, Lee et al. (2018) also used GANs to produce perturbations for making images misclassified. Given this, what is the main novelty of this approach comparing to the (Xioa et al., 2018) and (Lee et al., 2018)? These references should be discussed in details in the paper. <sep> Moreover, limited comparison with different attacks: Why did not compare against targeted attacks such as T-FGS, C&W or GAN-attack? <sep> It is really surprising that undefended network is working better (showing more robustness) than the defended network ""adversarial PGD"" on black-box attacks, why this is happening? <sep> References: <sep> - Xiao, C., Li, B., Zhu, J. Y., He, W., Liu, M., & Song, D. (2018). Generating adversarial examples with adversarial networks. arXiv preprint arXiv:1801.02610. <sep> - Lee, H., Han, S., & Lee, J. (2017). Generative Adversarial Trainer: Defense to Adversarial Perturbations with GAN. arXiv preprint arXiv:1705.03387.","The paper proposed a GAN approach to robust learning against adversarial examples, where a generator produces adversarial examples as perturbations and a discriminator is used to distinguish between adversarial and raw images. The performance on MNIST, SVHN, and CIFAR10 demonstrate the effectiveness of the approach, and in general, the performance is on par with carefully crafted algorithms for such task. <sep> The architecture of GANs used in the paper is standard, yet the defensive performance seems good. The reviewers wonder the reason behind this good mechanism and the novelty compared with other works in similar spirits. In response, the authors add some insights on discussing the mechanism as well as comparisons with other works mentioned by the reviewers. <sep> The reviewers all think that the paper presents a simple scheme for robust deep learning based on GANs, which shows its effectiveness in experiments. The understanding on why it works may need further explorations. Thus the paper is proposed to be borderline lean accept."
"abstract | rebuttal_process  ==> ====== Final Comments ======= <sep> I thank the authors for updating the manuscript with clarifications and for clear replies to my concerns. <sep> I agree with R2 to some extent that the empirical performance of the method, as well as the formulation,  is interesting. In general, the authors addressed my concerns regarding how optimal transport training model interfaces with MLE training and the choice of using scaled softmax for computing the Wasserstein distances. However, I still find myself agreeing with R3 that the choice of sets to compute Wasserstein distances (as opposed to sequences is somewhat unjustified); and it is not clear how the theory in Sec. 3 justifies using sets instead of words, as the data distribution p_d is over sequences in both the W-2 term as well as the MLE term. This would be good to clarify further, or explain more clearly how Sec. 3 justifies this choice. <sep> Also, I missed this in the original review, but the assumption that KL(\\mu| p_d) = KL(\\p_d| \\mu) since \\mu is close to p_d for MLE training does not seem like a sensible assumption under model misspecification (as MLE is mode covering). I would suggest the authors discuss this in a revision/ camera-ready version. <sep> In light of these considerations, I am updating the rating to 6 (to reflect the points that have been addressed), but I still do not believe that the method is super-well justified, despite an interesting formulation and strong empirical results (which are aspects the paper could still improve upon). <sep> ===================== <sep> **Summary** <sep> The paper proposes a regularization / fine-tuning scheme in addition to maximum likelihood sequence training using optimal transport. The basic idea is to match a sampled sentence from a model with a ground truth sentence using optimal transport. Experimental results show that the proposed modification improves over MLE training across machine translation, summarization and image captioning domains. <sep> **Strengths** <sep> + The proposed approach shows promising results across multiple domains. <sep> + The idea of using optimal transport to match semantics of words intuitively makes sense. <sep> **Weaknesses** <sep> 1. Generally, one would think that for optimal transport we would use probabilities which come from the model, i.e. given a set of words and probabilities for each of the words, one would move mass around from one word to another word (present in the ground truth) if the words were semantically relevant to each other and vice versa. However, it seems the proposed algorithm / model does not get the marginal distributions for the words from the model, and indeed does not use any beliefs from the model in the optimal transport formulation / algorithm [Alg. 1, Line 2]. This essentially then means, following objective 2, that optimal transport has no effect on the model's beliefs on the correct word, but merely needs to ensure that the cosine similarity between words which have a high transport mass is minimized, and has no direct relation to the belief of the model itself. This strikes as a somewhat odd design choice. (*) <sep> 2. In general, another issue with the implementation as described in the paper could be the choice of directly using the temperature scaled softmax as an estimate of the argmax from the model, instead of sampling utterances from the model. It seems worth reporting results, even if as a baseline what sampling from something like a concrete distribution [A] yeilds in terms of results. (*) <sep> 3. As a confirmation, Is the differentiable formulation for sentences also used for MLE training part of the objective, or does MLE still use ground truth sentences (one-hot encoded) as input. Does this mean that the model is forwarded twice (once with ground truth, and once with softmax outputs)? In general, instead of/ in addition to an Algorithm box that expains the approach of Xie et.al., it would be good to have a clear algorithmic explanation of the training procedure of the proposed model itself. Further, the paper is not clear if the model is always used for finetuning the an MLE model (as Section 3) seems to suggest or if the optimal transport loss is used in conjunction with the model as the bullet ``complementary MLE loss'' seems to suggest. (*) <sep> 4. Section 3: It is not clear that the results / justifications hold for the proposed model since the distance optimized in the current paper is not a wasserstein-2 distance. Sure, it computes cosine distance, which is L-2 but it appears wasserstein-2 distance is defined for continuous probability measures, while the space on which the current paper computes distances is inherently the (discrete) space of word choices. (*) <sep> Experiments <sep> 5. The SPICE metric for image captioning takes the content of the sentences (and semantics) into account, instead of checking for fluency. Prior work on using RL techniques for sequence predictions have used SPICE + CIDEr [B] to alleviate the problem with RL methods mentioned in page. 5. Would be good to weaken the claim. (*) <sep> Minor Points <sep> 1. It might be good to be more clear about the objective from Xie et.al. by also stating the modified formulation they discuss, and explaining what choice of the function yeilds the bregman divergence of the form discussed in Eqn. 3. <sep> 2. It would be nice to be consistent with how \\mathcal{L}_{ot} is used in the paper. Eqn. 4 lists it with two embedding matrices as inputs, while Alg. 1, Line 11 assigns it to be the inner product of two matrices. <sep> **Preliminary Evaluation** <sep> In general, the paper is an interesting take on improving MLE for sequence models. However, while the idea of using optimal transport is interesting and novel for training sequence models, I have questions about the particular way in which it has been implemented, which seems somewhat unjustified. I also have further clarifications about a claimed justification for the model. Given convincing responses for these, and other clarification questions (marked with a *) this would be a good paper. <sep> References <sep> [A]: Maddison, Chris J., Andriy Mnih, and Yee Whye Teh. 2016. ""The Concrete Distribution: A Continuous Relaxation of Discrete Random Variables."" arXiv [cs.LG]. arXiv. http://arxiv.org/abs/1611.00712. <sep> [B]: Liu, Siqi, Zhenhai Zhu, Ning Ye, Sergio Guadarrama, and Kevin Murphy. 2016. ""Improved Image Captioning via Policy Gradient Optimization of SPIDEr."" arXiv [cs.CV]. arXiv. http://arxiv.org/abs/1612.00370.","The paper proposes the idea of using optimal transport to evaluate the semantic correspondence between two sets of words predicted by the model and ground truth sequences. Strong empirical results are presented which support the use of optimal transport in conjunction with log-likelihood for training sequence models. I appreciate the improvements to the manuscript during the review process, and I encourage the authors to address the rest of the comments in the final version."
"rating_summary | decision  ==> The paper proposes a version of GANs specifically designed for generating point clouds. The core contribution of the work is the upsampling operation: in short, it takes as an input N points, and produces N more points (one per input) by applying a graph convolution-like operation. <sep> Pros: <sep> + The problem of making scalable generative models for point clouds is clearly important, and using local operations in that context makes a lot of sense. <sep> Cons: <sep> - The paper is not particularly well-written, is often hard to follow, and contains a couple of confusing statements (see a non-exhaustive list of remarks below). <sep> - The experimental evaluation seems insufficient: clearly it is possible to come up with more baselines. Even a comparison to other types of generative models would be useful (e.g. variants of VAEs, other types of GANs). There also alternative local graph-convolution-like operations (e.g. tangent convolutions) that are designed for point clouds. In addition, it is quite strange that results are reported not for all the classes in the dataset. <sep> Various remarks: <sep> p.1, ""whereby it learns to exploit a self-similarity prior to sample the data distribution"": this is a confusing statement. <sep> p.2, ""(GANs) have been shown on images to provide better approximations of the data distribution than other generative models"": This statement is earthier too strong (all other models) or does not say much (some other models) <sep> p.2, ""However, this means that they are unable to learn localized features or exploit weight sharing."": I see the point about no weight sharing in the generator, but feature learning p.3, ""the key difference with the work in this paper is that PointNet and PointNet++ are not generative models, but are used in supervised problems such as classification or segmentation."": Yet, the kind of operation that is used in the pointnet++ is quite similar to what you propose? <sep> p.4: ""because the high dimensionality of the feature vectors makes the gridding approach unfeasible."": but you are actually dealing with the point clouds where each point is 3D?","All reviewers gave an accept rating: 9, 7 &6. <sep> A clear accept -- just not strong enough reviewer support for an oral."
"strength | weakness | rating_summary | strength | weakness | suggestion | decision  ==>  ==> Brief summary: <sep> HRL method which uses a 2 level hierarchy for sparse reward tasks. The low level policies are only provided access to proprioceptive parts of the observation, and are trained to maximize change in the non-proprioceptive part of the state as reward. The higher level policy is trained as usual by commanding lower level policies. <sep> Overall impression: <sep> I think the paper has a major assumption about the separation of internal and external state, thereby setting the form of the low level primitives. This may not be fully general, but is particularly useful for the classes of tasks shown here as seen from the strong results. I would like to see the method applied more generally to other robotic tasks, and a comparison to Florensa et al. And perhaps the addition of a video which shows the learned behaviors. <sep> Introduction: <sep> the difficulty of learning a high-level controller when the low-level policies shifts -> look at ""data efficient hierarchical reinforcement learning"" (Nachum et al) <sep> The basic assumption that we can separate out observations into proprioceptive and not proprioceptive can often be difficult. For example with visual inputs or entangled state representations, this might be very challenging to extract. This idea seems to be very heavily based on what is ""internal"" and what is ""external"" to the agent, which may be quite challenging to separate. <sep> The introduction of phase functions seems to be very specific to locomotion? <sep>  Related work: <sep> The connection of learning diverse policies should be discussed with Florensa et al, since they also perform something similar with their mutual information term. DeepMimic, DeepLoco (Peng et al) also use phase information in the state, worthwhile to cite. <sep> Section 3.1: <sep> The pros and cons of making the assumption that representation is disentangled enough to make this separation, should be discussed. <sep> Also, the internal and external state should be discussed with a concrete example, for the ant for example. <sep> Section 3.2: <sep> The objective for learning diverse policies is in some sense more general than Florensa et al, but in the same vein of thinking. What are the pros and cons of this approach over that?  The objective is greedy in the change of external state. We'd instead like something that over the whole trajectory maximizes change? <sep> Section 3.3:  How well would these cyclic objectives work in a non-locomotion setting? For example manipulation <sep> Section 3.4: This formulation is really quite standard in many HRL methods such as options framework. The details can be significantly cut down, and not presented as a novel contribution. <sep> Experiments: <sep> It is quite cool that Figure 2 shows very significant movement, but in some sense this is already supervised to say ""move the CoM a lot"". This should be compared with explicitly optimizing for such an objective, as in Florensa et al. I'm not sure that this would qualify as ""unsupervised"" per se. As in it too is using a particular set of pre-training tasks, just decided by the form of choosing internal and external state. <sep> all of the baselines fail to get close to the goal locations.-> this is a bit surprising? Why are all the methods performing this poorly even when rewarded for moving the agent as much as possible. <sep> Overall, the results are pretty impressive. A video would be a great addition to the paper. <sep> Comparison to Eysenbach et al isn't quite fair since that method receives less information. If given the extra information, the HRL method performs much better (as indicated by the ant waypoint plot in that paper).","Strengths <sep> The paper presents a method of training two-level hierarchies that is based on relatively intuitive ideas and that performs well. <sep> The challenges of hierarchical RL makes this an important problem. The benefits of periodicity and the <sep> separation of internal state from external state is a clean principle that can potentially be broadly employed. <sep> The method does well in outperforming the alternative baselines. <sep> Weaknesses <sep> There is no video of the results. There is related work, i.e., [Peng et al. 2016] (rev 4) uses <sep> a policy ensemble; phase info is used in DeepLoco/DeepMimic; methods such as ""Virtual Windup Toys for Animation"" <sep> exploited periodicity (25y ago); More comparisons with prior work such as Florensa et al. would help. <sep> The separation of internal and external state is an assumption that may not hold in many cases. <sep> The results are locomotion focussed. There are only two timescales. <sep> Decision <sep> The reviewers are largely in agreement to accept the paper. <sep> There are fairly-simple-but-useful lessons to be found in the paper <sep> for those working on HRL problems, particularly those for movement and locomotion. <sep> The AC sees the novely with respect to different pieces of related work is the weakest point of the paper. <sep> The reviews contain good suggestions for revisions and improvements; the latest version may take care <sep> of these (uploaded after the last reviewer comments). Overall, the paper will make a good contribution <sep> to *CONF* 2019."
"weakness  ==> The paper introduces the concept of fluctuation-dissipation relations to stochastic gradient descent. These relations hold for certain observables in physical systems in equilibrium. In the context of SGD as a non-equilibrium process with a stationary density, they allow to quantify how far away this process is from its stationary state. <sep> One of the strengths of the paper is that it works in the discrete-time formalism and uses the master equation, as opposed to other recent works that used the continuous-time limit of SGD to derive related (yet different) results. Furthermore, the formalism does not even rely on a locally quadratic approximation of the loss function, or on any Gaussian assumptions of the SGD noise. To the best of my knowledge, all of this is very innovative. Ultimately, the authors propose a practical algorithm to adaptively lowering the learning rate based on testing fluctuation-dissipation relations. <sep> This is an interesting paper which I recommend to accept. It not only shows new theoretical results, but also conforms their validity in real-world experiments. <sep> I have only a few questions / comments: <sep> 1. In Eq. 17 and others where the scalar product of theta and grad(f) occurs, is it implicitly assumed that the optimum of f is at theta=0? <sep> 2. In Fig. 2, the distinction between solid and dotted curves could be made better visible. <sep> 3. For completeness, it would be good to add the following citation: <sep> Stephan Mandt, Matthew D. Hoffman, and David M. Blei. ""Continuous-time limit of stochastic gradient descent revisited."" NIPS 2015 Workshop on Optimization for Machine Learning.","The paper presents interesting idea, but the reviewers ask for improving further paper clarity - that includes, but is not limited to, providing in-depth explanation of assumptions and also improving the writing that is too heavy and difficult to understand."
"abstract | strength | rebuttal_process | rating_summary  ==> Summary: <sep> This paper proposes distributional concavity regularization for GANs which encourages producing generator distributions with higher entropy. The paper motivates the proposed method as follows: <sep> -       Using the concept of functional gradient, the paper interprets the update in the generator parameters as an update in the generator distribution <sep> -       Given this functional gradient perspective, the paper proposes updating the generator distribution toward a target distribution which has *higher entropy and satisfies monoticity* <sep> -       Then, the paper proves that this condition can be satisfied by ensuring that generator's objective (L) is concave <sep> -       Since it's difficult to ensure concavity when parametrizing generators as deep neural networks, the paper proposes adding a simple penalty term that encourages the concavity of generator objective <sep> Experiments confirm the validity the proposed approach. Interestingly, the paper shows that performance of multiple GAN variants can be improved with their proposed method on several image datasets <sep> Strengths: <sep> -    The proposed method is very interesting and is based on sound theory <sep> -    Connection to optimal transport theory is also interesting <sep> -    In practice, the method is very simple to implement and seems to produce good results <sep> Weaknesses: <sep> -       Readability of the paper can be generally improved. I had to go over the paper many times to get the idea. <sep> -       Figures should be provided with more detailed captions, which explain main result and providing context (e.g. explaining baselines). <sep> Questions/Comments: <sep> -       Equation (7) has typos (uses theta_old instead of theta in some places) <sep> -       Section 4.1 (effect of monoticity) is a bit confusing. My understanding is that parameter update rule of equation (3) and (6) are equivalent, but you seem to use (6) there. Can you clarify what you do there and in general this experiment a bit more? <sep> -       Comparing with entropy maximization method of EGAN (Dai et al, 2017) is a good idea, but I'm wondering if you can compare it on low dimensional settings (e.g. as in Fig 2). It is also not clear why increasing entropy with EGAN-VI is worse than baselines in Table 1. <sep> Overall recommendation: <sep> The paper is based on sound theory and provides very interesting perspective. The method seems to work in practice on a variety of experimental setting. Therefore, I recommend accepting it.","This paper proposes distributional concavity regularization for GANs which encourages producing generator distributions with higher entropy. <sep> The reviewers found the contribution interesting for the *CONF* community. R3 initially found the paper lacked clarity, but the authors took the feedback in consideration and made significant improvements in their revision. The reviewers all agreed that the updated paper should be accepted."
"abstract | strength | weakness | rebuttal_process | strength | decision  ==> Summary: <sep> This submission proposes a reinforcement learning framework based on human emotional reaction in the context of autonomous driving. This relies on defining a reward function as the convex combination of an extrinsic (goal oriented) reward, and an intrinsic reward. This later reward is learnt from experiments with humans performing the task in a virtual environment, for which emotional response is quantified as blood volume pulse wave (BVP). The authors show that including this intrinsic reward lead to a better performance of a deep Q networks, with respect to using the extrinsic reward only. <sep> Evaluation: <sep> Overall the proposed idea is interesting, and the use of human experiments to improve a reinforcement learning algorithm offers interesting perspectives. The weakness of the paper in my opinion is the statistical analysis of the results, the lack of in depth evaluation of the extrinsic reward prediction and the rather poor baseline comparison. <sep> Detailed comments: <sep> 1. Statistical analysis <sep> The significance of the results should be assessed with statistical methods in the following results: <sep> Section 4.1: Please provide and assessment of the significance of the testing loss of the prediction. For example, one could repetitively shuffle blocks of the target time series and quantify the RMSE obtained by the trained algorithm to build an H0 statistic of random prediction. <sep> Section 4.2: the sentence ""improves significantly when lambda is either non-zero or not equal to 1"" does not seem valid to me and such claim should in any case be properly evaluated statistically (including correction for multiple comparison etc…). <sep> Error bars: please provide a clear description in the figure caption of what the error bars represent. Ideally in case of small samples, box plots would be more appropriate. <sep> 2. Time lags in BVP <sep> It would be interesting to know (from the literature) the typical latency of BVP responses to averse stimuli (and possible the latency of the various mechanisms, e.g. brain response, in the chain from stimuli to BVP). Moreover, as latency is likely a critical factor in anticipating danger before it is too late, it would important to know how the prediction accuracy evolves when learning to predict at different time lags forward in time, and how such level of anticipation influence the performance of the Q-network. <sep> 3. Poor baseline comparison <sep> The comparison to reward shaping in section 4.4 is not very convincing. One can imagine that what counts is not the absolute distance to a wall, but the distance to a wall in the driving direction, within a given solid angle. As a consequence, a better heuristic baseline could be used. <sep> Moreover, it is unclear whether the approaches should be compared with the same lambda: the authors need to provide evidence that the statistics (mean and possibly variance) of the chosen heuristic is match to the original intrinsic reward, otherwise it is obvious that the lambda should be adapted. <sep> 4. Better analysis of figure 5-6(Minor) <sep> I find figure 5-6 very interesting and I would suggest that the authors fully comment on these results. E.g. : (1) why the middle plot of Fig. 6 mostly flat, and why such differences between each curve from the beginning of the training. (2) Why the goal oriented task leads to different optimal lambda, is this just a normalization issue?","The paper considers the problem of incorporating human physiological feedback into an autonomous driving system, where minimization of a predicted arousal response is used as an additional source of reward signal, with the intuition that this could be used as a proxy for training a policy that is risk-averse. <sep> Reviewers were generally positive about the novelty and relevance of the approach but had methodological concerns. In particular, concerns about the weighting of the intrinsic vs. extrinsic reward (why under different settings the optimal tradeoff parameter was different, how this affects the optimal policy if the influence of the intrinsic reward is not decreased with time). Additional baseline experiments were requested and performed, and the paper was modified to significantly incorporate other feedback such as drawing connections to imitation learning. A title change was proposed and accepted to reflect the focus on the application of risk aversion (I'd ask that the authors update the paper OpenReview metadata to reflect this). <sep> At a high level, I believe this is an original and interesting contribution to the literature. I have not heard from two of three reviewers regarding whether their concerns were addressed, but given that their concerns appear to me to have been addressed (and their initial scores indicated that the work met the bar for acceptance, if only marginally), I am inclined to recommend acceptance."
"strength | suggestion  ==>  ==> Dual Block-Coordinate Frank-Wolfe (Dual-BCFW) has been widely used in the literature of non-smooth and strongly-convex stochastic optimization problems, such as (structural) Support Vector Machine. To my knowledge, the submission is the first sound attempt to adapt this type of Dual-based algorithm for optimization of Deep Neural Network, which employs a proximal-point method that linearizes not the whole loss function but only the DNN (up to the logits) to form a convex subproblem and then deal with the loss part in the dual. <sep> The attempt is not perfect (actually with a couple of issues detailed below), but the proposed approach is inspiring and I personally would love it published to encourage more development along this thread.  The following points out a couple of items that could probably help further improve the paper. <sep> *FW vs BCFW* <sep> The algorithm employed in the paper is actually not Frank-Wolfe (FW) but Block-Coordinate Frank-Wolfe (BCFW), as it minimizes w.r.t. a block of dual variables belonging to the min-batch of samples. <sep> *Batch Size* <sep> Though the algorithm can be easily extended to the min-batch case, the author should discuss more how the batch size is interpreted in this case (i.e. minimizing w.r.t. a larger block of dual variables belonging to the batch of samples) and the algorithmic block (Algorithm 1) should be presented in a way reflecting the batch size since this is the way people use an algorithm in practice (to improve the utilization rate of a GPU). <sep> *Convex-Conjugate Loss* <sep> The Dual FW algorithm does not need to be used along with the hinge loss (SVM loss). All convex loss function can derive a dual formulation based on its convex-conjugate. See [1,2] for examples. It would be more insightful to compare SGD vs dual-BCFW when both of them are optimizing the same loss functions (either hinge loss or cross-entropy loss) in the experimental comparison. <sep> [1] Shalev-Shwartz, Shai, and Tong Zhang. ""Stochastic dual coordinate ascent methods for regularized loss minimization."" JMLR (2013) <sep> [2] Tomioka, Ryota, Taiji Suzuki, and Masashi Sugiyama. ""Super-linear convergence of dual augmented Lagrangian algorithm for sparsity regularized estimation."" JMLR (2011). <sep> *BCFW vs BCD* <sep> Actually, (Lacoste-Julien, S. et al., 2013) proposes Dual-BCFW to optimize structural SVM because the problem contains exponentially many number of dual variables. For typical multiclass hinge loss problem the Dual Block-Coordinate Descent that minimizes w.r.t. all dual variables of a sample in a closed-form update converges faster without extra computational cost. See the details in, for example, [3, appendix for the multiclass hinge loss case]. <sep> [3] Fan, Rong-En, et al. ""LIBLINEAR: A library for large linear classification."" JMLR (2008). <sep> *Hyper-Parameter* <sep> The proposed dual-BCFW still contains a hyperparameter (eta) due to the need to introduce a convex subproblem, which makes its number of hyperparameters still the same to SGD.","The paper was judged by the reviewers as providing interesting ideas, well-written and potentially having impact on future research on NN optimization. The authors are asked to make sure they addressed reviewers comments clearly in the paper."
"strength | suggestion | decision  ==> This submission closely builds upon an earlier work (Xie et al., 2017, Gal and Ghahramani, 2016) and proposes a new data noising technique motivated by Bayesian RNNs. Specifically, the key contribution is to extend Gal and Ghahramani (2016) to word embedding noising, while drawing inspiration from trational data smoothing techniques. Some variants are discussed, including those motivated by linear interpolation and Kneser-Ney smoothing, just as in Xie et al., 2017. Empirical evaluation is performed with language modeling experiments, and the proposed methods outperforms comparable baselines. One can imagine such a technique could be useful in many other sequence tasks. <sep> The paper is well-motivated and clearly written, and the experiments seem reasonable to me. Therefore I would vote for acceptance. My concern, which is not major, is that the proposed method might be a bit incremental based on Xie et al. (2017) and Gal and Ghahramani (2016). <sep> Pros: <sep> - Theoratical justification seems reasonably sound to me. <sep> - Strong empirical performance. <sep> Cons: <sep> - It would be interesting to see how the proposed technique works when applied to state-of-the-art models. <sep> Details: <sep> - I'm not entirely familiar with Gal and Ghahramani (2016), but I'm assuming the discussion in Section 3 and how it extends to word embeddings are from this earlier work. Please correct me if I'm wrong, so that I can adjust my recommendation accordingly. <sep> - I can't find anything describing how \\sigma is determined. <sep> - Page 4, the paragraph of `Training.` I can't parse `we go though sequence t multiple times`. <sep> - \\ell_2 Regularization for `VS` models. I'm confused here, isn't the coefficient for VS determined by Eq. 3? Why is it still tuned in Section 5.1? <sep> - Elementwise smoothing: I'm confused why one needs to sample \\alpha for each dimension. Can't it be done by sampling a mask, just as in dropout?","as r1 and r2 have pointed out, this work presents an interesting and potentially more generalizable extension of the earlier work on introducing noise as regularization in autoregressive language modelling. although it would have been better with more extensive evaluation that goes beyond unsupervised language modelling and toward conditional language modelling, but i believe this is all fine for this further work to be left as follow-up. <sep> r3's concern is definitely valid, but i believe the existing evaluation set as well as exposition merit presentation and discussion at the conference, which was shared by the other reviewers as well as a programme chair."
"abstract | strength | rebuttal_process | weakness | decision | suggestion  ==>  ==> Summary: <sep> The paper presents an analysis and numerical evaluation of stagewise SGD, ADAGRAD and Stochastic momentum methods for solving stochastic non-smooth non-convex optimization problems. <sep> Comments: <sep> I find the ideas presented in this paper very interesting. The convergence analysis seems correct and the paper is reasonably well written, and tackles an important problem. <sep> The analysis holds for μ-weekly convex functions. This assumption is really important for the development of the algorithm and the proposed analysis. I like the fact that the authors provide two examples showing that popular objective functions in machine learning satisfy this assumption. <sep> The numerical evaluation is adequate showing the effectiveness  of the proposed stagewise algorithms.  However i have the follow suggestions/minor comments: <sep> 1) It will be nice to have also some plots showing the performance of the proposed method on the ImageNet dataset. <sep> 2) Another possible nice experiment will be a comparison of the four stagewise methods (SGD,ADAGRAD,SHB,SNAG) on the same dataset. Which one behaves better? <sep> Minor Comments: <sep> 1) The captions of the figures can be more informative (mention also the division by column). First column is SGD, Second column Adagrad, etc. <sep> 2) Typos: <sep> Section 1, last bullet point, second line: ""stagwise"" <sep> Section 5, second paragraph , first line :""their their"" <sep> page 8, 3 line from the bottom:  ""seems, indicate"" <sep> 2) Missing reference. <sep> In the area of stochastic gradient methods with momentum many papers have been proposed recently for the case of convex optimization that worth to be mentioned: <sep> Gadat, Sébastien, Fabien Panloup, and Sofiane Saadane. ""Stochastic heavy ball."" Electronic Journal of Statistics 12.1 (2018): 461-529. <sep> Loizou, Nicolas, and Peter Richtárik. ""Momentum and stochastic momentum for stochastic gradient, Newton, proximal point and subspace descent methods."" arXiv preprint arXiv:1712.09677 (2017). <sep> Lan, Guanghui, and Yi Zhou. ""An optimal randomized incremental gradient method."" Mathematical programming (2017): 1-49. <sep> Overall, I suggest to accept this paper.","This paper develops a stagewise optimization framework for solving non smooth and non convex problems. The idea is to use standard convex solvers to iteratively optimize a regularized objective with penalty centered at previous iterates - which is standard in many proximal methods. The paper combines this with the analysis for non-smooth functions giving a more general convergence results. Reviewers agree on the usefulness and novelty of the contribution. Initially there were concerns about lack of comparison with current results, but updated version have addressed this issue. The main weakness is that the results only holds for \\mu weekly convex functions and the algorithm depends on the knowledge of \\mu. Despite this limitations, reviewers believe that the paper has enough new material and I suggest for publication. I suggest authors to address these issues in the final version."
"strength | weakness | rebuttal_process | decision  ==> OVERVIEW: <sep> The authors tackle the problem of detecting small/low resolution objects in an image. Their key idea is that detecting bigger objects is an easier task and can be used to guide the detection of smaller objects. This is done using the ""Feature Intertwiner""  which consists of two branches, one for the larger objects (more reliable set that is also easier to detect) and one for the smaller objects (less reliable set). The second branch contains a make-up layer learned during training (which acts as the guidance from the more reliable set) that helps compensate details needed for detection. The authors define a class buffer that contains representative elements of object features from the reliable set for every category & scale and an intertwiner loss that computes the L2 loss between the features from the less reliable set & the class buffer. They also use an Optimal Transport procedure with a Sinkhorn divergence loss between object features from both sets. The overall loss of the system is now a sum of the detection loss, the intertwiner loss and the optimal transport loss. They evaluate their model on the COCO Object detection challenge showing state-of-the-art performance. They also provide thorough ablation analysis of various design choices. The qualitative result in Fig.1 showing well clustered features for both high & low resolution objects via t-SNE is a nice touch. <sep> COMMENTS: <sep> Clarity - The paper is well written and easy to follow. <sep> Originality & Significance - The paper tackles an important problem and provides a novel solution. <sep> Quality - The paper is complete in that it tackles an important problem, provides a novel solution and demonstrates via thorough experiments the improvement achieved using their approach. <sep> QUESTIONS: <sep> 1. The Class Buffer seems very restricted in having a single element per object category per scale to represent all features. The advantage of forcing such a representation is tight clustering in the feature space. But, wouldn't a dictionary approach with multiple elements give more flexibility to the model and learn a richer feature representation at the cost of not-so-good clustering ? <sep> 2. Any comment on why you drop performance for couch ? (and baseball bat + bedroll) <sep> 3. In Table 4 of Appendix where you compare with more object detection results, I find it interesting that Mask RCNN, updated results has a might higher AP_S (43.5) compared to you (27.2) and everyone else. I was expecting you to be the best under that metric due to the explicit design for small objects. They (MaskRCNN, updated results) are also significantly better than the rest under AP_M but worse under AP_L. Can you explain this behavior ? Is the ResNeXt backbone that much better for small objects ?","The paper proposes an interesting idea (using ""reliable"" samples to guide the learning of ""less reliable"" samples). The experimental results and detailed analysis show clear improvement in object detection, especially small objects. <sep> On the weak side, the paper seems to focus quite heavily on the object detection problem, and how to divide the data into reliable/less-reliable samples is domain-specific (it makes sense for object detection tasks, but it's unclear how to do this for general scenarios). As the authors promise, it will make more sense to change the title to ""Feature Intertwiner for Object Detection"" to alleviate such criticisms. <sep> Given this said, I think this paper is over the acceptance threshold and would be of interest to many researchers."
"rating_summary | misc | suggestion  ==> This paper proposed ""adversarial reprogramming"" of well-trained and fixed neural networks, which can be viewed as learning a trainable input perturbation on a fixed network for multi-tasking by using a different dataset (e.g., MNIST) from the original dataset (ImageNet) as input. Domain mapping functions (h_g and h_f) are required if the data have different dimensions. The key factor to enable adversarial reprogramming of a fixed network to perform a different task is by training the additive adversarial program as defined in (1). Experimental results show that 7 different ImageNet models (adversarially trained or not) can be reprogrammed for performing counting tasks, and MNIST and CIFAR-10 classifications. The authors also show that adversarial reprogramming is less effective on untrained networks. <sep> Although the idea of this paper is interesting,  the contribution is unclear and the ""adversarial"" setting is not well motivated. The detailed comments are as follows. <sep> 1. Unclear contribution - As mentioned in this paper, the main difference between ""adversarial reprogramming"" and transfer learning or multi-task learning is the fact that the network to be reprogrammed is fixed during reprogramming and was trained on a single task that is independent of the targeted task. However, the reprogramming results are not surprising given the fact that multi-task learning can be achieved on the same network. Given the fact that the perturbed input data (e.g., MNIST) is different from the original input data (ImageNet), what adversarial reprogramming demonstrates is actually a simple way of learning a new task via input perturbation to an unseen dataset at training time. However, transfer learning can be done in a similar way by simply fine-tuning the last (few) layers of a well-trained network. So the number of parameters required to be modified in order to ""reprogram"" a network is already known to be quite small via fine-tuning, which may even be less than the dimension of the adversarial program. In addition, given that the input of ImageNet model is high-dimensional and ImageNet images are likely to lie on a low dimensional manifold (but they are very different from hand-written digits or CIFAR images), the capability of reprogramming using deep models under this setting is expected and thus the contribution is unclear. <sep> 2. The ""adversarial"" setting is vague - I am very confused about why the experimental settings should be considered ""adversarial"", given the fact that ImageNet images and the three sets of adversarially perturbed images are quite different. What the experiments show is that a well-trained classifier has a large enough capacity to perform other tasks by simply training a perturbation on a different (out-of-distribution) dataset as inputs. It would make more sense to call this method ""adversarial"" if it can be used on ImageNet images to secretly implement some programmed tasks, while on the surface they are seemingly simply performing a typical classification task. <sep> 3. Limited novelty - How is adversarial program different from additional perturbation? Let alone the mapping function M in eqn (3), the adversarial program is nothing but a constrained perturbation (ranging from [-1,1] in each dimension). The optimization formulation in (3) can be seen as a  Carlini-Wager L2 attack with a simplified attack loss + L2 distortion regularization. Therefore, the proposed method has limited technical contribution and novelty. <sep> In summary, this paper has some interesting ideas, but the current presentation lacks clear motivation, and its technical contribution and implications need to be better highlighted.  The authors are suggested to better motivate this paper from the angle of studying the learning capacity of input perturbation induced multi-tasking learning of a well-trained and fixed neural network model, and compare the pros and cons with transfer learning based on fine-tuning and joint multi-task learning / meta-learning on the same network architecture. Based on my own reading, I truly feel that advocating  ""adversarial"" reprogramming does not add any value to this work, as its use for an adversary is not properly motivated (e.g., visual imperceptibility) and its training has no adversarial nature (e.g., GAN training). Titles like ""(Out-of-domain) Input perturbation induced reprogramming of neural networks"" should better justify the contents and experiments presented in this work. Lastly, the authors need to specify how equation (3) is different from the formulation of finding adversarial perturbations in existing literature. Otherwise,  the novelty of ""adversarial program"" is quite limited. <sep> ---- <sep> Post-rebuttal review: <sep> I appreciate the authors' efforts in including the new experiments in Sections 4.4 and 4.5. In my opinion, these new results and the discussion in Section 5.2 add great values to this work and make the contributions of this paper substantially clear. I've increased my rating to 6.",Reviewers mostly recommended to accept after engaging with the authors. I have decided to reduce the weight of AnonReviewer3 because of the short review. Please take reviewers' comments into consideration to improve your submission for the camera ready.
"misc | weakness | suggestion  ==> This work extends the applicability of the spline theory of deep networks explored in previous works of Balestriero/ Baraniuk. The previous works setup DNs as layer-wise max-affine spline operators (MASOs) and recovers several non-linearities practically used as special cases of these MASOs. The previous works already recover RELU variants and some downsampling operators that the current submission characterizes as ""hard"" quantization. <sep> The major contribution of this work is extending the application to ""soft"" quantization that recovers several new non-linear activations such as soft-max. It is well-known that the k-means algorithm can be considered as a run of an EM algorithm to recover the mean parameters of a gaussian mixture model. The ""hard"" to ""soft"" transformation, and any interpolation in between follows from combining this insight with the previous works. As such there isnt a major technical contribution imho in this work. Furthermore, the presented orthogonalization for easier inference has been used before in many works, some of which this submission also cites, most importantly in the previous work of Balestriero/ Baraniuk that this submission extends. <sep> Nevertheless there is value in novel results that may follow from previous works in a straightforward but non-trivial fashion, as long as it is well-presented and thoroughly researched and implication well-highlighted. This paper does that adequately, so I will suggest weak accept. Furthermore, this work could spark interesting future works and fruitful discussions at the *CONF*. It is well-written and the experimental evaluation is adequate. <sep> I would suggest a couple of ways to possibly improve the exposition. The paper is somewhat notation heavy. When considering single layers, the superscript for the layer could be dropped in favor of clarity. I would suggest moving the definition of MASOs to the main text, and present Proposition 8 in some form in the main text as well. To a reader not familiar with previous works, or with splines, this could be helpful. Use of orthogonalization could be highlighted not just a tool for tractability but also regularization. For inference on GMMs, it corresponds to a type of variational inference, which could be mentioned.","Dear authors, <sep> All reviewers liked your work. However, they also noted that the paper was hard to read, whether because of the notation or the lack of visualization. <sep> I strongly encourage you to spend the extra effort making your work more accessible for the final version."
"abstract | strength | rebuttal_process | rating_summary | decision  ==>  ==> The manuscript entitled ""Kernel Change-Point Detection with Auxiliary Deep Generative Models"" describes a novel approach to optimising the choice of kernel towards increased testing power in this challenging machine learning problem.  The proposed method is shown to offer improvements over alternatives on a set of real data problems and the minimax objective identified is well motivated, however, I am not entirely convinced that (a) the performance improvements arise for the hypothesised reasons, and (b) that the test setting is of wide applicability. <sep> A fundamental distinction between parametric and non-parametric tests for CPD in timeseries data is that the adoption of parametric assumptions allows for an easier introduction of strict but meaningful relationships in the temporal structure---e.g. a first order autoregressive model introduces a simple Markov structure---whereas non-parametric kernel tests typically imagine samples to be iid (before and after the change-point).  For this reason, the non-parametric tests may lack robustness to certain realistic types of temporal distributional changes: e.g. in the parameter of an autoregressive timeseries.  On the other hand, it may be prohibitively difficult to design parametric models to well characterise high dimensional data, whereas non-parametric models can typically do well in high dimension when the available data volumes are large.  In the present application it seems that the setting imagined is for low dimensional data of limited size in which there is likely to be non-iid temporal structure (i.e., outside the easy relative advantage of non-parametric methods).  For this reason it seems to me the key advantage offered by the proposed approach with its use of a distributional autoregressive process for the surrogate model may well be to introduce robustness against Type 1 errors due to otherwise unrepresented temporal structure in the base distribution (P).  In summarising the performance results by AUC it is unclear whether it is indeed the desired improvement in test power that offers the advantages or whether it is in fact a decrease in Type 1 errors. <sep> Another side of my concern here is that I disagree with the statement: ""As no prior knowledge of Q ... intuitiviely, we have to make G as close to P as possible"" interpretted as a way to maximise test power; as a way to minimise Type 1 errors, yes. <sep> Across change-point detection methods it is also important to distinguish key aspects of the problem formulation.  One particular specification here is that we have already some labelled instances of data known to come from the P distribution, and perhaps also some fewer instances of data labelled from Q.  This is distinct from fully automated change point detection methods for time series such as automatic scene selection in video data.  Another dissimilarity to that archetypal scenario is that here we suppose the P and Q distributions may have subtle differences that we're interested in; and it would also seem that we assume there is only one change-point to detect.  Or at least the algorithm does not seem to be designed to be applied in a recursive sense as it would be for scene selection. <sep> Finally there is no discussion here of computational complexity and cost?","This paper proposes a new kernel learning framework for change point detection by using a generative model. The reviewers agree that the paper is interesting and useful for the community. One of the reviewer had some issues with the paper but those were resolved after the rebuttal. The other two reviewers have short reviews and somewhat low confidence, so it is difficult to tell how this paper stands among other that exist in the literature. Overall, given the consistent ratings from all the reviewers, I believe this paper can be accepted."
"abstract | weakness | suggestion  ==> Summary: <sep> The authors take up an important problem in unsupervised deep reinforcement learning which is to learn perceptual reward functions for goal-conditioned policies without extrinsic rewards from the environment. The problem is important in order to push the field forward to learning representations of the environment without predicting value functions from scalar rewards and learn more generalizable aspects of the environment (the authors call this mastery) as opposed to just memorizing the best sequence of actions in typical value/policy networks. <sep> Model-based methods are currently hard to execute as far as mastery is concerned and goal-conditioned value functions are a good alternative. The authors, therefore, propose to learn UVFA (Schaul et al) with a learned perceptual reward function r(s, s_g) where 's' and 's_g' are current and goal observations respectively. They investigate a few choices for deriving this reward, such as pixel-space L2 distance, Auto-Encoder feature space, WGAN Discriminator (as done in SPIRAL - Ganin and Kulkarni et al), and their approach: cosine similarity based log-likelihood for similarity metric (as in Matching Networks).  They show that their approach works better than other alternatives on a number of visual goal-based tasks. <sep> Specific aspects: <sep> 1. A slight negative: I find the whole pipeline extremely hacky and raises serious questions on whether this paper/technique is easy to apply on a wide variety of tasks. It gives me the suspicion that the environments were cherry-picked for showing the success of the proposed method, though, that's, in general, true of most deep RL papers. It would be nice if the authors instead wrote the paper from the perspective of proposing a new benchmark (it would be amazing if the benchmark is open sourced so that it will lead to more people working specifically on this setting and a lot more comparisons). <sep> -- Revision: The pipeline is hacky, but getting GAN based reward learning to work is also not very straightforward. The authors do plan to release the detectors used for the benchmarking. <sep> 2. To elaborate on the above, these are the portions I find hacky: <sep> (i) Need for decoy observations to learn an approximate log-likelihood <sep> (ii) Using sparse reward for all transitions except the final terminal state: Yes, I am aware of the fact that HER has already shown sparse rewards are easier to learn value functions with, compared to dense rewards. But I am genuinely surprised that you have pretty much the same setting (ie re-label only terminal transition, r(s_T, s_g)) and motivate the need for learning a perceptual metric. If the information bits per transition is similar to HER in terms of the policy network's objective function, I am not sure why you need to learn a perceptual reward then? There's also no baseline comparison with just naive HER on image observations. That will be worth seeing actually. I feel this kind of comparisons are more interesting and important for the message of the paper. Note that in other papers cited in this, such as SPIRAL, UPN, etc, the reward metrics are used for every state transition. <sep> (iii) In addition to naive image HER, I would really like to see a SPIRAL + HER baseline as is. ie use the GAN reward for all transitions and also use relabeling for successes. My prior belief is that this will work really well. I would really like to know how the reward for each transition in the trajectory works (both for SPIRAL and your approach) and how the naive HER works. <sep> --Revision: The authors have added HER baselines. Agreed with the authors that comparison of per-timestep perceptual reward vs terminal state perceptual reward is a good topic for future work. <sep> 3. Another place I really found confusing throughout the paper is the careless swapping of notations, especially in the xi(h) and e(h). Please use consistent notations especially in equation (3), the pseudocode and the rest of the paper. <sep> 4.  a. Would be nice to know if a VAE feature space metric is bad, but not a strict requirement if you don't have time to do it. But I think showing Euclidean metric baseline on VAE is better than an AE. <sep> b. Another baseline that is related is to learn a metric with a triplet loss as in Sermanet's work. Or any noise contrastive loss approach (such as CPC). The matching networks approach is similar in spirit. Just pointing out as reference and something worth trying, but not expecting it to be done for rebuttal. <sep> 5. Overall, I think this is a good paper, gives a good overview of an important problem; the matching networks idea is nice and simple; but the paper could be more broader in terms of writing than trying to portray the success of DISCERN specifically. I would be happy accepting it even if the SPIRAL baseline or VAE / AE baseline worked as well as the matching networks because I think those approaches are more principled and likely to require fewer hacks and could be applied to a lot of domains easily. I also hope the authors run the baselines I asked for just to make the paper more scientifically complete. <sep> 6. Not a big deal for me in terms of deciding acceptance, but for the sake of good principles in academics, related work could be stronger, though I can understand it must have been small purely due to page limits. <sep> Some papers which could be cited are (1) Unsupervised Perceptual Rewards (though it uses AlexNet pre-trained), (2) Time Contrastive Networks (which also uses AlexNet and doesn't really work on single-view tasks but is a good citation to add), (3) Original UVFA  (definitely has to be there given you even use the abbreviation for the keywords description of the paper) <sep> 7. Some slightly incorrect facts/wording in the paper: The two papers cited in model-based methods (Oh and Chiappa) are not really unsupervised. They use a ton of demonstrations to learn those world models. Better citation might be David Ha's World Models or Chelsea Finn's Video Prediction.","This paper introduces an unsupervised algorithm to learn a goal-conditioned policy and the reward function by formulating a mutual information maximization problem. The idea is interesting, but the experimental studies seem not rigorous enough. In the final version, I would like to see some more detailed analysis of the results obtained by the baselines (pixel approaches), as well as careful discussion on the relationship with other related work, such as Variational Intrinsic Control."
"misc | rebuttal_process | misc | decision | strength  ==> Summary: <sep> This paper proposes learning reward functions via inverse reinforcement learning (IRL) for vision-based instruction following tasks like ""go to the cup"". The agent receives the language instruction (generated via grammar templates) and a set of four images (corresponding to four cardinal directions) from virtual cameras mounted on the agent as input at every time step and its aim is either to 1. navigate to the goal location (navigation task) or 2. move an object from one place to another (pick task). <sep> The really interesting part in this paper is learning reward functions such that they generalize across different tasks and environments (e.g. indoor home layouts). This differentiates it from the standard IRL setting where reward functions are learnt and then policies optimized on this reward function on the *same* environment. <sep> In order to generalize across tasks and environments a slight modification to the max-ent IRL gradient equations are made: 1. Similar to meta-learning the gradient is taken with respect to multiple tasks (in a sampling-based manner) and 2. Making the reward function a function of not just states and actions but also language context. The overall algorithm (Algorithm 1) is simple and the critical step of computing an optimal policy to compute the IRL gradient is done by assuming that one has access to full state and dynamics and essentially running a planner on the MDP. This assumption is not unreasonable since in a simulator one has access to the full dynamics and can hence one can compute the optimal trajectories by planning. <sep> Experiments are presented on the SUNCG dataset of indoor environments. Two baselines are presented: One using behavior cloning (BC) and an oracle baseline which simply regresses to the ground truth reward function which is expected to be an upper bound of performance. Then DQN is used (with and without reward shaping) using the learnt reward functions to learn policies which are shown to have better performance on different tasks. <sep> Comments and Questions: <sep> - The paper is generally well-written and easy to understand. Thanks! <sep> - The idea of using IRL to learn generalizable reward functions to learn policies so as to aid transfer between environments in such vision-language navigation tasks is interesting and clearly shows benefits to behavior cloning. <sep> - One of my main concerns (and an interesting question that this paper naturally raises) is how does this approach compare to imitation learning (not vanilla behavior cloning which is straight-up supervised learning and has been theoretically and empirically shown to have worse performance due to distribution shifts. See Ross and Bagnell, 2011, Ross, Gordon, Bagnell 2012 (DAgger, Ross and Bagnell 2014 (AggreVate), Chang et al., 2015 (LOLS), etc). If the same budget of 10 demonstrations per environment is used via DAgger (where say each iteration of DAgger gets say 2 or 3 demonstrations until the budget is exhausted) how does it compare? Note online version of DAgger has already been used in similar settings in ""Vision-and-Language Navigation: Interpreting visually-grounded navigation instructions in real environments"" by Anderson et al, CVPR 2018. The main difference from their setting is that this paper considers higher level tasks instead of taking as input low-level turn-by-turn language inputs. <sep> - The following papers are relevant and should be cited and discussed: <sep> ""Vision-and-Language Navigation: Interpreting visually-grounded navigation instructions in real environments"" by Anderson et al, CVPR 2018. <sep> ""Embodied Question Answering"", Das et al, CVPR 2018. <sep> Update: <sep> ------------ <sep> After looking at other reviews and author rebuttals to all reviews I am raising my grade.","This paper generated a lot of discussion (not all of it visible to the authors or the public). <sep> R1 initially requested reasonable comparisons, but after the authors provided a response (and new results), R1 continued to recommend rejecting the paper simply because they personally did not find the manuscript insightful. Despite several requests for clarification, we could not converge on a specific problem with the manuscript. Ungrounded gut feelings are not grounds for rejection. <sep> After an extensive discussion, R2 and R3 both recommend accepting the paper and the AC agrees. Paper makes interesting contributions and will be a welcome addition to the literature."
"abstract | strength  ==>  ==> This work presents Backpropamine, a neuromodulated plastic LSTM training regime. It extends previous research on differentiable Hebbian plasticity by introducing a neuromodulatory term to help gate information into the Hebbian synapse. The neuromodulatory term is placed under network control, allowing it to be time varying (and hence to be sensitive to the input, for example). Another variant proposes updating the Hebbian synapse with modulated exponential average of the Hebbian product. This average is linked to the notion of an eligibility trace, and ties into some recent biological work that shows the role of dopamine in retroactively modulating synaptic plasticity. <sep> Overall the work is nicely motivated and clearly presented. There are some interesting ties to biological work -- in particular, to retroactive plasticity phenomena. There should be sufficient details for a reader to implement this model, thought there are some minor details missing regarding the experimental setup, which will be addressed below. <sep> The authors test their model on three tasks: cue-award association, maze learning, and Penn Treebank (PTB). In the cue-award association task the retroactive and simple modulation networks perform well, while the non-modulated and non-plastics fail. For the maze navigation task the modulated networks perform better than the non-modulated networks, though the effect is less pronounced. Finally, on PTB the authors report improvements over baseline LSTMs. <sep> One of the main claims of this paper is that neuromodulated plastic LSTMs...outperform standard LSTMs on a benchmark language modeling task"", and that therefore ""differentiable neuromodulation of plasticity offers a powerful new framework for training neural networks"". This claim is unfortunately unfounded for a very important reason: the LSTM performance is not at all close to that which can be achieved by LSTMs in general. The authors cite such models in the appendix (Melor et al), but claim that ""much larger models"" are needed, potentially with other mechanisms, such as dropout. Though this may be true, these models still undermine the claim that ""neuromodulated plastic LSTMs...outperform standard LSTMs on a benchmark language modeling task"". This claim is simply not true, and more care is needed in reporting the results here in the wider context of the literature. Also, I am left wondering what are considered the parameters of the models -- are only the neuromodulatory terms considered as the additional trainable parameters compared to baseline LSTMs? How are the Hebbian synapses themselves considered in this calculation? If the Hebbian synapses are not considered, then the authors need a control with matched memory-capacities to account for the extra capacity afforded by the Hebbian synapses. Given the ties between Hebbian synapses and attention (see Ba et al), an important control here could be an LSTM with Bahdanau (2014) style attention. <sep> Finally, the style (font) of the paper does not adhere to the *CONF* style template, and must be changed. <sep> Overall, the ideas presented in the paper are intriguing, and further research down this line is encouraged. However, in its current state the work lacks sufficiently strong baselines to support the paper's claims; thus, the merits of this approach cannot yet be properly assessed.","The authors consider the problem of active plasticity in the mammalian brain, seen as being a means to enable lifelong learning. Building on the recent paper on differentiable plasticity, the authors propose a learnt, neuro-modulated differentiable plasticity that can be trained with gradient descent but is more flexible than fixed plasticity. The paper is clearly motivated and written, and the tasks are constructed to validate the method by demonstrating clear cases where non-modulated plasticity fails completely but where the proposed approach succeeds. On a large, general language modeling task (PTB) there is a small but consistent improvement over LSTMS. The reviewers were very split on this submission, with two reviewers focusing on the lack of large improvements on large benchmarks, and the other reviewer focusing on the novelty and success of the method on simple tasks. The AC tends to side with the positive review because of the following observations: the method is novel and potentially will have long term impact on the field, the language modeling task seems like a poor fit to demonstrate the advantages of the dynamic plasticity, so focusing on that benchmark overly much is misleading, and the paper is high-quality and interesting to the community."
"abstract | misc | weakness | suggestion | ac_disagreement | decision  ==>  ==> Summary: This paper mixes automated theorem proving with machine learning models. The final goal, of course, is to be able to train a model that works in conjunction with an automated theorem proving system to efficiently prove theorems, and, ideally, in a way that resembles the way humans prove theorems. This is a distant goal, and the authors instead focus on several tractable tasks that are required for future progress in this direction. They start by integrating the Coq theorem proving environment with ML frameworks, allowing for the creation of models that perform various tasks related to theorem proving. In particular, they focus on two tasks. One is to estimate how many steps are left to complete the proof given a current proof state. The other is to determine what is a good choice of next step. Finally, they also consider issues surrounding representations of the various data structures involved in proofs (i.e., the proof tree, variables, etc.). They test various models on a synthetic nearly trivial logical expression proof, along with a more complicated (and meaningful real world) group theory result. <sep> Strengths: This is a very important area. Automated theorem proving has a potentially very significant impact, and being able to take advantage of some of the recent successes in ML would be excellent. The main environment proposed here, integrating PyTorch with Coq could potentially be a very useful platform for future research in this area. The paper exposes many interesting questions, and I generally think we need more exploratory papers that open up an area (as opposed to seeking to finalize existing areas) <sep> Weaknesses: The paper is pretty tough to understand without a lot of background in all of the existing theorem proving work (which might be fine for a conference in this area, but for this venue it would be nice to be more self-contained). The organization could also use some work, since it's often tough to figure out what the authors actually did. The experimental results seem very preliminary---although it's hard to say, as there is no easy way to compare the results to anything else out there. In general a lot of details seem missing. <sep> Verdict: The authors admit this is a preliminary work, and I agree with that. The paper certainly introduces many more questions than it answers. However, I think that in this case it's a good thing, and this type of paper has the potential to inspire a lot of new and exciting research, so I voted for acceptance. <sep> Comments and questions: <sep> - As mentioned, a lot of the terminology is introduced very quickly and could stand to be more self-contained, i.e., ""tactics"" could be defined as being simple transformations that are applied to a current proof state to obtain another proof state, and each language has a library of tactics available. <sep> - Probably the major contribution of the work is the integration of the CoQ and Pytorch, so a bit more content describing how the Python data structures that wrap around Coq structures would be interesting here. <sep> - I didn't really understand one of the major contributions: the embedding function for the M_i conditioned on the environment. How does the sampled Gaussian vector work here? In general this section is pretty confusing, it would be great to include a schematic to show how the different levels of embeddings for different structures work here. <sep> - How does the real-world dataset work? Does the dataset contain one automated proof of the entire theorem, or several different proofs (ultimately produced by different user choices)? Are you measuring accuracy on the proofs of individual lemmas?","This paper provides an RL environment defined over Coq, allowing for RL agents and other such systems to to be trained to propose tactics during the running of an ITP. I really like this general line of work, and the reviewers broadly speaking did as well. The one holdout is reviewer 3, who raises important concerns about the need for further evaluation. I understand and appreciate their points, and I think the authors should be careful to incorporate their feedback not only in final revisions to the paper, but in deciding what follow-on work to focus on. Nonetheless, and with all due respect to reviewer 3, who provided a review of acceptable quality, I am unsure the substance of their review merits a score as low as they have given. Considering the support the other reviews offer for the paper, I recommend acceptance for what the majority of reviewers believes is a good first step towards one day proving substantial new theorems using ITP-ML hybrids."
"strength | weakness | rating_summary  ==> The paper 'Learning Discrete Wasserstein Embeddings' describes a new embedding method that, <sep> contrary to usual embedding approaches, does not try to embed (complex, structured) data into an <sep> Hilbertian space where Euclidean distance is used, but rather to the space of probability measures endowed with the Wasserstein distance. As such, data are embed on an empirical distribution supported by Diracs, which locations can be determined by a map that is learnt from data. <sep> Interestingly, authors note a 'potential universality' for W_p(R^3) (from a result of Andoni et al., 2015), <sep> suggesting that having Diracs in R^3 could embed potentially any kind of metric on symbolic data. <sep> Experimental validations are presented on graph and word embedding, and a discussion on visualization of the embedding is also proposed (since the Diracs are located in a low dimensional space). <sep> All in all the paper is very clear and interesting. The idea of embedding in a Wasserstein space is original (up to my knowledge) and well described. I definitely believe that this work should be presented at *CONF*. I have a couple of questions and remarks for the authors: <sep> - It is noted in section 3.2 that both Diracs location and associated weights could be optimized. Yet the authors chose to only optimize locations. Why not only optimizing the weights (as in an Eulerian view of probability distributions) ? The sentence involving works of Brancolini and Claici 2018 is not clear to me. Why weighting does not improve asymptotically the approximation quality ? <sep> - Introducing the entropic regularization is mainly done for being able to differentiate the Wasserstein distance. However, few is said on the embeddability of metrics in W^\\lambda_p(R). Is using an entropic version of W moderating the capacity of embedding ? At least experimentally, a discussion could be made on the choice of the regularization parameter, at least in section 4.1. In eq. (9), it seems that it is not the regularized version of W. ? <sep> - I assume that the mapping is hard to invert, but did the authors tried to experiment reconstructing an object of interest by following a geodesic in the Wasserstein space ? <sep> - It seems to me that authors never give generalization results. What is the performance of the metric approximation when tested on unseen graphs or words ? This point should be clarified in the experiment.","+ An interesting and original idea of embedding words into the (very low dimensional) Wasserstein space, i.e. clouds of points in a low-dimensional space <sep> + As the space is low-dimensional (2D), it can be directly visualized. <sep> + I could imagine the technique to be useful in social / human science for data visualization, the visualization is more faithful to what the model is doing than t-SNE plots of high-dimensional embeddings <sep> + Though not the first method to embed words as densities but seemingly the first one which shows that multi-modality / multiple senses are captured (except for models which capture discrete senses) <sep> + The paper is very well written <sep> - The results are not very convincing but show that embeddings do capture word similarity (even when training the model on a small dataset) <sep> - The approach is not very scalable (hence evaluation on 17M corpus) <sep> - The method cannot be used to deal with data sparsity, though (very) interesting for visualization <sep> - This is mostly an empirical paper (i.e. an interesting application of an existing method) <sep> The reviewers are split. One reviewer is negative as they are unclear what the technical contribution is (but seems a bit biased against empirical papers). Another two find the paper very interesting."
"abstract | strength | decision  ==>  ==> The work proposes a Bayesian neural network model that is a hybrid between autoencoders and GANs, although it is not presented like that. Specifically, the paper starts from a Bayesian Neural Network model, as presented in Gal and Ghahramani, 2016 and makes two modifications. <sep> First, it proposes to define one Bernoulli variational distribution per weight kernel, instead  of per patch (in the original work there was one Bernoulli distribution per patch kernel). As the paper claims, this reduces the complexity to be exponential to the number of weights, instead of the number of patches, which leads to a much smaller number of possible models. Also, because of this modification the same variational distributions are shared between locations, being closer to the convolutional nature of the model. <sep> The second modification is the introduction of synthetic likelihoods. Specifically, in the original network the variational distributions are designed such that the KL-divergence of the true posterior p(ω|X, y) and the approximate posterior q(ω) is minimiezd. This leads to the optimizer encouraging the final model to be close to the mean, thus resulting in less diversity. By re-formulating the KL-divergence, the final objective can be written such that it depends on the likelihood ratio between generated/""fake"" samples and ""true"" data samples. This ratio can then be approximated by a GAN-like discriminator. As the optimizer now is forced to care about the ratio instead of individual samples, the model is more diverse. <sep> Both modifications present some interesting ideas. Specifically, the number of variational parameters is reduced, thus the final models could be much better scaleable. Also, using synthetic likelihoods in a Bayesian context is novel, to the best of my knowledge, and does seem to be somewhat empirically justified. <sep> The negative points of the paper are the following. <sep> - The precise novelty of the first modification is not clearly explained. Indeed, the number of possible models with the proposed approach is reduced. However, what is the degree to which it is reduced. With some rough calculations, for an input image of resolution 224x224, with a kernel size of 3x3 and stride 1, there should be about 90x90 patches. That is roughly a complexity of O(N^2) ~ 8K (N is the number of patches). Consider the proposed variational distributions with 512 outputting channels, this amount to 3x3x512 ~ 4.5K. So, is the advantage mostly when the spatial resolution of the image is very high? What about intermediate layers, where the resolution is typically smaller? <sep> - Although seemingly ok, the experimental validation has some unclarities. <sep> + First, it is not clear whether it is fair in the MNIST experiment to report results only from the best sampled model, especially considering that the difference from the CVAE baseline is only 0.5%. The standard deviation should also be reported. <sep> + In Table 2 it is not clear what is compared against what. There are three different variants of the proposed model. The WD-SL does exactly on par with the Bayes-Standard (although for some reason the boldface font is used only for the proposed method. The improvement appears to come from the synthetic likelihoods. Then, there is another ""fine-tuned"" variant for which only a single time step is reported, namely +0.54 sec. Why not report numbers for all three future time steps? Then, the fine-tuned version (WD-SL-ft) is clearly better than the best baselines of Luc et al., however, the segmentation networks are also quite different (about 7% difference in mIoU), so it is not clear if the improvement really comes from the synhetic likelihoods or from the better segmentation network. In short, the only configuration that appears to be convincing as is is for the 0.06 sec. I would ask the authors to fill in the blank X spots and repeat fair experiments with the baseline. <sep> - Generally, although the paper is ok written, there are several unclarities. <sep> + Z_K in eq. (4) is not defined, although I guess it's the matrix of the z^{i, j}_{k, k'} <sep> + In eq (6) is the z x σ a matrix or a scalar operation? Is z a matrix or a scalar? <sep> + The whole section 3.4 is confusing and it feels as if it is there to fill up space. There is a rather intricate architecture, but it is not clear where it is used. In the first experment a simple fully connected network is used. In the second experiment a ResNet is used. So, where the section 3.4 model used? <sep> + In the first experiment a fully connected network is used, although the first novelty is about convolutions. I suppose the convolutions are not used here? If not, is that a fair experiment to outline the contributions of the method? <sep> + It is not clear why considering the mean of the best 5% predictions helps with evaluating the predicted uncertainty? I understand that this follows by the citation, but still an explanation is needed. <sep> All in all, there are some interesting ideas, however, clarifications are required before considering acceptance.","This paper proposes a method to encourage diversity of Bayesian dropout method. A discriminator is used to facilitate diversity, which the method deal with multi-modality. Empirical results show good improvement over existing methods. This is a good paper and should be accepted."
"abstract | weakness | rebuttal_process  ==> This paper studies sufficient dimension reduction problem, and proposes an incremental sliced inverse regression algorithm. Numerical experiments are provided to demonstrate the effectiveness of the proposed algorithms. <sep> The sliced inverse regression here is nothing but generalized eigenvalue decomposition: <sep> Ax=lambda Bx. <sep> Note that Multiclass Fisher Linear Discriminant Analysis, Canonical Correlation Analysis, Nonlinear Manifold Embedding and many subspace learning methods can also be formulated as generalized eigenvalue decomposition. All these methods need to compute covariance-like matrices in the additive form, which makes incremental update very convenient. <sep> The incremental generalized eigenvalue decomposition has been extensively studied for over decades, especially between 1995 and 2005 in the face recognition community. I am just listing a few here: <sep> Ye et al., IDR/QR: An Incremental Dimension Reduction Algorithm via QR Decomposition, 2005 <sep> Law and Jain, Incremental Nonlinear Dimensionality Reduction by Manifold Learning, 2006 <sep> Yan et al. Towards incremental and large scale face recognition, 2011 <sep> Ghassabeh et al. A New Incremental Face Recognition System, 2007 <sep> Song et al. A Novel Supervised Dimensionality Reduction Algorithm for Online Image Recognition, 2006. <sep> Wang et al. Incremental two-dimensional linear discriminant analysis with applications to face recognition, 2010. <sep> Salman et al. Efficient update of the covariance matrix inverse in iterated linear discriminant analysis, 2010 <sep> Park and Park, A comparison of generalized linear discriminant analysis algorithms, 2008 <sep> Wang, INCREMENTAL AND REGULARIZED LINEAR DISCRIMINANT ANALYSIS, 2012 <sep> These algorithms become less popular/known now, because (1) they are not scalable and efficient for large p, and (2) these classical dimensionality reduction methods perform poorly in many tasks, compared with the state of art results. <sep> This paper only cites a few papers on incremental LDA,  but does not even mention that both LDA and SIR are essentially solving similar optimization problems. Moreover, it does not compare the results with any of the above references, either. <sep> This paper even claims applying the Sherman–Morrison formula as the contribution. However, such an  update has been used in Salman et al. 2010, Park and Park 2008, Wang 2012. <sep> In summary, this paper is far below the bar of *CONF*. <sep> Minor: There are numerous typos in this paper. The authors even misspell ""Morrison"" in the Sherman–Morrison formula as ""Morison"".","The paper investigates an incremental form of Sliced Inverse Regression (SIR) for supervised dimensionality reduction. Unfortunately, the experimental evaluation is insufficient as a serious evaluation of the proposed techniques. More importantly, the paper does not appear to contribute a significant advance over the extensive literature on fast generalized eigenvalue decompositions in machine learning. No responses were offered to counter such an opinion."
"weakness | rebuttal_process | rating_summary  ==> This paper identifies structure determination of novel natural products as a bottleneck in the drug discovery pipeline. The authors address this problem by using deep siamese neural networks to learn an representation of 2D NMR spectra that facilitates the rapid comparison of spectra for novel compounds with a database containing the spectra of molecules of known structure. <sep> The authors have laboriously collected spectra from the published literature, using a protocol that included manual processing and orientation steps, resulting in a set of 1,385 NMR spectra from 104 families after imposing a minimum requirement of 10 compounds per family. They then implement two baseline topic modeling techniques - PLSI and LDA, and compare the performance of these baselines to the deep siamese CNN that they develop. <sep> Using the random test/val/train split, the deep siamese CNN does show improved performance over the baseline models. However, the interesting and relevant use case is that in which the data is not randomly split, but rather complete families are held out from the training set for use in testing. The authors address this task by building a split in which they hold out four families: the aphanamixoids, teuvissides, tasiamides and macrolactins. <sep> To evaluate performance, they calculate the averaged Tanimoto score for the top five closest compound families of these probe families. Unfortunately, the authors were not able to carry out this calcuation for two of the four probe families, restricting the evaluation to the aphanamixoids and teuvissides (I am confused as to why they couldn't calculate these scores - they appear to be relying on a PubChem server?). They compare these Tanimoto scores to the similarity score of the NMR spectra generated by their NN model. The similarity score either matches (in one case) or differs (in the other case) from the Tanimoto score, and the authors make the argument that this performance is correct. <sep> I am confused that there is no requirement or criteria for the NN to detect the presence of a new compound family, which seems to be what this tasks calls for. This evaluation is rather limited, given that TC scores could not be computed for half of the probe families. There is also no comparison to the baseline models for this task. <sep> Overall I think that this manuscript does a good job of identifying an interesting question, and makes a start at answering the question. To improve the manuscript I would ask that the authors carry out a more comprehensive evaluation of the performance using splits in which whole families are held out from the training set. Different splits might be evaluated, and in addition the model should be required to 'call' when it believes that a new compound family is present. Furthermore, it would be interesting to compare the siamese CNN approach to other network architectures. In addition, the performance of baseline models on the hold-family-out split should be assessed.","The reviewers highlighted that the application in the paper is interesting, but note a lack of new methodology, and also highlight serious flaws in the testing methodology. Specifically, the reviewers are discouraged by the straightforward reuse of Siamese networks without clear modifications. Further, the testing setup might be unfairly easy, since chemical families are represented in both training and test sets, while in true application of the method would be exposed to previously unseen chemical families. <sep> The authors did not participate in the discussion, and address concerns. The reviewer consensus is a rejection."
"rating_summary  ==> Overview: <sep> The authors aim at finding and investigating criteria that allow to determine whether a deep (convolutional) model overfits the training data without using a hold-out data set. <sep> Instead of using a hold-out set they propose to randomly flip the labels of certain amounts of training data and inspect the corresponding 'accuracy vs. randomization' curves. They propose three potential criteria based on the curves for determining when a model overfits and use those to determine the smallest l1-regularization parameter value that does not overfit. <sep> I have several issues with this work. Foremost, the presented criteria are actually not real criteria (expect maybe C1) but rather general guidelines to visually inspect 'accuracy over randomization' curves. The criteria remain very vague and seem be to applicable mainly to the evaluated data set (e.g. what defines a 'steep decrease'?). Because of that, the experimental evaluation remains vague as well, as the criteria are tested on one data set by visual inspection. Additionally, only one type of regularization was assumed, namely l1-regularization, though other types are arguably more common in the deep (convolutional) learning literature. <sep> Overall, I think this paper is not fit for publication, because the contributions of the paper seem very vague and are neither thoroughly defined nor tested. <sep> Detailed remarks: <sep> General: <sep> A proper definition or at least a somewhat better notion of overfitting would have benefitted the paper. In the current version, you seem to define overfitting on-the-fly while defining your criteria. <sep> You mention complexity of data and model several times in the paper but never define what you mean by that. <sep> Detailed: <sep> Page 3, last paragraph: Why did you not use bias terms in your model? <sep> Page 4, Assumption. <sep> - What do you mean by the data being independent? Independent and identically distributed? <sep> - ""As in that case correlation in the data can be destroyed by the introduction of randomness making the data easier to learn."" What do you mean by ""easier to learn""? Better generalization? Better training error? <sep> - I don't understand the assumptions. You state that the regularization parameter should decrease complexity of the model. Is that an assumption? And how do you use that later? <sep> - What does ""similar scale"" mean? <sep> Page 4, Monotony. <sep> - You state two assumptions or claims, 'the accuracy curve is strictly monotonically decreasing for increasing randomness' and 'we also expect that accuracy drops if the regularization of the model is increased', and then state that 'This shows that the accuracy is strictly monotonically decreasing as a function of randomness and regularization.' Although you didn't show anything but only state assumptions or claims (which may be reasonable but are not backed up here). <sep> I actually don't understand the purpose of this paragraph. <sep> - Section 3.3 is confusing to me. What you actually do here is you present 3 different general criteria that could potentially detect overfitting on label-randomized  training sets. But you state it as if those measures are actually correct, which you didn't show yet. <sep> My main concern here, besides the motivations that I did not fully understand (s.b.), is the lack of measurable criteria. While for criterion 1 you define overfitting as 'above the diagonal line' and underfitting as ‚below the line', which is at least measurable depending on sample density of the randomization, such criteria are missing for C2 and C3.       Instead, you present vague of 'sharp drops' and two modes but do not present rigorous definitions. You present a number for C2 in Section 5, but that is only applicable to the present data set (i.e. assuming that training accuracy is 1). <sep> Criterion 2 (b) is not clear. <sep> - I neither understand ""As the accuracy curve is also monotone decreasing with increasing regularization we will also detect the convexity by a steep drop in accuracy as depicted by the marked point in the Figure 1(b)"" <sep> nor do I understand ""accuracy over regularization curve (plotted in log-log space) is constant""? <sep> Does that mean that you assume that whenever the training accuracy drops lower than that of the model without regularization, it starts to underfit? <sep> Due to the lack of numerical measures, the experimental evaluation necessarily remains vague by showing some graphs that show that all criteria are roughly met by regularization parameter \\lambda=0.00011 on the cifar data set.  In my view, this evaluation of the (vague) criteria is not fit for showing their possible merit.",The reviewers reached a consensus that the paper is not fit for publication for the moment because a) the paper lacks thorough experiments and b) the criteria provided by the paper are relatively evague (see more details in reviewer 3's comments.）
"abstract | weakness | decision | suggestion | rebuttal_process  ==> This paper is addressing several research challenges as a method to generate objects with desired functionalities, a method to extract form-to-function mapping, a method to operationally support a functionality arithmetic. The work illustrated in this paper is really interesting and is addressing relevant and open problems in the domain of product design. <sep> Nevertheless the manuscript has a couple of weaknesses, one concerned with the presentation and another related to the design of the study. <sep> The lack of a consistent choice for the lexicon is sometimes misleading. It is not always clear whether the use of different terms is addressing synonyms or to discriminate between two distinct concepts. For example let consider the following pairs: functionality versus affordance, function versus functional, class versus category, feature versus shape. <sep> The study addresses several questions. Not always is clear what is the purpose or better the research questions that are driving the design of the experiments. While in the manuscript the are many repetition of the objectives of the study, less attention is devoted to explain what are the working hypothesis underlying the proposed methods. For example, one of the objective is a method to generate objects with desired functionalities. Only in the final Section there is a brief mention of the dichotomy between meash-based versus voxel-based. As reported in Section 2 there are in literature other works but there is not a claim on what is the specific purpose of the present study. The contrast of voxel versus mesh looks like a motivation but it only a speculation. A similar comment might address the dichotomy deterministic (ontology) versus probabilistic (autoencoder). In this case the experiment design should provide some empirical evidence about this contrast. <sep> A minor comment. Figure 7a is illustrating the functional essence of table. According to the caption Figure 5a is illustrating the same functional essence for the same category/class table. Should the pictures look the same?","The paper presents a novel problem formulation, that of generating 3D object shapes based on their functionality. They use a dataset of 3d shapes annotated with functionalities to learn a voxel generative network that conditions on the desired functionality to generate a voxel occupancy grid. However, the fact that the results are not very convincing -resulting 3D shapes are very coarse- raises questions regarding the usefulness of the proposed problem formulation. <sep> Thus, the problem formulation novelty alone is not enough for acceptance. Combined with a motivating application to demonstrate the usefulness of the problem formulation and results, would make this paper a much stronger submission. Furthermore, the authors have greatly improved the writing of the manuscript during the discussion phase."
"abstract | strength | weakness | rebuttal_process | rating_summary  ==> The paper performs model-based reinforcement learning. It makes two main contributions. First, it divides training into two phases: the unsupervised phase for learning transition dynamics and the second phase for solving a task which comes with a particular reward signal. The scope of the paper is a good fit for *CONF*. <sep> The paper is very incremental: the ideas of using an ensemble of models to quantify uncertainty, to perform unsupervised pre-training and to explore using an intrinsic reward signal have all been known for many years. <sep> The contribution of the paper seems to be the combination of these ideas and the way in which they are applied to RL. I have the following observations / complaints about this. <sep> 1. The paper is very sparse on details. There is no pseudocode for the main algorithm, and the quantity v^i_t (the epistemic variance on page 5) isn't defined anywhere. Without these things, it is difficult for me to say what the proposed algorithm is *exactly*. <sep> 2. Sections 1 and 2 of the paper seem unreasonably bloated, especially given the fact that the space could have been more meaningfully used as per (1). <sep> 3. The experimental section misses any kind of uncertainty estimates. If, as you say, you only had the computational resources for three runs, then you should report the results for all three. You should consider running at least one experiment for longer. This should be possible - a run of 50K steps of HalfCheetah takes about one hour on a modern 10-core PC, so this is something you should be able to do overnight. <sep> 4. The exploration mechanism is a little bit of a  mystery - it isn't concretely defined anywhere except for the fact that it uses intrinsic rewards. Again, please provide pseudocode. <sep> As the paper states now, the lack of details makes it difficult for me to accept. However, I encourage the authors to do the following: <sep> 1. Provide pseudocode for the algorithm. <sep> 2. Provide pseudocode for exploration mechanism (unless subsumed by (1)). <sep> 3. Add uncertainty estimates to evaluation or at least report all runs. <sep> I am willing to re-consider my decision once these things have been done.","Strengths <sep> The paper proposes to include exploration for the PETS (probabilistic ensembles with trajectory sampling) <sep> approach to learning the state transition function. The paper is clearly written. <sep> Weaknesses <sep> All reviewers are in agreement regarding a number of key weaknesses: limited novelty, limited evaluation, <sep> and aspects of the paper are difficult to follow or are sparse on details. <sep> No revisions have been posted. <sep> Summary <sep> All reviewers are in agreement that the paper requires significant work and that it is not ready for *CONF* publication."
"abstract | rating_summary  ==> Summary: <sep> The paper's main contribution is to attribute the bias term seen at the output to each of the input dimension appropriately.  The other claim is that together with gradient information , this could enhance existing explainability methods. <sep> The paper considers DNNs with piecewise linear activation functions. Then the final DNN output is a piece linear function. So for any given point x, the point lies in one of the linear pieces and there fore, can be written as a linear model. The gradient term can be computed using back propagation methods (although back propagating keeping the weights fixed and keeping the input as the variable). However there are no know existing works that attribute the bias of the linear piece at the final layer to the input dimensions. This paper provides a method to do it such that when you add the vector contribution of all the dimension in the input - it results in the bias vector at the output layer. <sep> The basic idea is to distribute dimension wise bias attribution  at layer \\ell to layer \\ell-1 by using N_{\\ell} x N_{\\ell-1} attribution matrix where N_{\\ell} is the dimension of layer \\ell. This is done using two methods - one using exponential weights and the other using some sort of variance measure from a fixed average bias. <sep> The authors then show using examples from STL-10 and Imagenet datasets, how the gradients and biases attributed to the inputs compare for explanation purposes. <sep> Strengths: <sep> The notion of attributing final layer biases to input layer is novel. Its important given that it carries important information regarding the final classification output. <sep> Weaknesses: <sep> a) This paper lacks quite a bit on comparison with existing work. For example, LRP (layer wise relevance propagation) has been referred to by the authors. However, there is no comparison with LRP heat maps. This website - http://www.explain-ai.org/ - documents state of the LRP methods with code, videos, papers (some of which have been cited by the authors). I think the authors could produce heat map produced by LRP for all these different examples in page 8. For instance, pls look at Image A in Fig .2 in https://arxiv.org/pdf/1708.08296.pdf. There is 'cup' and a 'volcano' in the same picture. The paper compares gradient based heatmaps and LRP based ones. LRP based ones are very crisp (for this image of course). LRPcode is readily available from a well maintained project page - http://www.explain-ai.org/. Will the heat maps of grad/bias look crisper than LRP ? Other papers and more examples of LRP can be found at http://www.explain-ai.org/ <sep> LRP takes the final probability weight at the output layer and assigns recursively to other neurons in the penultimate very similar to what the current paper does for bias. However, the attribution mechanism (the weights) are different. A couple of variations are explored in this survey (https://arxiv.org/pdf/1708.08296.pdf). <sep> b)  This point is related to the first- The authors say ""Therefore, the interpretation of the DNN's behavior on the input data should be exclusive to the information embedded in the linear model."" - I disagree a little bit here. It is true that behavior of the DNN on the input is exclusive to the linear piece. However, interpretation of the behavior/ explanation of it is another matter. For example, I quote two methods in the literature that have been used for explanation of an input sample - but does not use the linear piece or the gradient information. <sep> 1)  Pls look at - https://arxiv.org/pdf/1703.02647.pdf - (Figure 3 + Section A.8 in the appendix). The paper used streaming submodularity algorithms to actually assemble a part of the image with everything else ""blacked out"" to determine which sparse parts of the image are responsible for the final output. They have exhaustive comparisons with LIME too. These methods actually rely on behavior of DNN far away from the actual x to explain the behavior at x. ""Zeroing out irrelevant"" parts is one of the ways of explaining adopted by LIME and these approaches. Ideally the authors should compare with these too. <sep> 2) In this paper - https://arxiv.org/abs/1802.07623 - authors provide pertinent negatives - what should ""not be"" there in the image so that the label does not change - In fact for MNIST data, this produces very interesting additional explanations that is not produced by methods (including LRP and LIME) that rely on things in the image. Again the explanation is not at all related to the linear piece. <sep> In general, linear pieces are so close by , the nearby movement and possibly change of gradients can also provide useful explanation of the behavior. <sep> Overall - at least the authors must discuss the above references + also survey works that highlights ""relevant parts of the image"" like LIME and streaming sub modularity etc. Further an actual comparison to LRP (code is easily available) is crucial to evaluate the efficacy of the proposed methods given that the authors of LRP have compared with gradient based methods. Comparison with LIME would also be interesting and desirable.","The work presents a method to back propagate and visualize bias distribution in network as a form of explainability of network decisions. Reviewers unanimous reject, no rebuttal from authors."
"abstract | weakness | rebuttal_process | suggestion  ==> The authors applied the external memory module proposed by Graves et al. (2016) to the image segmentation task. SHAMANN is an extension to allow memory sharing between directions. <sep> Authors claimed that one of the contributions is a reformulation of the semantic segmentation problem as a sequence learning task. <sep> There are many previous works done in this direction, <sep> - ""Multi-Dimensional Recurrent Neural Networks"", 2007 <sep> - ""Scene Labeling with LSTM Recurrent Neural Networks"", 2015 <sep> - ""ReSeg: A Recurrent Neural Network-Based Model for Semantic Segmentation"", 2016 <sep> - ""Robust, Simple Page Segmentation Using Hybrid Convolutional MDLSTM Networks"", 2017 <sep> and many more. <sep> Authors should compare with those LSTM-based image segmentation approaches as well. <sep> Their second contribution is a network with a shared external memory module between directions. However, the experiments are not enough to show the benefits of it. See the details below. <sep> Handling long-range dependencies: <sep> - In Section 3.3.2, authors mentioned that ""One limitation of Bi-LSTM is that the number of network parameters grows proportionally to the memorization capacity, making it unsuitable for sequences with long-range dependencies."". <sep> However, the experiments are not with long range sequences: 169 sequence length for X-ray dataset and 49 length for MNIST. A classic LSTM (not bi-directional) is known to handle up to 200 timesteps. Some comparison/analysis of handling long-range dependencies of Bi-LSTM, Bi-MANN, and SHAMANN are needed (ideally on high-resolution real images). <sep> Dataset: <sep> -  Authors compared 3 models only on MNIST. The structure on MNIST is simple, and the resolution of images is small to show the benefit of using (shared) external memory module instead of individual memory cells. It is not surprising that the reported performance difference is small. Authors could have reported such a comparison on X-ray dataset too but they did not. I would recommend authors pick another high-resolution real-image dataset and compare the performance of these 3 models. <sep> Additional comparisons: <sep> - Various patch size <sep> - Longer sequence length <sep> - Especially a trade-off between the patch size and the sequence length on the high resolution images (larger patch size with a shorter sequence length or shorter patch size with a longer sequence length) <sep> - A comparison of Bi-LSTM with sharing weights will also be a good baseline.",The paper addresses the problem semantic segmentation using a sequential patch-based model. I agree with the reviewers that the contributions of the paper are not enough for a machine learning venue: (1) there has been prior work on using sequence models for segmentation and (2) the complexity of the proposed approach is not fully justified. The authors did not submit a rebuttal. I encourage the authors to take the feedback into account and improve the paper.
"abstract | rating_summary | weakness  ==>  ==> This paper introduces a method to create a 3D scene model given a 2D image and a camera pose. The method is: (1) an ""encoder"" network maps the image to some latent code vector, (2) a ""decoder"" network uses the code and the camera pose to create a depthmap, (3) surface normals are computed from the depthmap, and (4) these outputs are fed to a differentiable renderer which reconstructs the input image. At training time, a discriminator provides feedback to (and simultaneously trains on) the latent code and the reconstructions. The model is self-supervised by the reconstruction error and the GAN setup. Experiments show compelling results in 3D scene generation for simple monochromatic synthetic scenes composed of an empty room corner and floating ShapeNet shapes. <sep> This is a nice problem, and if the approach ever works in the real world, it will be useful. On synthetic environments, the results are impressive. <sep> The paper seems to claim more ground than it actually covers. The abstract says ""Our method learns the depth and orientation of scene points visible in images"", but really only the depth is learned, and the ""orientation"" is an automatically-computed surface normal, which is a free byproduct of any depth estimate. The ""surfel"" description includes a reflectance vector, but this is never estimated or further described in the paper, so my guess is that it is simply treated as a scalar (which equals 1). Taking this reflectance issue together with the orientation issue, the model is not really estimating surfels at all, but rather just a depthmap, which makes the method seem considerably less novel. Furthermore, the differentiable rendering (eq. 1) appears to assume that all light sources are known exactly -- this is not a trivial assumption, and yet it is never mentioned in the paper. The text suggests that only an image is required to run the model, but Figure 3 shows that the networks are conditioned on the camera pose -- exact knowledge of the camera pose is difficult to obtain precisely in real settings, so this again is not an assumption to ignore. <sep> To rewrite the paper more plainly, one might say that it receives a monochrome image as input, estimates a depthmap, and then shades this depthmap using perfect knowledge of lighting and camera pose, which reconstructs the input. This may sound less appealing, but it also seems more accurate. <sep> The paper is also missing some details of the method and evaluation, which I hope can be cleared up easily. <sep> - What is happening with the light source? This is critical in the shading equation (eq. 1), and yet no information is given on it -- we need the color and the position of every light in the scene. <sep> - How is the camera pose represented? Section 3.3.3 says conditional normalization is used, but what exactly is fed to the network that estimates these conditional normalization parameters? <sep> - What is the exact form of the reconstruction error? An equation would be great. <sep> - How is the class-conditioning done in 4.2? <sep> - In Eq. 4, the first usage of D_\\theta should use only the object part of the vectors, and the second usage should use only the geometric part, right? Maybe this can be cleared up with a second D_subscript. <sep> - I do not understand the ""interleaved"" training setup in 4.4.1. Please explain that more. <sep> - It is not clear to me why the task in 4.4.2 needs any supervised training at all, if the classification is just done by computing L2 distances in the latent space. What happens with ""0 sampled labels""? <sep> Overall, I like the paper, and I can imagine others in my group liking it. I hope it gets in, assuming the technical details get cleaned up and the language gets softer.","This paper proposes an approach for learning to generate 3D views, using a surfel-based representation, trained entirely from 2D images. After the discussion phase, reviewers rate the paper close to the acceptance threshold. <sep> AnonReviewer3, who initially stated ""My second concern is the results are all on synthetic data, and most shapes are very simple"", remains concerned after the rebuttal, stating ""all results are on synthetic, simple scenes. In particular, these synthetic scenes don't have lighting, material, and texture variations, making them considerably easier than any types of real images."" <sep> The AC agrees with the concerns raised by AnonReviewer3, and believes that more extensive experimentation, either on more complex synthetic scenes or on real images, is needed to back the claims of the paper. Particularly relevant is the criticism that ""While the paper is called 'pix2scene', it's really about 'pix2object' or 'pix2shape'."""
"abstract | weakness | rating_summary | misc  ==>  ==> Summary: <sep> ========= <sep> The paper uses a proof-of-concept Bayesian parameter search-based simulation in virtual environment to probe biases of an already trained model towards specific categories that may have been sparsely represented in the training set. Understanding bias in trained models, especially in models involving end-to-end deep neural networks learners, is of high importance in machine learning. More specifically, probing the source of unintentional bias introduced as a result of skewed distribution in the training set and dissecting the biased performance is important for many applications such as surveillance, criminal profiling, medical diagnosis and predicting creditworthiness of a person. <sep> The authors used four commercial face recognition APIs (by Microsoft, Google, IBM, and Face++) as test bed for this investigation. <sep> Strength: <sep> ======== <sep> - The paper reads well and is easy to follow. <sep> - The application of face detection and recognition is a good choice as it is precursor to detailed analysis in surveillance and criminal profiling. <sep> - The choice of the controlled Bayesian parameter search enables one to control the amount of variation with respect to the expected uncertainty in performance of the classifier on the generated input from the simulator. <sep> - The use of standardized measures such as Fitzpatrick's skin tone and FACS intensity help in replication and consistency in the evaluation. <sep> Weakness: <sep> ========= <sep> - Although evaluating commercial APIs is good enough in demonstrating the existence of the bias, access to the trained model with possibility to retrain the model in such a way to mitigate a bias in particular parameter could have helped to further tie the drop in performance to the parameter variation. <sep> - This work is preliminary and only involves a single person face manipulated in the parameter space. It lacks diversity of in samples and as such limits the analysis to draw strong conclusions. As such a generative network (such as a GAN trained to generate diverse samples while controlling for the parameters under investigation could have helped to draw more generalized conclusion. <sep> - The age parameter variation is not convincingly different across the values considered. It would have been interesting to use models trained for age progression such as [1] for a more diverse variation in the age parameter space. <sep> Minor comments: <sep> =============== <sep> - Figures 4 and 5 could have used better captions describing the ranges for instance for age 1 is older and for skin tone 1 is darker (although indicated in Fig. 3). Captions should be self contained. Fig. 5 caption should describe the chance performance in each case. <sep> - The manuscript should be revised to make in text citation formats consistent (some places it uses authors (year) and other places it uses (authors, year)). Also, minor punctuation and syntactic errors should be fixed. <sep> [1] Yang, H., Huang, D., Wang, Y. and Jain, A.K., 2017. Learning face age progression: A pyramid architecture of gans. arXiv preprint arXiv:1711.10352.","The paper addresses an important problem of detecting biases in classifiers (e.g. in face detection), using simulation tools with Bayesian parameter search. While the direction of research and the presented approach seem to be practically useful, there were several concerns raised by the reviewers regarding strengthening the results (e.g., beyond single avatar, etc), and suggestions on possibly a more applied conference as a better venue. While thourough rebuttals by the authors addressed some of these concerns, which increased some ratings, overall, the paper was still in the borderline range. We hope the suggestions and comments of the reviewers can help to improve the paper."
"abstract | weakness  ==> This paper combines a number of ideas to train generative models with (deep) structured constraints. The general idea is similar to Flow-GAN, which learns a normalizing flow-based generator by optimizing the negative loglikelihood with an augmented GAN loss. However, It's difficult to impose prior structure information in the GAN framework. To address this problem, the authors proposed to minimize a so-called Gibbs-regularized variational bound of Jeffery divergence, which is the summation of KL and reverse KL divergence. The authors provide some justification that the Jeffery divergence works by yielding good mass-covering and mode-seeing properties. <sep> It appears that the parameterization and adaptation of v throughout optimization is the key contribution of this work --- the technical details are not clear from the paper. <sep> 1.    Typo in the training objective (Eq .1):  the second (or the first) ""sup"" should be removed? <sep> 2.    Section 2.3 is very confusing. Particularly, how is the parameter \\phi introduced? What's the detailed update of \\phi? <sep> - ""We now observe that our methods can also be interpreted as a way of learning v as a Gibbs distribution approximating p."" If v_\\phi(x) is a distribution, what's the parametric form of v? <sep> - ""Generally, this is achieved by structuring the energy function V_\\phi:=\\log v_\\phi."" It seems that V_\\phi(x) is a scalar-valued function that represents the negative energy of the distribution v_\\phi(x), however, why the distribution is self-normalized? Specifically, why \\int \\exp(V_\\phi) dx = 1? Otherwise, how the authors deal with the partition function \\int \\exp(V_\\phi(x)). <sep> - It is unclear to me why the inner loop optimization is connected with Itakura-Saito divergence minimization? The authors may consider including the detailed proofs? <sep> 3.    With the given description, the proposed algorithm is not easy to follow and implement by the reader. The paper would benefit from an Algorithm box with pseudocode. <sep> If the authors can fully address the concerns above, I will consider changing the scores. <sep> Other comments: <sep> 1. The empirical results are fairly weak. Similar datasets are used, the authors may consider evaluating their approach on various different tasks. <sep> 2. Duplicate citations – R2P2 [35] [36] <sep> 3. Other related papers: <sep> - Belanger et al., End-to-End Learning for Structured Prediction Energy Networks, ICML 17 <sep> - Tu et al., Learning Approximate Inference Networks for Structured Prediction, *CONF* 18","The paper proposes to define the GAN discriminator as an explicit function of a invertible generator density and a structured Gibbs distribution to tackle the problems of spurious modes and mode collapse. The resulting model is similar to R2P2, i.e. it can be seen as adding an adversarial component to R2P2, and shows competitive (but no better) performance. Reviewers agree, that these limits the novelty of the contribution, and that the paper would be improved by a more extensive empirical evaluation."
"abstract | weakness | rebuttal_process | decision  ==>  ==> This paper presents a novel concept of supervised learning, advocacy learning. In this framework, supervised learning procedure is given by two subnetworks, advocates and judge. Advocates generate evidence in the form of attention for individual classes and judge decide the final class labels. <sep> The main idea looks interesting, and the paper is clear enough to deliver the idea. However, this paper has the following major issues. <sep> 1. There is no formal justification of the idea. Although the idea looks interesting, there is no theoretical background and no clear intuition. <sep> 2. Experiment is weak and even inconsistent. Evaluation is performed on very small datasets only, where all baseline methods already show very high accuracy and accuracy gain given by the proposed method is very marginal. In particular, Table 1 and 2  have inconsistent results; advocacy network is better in Table 1 while worse in Table 2 compared to honest advocacy network. To make the idea more convincing, it is required to test it on much larger datasets, at least ImageNet scale, and more desirable to show results in other tasks such as object detection and image segmentation. <sep> 3. I am not sure if advocacy network has any separate supervision to enforce it to be learned in a class-conditional manner. Also, in honest advocacy network, each subnetwork can look at only a part of dataset (data corresponding to the class), and I wonder if there is any problem given by data deficiency issue. <sep> Overall, the paper does not look ready for publication because the idea is clearly justified neither theoretically nor empirically.","The paper presents a novel architecture, reminescent of mixtures-of-experts, <sep> composed of a set of advocates networks providing an attention map to a <sep> separate ""judge"" network. Reviewers have several concerns, including lack <sep> of theoretical justification, potential scaling limitations, and weak <sep> experimental results. Authors answered to several of the concerns, which did <sep> not convinced reviewers. The reviewer with the highest score was also the least <sep> confident, so overall I will recommend to reject the paper."
"abstract | strength | weakness | decision | suggestion  ==>  ==> This paper proposes a metric to measure the ""nonlinearity"" of neural network, and presents evidence that the value of this metric at initialization time is predictive of generalization performance. <sep> Apart from a few problems I think this paper is well written and thorough. The contribution is solid, although not earth shattering given previous work on such metrics.  There seems to be a basic error in some of the early math, although I don't think this will qualitatively affect the results in any significant way. <sep> ----------------- <sep> Detailed comments by section: <sep> ------------------ <sep> Section 3: <sep> It seems like a 1/sqrt(d) factor is missing from these Q_i(S_x x(i)) and Q_j(S_x f(x,j)) formulas.  As far as I can tell this doesn't affect Def 1 because you seemed to use the correct formula there. <sep> However, the rewritten version with the traces doesn't seem to be correct. There should be a d_in factor in the denominator (inside the square root). This error seems unrelated to the other one.  Assuming I'm correct and that this is an error, does this affect your results in the various figures?  And what is the actual final definition of NLC that you used? <sep> In general, it's annoying for the reader to verify that all of these forms are equivalent.  And it's fiddly enough with the sqrt(d) terms constantly disappearing and reappearing in the numerator and denominators that even you made multiple errors (as far as I can tell).  I would suggest making this section more rigorous and writing out everything carefully. And you probably don't need to rewrite it in so many equivalent forms with different notation unless they are useful somehow. <sep> The use of the Q and S symbols feels superfluous and counterproductive. Standard notation with expectations and squares wouldn't take much more space and would be a lot clearer. <sep> Section 4: <sep> ""we plot the relative diameter of the linearly approximable regions of the network as defined in section 3"": but you don't seem to define ""relative diameter"" there. As far as I can tell it's only defined in Appendix E, and this is only mentioned in the caption of figure 1.  It's impossible to interpret this result without knowing precisely what ""relative diameter"" is.  If you can't afford to describe this in the main paper you should at least mention that it's a different (more expensive) way of estimating the same thing that the NLC estimates. <sep> In Figure 2, are the higher test errors due to the optimizer failing to lower the training error, or due to a greater generalization gap?  I guess the Figure 3 results suggest the latter possibility, which is surprising to me. <sep> What does it mean to have a ""very biased output"".  What does that inequality mean intuitively?  Should there be an absolute value on the RHS?  It would be much easier to parse it if it were written in plain notation without these S and Q symbols. <sep> Section 6: <sep> ""metric also an"" -> ""metric also has an"" <sep> Can you generate a failure case for ""correlation information"" that doesn't involve Batch Norm layers?  I don't think the authors of those works meant for their results to deal with that. <sep> Note that there are actually a lot of papers going back to the 90s that discussed and proved representational benefits of depth in neural networks.","This paper proposes the NonLinearity Coefficient (NLC), a metric which aims to predicts test-time performance of neural networks at initialization. The idea is interesting and novel, and has clear practical implications. Reviewers unanimously agreed that the direction is a worthwhile one to pursue. However, several reviewers also raised concerns about how well-justified the method is: in particular, Reviewer 3 believes that a quantitative comparison to the related work is necessary, and takes issue with the motivation for being ad-hoc. Reviewer 2 also is concerned about the soundness of the coefficient in truly measuring nonlinearity. <sep> These concerns make it clear that the paper needs more work before it can be published. And, in particular, addressing the reviewers' concerns and providing proper comparison to related works will go a long way in that direction."
"abstract | rating_summary  ==>  ==> The paper proposes a multi-objective search algorithm that designs resource-efficient convolutional architectures. The key idea is to maintain a population of networks and to iteratively approach the Pareto front through evolution. The normal & reduction cells are searched on CIFAR-10 and then transferred to ImageNet. The resulting architectures empirically lead to better trade-offs than other baselines. <sep> Pros: <sep> The paper is well-written and easy to comprehend. <sep> Results are competitive against strong baselines such as NASNet. <sep> Resource budgets are handled in a principled manner with multi-objective optimization. <sep> Cons: <sep> My main concerns are on the technical novelty and experimental comparison. <sep> Technical novelty: <sep> The proposed algorithm seems highly similar to the existing multi-objective NAS algorithms, especially the ones based on Pareto optimality [1,2,3]. In Sect 2, the authors state that the main difference from prior works such as [2] and [3] is the usage of a different and larger search space and large-scale experiments. However, both aspects are of limited technical novelty. <sep> Experimental comparison: <sep> In sect 3.3, the authors say ""we noticed that the original NASNet search space can greatly benefit from extra connections from any given block"". If the proposed algorithm was investigated in an enhanced version of the NASNet space, it would be unclear whether we should attribute the reported performance to the proposed multi-objective evolution or this additional search space engineering. It would be better to report the results using the original space as well for fair comparison. <sep> The main claimed contribution is a multi-objective evolutionary algorithm. To demonstrate its effectiveness, it would be necessary to compare against existent multi-objective NAS strategies in the literature. Most of those strategies (e.g., scalarization, weighted product method) should be straightforward to implement on top of the current search space. The current results are less convincing since the authors only compared their method against single-objective baselines (e.g. NASNet, PNAS, AmoebaNet) which are completely unaware of additional dimensions of the desired objectives. <sep> The networks are searched on CIFAR-10 and then transferred to ImageNet. Unlike most prior works (including the ones focusing on resource-constrained NAS), the authors did not the final performance of their architecture on CIFAR-10. It would be informative to report the CIFAR-10 results as well. <sep> Other suggestions & questions: <sep> The authors did not report their training setup for ImageNet. It would be good to include those details to ensure the readers are informed should there are any additional augmentations. <sep> ""uniform mutation and a crossover probability of 0.1"" (sect 4.1) <sep> It would be better to included more details on these evolution forces for reproducibility. These are also important component of the proposed algorithm. <sep> ""We manually select 3 architectures that we will be fully train on ImageNet in Section 4.2"" (sect 4.1) <sep> I believe this part needs more clarifications since there can be a large number of architectures on the Pareto front. What's the criteria for manual selection? <sep> [1] Elsken, Thomas, Jan Hendrik Metzen, and Frank Hutter. ""Multi-objective architecture search for cnns."" arXiv preprint arXiv:1804.09081 (2018). <sep> [2] Kim Ye-Hoon, Reddy Bhargava, Yun Sojung, and Seo Chanwon. NEMO: Neuro-Evolution with Multiobjective Optimization of Deep Neural Network for Speed and Accuracy. ICML'17 AutoML Workshop, 2017. <sep> [3] Dong, Jin-Dong, et al. ""DPP-Net: Device-aware Progressive Search for Pareto-optimal Neural Architectures."" arXiv preprint arXiv:1806.08198 (2018).","The paper describes an architecture search method which optimises multiple objectives using a genetic algorithm. All reviewers agree on rejection due to limited novelty compared to the prior art; while the results are solid, they are not ground-breaking to justify acceptance based on results alone."
"abstract | strength | weakness | decision  ==> Summary: This paper proposes three new techniques for improving Atari performance over APE (Horgan 2018).  Two of them are closely linked in that they deal with improving stability.  Another involves integrating the use of expert trajectories from DQfD. <sep> I will summarize each: <sep> Transformed Bellman: This applies a rescaling function (it's basically a monotonically increasing version of the sqrt(x) function) to the Q-function and applies the inverse of the function to the max Q-value of the next state (such that the contracting effect h-function is not ""applied"" multiple times when doing the TD backup). <sep> Temporal Consistency: This encourages the ""next state"" after where the TD-update is applied to not change too much.  This addresses a problem discussed in (Durugkar 2018).  I think the intuition here is that the state which follows the state with the TD update may be visually similar, but it does not impact the value in the past states, so its value function should not have a highly correlated change with the previous state's change in value function. <sep> DQfD: Storing an expert replay buffer and an actor replay buffer.  The expert replay buffer is fixed and the actor replay buffer stores the most recent ""actor processes"".  Train with both a supervised imitation loss (only for the highest return episode) and the original TD loss.  Additionally, the pre-training phase is removed and the ratio of expert-learned trajectories is fixed (both seem like steps in the right direction). <sep> Review: This paper proposes a few changes to DQN training, two of which are aimed at reducing instability, and one is aimed at improving exploration (expert trajectories).  Because all of these changes are well justified and the experiments are fairly thorough, I recommend acceptance.  My main reservation is that the ideas presented are not very strongly thematically linked.  The presence of ablation studies compensates for this to some extent. <sep> Strengths: <sep> -The discussion of related work and comparison to baselines is pretty extensive.  For example I appreciated the ablation study removing ""transformed Q-learning"" and comparison to the pop-art method. <sep> -The results, at least for Ape-X DQfD seem impressive to me in that the method works without reward clipping and with a much higher discount factor.  Additionally the results generally outperform DQfD (uses expert trajectories) and Rainbow (no human trajectories).  Additionally evidence was presented that the learned policies often exceed the performance of the human demonstrations (for example in time to achieve rewards). <sep> Weaknesses: <sep> -Two of the techniques: ""transformed bellman"" and ""temporal consistency"" seem well-linked thematically, but the expert demonstration idea seems orthogonal.  I would have preferred splitting that idea out into a separate paper, given that the paper is already 20 pages. <sep> -The motivation for temporal consistency just references (Durugkar 2018).  The readability of this paper would be improved if it were discussed more here as well.  I also feel like the analysis could be more thorough here, for example a result using the temporal consistency loss on Baird's counter example really should be shown (like figure 2 in Durugkar's paper). <sep> -It would be nice to see a visualization or a toy problem with the ""transformed bellman"". <sep> Questions: <sep> -Is the ""highest return episode"" idea (3.4) general or is it exploiting the fact that Atari is deterministic?  It seems like in general we'd want to use many high reward episodes, or the highest reward episodes that go into different parts of state space.  It seems like it could be a very bad idea on certain settings (for example if the reward has a lot of randomness). <sep> -""Proposition 3.1 shows that in the basic cases when either h is linear or the MDP is deterministic, Th has the unique fixed point h ◦ Q∗"".  From 3.1, it looks if h is linear, then it distributes over r(x,a) + maxh^{-1}(Q) and then it also won't effect which is the max, so it would reduce to h*r(x,a) + max(Q) - which means it's just rescaling the original reward.  So then this result is trivial?  Please correct me if I misunderstood something here. <sep> -Could an MDP be constructed which causes the transformed bellman operator to perform badly?  I am imagining something where the MDP is just a single step, and there is a stochastic action which behaves like a lottery.  So perhaps there is a 1-in-1-million chance to win 1-billion dollars by taking an action.  If I understand correctly the transformed bellman operator will destroy the large reward here (because in a single step, there is just r(x,a) which h is applied to).  Which would make the action seem bad even though it's actually appealing. <sep> Notes: <sep> -I did not read the proofs in the appendix.","This paper proposes a combination of three techniques to improve the learning performance of Atari games. Good performance was shown in the paper with all three techniques together applied to DQN. However, it is hard to justify the integration of these techniques. It is also not clear why the specific decisions were made when combining them. More comprehensive experiments, such as a more systematic ablation study, are required to convince the benefits of individual components. Furthermore, it seems very hard to tell whether the improvement of existing approaches, such as Ape-X DQN, was from using the proposed techniques or a deeper architecture (Tables 1&2&4&5). Overall, this paper is not ready for publication."
"abstract | rating_summary  ==>  ==> The paper proposes a detailed empirical evaluation of the trade-offs achieved by various convolutional neural networks on the super resolution problem. The paper provides an extensive evaluation of different architectural changes and the trade-off between savings in terms of memory and computational cost and performance, measured in terms of PSNR and SSIM. <sep> This is an empirical paper, thus it does not provide technical contributions. I do think that the insights obtained from such an empirical evaluation could be of interest for practitioners and researchers working on the problem. My main concern is the method only evaluates the trade-offs between model efficiency (in terms of memory and/or computation) and performance measured using metrics that are known to not be well correlated with perceptual quality. Thus it is not obvious to me that the insights obtained in this work would translate to the other case. <sep> It is well known that PSNR favors blurry solutions over perceptually more appealing solutions. This comes from the fact that there is no information in the low resolution image to produce the missing high resolution details. Filling up plausible details in a way that is different from the original image would lead to high PSNR. Models that treat the super resolution problem as a regression task using similarity in pixel space, tend to produce blurry solutions and require very large models to improve the score. <sep> In recent years, many works have been studying the use of perceptual losses to mitigate this issue or simply treating the super resolution problem as conditional generative modeling.  For instance, models using L2 losses in a perceptually more relevant (or learned) feature spaces [A, B], or including GAN losses [C, D] (to list a few). To my knowledge, these models are the current state of the art in terms of perceptual quality. This has been evaluated empirically via perceptual tests [D]. <sep> This line of work needs to be cited. In my view, the paper needs to provide a detailed justification on why models using these losses are not considered. Would the conclusions drawn on this work transfer to that setting? Furthermore, it would be good to perform perceptual tests to perform this evaluation. It would be good to provide some canonical examples in the appendix. <sep> The overall writing of the paper could be improved. Several sentences are difficult to read, due to typos or the construction of the sentences. The paper evaluates many architectural modifications proposed by other works. It would be good to add an appendix with a small description of what these are. This would make the paper self-contained an easier to read (I had too look up a few of them). <sep> The authors mentioned that they first train models for scaling factor of x2 and then use them for training settings higher magnification. How is this exactly done? Please provide details. <sep> I am curious of weather using some for of distillation techniques would be useful here. <sep> Did you try scaling factors larger than x4? Scaling factors of x2 does not seem very relevant, as simpler methods can achieve already quite competitive results (such as simple interpolation methods) <sep> The authors seem to be citing Zhang et al (2018) as a reference to attention mechanisms. To my knowledge the paper that proposed these mechanisms is [E]. <sep> The citation style is not used properly throughout the manuscript. As an example: <sep> ""… proposed in StrassenNets Tschannen et al (2017)."" Should be ""… proposed in StrassenNets (Tschannen et al, 2017)."" Or ""… proposed in StrassenNets proposed by Tschannen et al (2017)."" <sep> [A] Johnson, J. et al. ""Perceptual losses for real-time style transfer and super-resolution."" ECCV, 2016. <sep> [B] Bruna, J. et al ""Super-resolution with deep convolutional sufficient statistics."" *CONF* 2016. <sep> [C] Ledig, C. et al. ""Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network."" CVPR. Vol. 2. No. 3. 2017. <sep> [D] Sønderby, C. K., et al. ""Amortised map inference for image super-resolution."" arXiv preprint arXiv:1610.04490(2016). <sep> [E] Bahdanau, D. et al ""Neural machine translation by jointly learning to align and translate."" arXiv (2014).","This paper targets improving the computation efficiency of super resolution task. Reviewers have a consensus that this paper lacks technical contribution, therefore not recommend acceptance."
"abstract | weakness | rebuttal_process | weakness  ==> Overall Score: 7/10. <sep> Confidence Score: 7/10. <sep> Detailed Comments: This paper introduces various Deep Recurrent Gaussian Process (DRGP) models based on the Sparse Spectrum Gaussian Process (SSGP) models and the Variational Sparse Spectrum Gaussian Process (VSSGP) models. This is a good paper and proposed models are very sound so I recommend for acceptance although as main weakness I can say that is very technical so it can be difficult to follow. Adding more intuitive ideas, motivation and maybe a figure for each step would be a solution. Apart from that it is a really good paper, congratulations. <sep> Related to: RNN models and Sparse Nystrom approximation. <sep> Strengths: Models are very sound, solutions are solid, the proposed methodology is correct and the empirical results and experiments are valid and properly done. <sep> Weaknesses: It is too difficult to follow and it is written in an extreme technical way. More intuitions and a proper motivation both in the abstract and introduction may be put in order to make the paper easier to read and, hence, more used by researchers and data scientists. <sep> Does this submission add value to the *CONF* community? : Yes it does, the experiments show the efficiency of the proposed methods in some scenarios and are valid methodologies. <sep> Quality: <sep> Is this submission technically sound?: Yes it is. <sep> Are claims well supported by theoretical analysis or experimental results?: Experimental results prove empirically the methods and appendixes show the analysis performed in a clear and elegant way. <sep> Is this a complete piece of work or work in progress?: Complete piece of work. <sep> Are the authors careful and honest about evaluating both the strengths and weaknesses of their work?: Yes, and I would enfatize that I have liked that some experiments are won by other methods such as GP-LSTM, they are very honest. <sep> Clarity: <sep> Is the submission clearly written?: Yes, but it is difficult for newcomers due to the reasons that I have stated before. <sep> Is it well organized?: Yes it is. <sep> Does it adequately inform the reader?: Yes it is. <sep> Originality: <sep> Are the tasks or methods new?: Yes, they are sound. <sep> Is the work a novel combination of well-known techniques?: Yes it is. <sep> Is it clear how this work differs from previous contributions?: Yes. <sep> Is related work adequately cited?: Yes, being a strength of the paper. <sep> Significance: <sep> Are the results important?: I would argue that they are and are a clear alternative to consider in order to solve these problems. <sep> Are others likely to use the ideas or build on them?: If the paper is written in a more friendly way, yes. <sep> Does the submission address a difficult task in a better way than previous work?: Yes I think. <sep> Does it advance the state of the art in a demonstrable way?: Yes, empirically. <sep> Arguments for acceptance: Models are very sound, solutions are solid, the proposed methodology is correct and the empirical results and experiments are valid and properly done <sep> Arguments against acceptance: Clarity of the paper. <sep> Minor issues and typos: <sep> -> (V)SS not defined before being used. <sep> -> Abstract should be rewritten adding a motivation and focusing more on the problems being solved and less in the details of the solutions. <sep> -> Recurrent indexes that go backwards (i) of Eq. 1. should be explained why are going backwards before being used like that. Newcomers may be confused. <sep> -> Section 2 writing style lacks a bit of cohesion, relating the paragraphs may be a solution. <sep> -> Q is not defined in section 3.1 paragraph 1. <sep> -> A valid covariance function must produce a PSD matrix, put that in section 3.1. <sep> -> I do not see how U marginalizes in Eq. 7, kind of confused about that, I think that it should be p(y|X,U). <sep> -> Section 3.4 statistics should be explained. <sep> Reading thread and authors response rebuttal decision: <sep> ================================================= <sep> I consider that the authors have perfomed a good rebuttal and reading the other messages and the authors response I also consider that my issue with clarity is solved. Hence, I upgrade my score to 7 and recommend the paper for publication.","This paper is concerned with combining past approximation methods to obtain a variant of Deep Recurrent GPs. While this variant is new, 2/3 reviewers make very overlapping points about this extension being obtained from a straightforward combination of previous ideas. Furthermore, R3 is not convinced that the approach is well motivated, beyond ""filling the gap"" in the literature. <sep> All reviewers also pointed out that the paper is very hard to read. The authors have improved the manuscript during the rebuttal, but the AC believes that the paper is still written in an unnecessarily complicated way. <sep> Overall the AC believes that this paper needs some more work, specifically in (a) improving its presentation (b) providing more technical insights about the methods (as suggested by R2 and R3), which could be a means of boosting the novelty."
"weakness | suggestion  ==> Summary. The authors propose a novel adversarial training method, e2SAD, that relies on a two-step process for generating sets of two training adversarial samples for each clean training sample. The first step is a classical FGSM that yields the first adversarial sample. The second adversarial sample is calculated with a FGSM that is based on the cross-entropy between the probabilities generated by the first adversarial sample and the probabilities generated by the second adversarial sample. The method is computationally efficient (two forward/backward passes per clean sample) w.r.t. powerful iterative attacks such as IFGSM or PGD requiring 40+ steps and the authors claim it gives comparable results to adversarial training with multi-step attacks methods in white and black-box settings. <sep> Clarity. Part 1 and 2 of the paper are well written and summarize the existing attacks/defense mechanisms, their pros and cons as well as the contributions clearly. The next sections could be made shorter (see comments below) to match *CONF*'s recommended soft limit of 8 pages instead of the 10 pages hard limit. This would also help the reader grasp the key ideas faster and have a standard formatting (no negative spaces for instance). <sep> Novelty. The idea of simulating the effect of iterative attacks using two distinct steps is novel and appealing to me. The first step increases the loss while the second step shifts the probability distributions apart. <sep> Pros and cons. <sep> (+) The paper is clear and easy to follow, although a bit long. <sep> (+) The idea is interesting and clearly motivated in terms of computational efficiency and in terms of desired properties (Figure 2 illustrates this point well). <sep> (-) Only one aspect of the idea is exploited in the article. It would be interesting to compare this method as an attacker (both in terms of performance and in terms of generated samples, see comment below). Powerful adversarial training should indeed rely on powerful generated adversarial samples. <sep> (-) The results seem somewhat mitigated in terms of significance and conclusions drawn by the authors. Also, the experimental setup is quite light, notably the used CNN architectures are quite small and other datasets could have been used (also linked to the significance of the results). <sep> Comments. <sep> - Shorter paper. Here are suggested modifications for the paper that could help strengthen the impact of your paper. Section 3.1 could be almost entirely discarded as it brings no new ideas w.r.t sections 1 and 2. Figure 1 summarizes the method well, thus the description in Section 3.2 could be made shorter, especially when displaying Equation (8) right after Figure 1. This would then help reduce the size of Sections 3.2.1 and 3.2.2 (because Equation (8) and Figure 1 would prevent you from repeating claims made earlier in the paper). Algorithm 1 is straightforward and could be placed in Appendix. Conclusion and Result sections could be shortened a little as well (not as much as Section 3 though). <sep> - Significance of the results. The significance of some results is unclear to me. Could the authors provide the standard deviation over 3 or 5 runs? For example, in rows 1, 3, 4, 5, 6 of Table 2, it is not clear it e2SAD performs better than FGSM adversarial training, thus raising the question of the necessity of Step 2 of the attack (which is the core contribution of the paper). <sep> - Experimental setup. The last two rows of Table 1 are encouraging for e2SAD. However, the authors could introduce another dataset, e.g. CIFAR10 or 100 or even ImageNet restricted to 20 or 100 random classes/with fewer samples per class and use deeper modern CNN architectures like ResNets (even a ResNet18). Those models are widely adopted both in the research community and by the industry, thus defense mechanisms that provably work for such models can have a huge impact. <sep> - Defense setup. Is the order of Steps 1 and 2 relevant? What if the authors use only iterations of Step 2? <sep> - Attack setup. Here are a few suggestions for assessing your method in an attack setting: what is the precision of the network, without any defense, given an average dissimilarity L2 budget in the training/test samples, in a white/black box setting? How does it compare to standard techniques (e.g. FGSM, IFGSM, DeepFool, Carlini)? What happens if the authors use their method both for both defense and attack? Could the authors display adversarial samples generated by their method? <sep> Conclusion. The idea presented in the paper is interesting, but (1) the experimental results are not entirely satisfactory for the moment and (2) only one aspect of the idea is exploited in the paper, which can be made more interesting and impactful while studying both attack and defense setups. I strongly encourage the authors to continue their research in this area due to the high potential impact and benefits for the whole community.","While the proposed method is novel, the evaluation is not convincing. In particular, the datasets and models used are small. Susceptibility to adversarial examples is tightly related to dimensionality. The study could benefit from more massive datasets (e.g., Imagenet)."
"rating_summary | decision  ==> [summary] <sep> This paper considers natural gradient learning in GAN learning, where the Riemannian structure induced by the Wasserstein-2 distance is employed. More concretely, the constrained Wasserstein-2 metric dW, the geodesic distance on the parameter space induced by the Wasserstein-2 distance in the ambient space, is introduced (Theorem 1). The natural gradient on the parameter space with respect to the constrained Wasserstein-2 metric is then derived (Theorem 2). Since direct evaluation of G−1 poses difficulty, the authors go on to considering a backward scheme using the proximal operator (3), yielding: <sep> (i) The Semi-Backward Euler method is proposed via a second-order Taylor approximation of the proximal operator dW2 (Proposition 3). <sep> (ii) From an alternative formulation for dW (Proposition 4), the authors propose dropping the gradient constraint to define a relaxed Wasserstein metric d, yielding a simple proximal operator given by the expected squared Euclidean distance in the sample space used as a regularizer (equation (4)). The resulting algorithm is termed the Relaxed Wasserstein Proximal (RWP) algorithm. <sep> [pros] <sep> The proposal provides an easy-to-implement drop-in regularizer framework, so that it can straightforwardly be combined with various generator update schemes. <sep> [cons] <sep> Despite all the theoretical arguments given to justify the proposal, the resulting proposal may simply be viewed as a naive application of the proximal operator. <sep> [Quality] <sep> See [Detailed comments] section below. <sep> [Clarity] <sep> This paper is basically clearly written. <sep> [Originality] <sep> Providing justification to the proximal operator approach in GAN learning via natural gradient with respect to the Riemannian structure seems original. <sep> [Significance] <sep> See [Detailed comments] section below. <sep> [Detailed comments] <sep> To the parameter space Θ⊂Rd, one can consider introducing several different Riemannian structures, including the conventional Euclidean structure and that induced by the Wasserstein-2 metric. Which Riemannian structure among all these possibilities would be natural and efficient in GAN training would not be evident, and this paper discusses this issue only in the very special single instance in Section 2.3. A more thorough argument supporting superiority of the Riemannian structure induced by the Wasserstein-2 metric would thus be needed in order to justify the proposed approach. <sep> In relation to this, the result of comparison between WGAN-GP with and without SBE shown in Figure 5 is embarrassing to me, since it might suggest that the proposed framework aiming at performing Wasserstein natural gradient is not so efficient if combined with WGAN-GP. The natural gradient is expected to be efficient when the underlying coordinate system is non-orthonormal (Amari, 1998). Starting with the gradient descent iteration derived from the backward Euler method in (3), which is computationally hard, the argument in this paper goes on to propose two methods: the Semi-Backward Euler method via a second-order Taylor approximation to the backward Euler scheme (Proposition 3), and RWP in (4) via approximation (dropping of the gradient constraint and finite-difference approximation in the integral with respect to t) of an alternative simpler formulation for the Wasserstein metric (Proposition 4). These two methods involve different approximations to the Semi-Backward Euler, and one would like to know why the approximations in the latter method is better in performance than those in the former. Discussion on this point is however missing in this paper. <sep> In Section 3, it would have been better if the performance be compared not only in terms of FID but also the loss considered (i.e., Wasserstein-1), since the latter is exactly what the algorithms are trying to optimize. <sep> Minor points: <sep> Page 4: The line just after equation (4) should be moved to the position following the equation giving d(θ0,θ1)2. <sep> In the reference list, the NIPS paper by Gulrajani et al. appears twice.","Both R3 and R1 argue for rejection, while R2 argues for a weak accept. Given that we have to reject borderline paper, the AC concludes with ""revise and resubmit""."
"abstract | weakness | abstract | weakness | suggestion | weakness | suggestion | weakness | strength | suggestion  ==> The authors introduce two new algorithms: remember and forget experience replay (ReF-ER), and an actor-critic architecture for continuous-action problems which is significantly more computationally efficient than previous approaches (RACER). ReF-ER manages the experience in the replay memory more directly and removes trajectories (episodes) that follow policies less related to the current parameterized policy (based on the importance weights). RACER's main contribution is provides a closed form approximation of the action values, enabling significant gains computationally. They provide several empirical studies in benchmark domains showing the competitiveness of their approach, and the provided more stability to various continuous control algorithms (NAF, PG, u-DDPG). <sep> Overall, I think it is a nicely written paper with a lot of empirical evidence of the usefulness of ReF-ER. I am quite interested in this algorithm specifically, as the active management of experience in the replay memory is an important step towards the ER acting as a proxy to short term memory. To my knowledge this algorithm is novel, and performs admirably. I'm less clear of the main benefits of RACER over previous approaches, except for better computational complexity. This primarily comes from a lack of empirical comparison, and not much explanation as to why key competitors were excluded. The inclusion of RACER seems to muddy the message of the paper, and a much stronger and deeper look at ReF-ER would have made for a stronger submission. <sep> I have several questions for clarity and more comments below, but overall I think the paper is quite useful for the community and contains interesting insight into active management of transitions in an experience replay buffer. <sep> Pros: <sep> ------ <sep> Lots of empirical studies. And a lot of details to impart intuition of the new experience replay. <sep> Interesting take on experience replay. <sep> Convincing results in many simulation benchmark domains (even though the competitors are sparse). <sep> Cons: <sep> ------ <sep> There is some ambiguity and maybe some confusion about the difference between control and off-policy learning. While I agree you are learning off-policy for control (due to the experience replay buffer containing old data), the terms off-policy and on-policy seem overused here. Statements such as ""ER has become one of the mainstay techniques to improve the sample-efficiency of off-policy RL"" aren't entirely correct as the experience replay buffer is primarily used in deep reinforcement learning to improve sample-efficiency, not off-policy reinforcement learning as a whole. <sep> The RACER algorithm seems to muddy up the message of the paper quite a bit. I would have much preferred an in-depth look at ReF-ER here, rather than the introduction of two algorithms. And I think your paper would have been stronger for it. That being said, the RACER algorithm seems incomplete. While it is an improvement over prior approachers (ACER) computationally, the need to use ReF-ER is concerning. I'm also a bit confused why ACER isn't used as a competitor against RACER? Even if you aren't outperforming the other approach on all benchmarks, the improved computational complexity is still a worthwhile improvement. <sep> No confidence bounds in the results, although these are somewhat shown in the appendix (without the competitors shown!!). I'm curious at the significance of the different parameter settings. <sep> Questions: <sep> ---------- <sep> I'm curious as to how this is related to something like rejection sampling? Or other importance sampling approaches more directly? How does your method compare with using retrace or some other off-policy algorithms? I'm unclear on the reasons why these types of comparisons aren't made empirically, could you clarify more directly? <sep> Does your algorithm help with variance issues of other off-policy algorithms? Such as just using importance weights instead of retrace? How would it effect tree backup or just the usual importance sampling? It seems likely that this would help here, as you are limiting the amount of data with high importance weights, although this might also add bias. <sep> Have you removed the target network in your experiments? This detail is not obvious in the paper currently and when you introduce ReF-ER you seem to be leading to this, but never say explicitly. <sep> You claim that ReF-ER ""reduces the sensitivity on the network architecture and training hyper-parameters."" I'm unclear how you show this in the results with the current paper. You do some hyperparameter studies in the appendix, but don't compare against other algorithms here. Could you share a bit further how you are measuring the sensitivities of your algorithm against the competitors? <sep> Do you need to anneal the cmax? What are the effects if this is set to some constant? <sep> Could you expand on the results of HumanoidStandup-v2? Why do you believe your approach does significantly worse than the baselines here? <sep> For DDPG, what happens if you change the bounds instead of removing them entirely? Also how does your method compare on a domain without unbounded actions? <sep> It is unclear why RACER does not work with ER/PER. Do you have any intuition here? Could this be fixed through means other than ReF-ER? <sep> Other minor comments (not taken into consideration for the review): <sep> ------- <sep> Pseudo code: It is a bit unclear what algorithm 1 is supposed to be, I'm assuming ReF-ER? <sep> Begin revision comments: <sep> ----- <sep> Given the revisions to this paper, I am more confident that it will be of interest to the community. The major contributions here I see is the removal of target networks given their approach. Given this I still have concerns on clarity and still am unhappy with the lack of confidence intervals in the main experimental section. I've increased my score to 7 to reflect my increase in confidence.","This paper introduces a novel idea, and demonstrates its utility in several simulated domains. The key parts of the algorithm are (a) to prefer keeping and using samples in the ER buffer where the corresponding rho_t, using the current policy pi_t, are not too big or small and (b) preventing the policy from changing too quickly, so that samples in the ER buffer are more on-policy. <sep> They key weakness is not better investigating the idea of making the ER buffer more on-policy, and the effect of doing so. The experiments compare to other algorithms, but do not sufficiently investigate the use of both Point 1 and Point 3. Further, the appendix contains an investigation into parameter sensitivity and gives some confidence intervals. However, the presentation of this is difficult to follow, and so it is difficult to gauge the sensitivity of Ref-ER. With a more thorough experimental section, better demonstrating the results (not necessarily running more things), the paper would be much stronger. <sep> For more context, the authors rightly mention ""It is commonly believed that off-policy methods (e.g. Q-learning) can handle the dissimilarity between off-policy and on-policy outcomes. We provide ample evidence that training from highly similar-policy experiences is essential to the success of off-policy continuous-action deep RL."" Q-learning can significantly suffer from changing the state-sampling distribution. However, adjusting sampling in the ER buffer using rho_t does not change the state-sampling distribution, and so that mismatch remains a problem. Changing the policy more slowly (Point 3) could help with this more. In general, however, these play two different roles that need to be better understood. The introduction more strongly focuses on classifying samples as more on or off-policy, to solve this problem, rather than the strategy used in Point 3. So, from the current pitch, its not clear which component is solving the issues claimed with off-policy updates. <sep> Overall, this paper has some interesting results and is well-written. With more clarity on the roles of the two components of Ref-ER and what they mean for making the ER buffer more on-policy, in terms of both action selection and state distribution, this paper would be a very useful contribution to stable control."
"strength | weakness | rating_summary | misc | decision  ==> The paper introduces a new cost function for training Wasserstein Autoencoders that combines reconstruction error with Sinkhorn distance on the latent space. Authors provide nice theoretical motivation, yet empirical results seem incremental and do not fully support the effectiveness of this approach. <sep> Pros: <sep> - Theorem 3.1 (although trivial) provides motivation for optimizing Wasserstein distance in the latent space in WAEs. <sep> - Theorem 3.2 shows sufficiency of optimization over deterministic encoders in WAEs. <sep> - The proposed SAE virtually does not favor any prior and can preserve some aspects of geometry of the original space. <sep> Cons: <sep> - It is unclear why Sinkhorn algorithm would provide better estimate of Wasserstein distance than e.g. adversarial WGANGP (which would be a variant of GAN-WAE). Sinkhorn convergence is discussed only in terms of sample size and  smoothing regularizer, not in the context of batch training. <sep> - Quantitative results are on par or marginally better than other methods, they also lack some comparisons (see details below). <sep> - There is no comparison to relevant models outside VAE scope, e.g. ALI [4]. <sep> The novelty of this paper is combining WAEs with Sinkhorn algorithm. Overall, it has potential, but the proposed method would probably require clearer evaluation. <sep> Detailed issues: <sep> - Notation for posterior seems somewhat inconsistent and misleading, namely push-forward G#P_Z = P_G, while Q#P_X = Q_Z. <sep> - It is unclear why MMD or GAN losses on WAS's latent space are referred to as heuristics, each of these constitutes a divergence in the same way as the proposed Sinkhorn distance. <sep> - FID scores for MNIST are incomparable due to the use of own network; using LeNet has been proposed [3]. <sep> - It is unclear what 'Empirical lower bounds' for MMD mentioned in Table 1. caption mean, as unbiased MMD estimator (e.g. [2]) is available. On the other hand, FID is known to be biased [3], so test-set FID should be provided for comparison. <sep> - Table 2. lacks comparison of SAE with normal prior even though a) authors note that MMDs are incomparable with different priors, b) SAEs is claimed to be prior-agnostic, c) in such setting MMD-WAE might be advantageous [1]. Again, no test-set FID scores. <sep> - Samples in Figure 2 too small. <sep> - MMD lacks citation (e.g. [2]). <sep> Typos: <sep> p.6 line 3 construcetion -> construction p.6 line 30 Hypersherical -> Hyperspherical <sep> P.8 line 1 this a sign -> this is a sign <sep> [1] Ilya Tolstikhin, Olivier Bousquet, Sylvain Gelly, and Bernhard Schölkopf. Wasserstein Auto-Encoders. *CONF* 2018. <sep> [2] Arthur Gretton, Karsten M. Borgwardt, Malte J Rasch, Bernhard Schölkopf, and Alex J. Smola. A kernel two-sample test. The Journal of Machine Learning Research, 13, 2012a. <sep> [3] Mikołaj Bińkowski, Dougal J. Sutherland, Michael Arbel, Arthur Gretton. Demystifying MMD GANs. *CONF* 2018. <sep> [4] Vincent Dumoulin, Ishmael Belghazi, Ben Poole, Olivier Mastropietro, Alex Lamb, Martin Arjovsky and Aaron Courville. Adversarially Learned Inference. *CONF* 2017","The reviewers appreciated the contribution of combining Wasserstein Autoencoders with the Sinkhorn algorithm. <sep> Yet R4 as well as the author of the WAE paper (Ilya Tolstikhin) both expressed concerns about the empirical evaluation. <sep> While R1-R3 were all somewhat positive in their recommendation after the rebuttal, they all have somewhat lower confidence reviews, as is also clear by their comments. <sep> The AC decided to follow the recommendation of R4 as they were the most expert reviewer. The AC thus recommends to ""revise and resubmit"" the paper."
"abstract | rebuttal_process | strength | decision | misc | rebuttal_process | strength | weakness | rebuttal_process | weakness | decision  ==> In this work, the authors attempt to unify existing adaptive gradient methods under the Bayesian filtering framework with the dynamical prior.  In Ollivier, 2017, a framework is proposed to connect Bayesian filtering and natural gradient.  On the other hand,  in Khan et al., 2018. an approach is proposed to connect natural gradient and adaptive gradient methods.  The main contributions of this work are (1)  introducing a dynamical prior and (2) recovering RMSProp and Adam as special cases. <sep> However, the proposed dynamical prior is very similar to the fading memory technique used in Ollivier, 2017. (see Proposition 3 of Ollivier, 2017) <sep> Furthermore, the authors argue that this work recovers a root-mean-square form while Khan et al., 2018 recovers a different sum-square form. Unfortunately, the authors have to use a series of unnatural approximations to recover the root-mean-square form. In fact, as mentioned in Khan, 2017b  this proposed method without these approximations is also a mean-square form. (also see Eq (2.28-2.29) of Ollivier, 2017) <sep> Since the authors mainly follow Ollivier, 2017 and make unnatural approximations,  the work has a limited impact.  To get a higher rating, the authors should clearly give justifications and insights of these approximations. <sep> Detailed comments: <sep> (1) On Page 1,  ""The typical approach to Bayesian filtering, where we infer a distribution, ... jointly, forces us to use extremely strong, factorised approximations, and it is legitimate to worry that these strong approximations might meaningfully disrupt the ability of Bayesian filtering to give close-to-optimal updates.   ... we instead consider ... that incorporates factorisation into the problem setting, and therefore requires fewer approximations downstream. "" <sep> The proposed method is equivalent to jointly perform Kalman filtering with full-covariance with an additional diagonal-approximation step. This additional step might also meaningfully disrupt the ability of Bayesian filtering. Furthermore, such approximation ignores the off-diagonal terms in the low-rank approximation at Eq (8). <sep> Minor: You should use \\approx at Eq (8) since a rank-1 approximation is used. <sep> (2) On page 2, ""It has been noted that under specific circumstances, natural gradient is approximate Bayesian filtering (Ollivier, 2017), allowing us to link Bayesian filtering to the rich literature on natural gradients.  However, this only occurs when the dynamical prior in the Bayesian filtering problem has a specific form: the parameters being fixed over time (i.e.  arguably an online data, rather than a true Bayesian filtering setting)."" <sep> The authors should comment the difference between the dynamical prior and the fading memory technique (see Proposition 3 of Ollivier, 2017) where at page 14 of Ollivier, 2017, Ollivier mentions that ""this is equivalent ... or to the addition of an artificial process noise ... in the model"".  I think Ollivier's idea is very similar to the dynamical prior used at Eq (1) of this submission.  Furthermore, the second-order Taylor expansion with a Fisher information-based estimation of Hessian (see the equation below Eq(1) of this submission) is exactly the same as Ollivier's Extended Kalman filter (see  Eq 2.25 at Lemma 9  and Lemma 10 of Ollivier, 2017).  The authors should cite Ollivier, 2017. <sep> Minor: Eq (6) should be E_p [ - \\nabla_z^2 \\log p(d|z) ] = E_p  [ e e^T ], where ""-"", the negative sign is missing. Please see the definition of the Fisher information matrix. <sep> (3) On page 2, ""While there have been attempts to use natural gradients to recover the Adam or RMSprop root-mean-square form for the gradient normalizer, in practice a different sum-square form emerges (Khan & Lin, 2017; Khan et al., 2018). In contrast, we show that to recover the Adam or RMSprop form for the gradient normalizer."" <sep> Khan et al., 2018 is a mean-square form for variational inference due to the entropy term of the variational distribution. (see Sec 3 and 5 of  Khan et al., 2018 and Khan, 2017b ) <sep> Unfortunately, the ""root-mean-square form"" does not appear naturally in this submission. In practice, the proposed update is also a mean-square form  (see Eq (2.28-2.29) of Ollivier, 2017 and Khan, 2017b) without a series of unnatural approximations used in this submission. <sep> To justify these assumptions, the authors should explain when ""the steady state posterior variance"" (see sec 2.21) and  ""a self-consistent solution"" (see sec 7.1) achieve.  As far as I know, \\sigma^2_t = \\sigma^2_{t+1} in sec 2.2.1 only holds in the limit case when t-> \\inifity.  Why does the equality hold at each time step t? The authors should give a justification or an intuition about these approximations since this paper is a theory paper. Please also see my next point. <sep> (4) Section 7.1 is also confusing. <sep> In sec 7.1, the authors assume that A \\in O(\\eta). However, A=\\eta^2/(2\\sigma^2) in sec 2.2 and A_{1,1} =  ( \\eta_w^2+\\eta^2 )/ (2\\sigma^2) at Eq (14). In both cases, A can be \\in O(\\eta^2). This is very *critical* since the authors argue that O(\\eta^3) can be neglected in sec 7.1.  The authors use this point to show that Adam is a special case. <sep> If A \\in O(\\eta^2), we know that ""A \\Sigma_{post}"" \\in O(\\eta^3) should be neglected. At the last equation on page 10,  the authors do not neglect ""A \\Sigma_{post}"". Why?  The authors should clarify this point to avoid doing *selective* neglection.  Again, the impact of this paper should be inspiring new adaptive methods. <sep> The authors also mention that the second-order term in A is neglected in sec 7.2. Any justification? <sep> References <sep> [1] Ollivier, Yann. ""Online Natural Gradient as a Kalman Filter."" arXiv preprint arXiv:1703.00209 (2017). <sep> [2] Khan, Mohammad Emtiyaz, and Wu Lin. ""Conjugate-computation variational inference: Converting variational inference in non-conjugate models to inferences in conjugate models."" arXiv preprint arXiv:1703.04265 (2017). <sep> [3] Khan, Mohammad Emtiyaz, et al. ""Vprop: Variational Inference using RMSprop."" arXiv preprint arXiv:1712.01038 (2017b). <sep> [4] Khan, Mohammad Emtiyaz, et al. ""Fast and Scalable Bayesian Deep Learning by Weight-Perturbation in Adam"" (2018)","The aim of this paper is to interpret various optimizers such as RMSprop, Adam, and NAG, as approximate Kalman filtering of the optimal parameters. These algorithms are derived as inference procedures in various dynamical systems. The main empirical result is the algorithms achieve slightly better test accuracy on MNIST compared to an unregularized network trained with Adam or RMSprop. <sep> This was a controversial paper, and each of the reviewers had a significant back-and-forth with the authors. The controversy reflects that this is a pretty interesting and relevant topic: a proper Bayesian framework could provide significant guidance for developing better optimizers and regularizers. Unfortunately, I don't think this paper delivers on its promise of a unifying Bayesian framework for these various methods, and I don't think it's quite ready for publication at *CONF*. <sep> There was some controversy about relationships to various recently published papers giving Bayesian interpretations of optimizers. The authors believe the added value of this submission is that it recovers features such as momentum and root-mean-square normalization. This would be a very interesting contribution beyond those works. But R2 and R3 feel like these particular features were derived using fairly ad-hoc assumptions or approximations almost designed to obtain existing algorithms, and from reading the paper I have to say I agree with the reviewers. <sep> There was a lot of back-and-forth about the correctness of various theoretical claims. But overall, my impression is that the theoretical arguments in this paper exceed the bar for a primarily practical/empirical paper, but aren't rigorous enough for the paper to stand purely based on the theoretical contributions. <sep> Unfortunately, the empirical part of the paper is rather lacking. The only experiment reported is on MNIST, and the only result is improved test error. The baseline gets below 99% test accuracy, below the level achieved by the original LeNet, suggesting the baseline may be somehow broken. Simply measuring test error doesn't really get at the benefits of Bayesian approaches, as it doesn't distinguish it from the many other regularizers that have been proposed. Since the proposed method is nearly identical to things like Adam or NAG, I don't see any reason it can't be evaluated on more challenging problems (as reviewers have asked for). <sep> Overall, while I find the ideas promising, I think the paper needs considerable work before it is ready for publication at *CONF*."
"rating_summary  ==> Summary <sep> This paper introduced a parameterized image processing technique to improve a robustness of visual recognition systems against noisy input data. The proposed method is composed of two components; a denoising network that suppresses the noise signals in an image, and gating network that predicts whether to use the original input image or the one produced by the denoising network. The proposed idea is evaluated on three tasks of object detection, tracking and action recognition. <sep> Originality and significance: <sep> The originality of the paper is very limited since the paper simply combines the existing image denoising technique with the idea of gating. The practical significance of the work is also limited since the model is trained and evaluated with only synthetically generated noise patterns; it is not surprising that the proposed method (both denoising and gating networks) works under this setting, as the noise is created synthetically under the same setting in both training and testing. To demonstrate the practical usefulness, it would be great if the model is evaluated with the actual source of noises (e.g. noises from input sensors, distortion by image compression, etc). <sep> Clarity: <sep> I think the title of the paper is misleading; the proposed model is actually not a mixture of preprocessing units, as it combines *a* denoising unit together with identity mapping. The gating network is also not designed to incorporate a mixture of more than two preprocessing units, as it outputs only ""on/off switches"" instead of weights for K mixture components (K>2). <sep> Minor comments: <sep> 1) the paper argued the importance of lightweight preprocessing but have not provided analysis on computation costs. From the current results, I don't see the clear benefit of the proposed method (denoising network) over the average filtering considering the tradeoff between computation vs. performance. <sep> 2) In Figure 5, I suggest highlighting the differences among the examples for clarity.","As the reviewers point out, the paper seems to be below the *CONF* publication bar due to low novelty and limited significance."
"abstract | strength | weakness | rebuttal_process | rating_summary | weakness | misc | decision  ==>  ==> This is a well written paper which proposes to learn heteroscedastic noise models from data by optimizing the prediction likelihood end-to-end through differentiable Bayesian Filters. In addition to existing Bayesian filters, the paper also proposes two different versions of the [differentiable] Unscented Kalman Filter. Performance of the different filters and noise models is evaluated on two real-world robotic problems: Visual Odometry and visual tracking of an object pushed by the robot. <sep> While the general idea of learning the noise variances through backpropagation are straightforward extensions of existing work on differential Bayesian filters, the questions that the paper explores are important to make end-to-end learning of Bayesian filter more common. The results will help future research select the correct differential filter for their use case, and insight in potential benefits (or lack thereof) by learning heteroscedastic or homoscedastic process noise, and/or observation noise. <sep> A downside is that the paper does not further explore how to weigh different loss terms which are apparently important to successfully train such models. Also unfortunate is the footnote which states that the current results are incomplete and will be updated, hence as a reviewer I am not sure which results and conclusions are valid right now. <sep> Pros: <sep> + clearly written <sep> + useful experiments for those seeking to select a differential Bayesian filter, and learning (heteroscedastic) noise from data. <sep> + experiments on real-world use cases rather than toy problems <sep> Cons: <sep> - Incomplete experiments according to footnote, thus results and conclusions might change after this review. <sep> - Unclear what the effect of the selected process / observation model is on the learned noise <sep> Below are more detailed comments and questions: <sep> * p6. Footnote: ""due to time constraints, ..., results will be updated"" Is this acceptable? I have never seen such a notice when reviewing. So, are the current results on a single fold? Will the numbers in the tables, or the conclusions change after this review? <sep> * If I understand correctly, the paper 'only' focuses on learning the heteroscedastic noise variance, but assumes that the deterministic non-linear parts of the process and observation models are fixed. I did not find this very clearly stated in the paper, though at least the Appendix explicitly states the used functions for the process models. <sep> * I would have liked to see in the paper more explanation on how the process and observations models were selected and validated  in the experiments, since I expect that the validity of these functions affects the learned noise variances. Since the noise needs to account for the inaccuracies in the deterministic models, would the choice for these functions not impact your conclusions? And, would it or would it not be possible to learn both these deterministic models and the noise jointly from the training data? <sep> * Is it possible to add priors on Q and R parameters for Bayesian treatment of learning model parameters? I can imagine that priors can guide the optimization to either adjust more of the Q or more of the R variance to improve the likelihood. <sep> * Section 1: <sep> * ""Our experiments show that ... "" This may be a matter of taste, but I did not expect to see the main conclusions already in the introduction. They should appear in the abstract to help out the quick reader. In the introduction, it appears as if you are talking about some separate preliminary experiments, and which you base some conclusions that will be used in the remainder of this paper. <sep> * Section 3: <sep> * So, mostly empirical study, since heteroscedastic noise models were already used? <sep> * ""Previous work evaluated ... "" please add citations <sep> * Section 4.1: <sep> * ""train a discriminative neural network o with parameters wo to preprocess the raw sensory data D and thus create a more compact representation of the observations z = o(D;wo)."" At this point in the paper, I don't understand this. How is z learned, via supervised learning (what is the target value for z)? Or is z some latent representation that is jointly optimized with the filters? This only became somewhat clearer in Sec. 5.2 on p.8 where it states that ""We ... train a neural network to extract the position of the object, the contact point and normal as well as ..."". So if I understand correctly, the function o for z = o(D) is thus learned offline w.r.t. some designed observation variables for which GT is available (from manual annotations?). <sep> * Section 4.2: <sep> * ""we predict a separate Qi for every sigma point and then compute Q as the weighted mean"" → So, separate parameters w_g for each sigma point i, or is a single learned non-linear function applied to all points? <sep> * Section 4.3: <sep> * Equation 14: inconsistent use of boldface script: should use bold sigma_t, and bold l_t ? <sep> * ""In practice, we found that during learning ... by only increasing the predicted variance"" →  This is an interesting observation, which I would have liked to see explored more. I understand that term (ii) is needed to guide the learning processes, but in the end wouldn't we want to optimize the actual likelihood? So, could you (after the loss with (ii) converged) reduce \\lambda_2 to zero to properly optimize only the log likelihood without guidance from a good initial state? Or is it not possible to reliably optimize the likelihood via back-propagation at all from some reason? <sep> * Section 5.1.1 <sep> * ""... of varying length (from 270 to over 4500 steps) ..."" it would be good to mention the fps, to get understand to what real-world time horizons 50 / 100 frames correspond. <sep> * Section 5.1.2: <sep> * Table 1: How are the parameters of the filters in the ""no learning"" column obtained? Are these tuned in some other way, or taken form existing implementations? Also, can you clarify if the 'no learning' parameters served as the initial condition for the learning approaches? <sep> * Table 1, first row column Q+R: ""0.2"" → Is there a missing zero here, i.e. ""0.20""? Otherwise, the precision of reported results in this table is not consistent. Hard to say: is the mean of R+Q 0.2, and slightly lower than R+Qh, or could it be as high as 0.24 ? <sep> * ""learning a heteroscedastic process noise model leads to big improvements and makes the filters competitive with the EKF"". Results for EKF still appear significantly better than the novel UKF, and even the PF (especially rotational error). <sep> * Section 6: <sep> * ""Large outliers in the prediction of the preprocessing networks were not associated with higher observation noise."" I don't see on what presented results these conclusions were drawn, as this is the first time the word ""outlier"" is mentioned in the paper. Outliers seem indeed important, as they contradict the typical assumptions e.g. of Gaussian noise, so it would be useful to clarify how the proposed techniques handle such outliers.","This paper shows experiments in favor of learning and using heteroscedastic noise models for differentiable Bayes filter. Reviewers agree that this is interesting and also very useful for the community. However, they have also found plenty of issues with the presentation, execution and evaluations shown in the paper. Post rebuttal, one of the reviewer increased their score, but the other has reduced the score. Overall, the reviewers are in agreement that more work is required before this work can be accepted. <sep> Some of existing work on variational inference has not been included which, I agree, is problematic. Simple methods have been compared but then why these methods were chosen and not the other ones, is not completely clear. The paper definitely can improve on this aspect, clearly discussing relationships to many existing methods and then picking important methods to clearly bring some useful insights about learning heteroscedastic noise. Such insights are currently missing in the paper. <sep> Reviewers have given many useful feedback in their review, and I believe this can be helpful for the authors to improve their work. In its current form, the paper is not ready to be accepted and I recommend rejection. I encourage the authors to resubmit this work."
"abstract | weakness | decision  ==> The paper proposes a method of searching for a Nash equilibrium strategy in games where the strategy-to-payoff mapping is defined by a neural network. The idea is to perform gradient optimization of the payoff w.r.t. the strategy. Preliminary results on tic-tac-toe and variations of the prisoner's dilemma task are presented. The paper has an interesting idea at the core. However, it is poorly written, does not properly discuss the related works and does not present a convincing method or experimental results. <sep> Pros: <sep> * The paper considers an interesting question of exploring the applications of neural networks to the game theory problems. <sep> * The idea of the paper is reasonable. It makes sense to me to perform gradient-based search over the strategies (assuming that the payoff is differentiable). <sep> Cons: <sep> * Writing <sep> - The paper is over the mandatory length limit of 10 pages. <sep> - The paper makes a grandiose claim: ""this paper provides a revolutionary way for reinforcement learning and a possible road toward general A.I."" However, there are arguably no revolutionary ideas, and certainly no reinforcement learning experiments! <sep> - Despite the claim, the novelty of the paper is limited. There is no discussion of the related work: optimization of the neural networks w.r.t. the inputs [1]; related RL ideas such as model-based learning [2,3] and Monte-Carlo Tree Search [4]. <sep> - The problem being solved is never formally stated. As far as I understand, Nash equilibrium is usually defined (1) in mixed strategies, while the paper seems to consider pure strategies; (2) in the scenario where every player attempts to maximize their payoff, while in the paper the players attempt to achieve some pre-fixed value of the payoff. <sep> - The flow of the paper is generally poor. Instead of presenting a general solution and then showcasing its applications, the paper iterates on similar ideas multiple times. For example, all four algorithms are just gradient-based optimization of either the weights or the inputs to a model. <sep> - The paper provides extremely misleading analogies and explanations. I am quite sure that a mosquito brain is not a one hidden layer fully-connected neural network! Also, the example of avoiding a moving hand is poor: since the outcome is life or death, the learning should happen via evolution, not during the lifetime of a single insect. The claim that the neural networks with sigmoid activation functions are less prone to local optima is questionable as well. <sep> * Method and experiments <sep> - The proposed method is essentially a greedy gradient-based planning procedure. For this to work, we need to have a very good environment model. This is a strong assumption that is not discussed. <sep> - The experiments are performed on very simple synthetic problems: matrix games and tic-tac-toe. They do not suggest that the method is general and can work on harder problems, say, Sokoban [2]. <sep> - The experiments do not present any baselines, so it is unclear how well the method performs compared to the alternatives. One obvious candidate is gradient-free optimization, such as Nelder-Mead, and gradient descent with momentum, which can be less prone to local optima. <sep> [1] Brandon Amos, Lei Xu, J. Zico Kolter ""Input Convex Neural Networks"", ICML 2017 <sep> [2] Racanière et al. ""Imagination-Augmented Agents for Deep Reinforcement Learning"", NIPS 2017 <sep> [3] David Ha, Jürgen Schmidhuber ""Recurrent World Models Facilitate Policy Evolution"", NIPS 2018 <sep> [4] Thomas Anthony, Zheng Tian, David Barber ""Thinking Fast and Slow with Deep Learning and Tree Search"", NIPS 2017","The paper presents ""deep deducing"", which means learning the state-action value function of 2 player games from a payoff table, and using the value function by maximizing over the (actionable) inputs at test time. <sep> The paper lacks clarity overall. The method does not contain any new model nor algorithm. The experiments are too weak (easy environments, few/no comparisons) to support the claims. <sep> The paper is not ready for publication at this time."
"abstract | weakness | rebuttal_process  ==> This is an interesting submission and I appreciate especially connecting to a body of literature which is not normally well known in our community (e.g. Fukumizu&Amari). I think the perspective is definitely new and probably quite relevant not only for practical approaches to escape saddles but also to understand learning in deep learning.  I have a few notes and suggestions: <sep> 1) Name of the paper: <sep> I think is not descriptive of the approach and actually the words ""magic"" makes it sound strange. I think this will reduce the amount of people reading the work. Please consider something more descriptive like: ""Escaping saddle points by increasing capacity"". Or something else more inspired, but that also hints what the work is about. <sep> 2) Notation: <sep> The notation used is not ML friendly (or generically) to the average reader. I strongly suggest to use b_v for bias, W_uv for weights, and not theta_v and theta_uv which is not typical notation. 'u' and 'v' are somewhat non-typical choices either, though I understand that they come from the graph notation. Transfer functions are usual sigma. In the text you explain the process by starting with a u' and then add the clone which is u. Normally you should have started with u and add the clone that is u'. x_u for the value of unit u (assuming this is in the middle of a deep net) is also quite a strange notation. I can guess the authors might be from a slightly different community, but I'm worried about people from the target audience (*CONF*) being turned away from the work or even worse confused because of notations. <sep> 3) Related work <sep> There is the Net2Net work that is related to what is going on here that is not cited (https://arxiv.org/pdf/1511.05641.pdf). I think there was some follow-up work after this. <sep> 4) Symmetry breaking <sep> I do not understand how symmetry is broken. If I clone a unit, and have a new variant of it u' that now has the same outbound connections but multiplied by alpha (while the original unit by 1-alpha) then while the norm of the gradients differ, their direction does not. Wouldn't this mean that the units will track each other and hence no tunnel is open? In Net2Net dropout was used to break symmetry (i.e. a source of noise that would pick one path over the other). There is no source of noise here to break symmetry. <sep> 5) Diagrams and analysis <sep> Connected to this, I feel like this could have been represented clearly with a diagram showing the net before and after. There could be some analysis, a more extended discussion of where the symmetry breaking comes from, empirical evidence that it does. I'm not necessarily worried that experiments are not scaled up, I'm more concerned that the hypothesis and solution is only tested by means of change in performance. What is this tunnel doing? How does it change the Hessian at the saddle? Any visualization to reinforce the intuition of what the approach is doing? <sep> 6) Closing tunnels & re-organizing <sep> I don't understand the mechanism for reorganizing weights and closing tunnels. It seems first of all to confirm my intuition that there is no symmetry breaking since we can ""close"" the tunnel by simple algebraic manipulation. So if those two units always stay in sync how do you actually change the error surface? How do you take advantage of this extra capacity to solve anything. Regardless, when it comes to re-organizing, it seems you pick two of these units that are in sync (previous tunnel I guess) and collapse them to open a new tunnel, right? How does this change anything? Which unit needs to be cloned? Any? So then why is the previous tunnel not efficient anymore?","The paper proposes a method to escape saddle points by adding and removing units during training. The method does so by preserving the function when the unit is added while increasing the gradient norm to move away from the critical point. The experimental evaluation shows that the proposed method does escape when positioned at a saddle point - as found by the Newton method. The reviewers find the theoretical ideas interesting and novel, but they raised concerns about the method's applicability for typical initializations, the experimental setup, as well as the terminology used in the paper. The title and terminology were improved with the revision, but the other issues were not sufficiently addressed."
"weakness | rebuttal_process | decision  ==> Note: This review is coming in a bit late, already after one round of responses. So I write this with the benefit of having read the helpful previous exchange. <sep> I am generally positive about the paper and the broader project. The idea of showing that causal reasoning naturally emerges from certain decision-making tasks and that modern (meta-learning) RL agents can become attuned to causal structure of the world without being explicitly trained to answer causal questions is an attractive one. I also find much about the specific paper elegant and creative. Considering three grades of causal sophistication (from conditional probability to cause-effect reasoning to counterfactual prediction) seems like the right thing to do in this setting. <sep> Despite these positive qualities, I was confused by many of the same issues as other reviewers, and I think the paper does need some more serious revisions. Some of these are simple matters of clarification as the authors acknowledge; others, however, require further substantive work. It sounds like the authors are committed to doing some of this work, and I would like to add one more vote of encouragement. While the paper may be slightly too preliminary for acceptance at this time, I am optimistic that a future version of this paper will be a wonderful contribution. <sep> (*) The authors say at several points that the approach ""did not require explicit knowledge of formal principles of causal inference."" But there seem to be a whole of lot of causal assumptions that are critically implicit in the setup. It would be good to understand this better. In particular, the different agents are hardwired to have access to different kinds of information. The interventional agent is provided with data that the conditional agent simply doesn't get to see. Likewise, the counterfactual agent is provided with information about noise. Any sufficiently powerful learning system will realize that (and even how) the given information is relevant to the decision-making task at hand. A lot of the work (all of the work?) seems to be done by supplying the information that we know would be relevant. <sep> (*) Previous reviewers have already made this point - I think it's crucial - and it's also related to the previous concern: It is not clear how difficult the tasks facing these agents actually are, nor is it clear that solving them genuinely requires causal understanding. What seems to be shown is that, by supplying information that's critical for the task at hand, a sufficiently powerful learning agent is able to harness that information successfully. But how difficult is this task, and why does it require causal understanding? I do think that some of the work the authors did is quite helpful, e.g., dividing the test set between the easy and hard cases (orphan / parented, unconfounded / confounded). But I do not feel I have an adequate understanding of the task as seen, so to say, from the perspective of the agent. Specifically: <sep> (*) I completely second the worry one of the reviewers raised about equivalence classes and symmetries. The test set should be chosen more deliberately - not randomly - to rule out deflationary explanations of the agents' purported success. I'm happy to hear that the authors will be looking more into this and I would be interested to know how the results look. <sep> (*) The ""baselines"" in this paper are often not baselines at all, but rather various optimal approaches to alternative formulations of the task. I feel we need more actual baselines in order to see how well the agents of interest are doing. I don't know how to interpret phrases like ""close to perfect"" without a better understanding of how things look below perfection. <sep> As a concrete case of this, just like the other reviewers, I was initially quite confused about the passive agents and why they did better than the active agents. These are passive agents who actually get to make multiple observations, rather than baseline passive agents who choose interventions in a suboptimal way. I think it would be helpful to compare against an agent who makes the same number of observations but chooses them in a suboptimal (e.g., random) way. <sep> (*) In relation to the existing literature on causal induction, it's telling that implementing a perfect MAP agent in this setting is even possible. This makes me worry further about how easy these tasks are (again, provided one has all of the relevant information about the task). But it also shows that comparison with existing causal inference methods is simply inappropriate here, since those methods are designed for realistic settings where MAP inference is far from possible. I think that's fine, but I also think it should be clarified in the paper. The point is not (at least yet) that these methods are competitors to causal inference methods that do ""require explicit knowledge of formal principles of causal inference,"" but rather that we have a proof-of-concept that some elementary causal understanding may emerge from typical RL tasks when agents are faced with the right kinds of tasks and given access to the right kinds of data. That's an interesting claim on its own. The penultimate paragraph in the paper (among other passages) seems to me quite misleading on this point. <sep> (*) One very minor question I have is why actions were softmax selected even in the quiz phase. What were the softmax parameters? And would some of the agents not perform a bit better if they maximized?","The reviewers raised a number of concerns including insufficiently demonstrated benefits of the proposed methodology, lack of explanations, and the lack of thorough and convincing experimental evaluation. The authors' rebuttal failed to alleviate these concerns fully. I agree with the main concerns raised and, although I also believe that the work can result eventually in a very interesting paper, I cannot suggest it at this stage for presentation at *CONF*."
"abstract | weakness | decision  ==> This paper demonstrates that CNNs are more robust to class-relevant label noise. They argue that real-world noise should be class-relevant. <sep> Pros: <sep> 1. The authors find a new angle to exploit robust learning with noisy labels. <sep> 2. The authors perform numerical experiments to demonstrate the effectiveness of their proposal. And their experimental result support their previous claims. <sep> Cons: <sep> We have two questions in the following. <sep> 1. Basic definition: in learning with noisy labels, there are two basic models. First, most research focuses on class-conditional noise (CCN) model [1]. Second, recent research explore a bit on instance-dependent noise (IDN) model [2, 3]. As far as I know, there is no class-irrelevant label noise and class-relevevant label noise. In CCN mode, people would like to use symmetric noise and asymmetric noise as a basic benchmark to conduct experiments. <sep> 2. Motivation: The authors want to claim CNNs are more robust to such realistic label noise than class-irrelevant label noise. However, they make one mistake. They do not have a clear definition about realistic label noise. In my mind, I believe Clothing1M [4] should be realistic label noise dataset. <sep> By the way, in learning with noisy labels, there are two kinds of research. First, people propose new robust methods for CCN model. Second, people propose new robust methods for IDN models. Proposing new setting should be encouraged. However, the setting and conclusion should be reasonable. <sep> References: <sep> [1] D. Angluin and P. Laird. Learning from noisy examples. Machine Learning, 1988. <sep> [2] A. Menon, B. Rooyen, and N. Natarajan. Learning from binary labels with instance-dependent corruption. Machine Learning, 2018. <sep> [3] J. Cheng, T. Liu, K. Ramamohanarao, D. Tao. Learning with bounded instance-and label-dependent label noise. arxiv 1709.03768, 2017. <sep> [4] T. Xiao, T. Xia, Y. Yang, C. Huang, and X. Wang. Learning from massive noisy labeled data for image classification. In CVPR, 2015.","The paper analyzes the performance of CNN models when data is mislabelled in different manners. <sep> The reviewers and AC note the critical limitation of novelty of this paper to meet the high standard of *CONF*. <sep> AC thinks the proposed method has potential and is interesting, but decided that the authors need more works to publish."
"abstract | rating_summary | rebuttal_process | decision  ==> This paper focuses on the extraction of the (multi) periodicities from a signal. The paper describes the conventional method based on the Fourier transformation and/or autocorrelation methods, and proposed method, which first detects a distribution of spectral leakages, and prune the periodicity hints by using a clustering algorithm. The proposed method is also extended to deal with multi-periodicities. The effectiveness of the proposed method is shown with the controlled simulation data and several real data. This paper is well written (note it is over 8 pages though), but it is not learning-based approach, and would not best fit to major *CONF* interests. <sep> Comments: <sep> - The abstract needs to be more self-consistent without referring the citation for a brief explanation. Also it should have more detailed experimental discussions. <sep> - Algorithm 1 needs some refinement (too code-like, although it is understandable). For example, several methods (nextBinValue and append) would be better to be replaced with other (human readable) expressions.","This paper presents an heuristic method to detect periodicity in a time-series such that it can handle noise and multiple periods. <sep> All reviewers agreed that this paper falls off the scope of *CONF* since it does not discuss any learning-related question. Moreover, the authors did not provide any response nor updated manuscript addressing the reviewers remarks. The AC thus recommends rejection."
"rating_summary | decision | weakness | decision  ==> In this submission, the authors propose a three-stage framework for large-scale graph embedding. The proposed method first constructs a small graph by graph coarsening, then applies any existing graph embedding method, and last refines the learned embeddings. It is useful, however, the experimental results are not convincing and cannot support the authors' claims about the proposed method. <sep> First, in many places, the authors claim that the embedding quality of the proposed method is improved. For example, the last sentence of Section 1, and ""MILE improves quality"" paragraph on Page 7. However, the experimental results fail to support this. As the proposed method is for the large-scale graph, let's focus on the results of YouTube dataset and Yelp dataset first. For Youtube dataset ((d) of Table 2), when m is set to be 8, for all the cases, the performance drops. For Yelp dataset (Figure 3), the authors do not provide Micro-f1 for the original graph (m = 0) or m = 1, 2, so it is hard or impossible to demonstrate that the quality of the proposed method is still good. <sep> Second, the comparison with existing methods is not sufficient. For the most important Yelp dataset (as this dataset fits the motivation scenario (large-scale graph) of this submission), the authors fail to report any comparison. Thus it might not be weak to demonstrate the benefit of the proposed method. <sep> Third, some experiment details are missing. For example, how the authors compute the running time of the proposed method? All the three stages are included? How the authors implement the existing methods? Are these implementations good enough to ensure a fair comparison? <sep> ******* <sep> Some other questions: <sep> a) On page 2, the authors mention that the proposed method ""can be easily extended to directed graph"". However, based on my understanding, directly graph will affect both the graph coarsening and embedding refining steps, and it seems not so easy to extend. Do the authors have the solution and experiments for directed graph? It would be interesting to see such results, which enlarges the application scope of the proposed method. <sep> b) The toy example on page 3 is very clear. However, for real-world graphs, does the proposed graph coarsening work well? For example, one property the proposed method utilizes is ""structurally equivalent"". What is the percentage of the nodes that can have such property for real-world graphs? <sep> ******** <sep> Some other comments: <sep> Generally speaking, this submission studies a very practical task. Although the authors claim that the proposed method has great efficiency while the embedding quality is comparable good or even better than the existing methods, I think that there is an efficiency-quality trade-off based on the experimental results in this submission. When m increases, the graph coarsening step causes more information loss, and thus the quality may decrease. Embedding refining step can be regarded as a procedure to reduce such information loss, but may not improve the embedding quality better than the original graph. So to me, it would be more meaningful to study such efficiency-quality trade-off for large-scale graph embedding.","Significant spread of scores across the reviewers and unfortunately not much discussion despite prompts from the area chair and the authors. The most positive reviewer is the least confident one. Very close to the decision boundary but after careful consideration by the senior PCs just below the acceptance threshold. There is significant literature already on this topic. The ""thought delta"" created by this paper and the empirical results are also not sufficient for acceptance."
"rebuttal_process | abstract | strength | weakness | suggestion | misc  ==> The paper formulates a definition of easy and hard examples and studies the properties and the training implications of such examples. The paper does not attempt to present insights that change training for the better (although suggests this could be future work), so the primary value it claims to add is our understanding of neural networks. I think the paper presents findings that most deep learning practitioners already find intuitive, which is why I think the paper falls short in its primary mission. An exposé like this could be valuable for an introductory text in deep learning, but I do not think the analysis meets the bar for a cutting edge insight and do not think it should be accepted. <sep> Strengths: <sep> - Quantifying easy and hard and using that as a starting point for further analysis is not a bad starting point at all. <sep> - The experiments form a good starting point for interesting analysis. <sep> - The paper is easy to follow and understand. <sep> Weaknesses: <sep> - The biggest weakness is that I just didn't find any of conclusions from this study to be that surprising or interesting. I think it's pretty obvious that neural networks start by learning the most immediately discriminative features (""frequent patterns""). The visual examples of easy and hard examples are not surprising at all (I think you get similar clusters if you just show bottom and top of model confidences, which I think many of us have). In section 7.1, the result that most misclassified examples are hard examples is presented as a surprising result. This is confusing, because this exactly what I would have expected given how you define easy/hard. It would be far more surprising if misclassified examples were all considered easy under your definition. <sep> - The paper only scratches the surface. In Figure 2, the results for the two different datasets are quite different. This means only two datasets is probably not enough for us to understand what is going on here in general. Just the conclusion that datasets may be different in terms of easy/hard samples does not take the analysis far enough. It's also unclear what the reader should make of these conclusions. <sep> - In Table 1, let's say the bottom 30% of samples are actually equally easy. This would mean that the ""easy"" examples are just a random 1/3 of those samples. Basically, I'm worried about the implications of having a hard cut-off at 10% and if there are situations where the bottom 10% actually changed quite a bit, but the broader picture of easy really didn't change that much. I guess I'm saying that I didn't quite gain confidence that definitions of easy/hard and matching rate are the correct way to go here and there might be a better metric that can look at the continuum of easy/hard from e = 0 to 1. You could have some kind of distance function where if an example moved from 5th percentile to 12th percentile, it would constitute a distance of 7. This is perhaps not the right thing either, but presenting an alternative metric and showing that the numbers (up to scale) and conclusions are unchanged would be nice. <sep> Other comments: <sep> - The new terminology of ""contradicted pattern"" and ""non-contradicted pattern"" is a bit confusing. Why aren't you just calling these ""non-discriminative"" and ""discriminative""? If a mantis and a ladybird are both typically on a leaf, the leaf is not discriminative for this task. However, if a mantis and a boat are typically on differently colored backgrounds, the background is discriminative. <sep> Minor comments: <sep> - page 1, ""easy and hard examples differ on various CNNs architectures"" -> ""CNN"" <sep> - page 2, ""as a criteria"" -> ""criterion"" <sep> - page 2, ""We then redefine easy and hard"" -> don't you mean just ""define""? Or do you mean that the words already have casual meanings, so this is a redefinition? I still think ""define"" is less confusing here. <sep> - page 2, I think it's confusing that both easy and hard use the threshold \\tau, suggesting it is the same. Maybe put a subscript to make it clear that the two \\taus are different. <sep> - page 6, ""accuracy does not drop"" -> could use a ""does not *even* drop"" for clarity <sep> - page 7, ""7.1 Do misclassified examples in validation dataset are hard examples"": ""Do""->""Are"", remove ""are","There is no author response for this paper. The paper formulates a definition of easy and hard examples for training a neural network (NN) in terms of their frequency of being classified correctly over several repeats. One repeat corresponds to training the NN from scratch. Top 10% and bottom 10% of the samples with the highest and the lowest frequency define easy and hard instances for training. The authors also compare easy and hard examples across different architectures of NNs. <sep> On the positive side, all the reviewers acknowledge the potential usefulness of quantifying easy and hard examples in training NNs, and R1 was ready to improve his/her initial rating if the authors revisited the paper. <sep> On the other hand, all the reviewers and AC agreed that the paper requires (1) major improvement in presentation clarity -- see detailed comments of R1 on how to improve as well as comments/questions from R3 and R2; try to avoid confusing terminology such as 'contradicted patterns'. <sep> R1 raised important concerns that the proposed notion of easiness is drawn from the experiment in Fig. 1 of Arpit et al (2017) which is not properly attributed. R3 and R2 agreed that in its current state the experimental results are not conclusive and often non informative. To strengthen the paper the reviewers suggested to include more experiments in terms of different datasets, to propose a better metric for defining easy and hard samples (see R3's suggestions). <sep> We hope the reviews are useful for improving the paper."
"abstract | strength | weakness | rebuttal_process  ==> Recommendation: Weak reject <sep> Summary: <sep> The paper proposes a variant of deep reinforcement learning (A2MC) for environments with sparse rewards.  The approach replaces the standard environment reward function with a combination of the current reward and the variability of rewards in the last T timesteps, with a goal of decreasing variability.  The authors further propose a ""hot-wiring"" exploration strategy to bootstrap agents by taking either random actions or actions that have been done in the recent history.  Empirical evaluations in standard benchmarks including several sparse reward Atari games show empirical improvement of this approach over a baseline (ACKTR). <sep> Review: <sep> The paper has strong empirical results that show the A2MC outperforming or reaching the same performance as the baselines in a large number of Atari and MuJoCo domains.  The authors also provide results with and without the hot-wiring feature, which helps isolate its contribution.  However, overall the paper lacks theoretical rigor and most of the proposed changes are done without principled reasons or convergence guarantees.   There is no way of telling from the current paper whether these changes could lead to divergence or suboptimal behavior in other domains.  Examples of such changes include: <sep> * The averaging of the reward terms at different timescales in Equation 4 is the core of the algorithm but is derived ad-hoc.  Why is this a good equation?  Is lack of variability really a desired property and may it lead to a suboptimal policy?  Can anything be said about how it changes behavior in a tabular representation? <sep> * The exponential equation with a constant of 100 appears out of nowhere in equation 5.  Is this a general equation that will really work in different domains and reward scales? <sep> * The variability weights in equations 6 and 7 are never tested empirically – what happens if they are left out?  Where did this equation come from? <sep> * Overall, it is unclear if the combination of rewards at different time scales in equation 10 is stable and leads to convergence.  The terms show resemblance to the eligibility trace equations but lack their theoretical properties. <sep> To make the paper ready for publication, the authors need to justify which of these changes are ""safe"" in that they guarantee the behavior of the algorithm cannot become much worse, or need to point directly to other methods in the literature that have used such changes and cite the pros and cons that were seen with those changes. <sep> Related to the theme above, the paper does not properly cite other methods used with sparse rewards in traditional RL or Deep RL, especially eligibility traces, which seem highly related to the current approach.  The following related work edits are needed: <sep> * The overall approach is thematically similar to eligibility traces (see the standard Sutton and Barto textbook), except that the authors here use variability rather than combining the reward terms directly.  Eligibility traces are built exactly for these sorts of sparse reward problems and combine short and long-term rewards in a TD update. But there has been substantial investigation of their theoretical properties in the tabular and function approximation cases. The current method needs to compare and contrast to this long-standing method both theoretically and empirically. <sep> * Two other methods that should have been considered in the experiments are experience replay and reward shaping, both of which are beneficial with deep RL in sparse domains.  Experience replay is mentioned in the paper but not implemented as a competitor.  I realize ER is not as computationally efficient as the new approach but it is an important (and stable) baseline.  Reward shaping is not mentioned at all, but is again an important and stable baseline that has been used in such problems – see ""Playing FPS Games with Deep Reinforcement Learning"" (AAAI, 2017). <sep> * Finally, the related work section mentions a lot of competitive algorithms but does not implement any of them in comparison, which makes it hard to claim the current approach is the best yet proposed.","The paper proposes an interesting idea for efficient exploration of on-policy learning in sparse reward RL problems. The empirical results are promising, which is the main strength of the paper. On the other hand, reviewers generally feel that the proposed algorithm is rather ad hoc, sometimes with not-so-transparent algorithmic choices. As a result, it is really unclear whether the idea works only on the test problems, or applies to a broader set of problems. The author responses and new results are helpful and appreciated by all reviewers, but do not change the reviewers' concerns."
"abstract | strength | decision | suggestion  ==> The authors present an information discovery approach based on (partial) variational autoencoders and an information theoretic acquisition function that seeks to maximize the expected information gain over a set of unobserved variables. Results are presented on image inpainting, UCI datasets and health data, namely ICU and NHANES. <sep> It is not clear why multiple recurrent steps improve perfromance. This is not conceptually justified and empirically (see Figure 8), it is also unclear whether PNP5 significantly outperforms PNP1. Further, results seem to support that PNP is always better than PN, so why introduce the methodology around PN or even present it at all. Note that the authors do not offer an explanation about the perfromance differences between PN and PNP. <sep> In the inpainting regions section, the authors write about well-calibrated uncertainties without any context. What do they mean by calibration, well-calibrated and how can they support their claim about it? <sep> In Figure 3 it is not clear that PNP+Ours outperforms PNP+SING. For Boston hosing seems to be marginally better but the error bars (which I assume are standard deviations, not stated) make difficult to ascertain whether the differences are significant. Although I understand the value of having ""personalized"" decisions, one wonders whether this personalization comes with any generalizable measurable gains given the results. <sep> The results in Table 2 need to be clarified and further explained. 1) what are the error bars, considering multiple runs and datasets? 2) How can EDDI be so much better than SING when individual AUICs in Tables 6-11, the only significant difference (accounting for error bars) is on Boston data? 3) according to Tables 6-11, PNP is only the best in 1 of 5 datasets, so how come is the overall beast by a large margin? This being said, the results in Table 2 are at best misleading. <sep> In Table 4, how can PNP-EDDI be so much better than PNP-SING, when in Figure 6 error bars overlap almost everywhere? <sep> I enjoyed reading the paper, the motivation is clear and the problem is important. The approach is modestly novel compared to existing approaches and in general well explained despite the fact that the need for multiple recurrent steps is not well justified and the differences between PN and PNP, advantages/disadvantages and when to use each are not described or explored in the experiments.","This paper develops an active variable selection framework that couples a partial variational autoencoder capable of handling missing data with an information acquisition criteria derived from Bayesian experimental design. The paper is generally well written and the formulation appears to be natural, with a compelling real world healthcare application. The topic is relatively under-explored in deep learning and the paper appears to attempt to set a valuable baseline. However, the AC cannot recommend acceptance based on the fact that reviewer 2 has brought up concerns about the competitiveness of the approach relative to alternative methods reported in the experimental section, and all reviewers have found various parts of the paper to have room for improvement with regards to technical clarity. As such the paper would benefit from a revision and a stronger resubmission."
"abstract | weakness | rebuttal_process  ==>  ==> This paper describes a novel method to provide inference mapping for GAN networks. The idea is to reuse the discriminator network's feature vector (output of layer before last) and learn a direct mapping to the GAN's latent space. This can be done very efficiently since the dimensionality of both layers are relatively small. Also, the mapping does not interfere with the learning process of the GAN itself and thus can be applied on top of any GAN method without affecting its performance. <sep> Inference mapping is useful in the GAN context for several reasons that are well described in the paper. First it allows to more efficiently generate ""edited"" images as the mapping provides a good starting point in the latent space. Second it provides a sound way to evaluate GAN's performance as the reconstruction of a given image through the inference mapping and the generator provides auto-encoder-like capabilities. Comparison of GAN models have been difficult due to a lack of adequate evaluation technique. This paper proposes a novel evaluation scheme that is both fair and technically simple. <sep> In the experimental part, the authors first compare their approach to the 'naive encoder' approach where the last layer of the discriminator is removed after training, a feature layer of the size of the encoder's latent space is added, and the rest of the discriminator's layers are frozen. The proposed approach outperforms the naive encoder approach on the CelebA dataset. The second set of experiments investigates reconstruction accuracy of various GAN models. Figure 2 shows reconstructed images for 7 GANs and 36 examples from 3 datasets. Unfortunately, no subjective comparison can be attempted since the examples are different for each GAN. In Figure 3, editing in performed on the CelebA dataset, but again, subjective comparison among the GAN's is precluded by the fact that different examples are chosen. This oversight does not affect the paper's relevance, since those comparison would be purely subjective, however it would add some visual interpretation to the quantitative comparison given in table 1. I also wish the authors would have provided the inception score for FashMNIST and CelebA and also provide the more recent FID (Frechet Inception Distance). Inception scores are trained on ImageNET and are too commonly applied to CIFAR-10 and CelebA. It would be good to compare them against the proposed method on those datasets to show that there are not good for datasets other than those on which they were trained. <sep> The article is technically sound. The citations are adequate. The English is fine with some extraneous articles being the only issue. The article lacks a graphic for the architecture of the system and many of the figures are too small to interpret when printed out. Also there's a typo on table 1. where the inception score for WGAN-GP on CelebA should be 6.869 and not 0.6869. <sep> Overall, I find this paper provides a simple, novel significant method for evaluating GAN models and making better use of their latent space arithmetic editing capabilities. Due to the algorithm's simplicity, most of the paper is devoted to experiments and discussions.","The paper presents a method to learn inference mapping for GANs by reusing the learned discriminator's features and fitting a model over these features to reconstruct the original latent code z. R1 pointed out the connection to InfoGAN which the authors have addressed. R2 is concerned about limited novelty of the proposed method, which the AC agrees with, and lack of comparison to a related iGAN work by Zhu et al. (2016). The authors have provided the comparison in the revised version but the proposed method seems to be worse than iGAN in terms of the metrics used (PSNR and SSIM), though more efficient. The benefits of using the proposed metrics for evaluating GAN quality are also not established well, particularly in the context of other recent metrics such as FID and GILBO."
"abstract | weakness | strength | decision  ==> The paper considers the problem of obtaining reliable predictive uncertainty estimates. The authors propose noise contrastive priors — the idea being to explicitly encourage high uncertainties for out of distribution (OOD) data through a loss in the data space.  OOD data is simulated by adding noise to existing data and the model is trained to maximize the likelihood wr.t. training data while being close in the KL sense to a (wide) conditional prior p(y | x) on the OOD responses (y).  The authors demonstrate that the procedure leads to improved uncertainty estimates on toy data and can better drive active learning on a large flight delay dataset. <sep> The paper is well written and makes for a nice read. I like the idea of using ""pseudo"" OOD data for encouraging better behaved uncertainties away from the data. It is nice to see that even simple schemes for generating OOD data (adding iid noise) lead to improved uncertainty estimates. <sep> My main concern about this work stems from not knowing how sensitive the recovered uncertainties are to the OOD data generating mechanism and the parameters thereof. The paper provides little evidence to conclude one way or the other.  The detailed comments below further elaborate on this concern. <sep> Detailed Comments: <sep> a) I like the sensitivity analysis presented in Figure 4, and it does show for the 1D sine wave the method is reasonably robust to the choice of \\sigma_x. However, it is unclear how problem dependent the choice of sigma_x is. From the experiments, it seems that \\sigma_x needs to be carefully chosen for different problems, \\sigma^2_x < 0.3 seems to not work very well for BBB + NCP for the 1D sine data, but for the flight delay data \\sigma^2_x is set to 0.1 and seems to work well. How was \\sigma_x chosen for the different experiments? <sep> b) It is also interesting that noise with a shared scale is used for all 8 dimensions of the flight dataset. Is this choice mainly governed by convenience — easier to select one hyper-parameter rather than eight? <sep> c) Presumably, the predictive uncertainties are also strongly affected by both the weighting parameter \\gamma and the prior variance sigma^2_y . How sensitive are the uncertainties to these and how were these values chosen for the experiments presented in the paper? <sep> d) It would be really interesting to see how well the approach extends to data with more interesting correlations. For example, for image data would using standard data-augmentation techniques (affine transformations) for generating OOD data help over adding iid noise. In general, it would be good to have at least some empirical validation of the proposed approach on moderate-to-high dimensional data (such as images). <sep> ============== <sep> Overall this is an interesting paper that could be significantly strengthened by addressing the comments above and a more careful discussion of how the procedure for generating OOD data affects the corresponding uncertainties.","The paper studies the problem of uncertainty estimation of neural networks and proposes to use Bayesian approach with noice contrastive prior. <sep> The reviewers and AC note the potential weaknesses of experimental results: (1) lack of sufficient datasets with moderate-to-high dimensional inputs, (2) arguable choices of hyperparameters and (3) lack of direct evaluations, e.g., measuring network calibration is better than active learning. <sep> The paper is well written and potentially interesting. However, AC decided that the paper might not be ready to publish in the current form due to the weakness."
"strength | weakness | rebuttal_process | rating_summary  ==>  ==> The authors propose a new defense against adversarial examples based on radial basis features. Prior work has suggested that the linearity of standard convolutional networks may be a factor contributing to their vulnerability against adversarial examples, and that radial basis functions may help alleviate this weakness. The current paper builds on this idea and proposes a concrete way to add radial basis features to existing convnet architectures. <sep> In the proposed approach, each layer of the network is augmented with a radial basis feature transform of the features in this layer. The output of this feature transform is then concatenated with the features in this layer. The centers of the radial basis features, the bandwidth, and the distance matrix are trained with the other network parameters. The distance matrix in the feature transform is used to compute the Mahalanobis distance between the features and centers in the radial basis functions. <sep> The authors evaluate their defense experimentally on the standard MNIST dataset and two medical image datasets: X-chest14 for classification and a 2D RGB skin lesion dataset from the 2017 IEEE ISBI International Skin Imaging Collaboration (ISIC) Challenge for image segmentation. The experiments show that their method improves over an undefended network on MNIST. On X-chest14, their method improves over features squeezing (input quantization) and Gaussian data augmentation. On the image segmentation dataset, the method improves over these baselines as well as adversarial training. <sep> While I find the overall idea interesting, I have some doubts about the experimental evaluation. For instance, the authors do not compare their MNIST numbers to the robust optimization results reported in Madry et al. (cited in the paper). Robust optimization achieves higher adversarial accuracy than the numbers reported in Table 1. <sep> More importantly, it is unclear to what extent unmodified first-order methods are effective for the proposed defense. While the authors investigate whether their networks exhibit gradient masking / obfuscation, the left plot in Figure 3 still leaves some questions. Based on the curves for FGSM and BIM, the proposed defense would still achieve a high accuracy even against attacks with eps = 0.5. However, this would be a clear failure of the first order attacks (but not a sign of true robustness) because an adversary with eps = 0.5 can trivially defend any network by setting every input pixel to 0.5. Hence the authors should investigate what happens in the regime between eps = 0.4 and eps = 0.5. <sep> While I support the use of non-standard datasets for evaluation, it would still strengthen the paper if the author also reported accuracy numbers on CIFAR-10. The X-chest14 and the segmentation dataset have not been frequently used in the adversarial robustness literature to the best of my knowledge. Hence it is less clear how well the proposed methods perform on these datasets. <sep> While I find the overall idea interesting, with the current experimental evaluation I unfortunately cannot recommend accepting the paper. <sep> Further comments: <sep> - The distinction between ""data-level"" and ""algorithmic-level"" approaches in the introduction is unclear to me. Adversarial training can also be seen as robust optimization, which is arguably an algorithmic approach. <sep> - At the beginning of Section 2, it would be helpful if the authors first introduced the meaning of the variables n, m, and k before using them. In general, it would be helpful if the authors described in more detail how the radial basis features are incorporated into the network. <sep> - How is the adversarial training baseline in Section 4.2 implemented? The choice of adversary in adversarial training / robust optimization can be crucial for the robustness of the resulting model. <sep> - Since the authors refer to the linearity of existing model as a potential weakness: there are also alternative explanations, e.g., see https://arxiv.org/abs/1801.02774 and https://arxiv.org/abs/1804.11285 . <sep> - The test sets used in the evaluation are fairly small (150 and 200 data points). In this regime, 95% confidence intervals can be as large as +/- 8%. Hence I would recommend increasing the size of the test sets to at least 1,000.","Strengths of the paper: <sep> Based on previous work suggesting that radial basis features can help defend against adversarial attacks, the paper proposes a concrete method for incorporating them in deep networks. The paper evaluates the method on multiple datasets, including MNIST and ISBI International Skin Imaging Collaboration (ISIC) Challenge. <sep> Weaknesses: <sep> Reviewers 2 and 3 felt that the paper was not clearly written, and cited several concrete questions about the method that could not be understood from the paper. There were additional concerns of lacking comparison to existing methods, and Reviewer 1 pointed out that a competing method gave higher performance, although this was not reported in the present submission. <sep> Points of contention: <sep> The authors did not provide a response to the reviewer concerns. <sep> Consensus: <sep> All reviewers recommended that the paper be rejected, and the authors did not provide a rebuttal."
"strength | weakness | decision  ==> The paper proposes Adaptive Sample-space & Adaptive Probability (ASAP) coding for image compression based on neural networks. In contrast to most prior methods, which adhere to a fixed quantization scheme (i.e. with fixed number of quantization levels, and fixing the level themselves), the proposed method jointly learns a probability model of the quantized representation (the bottleneck of an autoencoder model) for coding and a corresponding adaptive quantization scheme. The distribution of each entry in the bottleneck before quantization is modeled as a Gaussian, whose mean and variance are predicted by a neural network conditionally on bottleneck entries on a grid at different scales (similar as in Nakanishi et al. 2018). The same network also predicts quantization intervals to adaptively quantize the respective entry of the bottleneck. Together, the predicted means, variances, and quantization intervals are used to obtain an estimate of the code length. The proposed compression networks are trained with a novel multiplicative loss, showing clear improvements over prior methods Rippel & Bourdev 2017, Nakanishi et al. 2018 on the Kodak and Raise1k data sets in terms of MS-SSIM. <sep> Pros: <sep> The results presented in this paper seem to be state-of-the-art, and innovation on quantization, which has not attracted a lot of attention in the context of neural network-based image compression is a welcome contribution. The method also seems to outperform the recent method [1], which should be included for comparison. <sep> Questions: <sep> A major issue is, however, that it is unclear from the results whether the gains are due to the novel quantization system, or due to the novel loss. From Fig. 7 it looks like the loss BPP + \\lambda (1-MS-SSIM) (assuming the formula in (6)) is correct, and the legend in Fig. 7 incorrect) that is used in most other works performs essentially on par with  Rippel & Bourdev 2017, Nakanishi et al. 2018. For example at 1 bpp, this loss yields an MS-SSIM of 0.992 which is essentially the same as Rippel & Bourdev 2017, Nakanishi et al. 2018 obtain, cf. Fig. 2. To show that the improvement is due to the learned quantization and not just because of the loss (8) an ablation experiment should be done. One could e.g. train the proposed method with the same predictor, but without the employing the learned quantization scheme, and compare to the results obtained for the proposed method. <sep> Furthermore, a better understanding of the loss (8) would be desirable. How is the MSE factor justified? <sep> Also, it would be good to present visual examples at rates 0.1-1.0 bpp. All visual examples are at rates below 0.08 bpp, and the proposed method is shown to also outperform other methods at much higher rates. <sep> [1] Ballé, J., Minnen, D., Singh, S., Hwang, S.J. and Johnston, N. Variational image compression with a scale hyperprior. *CONF* 2018.","This paper presents an interesting approach to image compression, as recognized by all reviewers. However, important concerns about evaluating the contribution remains: as noted by reviewers, evaluating the contribution requires disentangling what part of the improvement is due to the proposed approach and what part is due to the loss chosen and evaluation methods. While authors have done a valuable effort adding experiments to incorporate reviewers suggestions with ablation studies, it does not convincingly show that the proposed approach truly improves over existing ones like Balle et al. Authors are encouraged to strengthen their work for future submission by putting particular emphasis on those questions."
"abstract | strength | weakness  ==>  ==> Summary: <sep> The authors present a video prediction model called SAVP that combines a Variational Auto-Encoder (VAE) model with a Generative Adversarial Network (GAN) to produce more realistic and diverse future samples. <sep> Deterministic models and certain loss functions such as Mean Squared Error (MSE) will produce blurry results when making uncertain predictions. GAN predictions on the other hand usually are more visually appealing but often lack diversity, producing just a few modes. The authors propose to combine a VAE model with a GAN objective to combine their strengths: good quality samples (GAN) that cover multiple possible futures (VAE). <sep> Strengths: <sep> [+] GANs are notoriously unstable to train, especially for video. The authors formulate a VAE-GAN model and successfully implement it. <sep> Weaknesses: <sep> [-] The combination of VAEs and GANs, while new for videos, had already been proposed for image generation as indicated in the Related Work section and its formulation for video prediction is relatively straightforward given existing VAE (Denton & Fergus 2018) and GAN models (Tulyakov et al. 2018). <sep> [-] The results indicate that SAVP offers a trade-off between the properties of GANs and VAEs, but does not go beyond its individual parts. For example, the experiment of Figure 5 does not show SAVP being significantly more diverse than GANs for KTH (as compared to VAEs). Furthermore, Figure 6 and Figure 7 in general show SAVP performing worse than SVG (Denton & Fergus 2018), a VAE model with a significantly less complex generator, including for the metric (VGG cosine similarity) that the authors introduce arguing that PSNR and SSIM do not necessarily indicate prediction quality. <sep> While the use of a GAN in general will make the results less blurry and visually appealing, it does not necessarily mean that the samples it generates are going to be plausible or better. Since a direct application of video prediction is model-based planning, it seems that plausibility might be as important as sample quality. This work proposes to combine VAEs and GANs in a single model to get the benefits of both models. However, the experiments conducted generally show that SAVP offers only a trade-off between the visual quality of GANs and the coverage of VAEs, and does not show a clear advantage over current VAE models (Denton & Fergus, 2018) that with simpler architectures obtain similar results. While the presentation is clear and the evaluation of the model is thorough, I am unsure of the significance of the proposed method. <sep> In order to better assess this model and compare it to its individual parts and other VAE models, could the authors: <sep> 1) Compare SAVP to the SVG-LP/FP model on a controlled synthetic dataset such as Stochastic Moving MNIST (Denton & Fergus, 2018)? <sep> 2) Comment on the plausibility of the samples generated by SAVP? Do some samples show imagined objects – implausible interactions for the robotic arm dataset? If so, what would be the advantage over blurry but plausible generations of a VAE?","This paper shows that combining GAN and VAE for video prediction allows to trade off diversity and realism. The paper is well-written and the experimentation is careful, as noted by reviewers. However, reviewers agree that this combination is of limited novelty (having been used for images before). Reviewers also note that the empirical performance is not very much stronger than baselines. Overall, the novelty is too slight and the empirical results are not strong enough compared to baselines to justify acceptance based solely on empirical results."
"abstract | weakness | decision  ==> Thanks for the rebuttal. But, I am still not very convinced with the proposed results. For CIFAR-100 (0%), you get about 0.2% gain, for ImageNet (0%), you get about 0.2% loss in top-5 accuracy, and for WebVision, you get about 0.3% gain. I am not sure whether you can call these as statistically significant gains. I believe such gain/loss can be obtained with many other tweaks, such as the learning rate scheduling, as the authors have done. <sep> I believe extensive testing the proposed method on many real noisy datasets, not the synthetically generated ones, and showing the consistent gains would much strengthen the paper. But, at the current version, the only such result is Table 5, which is, again, not very convincing to me. <sep> So, I still keep my rating. <sep> ======= <sep> Summary: <sep> The authors propose a simple empirical method for cleaning the dataset for training. By using the implicit regularization property of SGD-based optimization method, the authors come up with a method of setting a threshold for the training loss statistics such that the examples that show losses above the threshold are regarded as noisy examples and are discarded. Their empirical results show that ODD (their method) can outperform other baselines when artificial random label noise is injected. They also show ablation studies on the hyperparameters and show the final result seems to be robust to those parameters. <sep> Pros: <sep> - The method is very simple <sep> - The empirical results, particularly on the synthetic noisy training data, seems to be encouraging. <sep> - The ablation study argues that the method is robust to the hyperparameters, p, E, and h. <sep> Cons: <sep> - I think the results remains to be highly empirical. While it is interesting to see the division of the loss statistics in Figure 2, I am not very convinced about the real usage of the proposed method. The result in Table 5 shows that ODD can outperform ERM for real world datasets, but the improvement seems to be marginal. Moreover, the hyperparameter p was set to 30 for that experiment, but how did the authors choose that parameter? Clearly, if you choose wrong p, I think the performance will degrade, and it is not clear how you can choose p in real applications. The ablation studies are only with synthetic noisy label data, so I think the result is somewhat limited. <sep> - <sep> I think the paper shows interesting results, but my concern is that it seems to be quite empirical. The positive results are particularly on the synthetic data case.","The paper aims to clean data samples with label noise in the training procedure. <sep> The reviewers and AC note the following potential weaknesses: (1) the assumption of uniform noise, which is not the case in practice, (2) marginal gains under real-world datasets and (3) highly empirical and ad-hoc approach. <sep> AC thinks the proposed method has potential and is interesting, but decided that the authors need more significant works to publish the work."
"abstract | misc | weakness  ==>  ==> Pros: <sep> The paper shows that we could have a better document/sentence embedding by partitioning the word embedding space based on a topic model and summing the embedding within each partition. The writing and presentation of the paper are clear. The method is simple, intuitive, and the experiments show that this type of method seems to achieve state-of-the-art results on predicting semantic similarity between sentences, especially for longer sentences. <sep> Cons: <sep> The main concern is the novelty of this work. The method is very similar to SCDV (Mekala et al., 2017). The high-level flow figure in appendix H is nearly identical as the Figure 1 and 2 in Mekala et al., 2017. The main difference seems to be that this paper advocates K-SVD (extensively studies in Arora et al. 2016) as their topic model and SCDV (Mekala et al., 2017) uses GMM. <sep> However, in the semantic similarity experiments (STS12-16 and Twitter15), the results actually use GMM. So I suppose the results tell us that we can achieve state-of-the-art performances if you directly combine tricks in SIF (Arora et al., 2017) and tricks in SCDV (Mekala et al., 2017). <sep> In the document classification experiment, the improvement looks small and the baselines are not strong enough. The proposed method should be compared with other strong unsupervised baselines such as ELMo [1] and p-mean [2]. <sep> Overall: <sep> The direction this paper explores is promising but the contributions in this paper seem to be incremental. I suggest the authors to try either of the following extensions to strengthen the future version of this work. <sep> 1. In addition to documentation classification, show that the embedding is better than the more recent proposed strong baselines like ELMo in various downstream tasks. <sep> 2. Derive some theories. One possible direction is that I guess the measuring the document similarity based on proposed embedding could be viewed as an approximation of Wasserstein similarity between the all the words in both documents. The matching step in Wasserstein is similar to the pooling step in your topic model. You might be able to say something about how good this approximation is. Some theoretical work about doing the nearest neighbor search based on vector quantization might be helpful in this direction. <sep> Minor questions: <sep> 1. I think another common approach in sparse coding is just to apply L1 penalty to encourage sparsity. Does this K-SVD optimization better than this L1 penalty approach? <sep> 2. How does the value k in K-SVD affect the performances? <sep> 3. In Aorora et al. 2016b, they constrain alpha to be non-negative. Did you do the same thing here? <sep> 4. How important this topic modeling is? If you just randomly group words and sum the embedding in the group, is that helpful? <sep> 5. In Figure 2, I would also like to see another curve of performance gain on the sentences with different lengths using K-SVD rather than GMM. <sep> Minor writing suggestions: <sep> 1. In the 4th paragraph of section 3, ""shown in equation equation 2"", and bit-wise should be element-wise <sep> 2. In the 4th paragraph of section 4, I think the citation after alternating minimization should be Arora et al. 2016b and Aharon et al. 2006 rather than Arora et al., 2016a <sep> 3. In the 2nd paragraph of section 6.1, (Jeffrey Pennington, 2014) should be (Pennington et al., 2014). In addition, the author order in the corresponding Glove citation in the reference section is incorrect. The correct order should be Jeffrey Pennington, Richard Socher, Christopher D. Manning. <sep> 4. In the 3rd paragraph of section 6.1, ""Furthermore, Sentence"" <sep> 5. In the 6th paragraph of section 6.1, I thought skip-thoughts and Sent2Vec are unsupervised methods. <sep> 6. In Table 2 and 3, it would be easier to read if the table is transposed and use the longer name for each method (e.g., use skip-thought rather than ST) <sep> 7. In Table 2,3,4,5, it would be better to show the dimensions of embedding for each method <sep> 8. Table 10 should also provide F1 <sep> 9. Which version of GMM is used in STS experiment? The one using full or diagonal covariance matrix? <sep> [1] Peters, M. E., Neumann, M., Iyyer, M., Gardner, M., Clark, C., Lee, K., & Zettlemoyer, L. (2018). Deep contextualized word representations. NAACL <sep> [2] Rücklé, A., Eger, S., Peyrard, M., & Gurevych, I. (2018). Concatenated p-mean Word Embeddings as Universal Cross-Lingual Sentence Representations. arXiv preprint arXiv:1803.01400.",This paper proposes a document classification algorithm based on partitioned word vector averaging. <sep> I agree with even the most positive reviewer. More experiments would be good. This is a very developed old area.
"rating_summary  ==> Pros <sep> 1. The paper is fairly clear. <sep> 2. The problem is important: analyzing the internal computations of layered networks. <sep> 3. The method seems to be a slight improvement on an existing method: the use of hierarchical clustering is nice. <sep> 4. Figs. 3 and 5 superimposing the analyzed clusters on top of the network diagram are cool. <sep> Cons <sep> 5. The paper wastes valuable space writing out in detail the equations for backpropagation in a standard feed-forward MLP. <sep> 6. The paper does not have an acceptable review of relevant prior work. This is particularly problematic as the proposal seems to be a rather small tweak to prior work of two 2018 papers by Watanabe et al. But there is extensive other literature attempting to address this problem, especially in the vision domain, where their main example - poor over-worked MNIST - resides. <sep> 7. In my view, attempts to understand processing in NNs exclusively at the individual-unit level are essentially doomed at the outset. These networks crucially represent their information in distributed representations and it is joint action by multiple units rather than action by individual units that drives processing. Consider the ""effect"" variable analyzed in this paper, which is a simple correlation between the activity of a target hidden unit and the activity of a particular input or output unit. Suppose whenever hidden unit i is active, hidden unit j is also active, and vice versa. Now suppose j strongly drives output unit k via a connection with a large weight, while unit i has no connection at all to unit k. Then i will have a strong ""effect"" on k! The correlations between the activity of i and k is the same as the correlation between j and k, even though the causal interaction between i and k is nil, while the causal interaction between j and k is strong. In this especially transparent situation, it is the joint action of i and j that matters, and it so happens that this joint action has no contribution from i. <sep> 8. So in addition to the problems arising from analyzing exclusively at the individual-unit level, there is the problem of defining ""effect"" by correlation instead of causation. <sep> 9. I don't myself gain any insight into how the MNIST network is working by looking at the clusters diagrammed in Fig. 4. There is no discussion of the fact that nearly all of their input-""effect"" maps look like a slanted oval which is either on-center-off-surround or the reverse (no comment on the superficial, at least, connection to the receptive fields of neurons in the early mammalian visual system). Just how do these cluster maps explain anything? <sep> 10. The maps for the other example, time-series of prices of root vegetables, are even more baffling, but, superficially at least, the input maps suggest the hidden units are doing Fourier analysis; even this obvious observation is not made in the paper, however.","All reviewers agree to reject. While there were many positive points to this work, reviewers believed that it was not yet ready for acceptance."
"abstract | weakness | decision | suggestion  ==> The authors explore the possibility of using an end-to-end approach for predicting pharmacological assay outcome using fluorescence microscopy images from the public Cell Painting dataset. In my view, the primary contributions are the following: an interesting and relatively new application (predicting assay outcomes), enriching the CellPainting dataset with drug activity data, and a comparison of several relevant methods and architectures. The technical novelty is weak, and although the authors demonstrate that end-to-end holistic approaches outperform previous segmentation-and-feature-extraction approaches, this result is not surprising and has been previously reported in closely related contexts. <sep> OVERVIEW <sep> The authors evaluate the possibility of using and end-to-end deep learning approach to predict drug activity using only image data as input. The authors repurpose the CellPainting dataset for activity prediction by adding activity data from online ChEMBL databases. If made available as promised, the dataset will be a valuable resource to the community. The authors compare a number of previous approaches and state-of-the-art image classification network architectures to evaluate the use of CNNs instead of more classical image analysis pipelines. The comparison is a strong point of the paper, although some details are lacking. For example, the authors claim that GapNet is the quickest method to train, and while they report the number of hyperparameters and time per epoch, the number of epochs trained is never mentioned. <sep> The authors propose an architecture (GapNet) for the assay prediction task. While the way Global Average Pooling is used to extract features at different stages in the network might be new, it is a straightforward combination of GAP and skip connections. Little insight into why this approach is more efficient or evidence for its effectiveness is provided. Similarly, more explanation for why dilated convolutions and SELU activations would be appreciated. A comparison between GapNet and the same network without the GAP connections could possibly provide a more interesting comparison and might also provide a more pervasive argument as to why GapNet's should be used. Ultimately, the benefit of using GapNet over the other architectures is not strongly motivated, as training time is less of a concern in this application than predictive power. <sep> RELATED WORK <sep> The authors present previous work in a clear and comprehensive manner. However, the reported finding that ""CNNs operating on full images containing hundreds of cells can perform significantly better at assay prediction than networks operating on a single-cell level"" is not surprising, and partial evidence of this can be found in the literature. In [1], it was shown that penultimate feature activations from pre-trained CNNs applied to whole-image fluorescence microscopy data (MOA prediction) outperform the baseline segmentation-then-feature extraction method (FNN). Similarly, in [2] (the paper proposing MIL-Net), it is shown that end-to-end whole-image CNN learning for protein localization outperforms the baseline (FNN). In [3] whole image end-to-end learning outperforms whole image extracted features for a phenotyping task. All of these references use fluorescence microscopy data similar to the dataset in this work. <sep> [1] Pawlowski, Nick, et al. ""Automating morphological profiling with generic deep convolutional networks."" bioRxiv (2016): 085118. <sep> [2] Kraus, Oren Z., Jimmy Lei Ba, and Brendan J. Frey. ""Classifying and segmenting microscopy images with deep multiple instance learning."" Bioinformatics 32.12 (2016): i52-i59 <sep> [3] Godinez, William J., et al. ""A multi-scale convolutional neural network for phenotyping high-content cellular images."" Bioinformatics 33.13 (2017): 2010-2019. <sep> APPROACH <sep> The authors compile enrich the CellPaining dataset with activity data from various drug discovery assays. In my view, the creation of this dataset is the strongest and most valuable contribution of the paper. The method used to collect the data is described clearly and the choices made when compiling the dataset, including the thresholds and combinations of activity measures seems like a well founded approach. <sep> The authors then identify a number of approaches that are relevant for the problem at hand, binary prediction of drug activity based on image data. These include previous approaches used for cell images and modern image classification networks. <sep> EXPERIMENTS <sep> The different approaches/networks mentioned above were evaluated on a testset. The results indicate that end-to-end CNN approaches outperform all non-end-to-end with no significant difference between the individual end-to-end CNNs. The results are stated clearly and the presentation of different metrics is a nice addition to properly compare the results. It would however contribute valuable information if the authors stated how the confidence intervals of the F1 score are calculated (are the experiments based on several runs of each network or how is it done). <sep> NOVELTY/IMPACT <sep> + Creation of a new dataset on a new and interesting problem <sep> + Useful comparison of modern networks on the task <sep> - GapNet - lacking technical novelty, insight, and performance is unconvincing <sep> - Demonstrates that end-to-end learning outperforms cell centric approach - was this really surprising or even new information? <sep> OTHER NOTES: <sep> * Figure 3 is never mentioned in the main text <sep> * Figure 3 (*'s) are confusing. Do they represent outliers? Statistical significance tests? <sep> * Figure 5 which panel is which? <sep> * Be clear what you mean when you refer to ""upper layers"" of a network <sep> * An important point not mentioned: in practice, many assays use stains that are closely tied to the readout, unlike the dataset here which provides only landmark stains. The results found here do not necessarily apply in other cases.","This work studies the performance of several end-to-end CNN architectures for the prediction of biomedical assays in microscopy images. One of the architectures, GAPnet, is a minor modification of existing global average pooling (GAP) networks, involving skip connections and concatenations. The technical novelties are low, as outlined by several reviewers and confirmed by the authors, as most of the value of the work lies in the empirical evaluation of existing methods, or minor variants thereof. <sep> Given the low technical novelty and reviewer consensus, recommend reject, however area chair recognizes that the discovered utility may be of value for the biomedical community. Authors are encouraged to use reviewer feedback to improve the work, and submit to a biomedical imaging venue for dissemination to the appropriate communities."
"rating_summary | decision  ==>  ==> -- Paper summary -- <sep> The primary goal of this paper is to investigate the suitability of BNNs for carrying out post-calibration on trained deep learning models. The results are compared to equivalent models calibrated using temperature scaling, and the proposed technique is shown to yield superior uncertainty calibration. <sep> -- General Commentary -- <sep> The overall goal of this work is rather modest and the scope of the evaluation is limited. While not without challenges, carrying out offline calibration as a corrective measure is a simpler problem to tackle than developing well-calibrated models upfront, and limiting the comparison to just one other post-calibration method greatly narrows the overall vision such a paper should have. For instance, isn't post-calibration more likely to result in overfitting than a model that is implicitly calibrated at training time? <sep> I have plenty of concerns with the submission itself, listed below: <sep> - First and foremost, the paper is full of typos and grammatical errors. I genuinely struggled to read the paper end-to-end without being continually distracted by these issues. While some mistakes may indeed be genuine, others are only there due to sheer negligence and because the authors didn't properly check the paper before submission. <sep> - While the overall objective of this work (i.e. improving calibration of deep models) is clearly established, the overall presentation of ideas is very muddled and I initially struggled to properly understand what's being proposed.  A simple diagram or illustration would have clarified some of the notation at the very least. <sep> - The sloppiness in the presentation is also manifested in other ways. For example, in Figure 1, the plots should be individually titled ('uncalibrated', 'temp-scal' and 'BNN') in order to immediately distinguish between them; instead, all this information is contained in the caption whereas it could just as easily have been added to the plot. <sep> - As alluded to earlier, I am disappointed by the lack of scope in the paper. The experimental evaluation should have been widened to include direct comparisons against BNN models which one might expect to be slightly better-calibrated upfront. There has also been significant interest in improving the calibration of deep models by stacking different architectures in such a way that the model is implicitly calibrated at training time. Examples of such papers include 'Adversarial Examples, Uncertainty, and Transfer Testing Robustness in Gaussian Process Hybrid Deep Networks' (Bradshaw et al, 2017), and 'Calibrating Deep Convolutional Gaussian Processes' (Tran et al, 2018). The deep kernel learning schemes developed by Wilson et al. also discuss similar hybrid models.  <sep> - With reference to the papers cited above, one possible extension the authors could consider is to use a Gaussian process for post-calibration instead of a BNN, although I suspect this may have already been investigated in the past. In any case, this warrants further discussion.  <sep> - I can't disentangle the two contributions listed at the bottom of Pg 2 and the top of Pg 3. There is no theoretical evaluation of the 'alternative hypothesis' being mentioned, and the investigation is entirely limited to the offline setting, so I'm not entirely sure what distinction the authors are trying to make here.  <sep> - In the same section, the authors then remark that 'Our results open new perspectives to improve the variational approximation…' and 'we believe our results might foster further research in…', before proceeding to list a dozen or so papers which might be inspired by this work. However, I can't really see how the single contribution being presented in this paper can have significant impact on the related work. I encourage the authors to substantiate their claims with more concrete examples rather than simply include vague mentions of other papers.  <sep> - The structure and content of Section 3 is quite perplexing. Effectively, up until Equation 3, the authors are simply restating how to use VI for BNNs, with no mention whatsoever of how this fits in the storyline of model calibration. Whereas such a section should have contained novel methodology and/or intuition, the only reference to using BNNs for post-calibration is found in a small paragraph at the end of Pg 4, before immediately proceeding to the Experiments section. Once again, this makes any contributions of the paper unclear and inconclusive. Spurious comments such as the inconsequential connection to MDL further accentuate the paper's lack of identity and focus. <sep> - There are also some problematic technical details in this section, such as the definitive choice of using a two-layered BNN with no justification whatsoever. It is well known than plain BNNs also struggle to deliver well-calibrated outputs, and yet the authors immediately settle on a two-layered fully-connected network without stopping to consider whether some other network configuration or initialisation scheme might be more appropriate. Some introspection is later given in the experiment accompanied by Figure 2, but the analysis carried out there is just not sufficient.  <sep> - There are some instances where the authors use text while in math mode, which gives poor formatting as exemplified by 'conf' in Equation 4.  <sep> - Referring to 'datasets' as 'databases' in Section 4.1 is unusual. Some of the commentary in this subsection is also very difficult to interpret. For example, what is meant by 'uses BNNs'? Does this mean that a BNN appears in the model being calibrated or is this referring to the BNN used to carry out calibration? The majority of these ambiguous statements could have been avoided had more care been given to checking the paper properly before submission. <sep> - In their discussion of the results, the authors state that 'We cannot conclude that BNNs are calibrating at the cost of losing accuracy', which I consider to be an overly sunny view of the results. Even if minor, a dip in accuracy is observed in almost every example provided in the Experiments section, dropping as much as 3% for CIFAR-100. Given that calibration is the primary focus of this paper, it might also be worth including another metic for validating this criteria, such as the Brier score. <sep> -- Recommendation -- <sep> Unfortunately, the material presented here is neither significant enough nor sufficiently explored to spark much interest. The overall scope of the paper is disappointingly limited, while novel ideas and design choices are poorly motivated and communicated throughout. This submission feels rushed and incomplete, and consequently well below the conference's standards. <sep> Pros/Cons summary: <sep> +  The proposal yields good results in the provided experiments <sep> -   Minor contributions that are not convincing enough <sep> -   Muddled presentation of ideas <sep> -   Dubious or weakly motivated design choices <sep> -   Poorly written with plenty of typos <sep> -   Difficult to follow",Reviewers are in a consensus and recommended to reject after engaging with the authors. Please take reviewers' comments into consideration to improve your submission should you decide to resubmit.
"abstract | weakness | rebuttal_process | weakness | decision  ==>  ==> This paper motivates itself by observing that not all convolutional weights are required to make an accurate prediction. In the introduction the authors envision a system similar to a cascaded classifier [Viola and Jones 2001] (I draw this conclusion not the paper). However the wording of the introduction is not clear or it does not align with what is presented in the paper. <sep> The approach in the paper does not perform early stopping dynamically during the feedforward phase. The approach removes weights which do not impact the accuracy after training has completed and the fine tunes the resulting network. <sep> The clarity of the introduction must be addressed however the work is still interesting. I recommend the authors try to make the introduction as accessible as possible. <sep> Also there are very general statements like ""The activation layer introduces non-linearity into the system for obtaining better accuracy."" which do not contribute to the message of the paper. The paper will be fine without these statements. Shorter is better. <sep> Section 3 is good as a motivating example. However the conclusion ""Thus, our focus is to develop an effective method of choosing a good intermediate result for saving more MAC operations with less accuracy drop"" is not very clear. More insights written hear would be better. <sep> One major flaw is that no analysis with respect to time of computation was performed. GPUs offer the advantage of being optimized for convolutions so it is possible that there is no speedup. Because of this it is unclear if the method would save time. The results clearly show individual computations (MACs) are reduced but it is not clear how this correlates with wall clock time. <sep> Why do you start with the centre layers? I understand the heuristic you're using, that the middle layers won't have high or low-level features, and that they won't break the other layers as badly if you modify them, but I feel like this is core to your method and it's not adequately justified. I'd like to see some experiments on that and whether it actually matters to the outcome. Also, you don't say if you start at the middle then go up a layer, or down a layer. I think this matters since your proposed C10-Net has only 3 convolutional layers. <sep> All the filters in the same layer share a common checkpoint. Is that a good idea? What is the cost of doing this on a per-filter level? What is the cost on a per-layer level? Discussing runtime estimates for the added computation at training would make it more clear what the cost of the method is. <sep> In section 4.4.3. you mention that the majority of weight distributions in CNNs follow the Gaussian manner. Could you cite something of this? You might also want to move that in Step 1 (section 4.1.), since it seems to be your motivation for selection of checkpoint locations (5% and 32%) and I had no idea why you selected those values at that point. <sep> Typos: <sep> Typo on page 3: ""exploits redundancies inter feature maps to prune filters and feature maps"" <sep> Structural: <sep> Maybe section 4.3 should be part of the description of Section 4. Proposed Method, not its own subsection. <sep> Maybe table 2 should go at the end of section 4.4.1, because you describe it the error and columns in section 4.4.1.","This paper proposes a new method for speeding up convolutional neural networks. It uses the idea of early terminating the computation of convolutional layers. It saves FLOPs, but the reviewers raised a critical concern that it doesn't save wall-clock time. The time overhead is about 4 or 5 times of the original model. There is not any reduced execution time but much longer. The authors agreed that ""the overhead on the inference time is certainly an issue of our method"". The work is not mature and practical. recommend for rejection."
"abstract | strength | weakness | suggestion  ==>  ==> The paper proposes an approach to construct conformal prediction sets. The idea is to estimate p(x|y) and then construct a conformal set based on the p(x|y) instead of p(y|x). The paper claims that such a method produces cautious classifiers that can produce ""I don't know"" answers in the face of uncertainty. <sep> However, <sep> A] Although the paper is titled is ""Cautious Deep Learning"", the method is broadly applicable to any classifier, there is nothing in the method that restricts it to the domain of deep learning. A broad spectrum evaluation could have been done on standard multi-class classifiers. <sep> B] The paper provides multiple qualitative evaluation results. While it gets the point across, I still would have liked to see a quantitative evaluation, for e.g., there have been several papers that proposed generating adversarial examples for deep learning. The author could have taken any of those methods, generated adversarial examples for deep learning and compared the original classifier with the conformal prediction set. Also, such comparison would have made the paper more connected with deep learning. <sep> C] The paper uses Xception network as a feature extractor and then compares its result with Inception-v4. Honestly, I would have preferred if the comparison was between 1] Xception Feature Extractor + Conformal Set Prediction, 2] Xception network prediction, and 3] Inception-v4. The reason being that it is very difficult to understand how much of the cautious-ness is because of the proposed approach and how much is due to the Xception network being good. For example, in Figure 3b, does Xception network generate high probability values for the top classes or does it generate low probability values? Unless we can understand this difference, it is very difficult to appreciate what this approach is giving us. <sep> D] Another analysis that could have been done is to apply this approach and use several different pre-trained networks as feature extractor and check whether there is a decrease in false positives across all the networks, that would suggest that the method can truly make deep learning cautious across a wide variety of networks. <sep> E] Another analysis that could have been done is understand the impact of the quality of feature extractor. For example, take a deep network (of sufficient depth) and use the proposed approach but instead of using just the penultimate layer for feature extraction, one can keep on removing layers from the end and use the remaining as the feature extractor. Then analyze the quality of conformal predictions as each layer gets removed. One can understand the robustness of this method. <sep> Even though doing all these evaluations may be a tad too much, but definitely, quite a few of those could have been done to make the approach look convincing and enticing. I think bulking this paper with such analysis could make for a very good submission. However, as it stands, it still quite lacks.","The paper presents a conformal prediction approach to supervised classification, with the goal of reducing the overconfidence of standard soft-max learning techniques. The proposal is based on previously published methods, which are extended for use with deep learning predictors. Empirical evaluation suggests the proposal results in competitive performance. This work seems to be timely, and the topic is of interest to the community. <sep> The reviewers and AC opinions were mixed, with reviewers either being unconvinced about the novelty of the proposed work or expressing issues about the strength of the empirical evidence supporting the claims. Additional experiments would significantly strengthen this submission."
"abstract | weakness  ==> This paper proposed another GAN-based PU learning method. The mathematics in this paper is not easy to follow, and there are many other critical issues. <sep> ***** <sep> The clarity is really an issue. First of all, I cannot easily follow the meanings behind the equations. I guess the authors first came up with some concrete implementation and then formalize it into an algorithm. Given the current version of the paper, I am not sure whether this clarity of equations can be fixed without an additional round of review or not. <sep> Moreover, the logic in the story line is unclear to me, especially the 3rd paragraph that seems to be mostly important in the introduction. There are two different binary classification problems, of separating the positive and negative classes, and of separating the given and generated data. I cannot see why the generated data can serve as negative data. This paragraph is discussing GenPU, PGAN and the proposed method, and consequently the motivation of the current paper does not make sense at least to me. <sep> ***** <sep> The paper classified PU learning methods into two categories, one-stage methods and two-stage methods. This is interesting. However, before that, they should be classified into two categories, for censoring PU learning and for case-control PU learning. The former problem setting was proposed very early and formalized in ""learning classifiers from only positive and unlabeled data"", KDD 2008; the latter problem setting was proposed in ""presence-only data and the EM algorithm"", Biometrics 2009 and formalized in ""analysis of learning from positive and unlabeled data"", NIPS 2014. Surprisingly, none of these 3 papers was cited. By definition, GAN-based PU learning belongs to the latter problem setting while Rank Prune can only be applied to the former but was included as a baseline method. <sep> The huge difference between these two settings and their connections to learning with noisy labels are known for long time. To be short, class-conditional noise model corrupts P(Y|X) and covers censoring PU, mutual contamination distribution framework corrupts P(X|Y) and covers case-control PU, and mathematically mutual contamination distribution framework is more general than class-conditional noise model and so is case-control PU than censoring PU. See ""learning from corrupted binary labels via class-probability estimation"", ICML 2015 for more information where the above theoretical result has been proven. An arXiv paper entitled ""on the minimal supervision for training any binary classifier from only unlabeled data"" has some experimental results showing that methods for class-conditional noise model cannot handle mutual contamination distributions. The situation is similar when applying censoring PU methods to case-control PU problem setting. <sep> Furthermore, the class-prior probability pi is well-defined and easy to estimate in censoring PU, see ""learning classifiers from only positive and unlabeled data"" mentioned above. However, it is not well-defined in case-control PU due to an identifiability issue described in ""presence-only data and the EM algorithm"" mentioned above. Thus, the target to be estimated is defined as the maximal theta such that theta*P(X|Y)<=P(X) following ""estimating the class prior and posterior from noisy positives and unlabeled data"", NIPS 2016. BTW, ""mixture proportion estimation via kernel embedding of distributions"" is SOTA in class-prior estimation; the previous NIPS paper was written earlier and accepted later. <sep> In summary, as claimed in the paper and shown in Table 1 in the introduction, all discriminative PU methods and GenPU require to know pi for learning. This is true, but this is because they are designed for a more difficult problem setting---learning classifiers and estimating pi are both more difficult. Lacking some basic knowledge of PU learning is another big issue. <sep> ***** <sep> The novelty is to be honest incremental and thus below the bar of *CONF*. The significance is similarly poor, due to that the experiments mixed up methods for censoring PU and those for case-control PU. What is more, F1-score is a performance measure for information retrieval rather than binary classification. We all know GANs are pretty good at MNIST but not CIFAR-10. In fact, GenPU has a critical issue of mode collapse, and this is why GenPU reports 1-vs-1 rather than 5-vs-5 on MNIST. Even though, I still think GenPU makes much more sense than PGAN and D-GAN.",With positive unlabeled learning the paper targets an interesting problem and proposes a new GAN based method to tackle it. All reviewers however agree that the write-up and the motivation behind the method could be made more clear and that novelty compared to other GAN based methods is limited. Also the experimental analysis does not show a strong clear performance advantage over existing models.
"misc | strength | weakness | decision  ==> The paper proposes to combine several smaller, pretrained RBMs into a larger model as a way to solve combinatorial optimization problems. Results are presented on RBMs trained to implement binary addition, multiplication, and factorization, where the proposed approach is compared with the baseline of training a full model from scratch. <sep> I found the paper confusing at times. It is well-written from a syntactical and grammatical point of view, but some key concepts are stated without being explained, which gives the impression that the authors have a clear understanding of the material presented in the paper but communicate only part of the full picture to the reader. <sep> For instance, there's a brief exposition of the connection between Boltzmann machines and combinatorial optimization problems: the latter is mapped onto the former by expressing constraints as a fixed set of Boltzmann machine weights and biases, and low-energy states (i.e. more optimal solutions) are found by sampling from the model, which involves no training. What's less clear to me is what kinds of combinatorial optimization problems can be mapped onto the RBM *training* problem. The paper states that the problem of training ""large modules"" is ""equivalent to solving the optimization problem"", but does not explain how. <sep> Similarly, the paper mentions that the ""general approach to solving these combinatorial optimization problems is to recognize the atomic unit necessary to solve the problem"", but at that point the reader has no concrete example of what combinatorial optimization problem would be mapped onto training and inference in RBMS. <sep> A concrete example is provided in the Experiments section: the authors propose to implement invertible (reversible?) boolean logic circuits by combining smaller pre-trained RBMs which implement certain logical operations into larger circuits. I have two issues with the chosen example: 1) the connection with combinatorial optimization is not clear to me, and 2) it's not very well explained. As far as I understand, these reversible boolean logic operations are expressed as sampling a subset of the RBM's inputs conditioned on another subset of its inputs. An example is presented in Figure 3 but is not expanded upon in the main text. I'd like the authors to validate my understanding: <sep> An RBM is trained to implement a complete binary adder circuit by having it model the joint distribution of the adder's inputs and outputs [A, B, Cin, S, Cout] (A is the first input bit, B is the second input bit, Cin is the input carry bit, S is the output sum bit, and Cout is the output carry bit), where (I assume) the distribution over [A, B, Cin] is uniform, and where S and Cout follow deterministically from [A, B, Cin]. After training, the output of the circuit is computed from [A, B, Cin] by clamping [A, B, Cin] and sampling [S, Cout] given [A, B, Cin] using Gibbs sampling. <sep> The alternative to this, which is examined in the paper, is to train individual XOR, AND, and OR gates in the same way and compose them into a complete binary adder circuit as prescribed by Section 3. <sep> I think the paper has the potential to be a lot more transparent to the reader in explaining these concepts, which would avoid them spending quite a bit of time inferring meaning from figures. <sep> I'm also confused by the presentation of the results. For instance, I don't know what ""log"", ""FA1"", ""FA2"", etc. refer to in Figure 6. Also, Figure 6 is referenced in the text in the context of binary multiplication (""[...] is able to outperform a multiplier created just by training, as can be seen in Figure 6""), but presents results for addition and factorization only. <sep> The way I see it, implementing reversible boolean logic circuits using RBMs is an artificial problem, and the key idea of the paper -- which I find interesting -- is that in some cases it appears to be possible to combine RBMs trained for sub-problems into larger RBMs without needing to fine-tune the model. I think there are interesting large-scale applications of this, such as building an autoregressive RBM for image generation by training a smaller RBM on a more restricted inpainting task. The connection to combinatorial optimization, however, is much less clear to me.","Dear authors, <sep> Thank you for submitting your work to *CONF*. The original goal of using smaller models to train a bigger one is definitely interesting and has been the topic of a lot of works. <sep> However, the reviewers had two major complaints: the first one is about the clarity of the paper and the second one is about the significance of the tasks on which the algorith is tested. For the latter point, your rebuttal uses arguments which are little known in the ML community and so should be expanded in a future submission."
"rating_summary  ==> For the task of predicting interaction contact among atoms of protein complex consisting of two interacting proteins, the authors propose to train a Siamese convolutional neural network, noted as SASNet, and to use the contact map of two binding proteins' native structure. <sep> The authors claim that the proposed method outperforms methods that use hand crafted features; also the authors claim that the proposed method has better transferability. <sep> My overall concern is that the experiment result doesn't really fully support the claim in the two aspects: 1) the SASNet takes the enriched dataset as input to the neural net but it also uses the complex (validation set) to train the optimal parameters, so strictly it doesn't really fit in the ""transfer"" learning scenario. Also, the compared methods don't really use the validation set from the complex data for training at all. Thus the experiment comparison is not really fair. 2) The experiment results include standard errors for different replicates where such replicates correspond to different training random seeds (or different samples from the enriched set?), however, it doesn't include any significance of the sampling. Specifically, the testing dataset is fixed. A more rigorous setting is to, for N runs, each run splitting the validation and testing set differently. <sep> Since this paper is an application paper, rather than a theoretical paper that bears theoretical findings, I would expect much more thorough experimental setup and analysis. Currently it is still missing discussion such as, when SASNet would perform better and when it would perform worse, what it is that the state of the art features can't capture while SASNet can. Moreover, it is the prediction performance that matters to such task, but the authors remove the non-structure features from the compared methods. Results and discussion about how the previous methods with full features perform compared to SASNet, and also how we can include those features into SASNet should complete the paper. <sep> Overall the paper is well written, and I do think the paper could be much stronger the issues above are addressed. <sep> Some minor issues: <sep> 1) on page 4, Section 3, the first paragraph, shouldn't ""C_p^{val} of 55"" be ""C_p^{test} of 55""? <sep> 2) It is not clear what the ""replicates"" refer to in the experiments. <sep> 3) Some discussion on why the ""SASNet ensemble"" would yield better performance would be good; could it be overfitting?","Two out of three reviews for this paper were provided in detail, but all three reviewers agreed unanimously that this paper is below the acceptance bar for *CONF*. The reviewers admired the clarity of writing, and appreciated the importance of the application, but none recommended the paper for acceptance due largely to concerns on the experimental setup."
"abstract | strength | weakness | rebuttal_process  ==> This paper proposes a new defense to adversarial examples based on the 'fortification' of hidden layers using a denoising autoencoder. While building models that are robust to adversarial examples is an important and relevant research problem, I am not convinced by the evaluation of the defense.  Specific comments: <sep> - The authors mostly evaluate their defense using FGSM (particularly on CIFAR). To truly establish the merit of a new defense, the authors must benchmark against state-of-the-art defenses such as PGD. It also seems like the epsilon values used for the PGD attacks are fairly small. The authors should report accuracies to a range of epsilon values for the PGD attack, as is standard. <sep> - When the authors attack their models using PGD/FGSM, is this only on the classification loss or does this also include the denoising terms? Similar defenses which use denoisers have been broken once you run PGD on the full model [1]. <sep> - I do not really understand the motivation behind using an autoencoder here. Firstly, it is not clear that adversarial examples lie off the data manifold - they could form a very small set on the data manifold and thereby not affect standard generalization. Secondly, have the authors tried a simple regularization loss based on the error between hidden layer representations to a natural examples and the corresponding adversarial example? I think the authors must motivate the use of denoising autoencoders here by comparing to such a simple baseline. <sep> General comment: The results hard to parse given the arrangement of figures and tables. Also, which approach are the authors denoting as 'baseline adv. Train' in the tables? <sep> Overall I feel like building defenses to adversarial examples is a challenging problem and the empirical investigation in this paper is not sufficient to illustrate any real progress on this front. <sep> [1] Athalye, A., & Carlini, N. (2018). On the Robustness of the CVPR 2018 White-Box Adversarial Example Defenses. arXiv preprint arXiv:1804.03286.","This paper suggests a method for defending against adversarial examples and out-of-distribution samples via projection onto the data manifold. The paper suggests a new method for detecting when hidden layers are off of the manifold, and uses auto encoders to map them back onto the manifold. <sep> The paper is well-written and the method is novel and interesting. However, most of the reviewers agree that the original robustness evaluations were not sufficient due to restricting the evaluation to using FGSM baseline and comparison with thermometer encoding (which both are known to not be fully effective baselines). <sep> After rebuttal, Reviewer 4 points out that the method offers very little robustness over adversarial training alone, even though it is combined with adversarial training, which suggests that the method itself provides very little robustness."
"abstract | strength | rating_summary | misc | decision  ==> # Summary <sep> This paper proposes search-guided training for structured prediction energy networks (SPENs). SPENs are structured predictors that learn an input-dependent, non-linear energy function that scores candidate output structures. Many methods have recently been proposed for training SPENs. One in particular, rank-based training, has the advantage of supporting training from weak supervision in the form of a reward function. By performing gradient descent on this reward function, rank-based training generates output, improved output pairs that become margin-based constraints on the learning objective. Each constraint specifies a pair of outputs for a given input, and penalizes the current weights if the improved output is not scored higher than the other output by a certain margin. <sep> This paper addresses a limitation of rank-based training, that this gradient descent procedure for finding output pairs may get stuck in plateaus. In search-guided training, truncated randomized searches are performed starting at an initial output to find an improved output. The paper says that the random search procedure is informed by the reward function, but it is not specific. Are steps in the search space performed uniformly at random? The paper only says that the returned improved example must score higher in the reward function by some margin \\delta that is ""based on the features of the reward function (range, plateaus, jumps)"" but it is not discussed how to identify these features of the reward function or how to set \\delta accordingly. <sep> Experiments are conducted multi-label classification, citation field extraction, and shape parsing. On multi-label classification search-guided SPENs (SG-SPENs) outperform structural SVM training of SPENs. Why is it not compared with rank-based training (R-SPENs)? On citation field extraction, SG-SPENs improves accuracy by two percentage points over R-SPENs. On shape parsing, R-SPENs fail because it cannot produce valid parsing programs as improved outputs. SG-SPENs perform well relative to other methods like iterative beam search and neural shape parsing. <sep> # Strengths <sep> SG-SPENs are better across the experiments than other SPEN training methods, though I do not know why they are not compared against R-SPENs on multi-label classification. <sep> # Weaknesses <sep> The work seems incremental without any major new insights beyond the work on R-SPENs. The idea seems to reduce to doing random search instead of gradient descent on a reward function in order to produce output pairs. <sep> As mentioned above, the paper is also light on details about how the experiments were conducted, such as setting \\delta and creating the space of operators to use when searching for improved outputs.","This paper proposes search-guided training for structured prediction energy networks (SPENs). <sep> The reviewers found some interest in this approach, though were somewhat underwhelmed by the experimental comparison and the details provided about the method. <sep> R1 was positive and recommends acceptance; R2 and R3 thought the paper was on the incremental side and recommend rejection. Given the space restriction to this year's conference, we have to reject some borderline papers. The AC thus recommends the authors to take the reviewers comments in consideration for a ""revise and resubmit""."
"abstract | weakness | decision  ==>  ==> This paper presents the idea of splitting the training process into two phases: fast training on a subset of the original dataset and finetining on the full dataset. To find a good subset of the training dataset it is proposed to train an autoencoder and use its embeddings to choose examples that have large values of the embedding features. The experiments show that on CIFAR-10 dataset this may speed up the convergence. <sep> In general, I like the idea of being smart about which data and in which order to feed to the learner. <sep> Nonetheless, I disagree with several premises of this paper. The paper claims that by making the dataset smaller one can speed up the training by the means of fitting the dataset into the accelerator memory and thus avoiding slow memory copies from CPU to accelerator memory. However, modern deep learning data pipelines are built in a way that has virtually zero overhead, since the data is loaded from disk and preprocessed on CPU and then copied on the accelerator asynchronously (i.e. the GPU doesn't have to wait for the data, it can process the current batch and at the same time load the next one). Moreover, moving the data to GPU will introduce additional overheads in the case of random data augmentation, since this additional work would have to be done by GPU (while current deep learning frameworks asynchronously do this work on CPU). And finally, the authors claim that most modern datasets can fit to the accelerator memory if reduced 10x, but in my experience the network and it's activations (which are stored during training) occupies most of the GPU memory even on high end accelerators, not leaving enough space to store a large dataset even after data reduction. <sep> The paper cites Dunner  et al. (2017) as related work that focus on the similar problem: how to find a subset of the dataset to fit it into the GPU memory. However, I would argue that their setup is very different because they are using linear models (such as SVM): their learning steps are very fast compared to CNNs (which makes the memory bandwidth much slower in comparison), they don't have to store activations of the layers (which allows them to fit much more training samples into GPU memory), and they don't use data augmentation. <sep> Also, I don't think that the experimental comparison provides a strong enough evidence supporting the benefits of the proposed scheme. First, the experiments are only done on a small scale dataset (CIFAR-10), which is OK in general, but questionable when the proposed method explicitly targets big data regime and making the training faster. Second, the only baseline considered is choosing subset of data randomly. Third, the optimization method is plain SGD with momentum, while when presenting techniques for faster convergence it would make sense to compare on at least several standard optimization algorithms (e.g. Adam). Finally, the presented results are weak: in Fig 4 any improvements over random baseline are noticeable only after degrading the performance of the network by a large margin (to less than 67% accuracy on CIFAR-10); <sep> Fig 5 looks like it has an error: training on the full dataset performs on the level of random guess after some training, which contradicts the fact that the same network converged to something reasonable in Fig. 6. Also, I believe that the training on the full dataset is strictly better than training on a random subset of data for a few epochs and then finetuning on the full dataset (with the same time budget). The latter sees the same number of updates but with less data, which should only decrease the test performance. If it's correct, the results in Fig 5 for the full training should look similar (or better) than the results in Fig 4 for the random subset baseline, but it's very far from being the case. <sep> In Fig 6 I'm not sure what is being compared. Is a train or test loss? If it's train, then it's not a fair comparisons, since the two network are optimizing different train losses. I'm also very surprised not to see the moment of training mode transition on the plot (i.e. the moment when the model switched from restricted dataset to the full one), the lack of it can indicate an implementation error. <sep> And finally, I would like to see the text being improved. Right now the language is confusing, for example: ""this technique is shown to be effective"" (what does it mean ""effective"" and compared to what?), ""Unfortunately, while these techniques may be viable for smaller networks or datasets, large datasets have shown that they do not scale well."" (who have shown that the techniques doesn't work on large dataset?), ""the testing network is initialized using a weighted average of the final weights learned during training"" (what does it mean?), ""Qualitatively, the trained autoencoder succeeded in learning an adequate embedding."" (what does it mean?), etc. <sep> Also, there is a typo in formulas 1, 2, and 3: it probably should sum up to n-1. <sep> And I didn't get what formula 4 means, what is ""union of (for all i in n)""? <sep> This bit I also didn't get: ""This simple loss function, in essence, forces the network to learn to extract the key features from the input, so that it can reproduce it using said features only. If desired, one could elect to use a more sophisticated loss, such as the Wasserstein distance metric (Gulrajani et al., 2017; Arjovsky et al., 2017), that takes more into account than raw pixel values."". How can you substitute L2 loss in an autoencoder with Wasserstein metric (which is a metric between probability distributions, not images)? <sep> There is also some missing related work, e.g. the idea mentioned in conclusion on augmenting the dataset in the latent space is presented in DeVries et al. ""Dataset augmentation in feature space"". <sep> It would be interesting to connect this work with importance sampling off-policy RL (see e.g. ""Prioritized Experience Replay"") and look into sampling dataset points proportional to some importance probability with importance sampling correction. <sep> On the positive side, I really enjoyed the look of the figures and diagrams.","The paper proposes a filtering technique to use less training examples in <sep> order to train faster; the filtering step is done with an autoencoder. <sep> Experiments are done on CIFAR-10. Reviewers point to a lack of convincing <sep> experiments, weak evidence, lack of experimental details. <sep> Overall, all reviewers converge to reject this paper, and I agree with them."
"abstract | strength | weakness | rebuttal_process | weakness  ==> This paper presents a view of sentence-level prediction tasks as statistical relation learning problems. In particular the paper argues that composition functions used in recent SRL techniques developed for entity-to-entity relationship detection can be applied to sentence-level relation prediction tasks. <sep> Suppose there is a prediction training task defined over pairs of sentences (x1, x2). This task requires some function 'f' that composes the sentence representations h1 and h2 into a single representation which is then used to make the relation prediction i.e., we have a model g(f(h1, h2)) that is used to predict some relation between R(x1, x2).  This paper aims to show that with a better 'f' we can hope for a better result in transfer tasks (in addition to doing better on the training task). <sep> The paper argues that this setting, at a high level, is similar to the composition function used in entity-entity relation prediction. There have been many such methods in the recent past (e.g., TransE, ComplEx, RESCAL). This paper asks whether these composition functions can work well for sentence-level tasks. <sep> The paper then presents experiments which compare the performance of different composition functions against a basic composition function used in InferSent. <sep> Strengths of the paper: <sep> 1. I like the main question of what can we learn from SRL. This seeks to bridge some independent research threads. <sep> 2. The evaluation considers a range of composition functions used in SRL and applies them to the sentence tasks. <sep> 3. Points out that some of the composition functions used in existing models are not particularly strong. <sep> Issues: <sep> I like the starting point for this paper very much and agree that the existing composition functions for sentence relations are rather weak. However, I am struggling to see if there is (i) a convincing conceptual argument for why SRL view of compositions is necessarily the answer for sentence level tasks, or (ii) a convincing empirical case for the same.  Some details on these points: <sep> 1) The parallels between entity-entity relations and sentence-sentence relations seems a bit of a stretch to me. There is always some level of abstraction at which two problems might look similar, which can be advantageous for repurposing solutions. However, in this case I think the SRL view of the world hides the complexities in sentence-sentence relation tasks (e.g. aligning relevant pieces of information, requiring more complex composition functions to derive meaning etc.). <sep> 2) I am not sure what knowledge we are getting from an SRL view of the problem that is not already known already to the communities that work on sentence embedding. The minimum requirements laid out can be met easily by existing methods for sentence representations. For instance that we need to allow for asymmetric relations (entailment order) is very well known. As the authors themselves point out there are solutions for this problem. <sep> 3) The empirical results don't appear convincing. The average gain for any particular method over InferSent is 0.3 in macro average. There is no single SRL based composition method that works consistently clear gains across most tasks. <sep> Here are some suggestions that I think will improve the paper (or at least help me buy the motivation): <sep> 1. One question that might be useful to make a conceptual argument is how much work should be done in 'f' and should it change for the different type of target tasks. <sep> If the idea is to transfer h for single sentence target task, then a powerful 'f' can render h1 and h2 to be simple enough, such that bulk of the work in extracting task related information might be done by 'f' itself. Therefore, transferred h may not be as powerful as it could have been with a less powerful 'f'. <sep> If the idea is to transfer f(h1, h2) for sentence-pair target tasks, then a powerful 'f' might be a good thing. <sep> 2) Another useful discussion would be to discuss why more powerful alignment based sentence representations are not being considered at least for comparison purposes. <sep> The paper wants to go from a simple 'f' (i.e. concat(h1,h2), h1-h2) to some other choices for 'f' that are known functions from SRL. <sep> There are several sentence-level representation functions such as ESIM [Chen et al., 2016] which uses a combined representation of premise and hypothesis sentences using soft alignment to specifically address the issues in comparing sentences. A similar representation is computed in BiDAF [Seo et al., 2017] in the context of matching question representation with sentence representations. <sep> To summarize, I really like the basic starting point for the paper and would love to see a more compelling presentation of the conceptual argument and a stronger empirical comparison.","This paper offers a new angle through which to study the development of comparison functions for sentence pair classification tasks by drawing on the literature on statistical relational learning. All three reviewers seemed happy to see an attempt to unify these two closely related relation-learning problems. However, none of the reviewers were fully convinced that this attempt has yielded any substantial new knowledge: Many of the ideas that come out of this synthesis have already appeared in the sentence-pair modeling literature (in work cited in the paper under review), and the proposed new methods do not yield substantial improvements for the tasks they're tested on. <sep> I'm happy to accept the authors' arguments that sentence-to-vector models have practical value, and I'm not placing too much weight on the reviewer's comments about the choice to use that modeling framework. I am slightly concerned that the reviewers (especially R2) observed some overly broad statements in the paper, and I urge the authors to take those comments very seriously. <sep> I'm mostly concerned, though, about the lack of an impactful positive contribution: I'd have hoped for a paper of this kind to offer a a method with clear empirical advantages over prior work, or else a formal result which is more clearly new, and the reviewers are not convinced that this paper makes a contribution of either kind."
"abstract | weakness  ==> In this work, the authors propose a generalization of the batch normalization (BN) technique often used in training neural networks, and analyzed this convergence. In particular, a one hidden layer and one BN hidden layer fully connected network is considered, and a deterministic gradient descent algorithm with certain kind of BN has been considered in this work. The proposed ""generalized"" BN strategy is devised on the deterministic setting, but it is a slight generalization of the original BN by introducing a moving average operation. Classical results of Bertsekas is leveraged to show the asymptotic convergence of the algorithm. <sep> I have the following three main comments about the paper. <sep> 1) Only deterministic setting is considered, but in this case every time the entire data set will be used to perform the averaging, it appears to be much easier to analyze than the stochastic setting. Further the reviewer has doubt on whether the resulting deterministic algorithm has any practice value. <sep> 2) Because the authors have used the Bertsekas/Tsitsiklis (B/T) argument, only asymptotic convergence is shown. It is not clear, even in the deterministic case, whether some kind of sublinear convergence rate can be obtained. <sep> 3) Only one hidden layer of neural network with one BN operation is considered. It is not clear whether the analysis can be extended to multiple layers, despite the statement of the author saying that ""the technique presented can be extended to more layers with additional notation"". In particular, when there are multiple layers, the BN layers will be further composite together across multiple nonlinear operations. <sep> 4) The authors have mentioned that the derivative is always taking w.r.t. theta. However, in (9) is appears that the derivative is taken with respect to lambda, in order to get the Lipschitz condition on \\lambda. This is a bit confusing. Also it is not clear how the gradient in Assumption 5 is defined. <sep> 5) Assumption 5 does not make sense. Problem (1) is a constrained problem with both variables being confined in compact feasible sets. And this condition is important in Assumption2. Now the authors say that at stationary solution the gradient is zero? Please specify functions when this will happen. I will suggest that the authors use a proper definition of stationarity solution for constrained problems. <sep> 6) Follow up on the previous point. The analysis builds upon B/T argument for unconstrained optimization. However it is not suitable for the constrained problems that the authors started out at the beginning of the paper. The authors may consider develop new analysis tools to understand the problem at hand, rather than assuming away the difficulties.","The paper introduces a modification of batch normalization technique. In contrast to the original batch normalization that normalizes minibatch examples using their mean and standard deviation, this modification uses weighted average <sep> of mean and standard deviation from the current and all previous minibatches. The authors then provide some theoretical justification for the superiority of their variant of BatchNorm. <sep> Unfortunately, the empirical demonstration of the improved performance seems not sufficient and thus fairly unconvincing."
"abstract | weakness | rebuttal_process  ==>  ==> # Summary <sep> This submission proposes a method to combine the benefits of model-based RL and Imitation Learning (IL) for navigation tasks. The key idea is to i) learn a prior over trajectory distributions from a fixed dataset of demonstrations, and ii) use this learned dynamical model for path planning via probabilistic inference. Reaching target waypoints is done by maximizing the trajectory likelihood conditioned on the planning goal. The prior is learned using R2P2 on LIDAR features and past positions. Experiments using the CARLA driving simulator show that this method can outperform standard control, IL, and model-based RL baselines, while flexibly incorporating test-time goals and costs thanks to its probabilistic formulation. <sep> # Strengths <sep> The method is an elegant way to get the best of both worlds in RL and IL, leveraging the recent R2P2 work to estimate a powerful sequential model used for planning via probabilistic inference. The flexibility of the method in considering test-time cost maps and user-defined goals (e.g. to avoid potholes) is appealing, especially since it does not require on-policy data collection. <sep> The proposed planning-as-inference method can in theory handle the multi-modality present in human demonstrations by using a probabilistic model of the observed behaviors as prior over undirected expert trajectories. <sep> The approach seem to outperform both model-based and imitation learning baselines on a simplified version of the CARLA benchmark, including on interesting fine-grained metrics (e.g., comfort based). <sep> # Weaknesses <sep> The main weakness of this submission lies in its experimental evaluation, especially the absence of any dynamic objects in the tested environment (""static world CARLA"", section 1). It is unclear how this approach would generalize beyond just staying on the road. How would it handle traffic lights, pedestrians, other drivers, weather variations, and more complex driving tasks than waypoint following by traversing mostly free space? How does the prior generalize to more complex behaviors (e.g, by using more contextual information \\phi)? How robust is the method to noise in the demonstrations, i.e. non-expert or suboptimal behavior? It seems that estimating the generative prior on human behavior might suffer from the same issues as behavior cloning, e.g., the sample inefficiency due to the combinatorial explosion of causal factors explaining complex human behaviors. It might be in fact even harder to estimate that generative model than use a direct discriminative approach (e.g., a modular pipeline), at the cost of reduced flexibility at test time of course. The currently reported sample efficiency (7000 training samples) and near perfect success rate seem to suggest that this (non-standard) version of the CARLA benchmark is too simple (no weather variations, no dynamic obstacles). Comparison to the state of the art (beyond the baselines implemented here) on the original CARLA benchmark seems needed (especially in the ""Nav. dynamic"" task). <sep> The method is only described very succinctly in section 2. I do not believe there are enough details (especially around the learning algorithm, hyper-parameters, and other important technical elements) for reproducibility at this stage. Section 2.1 is also quite dense for people not familiar with the R2P2 paper. As the main contribution of the paper is to leverage that model for planning and control, it would be great to maybe discuss a bit deeper. Finally, the input modalities are not clear, especially for the baselines: the proposed method is using LIDAR and localization whereas the IL baseline seems to use vision (while the others just use the trajectory). This makes the fairness of the comparison really unclear (LIDAR is a much stronger signal for just staying on the road). <sep> Minor remarks: <sep> - Why use a proportional controller as a baseline instead of the standard PID one? <sep> - Section 2.3 seems like it's missing the extension of equation 2 to the multi-goal case? <sep> - Typos in section 3 (""trail-and-error""), section 4 (""autonmous"", ""knowledge to"") <sep> # Recommendation <sep> Although the theoretical benefits of the method are well-motivated and clear (off-policy learning, probabilistic model, flexibility at test time), the experimental evaluation (custom simple CARLA test, unclear comparison to baselines) and lack of details impeding reproducibility seems to suggest that this submission needs a bit more work. First, adding more details as suggested above and clarifying the experimental protocol seem like a must, but can be easily addressed by an update to the text. Second, it would be ideal to evaluate the approach on the standard CARLA benchmark in order to compare fairly to the prior art. This is much more involved. <sep> I personally like the approach, so although I think it is marginally below the acceptance threshold in its current form, I reserve my judgement for the time being and look forward to the authors' reply. <sep> # Update <sep> The submission has been drastically rewritten (the diff is massive) and I think it is in much better shape, answering some of my concerns around reproducibility and generalization. Furthermore, it reinforces the strengths of the approach (esp. around its flexibility). <sep> I am willing to recommend acceptance, but I have some further questions (hence I have only updated my score to a 6 for now). They are mostly related to the comparison with IL (important to validate the claim in the paper that the proposed approach is quantitatively better than both existing IL and RL methods). See discussion below for details.","This paper proposes to combine RL and imitation learning, and the proposed approach seems convincing. <sep> As is typical in RL work, the evaluation of the method is not strong enough to convince the reviewers. Increasing community criticism on RL methods not scaling must be taken seriously here, despite the authors' disagreement."
"misc | strength | misc | decision | misc | decision | weakness  ==> The paper is on applying GAN to steganography, and it is especially suited for people who does not know steganography. In fact, 1/4 of the paper is spent on introducing (in a very clear and catchy way) the basics. State of the art section nonetheless shows that the authors are grounded with the very last related work. The novel GAN framework is called ISS-GAN. Simply speaking, ISS-GAN is built by the combo: <steganography generative, steganalysis discriminative>. In the details, the implementation is not straightforward, and embeds interesting ideas. Two generative and two discriminative models are intertwined: one generative for the stego image generation process, the other generative for the secret image extraction process. The two discriminative models are for ensuring that the distribution of cover images is indistinguishable from the stego distribution and to ensure that the distribution of extracted secret images is indistinguishable from the distribution of the original secret images. In rough terms, the adversarial double loop serves to ensure that the embedding and the extraction function preserve both the aspect of the original and the secret images. PSNR and SSIM metrics are employed to give a quantitative check of the approach. <sep> Obvious questions are about the scalability of the approach, since it applies to 256x256 miniatures, which act well on the figures of an *CONF* paper but poor in reality. At the same time, is it reasonable to show images such that Figure 3? It is obvious stega processes are now very effective, and stressing that by showing seemingly same images it is not very illustrative. Figure 4 typos are reated to row column ordering, but even in this case is hard to spot something without spending time in magnifying the pdf playing at look at the differences… Figure 5 and 6 are even worse, with tens of practically equal images. Less images with differences pictures should have been better. In addition, with these qualitative results, it is hard to give a weight to the quantitative comparisons, in terms of both the PSNR and SSIM metrics. <sep> The most obvious and perhaps elementary question I have is how risky is that the ISS GAN model can be replicated by an attacker which sniffs the secret images by simply having a training set which approximatively is the same of the primary sender? <sep> The other questions are about how much realistic is that the images are affected by the noises of Table 5, nowadays?","1. Describe the strengths of the paper. As pointed out by the reviewers and based on your expert opinion. <sep> - The problem and approach, steganography via GANs, is interesting. <sep> - The results seem promising. <sep> 2. Describe the weaknesses of the paper. As pointed out by the reviewers and based on your expert opinion. Be sure to indicate which weaknesses are seen as salient for the decision (i.e., potential critical flaws), as opposed to weaknesses that the authors can likely fix in a revision. <sep> The original submission was imprecise and difficult to follow and, while the AC acknowledges that the authors made significant improvements, the current version still needs some work before it's clear enough to be acceptable for publication. <sep> 3. Discuss any major points of contention. As raised by the authors or reviewers in the discussion, and how these might have influenced the decision. If the authors provide a rebuttal to a potential reviewer concern, it's a good idea to acknowledge this and note whether it influenced the final decision or not. This makes sure that author responses are addressed adequately. <sep> Concerns varied by reviewer and there was no main point of contention. <sep> 4. If consensus was reached, say so. Otherwise, explain what the source of reviewer disagreement was and why the decision on the paper aligns with one set of reviewers or another. <sep> The reviewers did not reach a consensus. The final decision is aligned with the less positive reviewers, one of whom was very confident in his/her review. The AC agrees that the paper should be made clearer and more precise."
"weakness | suggestion  ==> In the vein of recent work on learning ""ticking"" behaviour for LSTMs such as Phased LSTM, this paper proposes to add additional data independent gates to LSTM units that are defined as Gaussian functions of time indices. <sep> The performance of the modified g-LSTM is compared to LSTM on the Addition, sequential MNIST and sequential CIFAR-10 tasks. The authors argue that g-LSTM results in better performance and has faster convergence on these tasks. <sep> Additionally, it is proposed that one can reduce the amount of computations performed by the network by adding a computation budget term to the optimized loss that encouraged the cells to update less often. Finally, a technique for gradually transitioning from a g-LSTM to an LSTM during training is proposed, with the objective of speeding up training over a regular LSTM. <sep> The paper is well written and easy to understand in general. However, the main results of this paper are experimental, and I am not entirely convinced by the experiments that g-LSTM is an improvement over the LSTM baseline for certain scenarios. <sep> One broad reason for my doubts is that the comparisons don't seem to utilise proper hyperparameter tuning for the baseline LSTM. Network sizes, learning rates, decay schedules, initialisations etc. all appear to be fixed, so one can not be sure of the ""real"" performance or convergence behavior of the models. Biased gate initializations are not used, though they have been used successfully in past work to aid in long term memory. <sep> I should note that for long term memory problems such as those proposed by Hochreiter and Schmidhuber (1997), the proposed LSTM did not use a forget gate (or even BPTT) and used biased gate initialisations. However, these features are useful for more realistic tasks, and popular LSTM designs are biased towards them instead of toy problems. <sep> I would consider the addition problem and sequential MNIST and CIFAR-10 to be interesting and difficult toy tasks for initial validation of ideas (and more extensive hyperparameter searches). It is unclear if the proposed techniques will perform provide  improvements over a well-tuned baseline for some realistic tasks, or are they suitable only for toy problems.","perhaps the biggest issue with the proposed approach is that the proposed approach, which supposedly addresses the issue of capturing long-term dependency with a faster convergence, was only tested on problems with largely fixed length. with the proposed k_n gate being defined as a gaussian with a single mean (per unit?) and variance, it is important and interesting to know how this network would cope with examples of vastly varying lengths. in addition, r3 made good points about comparison against conventional LSTM and how it should be done with careful hyperparameter tuning and based on conventional known setups. <sep> this submission will be greatly strengthened with more experiments using a better set of benchmarks and by more carefully placing its contribution w.r.t. other recent advances."
"abstract | weakness  ==> The authors propose a new training scheme for training neural networks for multi-label prediction tasks by introducing ontology relationships between labels. <sep> The paper motivates very well by the observations that some labels include very small amount of data points. <sep> However, the authors don't really investigate why such labels are rarely observed and the experiments don't include any significance. <sep> Thus overall I don't think the paper is ready for publishing for *CONF* yet. <sep> Below are some more detailed comments: <sep> 1) The authors discuss nicely about the intuition to introduce the Bayesian networks in the tasks of disease prediction. Essentially, the probability of assigning the label (leaf node) should be account for the probability of it being observed, namely the prior. Thus it is not surprising that for the rare labels, the proposed method would yield higher precision. However, the experiments don't really include any significance measurement; especially for such tasks where the number of testing examples with rare labels is small (5~10 positive examples), significance measurement or some forms of hypothesis testing is a must-have in order to draw conclusion about the performance comparison. Answering such significance issue with tests for overfitting would be nice. <sep> 2) My other major concern is for the protein function prediction task, the reason of why for certain labels, the number of instances is small, could be due to that a) there don't exist much biological web-lab evidence, or b) among the population, there indeed only exist small number of proteins associated with such labels. The proposed method can address b) but not necessarily address a). <sep> 3) The paper discusses the other results very briefly in Section 5 but doesn't include any experiment comparison. Thus it is not convincing that the proposed method is making contribution to the field of disease prediction, protein function prediction or even general multi-label prediction. I would suggest to include the comparison with the state of the art methods for each application. <sep> 4) Particularly for protein function prediction, another line of studies is to use protein protein interaction networks or other sources such as functional pathways rather than using sequence information alone (ref below). Some discussion would be nice. <sep> Schwikowski, Benno, Peter Uetz, and Stanley Fields. ""A network of protein–protein interactions in yeast."" Nature biotechnology 18.12 (2000): 1257. <sep> Cao, Mengfei, et al. ""New directions for diffusion-based network prediction of protein function: incorporating pathways with confidence."" Bioinformatics 30.12 (2014): i219-i227.","The paper proposes a nice approach to massively multi-label problems with rare labels which may only have a limited number of positive examples; the approach uses Bayes nets to exploit the relationships among the labels in the output layer of a neural nets. The paper is clearly written and the approach seems promising, however, the reviewers would like to see even more convincing empirical results."
"abstract | weakness  ==> The paper discusses a method to increase accuracy of deep-nets on multi-class classification tasks by what seems to be a reduction of multi-class to binary classification following the classical one-vs-all mechanism. I fail to see any novelty in the paper. The problem of reducing multi-class to binary classification has been studied thoroughly with many classical papers like: <sep> 1. Usual one-vs-all - this paper does the same thing as one vs all in my opinion even though this technique is known for a decade or so. <sep> 2. Weighted One-Against-All - http://hunch.net/~jl/projects/reductions/woa/woa.pdf and more sophisticated techniques like: <sep> 3. Alina Beygelzimer, John Langford, Pradeep D. Ravikumar. Error-Correcting Tournaments. CoRR, abs/0902.3176, 2009. <sep> 4. Erin L. Allwein, Robert E. Schapire, Yoram Singer, Pack Kaelbling. Reducing Multiclass to Binary: A Unifying Approach for Margin Classifiers. Journal of Machine Learning Research, 113—141, 2000. <sep> 5. Thomas G. Dietterich, Ghulum Bakiri. Solving multiclass learning problems via error-correcting output codes. Journal of Artificial Intelligence Research, 2:263—286, 1995. <sep> In my opinion the methods in [1,2,3,5] above can be used with any binary learners and therefore deep-networks. This paper makes no effort in comparing with any of these well-known papers. Moreover the experiments do not show any gain in state of the art performances in the data-sets used, as experiments are done with toy-networks. Further some rules for selecting the class is discussed in Section 3. There are many known rules for generating probability scores in one-vs-all classification and the relations to these are not discussed. <sep> Therefore, I fail to see any novelty in this paper (theoretical or empirical).","This work examines how to deal with multiple classes. Unfortunately, as reviewers note, it fails to adequately ground its approach in previous work and show how the architecture relates to the considerable research that has examined the question beforehand."
"abstract | weakness | rebuttal_process | rating_summary | decision  ==> Summary <sep> ------- <sep> The paper proposes a technique to make generative models more robust by making them consistent with the local density. It is hypothesized that robust model will be able to detect out-of-distribution samples better and improve anomaly detection. <sep> Main comments <sep> ------------- <sep> 1. The proposed technique adds additional regularizers to the GAN loss that, in effect, state that the best hypothesis under a WTA strategy should have a high likelihood under the discriminator 'D'. This is an interesting idea and certainly a reasonable thing to try. As stated in the abstract, the generative models are inefficient; it is likely that additional structure enforced by the regularizer helps in improving the efficiency. <sep> 2. The objective in GANs is to infer the underlying distribution correctly and so far it has been found that their accuracy is heavily dependent on both the architecture as well as the computational complexity (they may improve with more training, but maybe not consistently). Therefore, it becomes hard to compare the three architectures in Figure 2 since they are all different. A more rigorous comparison would try to keep as many pieces of the architecture the same as possible so that ConAD can be compared with 'all other things being same'. Some experiments seem to follow this idea such as 'MDN+ConAD-{2, 4, 8, 16}' in Table 2. But in these experiments the addition of ConAD offers a mild improvement and even degrades for the maximum number of hypothesis (i.e., 16). <sep> 3. Page 2, para 2, last two lines: ""For simplicity, imagine an ... the real distribution."" <sep> The argument is not clear. It seems too trivial and almost like a straw man argument. <sep> 4. Page 4: ""In anomaly detection, this is difficult since there is no anomalous data point contained in the training dataset."" <sep> This is not true in real-world applications where most data is contaminated with anomalies. This is part of the challenge in anomaly detection. <sep> The above also applies to the following on page 6: ""During model training, only data from the normal data class is used..."" <sep> 5. Page 5: ""...D minimizes Eq. 3"": Should be 'maximizes' since the reference is to the log likelihood of real data (or, add a negative sign). <sep> 6. Eq. 4: The last component should be negative since we trying to maximize the likelihood of the best hypothesis under WTA (right?). <sep> 7. Table 1: The datasets are not real anomaly detection datasets (too high proportion of 'anomalies') Moreover, the number of datasets is insufficient for rigor. <sep> 8. Section 5.4: ""With our framework ConAD, anomaly detection performance remains competitive or better even with an increasing number of hypotheses available."" <sep> Section 6: ""... and alleviates performance breakdown when the number of hypotheses is increased."" <sep> This is not entirely supported by the results in Tables 2, 3, and also 4 and 5 of supplement. The results for ConAD - {2, 4, 8, 16} are not consistently increasing. <sep> Since experiments are very few (and not real-world for anomaly detection task) because of which the observations cannot be generalized. <sep> 9. Page 4 (minor) in two places: ""one-to-mapping"" -> ""one-to-many mapping"" <sep> 10. Page 5 (minor): ""chap. 3"" -> ""section 3""","This paper proposes an anomaly-detection approach by augmenting VAE encoder with a network multiple hypothesis network and then using a discriminator in the decoder to select one of the hypothesis. The idea is interesting although the reviewers found the paper to be poorly written and the approach to be a bit confusing and complicated. <sep> Revisions and rebuttal have certainly helped to improve the quality of the work. However, the reviewers believe that the paper require more work before it can be accepted at *CONF*. For this reason, I recommend to reject this paper in its current state."
"abstract | strength | weakness | rebuttal_process | suggestion | rebuttal_process | decision  ==>  ==> Summary: The paper proposes methods for identifying prototypes. Unfortunately, a formal definition of a prototype is lacking, and the authors instead present a set of heuristics for sorting data points that purport to measure 'prototypicality', although different heuristics have different (and possibly conflicting) notions of what this means. The experiments are not very convincing, and often present results that are either inconclusive or negative, i.e. seem to demonstrate that prototypes are not very useful. <sep> Pros: <sep> - The notion of prototypes is used in various papers, but a formal definition is lacking, and the usefulness of prototypes is not demonstrated. The fact that this paper sets out to do both is laudable, although the paper needs work before it can be accepted for publications. <sep> Detailed comments / cons: <sep> *Defining prototypes: <sep> - The authors list desirable properties before defining (even informally) what a prototype is, and what its purposes are. Taking the first property as an example, is it reasonable to expect a metric for prototypes to be useful for image classification AND image generation? The answer completely depends on what one expects from a prototype, what its purpose is, etc. <sep> - The second property seems to indicate that prototypes are model-independent, i.e. two models trained on the same dataset will have the same prototypes. This is confusing as the metrics proposed are clearly model-dependent (e.g. adv completely depends on the trained model's decision boundary, conf obviously depends on the model providing the confidence score) <sep> - The third and fourth property are poorly defined. Human intuition presupposes that humans agree on what a prototype means. Using 'modes of prototypical examples' in trying to define a metric for prototypes is circular, as a mode of prototypical example depends on a working notion of prototypical examples. <sep> - The last property is completely dependent on which models are trained, and how they are trained. If a model has high label complexity, maybe it does not achieve high accuracy even when trained on high quality prototypes. In any case, this property is at odds with the first two properties. <sep> In sum: it's not clear what prototypes are, so it becomes hard to judge if the list of desiderata is reasonable. The list is in any case ill-defined, and contains contradictions. <sep> * Metrics for prototypicality <sep> - The second paragraph in this section is unnecessary <sep> - All of the metrics proposed are heuristics with little to no justification. Specific comments below. <sep> - Adversarial robustness is a property of a trained model, not of prototypical examples, unless prototypes are supposed to be model dependent (contra property 1). In any case, it is not clear why examples that are robust to adversarial noise are good 'prototypes'.  Using facial recognition as an example, a 'mean face' may be very robust to adversarial noise but not prototypical at all under common definitions. A face with a particular type of facial hair (e.g. nose hair) may be very representative of a class of faces (i.e. a prototype), but very susceptible to adversarial noise. In fact, any examples in the boundary of the decision function will be more susceptible to adversaries, but that does not make them 'less prototypical'. <sep> - Holdout retraining is again completely model dependent. Why should we expect a model to treat a prototype the same regardless of whether or not it is trained on it? This basically means that we expect the model to always be accurate on prototypes. <sep> - Ensemble agreement proposes a notion of prototypes that is based on prediction 'hardness'. It is clear that such a notion depends completely on which models are being considered, which features are being used, and etc, much more than on notions of prototypicality inherent in the data. The same criticism applies to model confidence. <sep> - Privacy preserving training assumes prototypicality has to do with the model being able to learn with some robustness to noise (related to Adversarial Robustness, but different). This assumes a definition of prototypes that is not congruent with the other metrics. <sep> In sum: the proposed metrics are basically heuristics with little justification, and different metrics assume different notions of what a prototype is. <sep> * Evaluation <sep> - Section 3.1 claims that the metrics are strongly correlated, but that is not true for MNIST or CIFAR, and is somewhat true for fashion-mnist. In any case, since the metrics are so model-dependent, it is not clear if these results would hold if other models were used. <sep> - Section 3.2 - The question asked of turkers in the study is too vague, and borderline irrelevant for the task at hand - what does the 'best image' of an airplane mean, and how does this translate to it being a prototype? All that the study demonstrates is that the proposed metrics score malformed images with low score. The results in Table 1 are very spread out, and seem to indicate a low agreement between the metrics and human evaluation - although Table 1 is almost irrelevant given the question that was asked of users. <sep> - The results in Section 4 are very discouraging: sometimes it is better to train on most prototypical examples according to the metrics, sometimes it is worse, sometimes it's better to take examples in the middle. That is, prototypes don't seem to help at all. 'Prototype percentile' is uncorrelated with robustness for MNIST in Appendix E, while being correlated for other datasets. It is clear why this would be the case for metrics such as confidence, but in general models trained on less examples are less robust than models trained on the whole dataset (again, as expected). As a whole, the results do not provide any help for a user who wants to produce a more robust model, other than 'ignore prototypes and use the whole dataset'.","This paper considers ""prototypes"" in machine learning, in which a small subset of a dataset is selected as representative of the behavior of the models. The authors propose a number of desiderata, and outline the connections to existing approaches. Further, they carry out evaluation with user studies to compare them with human intuition, and empirical experiments to compare them to each other. The reviewers agreed that the search for more concrete definitions of prototypes is a worthy one, and they appreciated the user studies. <sep> The reviewers and AC note the following potential weaknesses: (1) the specific description of prototypes that the authors are using is not provided precisely, (2) the desiderata was found to be informal, leading to considerable confusion regarding the choices that are made and their compatibility with each other, (3) concerns in the evaluation regarding the practicality and the appropriateness of the user study for the goals of the paper. <sep> Although the authors provided detailed responses to these concerns, most of them still remained. Both reviewer 1 and reviewer 2 encourage the authors to define the prototypes defined more precisely, providing motivation for the various choices therein. Even though some of the concerns raised by reviewer 3 were addressed, it still remains to be seen how scalable the approach is for real-world applications. <sep> For these reasons, the reviewers and the AC feel that the authors would need to make substantial improvements for the paper to be accepted."
"abstract | weakness | suggestion  ==>  ==> This paper presents a methodology to bring together independent subspace analysis and variational auto-encoders. Naturally, in order to do that, the authors propose a specific family of prior distributions that lead to subspace independence the Lp-nested distribution family. This prior distribution is then used to learn disentangled and interpretable representations. The mutual information gap is taken as the measure of disentanglement, while the reconstruction loss measures the quality of the representation. Experiments on the sPrites dataset are reported, and comparison with the state of the art shows some interesting results. <sep> I understand the limitations of current approaches for learning disentangled representations, and therefore agree with the motivation of the manuscript, and in particular the choice of the prior distribution. However, I did not find the answer to some important questions, and generally speaking I believe that the contribution is not completely and clearly described. <sep> P1) What is the shape of the posterior distribution? <sep> P2) How does the reparametrization trick work in your case? <sep> P3) How can one choose the layout of the subspaces, or this is also learned? <sep> Moreover, and this is crucial, the proposed method is not clearly explained. Different concepts are discussed, but there is no summary and discussion of the proposed method as a whole. The reader must infer how the method works from the different pieces. <sep> When discussing the performance of different methods, and even if in the text the four different alternatives are clearly explained, in figure captions and legens the terminology changes (ISA-VAE, ISA-beta-VAE, beta-VAE, beta-ISA-VAE, etc). This makes the discussion very difficult to follow, as we do not understand which figures are comparable to which, and in which respect. <sep> In addition, there are other (secondary) questions that require an answer. <sep> S1) After (10) you mention the subspaces v_1,...v_l_o. What is the formal definition of these subspaces? <sep> S2) The definition of the distribution associated to ISA also implies that n_i,k = 1 for all i and k, right? <sep> S3) Could you please formally write the family of distributions, since applying this to a VAE is the main contribution of your manuscript? <sep> S4) Which parameters of this family are learned, and which of them are set in advance? <sep> S5) From Figure 4 and 5, I understand that the distributions used are of the type in (7) and not (10). Can you comment on this? <sep> S6) How is the Lp layout chosen? <sep> S7) Why the Lp layout for ISA-beta-VAE in Figure 5 is not the same as in Figure 4 for ISA-VAE? <sep> S8) What are the plots in Figure 4? They are difficult to interpret and not very well discussed. <sep> Finally, there are a number of minor corrections to be made. <sep> Abstract: latenT <sep> Equation (3) missig a sum over j <sep> Figure 1 has no caption <sep> In (8), should be f(z) and not x. <sep> Before (10), I understand you mean Lp-nested <sep> I did not find any reference to Figure 3 <sep> In 4.1, the standard prior and the proposed prior should be referred to with different notations. <sep> For all these reasons I recommend to reject the paper, since in my opinion it is not mature enough for publication.","The paper proposes to improve VAE by using a prior distribution that has been previously proposed for independent subspace analysis (ISA). The clarity of the paper could be improved by more clearly describing the proposed method and its implementation details. The originality is not that high, as the main change to VAE is replacing the usual isotropic Gaussian prior with an ISA prior. Moreover, the paper does not provide comparison to VAEs with other more sophisticated priors, such as the VampPrior, and it is unclear whether using the ISA prior makes it difficult to scale to high-dimensional observations. Therefore, it is difficult to evaluate the significance of ISA-VAE. The authors are encouraged to carefully revise their paper to address these concerns."
"abstract | weakness | suggestion  ==> =============================== <sep> I have read the authors' response and other reviewers' comments carefully. Thank you for taking great efforts to improve the paper, including providing additional results on human evaluation. (Btw, Table 1 and Table 2 are also much nicer now.) <sep> However, from the reviews it seems that all the reviewers agree that the novelty of this paper is limited, and the contribution is incremental.  I understand that this paper is the first and only work using adversarial framework for persona multi-turn conversation models. However, from the modeling perspective, I still think the novelty is limited. <sep> As a summary, I have updated the score from 4 to 5 to reflect the efforts that the authors have been taken to improve the paper. However, due to reasons above, I still prefer a rejection recommendation. <sep> =============================== <sep> Contributions: <sep> The main contribution of this paper is the proposed phredGAN, which is a persona-based GAN framework for multi-turn dialogue modeling. Specifically, a persona-based HRED generator is developed, with two different kinds of discriminator design. Experiments are conducted on both the UDC and the TV series transcript datasets. <sep> Weaknesses: <sep> (1) Novelty: I would say the novelty of this paper is rather limited. This paper heavily rely on the previous hredGAN work (Olabiyi et al., 2018), and extends it by injecting attributes into the system, borrowing ideas from the persona-based Seq2seq model (Li et al, 2016b). <sep> phredGAN_a is a straightforward extension of hredGAN, while phredGAN_d further introduces a collaborative discriminator that tries to predict the attribute that generated the input utterance. However, in summary, I think this paper is not novel enough. <sep> (2) Presentation: The paper is generally easy to follow and understand. However, I would say the paper is poorly written, and needs further polishing. For example, Table 1 & 2 are pretty ugly. <sep> (3) Evaluation: Generally, I think the experiments are not convincing and also not well-executed, with detailed comments listed below. <sep> Questions: <sep> (1) In phredGAN_a, as shown in Eqn. (4), the attribute is used as input of the discriminator, while in phredGAN_d, as shown in Eqn. (5) & (6), the attribute is used as the target of the discriminator. My question is: why not use the attribute as both input & output? That is, why not combine (4) & (6), instead of using (5) & (6)? Please clarify this. <sep> (2) In experiments, Section 3.1, the authors mention that the generator and the discriminator use a shared encoder. However, the generator and discriminator has a different role. Since the encoder is shared, then: in one step, we update the encoder to minimize the GAN objective, in the alternative step, we update the encoder again to maximize the GAN objective. So, how to deal with this conflicting role of encoder during the training? Please clarify this. <sep> (3) From Table 2, it seems that it is difficult to see that phredGAN is better than hredGAN. Can you provide some explanations here? <sep> (4) In Table 4, if the responses generated by hredGAN can be provided, that would be better to demonstrate the advantage of phredGAN. How does phredGAN compare with hredGAN qualitatively? <sep> (5) From Table 1 & 2, it seems to me there is no metric that is specifically designed to evaluate whether the model captures the attribute information. Is there a way to quantitatively evaluate this? For example, pretrain an attribute classifier, or use the collaborative discriminator in the phredGAN_d model to measure how the generated response reflect the attribute. If we can observe the performance of phredGAN is better than that of hredGAN, that would be helpful for the paper. <sep> (6) Since the task is challenging, and the automatic metrics designed for this task is not perfect, like other papers, I think human evaluation is essential and desired for this task. However, such human evaluation is lacked in this paper.","This work presents extensions of dialogue systems to simultaneously capture speakers' ""personas"" (in the framing of Li et al's work) and adapt to them. While the ideas are interesting, reviewers note that the incremental contribution compared to previous work is a bit too limited for *CONF*'s expectation, without being offset by strongly convincing experimental results. Authors are encouraged to incorporated their ideas into future submissions after having combined them with other insights to provide a stronger overall contribution."
"abstract | weakness | suggestion  ==> The paper analyzes the strategy that a visual question answering model (FiLM) uses to verify statements containing the quantifier ""most"" (""most of the dots are red""). It finds that the model is sensitive to the ratio of objects that satisfy the predicate (that are red) to objects that do not; as the ratio decreases (e.g. 10 red dots compared to 9 blue dots), the model's performance decreases too. This is consistent with human behavior. <sep> Strengths: <sep> * The introduction lays out an ambitious program of comparing humans to deep neural networks. <sep> * The experimental results are interesting (although of modest scope) and support the hypothesis that the network is not counting the objects but rather is using an approximation that is sensitive to the ratio between the red and non-red items. <sep> Weaknesses: <sep> * The architecture of the particular model is described very briefly, and at multiple points there's an implication that this is an investigation of ""deep learning models"" more generally, even though those models may vary widely. While the authors are using an existing model, they shouldn't assume that the reader has read the paper describing that model. I would like to see more discussion of whether it is at all plausible for this model to acquire the pairing strategy, compared to alternative VQA models (e.g., using relation networks). <sep> * I found it difficult to follow the theoretical motivation for performing the work. The goal seems to be to test whether the network is performing the task in way that ""if not human-like, at least is cognitively plausible"". I don't understand what is meant by cognitively plausible but not human-like; perhaps an example of a cognitively implausible mechanism would help clarify this issue. Later in the same paragraph, the authors argue that ""in the case of a human-centered domain like natural language, ultimately, some degree of comparability to human performance is indispensable"". This assertion is not justified, and seems surprising to me; we have very useful natural language processing systems that do not perform in a way that is comparable to humans (the hedge ""some degree of"" is really neither here nor there). In general, I don't understand why we would want a visual question answering system that returns approximate answers -- isn't it better to have it count exactly how many red dots there are compared to non-red dots? <sep> * The authors assume that explicit counting is not ""likely to be learned by the 'one-glance' feed-forward-style neural network"" evaluated in the paper. What is this statement based on? Why would a ""one-glance"" network have trouble counting objects? (What is a ""one-glance network""?) <sep> * Another vague concept that is used without clarification: it is argued that if the network implements something like the Approximate Number System, that shows that it can ""learn and utilize higher-level concepts than mere pattern matching"". What is ""pattern matching"" and how does it differ from ""higher-level concepts""? <sep> * Why would the pairing strategy in a neural network be affected by the clustering of the objects? I understand why a human who needs to saccade back and forth between the two groups of objects might lose track of the objects that have been paired so far, but I don't understand why that would affect the architecture in question. <sep> Minor comments: <sep> * Is the definition of ""most"" really a central piece of evidence for ""the apparent importance of a cardinality concept to human cognition""? Our ability to count seems sufficient to me. Perhaps I'm not understanding what the authors have in mind here. <sep> * Please use the terms ""interpretation"" and ""verification"" consistently. <sep> * ""One over the other strategy"" -> ""one strategy over the other"". <sep> * The paper is almost 9 pages long, but the contribution does not appear more substantial than a standard 8-page submission.","The paper studies an narrowly focused but interesting problem -- if the Visual Question answering model ""FILM"" from Perez et al (2018) is able to decide if ""most"" of the objects have a certain attribute or color. While the work itself is appreciate by the reviewers, concerns remain about the conclusion being limited in scope due to the synthetic nature of the data, and the analysis fairly narrow (a single model with a single very specific task). We encourage the authors to use reviewer feedback to make the manuscript stronger for a future deadline."
"abstract | strength | weakness | suggestion | rating_summary  ==>  ==> This paper has proposed algorithms for inferring reward functions from demonstrations with unknown biases. To achieve this, the authors have proposed to learn planners from demonstrations in multiple tasks via value iteration networks to learn the reward functions. <sep> This paper has provided an excellent motivation of their work in Sections 1 & 2 with references being made to human behaviors and heuristics, though the authors can choose a more realistic running example that is less extreme than making orthogonal decisions/actions. The paper is well-written, up till Section 4. <sep> On the flip side, there does not seem to be any significant technical challenges, perhaps due to some of the assumptions that they have made. Like the authors have mentioned, I do find assumption 3 to be overly strong and restrictive, as empirically demonstrated in Section 5.2. Arguably, is it really weaker than that of noisy rationality? At this moment, it is difficult to overlook this, even though the authors have argued that it may not be as restrictive in the future when more sophisticated differentiable planners are developed. <sep> The experimental results are not as convincing as I would have liked. In particular, Algorithm 2 (learning a demonstrator's model) does not seem to outperform that of assuming an optimal demonstrator for the noiseless case and a Boltzmann demonstrator for the noisy case (Fig. 3). This was also highlighted by the authors as well: ""The learning methods tend to perform on par with the best of two choices."" It begs the question whether  accounting for unknown systematic bias can indeed outperform the assumption of a particular inaccurate bias when we know a priori whether the demonstrations are noisy or not. <sep> Other detailed comments are provided below: <sep> I would have preferred that the authors present their technical formulations in Section 4 using the demonstrator's trajectories instead of policies. <sep> The authors say that ""In some cases, like they naive and sophisticated hyperbolic discounters, especially the noisy ones, the learning methods outperform both optimal and Boltzmann assumptions."" But, Fig. 3 shows that Algorithm 2 does not perform better than either that of the optimal or Boltzmann demonstrator. <sep> In Section 5.2, the authors have empirically demonstrated the poor approximate planning performance of VIN, as compared to an exact model the demonstrator. What then would its implications be on the adaptivity of Algorithms 1 and 2 to biases? <sep> The following references on IRL with noisy demonstration trajectories would be relevant: <sep> Benjamin Burchfiel, Carlo Tomasi, and Ronald Parr. Distance Minimization for Reward Learning from Scored Trajectories. In Proc. AAAI, 2016. <sep> J. Zheng, S. Liu, and L. M. Ni. Robust Bayesian inverse reinforcement learning with sparse behavior noise. In Proc. AAAI, 2014. <sep> Minor issues: <sep> On page 4, the expression D : W × R -> S -> A -> [0, 1] can be more easily understood with the use of parentheses. <sep> For Assumption 2b, you can italicize ""some"". <sep> In the first paragraph of section 4.1, what are you summing over? <sep> Line 3 of Algorithm 1: PI_W? <sep> Page 7: For the learning the bias setting? <sep> Page 7: figure 3 shows? <sep> Page 7: they naive? <sep> Page 7: so as long as? <sep> Page 8: adaption? <sep> Page 8: predicated? <sep> Page 8: figure 4 shows?","The authors study an inverse reinforcement learning problem where the goal is to infer an underlying reward function from demonstration with bias. To achieve this, the authors learn the planners and the reward functions from demonstrations. As this is in general impossible, the authors consider two special cases in which either the reward function is observed on a subset of tasks or in which the observations are assumed to be close to optimal. They propose algorithms for both cases and evaluate these in basic experiments. The problem considered is important and challenging. One issue is that in order to make progress the authors need to make strong and restrictive assumptions (e.g., assumption 3, the well-suited inductive bias). It is not clear if the assumptions made are reasonable. Experimentally, it would be important to see how results change if the model for the planner changes and to evaluate what the inferred biases would be. Overall, there is consensus among the reviewers that the paper is interesting but not ready for publication."
"abstract | strength | weakness | rating_summary | rebuttal_process | decision  ==> This paper introduces an approach to compressing a trained neural network by looking at the correlation of the filter responses in each layer. Two strategies are proposed: one based on trying to preserve the energy of the original activations and one based on looking at the KL divergence between the normalized eigenvalues of the activation covariance matrix and the uniform distribution. <sep> Strengths: <sep> - The KL-divergence-based method is novel and has the advantage of not requiring to define any hyper-parameter. <sep> - The results show the good behavior of the approach. <sep> Weaknesses: <sep> Method: <sep> - One thing that bothers me is the spatial max pooling of the activations of convolutional layers. This means that is two filters have high responses on different regions of the input image, they will be treated as correlated. I do not understand the intuition behind this. <sep> - In Section 2, the authors mention that other methods have also proposed to take the activation into account for pruning, but that they aim to minimize the reconstruction error of these activations. In fact, this is also what PFA-En does; for a given dimension, PCA gives the representation that minimizes the reconstruction error. Therefore, the connection between this method and previous works is stronger than claimed by the authors. <sep> - While it is good that the KL-divergence-based method does not rely on any hyper-parameter, the function \\psi used in Eq. 3 seems quite ad hoc. As such, there has also been some manual tuning of the method. <sep> Experiments: <sep> - In Table 1, there seems to be a confusion regarding how the results of FGA are reported. First, in (Peng et al., 2018), the %FLOPS is reported the other way around, i.e., the higher the better, whereas here the lower the better. Similarly, in (Peng et al., 2018), a negative \\Delta in accuracy means an improved performance (as stated in the caption of their Table 2, where the numbers reported here were taken). As such, the numbers reported here, and directly taken from this work, are misinterpreted. <sep> - Furthermore, Peng et al., 2018 report much better compression results, with %FLOP compression going up to 88.58%. Why are these results not reported here? (To avoid any misunderstanding, I would like to mention that I am NOT an author of (Peng et al., 2018)). <sep> - Many of the entries in Table 1 are empty due to the baselines not reporting results on these datasets or with the same network. This makes an actual comparison more difficult. <sep> - Many compression methods report results on ImageNet. This would make this paper more convincing. <sep> - While I appreciate the domain adaptation experiments, it would be nice to see a comparison with Masana et al., 2017, which also considers the problem of domain adaptation with network compression and, as mentioned in Section 2, also makes use of the activations to achieve compression. <sep> Related work: <sep> - It is not entirely clear to me why tensor factorization methods are considered being so different from the proposed approach. In essence, they also perform structured network pruning. <sep> - The authors argue that performing compression after having trained the model is beneficial. This is in contrast with what was shown by Alvarez & Salzmann, NIPS 2017, where incorporating a low-rank prior during training led to higher compression rates. <sep> - The authors list (Dai et al., 2018) as one of the methods that aim to minimize the reconstruction error of the activations. Dai et al., 2018 rely on the mutual information between the activations in different layers to perform compression. It is not entirely clear to me how this relates to reconstruction error. <sep> Summary: <sep> I do appreciate the idea of aiming for a hyper-parameter-free compression method. However, I feel that there are too many points to be corrected or clarified and too many missing experiments for this paper to be accepted to *CONF*. <sep> After Response: <sep> I appreciate the authors' response, which clarified several of my concerns. I would rate this paper as borderline. My main concern now is that the current comparison with existing method still seems too incomplete, especially with ResNet architectures, to really draw conclusions. I would therefore encourage the authors to revise their paper and re-submit it to an upcoming venue.","The authors propose a technique for compressing neural networks by examining the correlations between filter responses, by removing filters which are highly correlated. This differentiates the authors' work from many other works which compress the weights independent of the task/domain. <sep> Strengths: <sep> Clearly written paper <sep> PFA-KL does not require additional hyperparameter tuning (apart from those implicit in choosing \\psi) <sep> Experiments demonstrate that the number of filters determined by the algorithm scale with complexity of the task <sep> Weaknesses: <sep> Results on large-scale tasks such as Imagenet (subsequently added by the authors during the rebuttal period) <sep> Compression after the fact may not be as good as training with a modified loss function that does compression jointly <sep> Insufficient comparisons on ResNet architectures which make comparisons against previous works harder <sep> Overall, the reviewers were in agreement that this work (particularly, the revised version) was close to the acceptance threshold. In the ACs view, the authors addressed many of the concerns raised by the reviewers in the revisions. However, after much deliberation, the AC decided that the weaknesses 2, and 3 above were significant, and that these should be addressed in a subsequent submission."
"abstract | strength | weakness | rating_summary | decision  ==>  ==> The paper addresses the incremental few-shot learning problem where a model starts with base network and then introduces the novel classes, building a connection between novel and base classes via an attention module. <sep> Strengths: <sep> + clear writing. <sep> + the experiments are compared with related work and the ablation studies can verify the effectiveness of the proposed (or ""introduced"" would be a precise term) recurrent BP. <sep> Weakness: <sep> - [Novelty] <sep> The paper title is called attention attractor network, which shares very relevance to previous CVPR work (Gidaris & Komodakis, 2018). So the first thing I was looking for is the clear description of the difference between these two. Unfortunately, in related work, authors mention the CVPR work without stating the difference (last few lines in Section 2). As such, I don't see much novelty in the paper compared with previous work. Eqn. (7)-(10) explicitly describes the attention formula. What's the distinction from the CVPR work? <sep> - [Motivation of the regularizer using Recurrent BP is not clear] <sep> The use of recurrent BP is probably the most distinction from previous work. However, I don't see a clear description on why such a technique is necessary. <sep> Starting from the first line in Section 3.3, ""since there is no closed-form of the regularizer in Eqn (13)"", E needs BPTT or the introduced recurrent BP. This part is simply a re-adaption of other algorithms. A very simple question is, how about use other regularizers to replace Eqn (13)? <sep> - [Some experiments missing] <sep> The experiments section 4.6 uses a case of None and ""best WD"" to address some of my concerns. This is good. Does the ""gamma random"" indicates only E is used without the ||W||^2? why the best WD for one-shot is zero? This implies the model is best for applying no weight decay? <sep> What's the effect of using the recurrent BP technique to the CVPR work? Is there some similar improvement? If yes, then the paper makes some contribution by the regularization. If not, what's the reason? <sep> How about using the truncated BPTT with a larger T? <sep> In general, I think the recurrent BP part should be the highlight of the paper and yet authors fail to spread such a spirit in the abstract or title. And there are some experiments missed as I mentioned above.","This paper proposes an approach for incremental learning of new classes using meta-learning. <sep> Strengths: The framework is interesting. The reviewers agree that the paper is well-written and clear. The experiments include comparisons to prior work, and the ablation studies are useful for judging the performance of the method. <sep> Weaknesses: The paper does not provide significant insights over Gidaris & Komodakis '18. Reviewer 1 was also concerned that the motivation for RBP is not entirely clear. <sep> Overall, the reviewers found that the strengths did not outweigh the weaknesses. Hence, I recommend reject."
"strength | weakness | rating_summary | decision  ==>  ==> ========= Summary ========= <sep> The authors propose a novel method for counterfactual inference (i.e. individual/heterogeneous treatment effect, as well as average treatment effect) with neural networks. They perform propensity score matching within each minibatch in order to match the covariate distributions during training, which leads to a doubly robust model. <sep> PM is evaluated on several standard semi-synthetic datasets (jobs, IHDP, TCGA) and PM shows state-of-the-art performance on some datasets, and overall looks quite promising. <sep> ======= Comments ======= <sep> The paper is well-written, presents a novel method of some interest to the community, and shows quite good performance across a range of relevant benchmarks. <sep> I have one major issue with this work: I don't see why propensity-score matching *within* a minibatch should provide a substantial improvement over propensity-score matching across the dataset (Ho et al 2011). I find the cursory explanation given (""it ensures that every gradient step is done in a way that is approximately unbiased"") unconvincing, since (a) proper SGD training should be robust to per-batch biases during training (the expected loss is identical for both methods, correct?), and (b) biases should go away in the limit of large batch sizes. If indeed SGD required unbiased *minibatches* then standard minibatch SGD wouldn't work at all. <sep> Looking at the experimental details in the appendix, it appears that the MatchIt package was used to do PSM, rather than a careful comparison under the same conditions. Are the exact matching procedure, PS estimator model, choosing ""one of 6 closest  matches by propensity score"", batch size, etc. the same between your PM implementation and MatchIt? I'd be very curious to see the results of a controlled comparison between Alg S1 and S2 under the same conditions (i.e. run your PM implementation on the whole dataset), and perhaps even some more clever experiments illustrating why matching within a minibatch is important. <sep> Another hypothesis for why PM is better than PSM is that the matching distribution for PM changes at each epoch (at least due to the randomization among the 6 closest matches). Could it be that the advantage of PM is that it actually provides a randomized rather than constant distribution of matched points? <sep> Can the authors provide more motivation for why PM should outperform PSM? Or some more careful comparison of these methods isolating the benefits of PM? I think a convincing justification and comparison here could change my opinion, as I like the paper otherwise. Thanks! <sep> Detailed Comments: <sep> - There is insufficient explanation of the PM method in the main text. The method is only mentioned in a single sentence buried in the middle of a long paragraph ""In PM, we match every sample within a minibatch..."". This should be made more clear, e.g. by moving Algorithm S1 to the main text. <sep> - The discussion on Model Selection and the argument for nearest-neighbor PEHE is clever and well-supported by the experiments. <sep> - In Table 3 and 4, it's not clear which numbers are reported by the original authors and which were replicated by the authors.","The reviewers found the paper to be well written, the work novel and they appreciated the breadth of the empirical evaluation. However, they did not seem entirely convinced that the improvements over the baseline are statistically significant. Reviewer 1 has lingering concerns about the experimental conditions and whether propensity-score matching within a minibatch would provide a substantial improvement over propensity-score matching across the dataset. Overall the reviewers found this to be a good paper and noted that the discussion was illuminating and demonstrated the merits of this work and interest to the community. However, no reviewers were prepared to champion the paper and thus it falls just below borderline for acceptance."
"abstract | strength | weakness | decision  ==>  ==> This paper seeks to understand the AlphaGo Zero (AGZ) algorithm and extend the algorithm to regular sequential decision-making problems. Specifically, the paper answers three questions regarding AGZ: (i) What is the optimal policy that AGZ is trying to learn? (ii) Why is cross-entropy the right objective? (iii) How does AGZ extend to generic sequential decision-making problems? This paper shows that AGZ's optimal policy is a Nash equilibrium, the KL divergence bounds distance to optimal reward, and the two-player zero-sum game could be applied to sequential decision making by introducing the concept of robust MDP. Overall the paper is well written. However, there are several concerns about this paper. <sep> In fact, the key results obtained in this paper is that minimizing the KL-divergence between the parametric policy and the optimal policy (Nash equilibrium) (using SGD) will converge to the optimal policy. It is based on a bound (2), which states that when the KL-divergence between a policy and the optimal policy goes to zero then the return for the policy will approach that of the optimal policy. This bound is not so surprising because as long as certain regularity condition holds, the policies being close should lead to the returns being close. Therefore, it is an overclaim that the KL-divergence bound (2) provides an immediate justification for AGZ's core learning algorithm. As mentioned earlier, the actual conclusion in Section 4.2 is that minimizing the KL-divergence between the parametric policy and the optimal policy by SGD will converge to the optimal policy, which is straightforward and is not what AlphaGo Zero does. This is because there is an important gap: the MCTS policy is not the same as the optimal policy. The effect of the imperfection in the target policy is not taken into account in the paper. A more interesting question to study is how this gap affect the iterative algorithm, and whether/how the error in the imperfect target policy accumulates/diminishes so that iteratively minimizing KL-divergence with imperfect \\pi* (by MCTS) could still lead to optimal policy (Nash equilibrium). <sep> Furthermore, the robust MDP view of the AGZ in sequential decision-making problems is not so impressive either. It is more or less like a reformulation of the AGZ setting in the MDP problem. And it is commonly known that two-player zero-sum game is closely related to minimax robust control. Therefore, it cannot be called as ""generalizing AlphaGo Zero"" as stated in the title of the paper.","This work examines the AlphaGo Zero algorithm, a self-play reinforcement learning algorithm that has been shown to learn policies with superhuman performance on 2 player perfect information games. The main result of the paper is that the policy learned by AGZ corresponds to a Nash equilibrium, that and that the cross-entropy minimization in the supervised learning-inspired part of the algorithm converges to this Nash equillibrium, proves a bound on the expected returns of two policies under the and introduces a ""robust MDP"" view of a 2 player zero-sum game played between the agent and nature. <sep> R3 found the paper well-structured and the results presented therein interesting. R2 complained of overly heavy notation and questioned the applicability of the results, as well as the utility of the robust MDP perspective (though did raise their score following revisions). <sep> The most detailed critique came from R1, who suggested that the bound on the convergence of returns of two policies as the KL divergence between their induced distributions decreases is unsurprising, that using it to argue for AGZ's convergence to the optimal policy ignores the effects introduced by the suboptimality of the MCTS policy (while really interesting part being understanding how AGZ deals with, and whether or not it closes, this gap), and that the ""robust MDP"" view is less novel than the authors claim based on the known relationships between 2 player zero-sum games and minimax robust control. <sep> I find R1's complaints, in particular with respect to ""robust MDPs"" (a criticism which went completely unaddressed by the authors in their rebuttal), convincing enough that I would narrowly recommend rejection at this time, while also agreeing with R3 that this is an interesting subject and that the results within could serve as the bedrock for a stronger future paper."
"abstract | weakness | rebuttal_process | rating_summary | decision  ==>  ==> Summary: This paper argues that improved resistance to adversarial attacks can be achieved by an implicit denoising method in which model weights learned during adversarial training are encouraged to stay close to a set of reference weights using the ell_2 <sep> penalty. Additionally, the authors claim that by introducing an attention model which focuses the model training on more robust features they can further improve performance. Some experiments are provided. <sep> Feedback: My main concerns with the paper are: <sep> * The experimental section is fairly thin. There are at this point a large number of defense methods, of which Madry et al. is only one. In light of these, the experimental section should be expanded. The results should ideally be reported with error bars, which would help in gauging significance of the results. <sep> * The differential impact of the two contributions is not entirely clear. The results in Table 1 suggest that implicit denoising can help, yet at the same time, Table 2 suggests that Black-box performance is better if we just use the attention model. Overall, <sep> this conflates the contributions unnecessarily and makes it hard to distingish their individual impact. <sep> * The section on gradient maps is not clear. The authors argue that if the gradient map aligns with the image the model depends solely on the robust features. While this may be (somewhat more) intuitive in the context of simple GLMs, it's not clear why it should carry over to DNNs. I think it would help to make these intuitions much more precise. Secondly, even if this were the case, the methodology of using a neural net to classify gradient maps and from this derive a robustness metric raises precisely the kinds of robustness questions that the paper tries to answer. I.e.: how robust is the neural net classifying the gradient images, and how meaningful are it's predictions when gradient maps deviate from ""clean"" images. <sep> Overall, I feel this paper has some potentially interesting ideas, but needs additional work before it is ready for publication.","The paper proposes an attention mechanism to focus on robust features in the <sep> context of adversarial attacks. Reviewers asked for more intuition, more <sep> results, and more experiments with different attack/defense models. Authors <sep> have added experimental results and provided some intuition of their proposed <sep> approach. Overall, reviewers still think the novelty is too thin and recommend <sep> rejection. I concur with them."
"rating_summary | suggestion  ==>  ==> The authors propose mean rescaled confidence interval (MERCI) as a way to measure the quality of predictive uncertainty for regression problems. The main idea is to rescale confidence intervals, and use average width of the confidence intervals as a measure for calibration. Due to the rescaling, the MERCI score is insensitive to the absolute scale; while this could be a feature in some cases, it can also be problematic in applications where the absolute scale of uncertainty matters. <sep> Overall, the current draft feels a bit preliminary. The current draft misses discussion of other relevant papers, makes some incorrect claims, and the experiments are a bit limited. I encourage the authors to revise and submit to a different venue. <sep> There's a very relevant ICML 2018 paper on calibrating regression using similar idea: <sep> Accurate Uncertainties for Deep Learning Using Calibrated Regression https://arxiv.org/pdf/1807.00263.pdf <sep> Can you clarify if/how the proposed work differs from this? I'd also like to see a discussion of calibration post-processing methods such as Platt scaling and isotonic regression. <sep> The paper unfairly dismisses prior work by making factually incorrect claims, e.g. Section 2 claims <sep> ""Indeed, papers like (Hernandez-Lobato & Adams, 2015; Gal & Ghahramani, 2016; Lakshminarayanan et al., 2017; Kendall & Gal, 2017) propose quantitative evaluations on several datasets, those classically used for evaluating the task, but only compare their average test performances in terms of RMSE. It is the quality of the prediction which is measured, and not the quality of the estimated uncertainty. They also show some qualitative results, where maps of the estimated uncertainty are displayed as images and visually evaluated. Yet, to the best of our knowledge, the literature on deep neural networks does not propose any method for the quantitative evaluation of the uncertainty estimates."" <sep> This is incorrect.  To just name a few examples of prior work quantitatively evaluating the quality of uncertainty: (Hernandez-Lobato & Adams, 2015) and (Gal & Ghahramani, 2016) report log-likelihoods on regression tasks, (Lakshminarayanan et al. 2017) report log-likelihoods and Brier score on classification and regression tasks. There are many more examples. <sep> The experiments are a bit limited. Figure 1 is a toy dataset and Table 2 / Figure 4 focus on a single test case which does not seem like a fair comparison of the different methods. The authors should at least compare their method to other work on the UCI regression benchmarks used by (Hernandez-Lobato & Adams, 2015).",Reviewers are in a consensus and recommended to reject after engaging with the authors. Please take reviewers' comments into consideration to improve your submission should you decide to resubmit.
"strength | weakness | misc  ==> This paper tests various pretraining objectives (language modeling, machine translation, skip-thought, and autoencoding) on two syntactic tasks: POS tagging and CCG tagging. It finds that language modeling outperforms the other pretraining objectives; additionally, randomly-initializing an encoder achieves decent performance when given a large amount of labeled data for the tagging task. The experiments in this paper are very thorough and explained well. By controlling for pretraining data size, the authors are able to reasonably claim that language modeling is superior to translation as a syntactic transfer learning task. On the other hand, I have some concerns regarding the significance of the paper's contributions, and as such I am borderline on its acceptance. <sep> comments: <sep> - the experiments in the paper feel biased towards language modeling. Language modeling is the only token-level prediction task of the four objectives here, but both of the two downstream tasks are at the token level. It is perhaps unsurprising then that language modeling performs best; perhaps the authors could have considered some sentence-level downstream tasks as well to properly control for this? Or added some more word-level pretraining objectives? <sep> - sort of relatedly, the authors do not provide any explanations as to *why* language modeling is a better pretraining objective than translation. What kinds of examples do the tagging models using LM pretraining get right that the translation models do not? Such an analysis could help provide more concrete insights into what kind of information each objective is encoding. <sep> - the claim that LMs > translation is not a new finding. The authors cite Blevins et al, who find the same result on the task of dependency arc prediction. Similarly, the surprisingly good performance of random encoders was also found in Conneau et al., ACL 2018. As the main contribution of this paper seems to be a more controlled study of Blevins et al on different syntactic tasks, I don't think there is enough here for an *CONF* submission. <sep> - what is the effect of the specific dataset and architecture on the results? Here we just look at a couple translation datasets (all news data) and LSTM models. Do things change when we move to transformers or more diverse domains?","Strengths: <sep> -- Solid experiments <sep> -- The paper is well written <sep> Weaknesses: <sep> -- The findings are not entirely novel and not so surprising, previous papers (e.g., Brevlins et al (ACL 2018)) have already <sep> suggested that LM objectives are preferable and also using LM objective for pretraining is already the standard practice (see details in R1 and R3). <sep> There is a consensus between the two reviewers who provided detailed comments and engaged in discussion with the authors."
"abstract | weakness | decision  ==> This paper studies the convergence of a primal-dual algorithm on a certain min-max problem and experimentally shows that it works in GANs and multi-task learning. <sep> This paper is clear and well-written. The convergence guarantee looks neat, and convergence to stationary points is a sensible thing on non convex-concave problems. I am not super familiar with the literature of saddle-point optimization and may not have a good sense about the significance of the theoretical result. <sep> My main concern is that the assumptions in the theory are rather over-restrictive and it's not clear what intuitions or new messages they bring in for the practice of GANs. The convergence theorem requires the maximization problem (over discriminators) to be strictly concave. On GANs, this assumption is not (near) satisfied beyond the simple case of the LQG setting (PSD quadratic discriminators). On the other hand, the experiments on GANs just seem to say that the algorithm works but not much more beyond that. There is a brief discussion about the improvement in time consumption but it doesn't have a report a quantitative comparison in the wall time. <sep> On multi-task learning, the proposed algorithm shows improvement over the baseline. However it is also unclear whether it is the *formulation* (12) that brings in the improvement, or it is the actual primal-dual *algorithm*. Perhaps it might be good to try gradient descent on (12) and see if it also works well. <sep> In general, I would recommend the authors to have a more convincing demonstration of the strength of this algorithm over baseline methods on min-max problems, either theoretical or empirical.","The paper studies the convergence of a primal-dual algorithm on a special min-max problem in WGAN where the maximization is with respect to linear variables (linear discriminator) and minimization is over non-convex generators. Experiments with both simulated and real world data are conducted to show that the algorithm works for WGANs and multi-task learning. <sep> The major concern of reviewers lies in that the linear discriminator assumption in WGAN is too restrictive to general non-convex mini-max saddle point problem in GANs. Linear discriminator implies that the maximization part in min-max problem is concave, and it is thus not surprise that under this assumption the paper converts the original problem to a non-convex optimization instance and proves its first order convergence with descent lemma. This technique however can't be applied to general non-convex saddle point problem in GANs. Also the experimental studies are also not strong enough. Therefore, current version of the paper is proposed as borderline lean reject."
"abstract | weakness | suggestion  ==>  ==> Summary: this submission proposes a modification of neural network architectures that allows the modulation of activation functions of a given layer as a function of the activations in the previous layer. The author provide different version of their approach adapted to CNN, DenseNets and LSTM, and show it outperforms a vanilla version of these algorithms. <sep> Evaluation: In the classical context of supervised learning tasks investigated in this submission, it is unclear to me what could be the benefit of introducing such ""modulators"", as vanilla ANNs already have the capability of modulating the excitability of their neurons. Although the results show significant, but quite limited, improvements with respect to the chosen baseline, more extensive baseline comparisons are needed. <sep> Details comments: <sep> 1. Underlying principles of the approach <sep> It is unclear to me why the proposed approach should bring a significant improvement to the existing architectures. First, from a neuroscientific perspective, neuromodulators allow the brain to go through different states, including arousal, sleep, and different levels of stress. While it is relatively clear that state modulation has some benefits to a living system, it is less so for an ANN focused on a supervised learning task. Why should the state change instead of focusing on the optimal way to perform the task? If the authors want to use a neuroscientific argument, I would suggest to elaborate based on the precise context of the tasks they propose to solve. <sep> In addition, as mentioned several times in the paper, neuromodulation is frequently associated to changes in cell excitability. While excitability is a concept that can be associated to multiple mechanisms, a simple way to model changes in excitability is to modify the threshold that must be reached by the membrane potential of a given neuron in order for the cell to fire. Such simple change in excitability can be easily implemented in ANNs architectures by affecting one afferent neuron in the previous layer to the modification of this firing threshold (simply adding a bias term). As a consequence, if there is any benefit to the proposed architecture, it is very likely to originate specifically from the multiplicative interactions used to implement modulation in this paper. However, approximation of such multiplicative interactions can also be implemented using multiple layers network equipped with non-linear activations. Overall, it would be good to discuss these aspects in great detail in the introduction and/or discussion of the paper, and possibly find a more convincing justification for the approach. <sep> 2. Weak baseline comparison results <sep> In the CNN experiments, modulated networks are only compared with a single vanilla counterpart equipped with ReLu. There are at least two obvious additional baseline comparison that would be useful: what if the Re-Lu activations are replaced with fixed sigmoids? And what if batch-normalization is switched on/off (I could not find whether it was used at all). Indeed it, we should exclude benefits that are simply due to the non-linearity of the sigmoid, and batch normalization also implements a form of modulation at training that may provide benefits equivalent to modulation (or on the contrary, batch norm could implement a modulation in the wrong way). It would be better to look at all possible combinations of these architecture choices. <sep> Due to lack of details in the paper and my personal lack of expertise in LSTMs, I will not comment on baselines for that part but I assume similar modifications can be done. <sep> Overall, given the weak improvements in performance, it is questionable whether this extra degree of complexity should be added to the architecture. Additionally, I could not find the precise description of the statistical tests performed. Ideally, the test, the number of samples, the exact p-value, and whether the method of correction for multiple comparison should be included each time a p-value is mentioned.","The paper adds a new level of complexity to neural networks, by modulating activation functions of a layer as a function of the previous layer activations. The method is evaluated on relatively simple vision and language tasks. <sep> The idea is nice, but seems to be a special case of previously published work; and the results are not convincing. Four of five reviewers agree that the work would benefit from: improving comparisons with existing approaches, but also improving its theoretical framework, in light of competing approaches."
"abstract | weakness | decision  ==> The paper presents a new Generative Adversarial Network (GAN) for learning a target distribution that is defined as the difference between two other distributions. Applications in semi-supervised learning and adversarial training are considered in the experimental evaluation and results are presented in computer vision tasks. <sep> The paper is not very well written and can be hard to follow. One very important issue for me was motivation for defining the target distribution as a difference between two other distributions. I am not familiar with this area, but reading through the introduction it was never clear to me why this is a useful scenario, <sep> in practice. Furthermore, some statements in the introduction felt quite arbitrary. For example, the authors state that PixelCNN ""does not have a latent representation"" in a manner that makes it sound as if that is a bad thing. If indeed it is, then why so? It would be very helpful to motivate the setting more and to provide a couple of examples of where this method would be useful, in the introduction. Also, regarding the MNIST example in the end of page 1, what is the ""universal set""? This paragraph also felt a bit arbitrary and unclear. <sep> Some comments about the rest of the paper: <sep> - The theoretical results of section 3 are just stated/listed, but are not connected to algorithm 1. Please connect them to the different parts of the algorithm and state in a couple sentences what they imply for the algorithm. <sep> - Right after theorem 1, which assumption are you referring to when you say <sep> ""the assumption in Theorem 1""? <sep> - The reformulation of section 3.1 is never justified. What led you to use this reformulation and why do you think it is more stable in practice? <sep> - You should mention in the caption of table 4, what quantity you are computing. <sep> Note that my evaluation for this paper is based mainly on the way it is written as, in its current state, it is hard for me to judge what is novel and what is useful, and what readers are supposed to take in by reading this paper. The main question that the paper definitely needs to answer, but does not do so currently <sep> (in my opinion) is: <sep> When is this method useful to readers? For solving which problems and under what conditions? And also, when is this method bad and should not be used? <sep> == Experiments == <sep> Section 5.1 is hard to follow and I don't quite get how it connects to the rest. <sep> Also, in section 5.1.2 you mention that in comparison to Dai et al. (2017) your method does not need to rely on an additional density estimation network. Even if that is true, I cannot see how it is a useful remark given that the method of <sep> Dai et al. seems to always beat your method. <sep> == Style == <sep> In figure 1, no labels or legends are provided making it hard to figure out what's going on at a glance. It would be very helpful to include labels and a legend. <sep> Equation 2 is not written correctly. The equals sign only refers to ""V(G, D)"" <sep> and not the min-max of that, right? Please make that explicit by first defining <sep> ""V(G, D)"" alone.","The paper presents a GAN for learning a target distribution that is defined as the difference between two other distributions. <sep> The reviewers and AC note the critical limitation of novelty and appealing results of this paper to meet the high standard of *CONF*. <sep> AC thinks the proposed method has potential and is interesting, but decided that the authors need more works to publish."
"abstract | weakness | suggestion | rebuttal_process | suggestion  ==>  ==> This problem of interest in this paper is Curriculum Learning (CL), in the context of deep learning in particular. CL refers to learning a non-random order of presenting the training examples to the learner, typically with easier examples presented before difficult ones, to guide learning more effectively. This has been shown to both speed up learning and lead to better generalization, especially for more challenging problems. In this paper, they claim that their contribution is to decompose the problem of CL into learning two functions: the scoring function and the pacing function, with the role of the former being to estimate the difficulty of each training example and the latter to moderate the schedule of presenting increasingly more challenging examples throughout training. <sep> Overall, I found it hard to understand from reading the paper what exactly is new versus what is borrowed from previous work. In particular, after reading Weinshall et al, I realized that they have already proposed a number of things that are experimented with here: 1) they proposed the approach of transfer learning from a previously-trained network as a means of estimating the 'scoring function'. 2) they also distinguish between learning to estimate the difficulty of examples, and learning the schedule of decreasing difficulty throughout learning, which is actually stated here as the contribution of this paper. In particular, in Section 3 of Weinshall et al, there is a sub-section named ""scheduling the appearance of training examples"" where they describe what in the terminology of this paper would be called their pacing function. They experiment with two variants: fixed, and adaptive, which are very similar to two of the pacing functions proposed here. <sep> Bootstrapping: <sep> A component of this work that didn't appear in Weinshall et al, is the bootstrapping approach to estimating the scoring function. In general, this involves using the same network that is being trained on the task to estimate the difficulty of the training examples. The authors explain that there are two ways to do this: estimate how easy each training example is with respect to the 'current hypothesis' (the weights of the network at the current step), and with respect to the 'final hypothesis', which they estimate if I understand correctly as the network at the end of training. The latter would necessitate first training the network in the standard way, and then using it to estimate how easy or hard each example is, and using those estimates to re-train the network from scratch using that curriculum. They refer to the former as self-paced learning and to the latter as self-taught learning. I find these names confusing in that they don't really convey what the difference is between the two. Further, while self-paced learning has been studied before (e.g. Kuman et al), I'm not sure about self-taught learning. Is this a term that the authors here coined? If not, it would be useful to add a reference. <sep> Using easy / hard examples as judged by the current / final hypothesis: <sep> When using the current hypothesis, under some conditions, Weinshall et al showed that choosing harder examples is actually more beneficial than easy examples, similar in spirit to hard negative mining. On the other hand, when using the final hypothesis to estimate examples' difficulty, using a schedule of increasing difficulty is beneficial. Based on this, I have two comments: 1) It would therefore be useful to implement a version that uses the current hypothesis to estimate how easy each example is (like the self-paced scoring function) but then invert these estimates, in effect choosing the most challenging instead of the easiest ones as is done for anti-curriculum learning. This would be a hybrid between the current self-paced scoring function and anti-curriculum scoring function that would essentially implement the hard negative mining technique in this context. 2) It would be useful to comment on the differences between the self-paced scoring function used here, and that in Kumar et al. In particular, in this case using a curriculum based on this scoring function seems to harm training but in Kumar et al, they showed it actually increased performance in a number of different cases. Why does one work but the other doesn't? <sep> Experiments: <sep> The experiments are presented in a subset of 5 classes from CIFAR-10 (also used by Weinshall et al.), but also in the full CIFAR-10 and CIFAR-100 datasets. They used both a small CNN (same as in Weinshall et al) as well as a VGG architecture. Overall, their results are comparable to what was previously known: using a curriculum computed by transfer leads to improved learning speed and final performance (though sometimes very slightly) compared to the standard training, and the training with a random curriculum. Further, the benefit is larger when the task is harder (as measured by the final vanilla-trained performance). By computing the distances between the gradients obtained from using a curriculum (via the transfer scoring function) and no curriculum confirms that these two training setups indeed drive the learning in different directions; an analysis similar to Weinshall et al. Also, since, as was previously known and they also observe, the benefit of CL is larger at the beginning of training, they propose a single-step pacing function that performs similarly to other pacing functions while is simpler and more computationally effective. The idea is to decrease only once the proportion of easy examples used in mini-batches, via a step function. Therefore at the start many easy examples are used, and after this threshold is surpassed, few easy examples are used. <sep> Overall, I don't feel the contribution of this paper is large enough to recommend acceptance. The main points that guided this decision are: <sep> 1) The relationship with previous work is not clear. In particular, Weinshall et al seems to have already proposed a few components that are claimed to be the contribution of this paper, as elaborated on above. The authors should mention that the transfer scoring function was borrowed from Weinshall et al, clarify the differences between their pacing functions from those in Weinshall et al., etc. <sep> 2) The usefulness of using easy or hard experiments when consulting the current or final hypothesis is discussed but not explored sufficiently. An additional experiment is proposed above to add another 'data point' to this discussion. <sep> 3) self-paced learning is presented as something that doesn't work and wasn't expected to work. However, in the past successes were shown with this method, so it would be useful to clarify the difference in setup, and justify this difference. <sep> 4) It seems that the experiments resulted to similar conclusions to what was already known. While it's useful to confirm these findings on additional datasets, I didn't feel that there was a significant insight gained from them.","This paper presents an interesting strategy of curriculum learning for training neural networks, where mini-batches of samples are formed with a gradually increasing level of difficulty. <sep> While reviewers acknowledge the importance of studying the curriculum learning and the potential usefulness of the proposed approach for training neural networks, they raised several important concerns that place this paper bellow the acceptance bar: (1) empirical results are not convincing (R2, R3); comparisons on other datasets (large-scale) and with state-of-the-art methods would substantially strengthen the evaluation (R3); see also R2's concerns regarding the comprehensive study; (2) important references and baseline methods are missing – see R2's suggestions how to improve; (3) limited technical novelty -- R1 has provided a very detailed review questioning novelty of the proposed approach w.r.t. Weinshall et al, 2018. <sep> Another suggestions to further strengthen and extend the manuscript is to consider curriculum and anti-curriculum learning for increasing performance (R1). <sep> The authors provided additional experiment on a subset of 7 classes from the ImageNet dataset, but this does not show the advantage of the proposed model in a large-scale learning setting. <sep> The AC decided that addressing (1)-(3) is indeed important for understanding the contribution in this work, and it is difficult to assess the scope of the contribution without addressing them."
"abstract | strength | rebuttal_process | decision  ==>  ==> The authors present a novel algorithm for dealing with domain adaptation in the setting of federated learning (classification, specifically). That is, they tackle the issue of learning a model on a new domain when access to the data points used in training the source models is not possible due to privacy constraints. The approach uses the gradients of the source models, reweighed to account for the differing shifts between the different sources and the target domain, to fit the model on the target domain. <sep> The authors motivate their approach by providing a novel bound on the generalization error of transfer learning when the hypothesis function used on the target domain is a convex combination of hypotheses fitted on multiple source domains. This bound shows that the weighted sum of divergences in the symmetric difference hypothesis space controls the generalization error, so the authors aim at deriving feature representations and using aggregation weights that ensure this weighted sum is small. <sep> The authors use a novel dynamic attention model to get the aggregation weights: they cluster the features in the target domain, and measure how much the intra-cluster variation decreases when information from a given source domain is incorporated. The aggregation weights for the model updates on the target domain are then weighed using a softmax transform of these contribution weights. <sep> The motivation up through and including section 3 is clear, the theoretical results are presented clearly, but the model details in section 4 are unclear: <sep> -  In the dynamic attention mechanism, how does one a priori choose the number of clusters in computing the gap statistics, and what is the impact? <sep> - the notation in the federated adversarial alignment section is unclear: what *exactly* are the model coefficients Theta that are being updated? <sep> - the statement ""optimize following objective"" is made several times. this is ambiguous, and should be corrected to ""miminize"" following objective. <sep> - the representation disentanglement process is intricate, and only vaguely addressed. how does one fit the neural net and use (8)? where is the l2 reconstruction loss balanced with the mutual information? the vagueness of this section means Algorithm 1 is not well-specified. <sep> The experiments are reasonable, and compare to baseline domain adaptation methods. <sep> The problem considered is of interest, and the approach is novel and interesting. However, the algorithm is not described in sufficient detail. After reading the paper, and spending considerable time rereading section 4, I still do not understand how Algorithm 1 is implemented in practice. For that reason I lean towards reject. I will update my score if the authors clarify the details of Algorithm 1. <sep> Comments: <sep> - the symmetric difference hypothesis space is incorrectly called the HdeltaH divergence in section 3","This paper studies an interesting new problem, federated domain adaptation, and proposes an approach based on dynamic attention, federated adversarial alignment, and representation disentanglement. <sep> Reviewers generally agree that the paper contributes a novel approach to an interesting problem with theoretical guarantees and empirical justification. While many professional concerns were raised by the reviewers, the authors managed to perform an effective rebuttal with a major revision, which addressed the concerns convincingly. AC believes that the updated version is acceptable. <sep> Hence I recommend acceptance."
"abstract | strength | decision  ==> The paper focuses on sparse neural networks. Typically, l1 regularization is the go-to strategy, however, it is not scale invariant. That is, all weights are affected by the regularization, not only those that are being driven to 0. l0 regularization is theoretically optimal, however, it is not smooth and has no gradients almost everywhere, so it cannot be used for training. As a compromise the paper proposes Hoyer regularization, that is the l1/l2 ratio. The Hoyer regularization has the same minima structure and leads to sparse solutions while being scale invariant, that is it does not affect all weights in the process. Additionally, the paper proposes structured Hoyer regularization. Last, it employs the said regularizations in deep networks: LeNet, AlexNet and ResNet on several datasets: MNIST, CIFAR, ImageNet. <sep> Strengths: <sep> + The described method is simple, intuitive and straightforward. By applying the said regularization (~ Σ |w_i|/sqrt(Σ w_i^2)), one arrives at seemingly sparser solutions, which is verified in practice. <sep> + The experiments are extensive and convincing. I particularly like that the authors have used their method with complex and deep models like ResNets, on large scale datasets like ImageNet. <sep> + The presentation is generally clear and one can understand the paper straightaway. <sep> Weaknesses: <sep> + The contributions of the paper are rather on the thin side. At the end of the day, Hoyer regularization is taken from another field (compressed sensing) and applied on deep networks. This is also witnessed by some moderate repetition in the writing, e.g., between the introduction and the related work. <sep> + There are some points where the paper becomes unclear. For instance, in Figure 3 what are the ""other methods""? <sep> + In Figure 1 it is explained that the Hoyer regularization leads to minima along the axis. The gradients then push the models ""rotationally"". Could this lead to bad multiple local optimal problems? Is there any guarantee that any particular axis will generate better solutions than the other? <sep> All in all, I would recommend for now weak accept. I find the work interesting and solid, although not that exciting.","The authors propose a scale-invariant sparsity measure for deep networks. The experiments are extensive and convincing, according to reviewers. I recommend acceptance."
"abstract | rating_summary | suggestion  ==> [Summary] <sep> This paper proposes Prox-SGD, a theoretical framework for stochastic optimization algorithms that (1) incorporates momentum and coordinate-wise scaling as in Adam, and (2) can handle constraint and (non-smooth) regularizers through the proximal operator. With proper choices of hyperparameters, the algorithm is shown to converge asymptotically to stationarity, for smooth non-convex loss + convex constraint/regularizer. The algorithm is empirically tested on training binary and sparse neural nets on MNIST and CIFAR-10. <sep> [Pros] <sep> The theoretical framework that incorporates most of the commonly used tweaks in stochastic optimization for deep learning, and a convergence result that establishes broadly the asymptotic convergence to stationarity. <sep> [Cons] <sep> The result of Theorem 1 sounds rather a straightforward application of classical results; important sub-cases such as Adam violates the assumption (Adam has \\rho_t = \\rho so \\sum \\rho_t^2 = \\infty) and thus are not contained in this case. From this it seems to me Theorem 1 says things mostly about the ""easier"" sub-cases, and thus is perhaps not very surprising and a bit limited in bringing in new messages. <sep> The experiments are mostly done on simple problems --- 3 of the 4 figures are on MNIST. The specific tasks (training sparse / binary neural nets with MLP / vanilla CNN architectures) considered in the experiments are all very extensively studied in prior work, and the results in this paper says at most that the proposed Prox-SGD works for these tasks. <sep> Overall, I like the idea in this paper that we can put together a unified framework for stochastic optimization algorithms and incorporate things like momentums and regularizations that were previously treated separately. However, beyond proposing such a framework, it seems that contributions on both the theoretical and empirical side are a bit limited at this point. <sep> *** <sep> Thank the authors for the response and the efforts in revising the paper. I am glad to see the additional experiments for training sparse networks on CIFAR-100 (a much harder task than MNIST and also CIFAR-10) in which the proposed method works well. This largely resolved my concerns on the experimental side. <sep> However, I'd still like to hold my evaluation on the theoretical side, in that approaches for handling constraints / non-smoothness / momentums are fairly well understood in the optimization literature. The present result (Theorem 1) conveys the message that these approaches can be combined to work (give an algorithm that converges to stationary points if it converges), but is not really a result that gives us new understandings / novel proof techniques beyond that. <sep> I have slightly improved my rating to reflect my updated evaluation.",This paper proposes a new gradient-based stochastic optimization algorithm by adapting theory for proximal algorithms to the non-convex setting. <sep> The majority of reviewers voted for accept. The authors are encouraged to revise with respect to reviewer comments.
"abstract | rebuttal_process | weakness | strength  ==> This paper presents a black-box style learning algorithm for Markov Random Fields (MRF). The approach doubles down on the variational approach with variational approximations for both the positive phase and negative phase of the log likelihood objective function. For the negative phase, the authors use two separate variational approximations, one of which involves the modeling of the latent variable prior under the approximating distribution, <sep> The approach is novel, as far as I know, though not particularly so, and I view this as one of the weak point of the paper. That said, it does seems like a fairly creative combination of existing approaches. As others have found in the past, a variational approximation to the partition function contribution to the loss function (i.e. the negative phase) results in the loss of the variational lower bound on log likelihood and the connection between the resulting approximation and the log likelihood becomes unclear. To deal with this issue, the authors argue (in Lemma 1) that the gradient of their approximate objective is at least in the same direction as the ELBO (lower bound) objective. The result is fairly obvious, but the conditions for validity have interesting consequences for the training algorithm, as it relates the approximation error to the norm of the gradient of the ELBO loss. <sep> I have a minor issue with the discussion (in the last paragraph of sec. 3.2) stating that the theoretical statement of the proposed objective relies on a much weaker assumption than the nonparametric assumption made in the theoretical justification of GANs. While I agree with the statement as such, the GAN development makes a stronger statement about the nature of the learning trajectory. Specifically, it states that the generator is minimizing a Jenson-Shannon divergence which has a fixed point at the true data density. In the current development, Theorem 1 only states that the optimization process will converge to the stationary points of the approximate ELBO objective (L1 in the paper's notation). <sep> Clarity: I found the paper to be very well written with a clear exposition of the material and sound development of the technical details. <sep> Relevance and Significance: This paper is highly relevant to the *CONF* community and -- to the extent that one believes that training and inference in MRFs is important -- also significant. One this last point, it seems ironic to me that the proposed strategy for training the MRF is through the use of three separate directed graphical models (an encoder q(h | x),  a decoder and a VAE to model the approximate prior over the latents h). In most modeling situations, one would simply impose the directed graphical model directly and skip the formalization in terms of an MRF. I would appreciate a more forceful motivation of the relevance of MRFs rather than just stating it as a important model with applications. What is unique about the MRF formalism that -- for practical applications -- could not be effectively captured in a directed graphical model? <sep> I note that I am aware of the theoretical representation differences between directed and undirected models, I am wondering how these differences actually matter in practical applications at scale. <sep> Experiments: The authors show the empirical advantages offered by the proposed method over the existing literature. I was surprised not to see how this model performs on the binarized MNIST dataset, and would like to see that result as well as CIFAR likelihood.  MNIST, in particular, is a well studied dataset that many readers will be able to easily interpret. Its absence seems like a serious omission. <sep> What is meant by ""RBM loss"" in Fig. 2(d), I do not see this defined? <sep> I am somewhat alarmed at the use of 100 updates of the joint model q(v,h) (K1 = 100) for every update of the other parameters. For larger scale domains, I fear this could become an important obstacle to effective model training. The comparison to PCD-1 in Fig. 3 seems a bit unfair in that the learning curve ends at 8000 iterations, while PCD-1 continues to improve NLL. I would like to see this curve extended until we start to see signs of overfitting. Perhaps PCD-1 results in performance that is far better than AdVIL. I would also like to see a comparison to CD-k, which often outperforms PCD-k. While I understand the stance taken by the authors that these methods leverage the tractability of the conditional distributions, these strategies are sufficiently general to be considered widely applicable and a true competitor for AdVIL. <sep> With respect to Deep Boltzmann Machine (DBM), I would prefer to see quantitative comparisons against published results. Here again, MNIST would be a useful dataset.  It seems as though, in the application of AdVIL to the DBM, the authors are exploiting the structure of the model in how they define their sampling procedure. Is that the case? More detail for this application of AdVIL would be nice. Also, I would like to see the test estimated NLL (via AIS) learning curves for VCD and AdVIL. Given the comparison to PCD in the RBM setting, I am somewhat surprised that AdVIL is so competitive with VCD in the case of the DBM.","The paper proposes a black box algorithm for MRF training, utilizing a novel approach based on variational approximations of both the positive and negative phase terms of the log likelihood gradient (as R2 puts it, ""a fairly creative combination of existing approaches""). <sep> Several technical and rhetorical points were raised by the reviewers, most of which seem to have been satisfactorily addressed, but all reviewers agreed that this was a good direction. The main weakness of the work is that the empirical work is very small scale, mainly due to the bottleneck imposed by an inner loop optimization of the variational distribution q(v, h). I believe it's important to note that most truly large scale results in the literature revolve around purely feedforward models that don't require expensive to compute approximations; that said, MNIST experiments would have been nice. <sep> Nevertheless, this work seems like a promising step on a difficult problem, and it seems that the ideas herein are worth disseminating, hopefully stimulating future work on rendering this procedure less expensive and more scalable."
"abstract | strength | misc  ==> This paper proposes a new sequential model-free Q-learning methodology for POMDPs that relies on variational autoencoders to represent the hidden state. The approach is generic, well-motivated and has  clear applicability in the presence of partial observability. The idea is to create a joint model for optimizing the hidden-state inference and planning jointly. For that reason variational inference is used to optimize the ELBO objective in this particular setting. All this is combined with a recurrent architecture that makes the whole process feasible and efficient. <sep> The work is novel and it comes with the theoretical derivation of a variational lower bound for POMDPs in general. This intuition is exploited to create a VAE based recurrent architecture. One motivation comes from maximal entropy reinforcement learning (MERL), but which has the ad hoc objective of maximizing the policy entropy. On the other hand SVQN optimizes both a variational approximation of the policy and that of the hidden state. Here the rest terms of the ELBO objective can be approximated generatively and some of them are conditioned on the previous state which calls for a recurrent architecture. The other parts are modeled by a VAE. <sep> The paper also explores two different recurrent models in this context: GRU and LSTM are both evaluated. Besides the nice theoretical derivation the paper presents compelling evidence by comparing this approach to competing approaches on four games of the flickering ATARI benchmark and outperforming the baselines significantly. Also both the GRU and LSTM version outperforms the baseline methods on various tasks of the VIZDoom benchmark as well. <sep> In general, I find that this well written paper presents a significant progress in modelling POMDPS in a model-free manner with nice theoretical justification and compelling empirical evidence.","The paper proposes a novel model-free solution to POMDPs, which proposes a unified graphical model for hidden state inference and max entropy RL. The method is principled and provides good empirical results on a set of experiments that relatively comprehensive. I would have liked to see more POMDP tasks instead of Atari, but the results are good. Overall this is good work."
"strength | misc | ac_disagreement  ==>  ==> Abstract: <sep> In this paper, the authors propose to hide information in phase of the input features. They proposed that if each layer of processing layer, sitting outside of the local unit, is phase preserving, then they can recover the phase back. They propose a modification to the most popular layers in DNN to satisfy that property. <sep> I think the general idea of the paper is interesting, but overall, the paper is very poorly written. It appears that it is written in a rush. <sep> *The abstract and introduction are poorly written. There are poorly written sentences in the text. Here are some examples: <sep> + ""... We propose a generic method to revise a conventional neural network to boost the challenge of adversarially inferring about the input but still yields useful outputs. ...."" <sep> + ""... given a transformed feature, an adversary can at best recover a number of features which contain at least another k − 1 features which are different but cannot be distinguished from the real feature.  ..."" -- This is so central to the whole paper and it is not well-written. Personally, didn't understand this attack. <sep> Please proofread your paper. <sep> *Why do you need both theta and b ?  It seems to me if \\| b \\| is comparable to \\| a \\|, it can destroy the information in original the a arbitrary bad, so it makes sense to keep the norm of b small. However, in the paper, it is suggested to use a random sample (I') and set b = g(I'). I don't see any ablation study in the paper. There is no free lunch, if you provide stronger identity preservation, there should be a compromise in the accuracy, and it seems to be the magnitude of the b is that compromise. <sep> * The whole idea of the GAN encoder is not well justified. What does it mean that the fake feature should contain information ""beyond"" a ? This very vague. <sep> * ""Inference attack 1 "" and ""Inference attack 4"" are the same; only the inference models used in each attack are different. I don't know why the author has separated them.","The reviewers are unanimous in their opinion that this paper offers a novel approach to secure edge learning. I concur. Reviewers mention clarity, but I find the latest paper clear enough."
"abstract | strength | decision | suggestion  ==>  ==> Update 11/21 <sep> With the additional experiments (testing a new image, testing fine-tuning of hand-crafted features), additions to related work, and clarifications, I am happy to raise my score to accept. Overall, I think this paper is a nice sanity check on recent self-supervision methods. In the future, I am quite curious about how these mono-image learned features would fare on more complex downstream tasks (e.g., segmentation, keypoint detection) which necessarily rely less on texture. <sep> Summary <sep> This paper seeks to understand the role of the *number of training examples* in self-supervised learning with images. The usefulness of the learned features is evaluated with linear probes at each layer for either ImageNet or CiFAR image classification. Empirically, they find that a single image along with heavy data augmentation suffices for learning the first 2-3 layers of convolutional weights, while later layers improve with more self-supervised training images. The result holds for three state-of-the-art self-supervised methods, tested with two single-image training examples. <sep> In my view, learning without labels is an important problem, and it is interesting what can be learned from a single image and simple data augmentation strategies. <sep> Comments / Questions <sep> It seems to me that for completeness, Table 4 should include the result of training a supervised network on top of random conv1/2 and Scattering network features, because this experiment is actually testing what we want - performance of the features when fine-tuned for a downstream task. So for example, even if a linear classifier on top of Scattering features does poorly, if downstream fine-tuning results in the same performance as another pre-training method, then Scattering is a perfectly fine approach for initial features. Could the authors please either correct this logic or provide the experiments? <sep> Further, it seems that the results in Table 4 might be a bit obscured by the size of the downstream task dataset. I wonder if the learned features require fewer fully supervised images to obtain the same performance on the downstream task? <sep> Can the authors clarify how the neural style transfer experiment is performed? The method from Gatys et al. requires features from different layers of the feature hierarchy, including deeper layers. Are all these features taken directly from the self-supervised network or is it fine-tuned in some way? <sep> While I appreciate the computational burden of testing more images, it does feel that Image A and B are quite cherry-picked in being very visually diverse. Because of this, it seems like a precise answer to what makes a good single training image remains unknown. I wonder how feasible it is to find a proxy metric that corresponds to the performance on downstream tasks which is expensive to compute. It might be interesting to try to generate synthetic images (or modify real ones) that are good for this purpose and observe their properties. <sep> I disagree with the claim of practicality in the introduction (page 2, top). While training on one image does reduce the burden of number of images, the computational burden remains the same. And as mentioned above, it doesn't seem likely that *any* image would work for this method. Finally, more images are needed to learn the deeper layers for the downstream task anyway. <sep> The paper is well-written and clear.","This paper studies the effectiveness of self-supervised approaches by characterising how much information they can extract from a given dataset of images on a per-layer basis. Based on an empirical evaluation of RotNet, BiGAN, and DeepCluster, the authors argue that the early layers of CNNs can be effectively learned from a single image coupled with strong data augmentation. Secondly, the authors also provide some empirical evidence that supervision might still necessary to learn the deeper layers (even in the presence of millions of images for self-supervision). <sep> Overall, the reviews agree that the paper is well written and timely given the growing popularity of self-supervised methods. Given that most of the issues raised by the reviewers were adequately addressed in the rebuttal, I will recommend acceptance. We ask the authors to include additional experiments requested by the reviewers (they are valuable even if the conclusions are not perfectly aligned with the main message)."
"abstract | strength | rebuttal_process | misc  ==> This work proposes a new environment, Read to Fight Monsters (RTFM), and correspondingly a new algorithm, txt2\\pi, for solving this problem. The RTFM requires the agent to read a description of the rules (x beats y, etc) and a description of the goal (to eliminate y), and perform the task correctly to win the game. The txt2\\pi algorithm uses the newly proposed FiLM^2 module and consists of integrating the visual input (grid world configuration) and the text input (descriptions) to learn a policy and baseline (actor-critic). <sep> Pros <sep> - The presentation is relatively clear. <sep> - A new environment that can be impactful for the field. <sep> Cons <sep> - Certain design choices are not discussed properly. <sep> - The experimental evidence is relatively weak. <sep> (1) For RTFM, a complete list of possible elements would be helpful, at least in the supplementary. For example, the possible monsters/elements/weapons, etc. <sep> (2) Some proposals lack motivation or explanation. <sep> (2.1) How is Eq.(8) a summary? <sep> (2.2) Why the attention in (18) should base on the vis-doc embedding instead of the goal embedding, or goal-conditioned doc embedding? <sep> (2.3) It would be helpful to provide an example of the inventory description. <sep> (2.4) What is the difference between the Goal and Goal-doc in Fig.3? <sep> (3) Experiment <sep> (3.1) ""no group"", ""no dyna"" and ""no nl"" seem to be suggesting a simpler environment. How is no natural language templated descriptions (no nl) considered as an easier scenario than with a template? The description would be harder to parse or understand without structure (template). <sep> (3.2) It is mentioned several times that the models are ""trained on one set of dynamics and evaluated on another set of dynamics"". Specifically, what are the differences? <sep> (3.3) For the curriculum training (Table 2), adding dyna does not hurt the performance (from 84 to 85). Further explanation would be helpful. <sep> (3.4) How are the baselines and alternative methods perform in the full environment (+group etc.)? <sep> (3.5) The texts on Fig.5 are not evenly spaced, which makes them harder to align with the attention (also some letters have dots above them). Does this figure correspond to (15)? In Fig.5b, a0 focuses on ""gleaming"" and a1 focuses on ""lighting"", which are not present in the entities (compared to the caption). Something is wrong or missing here. <sep> (3.6) Eventually, the txt2\\pi algorithm only achieves modest improvement over random agents (65% versus 50%), which is not very impressive. <sep> ## Update ## <sep> Thank you for the thorough explanations, which are very helpful. It would be better to include them in the paper to clarify potential misunderstandings. <sep> One more remark is that the goal-doc attention map (i.e., lg) in Fig. 5b focuses on lynx, which does not make sense in an environment with only a beetle and a wolf. This is strange and problematic given that it is an important attention map for many subsequent modules. <sep> I have changed my score accordingly. It also needs to be considered that I'm not very familiar with the field.","This paper proposes RTFM, a new model in the field of language-conditioned policy learning. This approach is promising and important in reinforcement learning because of the difficulty to learn policies in new environments. <sep> Reviewers appreciate the importance of the problem and the effective approach. After the author response which addressed some of the major concerns, reviewers feel more positive about the paper. They comment, though, that presentation could be clearer, and the limitations of using synthetic data should be discussed in depth. <sep> I thank the authors for submitting this paper."
"abstract | strength | decision  ==> The authors start with the valid observation that for controlled systems the threat model of explicitly flipping pixels or otherwise changing the inputs directly is less relevant than the case of identifying bad inputs in the environments (which the authors rename ""natural observations"") that cause the controller under attack to leave its stable region and end up in bad states for itself, unable to recover sometimes. This paper is an empirical exploration of this phenomenon, using as examples trained agents in a Gym environment and involving two humanoid robot simulations interacting with each other in various ways. The original policies are taken from previous training, so the main experimental contribution here is to solve an RL problem on the attacker's side to try to find the bad regions in the victim's control space, going via the means of the observation-based coupling. <sep> I think these are timely issues. I agree that, to the extent that DRL policies are being seriously considered as candidates for various practical problems, we should seriously ask questions about the (lack of) stability of solutions. <sep> I also think that the paper would be stronger if the authors considered the rich history in control design, formal methods and other communities where this approach to counter-example based thinking is fairly standard and in particular where the use of optimization methods to find 'bugs' has been studied in some depth. <sep> So, for instance, the authors give us useful quantification to support the observation that the process of attacker RL finds ways to put the victim outside its stable region where it is naturally unstable and ineffective. However, they do not observe that there is a literature on systematically performing 'controller testing' via optimization using various techniques. Here are just a few examples to show the diversity: <sep> Ghosh, S., Berkenkamp, F., Ranade, G., Qadeer, S., & Kapoor, A. (2018, May). Verifying controllers against adversarial examples with Bayesian optimization. In 2018 IEEE International Conference on Robotics and Automation (ICRA) (pp. 7306-7313). IEEE. <sep> Ravanbakhsh, H., & Sankaranarayanan, S. (2016, October). Robust controller synthesis of switched systems using counterexample guided framework. In 2016 international conference on embedded software (EMSOFT) (pp. 1-10). IEEE. <sep> The authors might find it interesting to note that this approach of viewing the control problem 'backwards' to generate new instances has a deeper history in control theory (of course without discussion of NNs etc), e.g., <sep> Doyle, J., Primbs, J. A., Shapiro, B., & Nevistic, V. (1996, December). Nonlinear games: examples and counterexamples. In Proceedings of 35th IEEE Conference on Decision and Control (Vol. 4, pp. 3915-3920). IEEE. <sep> All that said, I find the results presented here to be plausible and pointing in the desired direction for exploring how to robustify DRL. The authors defer training using these examples to future work but I think the paper would be more self-contained if that were actually demonstrated here already. For reasons I mention above, the existence of this kind of weakness in controllers is not really surprising to people who have thought deeply about control, and it is only to be expected that NN based parameterisations of control would only be even more vulnerable. So, the more satisfying result would be the positive one that shows how to train DRL to be (more) robust under such attacks.","This paper demonstrates that for deep RL problems one can construct adversarial examples where the examples don't really need to be even better than the best opponent. Surprisingly, sometimes, the adversarial opponent is less capable than normal opponents which the victim plays successfully against, yet they can disrupt the policies. The authors present a physically realistic threat model and demonstrate that adversarial policies can exist in this threat <sep> model. <sep> The reviewers agree with this paper presents results (proof of concept) that is ""timely"" and the RL community will benefit from this result. Based on reviewers comment, I recommend to accept this paper."
"abstract | strength | rebuttal_process | decision  ==>  ==> This paper improves the robustness of smoothed classifiers by maximizing the certified radius, which is more efficient than adversarially train the smoothed classifier and achieves higher average robust radius and better certified robustness when the radius is not much larger than the training sigma. It proposes a novel objective which is derived by decomposing the 0/1 certified loss into the sum of 0/1 classification error and 0/1 robustness error. Three conditions are identified to make the optimization doable. Two surrogate losses (CE and hinge loss on the certified radius) for the two 0/1 errors are proposed as upper bounds of the 0/1 loss. Certified radius is derived as a function of the logits of Soft-RS to make the hinge loss differentiable. Numerical stability of the proposed objective is also analyzed by showing its gradient is bounded. <sep> In general, the paper is well-written and the proposed objective is novel to my knowledge. I tend to accept the paper. Still, I am not sure about how much MACER improves upon the baselines, and would like to ask some questions. <sep> 1. Cross entropy is used as a surrogate for the 0/1 classification error. This is true for all cases (including all experiments in this paper) except for binary classification, where the cross entropy is less than 1 when the score on the correct class is around 0.5. It is not important but would be better if you could mention this point. <sep> 2. Have you ever tried using a tighter upper bound for the 0/1 classification error, e.g., using cross entropy loss only for the wrongly classified samples? How does it affect the results? <sep> 3. Despite showing better results, MACER seems to be using much more epochs than the two baselines (but the total hrs is smaller than (Salman et al. 2019)). Also, MACER is using a much larger k than (Cohen et al., 2019). From Figure 3 (a) we can see a larger k improves the result a lot, and from Figure 3 (b) it seems that setting lambda to a non-zero value only improves the accuracy when the radius is large. For fair comparisons, could the authors give the ACR with different values of lambda while keeping other hyper parameters unchanged? Is Salman's method still not as good when using the same number of epochs?","The submission proposes a robustness certification technique for smoothed classifiers for a given l_2 attack radius. <sep> Strengths: <sep> -The majority opinion is that this work is a non-trivial extension of prior work to provide radius certification. <sep> -The work is more efficient that strong recent baselines and provides better performance. <sep> -It successfully achieves this while avoiding adversarial training, which is another novel aspect. <sep> Weaknesses: <sep> -There were some initial concerns about missing experiments and unfair comparisons but these were sufficiently addressed in the discussion. <sep> AC shares the majority opinion and recommends acceptance."
"abstract | misc | strength | weakness | rebuttal_process | decision  ==> Summary: <sep> This paper proposes a general method for eliminating noisy labels in supervised learning based on the combination of two ideas: outputs of noisy examples are less robust under noise, and noisy labels are less likely to have a low loss. The authors then propose 3 concrete instantiations of the idea, and do a thorough empirical study (including ablations) across multiple architectures, datasets, noise types, and comparing to multiple related methods. The results show pretty convincingly that one of the new methods (LTEC) that uses past networks outputs to build an ensemble performs really well. <sep> Caveats: <sep> 1) I'm an emergency reviewer and had less time to do an in-depth review. <sep> 2) While my research is sufficiently close to review the paper, I'm not an expert on label noise specifically, so I cannot comment much on novelty and related work questions. <sep> Comments: <sep> * The readability of the paper could be dramatically improved by reporting results visually (eg bar-plots) and moving all tables into the appendix. <sep> * The authors state that peak performance is valid because it *could* have been found using a validation set -- then why not just do that, and report this early-stopping performance everywhere, instead of always two numbers (peak and final)? <sep> * Please make the perturbation properties in section 3.2 more precise: do you mean ""there exists a threshold and perturbation such that for some (x, y)""? Or ""For any threshold and any perturbation then for all (x, y) it holds...""? Or something in-between? <sep> * For the ""competing methods"" paragraph, please cite the relevant papers in the main text, not only in the appendix. <sep> * ""over 4 runs"" did you randomize the data noise in each run (and in the same way for each method?), or only the network initialisation? <sep> * Table 5 is cool, but it raises the question: is 1.1epsilon the best, or would performance keep going up? <sep> * Looking at the actual implementation of LTEC (Algorithm 4), I cannot resist the thought that M=infinity could work even better (at no extra cost): just maintain a monotonically shrinking set of samples? <sep> Minor comments: <sep> - Define the threshold symbol in section 3.2 <sep> - Define M in Algorithm 1 <sep> - Define (initial) \\mathcal{P}_0 in Algorithm 4 <sep> - Fig 2: can you clarify what green is, does it correspond to ""self-training""? <sep> - ""label precision … does not decrease"" -- well, not a lot, but it does decrease! <sep> - Fig 1, Fig 2: set max y to 100 <sep> - Table 4: Include M=1 (which is self-training) for comparison","This paper proposes an ensemble method to identify noisy labels in the training data of supervised learning. The underlying hypothesis is that examples with label noise require memorization. The paper proposes methods to identify and remove bad training examples by retaining only the training data that maintains low losses after perturbations to the model parameters. This idea is developed in several candidate ensemble algorithms. One of the proposed ensemble methods exceeds the performance of state-of-the-art methods on MNIST, CIFAR-10 and CIFAR-100. <sep> The reviewers found several strengths and a few weaknesses in the paper. The paper was well motivated and clear. The proposed solution was novel and plausible. The experiments were comprehensive. The reviewers identified several parts of the paper that could be more clear or where more detail could be provided, including a complexity analysis and <sep> extended experiments. The author response addressed the reviewer questions directly and also in a revised document. In the discussion phase, the reviewers were largely satisfied that their concerns were addressed. <sep> This paper should be accepted for publication as the paper presents a clear problem and solution method along with convincing evidence of method's merits."
"abstract | strength | rating_summary | decision  ==>  ==> This paper builds upon the random smoothing technique for top-1 prediction proposed by Cohen et al. for certifying top-k predictions with probabilistic guarantees, which enjoys good scalability to large neural networks and in principle can be applied to any classifier. <sep> - Contributions: <sep> 1. The authors aim to provide (probabilistic) certification on top-k predictions, which to my knowledge is the first work to consider this setup. Many applications such as recommendation systems indeed use top-k predictions as a performance measure. The problem setup is new and important in the research of robustness certification. <sep> 2. In terms of technical contributions, the authors identify the difficulty of extending top-1 prediction to top-k prediction, due to the requirement of simultaneous confidence interval estimation of the bounds on the actual class predictions. To cope with this difficulty, the authors proposed simultaneous confidence interval estimation based on Clopper-Pearson method and Bonferroni correction. However, I am not sure the difficulty is caused by the necessity of estimating multiple probability bounds, or simply the limitation of the proposed algorithm. I hope the authors can address my concerns in the Questions below. <sep> 3. Experimental results on Cifar-10 and ImageNet showed improved lower bound on certified L2-norm radius when increasing k. The authors also performed an ablation study of different parameters in the proposed algorithm. <sep> - Questions: <sep> 1. Intuitively, when extending top-1 certification to top-k certification, one would expect using ordered statistics of the prediction outputs from the randomly perturbed inputs. As long as the original label's prediction probability is in the top-k label set, the smoothed classifier is directly certified. Instead of ordered statistics, the authors tackle this problem by considering estimating upper and lower bounds of each class prediction probability. Therefore, the problem becomes more difficult as k increases, since this indirect approach needs to simultaneous estimate those probability bounds. I wonder the current approach will be suboptimal when compared to the ordered statistics approach. I would like to know the authors thoughts on this regard. That is, is the claimed difficulty an outcome when using the proposed indirect bound estimation for certification, or it's provably more difficult? <sep> 2. The discussion on Fig.3 says ""We observe that  \\sigma controls a trade-off between normal accuracy under no attacks and robustness. Specifically, when  is larger, the accuracy under no attacks (i.e., the accuracy when radius is 0) is larger, but the certified top-k accuracy drops more quickly as the radius increases."" However, it seems that larger \\sigma actually gives lower accuracy under no attacks in Figure 3. Please clarify. <sep> Overall, this paper brings some new insights and results in robustness certification, but some claims and statements need to be further justified. I am happy to increase my rating if my concerns are addressed.","The paper extends the work on randomized smoothing for certifiably robust classifiers developed in prior work to a weaker specification requiring that the set of top-k predictions remain unchanged under adversarial perturbations of the input (rather than just the top-1). This enables the authors to achieve stronger results on robustness of classifiers on CIFAR10 and ImageNet (where the authors report the top-5 accuracy). <sep> This is an interesting extension of certified defenses that is likely to be relevant for complex prediction tasks with several classes (ImageNet and beyond), where top-1 robustness may be difficult and unrealistic to achieve. <sep> The reviewers were in consensus on acceptance and minor concerns were alleviated during the rebuttal phase. <sep> I therefore recommend acceptance."
"abstract | weakness | rating_summary  ==> This paper propose an extension to deterministic autoencoders. Motivated from VAEs, the authors propose RAEs, which replace the noise injection in the encoders of VAEs with an explicit regularization term on the latent representations. As a result, the model becomes a deterministic autoencoder with a L_2 regularization on the latent representation z. To make the model generalize well, the authors also add a decoder regularization term L_REG. In addition, due to the encoder in RAE is deterministic, the authors propose several ex-post density estimation techniques for generating samples. <sep> The idea of transferring the variational to deterministic autoencoders is interesting. Also, this paper is well-written and easy to understand. However, in my opinion, this paper needs to consider more cases for autoencoders and needs more rigorous empirical and theoretical study before it can be accepted. Details are as follow: <sep> 1. The RAEs are motivated from VAEs, or actually CV-VAEs as in this paper. More precisely, the authors focus on VAEs with a constant covariance Gaussian distribution as the variational distribution and a Gaussian distribution with the identity matrix as the covariance matrix as the model likelihood. However, there might be many other settings for VAEs. For example, the model likelihood can be a Gaussian distribution with non-constant covariance, or even some other distributions (e.g. Multinomial, Bernoulli, etc). Similarly, the variational distribution can be a Gaussian distribution with non-constant covariance, or even some more complicated distributions that do not follow the mean-field assumption. Any of these more complex models may not be easily transferred to the RAE models that are mentioned in this paper. Perhaps it is better if the authors can consider RAEs for some more general VAE settings. <sep> 2. Perhaps the authors needs more empirical study, especially on the gain of RAE over CV-VAE and AE. <sep> a) As the motivated model (CV-VAE) and the most related model in the objective (AE), they are not appearing in the structured input experiment (Section 6.2). It will be great if they can be compared with in this experiment. <sep> b) The authors did not show us clearly whether the performance gain of RAE over VAE, AE and CV-VAE is due to the regularization on z (the term L_z^RAE) or the decoder regularization (the term L_REG) in the experiments. In table 1, the authors only compare the standard RAE with RAE without decoder regularization, but did not compare with RAE without the regularization on z (i.e. equivalent to AE + decoder regularization) and CV-VAE + decoder regularization. The authors would like to show that the explicit regularization on z is better than injecting the noise, hence the decoder regularization term should appear also in the baseline methods. It is totally possible that perhaps AE + decoder regularization or CV-VAE + decoder regularization perform better than RAE. <sep> c) The authors did not show how they tune the parameter \\sigma for CV-VAE. Since the parameter \\beta in the objective of RAE is tunable, for fair comparison, the authors needs to find the best \\sigma for CV-VAE in order to get the conclusion that explicit regularization is better than CV-VAE. <sep> d) Although the authors mention that the 3 regularization techniques perform similarly, from Table 1, it is still hard to decide which one should we use in practice in order to get a performance at least not too much worse compared to the baseline methods. RAE-GP and RAE-L2 perform not well on CelebA while RAE-SN perform not well on MNIST, compared to the baseline methods. We know that the best performance over the 3 methods is always comparable to or better than the baselines, but not none of the single methods do. It is better if the authors can provide more suggestions on the choice for decoder regularization for different datasets. <sep> 3. The authors provided a theoretical derivation for the objective L_RAE (Equation 11), but this is only for the L_GP regularization. Besides, this derivation (in Appendix B) has multiple technique issues. For example, in the constraints in Equation 12, the authors wrote ||D_\\theta(z1) - D_\\theta(z2)|| < epsilon for all z1, z2 ~ q_\\phi(z | x), this is impossible for CV-VAE since this constraint requires D_theta() to be bounded while q_\\phi(z | x) in CV-VAE has an unbounded domain. Moreover, in the part  (||D_\\theta(z1) - D_\\theta(z2)||_p=\\nabla D_\\theta(\\tilde z)\\cdot ||z_1-z_2||_p) of Equation 13, \\nabla D_\\theta(\\tilde z) is a vector well the other two terms are scalars, which does not make sense. There are many other issues as well. Please go through the proof again and solve these issues. <sep> Questions and additional feedback: <sep> 1. Can the authors provide more intuitions why do you think the explicit regularization works better compared to the noise injection? Can you provide a theoretical analysis on that? <sep> 2. Can the authors provide some additional experiments as mentioned above? Also, can the authors provide more details about how do they tune the parameters \\beta and \\lambda? <sep> ======================================================================================================== <sep> After the rebuttal: <sep> Thanks the authors for the detailed response and the additional experiments. I agree that the additional experiment results help to support the claims from the authors, especially for the CV-VAE for the structured data experiments and the AE + L2 experiment. So I think now the authors have more facts to support that RAE is performing better compared to the baseline methods. <sep> Therefore, I agree that after the revision, the proposed method RAE is supported better empirically. So I am changing my score from ""weak reject"" to ""weak accept"". But I still think the baseline CV-VAE + regularization is important for Table 1 and the technical issues in the theoretical analysis needs to be solved. Hope the authors can edit them in the later version.","This paper proposes an extension to deterministic autoencoders, namely instead of noise injection in the encoders of VAEs to use deterministic autoencoders with an explicit regularization term on the latent representations. While the reviewers agree that the paper studies an important question for the generative modeling community, the paper has been limited in terms of theoretical analysis and experimental validation. The authors, however, provided further experimental results to support the claims empirically during the discussion period and the reviewers agree that the paper is now acceptable for publication in *CONF*-2020."
"abstract | strength | rating_summary  ==> Contributions: <sep> The paper extends variational continual learning with memory. In this setting the posterior distribution of the model parameters is approximated using a mean-field Gaussian approximation and a small set of datapoints is kept in a memory to combat catastrophic forgetting. <sep> - The paper proposes a new rule for updating the memory and the Gaussian approximation after examining each batch of points. For the memory update, instead of the previously used k-means or random sampling, it examines the factor 'r(d|w)' that each datapoint contributes and selects the most significant ones. <sep> - A second contribution is the use of two adaptation methods in the online learning setting in case there is a distribution shift in the data: Bayesian forgetting and the Ornstein-Uhlenbeck process. <sep> (1) The first contribution, the new memory update rule and the Gaussian update, is novel to my knowledge. The idea is to calculate the Gaussian factor that each datapoint contributes to the posterior (the update corresponding to each datapoint in the message-passing interpretation) and the select the points that contribute the most. Effectively, it calculates the change in the ELBO if each candidate datapoint was moved to the memory and selects the set that minimizes the ELBO of the remaining points. This  is a sensible heuristic because we want to keep datapoints in the memory that contribute the most to the posterior approximation. If a datapoint contributes nothing to the posterior then it shouldn't be kept in memory. <sep> The idea clever and it seems to work well in practice. Questions: <sep> - The Gaussian that each datapoint contributes is an approximation in the sense that if we take that datapoint and its contribution away, the remaining approximate posterior might be suboptimal. This has the consequence that S_tk (eq. 8) is only an approximation to what the optimal ELBO would be without the datapoints selected for the memory. Did the authors conduct experiments or have thoughts on how well S_tk approximates the ELBO of the variational approximation of the remaining points? <sep> -Regarding the experiments, there is a comparison to k-centre and random selection, both of which are proposed in VCL. I am very surprised to see in the experiments that both of these are outperformed by 'no-memory'. For example on 'energy', the baselines at t50 are outperformed by the non memory model at t0. Can this all be contributed to the baselines training their variational approximation without the memory at t0? <sep> - It is unclear whether the gains can be contributed to the new Gaussian update rule or the new memory update rule. To see the contribution of each, there should be an experiment where the proposed Gaussian update rule is used along with k-centre for the memory update. The fact the the no-memory model performed to close to GRS suggests that k-centre with the new Gaussian update rule would be even closer to it. <sep> (2) The second contribution of the paper is the use of Bayesian forgetting and OU to use to deal with the data distribution shift. It shows how these two approaches can be used along with the proposed Gaussian and memory updates. The paper is already very long, so there likely isn't any space for fleshing out this section, but it would be nice to have the experimental results included in the main paper, because most of the experiments are left to the appendix. Perhaps it would be a good idea to focus only on Bayesian forgetting (or the OU process) and try to shorten the section a bit. <sep> In terms of writing and clarity, I have no complaints. The paper is well written and easy to understand. <sep> Minor: <sep> - 3.2 'If the likelihood term p(dtk|w) is well approximated by r(w;dtk)' -  I find this sentence a bit confusing. What does it mean for p(dtk|w) to be well approximated by  r(w;dtk)? <sep> - 3.3 'In order to reduce the variance of the Monte-Carlo approximation'. What is the MC approximation made here? <sep> - The method never actually uses the assumption that the datapoints are sampled i.i.d.. The algorithm should still work if the datapoints are not examined in a random order, e.g. consider seeing all the images with label '0' and then all the images with label '1' etc.. This would likely degrade the performance significantly but the method should still work. <sep> Overall assessment: <sep> Pros: I like the ideas in the paper and they are presented well. <sep> Cons: The paper is a bit too long. The experiments could more thoroughly investigate the source of the gains.","This paper introduces an algorithm for online Bayesian learning of both streaming and non-stationary data. The algorithmic choices are heuristic but motivated by sensible principles. The reviewers' main concerns were with novelty, but because the paper was well-written and addressing an important problem they all agreed it should be accepted."
"abstract | strength | rebuttal_process | strength | rebuttal_process | strength | rebuttal_process | decision | strength  ==> The paper proposes a model for stochasticity for conditional image generation, building upon the previously available (DCFNet) results on composition of  convolutional filters out of the elements of the filter basis. <sep> The idea of introducing stochasticity by convolutional filters into the conditional generative models seems to be novel and the reviewer thinks it could be of interest for the community. <sep> The following remarks could be given to improve the presentation: <sep> 1) Theorem 1 is an existence theorem, so it does not give the procedure for construction of the basis. Does the construction procedure for the basis, described under the theorem formulation, meet the conditions of Theorem 1? <sep> 2) The Theorem 1 formulation states that "" If there exists a set of deterministic linear transforms"". Should the linear independence be stated as well as one of the theorem conditions ( so that the space dimensionality would indeed be K)? <sep> 3) The reviewer finds the structure of Section 4 confusing: it starts from the problem statement (first paragraph 'Using the method above, filters of each stochastic layer…'), then provides the description of the approach and only then outlines Theorem 1. It might be that stating Theorem 1 and then defining the method for generation of the basis (how exactly could we get to the basis? ) could improve readability of the paper. Essentially, the question is: is there any way to emphasise the procedure for filter generation and inform the reader in which circumstances these filters would be the basis (e.g. why it wouldn't be prone to the analogue of mode collapse when the filters do not effectively have enough diversity for linear independence)? <sep> *** <sep> In addition to this list, it might be useful to provide some evidence on whether there is any inherent mechanism to regulate the diversity of filters and therefore of samples (so that to change the variability of the conditional samples from the model with the impact analogous to the one of temperature in Glow (Kingma et al, 2018)). If there is one, further experimental evidence, which shows the impact on diversity of filters, would contribute to improvement of the paper.","Main content: BasiGAN, a novel method for introducing stochasticity in conditional GANs <sep> Summary of discussion: <sep> reviewer1: interesting work and results on GANs. Reviewer had a question on pre-defned basis but i think it was answered by the authors. <sep> reviewer3: interesting and novel work on GANS, wel-written paper and improves on SOTA. The main uestion is around bases again like reviewer 1, but it seems the authors have addressed this. <sep> reviewer4: Novel interesting work. Main comments are around making Theorem 1 more theoretically correct, which it sounds like the authors addressed. <sep> Recommendation: Poster. Well written and novel paper and authors addressed a lot of concerns."
"rating_summary | decision  ==>  ==> Summary: <sep> This paper targets the maximization issue in continuous value based methods, especially Q Learning. The idea is to use Mixed Integer Programming (MIP) to solve the Q maximization step by formulating the neural network structure of the Q function as a constrained mixed integer program. Further improvements are made by approximating the MIP solution in order to make training/inference faster. The method is tested on tasks from the Mujoco domain and compared with other value based methods for continuous control. I found the paper simple to follow and well structured. The problem is well motivated too and the empirical analysis is quite rigorous. <sep> The obvious concerns are regarding scalability of the method; both in terms of 1) using other forms of neural network components (ex. Other activation functions, Convolutional networks in the case of vision based problems such as robotic manipulation) and 2) problems that are inherently less sample efficient, i.e. cases where shortening the sampling horizon is not a feasible option for learning meaningful policies. <sep> Overall, I feel the positive aspects more or less outweigh the drawbacks and therefore my vote is for a weak accept. <sep> Comments/Questions: <sep> - Table 8 description says hyper-parameter sweeps were done for temperature and exploration noise decay values but the table is missing their values. <sep> - What happens when action range is increased from default? One of the reasons mentioned for constraining the action space is to validate how well policy-based methods work. To really take this point home, I feel it might be good to check with an increased action range. <sep> - Controlling w.r.t symmetry of the env (esp. in Mujoco domains), thus reducing the number of actions by half, might help faster MIP computation times. <sep> - Figure 3 x-axis runs till different values, but the description says training steps are 1000. How long are the experiments run for? <sep> - Can the authors elaborate on why the episode length is decreased from 1000 to 200?",All three reviewers gave scores of Weak Accept. AC has read the reviews and rebuttal and agrees that the paper makes a solid contribution and should be accepted.
"abstract | rating_summary | rebuttal_process | rating_summary | decision  ==> This paper investigates the problem of predicting the truth of quantified boolean formulae using deep reinforcement learning. In this setting, the problem is formulated as a reinforcement learning task, in which the learner is interacting with a solver (CADET), and its goal is to find a sequence of actions (each associated with a choice of a variable and a value) in order to reach a terminal state as fast as possible. The neural architecture includes a GNN encoder for the input formula, a policy neural net for iteratively assessing the quality of literals, and a final softmax layer for choosing the final literal. Experiments, performed on various 2QBF instances, address several questions such as the ability to compete with existing heuristics (VSIDS) in CADET and to generalize predictions on long episodes or different formulae. <sep> Overall, the paper is well-motivated. The introduction is well-written and explains the interest of learning new heuristics for QBF problems. The learning framework is relatively simple and elegant. Unfortunately, the paper suffers from many clarity issues in the problem formulation, the neural-net architecture, and the experiments. So, it is quite difficult to accept the paper in its current state. <sep> Section 2: In this section, some background knowledge about boolean problems and solvers are provided. Although the second paragraph about CNF formulae and CDCL solvers is well-written, the third paragraph about QBF should be clarified. For QBF formulae, the authors write ""The algorithmic problem considered for QBF is to determine the truth of a quantified formula (TQBF)"". Well, this is not an algorithmic problem, but a decision problem. Furthermore, what is TQBF? I guess that the authors are talking about 2-QBF, for which the prenex is of the form ∀∃. Here, the decision task should be explained in more detail using, for example, a game tree for explaining the distinct roles of ""for all"" variables and ""there is"" variables. As the decision problem for 2-QBF is more complex than the satisfiability problem for CNF, this point should be emphasized (i.e. the decision problem for 2-QBF is Π2P complete). <sep> Section 3: The problem of predicting the truth of 2-QBF is formulated as an MDP, where the environment is essentially controlling the input instances (of 2-QBF) and the states of the solver, and the learner's actions are variable-value selections. First of all, I am not entirely convinced that an MDP is the right choice for specifying this problem. Basically, 2-QBF is a two-player game (""for all"" vs ""there is""), and the goal of the solver is to play ""there is"" by finding a satisfying assignment for each possible play (i.e. variable assignment) of the ""for all"" player. So, a natural framework here would be a stochastic game (SG) which generalizes the MDP framework. Actually, in the present MDP framework there are no distinctions between ""for all"" variables and ""there is"" variables. It seems that the learner can choose ""for all"" variables (which is wrong). Furthermore, the MDP is ill-defined. It is said that a policy is a mapping π:S×A. This is ill-defined: what is the range of π? Usually, a (mixed) policy is a mapping π from S into the |A|-dimensional simplex. The reward function is also ambiguous: it is using a discount factor γ but, unless I missed something, this factor is not clarified in the rest of the paper. Finally, what is an ""episode""? In the paper it is said ""An episode is the result of the interaction of the agent with the environment"". Well, this is quite unclear. Usually, in an episodic-MDP the state space is partitioned into layers, i.e. X=⋃i=0LXi, where X0 is a singleton set (the initial state), and XL specifies the terminal states. Transitions are possible only between consecutive layers.  According to this usual framework, an episode is a sequence of actions made by the agent, starting from X0 and moving forward across the consecutive layers until it reaches a state in XL. For the 2-QBF decision problem, each terminal state in XL would naturally consists in the affectation of each variable in the prenex to a Boolean value. But this is not clear in the paper, because the authors are saying that ""We consider an episode to be complete, if the solver reaches a terminal state in the last step"". This would mean that the number of actions per episodes is capped, and hence, the agent can reach a terminal, yet non-final, decision state. <sep> Section 4: The overall architecture (GNN encoder + Policy Network) is relatively standard, but the choice of the constants for dimension parameters λV, δL and δC is a bit disconcerting. Notably, λV is used to capture the features of variables. Specifically, it is written that ""λV=7 indicates whether the variable is universally or existentially quantified, whether it currently has a value assigned and whether it was selected as a decision variable already on the current search branch."" But what is the difference between the second feature and the third one? If a variable has already been branched, then it has been assigned, and conversely. Furthermore, if you have 3 boolean features λV should be fixed to 3. So, why choosing 7? For δL and δC, it seems that their values correspond to the ""best model"". But how this model is chosen? Did the author perform some grid search to find those values?  Finally, some comments about the last layer of the architecture (softmax function) would be welcome. In the end, we get a probability distribution over literals (agent's available actions) ""after masking illegal actions"" (as written by the authors). But what is an illegal action? Is it a literal defined on a universally quantified variable? A literal defined on an already assigned existential variable? <sep> Section 5: In the experiments, the authors are examining four different questions, which are all interesting. But the experimental setup and the reported results are quite unclear. In fact, the experimental setup looks wrong, because if the ""Reductions"" dataset is taken from Jordan & Kaiser (SAT'13), it consists of formulae for which the prenex is of the form ∃∀"". Unless I am wrong, this is the inverse of the 2QBF problem examined in the present paper  - Jordan and Kaiser were examining a Σ2P-complete problem, while you are examining a Π2P-complete problem. So, did you reverse the quantifiers for making experiments? This should be clarified in the paper. Furthermore, for this dataset which originally consists of 4500 instances, the authors say that ""We filtered out 2500 formulas that are solved without any heuristic decisions."" What does this mean? Are all those 2500 formulas containing only universal quantifiers? In the remaining 2000 instances, what is the ratio between universally quantified variables and existentially quantified ones? By the way, how can we get 1835 training instances? 4500 - 2500 - 200 = 1800. <sep> In the experimental results, which formulae have been used to compute the cactus plots? Training instances (1800)? Test instances (200)? Both of them? For training instances, the protocol reported in Section 5.2 is relatively clear. But for test instances, what is the protocol? Is CADET using the best policy trained on the 1800 instances for solving the remaining 200 instances? Furthermore, the notion of ""decision limit"" is quite confusing. According to Section 5.2, it seems that the decision limit is the horizon of each episode, i.e. the number of calls to the solver CADET using the latest policy estimated by the NN architecture. But this should be clarified unambiguously.","This paper proposes a new method to learning heuristics for quantified boolean formulas through RL. The focus is on a method called backtracking search algorithm. The paper proposes a new representation of formulas to scale the predictions of this method. <sep> The reviewers have an overall positive response to this paper. R1 and R2 both agree that the paper should be accepted, and have given some minor feedback to improve the paper. R3 initially was critical of the paper, but the rebuttal helped to clarify their doubt. They still have one more comment and I encourage the authors to address this in the final version of the paper. <sep> R3 meant to increase their score but somehow this is not reflected in the current score. Based on their comments though, I am assuming the scores to be 6,8,6 which makes the cut for *CONF*. Therefore, I recommend to accept this paper."
"abstract | strength | decision  ==> This paper proposes a new method for making 3D point clouds by automatically completing 3D scans. It does not require paired data samples for training which makes it possible to train it on real data instead of synthetic data. The authors use a generative adversarial network (GAN) to ""generate"" complete point clouds from noisy or partial point clouds obtained by 3D scanning. The generator learns to perform mapping from point set manifold of scanned noisy and partial input X_r to manifold of clean shapes X_c. The discriminator tries to tell between encoded clean shapes (synthetic data point clouds) and mappings of noisy input (point clouds from real-life data 3D scans). <sep> An encoder-decoder network similar to those in Achlioptas et al. (2018) and Qi et al. (2017a) is trained to transform original point clouds to a low-dimensional latent space prior to training the GAN. The authors find that using the encoder-decoder trained on clean shape data even for noisy input yields better results. <sep> One of the issues of completing noisy and partial scans is that the desired complete scan can have a very different shape compared to the noisy input. The generator can map latent vectors to any points on the target manifold which allows it to generate shapes that are far different from the original inputs. In order not to generate random clean shapes, the authors add a reconstruction loss to the generator which encourages it to preserve the partial shape of the input end reconstruct it in the completed clean shape. The choice of Hausdorff distance for reconstruction loss is sound and the ablation study confirms it. <sep> The authors perform rather extensive experimental evaluation of their proposed method. They perform qualitative and quantitative analysis on several datasets, both real-life and synthetic ones. The proposed method outperforms existing methods in real-life data scenario. On the synthetic dataset (3D-EPN), the quantitative results are not as good as those of PCN (which is a supervised method, unlike the proposed method), but the qualitative results look plausible and comparable to PCN results. In terms of plausibility score, the proposed method outperforms existing methods in all experiments, which is probably thanks to its objective - map the input into the latent space of clean and complete shapes. However, plausible looking point clouds do not necessarily have to precisely match the input, which is the objective of 3D scan point cloud completion. <sep> The ablation study also confirms the effectiveness of individual parts in the proposed method. <sep> The main contribution of this paper is training with unpaired data, which enables training on real-life data, leading to better results on real-life scans. While I understand the difficulties, I believe it would be better to try to focus more on real-life data in the evaluation. There is only one experiment with quantitative analysis on real-life data in the paper. Supporting that with a visual Turing test would have been great. <sep> Questions raised: <sep> In 4.3, you say that ""ground truth complete scans are not available for training."" How do you then train the supervised methods? Are they trained on 3D-EPN? In that case, is your proposed method also trained on 3D-EPN? If your method is the only one trained on your synthetic data (dataset D), which is also used for testing, then I do not think you could claim that your method is better at dealing with different data distributions. Please make clear in the comments what data you use for training in this experiment. <sep> The meaning of section 4.4 is not very clear to me. If my understanding is correct, when completing noisy or partial point clouds, low diversity in results is better than high diversity because it is a task of repairing the input, which only has one correct result. Are you trying to say in this section that your method yields more consistent results with lower diversity than the method from previous work? If that is the case, you should consider rewriting that part to make it clearer to readers. <sep> The PCN paper (Yuan et al., 2018) shows that PCN can generalize and complete point clouds of objects unseen during training very well. Unfortunately, this paper does not discuss performance on objects of unseen classes. Were such experiments considered? It would be beneficial to do a qualitative analysis of performance on unseen objects. <sep> The description of the left table in Table 1 first says that it shows performance on real-life scans, but performance on synthetic data also appears there. Shouldn't it rather say that it is plausibility comparison on real-life scans and synthetic data? <sep> Summary: <sep> The proposed method is easy to understand, exploits recent progress in generative models, and allows training on real-life scans as it does not require paired training data. <sep> The paper does not contain much algorithmical novelty and mostly combines existing methods to solve the problem of obtaining clean and complete point clouds from real 3D scans. However, the ablation study shows that adding a good reconstruction loss to the generator is crucial for the performance. <sep> The main strengths of this paper are extensive experimental evaluation using both quantitative and qualitative analysis, and significant improvement in performance on real-life data over previous works. <sep> The main weakness is limited novelty in terms of the techniques used in the proposed method. <sep> Additional comments that do not affect the review decision: <sep> Citations in the text have to be revised and correctly put into parentheses where necessary. E.g., on page 2: ""Since its introduction, GAN Goodfellow et al. (2014) has been used..."" should be rewritten to ""Since its introduction, GAN (Goodfellow et al., 2014) has been used..."" <sep> Table 2 is the only table where the best results are not highlighted by bold text. It is also the only table where the proposed method is not the best performing method as it loses to PCN. I apologize if it is just a pure coincidence but it seems as if the authors did not want to draw attention to the fact that their proposed method loses to an existing method in that experiment. I believe that you should be fair and highlight best results in all tables.","This paper presents an unsupervised method for completing point clouds obtained from real 3D scans based on GAN. Generally, the paper is well-organized, and its contributions and experimental supports are clearly presented, from which all reviewers got positive impressions. <sep> Although the technical contribution of the method seems marginal as it is essentially a combination of established methods, it well fits in a novel and practical application scenario, and its useful is convincingly demonstrated in intensive experiments. We conclude that the paper provides favorable insights covering the weakness in technical novelty, so I'd like to recommend acceptance."
"rating_summary  ==> In this paper, the authors propose an approach to learning combinations of (instance-wise) distance metrics and (cluster-wise) merge functions to optimally cluster instances from a  particular data distribution. In particular, given a set of clustering instances (each of which is a set of instances from the domain and their cluster assignment), a set of distance metrics, and a set of merge functions, the proposed approach aims to learn a convex combination of the distance metrics and merge functions to reconstruct the given clusterings. <sep> The paper has two main contributions. First, a PAC learning type of guarantee is given on the quality of the learned clustering approach. Second, an efficient data structure for identifying the convex combinations is given. A small set of experiments suggests that, in practice, the learned combinations can outperform using single distance metrics and merge functions. <sep> Comments <sep> I am not an expert in this area; I had trouble following the details of the theoretical developments. However, I appreciated that intuition was given on both what the theorems and lemmas were showing as well as the main steps of the proofs. <sep> Concerning Theorem 1, it is not exactly clear to me what the contribution is on top of [Balcan et al., 2019]. The text mentions that they already give sample complexity guarantees in what seems like the same setting (piecewise-structured cost function). <sep> The authors point out that depth-first traversal is a good choice here due to its memory efficiency. However, in cases where the search space is a graph rather than a tree (i.e., there are multiple paths to some nodes), then DFS can exponentially increase the work compared to breadth-first or other search strategies (e.g., [Edelkamp and Schroedl, 2012]). While the name suggests that the ""execution tree"" is, indeed, a tree, is this guaranteed to be the case? or could multiple paths lead to the same partition? <sep> For the experimental evaluation, it seems as though there is no ""test"" set of clustering instances. It would be helpful to also include performance of the learned combinations on some test clustering instances to give an idea of how generalizable to approach is to other instances within the data distribution. (Of course, the main contributions of this work are the theoretical developments, so just one or two examples would be sufficient.) <sep> For motivation, it would be helpful to give some examples where the prerequisites of this work are actually met; that is, cases where sufficiently large number of labeled cluster instances are available, but the generative mechanism of the clusters is not. <sep> For context, it could be helpful to briefly mention how, if at all, the current results apply to widely-used clustering algorithms such as k-means or Gaussian mixture models. <sep> Typos, etc. <sep> The references are somewhat inconsistently formatted. Also, some proper nouns in titles are not capitalized (e.g., ""lloyd's families""). <sep> ""leaves correspond to"" -> ""leaves corresponding to"" <sep> What does the ""big-Oh tilde"" notation in Theorem 1 mean?",All reviewers come to agreement that this is a solid paper worth publishing at *CONF*; the authors are encouraged to incorporate additional comments suggested by reviewers.
"abstract | suggestion  ==> This paper introduces the problem of overlearning, which can be thought of as unintended transfer learning from a (victim) source model to a target task that the source model's creator had not intended its model to be used for. The paper raises good points about privacy legislation limitations due to the fact that overlearning makes it impossible to foresee future uses of a given dataset. <sep> Background on censoring is well developed, and helps position the submission. However, the relation to transfer learning is not sufficiently outlined in the introduction. In some ways, overlearning is a form of unintended transfer learning. In particular, the connection appears explicit in the case of model repurposing (Section 3.2). Editing the introduction to tease apart this relationship to transfer learning would help readers forge an intuition for what the paper considers. <sep> While the de-censoring algorithm is intuitive, it is not clear what assumptions are being made because of ambiguous notation in Algorithm 1. How does Algorithm 1 train E_aux and C_aux? Does that step assume the adversary has knowledge of the algorithm use to train E and C? Descriptions of the experimental setup from Section 4.2 seem to indicate that this is the case. This is not necessarily an issue if the proposed attack is demonstrating a limitation of learning rather than a practical attack. <sep> Experiments overall show that censoring does not mitigate overlearning in a way that is robust to de-censoring. One aspect of the experiments that is unclear is the auxiliary dataset: what is the auxiliary dataset used by the adversary for each of the datasets considered in experiments? <sep> Question: how does simultaneous censoring of all layers affect overlearning and repurposing? <sep> Question: what is the connection between learning more complex representations and overlearning? Is the intuition that if the representation is more complex, it is more likely to contain features useful to identify the sensitive attribute? <sep> The paper is overall well-written. Some parts of the paper omit important details (perhaps due to space constraints?), but an editorial pass should address most of these. Detailed feedback: <sep> 1 / Explaining what model partitioning means in this context would help make the introduction more self-contained. <sep> 1 / Is overlearning specific to attributes that raise fairness issues? Or is this phenomenon more general? The ""Censoring representations"" paragraph on page 2 seems to indicate that the phenomenon is more general. <sep> 5/ Is accuracy the best metric to report (e.g., for race attribute prediction) given that the random guessing figure suggests the data is not balanced across attribute values? <sep> 6/ Why is the effect not monotonic in Table 4? Were multiple runs averaged?","This paper introduces the problem of overlearning, which can be thought of as unintended transfer learning from a (victim) source model to a target task that the source model's creator had not intended its model to be used for. The paper raises good points about privacy legislation limitations due to the fact that overlearning makes it impossible to foresee future uses of a given dataset. <sep> Please incorporate the revisions suggested in the reviews to add clarity to the overlearning versus censoring confusion addressed by the reviewers."
"abstract | rating_summary | rebuttal_process | misc | strength | decision | suggestion  ==> This paper looks at how deep convolutional neural networks for image denoising can generalize across various noise levels. First, they argue that state-of-the-art denoising networks perform poorly outside of the training noise range. The authors empirically show that as denoising performance degrades on unseen noise levels, the network residual for a specific input is being increasingly dominated by the network bias (as opposed to the purely linear Jacobian term). Therefore, they propose using bias-free convolutional neural networks for better generalization performance in image denoising. Their experimental results show that bias-free denoisers significantly outperform their original counter-parts on unseen noise levels across various popular architectures. Then, they perform a local analysis of the bias-free network around an input image that is now a strictly linear function of the input. They empirically demonstrate that the Jacobian is approximately low-rank and symmetric, therefore the effect of the denoiser can be interpreted as a nonlinear adaptive filter that projects the noisy image onto a low-dimensional signal subspace. The authors show that most of the energy of the clean image falls into the signal subspace and the effective dimensionality of this subspace is inversely proportional to the noise level. <sep> Even though it is theoretically not too well-motivated in the paper why the bias term degrades generalization performance, the experimental results seem to clearly demonstrate the merit of bias-free denoisers. Moreover, the analysis of the network Jacobian and its interpretation as a nonlinear adaptive filter provides some interesting insight in the local properties of bias-free denoisers. Therefore, I would recommend accepting this paper, if the authors provide a theoretical discussion on why the bias term might degrade generalization performance. <sep> Some smaller comments: <sep> -It is not clear if d should be multiplied by \\sigma^2 in its definition on page 7. The definition mentions dependence on noise variance, but the formula does not have it. <sep> -In Section 3 in the expression of the mean squared error it is not defined what g(y) means. <sep> -Axis labels are missing on Fig. 3.","This paper focuses on studying neural network-based denoising methods. The paper makes the interesting observation that most existing denoising approaches have a tendency to overfit to knowledge of the noise level. The authors claim that simply removing the bias on the network parameters enables a variety of improvements in this regard and provide some theoretical justification for their results. The reviewers were mostly postive but raised some concerns about generalization beyond Gaussian noise and not ""being very well theoretically motivated"". These concerns seem to have at least partially been alleviated during the discussion period. I agree with the reviewers. I think the paper looks at an important phenomena for denoising (role of variance parameter) and is well suited to *CONF*. I recommend acceptance. I suggest that the authors continue to further improve the paper based on the reviewers' comments."
"abstract | misc | rebuttal_process | misc | rebuttal_process | strength | misc | suggestion  ==>  ==> This paper revisits Random+FGSM method to train robust models against strong PGD evasion attacks. Coupled together with tricks for accelerating natural training, such as cyclic learning rate, mixed precision, the robust models can be trained faster than previous methods. <sep> +The experimental results are impressive. The trained model is robust (at Madry's PGD level), and the total training procedure is fast (6 min for CIFAR-10 and 12 hr for ImageNet). <sep> + The method is simple, and I guess reproducible. <sep> +The paper shows surprising facts of a well-known method. <sep> +The paper is generally well-written and easy to follow. <sep> I do have some concerns of the work <sep> - The paper is empirical and the techniques are combinations of previous methods. Even for the surprising fact that Random+FGSM, it has been discussed in several previous papers, for example,  *CONF* 2019 Defensive Quantization: When Efficiency Meets Robustness  https://openreview.net/forum?id=ryetZ20ctX. So the main contribution of the paper is limited to show RFGSM works well when combined with optimization tricks like cyclic learning rate. <sep> -In previous methods claiming random+FGSM can train robust model, their method seems to be slightly different from Alg 3 in page 4 of this paper. The alg in this paper seems to be identical to Madry's implementation of R-FGSM, which is shown not robust to PGD attacks. See discussions in https://openreview.net/forum?id=rJzIBfZAb and https://openreview.net/forum?id=ryetZ20ctX. I would like the authors to clarify their method to resolve such conflicts and make it clear how R-FGSM can be as robust as PGD as in table 1. <sep> -The first two paragraphs of section 4.1 seem to be inaccurate. One important trick in the ""adversarial training for free"" paper is to replay each minibatch m times. It is hard to say how much nonzero initialization helps. According to ""universal adversarial training"" (https://arxiv.org/pdf/1811.11304.pdf). It may help, but cannot compete with Madry's PGD training when defending against PGD attacks. <sep> -I am not sure if using a larger norm 1.25 * \\epsilon is a fair comparison. A baseline of PGD training bounded by 1.25 * \\epsilon would help. <sep> -Could the authors combine table 4 and 5 for easy comparison of robust accuracy and training time? Did the authors try the optimization tricks on ImageNet for the baseline free adversarial training method? <sep> ================== after rebuttal ================= <sep> I change my rating to weak accept. I tend to accept for the following reason <sep> (1) there seems to be no obvious flaw in the authors implementation. I quickly skimmed their code, and looks like a few researchers have tried their code and responded in public discussion. The surprising robustness of RFGSM, though the originality is questionable and the technical difference comparing to previous methods are subtle, seems to hold true. <sep> (2) The authors work hard to address the comments. <sep> I still have some concerns, mainly regarding the fairness of experimental comparison. <sep> (1) As pointed out in public discussion, the success of the proposed RFGSM relies on early stopping. I am wondering if early stopping also helps other methods since it turns out to be some sort of selection procedure. <sep> (2) The authors did not update time in table 1 for CIFAR-10 results, which I consider almost no extra efforts. I am wondering how much more time each method needs from 45% in figure 2 to higher robust accuracy in table 1. <sep> (3) I cannot understand why the proposed method is a particular good fit with cyclic LR and low precision tricks comparing to other methods.","This paper provides a surprising result: that randomization and FGSM can produce robust models faster than previous methods given the right mix of cyclic learning rate, mixed precision, etc. This paper produced a fair bit of controversy among both the community and the reviewers to the point where there were suggestions of bugs, evaluation problems, and other issues leading to the results. In the end, the authors released the code (and made significant updates to the paper based on all the feedback). Multiple reviewers checked the code and were happy. There was an extensive author response, and all the reviewers indicated that their primary concerns were address, save concerns about the sensitivity of step-size and the impact of early stopping. <sep> Overall, the paper is well written and clear. The proposed approach is simple and well explained. The result is certainly interesting, and this paper will continue to generate fruitful debate. There are still things to address to improve the paper, listed above. I strongly encourage the authors to continue to improve the work and make a more concerted effort to carefully discuss the impacts of early stopping."
"abstract | strength | weakness  ==> This paper describes an approach to learn word embedding functions more efficiently and with fewer parameters. This is done by replacing the embedding lookup function which is typical in NLP tasks such as language modeling and machine translation with a hierarchical embedding model. This allows for a low dimensional embedding layer, reducing total parameters and training time. A novel skip-connections architecture is introduced as a part of the ""embedding generation model"". Experiments are conducted for language modeling and machine translation tasks and performance improvements are observed with a reduction in parameters and lesser training time. <sep> The direction of this work is nice, the problem that is being tackled is indeed important. <sep> The obtained results are nice (though this can be improved) and there is indeed some potential value in this work. <sep> However, I have the following concern. The paper completely ignores a lot of previous and concurrent work in reducing the size of the embedding layer. These works are in most cases not even cited and no empirical comparisons are provided. For example, please see below works in matrix factorization approaches, sparse word representation learning, codebook learning and other quantization approaches for compressing word embeddings: <sep> https://www.aclweb.org/anthology/P16-1022/ <sep> https://aaai.org/ojs/index.php/AAAI/article/download/4578/4456 <sep> http://web.cs.ucla.edu/~chohsieh/papers/Mulcode_Compressor.pdf https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/viewFile/17042/16071 <sep> https://storage.googleapis.com/pub-tools-public-publication-data/pdf/f158f7c81ed8e985fd51a20d193103ce427cad51.pdf https://arxiv.org/pdf/1711.01068.pdf https://arxiv.org/abs/1510.00149 <sep> I would appreciate if comparisons with some of these approaches is provided in the next iteration of this work. <sep> Other suggestions: <sep> 1. I think the paper would benefit from some analysis of the differences in the word embeddings learnt by a general lookup table learning model in comparison with the word embeddings learnt by this model. How are the embeddings compressed? How do the decompressed embeddings compare to the embeddings learnt by the lookup approach? More insights in the machinery via some visualizations would help. <sep> 2. How do the gains of this method change as more or less training data is provided. For example, are the gains lesser on Gigaword? This would be interesting to know. <sep> 3. GLT is mentioned twice in this paper. Perhaps a slightly more detailed explanation of the same would help improve the readability of this paper. <sep> Based on the presentation improvements and new experiments added to the paper in the rebuttal time period, I am updating my evaluation of this work.","The authors design a deep model architecture for learning word embeddings with better performance and/or more efficient use of parameters. Results on language modeling and machine translation are promising. Pros: Interesting idea and nice results. New model may have some independent value beyond NLP. Cons: Empirical comparisons could be more thorough. For example, it is not clear (to me at least) what would be the benefits of this approach applied to whole words versus a competitor using subword units."
"abstract | decision  ==> Summary: This paper presents an approach for generating confidence set predictions from deep networks. That is, the smallest set of predictions where the true answer is included in that set. Theory is used to derive an algorithm with PAC-style bounds on the population risk. <sep> Overall Assessment: I like the core idea of this paper---developing region-based prediction algorithms with theoretical backing by taking advantage of a small calibration dataset and simple models of uncertainty. However, there are significant clarity and evaluation issues that must be addressed before the paper is ready for publication.  In current form, I rate the paper between Weak reject and strong reject overall. <sep> Strengths: <sep> + Nice general direction. <sep> + New algorithm for confidence set prediction with theoretical guarantees. <sep> + Experiments show favourable performance vs a few ablations. <sep> Weaknesses & Questions: <sep> 1. It is unclear to me why the temperature scaling component is needed. It does not actually change the ordering of the probabilities assigned to each class, so from what I can tell all it does is change the optimal T, but not the final performance. <sep> 2. The paper presents a bound on the population risk, but the experiments do not include a comparison of the expected worst-case error rates with the empirical error rates achieved on the test set. This should be corroborated. <sep> 3. The  description for the Model-based reinforcement learning subsection is impossible to follow due to ambiguity and lack of detail overall. <sep> 3.1. Some specific confusions about this section: Paper overloads f to be both a deterministic transition function, and also a distribution over possible states. It also switches between using x_{t+1} / x_t and x^\\prime / x to mean the same thing. The notation used to describe the multi-step setting also seems to use ""t"" for two different things: the current time-step, and some arbitrary time-step in the past. <sep> 3.2. It is unclear what metric is being used to evaluate the state transition models---L2 distance between what? Given that the predictions are not point estimates, I would expect something that takes uncertainties into account. <sep> 5. For both sets of experiments (classification and RL), there are no alternative methods used as points of reference. There are a multitude of other approaches incorporating uncertainty into predictions, as mentioned in Section 1. A trivial baseline is to heuristically generates confidence sets by trusting the probabilities produced by the model and building a set out of the classes corresponding the top 1-\\epsilon probability mass should be essential. This could be improved by applying difference calibration approaches to make the probabilities more trustworthy. Model ensembles provide another easy baseline. Such simple baselines are a minimum expectation, before even getting to state of the art alternatives. <sep> 6. There seem to be no vanilla regression experiment, only the harder-to-interpret RL experiment. EG: Since we already have a vision context: If you are doing facial age estimation , or interest point tracking, could one make a PAC prediction about the true age region/interest point location? <sep> 7. Overall the paper introduction misses some explanation on the motivating scenarios where such confidence set predictions are useful. One can perhaps imagine this for vision, but some help connecting the dots to how it could be useful in regression and RL would help. <sep> 8. It is claimed that Theorem 1 provides a ""better"" bound than the one based on the VC dimension. What is meant by better? <sep> Minor comments: <sep> * There are a couple of small mistakes in the proof of theorem 1. The \\tilde{X} and \\tilde{Y} in the definition of \\tilde{Z}_{val} are in the wrong place. The ""sum of k i.i.d. random variables"" should be ""sum of n i.i.d. random variables"". <sep> * In general, the proof is quite hard to follow. At times it was quite unclear how you get from one step to the next, because it relies on something shown several steps early, which is not referenced. <sep> * Notation, in general, is a bit of an issue in this paper. See comments above, but also: switching between \\theta and T for the parameter used in the confidence set predictor, some confusion between T and \\hat{T}. <sep> * It is unclear how the neural network used in model-based RL predicts a PSD covariance matrix. <sep> * \\hat{T} is not defined when it is first referenced (Section 3.3). <sep> * Algorithm 1 appears several pages before it is referenced. <sep> -------------- POST REBUTTAL ------------ <sep> I modify my score to 6: Weak accept.","This paper describes a method for bounding the confidence around predictions made by deep networks. Reviewers agree that this result is of technical interest to the community, and with the added reorganization and revisions described by the authors, they and the AC agree the paper should be accepted."
"abstract | strength  ==> ** Summary <sep> The paper studies how to recover the span of a NN from a limited number of queries. The problem belongs to the general question of how to reconstruct functions from black-box interaction and it may find application in obfuscation attacks where very large perturbations of the input do not affect the output. The main contribution of the paper is on the theoretical analysis of a simple non-adaptive and a more sophisticated adaptive algorithm. The main finding is that under mild conditions on the structure of the NN partial recovery is possible. The empirical validation show that in practice, it is often the case the full span recover is actually possible, as the structure and weights of common NN are ""friendly"" enough. <sep> ** Evaluation <sep> While the content of the paper lies a bit off my expertise, my impression is that this is a solid technical and theoretical contribution. <sep> Detailed comments: <sep> 1- Theoretical results: The properties proved in Thm.3.4 and 4.3 are quite powerful, showing that (partial/approximate) span recovery is possible with a relatively small amount of samples (of order of n*k, where n is the original input size and k is the size of the span), in a computationally efficient way (in particular for the non-adaptive algorithm), and for Relu NN or NN with differentiable layers and final threshold function. The main question is of course the validity of the assumptions needed to prove the theorems. Asm.1 and 2 are overall reasonable and they are very well supported by Lemma 3.1. The first two assumptions in page 6 are straightforward, while I'll less convinced of 3 and 4. In fact, they need to hold for any subspace V of any dimension smaller than k. I wonder whether the assumptions may become less and less likely as the size of the subspace decrease. <sep> 2- The theorems and the paper are mostly well written but some parts may be clearer. <sep> 3- Alg.1: the computation of the gradient is never really explained apart from the high-level lemma 3.2. While an actual algorithm is reported in the appendix, it would be better to have it explained already in the main text. <sep> 4- Right after Lemma 3.2 it is said ""which demonstrates the claim"". I am not sure which claim it refers to. <sep> 5- In alg.1 there is a parameter r which defines the number of queries of the algorithm. Thm 3.4 provides an upper bound on the number of queries needed and it does depend on k. Since k is initially unknown, how do you actually parameterize the algorithm? is there a stopping condition that can be tested? <sep> 6- In Thm 3.4 it is said that the algorithm returns the subspace V in time that is polynomial in the main parameters of the problem. Yet, I'm not sure where such complexity comes from. In Alg.1 it seems like the subspace is the direct output of the algorithm, so the complexity is r times the cost of computing the gradient, which according to Lem3.2. is poly(n). Is this the way you finally obtain the complexity? <sep> 7- One thing I'm doubtful about is the fact that the result in Thm 3.4 seems to be independent from the depth d and width k_i of the different layers. Some conditions may be implicit in the Asm.1 and 2, though. Furthermore, in the experiments it is clearly showed that thin NNs may make the support not recoverable. Could you please make such limit more explicit in the theory? <sep> 8- In alg.4 I think lines 5-7 are just the way to execute line 4. Is that correct? If not, how do you execute line 4? <sep> 9- In alg.4 line 8 and 9 are not easy to follow and they are not really discussed in the main text. Could you please clarify? <sep> 10- The empirical validation is relatively simple but it illustrates quite well the theory. Still I wish the authors could report results that dig more in detail in the theoretical results showing how tight they are (e.g., in the dependency on n, k, and other factors). The current results provide just a hint on how accurate/informative the theory is. <sep> 11- In the empirical result, it would be great to have a much more thorough validation of the difference between the non-adaptive and the adaptive algorithms. In the current results it seems like there is very limited difference.",The authors propose a way to recover latent factors implicitly constructed by a neural net with black box access to the nets output. This can be useful for identifying possible adversarial attacks. The majority of reviewers agrees that this is a solid technical and experimental contribution.
"abstract | strength | rebuttal_process | rating_summary | decision  ==>  ==> This work is focused on learning 3D object representations (decoders) that can be computed more efficiently than existing methods.  The computational inefficiency of these methods is that you learn a (big) fixed decoder for all objects (all z latents), and then need to apply it individually on either each point cloud point you want to produce, or each voxel in the output (this problem exists for both the class of methods that deform a uniform distribution R^3 -> R^3 a la FoldingNet, or directly predict the 3D function R^3 -> R e.g. DeepSDF). The authors propose that the encoder directly predict the weights and biases of a decoder network that, since it is specific to the particular object being reconstructed, can be much smaller and thus much cheaper to compute. <sep> The authors then note the fact that their method lacks a continuous latent space that allows for interpolation, as provided by existing (VAE-like) methods. They propose to solve this by learning an MLP that produces the output by recurrent application, and then composing subapplications of different networks as a type of interpolation. <sep> ------------------- <sep> I like this work, it addresses a real problem in a number of models for 3D representation learning (similar models are also used for e.g. cryo-EM reconstruction). While the fast weights approach is not totally original, its application to this problem is novel and very well-suited to it. I was a bit surprised by just how much the decoder network could be shrunk by using fast weights. <sep> The paper is also quite well written. I especially like how Section 2 synthesizes existing work into model categories which make it easier to think about their relationships. I also think the explanation in Sec. 3.2, while kind of obvious, is a nice way think about decoder vs. fast weights. <sep> I like that the authors are straightforward about the deficiency of the method (i.e. that you can't interpolate in latent space). Their proposed solution of functional composition is exceedingly clever but in my opinion too impractical to really be useful. It adds extra complexity, requires you to do function composition which may be less expressive and takes more coomputation, etc. And to what end? The purpose of generative models is not to interpolate per se; the interpolation is really a sanity check that the model is capturing the underlying distribution rather than just memorizing training examples. The function composition doesn't capture that. I think the authors should just acknowledge that you can't soundly *sample* from their generative model the way e.g. VAE or GAN allows (their function composition is not a sampling method). But I think there are lots of useful things you can do without that capability, e.g. do 3D point cloud completion, go image -> structure, etc. I think this function composition angle should be deemphasized in the title/abstract, but I think the paper stands  reasonably on its own without that. <sep> Nits: <sep> - In Figure 2 it's pretty hard to see the differences between the methods. What exactly is being visualized here? DeepSDF shold be visualizing surface normals vs. HOF which is point clouds, right? <sep> - For predicting a deformation R^3 -> R^3 function composition sort of makes sense, but how generalizable is this approach e.g. to directly predicting a function R^3 -> R (a la DeepSDF)? I think there are ways this function composition approach could generalize, e.g. using skip connections and layer dropout (which encourages layers to be composable).","The submission presents an approach to single-view 3D reconstruction. The approach is quite creative and involves predicting the weights of a network that is then applied to a point set. The presentation is good. The experimental protocol is well-informed and the results are convincing. The reviewers' concerns have largely been addressed by the authors' responses and the revision. In particular, R2, who gave a ""3"", posted ""I would now advise to raise my score (3 previously) to a be in line with the 6: Weak Accept given by the other reviewers."" This means that all three reviewers recommend accepting the paper. The AC agrees."
"misc | abstract | rating_summary | suggestion  ==>  ==> *** Update *** <sep> I'd like to thank the authors for answering my questions, and I am satisfied with their response. I have read the other reviews for this paper as well, and I am keeping my score. <sep> This paper proposes BERTScore, a method for automatic evaluation of text. Their method uses BERT to produce contextualized word representations for the words in the reference and hypothesis. Then they compute the precision, recall, and F1 by greedily matching up words between the hypothesis and reference. To be more specific, for say recall they take each word in the reference and compute the cosine with all words in the hypothesis. Then they add up the largest cosine similarity for each word and average them together. Precision is defined similarly but with the roles of hypothesis and reference switched. F1 is then the harmonic mean of these two scores. They also experiment with using idf to weight importance. <sep> Their method is simple, but achieves very strong results and there are a ton of experiments in this paper (it is 41 pages). The focus is largely on metrics for MT, but they also evaluate on image captioning. The paper is also very thorough and many of the questions I had when reading it are answered (like effect of optimal matching, running time, etc.). The latter (running time) being one of the downsides of the method if it was to be used for fine-tuning MT systems. 40 times slower than BLEU, but I think this increased cost would be worth it and could be engineered around. <sep> Overall, I like the paper - it is simple and effective on its goal task of automatic evaluation for text generation. I think we are moving that way as a field and this paper proposes a useful method and is additionally a good study on the subject. <sep> A question I have is why the method doesn't perform well in certain cases. For instance, in Table 2 and 3 - some of the evaluations with tr and fi fall well below relative performance for other language pairs. Does this have to do with the quality of the representations in multilingual BERT? What is YiSi-1 doing, for instance for model selection of en-fi and en-tr that makes it have so much better performance? <sep> Edit: I also wonder if incorporating idf would be better if the values were computed by a larger corpus. I think it would make the most sense to compute these from the training data for the underlying BERT models. Since BERT itself is a function of this training data, it seems appropriate that these values would be as well (or perhaps at least a subset of this data). <sep> Missing citations: <sep> A citation to ""Beyond BLEU:Training Neural Machine Translation with Semantic Similarity"" from ACL 2019 should be incorporated into the related work. They use semantic similarity to fine-tune NMT systems with their own embedding-based (semantic similarity) metric and they found some nice properties from training in this way. Have you tried BERTScore on sentence similarity tasks? It's possible BERTScore could have strong performance and some readers may wonder this. There are evaluations on PAWS for paraphrase detection which I appreciated, but that is a little different. <sep> A citation to ""Deep Reinforcement Learning with Distributional Semantic Rewards for Abstractive Summarization"" from EMNLP 2019 should also be incorporated. This paper is a big boon to BERT score showing that it is a very helpful metric for fine-tuning summarization systems. They don't even need a cross-entropy term since BERTScore captures fluency so well. I'd like to see it for MT as well, but perhaps that is the next paper. <sep> Typos: <sep> The word ""language"" is misspelled twice in Appendix E.","Thanks for an interesting discussion. The authors present a supposedly task-independent evaluation metric for generation tasks with references that relies on BERT or similar pretrained language models and a BERT-internal alignment. Reviewers are moderately positive. I encourage the authors to think about a) whether their approach scales to language pairs where wordpieces are less comparable; b) whether second order similarly, e.g., using RSA, would be better than alignment-based similarity; c) whether this metric works in the extremes, e.g., can it distinguish between bad output and super-bad output (where in both cases alignment may be impossible), and can it distinguish between good output and super-good output (where BERT scores may be too biased by BERT's training objective)."
"abstract | misc | weakness | decision  ==>  ==> What is the specific question/problem tackled by the paper? <sep> This paper addresses the hierarchical RL problem of combining multiple primitive policies (pi_1, …, pi_K) into policies for more complex tasks. Given a number of primitive skills and a new task within an environment, the paper aims to learn to pick and combine the primitives as needed to solve the new task. This problem statement is interesting and the method performs well on difficult tasks. <sep> However, I argue for rejecting this paper because it lacks meaningful contributions to the field. I do not see how the method presented in the paper is more than RL over  hand engineered action spaces that are better for the tasks. While this improves results, we already know that for any task, there is some best action space for performing that task. This is why most HRL work aims to also find the primitive policies in additional to composing them. Is this any better than option-critic if the options are hardcoded to be the primitives? The experiment state that the option-critic method did not work, but did you give it access to the same primitives? <sep> Summary: <sep> The method presented in the paper is as follows: at each state s_t, the K primitives are queried for their action a_k ~ pi_k(s_t). Then, a biRNN reads in the actions in order from 1 to k. In parallel, the state and a goal are encoded by a network named ""Decoder"". The encoded state and the hidden states of the RNN are used to output an attention weight over each primitive. Finally, the output action is the weighted combination of all the actions. The encoders and attention weights are trained with RL. <sep> This method is evaluated on several mujoco tasks, such as making a cheetah jump hurdles by combining ""forward"" and ""jump"" primitives. Each environment has predefined primitives such as ""forward"" ""left"" ""right"" etc. <sep> This method is compared against HIRO, which does not have access to the primitive policies. It is not surprising that hand engineering primitives helps performance. <sep> Is the approach well motivated? <sep> The general idea behind the approach is well motivated: using primitive skills to learn complex skills is a useful goal. The details of the method are strange. <sep> I would like to see a better motivation and empirical justification for the biRNN. Why should the primitive's action be encode in order? The ordering of the primitives is arbitrary and constant: a fully connected network could be used, or the attentions could be output entirely independently per primitive. <sep> In fact, I do see not why the primitives' actions need to be encoded at all. It would be much simpler for the encoder to look at (s_t, g_t) and output a discrete probability over the K primitives. <sep> The ablations in 5.2 are for outputting actions directly rather than mixture weights. The paper would benefit from ablations where mixture weights are output but without the biRNN or without passing in the primitive's actions. <sep> Is the method well placed in the literature? <sep> The main idea of predicting weights over multiple experts is not novel (see ""Adaptive mixtures of local experts"" from 1991). In the context of RL literature, we can interpret the primitive skills as actions directly, and then the method is performing basic RL over a better action space (the better actions being ""go left"", ""jump"" etc. We can also interpret these as options, but unlike options a single primitive is not followed for multiple time steps with a termination condition. Functionally, this is equivalent to regular RL using domain knowledge to engineer the action space.","This paper considers deep reinforcement learning skill transfer and composition, through an attention model that weighs the contributions of several base policies conditioned on the task and state, and uses this to output an action. The method is evaluated on several Mujoco tasks. <sep> There were two main areas of concern. The first was around issues with using equivalent primitives and training times for comparison methods. The second was around the general motivation of the paper, and also the motivation for using a BiRNN. These issues were resolved in a comprehensive discussion, leaving this as an interesting paper that should be accepted."
"weakness | decision  ==>  ==> *Summary* <sep> This paper studies the convergence of multiple methods (Gradient, extragradient, optimistic and momentum) on a bilinear minmax game. More precisely, this paper uses spectral condition to study the difference between simultaneous (Jacobi) and alternating (Gau\\ss-Seidel) updates. The analysis is based on Schur theorem and give necessary and sufficient condition for convergence. <sep> *Decision* <sep> This paper tries to study a phenomenon that has known a recent surge of interest due to the numerous practical minmax application: the impact of alternating updates in minmax optimization. This problem is challenging because most of the theoretical analysis techniques have been developed to analyse simultaneous updates. <sep> I think that this paper has treated well the related work and underlines well its contributions. <sep> Even if the main contribution are theoretical the authors of this paper provide some experiments to confirm that the theory provides some meaningful insights. <sep> However, I have the concern that the study is still limited to bilinear example and thus it is not clear how to apply it to non-bilinear objective (even locally because since the Jacobian has pure imaginary eigenvalues, even locally the Jacobian may have eigenvalues with negative real part). <sep> Moreover this work do not provides significantly better (factor 8 improvement in the constants) convergence rate that the one provided on the literature (Tseng 1995, Mokthari et al 2019, Gidel et al. 2019) to solve bilinear games. The contributions is more about proving new (interesting) analysis tools and showing a better robustness of GS updates with respect to hyperparameter tunning. <sep> To me, it is an accept. <sep> *Questions* <sep> - In Theorem 4.1 and 4.2 you use the word 'optimal exponent'. In what sense these algorithms are optimal ? Usually the sense of optimal is when you achieve the lower bound of convergence. Is it the case here ? <sep> - Right after theorem 2.3 you claim that GS updates simply leads to a shift of index for Li setting Lk+1=0 but it seems to me that then λL1 is missing in the sum. Is it just an unfortunate oversight? Does it affect the proofs of you main theorems ? These results only marginally (improve by a factor 8) improve upon Moktari et al. (2019). <sep> *Remarks* <sep> - To me the J and GS conditions in Theorem 3.2, 3.3 and 3.4 are more Lemmas than Theorems: They are condition that are hard to interpret and they have small interest if one cannot conclude on simple conditions (such as the ones you actually provide in the Theorem). My point is that providing such complicated conditions without condition is only half solve the problem of convergence.  In that sense, in the discussion of Theorem 3.4, you are a bit unfair with Gidel et al (2019) since unlike them you do not provide any convergence rate. On one side theorem 3.4 gives an interesting characterization for the convergence (that leads to the fact that at least one of the beat has to be negative) but if it does not lead to any rate it is a less interesting result. <sep> - In Table 1 and 2, α,β1 and β2 have not been introduced. It is thus hard to get the most of them. <sep> - Theorem 3.1 has been stated in previous works (like for instance Gidel et al 2019). You should not claim it as a contribution. <sep> === After rebuttal === <sep> I've read the authors's response. <sep> I agree with the concerns raised by Reviewer 2 regarding the experimental part and whether or not such theoretical study (far from the practical aspect) are of interest to the *CONF* community. <sep> I think that that paper would be more suited for a theoretical venue such as AISTATS (or ICML) but I also think that this work remains of interest to the *CONF* community. <sep> I am not convinced by the fact that this analysis could be generalized beyond bilinear game with a local analysis  because even an arbitrarily small perturbation can transform a bilinear example into a non-stable equilibrium (with no hope of local convergence), take for instance minxmaxy−ϵ∥x∥2+x⊤Ay+ϵ∥y∥2",All reviewers found the work interesting but worried about the extension to non-bilinear games. This is a point the authors should explicitly address in their work before publication.
"abstract | weakness | rating_summary  ==> This paper aims at exploring the properties of neural network training during the early phase. By some studies on the lottery ticket hypothesis, something important happens during the early phase of training so rewinding the network should go to these early phases instead of the initial phase. So, what is important during training? The paper explores this problem from four aspects through empirical studies: <sep> 1. By showing the various statistics through different training iterations, especially the gradient magnitude, the training phase is divided into three parts, and each part has different behaviors. <sep> 2. The paper explores what is more important for the early phase of training: signs of the weights or magnitude of the weights. <sep> 3. The paper explores what is more important for the early phase of training if we redistribute the weights, signs or magnitudes. <sep> 4. The paper explores how training depends on data. Giving weak information such as self-supervised information may work but giving wrong information such as random labels will hurt the performance. <sep> This paper studies the properties of deep neural networks. Through a series of carefully designed experiments, the paper shows what is important for the weights (magnitude or signs), and what is important for the data (weak information or random information). It enables a deep understanding of neural networks and may motivate new neural network compression methods to be proposed. Generally, the paper is well-written, although some parts can be improved. I would vote for acceptance of the paper. <sep> Some questions/suggestions to make the paper clearer: <sep> 1. It is better to have a table summarizing various results of the paper, to give readers an overall impression after going through so many detailed experimental results. <sep> 2. The results of Fig. 4 and Fig.6 can be inconsistent. In Figure 4, it says that signs are less important than magnitudes. In Figure 6, it says that signs are more important than magnitudes if shuffling filters and keep signs. Any explanation on the inconsistency? <sep> 3. There is no explanation of the ""weight trace"" in Figure 3. <sep> 4. It is not clear what is the difference between ""Init"" and ""Final"" in ""L2 Dist"" and ""Cosine Similarity"" in Figure 3. <sep> Finally,  it is said in the ""call for papers"", ""Reviewers will be instructed to apply a higher standard to papers in excess of 8 pages"". This paper is nearly nine pages, and higher standards should be applied. <sep> -------------------------------------- <sep> I am satisfied with the rebuttal. Since the paper is now within the 8 pages limit, I would not apply a high standard and increase my score.","This paper studies numerous ways in which the statistics of network weights evolve during network training. Reviewers are not entirely sure what conclusions to make from these studies, and training dynamics can be strongly impacted by arbitrary choices made in the training process. Despite these issues, the reviewers think the observed results are interesting enough to clear the bar for publication."
"abstract | weakness | decision  ==> The paper considers the problem of distributed multi-arm bandit, where M players are playing in the same stochastic environment. The goal of the paper is to have small over-all regret for all the players without a significant amount of communication between the players. <sep> The main contribution of this paper is obtaining regret ~root(M KT) with ~M bits of communication in MAB, and regret ~d*root(MT) with ~Md bits of communication in linear bandit setting. <sep> The main intuition of the algorithms in this paper is to do ""best arm identification"" with epoching: At every epoch t, the central server sends the set of possible best arms to each player and each player pulls it for 2^t /M times, followed by a communication round. Thus, the cumulative regret is comparable to having one player doing this epoch strategy for MT iterations, where the regret follows. <sep> The problem considered in this paper is interesting and the result is new, the technique looks simple on paper but it requires a masterful combination of known tricks in (linear) MAB to obtain the best bound. <sep> It seems that in the MAB setting, the lower bound could be further strengthened with a log(K) factor, since removing this factor would ultimately require ""dynamic epoching"" which is not possible with limited communication. This would mostly complete the picture in the distributed MAB regime. <sep> Missing citation: <sep> The authors are missing citations relevant to distributed MAB with collisions, see for example <sep> ""Non-Stochastic Multi-Player Multi-Armed Bandits: Optimal Rate With Collision Information, Sublinear Without"" <sep> After Rebuttal: I have read the authors' responses and acknowledge the sensibility of the statement.","This paper tackles the problem of regret minimization in a multi-agent bandit problem, where distributed learning bandit algorithms collaborate in order to minimize their total regret. More specifically, the work focuses on efficient communication protocols and the regret corresponds to the communication cost. The goal is therefore to design protocols with little communication cost. The authors first establish lower bounds on the communication cost, and then introduce an algorithm with provable near-optimal regret. <sep> The only concern with the paper is that *CONF* may not be the appropriate venue given that this work lacks representation learning contributions. However, all reviewers being otherwise positive about the quality and contributions of this work, I would recommend acceptance."
"abstract | rebuttal_process | strength | decision | weakness  ==> #rebuttal responses <sep> Thanks for the clarification!  However, I will keep my original score for these reasons: <sep> (1) Only 3 random seeds are used for each environment, which is not convincing as the variance of MAAC is large in some figures. <sep> (2) Baselines are only trained with 10^5 steps and do not converge. Thus it not fair to say that MAAC matches the asymptotic performance of model-free algorithms. <sep> #review <sep> This paper constructs a model-based policy optimization algorithm (MAAC) that uses the pathwise derivative of the learned model and policy across future timesteps. The terminal value function is used to improve stability.  The theoretical guarantee of the error of the model-based gradient is presented. Experimental results show that MAAC outperforms SAC, STEVE, and MBPO on four environments in terms of the sample efficiency. <sep> The experimental results are strong and I appreciate the plots of the gradient error in a simple task, shown in Figure 3. But I want to see the comparison of the final performance of each algorithm in these environments, and I doubt that the baselines do not converge. <sep> However, the paper is badly written. First of all, the authors claim that the pathwise derivate method is applied to optimize the objective function. But the detail of the method is missing. Secondly, I can not follow the procedure of how Q function is learned in the MAAC algorithm. Thirdly, Theorem 4.1 gives a performance improvement bound of the new policy w.r.t J_pi without the entropy term. But the MAAC algorithm optimizes the objective with the policy entropy term. <sep> Also, MAAC applies many techniques in other papers. The paper does not clearly show the advantage of each component: <sep> (1) I want to see the experimental results of MAAC optimizing the objective without the entropy. <sep> (2) The SVG(1) algorithm should be compared as a baseline as the SVG(1) also uses the gradient of the learned model to optimize the policy. <sep> (3) The policy and the Q function are optimized on both the real samples and the generated samples. I want to see the ablation study or justification on whether training on real samples or both samples. <sep> Questions: <sep> 1. How many independent runs are used in experiments? <sep> 2. Does the computation of the pathwise derivate method cost much time？ Is MAAC much slower than SAC? <sep> I am happy to increase my score if the authors justify these questions.","The authors propose a novel model-based reinforcement learning algorithm. The key difference with previous approaches is that the authors use gradients through the learned model. They present theoretical results on error bounds for their approach and a monotonic improvement theorem. In the small sample regime, they show improved performance over previous approaches. <sep> After the revisions, reviewers raised a few concerns: <sep> The results are only for 100,000 steps, which does not support the claim that the models achieves the same asymptotic performance as model – free algorithms would. <sep> The results would be stronger as the experiments were run with more than 3 random seats. <sep> In the revised version of the text, it's unclear if the authors are using target networks. <sep> Overall, I think the paper introduces some interesting ideas and shows improved performance over existing approaches. I recommend acceptance on the condition that the authors tone down their claims or back them up with empirical evidence. Currently, I don't see evidence for the claim that the method achieves similar asymptotic performance to model free algorithms or the claim that their approach allows for longer horizons than previous approaches."
"abstract | strength | suggestion  ==> Summary: This paper addresses the problem of representation learning for temporal graphs. That is, graphs where the topology can evolve over time. The contribution is a temporal graph attention (TGAT) layer aims to exploit learned temporal dynamics of graph evolution in tasks such as node classification and link prediction. This TGAT layer can work in an inductive manner unlike much prior work which is restricted to the transduction setting. Specifically, a temporal-kernel is introduced to generate time-related features, and incorporated into the self-attention mechanism. The results on some standard and new graph-structured benchmarks show improved performance vs a variety of baselines in both transduction and inductive settings. <sep> Pros: <sep> + Dynamic graphs are an important but challenging data structure for many problems. Improved methods in this area are welcome. <sep> + Dealing with the inductive setting is an important advantage. <sep> + Clear performance improvements on prior state of the art is visible in both transductive+inductive settings and node+edge related tasks. <sep> Cons+Questions: <sep> 1. Technical significance: Some theory is presented to underpin the approach, but in practice it seems to involve concatenating or adding temporal kernels element-wise to the features already used by GAT. In terms of implementation the concatenation in Eq 6 seems to be the only major change to GAT. I'm not sure if this is a major advance. <sep> 2. Insight. The presented method apparently improves on prior work by learning something about temporal evolution and exploiting it in graph-prediction tasks. But it's currently rather black-box. It would be better if some insight could be extracted about *what* this actually learns. What kind of temporal trends exist in the data that this method has learned? And how are they exploited in by the prediction tasks? <sep> 3. Writing. The English is rather flaky throughout. One particular recurring frustration is the use of the term ""architect"" which seems wrong. Probably ""architecture"" is the correct alternative. <sep> 4. Clarity of explanation. The paper is rather hard to follow and ambiguous. A few specific things that are not explained so well: <sep> 4.1. Eq 1+2 is not a sufficiently clear and self-contained recap of prior work. <sep> 4.2. Symbol d_T used at the start of Sec 3.1 seems to be used without prior definition making it hard to connect to previous Eq1+2. <sep> 4.3 The claim made about alternative approaches (Pg4) ""Reparameterization is only applicable to local-scale distribution family, which is not rich enough"". Seems both too vague and unjustified. <sep> 4.4 The relationship between ti and the neighbours of the target node in Eq. 6 is not very clear.","The major contribution of this paper is the use of random Fourier features as temporal (positional) encoding for dynamic graphs. The reviewers all find the proposed method interesting, and believes that this is a paper with reasonable contributions. One comment pointed out that the connection between Time2Vec and harmonic analysis has been discussed in the previous work, and we suggest the authors to include this discussion/comparison in the paper."
"abstract | strength | decision  ==>  ==> This paper aims to improve the efficiency of ensembles of neural nets in traditional supervised learning and life-long learning (learning on a series of tasks). The main idea is to let all the neural nets in an ensemble share the same weights W for each layer, and the weights for each neural net is generated by the Hadamard product of W and a specific rank-one matrix of the same size as W that is different across members in the ensemble. In experiments, they evaluate the method with some baselines on life-long learning, traditional classification, NMT tasks, and uncertainty modeling. <sep> The paper relates the proposed method to several different learning problems and applications and lists many potential advantages in these applications: it covers a lot of things. However, it lacks in-depth discussion to several key problems, rigorous analysis or complete experimental study to support the main claims, for example: <sep> Why can the simple method achieve a more compelling trade-off between accuracy and efficiency/memory costs comparing to a large single model or a naive ensemble of small models? Any mathematical or information-theoretical explanation behind that? <sep> It is easy to understand that the ensemble defined here can improve efficiency and reduce memory cost. But as an alternative to the naive ensemble, we also expect the performance to not suffer from severe drawbacks. How to control efficiency-performance trade-off in the proposed method? <sep> How were the baselines for each experiment selected? How to determine the specific setting in each experiment (any reason behind choosing the parameters in the settings)? <sep> In the life-long learning settings, the shared weights W is only trained on the first task and then keeps fixed: this can leads to both large variance and bias. Why does it simply work well without causing any serious problems? <sep> The rank-one extension of a shared model W enforces a very strong regularization to the model for each task. Will the method work promisingly when the tasks are more different from each other or harder to solve? For example, what if we increase the classes in each task? Is the rank-one extension still flexible and expressive enough to handle this situation? <sep> These are some of the most important questions needed to be answered in the first place before showing higher evaluation metrics and listing the potential advantages of the proposed method. But it is not clear to me at all how they can be answered according to the contents in the current paper. I notice that the authors mentioned the last two questions at the end of Section 3, but no explanations/discussions were given. <sep> Other major concerns: <sep> 1) Mathematically, comparing to single model Wx, the proposed ensemble method equals to applying a dimension-wise scaling to the input x and a dimension-wise scaling to the output Wx, and the scaling factors vary across different tasks. Hence, the proposed structure is exactly the same as fine-tuning two groups of batch normalization scaling factors before and after applying transformation W. It does not make much sense in the experiments that the performance of BN-Tuned in Figure 3a is much worse than the proposed method since they share exactly the same structure and math (note the memory and computational costs are also the same). The paper does not give an explanation about this. Moreover, the baseline BN-Tuned is only compared on only one of those datasets in the paper. It should be one of the most important baselines and needs to be compared in all experiments. <sep> 2) On each benchmark dataset (except the last one), only 1-2 baselines are compared and most baselines are not state-of-the-art methods or not methods specifically designed for the problem (e.g., many are dropout and its variants). This makes the comparisons not convincing, especially considering that the experimental settings are determined by the authors and might be chosen for the best performance of the proposed method. <sep> 3) At least two baselines should be included in all experiments: 1) single model with the equal number of model parameters, and 2) naive ensemble not sharing parameters across member models. However, each experiment only includes one or even none of these two baselines. <sep> 4) Memory and training/test computational costs need to be reported for each experiment. However, the currently reported results are incomplete here and there. <sep> 5) Comparing to the currently limited number of baselines on the incomplete evaluation metrics, the proposed method does not show significant improvements, for example, the results in Figure 4, Table 1 and Table 2. <sep> 6) The proposed method requires the models for different tasks should have exactly the same architecture. This could be a strong limitation in many scenarios. For example, when different tasks have significantly different numbers of classes.","This paper proposed an improved ensemble method called BatchEnsemble, where the weight matrix is decomposed as the element-wise product of a shared weigth metrix and a rank-one matrix for each member. The effectiveness of the proposed methods has been verified by experiments on a list of various tasks including image classification, machine translation, lifelong learning and uncertainty modeling. The idea is simple and easy to follow. Although some reviewers thought it lacks of in-deep analysis, I would like to see it being accepted so the community can benefit from it."
"abstract | strength | weakness | rebuttal_process | decision  ==> *** Increased to Accept from Weak Accept after author rebuttal and changes to the paper *** <sep> This paper proposes a method, SNOW, for improving the speed of training and inference for transfer and lifelong learning. SNOW starts with a pre-trained, frozen source model, and trains delta models for target tasks which, at each layer, concatenate a small number of task-specific features with the top-K most useful subset of features in the corresponding layer in the source model. As long as the target tasks are sufficiently related to the source task, it allows for small delta models and a small additional parameter overhead in the form of one weight per source model feature map. While there are (i) some issues with the presentation of results for training efficiency, (ii) some question marks over the sensitivity of the model to hyper-parameters, and (iii) several grammatical errors / typos in the manuscript, if these can be addressed I recommend the paper for acceptance because it seems to strike a superior balance of efficiency (regarding memory usage and inference speed) and accuracy when compared to a number of baselines, and to my knowledge it is a novel approach. <sep> Detailed comments: <sep> * Section 2.2 - How is sigma (the exploratory noise added for feature selection during training) chosen and how sensitive is the approach to its value? It seems like it was fine-tuned, given that a different sigma is chosen for the Action dataset (several orders of magnitude difference). In practice, tuning sigma could significantly increase training time. <sep> * It seems like the performance of only one run was plotted per hyperparameter setting - it would be informative to see a mean and standard deviation especially since the approach seems like it could be unstable for the wrong hyperparameter settings. <sep> * Related to the previous point, how much do the top-K feature selections change throughout training? One would have thought that this could cause instability during training for a high sigma. If sigma is too low, you could end up with suboptimal feature selection. <sep> * Figure 4 graphs are a bit misleading because the throughput on the x-axis is reported per GPU and the larger models all need 2 or more GPUs. While this is mentioned in the main text, it is still optically deceptive and the results are GPU-dependent - presumably if the GPUs had a larger memory, the larger models would not seems as slow. I think it would be clearer to plot images/sec on the x-axis or to rerun the experiments just using a single GPU. <sep> * It is stated that  ""[d]etermining K […] has a critical impact on both size and target accuracy in the target models"", where K is the number of feature maps in the source model that the delta model subscribes to in each layer. How sensitive is the accuracy exactly? Can this be quantified or discussed in more detail? <sep> * Furthermore, how sensitive is the performance to the number of target-model-specific features at each layer? <sep> * Different learning rate schedules were used for SNOW and baselines - initial lr for SNOW is 1.0, while for all other models it is 0.1. Was it checked whether the baselines improve when they are run with an initial lr of 1.0? Was this hyperparameter more heavily tuned for SNOW than for the baselines? <sep> * Since the source model is fixed, the applicability of the approach to lifelong learning is heavily dependent on the usefulness of the source model to subsequent tasks. If it is not, then one will have to incorporate large delta models. Furthermore, there can be no transfer between the tasks trained in the delta models. <sep> Grammatical errors / suggestions: <sep> * Page 1, first line: ""hallmark"" doesn't make sense in this context - maybe ""key objective"" or ""goal""? <sep> * Page 1, 2nd paragraph, first line: ""wee"" -> ""we"". <sep> * Page 1, 2nd paragraph, line 6: ""best top-K"" -> either ""K best"" or ""top K"" <sep> * Page 2, last paragraph, first line: ""three folds"" -> ""threefold"" <sep> * Section 2.1, line 2: ""pooing""->""pooling"". Same typo on Page 4, last line. <sep> * Page 6, line 1: ""training from the scratch"" -> ""training from scratch"" <sep> * Page 6, line 9: ""more 6x than"" -> ""6x more than"" <sep> * Overall, the manuscript needs to be proofread a few times.","This paper proposes a method, SNOW, for improving the speed of training and inference for transfer and lifelong learning by subscribing the target delta model to the knowledge of source pretrained model via channel pooling. <sep> Reviewers and AC agree that this paper is well written, with simple but sound technique towards an important problem and with promising empirical performance. The main critique is that the approach can only tackle transfer learning while failing in the lifelong setting. Authors provided convincing feedbacks on this key point. Details requested by the reviewers were all well addressed in the revision. <sep> Hence I recommend acceptance."
"abstract | decision  ==>  ==> The paper proposes a scheme to learn representations and a random forest model in a closely coupled manner, where the representations are learned to simultaneously improve the predictive performance of the individual trees in the random forest and reduce the correlation between these trees. This is achieved via a novel triplet sampling scheme where the positive samples are used to improve the predictive performance of the individual trees, while the negative samples are used to reduce the correlation between the trees. The empirical results indicate the improvement of the proposed scheme over random forests learned over the original representations across various datasets. <sep> The manuscript is well presented and I am leaning towards accept, one major issue with this manuscript is the number of pages which crosses the 8 page recommended limit in the *CONF* CfP. Given that higher standard, I feel that there are multiple minor issues which should be addressed. <sep> - It is not clear why the proposed scheme is especially well suited for ""domain generalization"" and ""visual recognition"" and not for general supervised classification tasks (like those on CIFAR, SVHN, <sep> etc). This needs to be clarified. <sep> - The theoretical results for random forest rely on the independence of the individual trees. Given that the trees in the random forest are no longer independent in the proposed scheme, the upper bound in <sep> Equation (1) may no longer be valid. While this does not affect the empirical performance, it might be good to discuss the theoretical implications of the proposed scheme. <sep> - It is not clear why the curves for the canonical features in Figure <sep> 3 not improving with the number of iterations (which corresponds to number of trees to the best of my understanding). The results in <sep> Table 1 do indicate improvement in the performance with canonical features, putting Figure 3 and Table 1 at odds. <sep> Minor: <sep> - The individual trees in a random forest are supposed to have low bias but high variance, and the heterogeneity between the trees is supposed to squash the variance with the bagging. The introduction mentions that the individual trees in the forest are ""less biased"". Maybe this is because of our different definitions of bias but any clarification here would be helpful.","The authors introduce an approach to learn a random forest model and a representation simultaneously. The basic idea is to modify the representation so that subsequent trees in the random forest are less correlated. The authors evaluate the technique empirically and show some modest gains. While the reviews were mixed, the approach is quite different from the usual approaches published at *CONF* and so I think it's worth highlighting this work."
"abstract | strength | rating_summary  ==>  ==> -- Overall -- <sep> This submission tackles to verify the ""under-sensitivity"" problem of neural network models in the natural language inference by ensuring modes do not become more confident in the predictions when arbitrary subsets of words from the input text are deleted. The authors developed new verification approaches based on decomposable attention mechanism with interval bound propagation (IBP), which can prove the under-sensitivity issue given a model and a particular sample. The experimental results on SNLI and MNLI show that the proposed approach leads to a much improved verified accuracy. <sep> -- In general, ""under-sensitivity"" is a very critical problem for applying neural models in natural language understanding where powerful neural networks tend to capture spurious correlations from the biased datasets. This submission formulates ""under-sensitivity"" as a mathematical specification and then try to verify it with IBP verification. Although the used technique IBP is not new, it would interesting to have the verification in NLI models. <sep> -- Section 5 is a bit unclear how to compute the IBP for deleting several words, and what is the output. It would be better to have a clear example for how this was computed. <sep> -- As the author mentioned, the verification of under-sensitivity can also be done by using beam-search, although it is costly and not accurate. IBP is another more efficient option, but not the optimal neigher. Maybe consider to change the title as ""efficient verification""? <sep> -- Specific Questions -- <sep> The entire paper builds on decomposable attention. Is the same approach also applicable to other model types, or only single layer attention-based models? <sep> Also, how this methods work for other NLI or NLU tasks? <sep> In experiments, how the data augumentation penalize the model with a loss for specification violation? What does the equation look like? <sep> Can you explain a bit more for IBP-training? How that hinge loss applies to the objective function? Is the IBP training differentiable?",This paper deals with the under-sensitivity problem in natural language inference tasks. An interval bound propagation (IBP) approach is applied to predict the confidence of the model when a subsets of words from the input text are deleted. The paper is well written and easy to follow. The authors give detailed rebuttal and 3 of the 4 reviewers lean to accept the paper.
"abstract | strength | rebuttal_process | decision  ==>  ==> =============================== Update after revisions ===================================================== <sep> In my initial review, I had raised some issues with the interpretation of the results and suggested some control experiments to tighten the conclusions. The authors chose to weaken their initial claims by rephrasing their conclusions instead. I understand that there may not have been enough time to run many of the experiments I suggested, but I still think they are worth considering for the future. I'm mostly satisfied with the rephrasing of the conclusions in the revised paper, so as promised, I'm happy to increase my score and recommend acceptance. <sep> I spotted several typos in the revised paper, however: section 4.1: ""we choose to consider negation ..."", p. 5: ""for for ..."", a citation on p. 5 is not compiled correctly. There may be more. For the final version please make sure to go through the paper thoroughly a couple of times and fix all the typos. <sep> ======================================================================================================== <sep> The authors present a systematic study of generalization in agents embedded in a simulated 3d environment. I think there are some interesting results in this paper that might be useful for people to know about. I appreciate the thoroughness of the experiments, in particular. I have, however, some issues with the interpretation of several of the main results. I would be happy to increase my score if we can resolve some of these issues. Here are my main concerns: <sep> 1) In the experiments in section 3, only a limited test set is used. How is the train/test split decided in these experiments? Table 6 suggests that you have a much larger repository of objects. Why not use all possible objects in the test set? It is a bit premature to declare your results as systematic generalization if you can't show that it actually works for a much larger set of test objects (ideally for all possible objects). <sep> 2) Section 4.1: in these experiments, the training set size is increased, but the test set size is kept constant (and small), so the train/test size ratio also increases. So, an alternative explanation of the results in this section is that the model behaves largely according to visual similarity and as the training set size is increased, it becomes easier to find a training set object that is visually similar to any test set object. I think the authors should run an experiment where both training and test set sizes increase by the same amount so that the train/test set size ratio stays constant. If the model can't achieve systematic generalization in that case, it would be wrong to conclude, as the authors do now, that increasing the training set size itself improves systematic generalization. The correct conclusion would rather be that increasing the train/test size ratio improves generalization, which is a weaker conclusion. Please note that the results in this section are quite similar to those in Lake & Baroni (2018) and in Bahdanau et al. (2018) (see their Figure 3). Bahdanau et al. (2018), for example, also show that increasing train/test set size ratio (their ""rhs/lhs"" ratio) improves generalization in generic neural networks. It is interesting to note, however, that neither Lake & Baroni (2018) nor Bahdanau et al. (2018) interpret these results positively (i.e., these results don't show systematicity), whereas the current paper seems to put a more positive spin on essentially the same result. I think these earlier results should be explicitly discussed here and the authors should justify why they are interpreting the results differently (if they are). It should also be noted that in the real world the train/test size ratio for humans is presumably very small, perhaps zero (given the compositional abilities of humans). <sep> 3) Section 4.3: I don't think the results in this section are sufficient to establish the egocentric frame per se as the key factor. One possibility is that perhaps the frame doesn't have to be centered on the agent, but as long as it has some systematic relationship to the agent's location (for example, the center of the visibility window could be some distance away from the agent, and the agent itself may or may not be inside this window), that's good enough to get generalization improvements. An even weaker possibility is that simply a moving frame is enough for improved generalization. In this case, the reference frame doesn't even need to have a systematic relationship to the agent's location. For example, the frame could be relative to a fictitious agent that randomly explores the environment. I think the authors should run some experiments to rule out these possibilities if they want to claim that the egocentric frame itself is responsible for generalization improvements. <sep> 4) Section 4.4: In the experiments in this section, I think there are two relevant factors that need to be better disentangled: 1) the number and variability of image frames experienced by the two models; 2) the active perception aspect (the fact that the agent interacts with the environment and affects its own perceptual experience in one case). The authors claim the second factor as the key aspect enabling better generalization, but 1) is equally likely (this would be more in line with a standard data augmentation type result). A good control experiment here would be to not just use the first frame but a larger number of more variable frames for training the non-situated agent (for example, one can use image frames that would be seen by a camera that more or less randomly moves in front of the objects perhaps with the constraint that both objects are always at least partially visible). If the classification model generalizes as well as the situated agent in this control condition, you cannot claim active perception as the key factor. <sep> 5) As a more general point, it's a bit frustrating to have to judge systematic generalization by only looking at the results of some limited set of experiments. How do I interpret the results if the agent achieves only 84% accuracy in some experiment (as opposed to 100%)? It would be much better if the authors could somehow more rigorously prove systematicity. Here, I don't necessarily mean ""prove"" in a mathematical sense, but just analyzing the learned representations a bit more rigorously and being able to say something along the lines of: here's exactly how the trained model represents ""lift""; because of reason X, Y, Z, this representation is completely disentangled from all object representations in the dataset (and ideally from all possible object representations, because that's really what true systematicity entails, although I highly doubt that any generic model of the type studied in this paper will be able to achieve this, regardless of the amount and type of input it receives). <sep> More minor issues: <sep> 6) In Table 5, ""table lamp"" appears both in training and test sets. Is this a typo? <sep> 7) Some results are presented in the appendix without any mention in the main text (Appendix D. 2). I think this is not a good practice in general. In the main text, please make sure to mention, however briefly, every result that appears in the appendix (something along the lines of ""This result could not be explained by confound X or Y (Appendix Z)"" would suffice). <sep> 8) Font size in Figure 2 is tiny (axis labels are impossible to read), please make it bigger. You don't need that many ticks on the axes.","The paper studies out-of-sample generalisation that require an agent to respond to never-seen-before instructions by manipulating and positioning objects in a 3D Unity simulated room, and analyzes factors which promote combinatorial generalization in such environment. <sep> The paper is a very thought provoking work, and would make a valuable contribution to the line of works on systematic generalization in embodied agents. The draft has been improved significantly after the rebuttal. After the discussion, we agree that it is worthwhile presenting at *CONF*."
"abstract | rebuttal_process | strength | decision  ==> [score raised from weak reject to weak accept after rebuttal - on a more fine-grained scale, I would rate this paper now an accept (7), but not a strong accept (8), however since this year's scale is quite coarse, I'll stick with a score of 6] <sep> Summary <sep> The paper proposes a new perturbation-based measure for computing input-saliency maps for deep RL agents. As reported in a large body of literature before, such saliency maps are supposed to help ""explain"" why a deep RL system picked a certain action. The measure proposed in the paper aims at combining two aspects: specificity and relevance, which should ensure that the saliency map highlights inputs that are relevant for a particular action to be explained (specificity), and this particular action only (relevance). The paper shows illustrative examples of the proposed approach and two previously proposed alternatives on Chess, Go and three Atari games. Additionally the paper evaluates the method and the two previous alternatives on two interesting chess-tasks: chess-puzzles where human players were shown to be able to solve puzzles faster and with higher accuracy when given the proposed saliency map in addition, and evaluation against a curated dataset of human-expert saliency maps for 100 chess puzzles. <sep> Contributions i) Novel, perturbation-based saliency measure composed of two parts. Main idea of specificity is (similar to Iyer et al. 2018) to use State-Action value function (Q-function) with a specific action, instead of State-Value function only. Main idea of relevance is to ""normalize"" by taking into account change in Q-value for all other actions as well. Both parts are combined in a heuristic fashion. <sep> ii) More objective/reproducible assessment of how saliency maps produced by different methods overlap with human judgement of saliency of pieces in chess. To this end: two experiments with human chess players/experts (puzzle solving, and expert-designed saliency maps). <sep> Quality, Clarity, Novelty, Impact <sep> The paper is well written and most sections are easily understandable, though for some parts it might help if the reader is quite familiar with Chess/Go. The proposed saliency measure seems to address some shortcomings of previously proposed measures - my main criticism is that the construction of the measure seems rather ad-hoc and heuristic. It would have been great to formally define specificity and relevance (e.g. in an information-theoretic framework as Sparse Relevant Information) and then derive a suitable measure that is shown to satisfy/approximate the formal definitions. At least, there is one ablation study that justifies parts of the heuristic construction to some degree. <sep> The proposed measure seems to do reasonably well on board-game domains, in particular chess. However it might also be the case that the measure does particularly well for generating saliency maps for Stockfish (which is the agent that happens to be evaluated in the chess domain), which might be quite different from previously reported methods that have been designed for deep neural network RL agents. The illustrative examples on Atari and Go do not allow for a statistically significant judgement of the quality of the proposed method. <sep> On a conceptual level, a bigger issue (of many saliency methods in general, but the criticism also applies to the paper) is that the ""explanations"" drawn from saliency maps are rarely evaluated rigorously. The paper makes a nice attempt by trying to establish some ""ground-truth"" saliency in chess using humans to increase the degree of objectivity, which I greatly appreciate. However, it remains unclear whether explanations that happen to coincide with human notions of saliency really are of higher quality for assessing how an artificial system makes its decisions. The main goal of explainability/interpretability methods must be to come up with testable hypotheses that tell us something about how the artificial system makes its decisions in novel/unseen situations. The goal is not to explain a decision mechanistically after the fact (which is trivial, given a deterministic, differentiable feed-forward system), but to come up with non-trivial explanations that extrapolate/generalize. Specificity and Relevance are probably important ingredients of such explanations, but I think it's important to formalize them properly first. Currently I am in favor of suggesting a major revision of the work, but I am happy to reconsider my decision based on the rebuttal and other reviewers' assessment. Having said that, I do want to reiterate that I think it is great that the authors included some ablation studies and measures of ""usefulness"" of the saliency method. <sep> Improvements / Major comments i) Formally define specificity and relevance (e.g. as sparsity and compression?). Ideally derive a saliency measure based on these formal definitions. <sep> ii) Show how saliency maps (of the same situation) change when producing explanations for different actions. I assume that currently the illustrative examples only show the action with the highest Q-value, what changes when using e.g. a less likely action? <sep> iii) Show that the saliency map stays roughly constant for seemingly irrelevant perturbations. In particular, using the chess-dataset with expert annotations, apply various kinds of perturbations to non-salient pieces (e.g. removing them, swapping them for other pieces and potentially moving them in irrelevant ways) and see whether the AUC stays roughly constant. <sep> iv) Apply non-relevant perturbations to the salient pieces. I.e. take the same puzzle/scene and move it across the board, does the saliency-map move in (roughly) the same way. <sep> v) The saliency method might be particularly suited for Stockfish (whose action-value estimates might be strongly influenced by human saliency and chess theory). See whether the method still produces good results for other chess agents (ideally trained without human heuristics or data). If this is hard to do for chess, think about a different application where this is easier. <sep> vi) Add a section that discusses current shortcomings and caveats with the method, and saliency maps for explainability in general. <sep> vii) (Experimental details) For each domain, please explain the perturbations that you used (removing pieces in Chess/Go, blurring pixels in Go, anything else e.g. replacing pieces?). In all experiments, was it always the action with the highest Q-value that was being explained? <sep> Minor comments a) Table 1: (add error-bars) What is the variance across players? Are the results for the proposed method statistically significant? <sep> b) Chess saliency dataset. Are the expert saliency ratings binary? Why not have multiple degrees of saliency? <sep> c) Would the Greydanus et al. 2018 approach deliver similar results when using a threshold to cut off low-saliency inputs? <sep> d) Why is the saliency map in 3.4 binary (pieces are either salient or not)? How was the binarization threshold chosen? What would the non-binary saliency maps look like? <sep> e) Please provide all experimental details (in the appendix) that are necessary to reproduce the experiments. Referring to a code-repository is not a replacement for describing the methods in detail.","A new method of calculating saliency maps for deep networks trained through RL (for example to play games) is presented. The method is aimed at explaining why moves were taken by showing which salient features influenced the move, and seems to work well based on experiments with Chess, Go, and several Atari games. <sep> Reviewer 2 had a number of questions related to the performance of the method under various conditions, and these were answered satisfactorily by the reviewers. <sep> This is a solid paper with good reasoning and results, though perhaps not super novel, as the basic idea of explaining policies with saliency is not new. It should be accepted for poster presentation."
"abstract | strength  ==> The paper tries to bring some theoretical foundation to the weakly supervised disentanglement. Overall it is a good contribution, but the message of the paper is not clear.  The authors propose two notions: consistency and restrictiveness, which they don't imply each other. However, the experiment on real data shows that they are highly correlated.   Up until the experiment section, the paper is well written (although a bit verbose). It seems that it is great but unfinished work. <sep> The paper is well written, but in my opinion, there is too much verbosity on page 4-5 on rather trivial definitions consistency and restrictiveness and a big box in the calculus of disentanglement that steals space from the main results.  In my opinion, those sections can be reduced so that other theorem can be covered. In my opinion, the theorem nine should be part of the main text. <sep> I understand the definition of ""Sufficiency for Disentanglement "" but it is not clear why it is important. Sure, it is a strong definition that says for any  H (and not a subset) the algorithm (A) should be able to match the distribution of the observation but why is it a big deal according to the next paragraph? <sep> I don't see any proof that Eq.11 should be between [0,1]. Yes, g is optimal, and if you enter suboptimal values to it, one expects the nominator to be less than dominator. However, g a function that is optimal in expectation, which does not mean for every s value it nominator is less than the denominator. In fact, some of the values in fig 3 are small negatives. <sep> Fig 3 is not explained well: you are showing normalized consistency and restiveness. First of all, what is the dataset you tried this on? Second, why some values are negative?! These are supposed to be between [0,1]. Third, what is the take-home-message of this figure? the first two matrices from left show that the factors are consistent b/c they are almost diagonal. The third one from left shows that the algorithm you used is not restrictive? Then are you suggesting this as a metric of evaluation? I am not sure I understand the first figure from the right. <sep> Overall, the authors perform a significant amount of experiments, but they did a poor job in summarizing the results. <sep> Finally, the authors claim <sep> ""...We believe this correlation between consistency and restrictiveness to have been a general source of confusion in the disentanglement literature, causing many to either observe or believe that restricted labeling or share pairing on Si (which only guarantees consistency) is sufficient for disentangling Si ..."" <sep> Each of those methods should be analyzed separately to ensure that their algorithms do not induce restiveness. I just don't see the natural connection between your figure 4 and this conclusion that you made. <sep> Minor: <sep> Where is the proof for Theorem 1? In the Supp, it starts with Theorem 8, I guess you meant Lemma 8? You need to clean up the Supp so that one can find the proof easily. I suggest restructuring the Supp to less and finally proof of Thorem 1.","This paper first discusses some concepts related to disentanglement. The authors propose to decompose disentanglement into two distinct concepts: consistency and restrictiveness. Then, a calculus of disentanglement is introduced to reveal the relationship between restrictiveness and consistency. The proposed concepts are applied to analyze weak supervision methods. <sep> The reviewers ultimately decided this paper is well-written and has content which is of general interest to the *CONF* community."
"strength  ==> The paper aims at providing a better understanding of generalization for Deep Learning models. The idea is really interesting for the ML community as, despite their broad use, the astounding property of deep neural networks to generalize that well is still not well understood. <sep> The idea is not to show new theoretical bounds for generalization gaps but stress the results of an empirical study comparing the already existing measures. The authors choose 7 common hyperparameters related to optimization and analyze the correlation between the generalization  gaps effectively observed and the ones predicted by different measures (VC dim, cross-entropy, canonical ordering …). <sep> The writing of the paper is clear and easily understandable. Besides, I believe that the study is relevant to *CONF* conference. <sep> However, I believe the level of the paper is marginally below the threshold of acceptance and therefore would recommend to reject it. <sep> The paper is solely empirical but I believe that the empirical section is a bit weak, or at least some important points remain unclear. If I appreciate the extent efforts made in trying to evaluate different measures of generalization gaps, I do not believe that the findings are conclusive enough. <sep> 1) First, all this empirical result are based on one-dataset (CIFAR-10) only thus limiting the impact of the study. Indeed, a given measure might very correlate with generalization gap on this specific dataset but not on others. <sep> 2) Specifically, we see that on this specific dataset, all training accuracies are already quite good (cf : Figure 1, distribution of the training losses). Consequently, authors are more correlating the chosen measures with the test error rather than with the generalization gaps. On other more complicated datasets where the training loss is higher, the VC dimension might consequently have way better results. <sep> Similarly, in Section 6, the authors say that the results «confirm the widely known empirical observation that over-parametrization improves generalization in deep learning. » In this specific case, no reference was given to support the claim. I would agree with the claim « over-paramatrization improves test accuracy (reduces test error) » but the link between over-parametrization and generalization is less clear. <sep> 3) In Section 4, the authors say « drawing conclusion from changing one or two hyper-parameters » can be a pitfall as « the hyper-parameter could be the true cause of both change in the measure and change in the generalization ». I totally agree with the authors here. Consequently, I do not understand why the correlations were measured by only changing one hyper-parameter at a time instead of sampling randomly in Theta. <sep> 4) It is still not clear to me how the authors explain why some measures are more correlated with generalization gaps than others. Are some bounds tighter than others ? This empirical study was only applied to convolutional neural networks and consequently one may wonder that for example the VC dim bounds computed in the specific case of neural networks are too loose. However, this measure could be efficient for type of models. <sep> I would like the authors to clear the following points : <sep> - How do you ensure that the empirical study clearly correlated measures predictions with generalization gaps and not simply with test errors (or accuracies) ? (point 2) <sep> - Could you please also answer Point 4 ? <sep> - Finally, how would you explain the fact that the canonical order performs so well compare to many other measures and that it is a really tough-to-beat baseline ?","This paper provides a valuable survey, summary, and empirical comparison of many generalization quantities from throughout the literature. It is comprehensive, thorough, and will be useful to a variety of researchers (both theoretical and applied)."
"abstract | strength | weakness | misc | decision  ==> I have read the rebuttal of the authors . Thank you for you answer and for addressing some concerns.  While the question addressed is important, the theory presented here does not seem to hint to a solution, hence I am keeping my score. <sep> ### <sep> Summary of the paper: <sep> This paper shows that the cycle GAN loss suffers from the presence of lot of symmetries that make the existence of a unique solution not possible , and moreover adding a regularizer that uses the identity loss is not enough to make the problem less prone to those invariances. <sep> Review of the paper: <sep> The notations and the formalism  in the paper are heavy and cumbersome and don't come with any surprising result, since the transforms between unpaired spaces will be found always up to   symmetries since we have the composition of one map with another. The use of the identity loss is also shown to not to help either in fixing this invariance issue. <sep> Experiments are not interesting since without any structure on the map of F and G , the source domain and the target domain, one is expected to get permutations. <sep> The paper points in the conclusion  that the use of skipconnection in F and G has the major influence. <sep> The study of cycle GAN might need some assessment of what is the mutual information between the domains , as on what  information needs be preserved , and information needs to match , skip connection maintain the content in image generation as the information is kept from lower layer and its modified to target the style of the target images. <sep> An information theoretic analysis of cycle gan is needed using for example the objective of ""MISO: Mutual Information Loss with Stochastic Style Representations for Multimodal Image-to-Image Translation"". <sep> or by using a radically new approach  for cycle gan such as the Gromov Wasserstein distance as done in "" Learning Generative Models Across Incomparable Spaces""","This paper theoretically studied one of the fundamental issue in CycleGAN (recently gained much attention for image-to-image translation). The authors analyze the space of exact and approximated solutions under automorphisms. <sep> Reviewers mostly agree with theoretical value of the paper. Some concerns on practical values are also raised, e.g., limited or no-surprising experimental results. In overall, I think this is a boarderline paper. But, I am a bit toward acceptance as the theoretical contribution is solid, and potentially beneficial to many future works on unpaired image-to-image translation."
"abstract | weakness  ==> Summary: This paper studies conditional channel gated networks. The network is designed to disable certain channels depending on the inputs. This can be used to save computation. The idea is built on top of ""Convolutional Networks with Adaptive Inference Graphs."" The authors propose the technique of batch shaping to encourage the marginal statistics of the gating to be selective to different inputs. With similar inference time, the gated network can achieve better accuracy it can afford adding more layers in the network. <sep> Detailed comments: <sep> - The results show quite significance difference compared to ConvNet-AIG, which demonstrates that batch-shaping is very helpful. Table-1 show 3% increase in accuracy compared ResNet-18 by using similar inference time. It is good that the paper reports wall-clock time measurements. <sep> - It's good to see some visualizations in the paper, including the image samples and gate locations. I recommend to move Figure 7 to the main paper. A regular neural network can also be used to visualize the sensitivity of patterns of specific neurons. What would be the qualitative differences? <sep> - 1-2x reduction in MAC is not super impressive, especially taking into consideration of the overhead for gathering the active channels for convolution. <sep> - Figure 3a) plot is cut off on the right. The baselines only have a single point in the plot, I guess it is also valid to simply add/remove layers in the baseline models to generate a curve in the plot. <sep> - ResNet-50-L0 is missing in Figure 3b). It would be better if the plots can be grouped better. Currently there are too many lines and it is hard to understand the differences. <sep> - It would be good to see comparisons to some other alternatives to batch shaping. For example, one can penalize so that the average value is around 0.5 by using a L1 loss |E(x) – 0.5|. <sep> - The ImageNet experiment has a very complicated set-up, where L0 loss is applied in the middle of the training. Is this necessary? How important is this step? What would happen if L0 loss is not applied in ImageNet? And what would happen if L0 loss is applied from the beginning? Why is L0 loss not applied in other experiments (e.g. CIFAR or Cityscapes), will L0 loss be beneficial on these benchmarks as well? <sep> - There are a number of related works on adaptive spatial attention for faster inference, which can be included in the related work section. <sep> 1) M.  Figurnov,  M.  D.  Collins,  Y.  Zhu,  L.  Zhang,  J.  Huang,D.  P.  Vetrov,  and  R.  Salakhutdinov. Spatially  adaptive computation  time  for  residual  networks. CVPR, 2017. <sep> 2) X. Li, Z. Liu, P. Luo, C. C. Loy, and X. Tang.  Not all pixelsare equal:  Difficulty-aware semantic segmentation via deeplayer cascade. CVPR, 2017. <sep> 3) M. Ren, A. Pokrovsky, B. Yang, R. Urtasun. SBNet: Sparse Blocks Network for Fast Inference. CVPR, 2018. <sep> Conclusion: The batch shaping technique introduced in this paper has significant improvement on networks that exploit conditional inference. Further understanding of the effect of L0 loss and other alternative loss function is recommended. My overall rating is weak accept.","The paper describes a method to train a convolutional network with large capacity, where channel-gating (input conditioned) is implemented - thus, only parts of the network are used at inference time. The paper builds over previous work, with the main contribution being a ""batch-shaping"" technique that regularizes the channel gating to follow a beta distribution, combined with L0 regularization. The paper shows that ResNet trained with this technique can achieve higher accuracy with lower theoretical MACs. Weakness of the paper is that more engineering would be required to convert the theoretical MACs into actual running time - which would further validate the practicality of the approach."
"abstract | strength | decision  ==> This paper applies 3D convolutions to the problem of Lagrangian fluid simulation. The primary difficulty in this is that, unlike for Eulerian fluid simulation, which represents the fluid as a grid and adapts nicely to 3D convolutions, in Lagrangian simulations the fluid is represented as an unordered set of particles. It is not straight-forward to apply 3D convolutions on such a data structure, however this paper proposes a method to apply the same regular-grid kernels used in grid-based convolutions to the particle structure. To do this, several points around the kernel are evaluated by first using trilinear interpolation between the particles to get feature values at those points and then convolving those values with the kernel weights. This results in a new particle in the next layer up with those features. In the paper, this method is used to train the weights of the network to reproduce fluid dynamics generated by a simulator. The results show that the proposed method was able to model fluid dynamics over 2 timesteps more accurately than other methods and can do so quickly. <sep> While I have some reservations about this paper (detailed below), on the whole I think it is a quality contribution and should be accepted. This paper contributes a novel method for performing 3D convolutions on unordered particle sets, and it shows that the learned fluid dynamics generalize to novel situations. One major hurdle to applying modern convolutional learning techniques to Lagrangian methods is the mismatch between the layout of the data (unordered particles) and the layout of the kernels (regular grid). This paper presents a novel way of bridging that divide, and it shows that the proposed method actually works by applying it to the problem of fluid dynamics and successfully learning it. However, one major concern I had was that it seems all of the training data was generated in box-like environments, which could easily lead to overfitting. This was alleviated by the results showing that although the network was trained only in boxes, it generalized to environments with channels and waterfalls (as seen in the video). This is a powerful result and shows that this method really did learn fluid dynamics and not just a shortcut that only works in boxes. <sep> I do think this paper can be improved in a few aspects however. The biggest issue is that the quantitative analysis of the core functionality (reproducing fluid physics) is lacking. The paper only reports results for error after at most 2 timesteps, which is not nearly long enough to determine if the output is accurate. Furthermore, the results are only reported for the box scenes, not the generalization scenes mentioned above. Qualitatively, from the videos, it is clear that the output does at least somewhat model fluid dynamics, but it would be much better to have hard numbers to back that up. I suspect the authors discovered that Lagrangian systems are sufficiently chaotic that after only a few timesteps the particle positions have diverged significantly. This is not a bug but a feature of such systems. In Lagrangian fluids, the particles are but an approximation of the fluid, and unlike Eulerian systems*, multiple different sets of particles can approximate the same fluid. This makes particle position only useful as a measure of error if the trained model can perfectly reproduce the fluid dynamics. But of course it can't (trained networks aren't ever perfect in practice), and so small errors quickly compound into large particle position disparities. So even though the trained network models the fluid well overall, the particles end up in completely different locations. Instead a better error metric would be something like measuring the difference between the surface of the fluids, or the velocities or densities at various locations. These are agnostic to the particular particle positions, but still measure how well two different sets of particles represent the same fluid. Using a metric like this, it would be nice to see error graphs over time for both the box and generalization scenes. <sep> A couple other smaller points. The chaotic divergence behavior of DPI-Nets seems inconsistent with that paper. Is this possibly a bug in the way it was implemented here? Additionally, the paper states that the convolutions of SPNets were ""specifically designed to implement the position-based fluids algorithm"" but that it was used in the paper with a much larger number of channels. If it was designed only to work for that one algorithm, how were the number of channels increased? That is unclear. Also, the average error for SPNets is not shown in Table 1 and it is not stated why. <sep> *Assuming same grid shape, size, and position.",The paper proposes an approach for N-D continuous convolution on unordered particle set and applies it to Lagrangian fluid simulation. All reviewers found the paper to be a novel and useful contribution towards the problem of N-D continuous convolution on unordered particles. I recommend acceptance.
"strength | rebuttal_process | suggestion  ==> Positives: <sep> +The system makes sense and is explained well <sep> +The factoring of scenes into objects and multiple background components is good <sep> +I think overall the experiments are reasonable, although I have a number of questions about whether aspects of them are apples-to-apples <sep> Negatives: <sep> -Some of the experiments do not appear apples-to-apples <sep> -There are a large number of changes, and there aren't any ablations. It's a little hard to follow and verify that the gains are credited properly. <sep> Overall, I'm favorably inclined towards accepting this paper so long as the experiments are more clearly made apples to apples. Right now, since I'm forced to give a binary decision and I'm not positive about comparisons, I have to lean towards rejection -- I'd peg my actual rating as 4.5. <sep> Method: <sep> +The method is well-explained and straight-forward (in a good way). <sep> +The factoring of scenes into objects and multiple background components is good <sep> +The parallelization is good, and the fact that it works far faster than SPAIR with similar results is quite nice <sep> Experiments: <sep> +Overall the experiments are pretty good and compare against the baselines I would expect, and have both qualitative and quantitative results. <sep> +The method appears to do a good job of segmenting the objects, and if Figure 1 is representative, this is quite impressive. <sep> -Why does Figure 1 show results from different systems on different images? This makes comparison impossible. Paired samples are always more informative. <sep> -It's not clear to me that fair comparisons were done, especially to GENESIS. <sep> (a) It's never listed how K for genesis was picked -- this should presumably be tuned somewhere to optimize performance. The paper mentions in the 4.2 that it was impossible to run the experiments for GENESIS for more than 256 components -- but the GENESIS paper has numbers more like K=9. If there are an overabundance of components, this might explain some of the object splitting observed in the paper. <sep> (b) Unless I'm missing something, in Figure 5, for 4x4 and 8x8, it doesn't appear that IODOINE or GENESIS have converged at all. Does the validation MSE just then flatten (or go up) there? This is also wall-clock, so I'm not sure why things would stop there. This seems to conflate training speed with performance (although also note that the wall clock times being discussed are pretty small -- the rightmost side of the graph is ~14 hours -- hardly a burdensome experiment). <sep> (c) Similarly, for 16x16 cells, SPAIR seems to be improving consistently. Is it being cut off?-Figure 5 -- The caption for the figure things appears to not make sense: GENESIS is listed as having K = HxW+5 components and SPACE has K=5 listed. Neither make sense to me. Are they out of order? <sep> (d) For SPAIR in Table 1, it's not clear whether it's the slow SPAIR that was mentioned previously or the fast one (e.g., the predicted boxes are described as the same quality as SPAIR -- but is it the slow SPAIR or fast one?). I think the paper would benefit from being a bit clearer about this. I get that the parallel decomposition, in some sense, may be necessary to get any results. But I wish the paper were a bit more explicit. <sep> -There are some reasonable results. I realize that there isn't existing ground truth on atari and other games, but why not label a few hundred frames manually? <sep> -It would have been nice to have an ablation of some of the components, including the boundary loss. Unfortunately, there's a complex multi-part system and it's not clear how to break off components apart for reuse elsewhere. <sep> Small stuff that doesn't affect my review: <sep> 1) Figure 5 -- the figure text size is tiny and should be fixed. <sep> 2) Eqn 3, subscript of the product ""i"" -> ""i=1"" <sep> 3) Table 1 -- captions on tables go on top <sep> 4) Now that the systems work like this, I'd encourage the authors to go and try stuff on more realistic data. <sep> 5) I would be a little wary of making a big deal out of the discovery of the Montezuma's Revenge key. I realize this is indeed important, but I don't see why something like the slic superpixel objective, or felzenswalb-huttenlocher wouldn't find it either. I think it's great that there's a setting in terms of network capacity (for fg/bg networks) that yields this result, but this seems to depend heavily on the particular networks used for each of the parts of the method, and not on the general method. Also, it seems largely a function of the fact that they're a small region with a different color. <sep> ----------------------------------------- <sep> Post-rebuttal update: I have read the authors' response and I am happy to increase my rating to 6.","The paper makes a reasonable contribution to generative modeling for unsupervised scene decomposition. The revision and rebuttal addressed the primary criticisms concerning the qualitative comparison and clarity, which caused some of the reviewers to increase their rating. I think the authors have adequately addressed the reviewer concerns. The final version of the paper should still strive to improve clarity, and strengthen the evaluation and ablation studies."
"abstract | rebuttal_process | misc | suggestion | strength  ==> This paper proposed another graph embedding method. It focuses on directed graphs, and it embedded the graph nodes into exponential power distributions, which include the Gaussian distribution as a special case. The method is implemented by optimizing with respect to the free distributions on a statistical manifold so as to achieve the minimum distortion between the input/output distances.  The method is tested on several directed graph datasets and showed superior performance based on several metrics. <sep> Overall, the submission forms a complete and novel contribution in the area of graph embeddings. A key novelty is that the authors used the asymmetry of KL divergences to model the asymmetry of the distances in directed graphs, and they use the fact that KL is unbounded to model the infinite distances in undirected graphs. The proposed method has three main hyperparameters, \\lambda in eq.(1), \\beta in eq.(2), and the dimensionality of the target embedding. The author showed that \\lambda and \\beta are not sensitive and can be set to the default values, and 2-dimensional distributions already give much better results as compared to alternative embeddings. Moreover, the author proposed a scalable implementation based on sampling. Furthermore, the authored justified their choice of the target embedding space through some minor theoretical analysis given in section 3. <sep> The writing quality and clarity are good (well above average). <sep> To further improve this paper (e.g., in the final version), the authors are suggested to incorporate the following comments: <sep> In the experimental evaluation, it should include some cases when the dimensionality of the target embedding has a large value (e.g., 50). This will make the evaluation more complete. <sep> There are some typos and unusual expressions.  For example, page 3, what is ""a good proposal function""? <sep> After eq.(1), mention \\lambda is a hyperparameter (that is not to be learned). <sep> Theorem 1 (2), mention the Fisher information matrix is wrt the coordinate system (\\sigma^1, \\cdots,\\sigma^k, \\mu^1, \\cdots, \\mu^k) <sep> Ideally, the experiments can include an undirected graph and show for example that the advantages of the proposed method become smaller in this case.","The paper proposes an embedding for nodes in a directed graph, which takes into account the asymmetry. The proposed method learns an embedding of a node as an exponential distribution (e.g. Gaussian), on a statistical manifold. The authors also provide an approximation for large graphs, and show that the method performs well in empirical comparisons. <sep> The authors were very responsive in the discussion phase, providing new experiments in response to the reviews. This is a nice example where a good paper is improved by several extra suggestions by reviewers. I encourage the authors to provide all the software for reproducing their work in the final version. <sep> Overall, this is a great paper which proposes a new graph embedding approach that is scalable and provides nice empirical results."
"abstract | rating_summary | decision  ==>  ==> EDIT After Rebuttal: My understanding of the contributions of this paper has improved. I now increase my score to a weak accept. <sep> This paper proposes a new backpropagation algorithm learning algorithm ""SpikeGrad"" for Spike-based neural network paradigm. Simulating this algorithm on a classical hardware would require a lot of time-steps. To circumvent this, they show how to construct a corresponding artificial neural net (that can be trained using the traditional gradient based algorithms) which is equivalent to the spiking neural net. Using this equivalence they simulate a large scale SNN on many real-world dataset (first paper to do so). In particular, they use MNIST and CIFAR-10 for this purpose. They show that training a fixed architecture using their method is comparable to other prior work which uses high-precision gradients to train them. They also show how to exploit sparsity of the gradient in the back propagation for SNN. <sep> This paper is hard-to-follow for someone not familiar with the background material. In particular, without looking at prior literature it was hard to understand that ""integrate and fire neuron model"" is essentially the feedforward mechanism for the SNN. I would suggest the authors make this a bit more explicit. Moreover, it would serve the structuring of the paper to have a formal ""Preliminaries"" section, where all known stuff goes. It was hard to discern what is new in this paper, and what is from prior work and these are mixed in section 2. For instance, section 2 states ""SpikeGrad"" algorithm; but the main contribution (ie., the back propagation algorithm) only appears in the middle of this section. Likewise, I think section 3 can be arranged better. In particular, the equivalence is a ""formal"" statement and thus, could be stated as a theorem followed by a proof. It will also make it explicit as to what does it mean by an ""equivalent"" network. In fact, it is still not clear to me at this point what that statement means. Could you please elaborate this in the rebuttal? <sep> Regarding the conceptual contribution of this paper, if I understood things correctly, the main claim is that they give a new way to train SNN whose performance on MNIST and CIFAR-10 is comparable to other works. The second contribution is that they give the equivalence between ANN and SNN (point above). It is also unclear to me what the point regarding the sparse gradient in the backpropagation in the experimental section is trying to make? Could you please clarify this in the rebuttal as well? <sep> At this point, the writing of this paper leaves me with many unanswered questions that needs to be addressed before I can make a more informed decision. Please provide those in the rebuttal and based on those will update my final score. But with my current understanding of this paper, I think this does not meet the bar. The contributions in this paper do not seem too significant.","This paper proposes a learning framework for spiking neural networks that exploits the sparsity of the gradient during backpropagation to reduce the computational cost of training. The method is evaluated against prior works that use full precision gradients and shown comparable performance. Overall, the contribution of the paper is solid, and after a constructive rebuttal cycle, all reviewers reached a consensus of weak accept. Therefore, I recommend accepting this submission."
"rating_summary | suggestion  ==> The authors argue for the importance of transduction in few-shot learning approaches, and augment the empirical Bayes objective established in previous work (estimating the hyperpior ψ in a frequentist fashion) so as to take advantage of the unlabelled test data. <sep> Since the label is, by definition, unknown, a synthetic gradient is instead learned to parameterize the gradient of the variational posterior and tailor it for the transductive setting. The authors provide an analysis of the generalization ability of EB and term their method _synthetic information bottleneck_ (SIB) owing to parallels between its objective of that of the information bottleneck. SIB is tested on two standard few-shot image benchmarks in CIFAR-FS and MiniImageNet, exhibiting impressive performance and outperforming, in some cases by some margin, the baseline methods, in the 1- and 5-shot settings alike, in addition to a synthetic dataset. <sep> The paper is technically sound and, for the most part, well-written, with the authors' motivations and explanation of the method conceived quite straightforwardly. The basic premise of using an estimated gradient to fashion an inductive few-shot learning algorithm into a transductive one is a natural and evidently potent one. The paper does, however, at times feel to be disjointed and, to an extent, lacking in focus. The respective advantages of EB and the repurposing of synthetic gradients to enable the transductive approach are clear to me, yet while they might indeed be obviously complementary, what is not obvious is the necessity of the pairing: it seems there is nothing prohibiting the substitution of the gradient for a learned surrogated just as well under a deterministic meta-initialization framework. As such, despite sporting impressive results on the image datasets, I am not convinced about how truly novel the method is when viewed as a whole. <sep> On a similar note, while the theoretical analysis provided in section 4 was not unappreciated, and indeed it was interesting to see such a connection between EB with information theory rigorously made, it does feel a little out of place within the main text, especially since it is not specific to the transductive setting considered, nor even to the meta-learning setting more broadly. Rather, more experiments, per Appendix C, highlighting the importance of transduction and therein the synthetic gradients and its formulation would be welcome. Indeed, it is stated that an additional loss for training the synthetic gradient network to mimic the true gradient is unnecessary; while I agree with this conclusion, I likewise do not think it would hurt to explore use of the more explicit formulation. <sep> Considering the authors argue specifically for the importance of transduction in the zero-shot learning regime, I think it would be reasonable to expect experiments substantiating this, and the strength of their method in this regard, on non-synthetic datasets. As far as the toy problem is concerned, I am slightly confused as to the choice of baseline, both in the regard to its training procedure and as to why this was deemed more suitable than one purposed for few-shot learning, so that we might go beyond simple verification to getting some initial sense for the performance of SIB. Moreover, it is not clear from the description as to how λ is implemented here. As it stands, Section 5, for me, offers little in the way of valuable insights. The experiments section on the whole, results aside, feels somewhat rushed; the synthetic gradients being a potential limiting factor for instance feels ""tacked on"" and seems to warrant more than just a passing comment. <sep> Minor errors <sep> - Page 7: the ""to"" in ""let pψ(w) to be a Gaussian"" is extraneous <sep> - Page 8: ""split"" not ""splitted"". <sep> - Further down on the same page, ""scale"" in ""scale each feature dimension"" should be singular and Drop"" is misspelled as ""dropp"". <sep> - Page 9: ""We report results **with using** learning rate..."" <sep> - _Equation 17_ includes the indicator function k≠i but i is not defined within the context. <sep> EDIT: changed score","Three reviewers have assessed this paper and they have scored it 6/6/6 after rebuttal. Nonetheless, the reviewers have raised a number of criticisms and the authors are encouraged to resolve them for the camera-ready submission."
"rating_summary | suggestion  ==>  ==> The paper proposes to use memory network (self-attention) to summarize information from graphs. The proposed approach ""soft clusters"" the node embeddings into coarser representation and eventually fed to a final MLP for classification. The empirical results on some standard datasets show promising gains of the algorithm. <sep> The proposed approach stacks a few layers of self-attention on top of either some features of the nodes (including projected edge connections, parametrized by W_0) or the node embeddings of some form of graph neural network. And this stacking seems to be simple combination of existing approaches, without fully integrating them. In fact, the training process is also separated: ""task-specific loss are back-propagated batch-wise while the gradients of the unsupervised loss are applied epoch-wise"". It makes me wonder whether we can just separate it into two stages, i.e., first learn a node embedding using graph neural network, then learn this self attention transformation. Due to the above issues, I feel the novelty of the approach is limited and incremental. <sep> Another issue with the paper is that the notations seem to be messed up and some concepts are not explained clearly. For example, the C_{i,j} soft assignment matrix is normalized row-wise, then Eqn (3) seems very suspicious, because it averages the queries using weights along the other direction, thus not normalized. Also, the dimension of the MLP weights do not align well with inputs, for instance in Eqn (4), it should be written as V^(l) W. <sep> There are more questions that are not clearly specified in the current manuscript. For example, where does the keys K come from? From the text description, it seems to be cluster results, and do you do the clustering on every gradient update? Or are they learned from scratch? The distribution P defined in Eqn (11) also seems to be difficult to optimize since it depends on C_{i,j} and is connected to different entries. Is simple SGD sufficient to optimize over P? Moreover, in the experiment section, it is unknown how many layers of self-attention is applied and what are the important parameters. For better comparison, the experiment section should include some estimate of parameter size as well. <sep> The experiment results seem interesting since the approach indeed achieves good performance across many datasets. Also the visualized keys are interesting as well because it captures some meaningful patterns from the data. <sep> There is some related work that you should cite: <sep> Hanjun Dai, Bo Dai and Le Song. Discriminateive Embeddings of Latent Variable Models for Structured Data. International Conference on Machine Learning (ICML) 2016. <sep> ============================================================ <sep> Based on the authors' reply and other reviews, I have changed my rating to ""Weak accept"". <sep> The authors' reply has clarified the important detail of how the key matrix is learned and now it is clear that the algorithm is not just two-stage separate learning. In light of this clarification, I think the proposed algorithm is novel enough and the jointly training mechanism is also beneficial for the state-of-the-arts results reported in the experiments.","Four reviewers have assessed this paper and they have scored it as 6/6/6/6 after rebuttal. Nonetheless, the reviewers have raised a number of criticisms and the authors are encouraged to resolve them for the camera-ready submission."
"strength | suggestion  ==> UPDATE TO MY EARLIER REVIEW <sep> ============================ <sep> Since this paper presets new findings that will be of significant interest to much of *CONF*'s audience, and the paper is is well-written, I am changing my rating to ""Accept"". Since Reviewer #1 did not submit a review and Reviewer #2 indicated that (s)he does not feel well-qualified to review this paper (it is very much on the theoretical side after all), it would be great to get one further review from an area chair or otherwise qualified person. <sep> MY EARLIER REVIEW <sep> ================= <sep> This this exciting submission presents a new proof of Leshno's version of the universal approximation property (UAP) for neural networks  -- one of the foundational pillars of our understanding of neural networks. The new proof provides new insights into the universal approximation property. I consider these the main contribution of the paper. Specifically, the authors <sep> - provide an upper bound on the required width for the neural network <sep> - show that the approximation property still holds even if strong further requirements are imposed on the weights of the first or last layer. <sep> I rate this submission a weak accept. It's a very good paper. The work makes useful contributions that should and will be of interest to many in the field. The paper is generally well-written. <sep> Some remarks: <sep> - Being somewhat long, the ""Proof of Theorem 3.1"" would be a much better read if the authors prefixed it  with an outline of the strategy that the proof takes. <sep> - The authors point out that the lack of dependence of Theorem 3.1 on epsilon is surprising, and cite Lin's work from 2017 who previously found such an independence. Lin's derivation of the epsilon-independent UAP is much more intuitive than that of this submission, in which the epsilon independence really pops out somewhat magically and for me only made sense when I read the paper again. I would encourage the authors to add to Lin's paper's citation sentence that this paper motivates the epsilon independence well. Alternatively, the authors could add a few sentences to their paper to provide intuition on how the epsilon-independence comes about in their line of argument.","This is a nice paper on the classical problem of universal approximation, but giving a direct proof with good approximation rates, and providing many refinements and ties to the literature. <sep> If possible, I urge the authors to revise the paper further for camera ready; there are various technical oversights (e.g., 1/lambda should appear in the approximation rates in theorem 3.1), and the proof of theorem 3.1 is an uninterrupted 2.5 page block (splitting it into lemmas would make it cleaner, and also those lemmas could be useful to other authors)."
"abstract | weakness | rebuttal_process | misc  ==> The paper proposes a method of discovering causal mechanisms through meta-learning, by assuming that models transfer faster if their causal graph is correct. <sep> For a possible causal graph, for adaptation to one interventional dataset, the samples are iteratively revealed and the log likelihood of the next sample is measured, after which the parameters are updated using one step of gradient descent on that sample. The sum of these log likelihoods is the 'online likelihood' and a measure of speed of adaptation. The parameters are initialised using maximum likelihood estimation on the train dataset. <sep> The meta learning procedure then at each episode samples an interventional distribution and an interventional dataset from that distribution. It then performs a gradient based update of the belief over graphs based on the difference between the online likelihoods of each graph on that dataset. <sep> The meta-learning approach appears to be a novel contribution. The authors provide a theoretical argument by counting 'effective parameters' to suggest why models using the right causal model obtain a higher online likelihood. Additionally, they prove that the gradient updates to the graph belief are easy to compute and converge. The method is validated with several synthetic experiments which discover the direction of the arrow between two random variables, each either continuous or discrete. Furthermore, they successfully experiment with the combination of learning a representation of a raw data to two random variables with learning the causal direction. The paper is very well written and most claims are carefully proven. <sep> I recommend a weak rejection for this paper, because: <sep> 1) The empirical validation is not strong enough, as no real dataset is used, only toy datasets. The toy experiments themselves could also be more extensive. <sep> 2) I am unconvinced of two of the theoretical claims made: (A) the fact that the expected gradients in Prop 1 are 0, implies that the right causal graph has better online likelihood and (B) that the method is easily extensible to more than two random variables [Appendix E]. <sep> Supporting arguments: <sep> 1.1) Fig D.1 suggests that, in the experiments using continuous random variables, the training dataset alone is sufficient to discover the true causal model, under the assumption of independent additive noise, as is done in e.g. [1]. I find it plausible to believe that the training curve on the training dataset alone already makes it possible to disambiguate the causal from anti-causal model. A similar pattern is shown by the authors themselves in Appendix B on discrete variables. <sep> For finite training data, the models are distinguishable, while for infinite training data they are not [Fig B.1]. The exact same holds for finite and infinite interventional data [Fig 1]. <sep> Are these experiments then good benchmarks for causal discovery based on intervention when the causal model can already be inferred from the non-interventional training data? <sep> 1.2) The simplicity of the representation learning setup doesn't convince that the method is applicable to more real-world settings with more complicated encoders. Additionally, some important details on this experiment are missing (see below). <sep> 1.3) All experiments show the effect of intervention on the cause p(A). No experiments are given for intervention on p(B|A). Does the method then still work? <sep> 1.4) No experiments for more than two random variables are performed in this paper. <sep> 2.A) Prop 1 shows that the expected maximum likelihood gradient for one conditional probability distribution is zero if the graph is correct and that CPD is not intervened on. Subsequently a claim is made that this effectively reduces the number of parameters and thus that adaptation is faster. However, the zero-expectation gradient may still be non-zero and even large on the small intervention sample. It is unclear to me why they therefore can be excluded in the number of parameters. Furthermore, whether the online likelihood is large will depend not only on generalisation, but also on the training convergence, since not empirical risk minimization is used, but SGD with a fixed number of steps. Thus, even though the authors prove that the method will converge to the causal graph with lowest online likelihood, it is unclear why this is necessarily the correct causal graph. <sep> 2.B) In appendix E it is mentioned that cycles can occur in causal models. However, it is unclear why factorization (76) is still correct in the cyclic case. Perhaps I am misunderstanding, but it seems to me that p(x) = \\prod_i p(x_i | x_{Pa_i}) only makes sense for a DAG. Hence, how would the online likelihood be computed for cyclic models? <sep> Suggestions for improvement: <sep> - Could you explain in what realistic settings we would have access to data from a large number of different interventional distributions? <sep> - Could you show a plot similar to Fig 1, but with online likelihoods? Such a plot may be more indicative of the ideal episode length than Fig 1. <sep> - Could you provide details on the representation learning experiment? In particular: (1) is \\theta_D different in the train dataset and each interventional dataset? (2) How do the gradients of theta flow through the meta-update steps? <sep> - A reference to Appendix B appears missing in main text. <sep> [1] Nonlinear causal discovery with additive noise models. Hoyer et al 2009","This paper proposes to discover causal mechanisms through meta-learning, and suggests an approach for doing so. The reviewers raised concerns about the key hypothesis (that the right causal model implies higher expected online likelihood) not being sufficiently backed up through theory or through experiments on real data. The authors pointed to a recent paper that builds upon this work and tests on a more realistic problem setting. However, the newer paper measures not the online likelihood of adaptation, but just the training error during adaptation, suggesting that the approach in this paper may be worse. Despite the concerns, the reviewers generally agreed that the paper included novel and interesting ideas, and addressed a number of the reviewers' other concerns about the clarity, references, and experiments. Hence, it makes a worthwhile contribution to *CONF*."
"abstract | weakness | strength | decision  ==> This paper proposes to use Consistency Regularization for training GANs, a technique known to work well in unsupervised learning. The technique consists in applying a transformation to real images and enforcing that the features of the discriminator between the transformed inputs and the original inputs are similar. The author show that using this technique enables them to improve the performance of a standard GAN significantly on CIFAR10. They also carry an ablation study studying the influence of the different part of the proposed technique. <sep> Overall I'm in favor of accepting this paper. The paper is well written, with convincing experiments and an interesting ablation study. However I have several minor issues that I think could greatly improve the paper if addressed. <sep> Minor comments: <sep> - I think an idea which is somewhat related but hasn't been mentioned in the paper, is the idea of adding noise to the input when training GANs [1]. I think this is worth mentioning in the related work. <sep> - Related to the previous point, why penalizing features and not directly output ? What about also trying to classify the transformed images as real ? Also you say that penalizing the last layer, I think including the influence of m (eq 2) in the ablation study would be interesting. <sep> - The authors provide some measure of standard deviation on some experiments but not on all of them. It would be nice to systematically report the standard deviation for every experiments. <sep> - In figure 1 the author make the hypothesis that the discriminator will output very different score to images semantically close together. Did the author verify this hypothesis experimentally ? <sep> - Also why penalizing only the samples from the real distribution and not from the generator ? have you tried both ? <sep> - When the test accuracy of the discriminator is low, it could also be that the discriminator is under-fitting, it would be nice to also report the train accuracy for the discriminator. <sep> - I think the conclusion about the effect of consistency regularization vs data augmentation is a bit vague since consistency regularization has no sense without data-augmentation. <sep> - It's quite interesting but also disappointing that combining transformations doesn't give that much of an improvement. Do the author have any intuition why this is the case ? and why learning them one after the other would work ? <sep> References: <sep> [1] Arjovsky and Bottou. ""Towards Principled Methods for Training Generative Adversarial Networks."" (*CONF* 2017)","The paper proposes a simple and effective way to stabilize training by adding consistency term to discriminator. Given the stochastic augmentation procedure T(x) the loss is just a penalty on D. The main unsolved question why it help to make discriminator ""smoother"" in the consistency case for a standard GAN (since typically, no constraints are enforced). Nevertheless, at the moment this a working heuristics that gives new SOTA, and that is the main strength. The reviewer all agree to accept, and so do I."
"abstract | rating_summary | strength | misc | weakness | misc | decision  ==> This paper proposes an extension of NeuralLP that is able to learn a very restricted (in terms of expressiveness) set of logic rules involving numeric properties. The basic idea behind NeuralLP is quite simple: traversing relationships in a knowledge graph can be done by multiplicating adjacency matrices, and which rules hold and which ones don't can be discovered by learning an attention distribution over rules from data. <sep> The idea is quite clever: relationships between numeric data properties of entities, such as age and heigh, can also be linked by relationships such as \\leq and \\geq, and those relations can be treated in the same way as standard knowledge graph relationship by the NeuralLP framework. <sep> A major drawback in applying this idea is that the corresponding relational matrix is expensive to both materialise, and use within the NeuralLP framework (where matrices are mostly sparse). To this end, authors make this process tractable by using dynamic programming and by defining such a matrix as a dynamic computation graph by means of the cumsum operator.  Furthermore, authors also introduce negated operators, also by defining the corresponding adjacency matrices by means of computation graphs. <sep> Authors evaluate on several datasets - two real world and two synthetic - often showing more accurate results than the considered baselines. <sep> One thing that puts me off is that, in Table 2, AnyBurl (the single one baseline authors considered other than the original NeuralLP) yields better Hits@10 values than Neural-LP-N, but the corresponding bold in the results is conveniently omitted. <sep> Another concern I have is that the expressiveness of the learned rules can be somehow limited, but this paper seems like a good star towards learning interpretable rules involving multiple modalities. <sep> Missing references - authors may want to consider citing https://arxiv.org/abs/1906.06187 as well in Sec. 2 - it seems very related to this work.","This paper presents a number of improvements on existing approaches to neural logic programming. The reviews are generally positive: two weak accepts, one weak reject. Reviewer 2 seems wholly in favour of acceptance at the end of discussion, and did not clarify why they were sticking to their score of weak accept. The main reason Reviewer <sep> 1 sticks to 6 rather than 8 is that the work extends existing work rather than offering a ""fundamental contribution"", but otherwise is very positive. I personally feel that <sep> a) most work extends existing work <sep> b) there is room in our conferences for such well executed extensions (standing on the shoulders of giants etc). <sep> Reviewer 3 is somewhat unconvinced by the nature of the evaluation. While I understand their reservations, they state that they would not be offended by the paper being accepted in spite of their reservations. <sep> Overall, I find that the review group leans more in favour of acceptance, and an happy to recommend acceptance for the paper as it makes progress in an interesting area at the intersection of differentiable programming and logic-based programming."
"abstract | weakness | rebuttal_process  ==>  ==> This paper introduces DiffSim, a programming language for high-performance differentiable physics simulations. The paper demonstrates 10 different simulations with controller optimization. It shows that the proposed language is easier to use and faster than the other alternatives, such as CUDA and TensorFlow. At the end, the paper provides insightful discussions why the gradient of the simulation could be wrong. <sep> Differentiable physics simulation is an important research area, especially for optimal control and reinforcement learning. While I am impressed by the large variety of examples demonstrated in the paper, I am leaning towards rejecting the paper because of its poor presentation. The paper only gives a simple and high-level example of the language (optimizing the rest length of springs that form a triangle), very brief descriptions of 10 examples and some discussions about the difficulty of computing useful gradients, but without any in-depth discussion how everything is implemented. This is not enough for an *CONF* paper. For example, the paper does not answer some of the fundamental problems of differentiable physics. For example, collision and contact are inherently non-differentiable. How does the paper handle it in the examples of locomotion and billiards (Figure 4)? In addition, how does the paper back-propagate the gradient through the incompressibility conditions (Poisson solve) of fluid simulation? <sep> Here is my suggestions how to improve the writing. There are several ways to write the paper, with different emphasis. If this paper is more about introducing a new programming language, Appendix B Compiler Design and Implementation would be important and should be moved to main text. If the paper want to emphasize how to handle the non-differentiable cases of the simulation, then detailed derivations of contact, collision, and linear/nonlinear solving (due to incompressibility conditions or implicit integrators) should be presented. If the paper would like to demonstrate how differentiable physics simulation can help with controller optimization, then two to three examples, such as the locomotion control for soft bodies or rigid bodies, should be analyzed in far more details, and compared with traditional method without differentiable simulation. It is good to focus on one of the above points, based on the venue that this paper is submitted to. Currently, the paper is trying to touch all three. But due to the page limit, it is not thorough, or detailed in any one of them. <sep> -------------------------Update after rebuttal------------------------------ <sep> Thank you for the revision of the paper and the additional comparisons with Jax. The revised version reads much better. The response and the revision addressed most of my concerns. Thus, I raised my rating to weak accept.","The paper provides a language for optimizing through physical simulations. The reviewers had a number of concerns related to paper organization and insufficient comparisons to related work (jax). During the discussion phase, the authors significantly updated their paper and ran additional experiments, leading to a much stronger paper."
"abstract | weakness | rebuttal_process | rating_summary | decision  ==> The paper provides an in-depth exploration of stochastic binary networks, continuous surrogates, and their training dynamics with some potentially actionable insights on how to initialize weights for best performance. This topic is relevant and the paper would have more impact if its structure, presentation  and formalism could be improved. Overall it lacks clarity in the presentation of the results, the assumptions made are not always clearly stated and the split between previous work and original derivations should be improved. <sep> In particular in section 2.1, the author should state what exactly the mean-field approximation is and at which step it is required (e.g. independence is assumed to apply the CLT but this is not clearly stated). Section 3 should also clearly state the assumptions made. That section just follows the ""background"" section where different works treating different cases are mentioned and it is important to restate here which cases this paper specifically considers. Aside from making assumptions clearer, it would be helpful to highlight the specific contributions of the paper so we can easily see the distinctions between straightforward adaptations of previous work and new contributions. <sep> Specific questions: <sep> It might be worth double checking the equation between eq. (2) and eq. (3) , the boundary case (l=0) does not make sense to me, in particular what is S^0 ?. <sep> What does the term hat{p}(x^l) mean in the left hand side of eq.(3)? <sep> In eq. (7) (8) why use the definition symbol := ? <sep> At the beginning of section 3.1, please indicate what ""matcal(M)"" precisely refers to. Using the term P(mathcal(M) = M_ij) does not make much sense if the intent is to use a continuous distribution for the means. <sep> Just after eq. (9), please explain what Xi_{c*} means. <sep> Small typo: Eq. (10) is introduced as ""can be read from the vector equation 31"", what is eq. (31)? <sep> In section 5.2, why reducing the training set size to 25% of MNIST?","The authors study neural networks with binary weights or activations, and the so-called ""differentiable surrogates"" used to train them. <sep> They present an analysis that unifies previously proposed surrogates and they study critical initialization of weights to facilitate trainability. <sep> The reviewers agree that the main topic of the paper is important (in particular initialization heuristics of neural networks), however they found the presentation of the content lacking in clarity as well as in clearly emphasizing the main contributions. <sep> The authors imporved the readability of the manuscript in the rebuttal. <sep> This paper seems to be at acceptance threshold and 2 of 3 reviewers indicated low confidence. <sep> Not being familiar with this line of work, I recommend acceptance following the average review score."
"abstract | strength | rating_summary  ==>  ==> The paper propose a novel way to measure similarity between datasets, which e.g. is useful to determine if samples from a generative model resembles a test dataset. The approach is presented as an efficient numerical algorithm for working with diffusion on data manifolds. <sep> I am very much in doubt about this paper as there are too many aspects of the work I do not (currently) understand. I suspect that the underlying idea is solid, but in its current form I cannot recommend publication of the paper. <sep> Detailed questions and comments: <sep> *) It seems that the focus is on GANs and related models that do not come with a likelihood. In my reading, the practical problem that the paper address is model comparison in models that do not have a likelihood. Is that correct? If so, I am left to wonder why models with a likelihood a not discussed, and why the such models aren't included in the experiments? Wouldn't the likelihood be a sensible baseline? <sep> *) The term ""multi-scale"" is not defined until page 3. I'd recommend defining this term much more early or avoid its use until its defined. <sep> *) I found the geometric exposition to be rather confusing. In Sec. 3.1 emphasis seem to initially be on Euclidean diffusion, and at some point emphasis changes to diffusion on graphs. Sometimes the paper seem focused on diffusion on general manifolds. I found this exposition to be rather chaotic, and I suspect that this is the root cause of me not understanding many aspects of the work. <sep> *) In Sec. 3.2 the notion of diffusion *on* manifolds is discussed. At some point (not quite clear to me when) the results of this discussion is applied to diffusion *between* manifolds. I don't quite understand what it means to diffuse between manifolds. I would have appreciated a more explicit exposition here. <sep> *) In the concluding remarks it is stated that IMD provides guarantees. Which guarantees are that?","This paper introduces a way to measure dataset similarities. Reviewers all agree that this method is novel and interesting. A few questions initially raised by reviewers regarding models with and without likelihood, geometric exposition, and guarantees around GW, are promptly answered by authors, which raised the score to all weak accept."
"abstract | rating_summary  ==>  ==> Comments : <sep> This paper provides an approach for reducing bias in long horizon off-policy evaluation (OPE) problems, extending recent work from Liu et al., 2018 that estimates the ratio of the stationary state distributions in off-policy evaluation for reducing variance. The paper provides a doubly robust method for reducing bias, since it requires separately estimating a value function. The key idea of the paper is to provide low variance, low bias OPE since their approach relies on accurately estimating the state distribution ratio and also the estimation of the value function. <sep> The paper provides a follow up on Liu et al., 2018 and other recent works in off-policy learning that tries to estimate the state distribution ratio directly, but can introduce high inaccuracy if the obtained ratio estimates are inaccurate. In line of that, this approach seems useful as it tries to reduce the error from inaccurate ratio estimates by incorporating prior works with an additional value function estimation. <sep> Furthermore, the paper introduces a new perspective to doubly robust estimation that tries to reduce bias, instead of variance in previously known OPE literature (Jiang et al., 2016; Thomas et al., 2016). It is interesting to see how doubly robust can be related to primal-dual relations and the connections between these approaches, which is a novel contribution to the best of my knowledge. <sep> - The fundamental connection comes from observing OPE withdoubly robust estimator that estimates \\hat{V} and equation 6 that incorporates the ratio of the state density. This is clearly written in equations 7 and 8, that are two ways of evaluating the value function with off-policy samples known in the literature. <sep> - It uses the property of the Bellman recursive expression for the estimated value function of doubly robust OPE estimator and uses it in the OPE with state density ratio, leading to equation 9, and obtaining the bridge estimator that now relies on both estimates of value function \\hat{V} and the state density ratio. Although initially this does not seem useful, but the authors clarify this and how a bias reducting method can be achieved as in equation 11. <sep> - It is a nice and elegant trick, exploiting the connections between OPE estimators leading to a bias reduction method that seems quite interesting. <sep> - The most interesting part of the paper comes from section 4 that exploits the connection between doubly robust approaches with Lagrangian duality, and that their approach is equivalent to a primal dual formulation of policy evaluation. This stands out in itself as a novel contribution of the paper. Equations 14 and 15 best writes down the connections, as to how policy evaluation can be formualted as a Lagrangian function for optimization. <sep> - Although the authors point out how equation 15 is equivalent to equation 11 - this does not seem straight forward at first unless carefully looked at. I would encourate the authors to perhaps add more clarity that exploits this connection, to make the paper more self-contained. <sep> - In terms of experiments, the paper compares the doubly robust approach with previous works that estimates the density ratio, along with other baseline comparisons such as weighted DR. In both the two evaluated problems, their approach seems useful in terms of obtaining better accuracy (lower MSE). <sep> - However, I am not sure to what extent this approach can be scaled to more complicated tasks for OPE? Are there any example domains where the proposed method fails, or is difficult to scale up to more complicated tasks? <sep> - The current set of experiment results seems adequate, given the theoretical contribution and that most OPE papers evaluate on such domains. But overall, it might be useful to analyse the significance of this approach when scaling to more complicated domains.  <sep> Overall, I think the paper has a neat and elegant theoretical contribution, exploiting the connection of OPE with primal-dual frameworks that seems quite novel to me. Experimental set of results are properly presented too showing significance of the approach compared to previously known baselines. I think such papers exploting connections with other literature is useful for the community in general, and this paper has significantly novel theoretical contribution. Hence, I would recommend for acceptance of this paper.",The paper proposes a doubly robust off-policy evaluation method that uses both stationary density ratio as well as a learned value function in order to reduce bias. <sep> The reviewers unanimously recommend acceptance of this paper.
"abstract | rebuttal_process | decision  ==>  ==> This paper's contribution is a sample complexity lower bound for linear value-based learning and policy-based learning methods. The bound being exponential in the planning horizon is bad news, and has some implications with respect to further analysing sample complexity in RL. <sep> The gist of this paper is that one can craft a hard MDP which requires visiting every state at least once, and that since this MDP's state space is exponential in the MDP's horizon, then there exists a set of MDPs which require an exponential (in the horizon) number of trajectories to be solved. As a consequence, further analysis of sample complexity in RL may need some much stronger assumptions. <sep> The writing of the paper is good, I was able to understand everything (I think). As far as I can tell, this is novel work. Unfortunately I am currently unable to see why this contribution is valuable. I have set my score to weak reject but I am very open to having my mind changed, as I feel I may have missed some critical element. <sep> I have two criticisms: <sep> A- I don't understand why this bound is significantly different than previous bounds. <sep> B- I don't understand why this is bad news for representation learning, nor how this failure mode of linear features translates to the ""deep"" case. <sep> In the same spirit, I find rather odd the way the paper is introduced. Discussions of representations usually involve some discussion of generalization, but that's not what this paper is about. Deep neural networks/representation learning are only useful if there is an opportunity for generalization. <sep> With respect to A, I am either grossly misunderstanding past bounds and/or your bounds, or something is wrong with the way complexities are compared: <sep> - In Wen & Van Roy, the ""polynomial"" sample complexity is in the number of states, it is related to |S|x|A|xH^2 (Theorem 3 of Wen & Van Roy) <sep> - In this paper, Theorem 4.1 states that the sample complexity is exponential because it is of the form 2^H. One *critical* assumption for this bound is precisely that |S| >= 2^H. Thus the bound that you propose is still polynomial in |S|. <sep> I am thus puzzled, how is this bound significantly different? <sep> With respect to B, I don't see how this bound has much to do with good representations, or even representations at all. <sep> In Lemma A.1, you essentially craft a set of features that, being mutually orthogonal, are in some sense ""mutually linearly separable"", making learning the mapping from those features to a value function ""trivial"" once data is obtained. This is barely different from saying that you assume there is a magical learner that learns in O(1) given the data, because in either case, you need to visit _every_ of the 2^H state in order to solve the MDP, because by construction of your problem, there is _no hope_ of generalization*. Since learning features or creating ""good"" features has everything to do with generalization (otherwise we'd just to tabular), I don't see how this bound is relevant to representations. (We already have Wolpert's no free lunch theorem to tell us that there are always some problems that ML just can't be general enough to solve efficiently. What is more interesting is understanding how we can efficiently learn where there _is_ structure to a problem.) <sep> * There is no hope of generalization, unless something about the observation space (which is left undiscussed in the paper) contains *information* about the agent being to the unique path to the reward. In such a case, I can see a probabilistic argument being made where in the worst case the agent needs to visit all 2^H states, but in the average case, the agent may learn to ignore paths where it can generalize that there is no reward. This is not entirely unreasonable, think of e.g. AlphaGo, where very few states end in victory, where there is an exponential number of states in the horizon, yet learning is totally reasonable because of structure in observation. This is where I don't agree with a statement like: ""Since the class of linear functions is a strict subset of many more complicated function classes, including neural networks in particular, our negative results imply lower bounds for these more complex function classes as well.""","The authors challenge the idea that good representation in RL lead are sufficient for learning good policies with an interesting negative result -- they show that there exist MDPs which require an exponential number of samples to learn a near-optimal policy even if a good-but-not-perfect representation is given to the agent for both value-based and policy-based learning. Reviewers had some minor technical questions which were clarified sufficiently by the authors, leading to a consensus of the contribution and quality of this work. Thus, I recommend this paper for acceptance."
"abstract | strength  ==> In this paper, the authors present a new adversarial training algorithm and apply it to the fintuning stage large scale language models BERT and RoBERTa. They find that with FreeLB applied to finetuning, both BERT and RoBERTa see small boosts in performance on GLUE, ARC, and CommonsenseQA. The gains they see on GLUE are quite small (0.3 on the GLUE test score for RoBERTa) but the gains are more substantial on ARC and CommonsenseQA. The paper also presents some ablation studies on the use of the same dropout mask across each ascent step of FreeLB, empirically seeing gains by using the same mask. They also present some analysis on robustness in the embedding space, showing that FreeLB leads to greater robustness than other adversarial training methods <sep> This paper is clearly presented and the algorithm shows gains over other methods. I would recommend that the authors try testing their method on SuperGLUE because it's possible they're hitting ceiling issues with GLUE, suppressing any gains the algorithm may yield. <sep> Questions, <sep> -  In tables 4 and 5, why are only results on RTE, CoLA, and MRPC presented? If this is because there was not noticeable difference on the other GLUE datasets, please mention it in the text. <sep> - I realize that this method is meant to increase robustness in the embedding space, but did you do any error analysis on the models? Did they make different types of errors than models fine-tuned the vanilla way? <sep> Couple typos, <sep> - Section 2.2, line 1: many -> much <sep> - Section 4.2, GLUE paragraph: 88 -> 88.8","The paper proposes a new algorithm for adversarial training of language models. This is an important research area and the paper is well presented, has great empirical results and a novel idea."
"strength | decision  ==>  ==> This paper analyzes the effect of regularization on spectral embeddings in a deterministic block model and explicitly characterizes the spectra of the Laplacian of the regularized graph in terms of the regularization parameter and block sizes. To my knowledge, this has not been done before. Prior work either derives sufficient conditions for the recovery of all blocks in the asymptotic limit of an infinite number of nodes in the case of (Joseph & Yu, 2016), or lower bounds the number of small eigenvalues of the Laplacian of the unregularized graph on random graphs in expectation (therefore arguing in favor of regularization) in the case of (Zhang & Rohe, 2018). This paper, on the other hand, gives a precise characterization of the eigenvalues and eigenvectors (albeit in the case of a deterministic graph); the results are elegant and the analysis uses simple elementary techniques, which is very satisfying and seems to be easy to build on. The authors mention that they would like to extend this analysis to stochastic block models, which would indeed be interesting. The paper is also well written and the results are clearly presented. Overall, this is a nice contribution to spectral graph theory and so I recommend acceptance.","The paper proposes a nice and easy way to regularize spectral graph embeddings, and explains the effect through a nice set of experiments. Therefore, I recommend acceptance."
"abstract | weakness | rebuttal_process | decision | suggestion  ==> This paper proposes a new definition of algorithmic fairness that is based on the idea of individual fairness. They then present an algorithm that will provably find an ML model that satisfies the fairness constraint (if such a model exists in the search space). One needed ingredient for the fairness constraint is a distance function (or ""metric"") in the input space that captures the fact that some features should be irrelevant to the classification task. That is, under this distance function, input that differ only in sensitive attributes like race or gender should be close-by. The idea of the fairness constraint is that by perturbing the inputs (while keeping them close with respect to the distance function), the loss of the model cannot be significantly increased. Thus, this fairness constraint is very much related to robustness. <sep> --- <sep> Overall, I like the basic idea of the paper but I found the presentation lacking. <sep> I do think their idea for a fairness constraint is very interesting, but it gets too bogged down in the details of the mathematical theory. They mention Dwork et al. at the beginning but don't really compare it to their idea in detail, even though I think there would be a lot of interesting things to say about this. For example, the definition by Dwork et al. seems to imply that some labels in the training set might be incorrect, whereas the definition in this paper does not seem to imply that (which I think is a good thing). <sep> The main problem in section 2 is that the choice of distance function is barely discussed although that's what's most important to make the result fair. For all the mathematical rigor in section 2, the paragraph that is arguing that the defined constraint encourages fairness is somewhat weak. Here a comparison to other fairness definitions and an in-depth discussion of the distance function would help. <sep> (In general I felt that this part was more trying to impress the reader than trying to explain, but I will try to not hold it against this paper.) <sep> As it is, I feel the paper cannot be completely understood without reading the appendix. <sep> There is also this sentence at the bottom of page 5: ""A small gap implies the investigator cannot significantly increase the loss by moving samples from P∗ to comparable samples."" This should have been at the beginning of section 2 in order to motivate the derivation. <sep> In the experiments, I'm not sure how useful the result of the word embedding experiment really is. Either someone is interested in the sentiment associated with names, in which case your method renders the predicted sentiments useless or someone is not interested in the sentiment associated with names and your method doesn't even have any effect. <sep> Final point: while I like the idea of the balanced TPR, I think the name is a bit misleading because, for example, in the binary case it is the average of the TPR and the TNR. Did you invent this terminology? If so, might I suggest another name like balanced accuracy? <sep> I would change the score (upwards) if the following things are addressed: <sep> - make it easier to understand the main point of the paper <sep> - make more of a comparison to Dwork et al. or other fairness definitions <sep> - fix the following minor mistakes <sep> Minor comments: <sep> - page 2, beginning of section 2: you use the word ""regulator"" here once but everywhere else you use ""investigator"" <sep> - equation 2.1: as far as I can tell M is not defined anywhere; you might mean Δ(Z) <sep> - page 3, sentence before Eq 2.3: what does the # symbol mean? <sep> - page 3, sentence before Eq 2.3: what is T? is it Tλ? <sep> - Algorithm 2: what is the difference between λt∗ and λ^t? <sep> - page 7: you used a backslash between ""90%"" and ""10%"" and ""train"" and ""test"". That would traditionally be a normal slash. <sep> - in appendix B: the explanation for what Pran(A) means should be closer to the first usage <sep> - in the references, you list one paper twice (the one by Zhang et al.) <sep> EDIT: changed the score after looking at the revised version","The paper addresses individual fairness scenario (treating similar users similarly) and proposes a new definition of algorithmic fairness that is based on the idea of robustness, i.e. by perturbing the inputs (while keeping them close with respect to the distance function), the loss of the model cannot be significantly increased. <sep> All reviewers and AC agree that this work is clearly of interest to *CONF*, however the reviewers have noted the following potential weaknesses: (1) presentation clarity -- see R3's detailed suggestions e.g. comparison to Dwork et al, see R2's comments on how to improve, (2) empirical evaluations -- see R1's question about using more complex models, see R3's question on the usefulness of the word embeddings. <sep> Pleased to report that based on the author respond with extra experiments and explanations, R3 has raised the score to weak accept. All reviewers and AC agree that the most crucial concerns have been addressed in the rebuttal, and the paper could be accepted - congratulations to the authors! The authors are strongly urged to improve presentation clarity and to include the supporting empirical evidence when preparing the final revision."
"abstract | strength | decision  ==> This paper presents a method to speed up training of deep neural networks. The main contribution is a method to quickly identify winning lottery tickets (denoted early-bird, or EB by the authors), without running the model to convergence. The authors present interesting preliminary experiments that motivate their method, and show that it works on two image recognition datasets using two models. <sep> This paper addresses an under-explored, but very important problem in AI: the increasing cost of training models. The authors present interesting evidence about the potential to detect EBs early on. The experiments presented in Figures 1 and 3 are convincing and will be of interest to the community. The proposed method seems to work, at least on the setups explored by the authors. I am leaning towards acceptance, but am concerned with the following: <sep> 1. The authors experiment with a limited set of datasets (CIFAR-10 is a relatively easy task), and with a set of non-competitive baselines (SOTA for CIFAR-10/100 is 99%/91.3%, see https://benchmarks.ai/cifar-10{,0}). I would have liked to see whether the proposed method translates to harder datasets and stronger models. <sep> 2. I might be missing something here, but to the best of my understanding the large learning rate part (page 4) does not demonstrate the benefits of increasing the learning rate, but the problems with *decreasing* it. The two might seem like the same thing, but in fact they're not: the authors claim the [80,120] policy is standard, and use it when training the subnetwork, so showing that [0,100] is inferior does not present a way to improve over the current approach, but evidence that the other approaches are inferior. <sep> Other questions: <sep> 1. In Figure 1, it seems that the extracted subnetworks are doing very well even after 0 epochs. Does this mean that a trained version of a random subnetwork could reach within 1-2 points of the unpruned model? or is it pruned after training for 1 epoch? <sep> 2. If I understand correctly, Figure 5 should be illustrating the proposed method, which automatically identifies the early stopping point. In that case, I am not sure why the plot is a function of the epoch. <sep> 3. Do the authors have any intuition as to the sharp decrease in the 70% graph in Figures 1 and 2 around epoch 50? <sep> Writing: <sep> 1. The language used by the authors is sometimes exaggerated. Expressions such as ""bold guess"" (section 3.2), ""innovative ... scheme"" (section 4) and comparisons to Winston Churchill would be better left out of the manuscript. <sep> 2. Typos and such: <sep> -- several across the paper. For instance: <sep> - Intro: After *bring* identified (should be ""being"") <sep> - Related work: when training *it* isolation (in) <sep> -- Missing venue for Frankle and Corbin (2019)","This work studies small but critical subnetworks, called winning tickets, that have very similar performance to an entire network, even with much less training. They show how to identify these early in the training of the entire network, saving computation and time in identifying them and then overall for the prediction task as a whole. <sep> The reviewers agree this paper is well-presented and of general interest to the community. Therefore, we recommend that the paper be accepted."
"abstract | strength | suggestion  ==> The paper proposes an Energy-Based-Model (EBM) for scoring the possible configurations of amino acid side chain conformations in protein structures with known amino acid backbone structure. The energy of the side-chain conformation (the chi-angle) for a given amino acid in the structure is calculated as a function of a local neighbourhood of atoms (A), where each atom is embedded into a 256d vector using its cartesian coordinates, atom identity, atom side-chain position and amino acid identity. The model is trained using approximate likelihood where the model samples are generated using precalculated table (from literature) of possible Chi angles conformations conditioned on the back-bone amino acid identity and back-bone angles. The results seem comprehensive comparing the transformer based energy function parameterization with two sensible baselines as well as the Rosetta energy function  which is the de facto standard tool for these types of calculations. Using rotamer recovery accuracy as the benchmark measure the empirical results are close to performance as the Rosetta energy model however always slightly worse. Further visualizations of the energy levels for different Chi angles seems to support that the learned energy function captures well known characteristics of the rotamer configuration energy landscape. <sep> Score: <sep> Overall I think that the paper is solid and tackles an interesting problem of learning energy functions for physical systems from data. The experimental results are comprehensive and mostly supports the claims made in the paper. My main (slightly minor) concern about the paper is the obsession with discarding years of learned knowledge about handcrafted energy functions with fully learned functions. It seems to me that combining domain knowledge with a learned model should close the last small gap (and presumably surpass) the performance of e.g Rosetta. Combining I think the paper should be accepted at the *CONF*. <sep> Comment/Questions: <sep> Motivation <sep> Q1.1) Paper motivation: The paper several times seems to suggest that energy functions derived from physical knowledge are problematic (e.g. abstract). For many physical systems I don't think this is true and would like the author's comments on why learned energy functions are preferable - arguably they can only capture properties present in the data and not any prior knowledge? <sep> Q1.2) Given that the paper is motivated by fully learning an energy function from data without injecting any prior knowledge i would like the authors comment on the construction of the q(x|c) distribution. This is based on essentially a contingency table between Phi/Psi/Chi angles and to me seems like injecting prior knowledge about physically possible rotamer configurations into the learned procedure? <sep> Method / Experimental Results: <sep> Q2.1) With respect to the primary results in table 1) and table 2). The authors claim comparable results to the Rosetta Energy function (page 5. Sec 4.3, page 9, sec 6). However the experimental results are always slightly worse than the Rosetta energy function and I (strongly) suggest that the authors rephrase those statements to reflect that. <sep> Q2.2 With Respect to Table 3) Firstly, Why are the only results for 16 amino acids in the table ? Secondly, just naively counting the Atom Transformer are better than Rosetta in 11 of 16 amino acids - this seems slightly at odds with the main result where the Atom Transformer is performing slightly worse than Rosetta? <sep> Clarity <sep> Q3): The notation in section 3 and 4 is slightly confusing. Especially I think q(x|c) is slightly misleading since it, (to my understanding) is a conditional distribution over Chi, conditioned on Psi/Phi/AminoAcid and not rotamer (x) and surrounding molecular context (c).","The paper proposes a data-driven approach to learning atomic-resolution energy functions. Experiment results show that the proposed energy function is similar to the state-of-art method (Rosetta) based on physical principles and engineered features. <sep> The paper addresses an interesting and challenging problem. The results are very promising. It is a good showcase of how ML can be applied to solve an important application problem. <sep> For the final version, we suggest that the authors can tune down some claims in the paper to fairly reflect the contribution of the work."
"abstract | rebuttal_process | misc  ==>  ==> In this paper the authors proposed a framework for off-policy value estimation under the scenario of infinite horizon RL tasks. The new proposed method utilize the variational representation of f-divergence, which quantifies the difference between Tτ and τp, where τ is the parametric density ratio between the unknown behavior policy data and the target policy. If only if τ is the true density ratio, the loss Df(Tτ||τp)=0. <sep> Compared with prior work (Nachum et. al 2019), the new proposed framework can generalize the undiscounted case γ=1, and the derivation for the new algorithm is quite simple and easy to follow. The experimental results show the advantage of the proposed methods over baseline methods such as model-based, DualDice etc, for both discrete and continuous cases. Moreover, I have two specific questions: <sep> - The choice of f-divergence. Although the author mentioned the difficulty of using the dual representation of KL divergence, it would be nice to have an ablation study that shows the effectiveness of various f-divergence (Personally I think Jensen-Shannon Divergence may be also a good choice). <sep> -The authors should also have a discussion that similar idea can be generalized to more general  distribution metrics such as Integral Probability Metrics, specifically wasserstein-1 distance (similar to wasserstein-gan) or maximum mean discrepancy (Maybe it is unnecessary to conduct experiments, some discussion should be enough to clarify the relationship. I think there is a concurrent submission using MMD metrics). <sep> Overall I think this is a good paper and I recommend for acceptance. <sep> Reference Papers: <sep> - Nowozin, Sebastian, Botond Cseke, and Ryota Tomioka. ""f-gan: Training generative neural samplers using variational divergence minimization."" Advances in neural information processing systems. 2016. <sep> - Arjovsky, Martin, Soumith Chintala, and Léon Bottou. ""Wasserstein gan."" arXiv preprint arXiv:1701.07875 (2017). <sep> - Nachum, Ofir, et al. ""DualDICE: Behavior-Agnostic Estimation of Discounted Stationary Distribution Corrections."" arXiv preprint arXiv:1906.04733 (2019). <sep> - Anonymous, ""Black-box Off-policy Estimation for Infinite-Horizon Reinforcement Learning"", submitted to *CONF* 2020.","The authors develop a framework for off-policy value estimation for infinite horizon RL tasks, for estimating the stationary distribution of a Markov chain. Reviewers were uniformly impressed by the work, and satisfied by the author response. Congratulations!"
"abstract | decision | rebuttal_process | misc  ==> The paper propose a new MCMC scheme which is demonstrated to perform well for estimating Bayesian neural networks. The key idea is to not keep lowering the step sizes, but -- at pre-specified times -- go back to large step sizes. <sep> The paper is timely, the proposed algorithm is novel, and the theoretical analysis also seem quite novel. <sep> My key concern is that with MCMC sampling it is often quite difficult to tune parameters, and by introducing more parameters to tune when step sizes should increase, I fear that we end up in a ""tuning nightmare"". How sensitive is the algorithm to choice of parameters? <sep> I would expect that the proposed algorithm is quite similar to just running several MCMCs in parallel. The authors does a comparison to this and show that their approach is significantly faster due to ""warm restarts"". Here I wonder how sensitive this conclusion is to choice of parameters (see nightmare above) ? I would guess that opposite conclusions could be reached by tuning the algorithms differently -- is that a reasonable suspicion ? <sep> It is argued that the cyclic nature of the algorithms gives a form of ""warm start"" that is beneficial for MCMC. My intuition dictate that this is only true of the modes of the posterior are reasonable close to each other; otherwise I do not see how this warm starting is helpful. I would appreciate learning more about why this intuition is apparently incorrect. <sep> Minor comments: <sep> * on page 4 it is stated that the proposed algorithm ""automatically"" provide the warm restarts -- but is it really automatic? Isn't this a priori determined by choice of parameters for the algorithm? <sep> * It would be good to use \\citet instead of \\cite at places, e.g. ""discussed in (Smith & Topin, 2017)"" should be ""discussed by Smith & Topin (2017)"". This would improve readability (which is generally very good). <sep> * For the empirical studies I think it would be good to report state-of-the-art results as well. I expect that the Bayesian nets still are subpar to non-Bayesian methods, and I think the paper should report this.","This paper proposes a novel stochastic gradient Markov chain Monte Carlo method incorporating a cyclical step size schedule (cyclical SG-MCMC). The authors argue that this step size schedule allows the sampler to cross modes (when the step size is large) and locally explore modes (when the step size is smaller). SG-MCMC is a very promising method for Bayesian deep learning as it is both scalable and easily to incorporate into existing models. However, the stochastic setting often leads to the sampler getting stuck in a local mode due to a requirement of a small step size (which itself is often due to leaving out the Metropolis-Hastings accept / reject step). The cyclic learning rate intuitively helps the sampler escape local modes. This property is demonstrated on synthetic problems in comparison to existing SG-MCMC baselines. The authors demonstrate improved negative log likelihood on larger scale deep learning benchmarks, which is appreciated as the related literature often restricts experiments to small scale problems. The reviewers all found the paper compelling and argued for acceptance and thus the recommendation is to accept. Some questions remain for future work. E.g. all experiments were performed using a very low temperature, which implies that the methods are not sampling from the true Bayesian posterior. Why is such a low temperature needed for reasonable performance? In any case a very nice paper."
"abstract | strength | rating_summary  ==>  ==> This work is an attempt to bridge the gap between discriminative models, which currently obtain the state of the art on most classification problems, and generative models, which (through a model of the marginal p(x)) have the potential to shine on many tasks beyond generalization to a hold-out set with minimal shift in distributions: out of distribution detection, better generalization out of distribution, unsupervised learning etc. <sep> While much of the current work is related to normalizing flows / invertible neural networks, the authors here propose a quite simple but appealing method: A standard neural classifier is taken and the softmax is layer chopped off and replaced by an energy based model, which models the joint probability p(x,y) instead of the posterior p(y|x). The advantage is an additional degree of freedom in the scale of the logit vector, which is would have been otherwise normalized by the softmax layer and now can now model the data distribution. The downside is the loss in ease of training. Whereas (discriminative) deep networks can be easily trained by gradient descent on a cross-entropy objective, the partition function in the energy model makes this un tractable. This is addressed through sampling, similar to (Welling & Teh, 2011). <sep> One of the biggest achievements reported by the authors is that the performance on discriminative tasks is not hurt (much) by adding the generative model. There is only a 3 point gap between Wide-ResNet and the proposed model (92.9% vs. 95.8%) … but on what dataset? 3 datasets are mentioned in the experimental section, but table 1 does not mention on which datasets the accuracy is reported. My guess is that this is a mean or mixture, since GEM performances of 96.7% and 72.2% are reported for SVHN and CIFAR10, respectively, but this should be made clearer. <sep> On out of distribution detection, could the authors comment on the histograms in table 2, in particular the difference between the new measure (AM JEM) compared to JEM log p(x) on CelebA? The proposed measure does not seem to fare well here. <sep> Although the method does not outperform the gold standard of adversarial training, I found the models robustness to adversarial examples quite appealing, given that it was not trained for this objective (which also means that it does not require an adaptation to a norm). <sep> I was very impressed by Figure 6 showing distal adversarial initialized from random images, showing pretty clear images of the modelled class. The modelled variations require more investigation to verify whether we have a collapse for each class, but the results look very promising. <sep> The paper is well written and easy to understand. A couple of details on the training procedure are missing in the experimental part. It is stated that, both, p(y|x) and the generative part p(x), are optimized, but how are these exactly integrated? Given the difficult in training this model reported in the paper, this seems to be particularly important. <sep> I also appreciated the description of the limitations of the algorithm, and the details in the appendix (*CONF* should go back to unlimited paper lengths, btw.). <sep> More information on complexity (training times etc.) should also be helpful.","This paper uses energy based model to interpret standard discriminative classifier and demonstrates that energy based model training of the joint distribution improves calibration, robustness, and out-of-distribution detection while generating samples with better quality than GAN-based approaches. The reviewers are very excited about this work, and the energy-based perspective of generative and discriminative learning. There is a unanimous agreement to strongly accept this paper after author response."
"abstract | rating_summary | ac_disagreement | weakness | decision  ==> This paper proposes a new threat model for generative impersonation attacks: The attacker has access to several leaked images of a person; the authenticator knows several registration images per person and decides a person's identify by comparing some newly-sampled images from that person with corresponding registration images. The authors formulate this threat model as a minimax game and analyzed its Nash equilibrium. In the simplified case that observations are multivariate Gaussian, the authors are able to characterize the optimal strategies of the attacker and authenticator explicitly, which gives a nice intuition on how the theoretical optimum changes with respect to data dimension, number of leaked images, etc. Additionally, the authors implemented this attack (named Gan-in-the-middle attack) with an objective similar to GANs, empirically verified the theoretical results, and demonstrated the success of their approach on VoxCeleb2 and <sep> As far as I know, this formulation of generative impersonation attacks is novel. The threat model nicely captures the most important aspects of impersonation attacks and is relatively realistic. <sep> The theoretical analysis is insightful. I especially like that the authors can prove no defense is possible when n <= m, which nicely matches the intuition. The results on Gaussian case not only provide intuition, but also provide motivation for the design of attacker and defender architectures in GIM attacks. <sep> The experiments are well-designed. The model architectures are well-motivated from Theorem 4.2. It is great to see that results of toy experiments match the theoretical analysis in Figure 1(a). The GIM attack on the Voxceleb2 images generates very realistic and reasonable portraits in Figure 2(a). The data augmentation experiment can be naturally fit into the framework of impersonation attacks and the application of their techniques in this direction is very exciting. <sep> I only have two minor suggestions: <sep> 1. In Theorem 4.1, the symbol g_{X | Y} was introduced previously, whereas g_{X | A} was never introduced. I have to go to the appendix to understand the definition of g_{X | A}. <sep> 2. There is a minor issue in the proof of Lemma D.2 in page 16. The authors seem to miss a 1/2 factor in the second to last row in equation (D.3).","This paper concerns the problem of defending against generative ""attacks"": that is, falsification of data for malicious purposes through the use of synthesized data based on ""leaked"" samples of real data. The paper casts the problem formally and assesses the problem of authentication in terms of the sample complexity at test time and the sample budget of the attacker. The authors prove a Nash equillibrium exists, derive a closed form for the special case of multivariate Gaussian data, and propose an algorithm called GAN in the Middle leveraging the developed principles, showing an implementation to perform better than authentication baselines and suggesting other applications. <sep> Reviewers were overall very positive, in agreement that the problem addressed is important and the contribution made is significant. Most criticisms were superficial. This is a dense piece of work, and presentation could still be improved. However this is clearly a significant piece of work addressing a problem of increasing importance, and is worthy of acceptance."
"abstract | rating_summary | rebuttal_process | decision  ==>  ==> Summary: This paper proposes GraphZoom, a framework for augmenting unsupervised graph embedding methods by (a) fusing feature information into the graph topology, (b) learning embeddings on a coarsened graph, and (c) refining the coarsened embeddings to obtain embeddings for the original graph nodes. In particular, a nearest neighbor graph over node features is computed and this adjacency matrix is linearly combined with the original adjacency matrix to obtain a graph with feature information ""fused in"". The graph is then coarsened using a spectral approach, embeddings are learned on the coarsened graph (via any strategy), and the embeddings are then refined back to the original nodes (again using a spectral approach). The authors take care to heed the advice of Maehara et al. and remove high-frequency information from the features. <sep> Assessment: Overall, this is a borderline contribution with some interesting motivation, original ideas, and sound derivations. However, the primary limitation of this work is the empirical comparison. First, the empirical comparison includes DeepWalk and GraphSAGE as the two base models, and while these are reasonable models, they are known to no longer be state of the art in this area (e.g., see https://arxiv.org/pdf/1809.10341.pdf). It would be more appropriate to include a more recent and better performing method (e.g., DGI; linked previously), as the reported numbers are very far from state-of-the-art. In addition---and perhaps a more concerning issue---is that seems that a randomly initialized GCN can obtain similar or superior performance compared to the numbers reported in this work (again, see the DGI paper linked above). While it is possible that GraphZoom+DGI or GraphZoom+[some other more recent method] could achieve stronger results, the fact that the current results seem to be below performance of a randomly initialized GCN is a major issue. Stronger empirical results with better baselines and base models would drastically improve the paper. <sep> As another point regarding the empirical results, the datasets used are known to be problematic (e.g., see https://arxiv.org/abs/1811.05868). If these datasets are used, then multiple random splits should be employed and more robust summary statistics should be reported. <sep> Regarding the fusion step, there were also two points that should be addressed in the paper: <sep> 1) It seems that this fusion setup is assuming that the network exhibits homophily (i.e., it assumes that nearby nodes have similar features). This is common in many networks (e.g., the benchmarks that are analyzed) but not always the case. Some commentary on when (if ever) this fusion process might *not* be appropriate would improve the paper. <sep> 2) The authors state the they use the coarsening process to compute the nearest neighbor graph in order to avoid the quadratic time complexity. However, there are numerous well-established approaches to deal with this issues (e.g., locality sensitive hashing). Why was one of these standard approaches not employed? <sep> Reasons to accept: <sep> - Original and well-motivated idea <sep> - Clearly written paper <sep> Reasons to reject: <sep> - Problematic empirical evaluation (e.g., lacking recent baselines) <sep> - Several performance numbers appear to be below random GCN baseline performance <sep> - General applicability of the approach (e.g., to non-homophilous networks) is not clear",The authors present an approach for learning graph embeddings by first fusing the graph to generate a new graph with encodes structural information as well as node attribution information. They then iteratively merge nodes based spectral similarities to obtain coarser graphs. They then use existing methods to learn embeddings from this coarse graph and progressively refine the embeddings to finer graphs. They demonstrate the performance of their method on standard graph datasets. <sep> This paper has received positive reviews from all reviewers. The authors did a good job of addressing the reviewers' concerns and managed to convince the reviewers about their contributions. I request the authors to take the reviewers suggestions into consideration while preparing the final draft of the paper and recommend that the paper be accepted.
"abstract | strength | decision | misc | rebuttal_process  ==>  ==> This paper presents a method to make Transformer models more efficient in time and memory. The proposed approach consists mainly of three main operations: <sep> - Using reversible layers (inspired from RevNets) in order to prevent the need of storing the activations of all layers to be reused for back propagation; <sep> - Using locality sensitive hashing to approximate the costly softmax(QK^T) computation in the full dot-product attention; <sep> - Chunking the feed-forward layers computations to reduce their cost. <sep> This approach is first applied to a toy dataset to analyze its complexity, then tested on enwik8 language modelling task and imagenet-64 image generation task for ablation study and performance assessment. <sep> The problem approached by the paper is interesting and the proposed approach is novel to the best of my knowledge. The paper is well structured and clearly written a part from some small typos (see minor comments below). <sep> While the analysis of complexity is sound and convincing, and the fact of being able to train larger Reformers is very interesting, I have some questions and concerns about the approach and experiments. <sep> - Effect of reversible layers: It is clear for the experiment of Imagenet64 that the effect is negligible, but the experiment on enwik8 in the paper seems unfinished. Did the authors manage to finish the training, and does it confirm the observation? <sep> - Sharing QK: I am a bit confused about the effect and usefulness of this operation. Can the authors comment on why it is needed for LSH attention? It seems to me that the same operations can be achieved with different Q and K. Indeed, doing so, the authors slightly reduce the capacity of the model. The observed non-significantly decreased performance can be an effect of using only 3-layers. This may explain why the results reported for larger models in figure 5 show higher bpc than similar size state of the art models. <sep> - Time per iterations: Can the authors report the time per iteration for the larger hash rounds (8 and 16) that are closer to full attention? For the highest reported number (4), from a quick and not precise look at figure 4, it seems that the performance achieved by the proposed method after 140k iterations is achieved by the full attention after ~40k iterations. The gain in time per iteration for this particular number of hash rounds can be lost by the loss in performance. <sep> - Can the authors detail how they chose the hyperparameters of their approach? e.g. the size of hash buckets, the distribution used to generate the random matrix R .. <sep> - The reported results can be made stronger by reporting average/error bars across several trial to show consistency. <sep> Minor: typos: <sep> Dimension of matrix R [d_k, d_b/2] -> [d_k, b/2] <sep> Last paragraph of page 6: state of these art -> state of the art <sep> ——————————————— <sep> After rebuttal: <sep> I have read the authors answer, and found they addressed my concerns. I'm therefore increasing my score.","Transformer models have proven to be quite successful when applied to a variety of ML tasks such as NLP. However, the computational and memory requirements can at times be prohibitive, such as when dealing with long sequences. This paper proposes locality-sensitive hashing to reduce the sequence-length complexity, as well as reversible residual layers to reduce storage requirements. Experimental results confirm that the performance of Transformer models can be preserved even with these new efficiencies in place, and hence, this paper will likely have significant impact within the community. <sep> Some relatively minor points notwithstanding, all reviewers voted for acceptance which is my recommendation as well. Note that this paper was also vetted by several detailed external commenters. In all cases the authors provided reasonable feedback, and the final revision of the work will surely be even stronger."
"abstract | strength | weakness | rebuttal_process | decision | rebuttal_process | strength  ==>  ==> The focus of the presented paper is on formulating the automated discovery of self-organized patterns in high-dimensional dynamic systems. The introduced framework uses cellular automata (game of life) as a testbed for experimentation and evaluation and existing machine learning algorithms (POP-IMGEPs). The goal of the paper is to show that these algorithms can be used to discover and represent features of patterns. Moreover, an extension of SOTA algorithms is introduced and several approaches to define goal space representations are compared. <sep> Overall, I have the impression this is an interesting paper that could be accepted to *CONF*. The idea of applying IMGEPs to explore parameters of a dynamic system is novel and interesting, which could also simulate further research in this field. Furthermore, the paper well-written, technically sound, and the results are interesting. The overall contribution of the paper is in applying IMGEP algorithms to exploring parameters of dynamic systems and in comparing different algorithms along with an extensive set of experiments. As a point of criticism, a lot of (interesting) material was pushed to the Appendix. Resolving the references makes reading the paper harder. Moreover, given that this paper has more than 35 pages appendix material, it seems this work would better be suited for a journal as for a conference. There is a reason for papers to have a page limit and this work circumvents this limit by presenting a lot of additional material. Therefore, I am not willing to strongly support this work. <sep> Specific Comments: <sep> - Section 3.1: It is not clear how the initial system state is established. In Section 3.1. the text states that 'parameters are randomly sampled and explored' before the process starts, but it is not clear why a random sampling is used and what this means for the subsequent sampling. Later in the text (3.3) it becomes more clear, but here this appears too unclear. <sep> -  Section 3.1: ""distribution over a hypercube in \\mathcal{T} chosen to be large enough to bias exploration towards the frontiers of known goals to incentivize diversity."" This sentence is not clear and needs more details. How is the distribution chosen exactly? <sep> - Section 3.2 appears a bit repetitive and could be more concise. I don't think it is necessary here to contrast manual vs learned features of the goal space. <sep> - Section 3.2 (P3): the last sentence of this paragraph reads as if there exists no approaches for VEAs in online settings. This should be toned down or backed up by a reference. <sep> - Section 3.2: (last sentence): it is not clear how the history is used exactly to train the network. Which strategy is used to sample from the history of observations? <sep> - Section 3.3: What is meant by ""The CPPNs are used of the parameters \\{theta}""? The details provided after this sentence are not clear and need more details. <sep> - Section 4.2: Please provide more details what ""very large"" dataset means. <sep> - Section 4.2: 'HGS algorithm' is not defined. <sep> - Section 5: It seems unnecessary to explain what t-SNE does as a method.","The authors introduce a framework for automatically detecting diverse, self-organized patterns in a continuous Game of Life environment, using compositional pattern producing networks (CPPNs) and population-based Intrinsically Motivated Goal Exploration Processes (POP-IMGEPs) to find the distribution of system parameters that produce diverse, interesting goal patterns. <sep> This work is really well-presented, both in the paper and on the associated website, which is interactive and features source code and demos. Reviewers agree that it's well-written and seems technically sound. I also agree with R2 that this is an under-explored area and thus would add to the diversity of the program. <sep> In terms of weaknesses, reviewers noted that it's quite long, with a lengthy appendix, and could be a bit confusing in areas. Authors were responsive to this in the rebuttal and have trimmed it, although it's still 29 pages. My assessment is well-aligned with those of R2 and thus I'm recommending accept. In the rebuttal, the authors mentioned several interesting possible applications for this work; it'd be great if these could be included in the discussion. <sep> Given the impressive presentation and amazing visuals, I think it could make for a fun talk."
"abstract | strength | rating_summary | misc | decision | suggestion  ==>  ==> Post Rebuttal Summary <sep> --------------------------------- <sep> I have nudged my score up to an ""Accept"", based on my comments to the rebuttal below. I hope the authors continue to improve the readability of Sec. 2.1 <sep> Review Summary <sep> -------------- <sep> Overall I think this is almost above the bar to be accepted, and I could be persuaded with a strong rebuttal.  The strengths here are the extensive experiments and the easy-to-implement method. The primary weakness of this paper is that it is a ""straightfoward"" way to extend the BBP-MAP method to CNNs and RNNs, so the methodological novelty is weak relative to the BBP-MAP past work (Yurochkin et al. ICML 2019). Other technical weaknesses limit the ability to use this method on clients with diverse class distributions, which will be common in real deployments. <sep> Paper Summary <sep> ------------- <sep> This paper addresses the problem of federated learning, where J separate ""clients"" with disjoint datasets each train a neural network model for a supervised problem, and then try to aggregate all J individual client models into one ""global model"" in a coherent way. The natural problem is that due to hidden units being permutable within one network, naively taking parameter averages across two client models will lead to bad accuracy without first coming up with a consistent ordering of the units in each layer. <sep> Previous work (Yurochkin et al. ICML 2019) has developed a Bayesian nonparametric model based on the Beta-Bernoulli Process (BBP) for the case of federated learning of multi-layer perceptrons. However, the extension to convolutional layers or recurrent layers has yet to be solved, which is the focus of this paper. <sep> This paper's algorithm (Federated Matched Averaging (FedMA), see Alg 1), proceeds by iteratively stepping thru the CNN or RNN layer by layer greedily from input to output. At each layer, we first solve a BBP-MAP optimization (bipartite matching using a BBP maximum a-posteriori objective as cost function, a subprocedure taken direclty from Yurochkin et al.). This obtains a consistent low-cost permutation for each client model. Then, the global model weights for that layer is the average of the aligned client weights. After the current layer update, each client keeps training, keeping all layers up to the current frozen but revising later layers. This layer-by-layer training can be applied to both CNNs and RNNs. <sep> The proposed approach is compared to FedAvg and FedProx on MNIST and CIFAR image classification tasks with CNNs, and Shakespeare text classification tasks with RNNs. Later experiments explore the effect of communication efficiency (MB transfered between client and master), effect of local training epochs, handling biased class distributions, and interpretabilty. <sep> Novelty & Significance <sep> ----------------------- <sep> Solving federated learning problems is of increasing practical importance, and certainly trying to do so for CNNs and RNNs (more than just large MLPs) is important. So I like where the paper is going. <sep> Although the method is ""new"", it is more or less a straightforward extension of work by Yurochkin et al. (ICML 2019) to CNNs and RNNs. If you read the last few sentences of Yurochkin et al., you'll see ""Finally, it is of interest to extend our model-ing framework to other architectures such as Convolutional Neural Networks (CNNs) and Recurrent Neural Networks (RNNs). The permutation invariance necessitating matching inference also arises in CNNs since any permutation of the filters results in the same output, however additional bookkeeping is needed due to the pooling operations."" I view this paper as a well-executed implementation of this ""bookkeeping"". Certainly not trivial, but to some readers perhaps not clearly ""above the bar"" for a top conference like *CONF*. <sep> Technical Concerns <sep> ------------------ <sep> ## Concern 1: Client models will not always be alignable after permutation <sep> My first concern is that there will not always be a one-to-one permutation of the neurons learned by two client models with different class distributions. Given fixed capacity at each layer, some clients may learn a filter for ""horse hooves"" (esp. if horse images are common to that client), while other clients may learn a filter for ""snake skin"" (if snakes are more common to that client). I wonder if we can quantify how well the aligned filters match in practice, and if there is any benefit to revising the alignment to allow some client-specific customizations (e.g. by having the global model can learn more units than the client model). <sep> ## Concern 2: Use of the BBP-MAP subprocedure poorly motivated <sep> The paper prioritizes a clean and easy-to-implement algorithm to resolve practical alignment issues between client CNN and RNN models. However, I was a bit underwhelmed that the BBP-MAP solution used by Yurochkin et al. was treated as a black-box subprocedure without much justification. I could see 2 preferable alternatives to the current use of BBP-MAP. Either a simpler approach using Eq. 2 with a squared error cost and the Munkres algorithm to solve bipartitite matching to obtain the permutation (which seems more in spirit of the rest of the paper). Or, a more sophisticated probabilistic approach (taking a Bayesian hierarchical model from Yurochkin et al. seriously and forming the estimated global weights from a weighted sums that includes both the clients (weighted by dataset size) and the assumed prior). As it is, I feel the BBP-MAP subprocedure in the current Algorithm 1 is poorly motivated for the task at hand. <sep> Experimental Evaluation <sep> ----------------------- <sep> Overall the experiments were extensive and demonstrated several apparent advantages (reduced need to transfer large memory during communication, etc.). <sep> Minor Presentation Concerns <sep> --------------------- <sep> Before Eq. 2, you should introduce the ""\\theta"" notation <sep> I'm a bit confused about how ""FedMA"" differs from ""FedMA with communication"", even after reading Sec. 2.3. How exactly are communicate costs kept down? What are you sending from master to client at beginning of every ""round"" if not the full global model (all weights of the CNN)?","The authors presented a Federate Learning algorithm which constructs the global model layer-wise by matching and averaging hidden representations. They empirically demonstrate their method outperforms existing federated learning algorithms <sep> This paper has received largely positive reviews. Unfortunately one reviewer wrote a very short review but was generally appreciative of the work. Fortunately, R1 wrote a detailed review with very specific questions and suggestions. The authors have addresses most of the concerns of the reviewers and I have no hesitation in recommending that this paper should be accepted. I request the authors to incorporate all suggestions made by the reviewers."
"abstract | weakness | rebuttal_process | decision  ==> This paper studies overparameterized fully-connected neural networks trained with squared loss. The authors show that the resulting network can be decomposed as a sum of the solution of a certain interpolating kernel regression and a term that only depends on initialization. Based on this, the authors also derive a generalization bound of deep neural networks by transferring it to a kernel method. My major concern about this paper is the novelty and significance of its results: <sep> In terms of connection to NTK, It seems that the connection between neural networks trained with squared loss and the result of NTK-based kernel regression has already been well-studied by <sep> Arora, Sanjeev, Simon S. Du, Wei Hu, Zhiyuan Li, Ruslan Salakhutdinov, and Ruosong Wang. ""On exact computation with an infinitely wide neural net."" arXiv preprint arXiv:1904.11955 (2019). <sep> which is a missed citation. Without a clear explanation on the difference between the submission and this paper above, I don't think this paper is ready for publication. <sep> In terms of generalization, it is also very difficult to judge whether this paper's result is novel. In fact this paper misses almost all citations on generalization bounds for neural networks. Moreover, the generalization bound given in this paper does not seem to be very complete and significant, since the authors do not show when can L_{test}^{int} be small. To demonstrate the novelty and significance of the result, the authors should at least compare their generalization result with the following generalization bounds for over-parameterized neural networks in Section 4: <sep> Allen-Zhu, Zeyuan, Yuanzhi Li, and Yingyu Liang. ""Learning and generalization in overparameterized neural networks, going beyond two layers."" arXiv preprint arXiv:1811.04918 (2018). <sep> Cao, Yuan, and Quanquan Gu. ""A generalization theory of gradient descent for learning over-parameterized deep relu networks."" arXiv preprint arXiv:1902.01384 (2019). <sep> Arora, Sanjeev, Simon S. Du, Wei Hu, Zhiyuan Li, and Ruosong Wang. ""Fine-grained analysis of optimization and generalization for overparameterized two-layer neural networks."" arXiv preprint arXiv:1901.08584 (2019). <sep> Cao, Yuan, and Quanquan Gu. ""Generalization Bounds of Stochastic Gradient Descent for Wide and Deep Neural Networks."" arXiv preprint arXiv:1905.13210 (2019). <sep> Overall, I suggest that the authors should make a clear discussion on the relation of this paper to many existing works mentioned above. As long as the authors can give a convincing demonstration of the novelty and significance of their results, I will be happy to increase my score. <sep> A minor comment: how can the bound in Theorem 3 be derived based on Theorem 2? Should there be a constant factor in the bound?","This paper proves that fully-connected wide ReLU-NNs trained with squared loss can be decomposed into two parts: (1) the minimum complexity solution of an interpolating kernel method, and (2) a term depends heavily on the initialization. The main concerns of the reviewers include (1) the contribution are not significant at all given prior work; (2) flawed proof, and (3) lack the comparison with prior work. Even the authors addressed some of the concerns in the revision, it still does not gather sufficient support from the reviewers after author response. Thus I recommend reject."
"abstract | weakness | misc | weakness | decision  ==> Main contribution of the paper <sep> - The paper argues that the base assumption, the i.i.d. of the activated elements (activations) in the hidden layers, the existing methods (lee.et.al 2018) hold is not convincible. <sep> - Instead, the author proposes a new way to probabilistically model the hidden layers, activations, and layer/layer connections. <sep> - Based on the probabilistic model, the paper proposes a new regularizer. <sep> Methods <sep> - The author argues that the activation is not iid by empirically showing that the trained MLP (in most cases) does not un-correlated. <sep> - The author proposes a new probabilistic model for MLP, and CNN assuming the Gibbs distribution to each activation and also assuming the product of expert (poE) model to explain the layer/layer relationship. <sep> - And according to their model, CNN will be explained by the MRF model. <sep> - The author proposes a regularization term regarding layer/layer connection. <sep> - They argue that the SGD training can be seen as a first-order approximation of the inference of the hidden activations in MLP. <sep> Questions <sep> - See the Concerns <sep> Strongpoints <sep> - The probabilistic explanation of the MLP and the CNN seems novel and was interesting to the reviewer <sep> - The proposed explanation assumes a weaker condition compared to the existing methods. <sep> Concerns <sep> - The main concern is that the reviewer cannot fully convince that i.i.d. assumption is wrong. <sep> Even though the trained MLP does not support the i.i.d. condition, one can suppose that the reason would be the typical training method (SGD), just finding the local minima in a deterministic way. <sep> Maybe the proof in Appendix.G. supports the argument of the author, but the reviewer failed to clearly agree with the argument. <sep> A clear explanation regarding the issue would be required. <sep> - As far as the author understands, the paper proposes a probabilistic (Bayesian) model for explaining MLP, but it seems that they just used SGD for training the model. <sep> In that case, the reviewer is little suspicious of the role of the proposed regularization in that the regularization comes from Bayesian formulation, but the model was trained in a deterministic way. <sep> The reviewer wants to ask the author that <sep> (1) is it possible to infer the model in a Bayesian manner such as sampling? <sep> (2) Is there any justification for using SGD when conducting the experiments regarding the regularization? If it is related to Appendix.G, clearer explanation would be appreciated. <sep> - As far as the reviewer understands, the regularization deals with the practical part of the paper. It would be better to see the effect of the regularization of widely used networks such as small-layered ResNet or others. <sep> If the proposed formulation has other practical strongpoints, it would be nice to clarify them. <sep> - The explanation using Gibbs distribution and PoE looks similar to RBM. The reviewer strongly wants a clear explanation of the difference and the strongpoints compared to RBM. <sep> Conclusion <sep> - The author proposed a new probabilistic explanation of the neural network, which seems novel and worth reporting. <sep> - However, the reviewer failed to fully agree on some steps in the process of the paper. <sep> Therefore, the reviewer temporary rates the paper as weak-reject, but this can be adjusted after seeing the answers of the author. <sep> Inquiries <sep> - See the concerns parts.","This paper makes a claim that the iid assumption for NN parameters does not hold. The paper then expresses the joint distribution as a Gibbs distribution and PoE. Finally, there are some results on SGD as VI. Reviewers have mixed opinion about the paper and it is clear that the starting point of the paper (regarding iid assumption) is unclear. I myself read through the paper and discussed this with the reviewer, and it is clear that there are many issues with this paper. <sep> Here are my concerns: <sep> - The parameters of DNN are not iid *after* training. They are not supposed to be. So the empirical results where the correlation matrix is shown does not make the point that the paper is trying to make. <sep> - I agree with R2 that the prior is subjective and can be anything, and it is true that the ""trained"" NN may not correspond to a GP. This is actually well known which is why it is difficult to match the performance of a trained GP and trained NN. <sep> - The whole contribution about connection to Gibbs distribution and PoE is not insightful. These things are already known, so I don't know why this is a contribution. <sep> - Regarding connection between SGD and VI, they do *not* really prove anything. The derivation is *wrong*. In eq 85 in Appendix J2, the VI problem is written as KL(P||Q), but it should be KL(Q||P). Then this is argued to be the same as Eq. 88 obtained with SGD. This is not correct. <sep> Given these issues and based on reviewers' reaction to the content, I recommend to reject this paper."
"abstract | misc | rebuttal_process | misc  ==>  ==> Major caveat: I have published in the area of adversarial attacks on NLP models, but the specifics of the methods presented in this paper are quite outside of my expertise, and I do not have time to become familiar with them for this review.  I hope there are other reviewers that are more qualified than I am to check the specifics of the methods. <sep> This paper presents a new technique for generating adversarial examples, by first learning the data manifold in an embedding space, then finding an adversarial example that lies on the manifold.  I like this idea, it intuitively seems like a promising method for obtaining semantically meaningful adversarial examples. <sep> As I said above, I do not feel qualified to review whether the method should _theoretically_ accomplish its goals, so my judgment of this paper is on the intuition behind the idea (which I like), and the results that I can see (which are less promising).  In order to have a ""semantics preserving"" attack, the method needs to (1) remain on the data manifold, and (2) not change the label a human would give to the input. <sep> For (1), this appears to have been accomplished on most datasets, though it seems pretty hard to argue that the artifacts seen in the MNIST examples shown are on the data manifold - there are no such artifacts in any of the inputs, or in the clean reconstruction.  How do the authors claim that this actually did a reasonable job of staying in the data manifold? <sep> For (2), most of the images do indeed look like they should retain their human labels, which is good (but also not hard for adversarial images).  Almost all of the textual examples, however, have correct predictions from the model after the adversarial change to the input.  You can't really argue that these are ""semantics preserving"", or even ""successful attacks"", as they change the expected input label.  This is why semantics-preserving attacks are so hard in NLP, and I don't think that this method has accomplished its goal here at all, at least for text.  The authors should consult with experts in NLP before making claims about successfully constructing semantics preserving attacks on NLP models. <sep> I'm pretty on the fence about this paper, as I like the intuition, and the method appears to work reasonably well for vision.  It does not work as claimed for text, however, and that should be fixed before this paper is published (either with softened claims or with better results).  Hopefully people from other perspectives can pipe in and give a more clear picture on this paper. <sep> EDIT: See discussion below for my justification for reducing my score from a 3 to a 1. <sep> EDIT 11/14: The authors' revisions have satisfied my concerns about how the NLP attacks are described.  I'm a little bit nervous about how the examples were changed - it seems that nothing changed about the method itself, so the authors probably cherry-picked better examples - but that's not sufficiently worrying to me to justify rejection.  The pilot study is also quite weak, as the number of inputs that were evaluated was only 20, and the questions presented don't appear to ask about changes in the label.  I don't know how you could get 100% on that given the examples that I saw in the previous version of the paper.  This is all to say that I don't think the NLP attacks are actively problematic anymore, as they were previously, now they are just weak.  The main contribution here is the technical contribution, anyway, so weaker results on one of the datasets tested is not a deal-breaker to me.  Assuming the technical contributions pass muster (which, as I said, I don't really feel qualified to judge), I'm satisfied with this paper as it is now.","This paper describes a method for generating adversarial examples from images and text such that they maintain the semantics of the input. <sep> The reviewers saw a lot of value in this work, but also some flaws. The review process seemed to help answer many questions, but a few remain: there are some questions about the strength of the empirical results on text after the author's updates. Wether the adversarial images stay on the manifold is questioned (are blurry or otherwise noisy images ""on manifold""?). One reviewer raises good questions about the soundness of the comparison to the Song paper. <sep> I think this review process has been very productive, and I hope the authors will agree. I hope this feedback helps them to improve their paper."
"abstract | weakness  ==> This paper proposed an auxiliary loss based on mutual information for graph neural network. Such loss is to maximize the mutual information between edge representation and corresponding edge feature in GNN 'message passing' function. By assuming a gaussian distribution of edge feature given edge representation, the training can be done efficiently with tractable density. Experiments on molecule regression and knowledge graph completion show better performance than MPNN. <sep> Overall the paper is written in a clear way which is easy to follow. The idea of using mutual information as some kind of regularization is also interesting. However, there are some concerns I have with the paper: <sep> Regarding formulation <sep> 1. The derivation up to Eq(8) looks fine to me, where the assumptions are reasonable. However from Eq(8) one can see this is reduced to an 'auto-encoder' type of regularization, where one can have a trivial solution for reconstruction -- the identity network, when the hidden dimension is larger than input dimension. And in this paper, dimension of W should always be larger than the dimension of e (for example, in molecules e should be low dimension vector with the bond type, distance, etc., while W should have dimension that matches the node embeddings). <sep> I think the original loss (i.e., the supervised MSE, cross entropy etc) would help a bit with such degenerated case, but it is possible that the learned f(e) contains both identity mapping (or equivalent) and the representation that contributes to original loss. <sep> 2. Actually I'm also not sure if I get the motivation here. If one needs to do this regularization for edges, why don't we consider this auxiliary loss for node embeddings as well? As in molecules, atoms have more interesting features than bonds, which should account more if the mutual information loss is needed. <sep> Regarding experiment <sep> 1. In Figure 1, the training loss of EIGNN is better than MPNN. This is a bit counterintuitive to me, as I think the auxiliary is a kind of regularization -- which might help with generalization but not necessarily the training loss. <sep> 2. The original paper of MPNN reports the relative MAE. Is it possible to report the results using the same metric as previous paper? It would make the comparison more consistent -- though showing the improvement in current way is not too bad. <sep> 3. I think one simple ablation study would to concat the edge feature directly inside the 'message passing' procedure, or have some 'residual' type of connection for edge features.","This paper proposed an auxiliary loss based on mutual information for graph neural network. Such loss is to maximize the mutual information between edge representation and corresponding edge feature in GNN 'message passing' function. GNN with edge features have already been proposed in the literature. Furthermore, the reviewers think the paper needs to improve further in terms of explain more clearly the motivation and rationale behind the method."
"abstract | weakness | rebuttal_process | decision  ==> This paper proposes a meta learning approach based on data valuation for reinforcement learning tasks. The core idea is to train a second network (the data value estimator) in conjunction to a regular predictor network. The predictor is then trained with samples chosen via the data value estimation. The authors motivate this construction with the goal to filter out unreliable and corrupted data. <sep> It's well established that RL poses a difficult learning problem, and as such the goal to improve the RL process is definitely a good one. To the best of my knowledge the approach proposed here is new. The exposition of the paper is also quite clear, and all parts of the approach are explained nicely. In addition, the submission contains a thorough evaluation of the method. <sep> A central point for the method seems to be the validation data set which is used to train the data value estimator. The text emphasizes that this data set can be ""small"" several times, and the discussion and results of section 4.5 try to shed light here. However, Figure 5 indicates that a fairly large fraction of samples is needed to identify, e.g., more than 50% of the corrupted samples. <sep> Another cricital aspect for meta-learning approaches such as this one is also the training time. RL is already expensive, so if the meta learning introduces a large factor, the training could quickly become infeasible. Here, the text gives a factor of about 3, which is noticeable, but not overly big. This still seems practical. Potentially, the estimator could also be reused (at least partially) for repeated training runs. <sep> The tests in figure 2 are somewhat unituitive at first - I was expecting results of models trained on datasets with different samples being removed beforehand, rather than removing samples based on a trained estimator. However, this test makes sense on second sight, and e.g., the significant drop in performance after removing the most important samples indicates that the estimator was able to correctly identify a certain portion of data that is actually important for successful predictions. In addition to the noisy label and domain adaptation tests, this paints a positive picture. The method seems to yield useful (be it somewhat small) improvements in terms of learning performance. <sep> One aspect that could be justified better in my opinion is the choise for a discrete representation for the data value estimator. Shouldn't the method likewise work with a continuous representation here? The paper explain how the discrete model is trained with quite some detail, but the choice itself could be motivated more clearly. <sep> However, overall I think the method is interesting and yields nice performance improvements. It is described and evaluated in detail, so I think the paper could me included in the *CONF* program.","The paper suggests an RL-based approach to design a data valuation estimator. The reviewers agree that the proposed method is new and promising, but they also raised concerns about the empirical evaluations, including not comparing with other approaches of data valuation and limited ablation study. <sep> The authors provided a rebuttal to address these concerns. It improves the evaluation of one of the reviewers, but it is difficult to recommend acceptance given that we did not have a champion for this paper and the overall score is not high enough."
"abstract | weakness  ==> The paper proposed an unified model for Label Propagation (LPA) and Graph Convolutional Neural Networks (GCN). It is shown how it is possible to infer the relationship between LPA and GCN in terms of label or feature smoothing (how label/feature does propagate over the neighbors) and label or feature influence over the other nodes. The results are given in terms of two theorems (whose proofs are in an appendix) which essentially state that the total label influence of nodes with a particular label ""l"" on a specific node is proportional to the probability that node is labelled as ""l"" by LPA. In practice, LPA acts as a regularizer to learn transformation matrices and edge weights simultaneously in GCN. By means of a simple joint loss (eq.8), the regularized training show that transductive learning with the joint model surpasses GCN/GNN baselines. <sep> While the proof of the first theorem is reasonable and seems correct (Taylor+ Schwarz inequality + L-Lip), the second left me a little puzzled, in particular wrt eq 16 and 17. Is it possible to add a graphical explanation or idea of the proof? Why ""y"" has to be reset at each iteration? <sep> Unfortunately, the improvement brought by the framework is marginal, even if it seems general. <sep> The other limitation is that we have transductive training, but the authors are well aware about this.",The authors attempt to unify graph convolutional networks and label propagation and propose a model that unifies them. The reviewers liked the idea but felt that more extensive experiments are needed. The impact of labels needs to be specially studied more in-depth.
"abstract | rebuttal_process  ==>  ==> The authors propose a framework where one component is an attacker network that keeps learning about how to perturb the loss more, and one component is a defense network that robustify learning with respect to the attacker network. The framework is flexible on how the attacker network can be trained, and advances over previous works where the attacker is a human-designed algorithm rather than a learning model. Experiment results show that the framework reaches superior defense performance. The authors also extend the framework to help imitation learning. <sep> Overall the paper is a pleasure to read. My questions/suggestions are <sep> (1) Given that the framework seems natural in design, a deeper contribution would be talking about how to successfully train the framework in practice. The authors talk about the connection of the framework to GAN, and the latter is not that easy to train. However, we see very little information on how to train the framework in the paper. Was it super easy to train the framework (why?), or did the author encounter any difficulties? Are there important heuristics that help train the framework successfully? <sep> (2) While the framework leads to a better defense mechanism (the authors' goal), one could wonder whether it leads to a better attacker as well. Instead of just checking the differences of the attacking examples generated, can we take the inner attacker and see if it is more effective in attacking than PGM and CW? How does the goodness of the attacker improve with time? Do different L2L variants generate attackers with different quality? Do the quality connect with the defense performance? <sep> (3) Section 4 looks distracting to me. It is good to know that the framework can be extended to imitation learning, but the section is best put at a longer version or another paper, rather than occupying a significant amount of space in the current paper. <sep> I have read the rebuttal and thank the authors for the response.","This paper considers solving the minimax formulation of adversarial training, where it proposes a new method based on a generic learning-to-learn (L2L) framework. Particularly, instead of applying the existing hand-designed algorithms for the inner problem, it learns an optimizer parametrized as a convolutional neural network. A robust classifier is learned to defense the adversarial attack generated by the learned optimizer. The idea is using L2L is sensible. However, main concerns on empirical studies remain after rebuttal."
"misc | weakness  ==> The paper studies the training process of NNs through the lens of Fourier analysis. The authors argue that during the training process, NNs will first learn low frequencies part of the function first and then the high frequency part. To verify this claim empirically, the author propose two methods: 1. examine the convergence of different frequencies in a pre-selected direction in the frequency space during training; 2. examine the convergence rate of the 2-norm of low v.s. high frequencies during training.  Through the experimental results of these two methods, the authors conclude that NNs learn the low  frequency components before the high frequency components. The authors also discuss a potential application of this observation to solving high dimensional PDEs: coupling DNNs training (good at learning low frequency components) with the Jacobi method (good at learning high frequency components). Finally, the authors also provide some theoretical intuition (Thm 1., 2.) why low frequency components are learned faster and an explanation why NNs could generalize well on images but perform poorly on tasks like learning parity functions. <sep> Other comments: <sep> 1. It seems the filtering method is a better (might be a sufficient) way to justify the F-Principle than the projection method, given the projection method examines only one direction (also appointed out in the paper). <sep> 2. When talking about Fourier transform, would you specify what is the domain of the functions and how the functions are defined (section 3.1) The notation there is somewhat confusing (which makes the rest of the paper difficult to follow) since you are mentioning the Fourier transform of the set {(x_i, y_i)}. It will be helpful to define the function before defining its Fourier transform.  Please also mention what is the domain of the function, {x_i}_i or R^d? <sep> 3. According to equation (4), it seems the domain of the functions is {x_i}_i, otherwise equation (4) should be a function of x\\in R^d, not x_i. <sep> 4. Could you elaborate why (4) is a good approximation of the low frequency energy rather than the L2 norm (over x\\in R^d) of (4) with x_i replaced by x\\in R^d. <sep> 5. It might be useful to refine the related work section. It is not clear what are the previous contributions prior to this paper, and it seems [1] shares some similar results/observation with the this paper. <sep> Overall, I lean to a weak rejection. The key findings (and similar results, e.g. NNs learn simple functions first), i.e. F-Principle seems to have already appeared in previous works, e.g. [1] and the theoretical results of this paper are limited to an idealized setting (results of more general setting appear in another work, mentioned in the paper.) <sep> [1]Nasim Rahaman, Devansh Arpit, Aristide Baratin, Felix Draxler, Min Lin, Fred A Hamprecht, <sep> Yoshua Bengio, and Aaron Courville. On the spectral bias of deep neural networks. arXiv preprint arXiv:1806.08734, 2018. 1, 8, A","Borderline decision. The idea is nice, but the theory is not completely convincing. That makes the results in this paper not be significant enough."
"abstract | weakness | rebuttal_process | decision  ==> After rebuttal: <sep> Thank you to the authors for responding to my review. <sep> 1) The title of the conference is ""... on Learning Representations"". As I stated in the review (""no, e.g., neural networks are employed""), neural networks are an *example* of, but do not subsume, all representation learning methods. Therefore, I agree that papers that do not cover neural networks are welcome at the conference. However, as I stated in the review, my evaluation of the method proposed in the submission is that it does not concern representation learning (""The employed features in Table 3 are handcrafted""). I believe this evaluation is defensible, but of course the final evaluation is up to the chairs. However, I note that the authors did not respond directly to my evaluation that the method is not engaging in representation learning. <sep> 2-7) As the other reviewer notes, the paper lacks clarity in many places, and does not sufficiently discuss prior work, including in postural control (there is one citation in the references that is not mentioned in the main text), hierarchical Bayesian optimization within or without a Gaussian processes framework (https://scholar.google.com/scholar?hl=fr&as_sdt=0%2C5&q=hierarchical+bayesian+optimization&btnG=), or experience replay (https://scholar.google.com/scholar?hl=fr&as_sdt=0%2C5&q=replay+machine+learning&btnG=). Therefore, it is difficult to ascertain the research contribution. <sep> As such, I stand by my evaluation that this submission is not ready for publication at *CONF*. <sep> =========================== <sep> Before rebuttal: <sep> The submission presents a hierarchical Bayesian optimization (HiBO) approach to solving a postural control task expressed as a proportional-derivative (PD) controller. <sep> Strengths: <sep> - The HiBO approach outperforms the non-hierarchical BO approach on the task of postural control. <sep> Weaknesses: <sep> - The paper does not make use of representation learning (no, e.g., neural networks are employed) and is therefore out-of-place at *CONF*. The employed features in Table 3 are handcrafted. <sep> - The task (simulating human postural control) is not well-situated in the context of prior work using HiBO for robotics, so the contribution remains unclear. <sep> - It is not clear why this problem should be formulated as contextual policy search (i.e., to what the context variable refers). <sep> - Only one baseline (Bayesian optimization (BO)) is reported. This baseline corresponds to the ablation of the HiBO method (i.e., the omission of the context variable), and so does not represent, more broadly, an alternative approach. <sep> - The concept of ""mental replay"" is briefly introduced, but no reference is made to prior work in imagined rollouts, and no ablation study on the impact of this component is performed. <sep> Minor points: <sep> - It is unclear why the problem setting should be labeled as ""psychological"" postural control. <sep> - There are several missing references (""?"") in the text.","The paper proposes hierarchical Bayesian optimization (HiBO) for learning control policies from a small number of environment interaction and applies it to the postural control of a humanoid. Both reviewers raised issues with the clarity of presentation, as well as contribution and overall fit to this venue. The authors' response helped to clarify these issues only marginally. Therefore, primarily due to lack of clarity, I recommend rejecting this paper, but encourage the authors to improve the presentation as per the reviewers' suggestions and resubmitting."
"abstract | misc | weakness | decision  ==> This paper proposed a model compression method: Falcon and rank-k Falcon. Both are used to compress CNN type of models by replacing standard convolution layer with a compact Falcon or rank-k Falcon layer to compress the model. Falcon's main idea is to decompose the traditional convolution kernel K into two smaller tensors, one is depthwise convolution kernel D and pointwise convolution kernel P. And DP will reconstruct the original kernel K. Since D+P's memory is  D*D*M+N*M which is smaller than the original size D*D*M*N, and thus when N is large, the memory saving could be large. The paper is in general in good writing and very easy to read. <sep> Below I have several concerns/suggestions for this paper: <sep> 1: Novelty. What is the main difference between this method and all the other tensor decomposition based methods for CNN compression? There are so many tensor decomposition based methods for CNN, and seems Falcon belongs to one of them. The one (maybe) special for Falcon is that it only decomposes along one dimension. Why this method could perform better than other tensor based decomposition methods(some of them are having even smaller memory footprint as they decompose more dimensions) or Falcon could be one special case of it? <sep> 2: In Section 3.3 and 3.4, the proposed Falcon and rank-k Falcon seems is fully recovering the original K, see equations above Theorem 2 and above Section 3.5, should it be minimizing the reconstruction error as other tensor decomposition methods? And how to find the solutions P and D from K? how the model is getting trained if using Falcon or rank-k Falcon? Do you have retrain step after the decomposition of K? <sep> 3: In the experiment, no any other standard compression techniques such as quantization, low-rank, weight-sharing, sparse, etc are compared.  This makes us curious about the benefit of the proposed methods over other methods. <sep> In summary, I am mostly worried about the novelty of the paper, and wondering how the model is getting trained, and the comparison with other compression techniques.","The submission presents an approach to accelerating convolutional networks. The framework is related to depthwise separable convolutions. The reviews are split. R3 expresses concerns about the experimental evaluation and results. The AC agrees with these concerns. The AC also notes that the submission is 10 pages long. Taking all factors into account, the AC recommends against accepting the paper."
"abstract | strength | weakness | rebuttal_process | weakness | decision  ==> UPDATE: bumping up my score after the revisions <sep> --- <sep> Nice connections but novelty and practical takeaways unclear <sep> SUMMARY OF THE PAPER: <sep> This paper views recent IWAE-based [1] methods (IWAE-STL [2], IWAE-DREG [3], RWS [4, 5]) for training generative models p and inference networks q under a common framework, AISLE. <sep> This heavily relies on the ""double-reparameterization"" property by [2] and is restated in Lemma 1. <sep> This framework makes it explicit that we're interested in <sep> 1) maximizing the (log) marginal likelihood wrt p parameters, and <sep> 2) minimizing some divergence between the posterior in the learned model to q. <sep> In AISLE, IWAE-STL's q-gradient is viewed as a doubly-reparameterized self-normalized importance sampling (SNIS) estimate of KL(p || q). <sep> This is in contrast to viewing it as a biased estimator of the IWAE's q-gradient. <sep> This can potentially explain why it performs well when number of SNIS samples are increased. <sep> It is also some evidence against the fact that having no unified objective is bad (because there isn't evidence of IWAE-STL diverging despite there being no unified objective). <sep> IWAE-DREG's q-gradient is viewed as a doubly-reparameterized SNIS estimate of X-divergence(p || q) (up to multiplicative constant of the number of SNIS samples). <sep> This is in contrast to viewing it as an unbiased estimator of the IWAE's q-gradient. <sep> The view on RWS is unchanged: the q-gradient is a SNIS estimate of KL(p || q). <sep> For me, the main contribution is viewing IWAE-STL and IWAE-DREG q-gradient estimators as biased gradients of an explicit divergence rather than of the IWAE objective. <sep> I also found the observation that the signal-to-noise (SNR) decrease in IWAE's q-gradient can be proved by noting that it is a SNIS estimator of a zero vector nice. <sep> STRUCTURE: <sep> The article is well-written and easy to understand. <sep> NOVELTY: <sep> A different view on IWAE-STL and IWAE-DREG is interesting and novel (as mentioned above). <sep> This means that IWAE-STL and IWAE-DREG are good not only because they reduce gradient variance (as previously understood) but also potentially because they directly target a divergence. <sep> Viewing generalization of RWS as a main contribution (first bulletpoint of Section 1.2: ""...we show that AISLE admits RWS as a special case."") is a bit of a stretch since this generalization is very straightforward from the way RWS is formulated. <sep> The recommendation of using RWS-style algorithms over IWAE as given in the abstract (""we argue that directly optimising the proposal distribution in importance sampling as in the RWS algorithm is preferable to optimising IWAE-type multi-sample objectives) is also not novel since this is also advocated by [5] (section 3.2: ""This makes RWS a preferable option to IWAE for learning inference networks because the phi updates in RWS directly target minimization of the expected KL divergences from the true to approximate posterior""). <sep> The recommendation as a method for non-reparameterisable latent variables at the end of section 1.2 (""as well as further algorithms which do not require reparameterisations"") is also given in [5]. <sep> Are there different adaptive importance sampling algorithms that could be used within AISLE that would improve on IWAE-STL/IWAE-DREG/RWS? <sep> EXPERIMENTS: <sep> There are no experiments in the main paper. <sep> However, experiments that would support/falsify the following points could be good: <sep> - RWS and IWAE-STL don't suffer from non-unified objectives because IWAE-STL has non-unified objectives but doesn't diverge, <sep> - [targeting direct divergence] is more useful than (or as useful as) [lower variance gradient estimators]. <sep> CONCLUSION: <sep> While I really like the presentation and connections made in the paper, I'm not sure what the practical takeaways are (other than use IWAE-STL, IWAE-DREG, RWS over IWAE which is advocated by [2], [3], [5]). <sep> I'm giving this a weak accept due to the former. <sep> I'm willing to bump up my score if <sep> - the paper is modified to more accurately reflect the contributions or <sep> - there are experiments that provide additional support for the [targeting direct divergence] view in addition to [2, 3, 4, 5], or <sep> - there is a new practical algorithm that the AISLE generalization would suggest that is better than IWAE-STL, IWAE-DREG, RWS in some respects. <sep> [1] Importance Weighted Autoencoders. https://arxiv.org/abs/1509.00519 <sep> [2] Sticking the Landing: Simple, Lower-Variance Gradient Estimators for Variational Inference. https://arxiv.org/abs/1703.09194 <sep> [3] Doubly Reparameterized Gradient Estimators for Monte Carlo Objectives. https://arxiv.org/abs/1810.04152 <sep> [4] Reweighted Wake-Sleep. https://arxiv.org/abs/1406.2751 <sep> [5] Revisiting Reweighted Wake-Sleep for Models with Stochastic Control Flow. https://arxiv.org/abs/1805.10469 <sep> [6] Variational Inference via χ-Upper Bound Minimization. https://arxiv.org/abs/1611.00328 <sep> [7] Tighter Variational Bounds are Not Necessarily Better. https://arxiv.org/abs/1802.04537","The authors argue that directly optimizing the IS proposal distribution as in RWS is preferable to optimizing the IWAE multi-sample objective. They formalize this with an adaptive IS framework, AISLE, that generalizes RWS, IWAE-STL and IWAE-DREG. <sep> Generally reviewers found the paper to be well-written and the connections drawn in this paper interesting. However, all reviewers raised concerns about the lack of experiments (Reviewer 3 suggested several experiments that could be done to clarify remaining questions) and practical takeaways. <sep> The authors responded by explaining that ""the main ""practical"" takeaway from our work is the following: If one is interested in the bias-reduction potential offered by IWAEs over plain VAEs then the adaptive importance-sampling framework appears to be a better starting point for designing new algorithms than the specific multi-sample objective used by IWAE. This is because the former retains all of the benefits of the latter without inheriting its drawbacks."" I did not find this argument convincing as a primary advantage of variational approaches over WS is that the variational approach optimizes a unified objective. At least in principle, this is a serious drawback of the WS approaches. Experiments and/or a discussion of this is warranted. <sep> This paper is borderline, and unfortunately, due to the high number of quality submissions this year, I have to recommend rejection at this point."
"abstract | rating_summary | weakness | decision  ==> After responses: <sep> I read the authors response and decided to stick to my original score mostly because: <sep> 1 - I understand that interpretability is hard to define. I also agree with the authors response. However, this is still not reflected in the paper in any way. I expect a discussion on what is the relevant definition used in the paper and how does it fit to that definition. Currently, it is very confusing to the reader. <sep> 2 - I understand the authors' response that few-shot learning is a different empirical setting. However, authors also agree that settings are some-what relevant. I really do not see any gain by NOT discussing the few-shot learning literature. At the end, a reader is interested in this work if they have limited data. Moreover, other ways to address limited data issue should be discussed. <sep> ----- <sep> The manuscript is proposing a few-shot classification setting in which training set includes only few examples. The main contribution is using prototype embeddings and representing each word as cosine distances to these prototype embeddings. Moreover, the final classification is weighted summation of the per-token decisions followed by a soft-max. Per-token classifiers are obtained with an MLP using the cosine distances as features. When the relevance labels are available, they are used in training to boost gradients. <sep> PRO(s) <sep> The proposed method is interesting and addressing an important problem. There are many few-shot scenarios and finding good models for them is impactful. <sep> The results are promising and the proposed method is more interpretable than the existing NLP classifiers. I disagree with the claim that the model is interpretable. However, I appreciate the effort to interpret the model. <sep> CON(s) <sep> The model is not interpretable because 1) it starts with embeddings and they are not interpretable, 2) model is full of non-linearities and decision boundaries are not possible to find. In other words, it is not possible to answer ""what would make this model predict some other classifier"". <sep> The authors should discuss the existing few-shot learning mechanisms. Especially, ""Prototypical Networks for Few-shot Learning"" is very relevant. I also think it can be included as a baseline with very minimal modifications. <sep> The writing is not complete. The authors do not even discuss how the prototypes are learned. I am assuming it is done using full gradient-descent over all parameters. However, this is not clearly discussed. Implementation details should be discussed more clearly. <sep> SUMMARY <sep> I believe the manuscript is definitely interesting and has a potential. In the mean time, It is not ready for publication. It needs a through review of few-shot learning. Authors should also discuss can any of the few-shot learning methods be included in the experimental study. If the answer is yes, it should be. If the answer is no, it should be explained clearly. <sep> Although my recommendation is weak-reject, I am happy to bump it up if these issues are addressed.","The authors focus on low-resource text classifications tasks augmented with ""rationales"". They propose a new technique that improves performance over existing approaches and that allows human inspection of the learned weights. <sep> Although the reviewers did not find any major faults with the paper, they were in consensus that the paper should be rejected at this time. Generally, the reviewers' reservations were in terms of novelty and extent of technical contribution. <sep> Given the large number of submissions this year, I am recommending rejection for this paper."
"abstract | weakness  ==>  ==> The paper makes the following two contributions: 1) a new metric to measure the realism of uncertainty estimates for regression problems which uses a Mahalanobis distance-based statistical test. 2) a new probabilistic architecture for semantic segmentation. <sep> Overall I do not think that the paper is qualifies for acceptance, because a) both contributions are only loosely connected and b) some parts are confusingly written or poorly motivated, making the paper hard to follow. <sep> After reading the paper it still remains unclear to me why the proposed statistical test is superior to other popular metrics, such as the log-likelihood or the metrics proposed by Mukhoti and Gal. I think the paper would benefit from a more detailed discussion that highlights the differences between the proposed metric and other commonly used metrics. <sep> Furthermore, in the experiments, the paper only shows that MC dropout doesn't achieve realistic uncertainty estimates. I think concluding that variational approaches underestimate the variance is a bit of stretch (see section 4.1) , i e. it would be more convincing if other approaches (e.g Blundell et al. Hernandes-Lobato and Adams) are also considered. <sep> The paper also just states that uncertainty estimates obtained by MC dropout are unrealistic but doesn't elaborates how to improve them. <sep> The second contribution, a probabilistic architecture for semantic segmentation, is not introduced in the main paper. Instead details are only provided in the appendix. In my opinion the paper would be easier to follow if it would contain a section that motivates and described the proposed architecture before jumping directly to the experiments. <sep> Minor comments: <sep> - the acronym FRRN is not defined <sep> - Section 4.2 last paragraph: I don't understand why samples obtain by MC dropout and from the latent space are considered to be from the same distribution? While both methods approximate the weight posterior they use different approximations for that. <sep> - In the introduction the paper states that the proposed segmentation network is a U-net like FRRN architecture , however in section 3 it says instead of a U-net based architecture a FRRN based architecture is used. This is somewhat confusing. <sep> - Section 4.1: how are the variances scaled for Figure 3 middle? <sep> - Section 4.1 It would be also interesting if other metrics, such as log-likelihood or RMSE, to see how well the model is able to fit the data. <sep> - I am also surprised that CVAE + MC underperforms to just using CVAE? <sep> - Section 4.1: how are the variances scaled for Figure 3 middle? <sep> References: <sep> Evaluating Bayesian Deep Learning Methods for Semantic Segmentation <sep> Jishnu Mukhoti, Yarin Gal arXiv:1811.12709 [cs.CV] <sep> Weight Uncertainty in Neural Networks <sep> Charles Blundell, Julien Cornebise, Koray Kavukcuoglu, Daan Wierstra <sep> ICML 2015 <sep> Hernández-Lobato J. M. and Adams R. <sep> Probabilistic Backpropagation for Scalable Learning of Bayesian Neural Networks, <sep> ICML 2015","This paper proposes two contributions to improve uncertainty in deep learning. The first is a Mahalanobis distance based statistical test and the second a model architecture. Unfortunately, the reviewers found the message of the paper somewhat confusing and particularly didn't understand the connection between these two contributions. A major question from the reviewers is why the proposed statistical test is better than using a proper scoring rule such as negative log likelihood. Some empirical justification of this should be presented."
"abstract | rating_summary | weakness | decision  ==>  ==> This paper provides a formal proof that the data produced by a GAN are concentrated vectors. The proof is rather intuitive: a GAN essentially consists of a Lipschitz function that takes as input a latent vector drawn from a Gaussian distribution and therefore one can use Lipschitz concentration bounds to prove that the transformed output satisfies a concentration property. Overall, I think this is a very intuitive idea and I fail to see the added value of such an observation. I provide detailed feedback below, I would really like to get a sensible answer regarding the novelty aspect. <sep> Gaussian assumption <sep> It is of course very common to sample the latent vectors from a Gaussian distribution, although one could potentially use a different distribution (uniform, gamma, …). To what extent could these results extend to other distributions? <sep> Prior work <sep> The paper does not appropriately discuss prior work that impose a Lipschitz constraint while training, see e.g.: <sep> Gulrajani, Ishaan, et al. ""Improved training of wasserstein gans."" Advances in neural information processing systems. 2017. <sep> Roth, Kevin, et al. ""Stabilizing training of generative adversarial networks through regularization."" Advances in neural information processing systems. 2017. <sep> Theorem 3.3 <sep> I would like further clarifications regarding this theorem. This theorem is essentially a concentration bound for the resolvent of the gram matrix G around its mean. The expectation is already computed in (Louart & Couillet, 2019) so the contribution in this theorem seems to be in showing that the result only depends on the first and second order statistics. You claim this is a surprising result, although it does not seem so surprising to me given that this is an asymptotic result. Could you comment on deriving a non-asymptotic result instead? <sep> Added value <sep> As mentioned earlier, I do not see what particular insight is this paper bringing. There are some obvious connections that one could make, e.g. implications such as robustness to adversarial samples depending on the Lipschitz constant, or perhaps improve generalization property, but some of these connections are already made in prior work (see comment above). What particular insight do we get from your analysis? <sep> Experiments <sep> I think a more detailed study regarding the effect of various regularization schemes would be valuable. One could for instance compare various networks with/without batchnorm, resnet connections, ...","The paper theoretically shows that the data (embedded by representations learned by GANs) are essentially the same as a high dimensional Gaussian mixture. The result is based on a recent result from random matrix theory on the covariance matrix of data, which the authors extend to a theorem on the Gram matrix of the data. The authors also provide a small experiment comparing the spectrum and principle 2D subspace of BigGAN and Gaussian mixtures, demonstrating that their theorem applies in practice. <sep> Two of the reviews (with confident reviewers) were quite negative about the contributions of the paper, and the reviewers unfortunately did not participate in the discussion period. <sep> Overall, the paper seems solid, but the reviews indicate that improvements are needed in the structure and presentation of the theoretical results. Given the large number of submissions at *CONF* this year, the paper in its current form does not pass the quality threshold for acceptance."
"abstract | strength | weakness | rebuttal_process | decision  ==>  ==> UPDATE: <sep> I acknowledge that I've read the author responses as well as other reviews. <sep> After reading, I would keep my rating at 3 (Weak Reject), since the key reasons for my rating still hold. <sep> #################### <sep> This work makes a connection between recently introduced one-class neural networks [8, 4] and the unsupervised approximation of the binary classifier risk under the hinge loss [1]. An explicit expression of this risk approximation is derived for the case that the prior class probabilities are known and that the class-conditional distributions of classification scores are Gaussian. This solution is then used to formulate an end-to-end differentiable loss for unsupervised binary classification which is combined with a posterior class probability regularizer to avoid trivial solutions. Finally, the paper presents an experimental evaluation on synthetic data, the Wisconsin Breast Cancer dataset, four NLP tasks, as well as on the anomaly detection task on MNIST where the proposed method slightly outperforms the two existing one-class networks [8, 4]. <sep> I think this paper makes an interesting, original connection between unsupervised-supervised risk estimation and one-class neural networks which provides a principled motivation for existing methods [8, 4] and also hints to potential flaws in their formulations, namely that OC-NN [4] and soft-boundary Deep SVDD [8] make no use of positive samples during learning (as illustrated in Figure 2). The paper is not yet ready for acceptance in my opinion, however, due to the following key reasons: <sep> (i) The experimental evaluation is not convincing and not sufficient to assess the significance of results; <sep> (ii) Though making this connection is interesting, the theoretical derivations presented in the paper are rather straightforward. <sep> (i) I think the experimental evaluation is the weakest part of the paper at the moment which I find not convincing due to the lack of competitors, the use of rather simple datasets, and missing experimental details. The synthetic experiment only serves as a sanity check not giving any additional insights. As the proposed unsupervised method approximates the risk of a supervised binary classifier, I agree that it makes sense to compare to the supervised ""gold standard"" on binary classification tasks (Wisconsin and NLP sentiment tasks) to infer the unsupervised-supervised performance gap. However, there is no comparison to other unsupervised competitors (OC-SVM, GMM, Deep SVDD, OC-NN, etc.) to put the performance of the proposed method in these experiments into perspective (only K-Means does not establish a strong baseline). Those classification tasks further are not really a convincing use case in my mind since labels here are usually available. In contrast, I find anomaly detection to be an important application of this method, but an evaluation solely on MNIST that also lacks recent deep competitors [6] is not sufficient to assess the significance of the presented results. Moreover, from the text it seems that only the hyperparameters of the proposed method are tuned on some validation set that includes positive as well as negative samples which would be an unfair advantage and might explain the slight edge in performance. Finally, many experimental details are not reported: (ia) Are the networks used randomly initialized or pretrained? (ib) How are prior class probabilities set? (ic) What are the batch sizes (relevant for quantile estimation) (id) What score are the hyperparameters tuned on? (ie) Are negative samples in the validation set from all the anomaly classes? <sep> (ii) The technical derivations in the paper (and the appendix) are correct but rather straightforward. The theoretical heavy-lifting is from Balasubramanian et al. [1] and this paper presents an explicit solution for the risk approximation under the assumption that the prior class probabilities are known and the class-conditional score distributions are Gaussian. I do not want to discount that making this connection and loss derivation may lead to significant results (which is left to be demonstrated experimentally), but I find the current theoretical contribution on its own not sufficient. Parts of the theoretical Section 3 could also be greatly cut in my opinion (no need to define a quantile or sample mean and variance etc.). Finally, one key property of the loss expressed in the paper is its differentiability and use with autograd, but this does not hold after adding the posterior regularization term which is based on the empirical p0-quantile, correct? (the gradient of the quantile is zero almost everywhere due to the argmin) <sep> Apart from the two key points above, the presentation of the paper is unpolished (nested lists in the main text, etc.) and major deep anomaly detection related work [10, 6, 5, 7, 3] is missing. <sep> #################### <sep> *Additional Feedback* <sep> *Positive Highlights* <sep> 1. The paper makes an interesting connection between unsupervised-supervised risk approximation [1] and recently introduced one-class neural networks [8, 4] that provides a principled motivation and points to potential flaws in existing formulations. <sep> 2. I think the question how to learn neural classifiers in an unsupervised manner is original and interesting. <sep> 3. The technical derivations in the paper are rigorous and correct. <sep> *Ideas for Improvement* <sep> 4. Make a comparison to state-of-the-art unsupervised deep anomaly detection (AD) methods. <sep> 5. Run AD experiments on more complex datasets like Fashion-MNIST, CIFAR-10, and the recently introduced MVTec [2]. <sep> 6. Include major deep AD works [10, 6, 5, 7, 3] into the related work. <sep> 7. Compress Sections 1–3 (fewer lists; no need to give definitions of a quantile, sample mean and variance; etc.) <sep> 8. Motivate the non-AD experiments. Currently these appear rather constructed artificially. <sep> 9. In the NLP tasks, make a comparison to text-specific one-class classifiers [9]. <sep> 10. Add a sensitivity analysis w.r.t. the prior class probability p0 to infer robustness to this parameter that seems crucial. <sep> 11. Provide guidance how to select p0 in a particular application. <sep> 12. Consistently report performance metrics with standard deviations in your tables to allow to infer statistical significance. <sep> 13. What to do if there are no negative, but only normal samples as in fully unsupervised AD? Nevertheless make an assumption on the class prior? <sep> *Minor comments* <sep> 14. Unordered lists should be used sparsely in a main text, stylistically speaking. Avoid nesting as in the introduction. <sep> 15. Section 2.1, first sentence (and elsewhere): ""Let be given ..."" is grammatically wrong. Correct would be either ""Let f be a binary linear classifier ..."" or ""Given a binary linear classifier f ..."". <sep> 16. In Section 2.1, index the classifier f with parameter θ, i.e. fθ. Otherwise the risk optimization parameter θ does not even appear on the right hand side of the risk Eq. (1). <sep> 17. Combine Eqs. (1) and (2) into one equation. <sep> 18. Consistently enumerate equations throughout the paper or do not enumerate at all. <sep> 19. In Eq. (3) index i misses in the sum. <sep> 20. A subsection title following a section title directly is bad style. A new major section should at least be introduced with a few sentences on what this section is about. <sep> 21. Mention that erf is the Gaussian error function. <sep> 22. Center the equation in Section 3.2. <sep> 23. Plots is Figure 3 are rather poorly formatted: use thicker lines and more distinctive colors; place the legend legibly. <sep> 24. Show the average with confidence intervals over the 10 runs in Figure 3. <sep> 25. Put results, as in Section 4.2 on the Wisconsin Breast Cancer dataset rather in a table. <sep> #################### <sep> *References* <sep> [1] K. Balasubramanian, P. Donmez, and G. Lebanon. Unsupervised supervised learning ii: Margin-based classification without labels. Journal of Machine Learning Research, 12(Nov):3119–3145, 2011. <sep> [2] P. Bergmann, M. Fauser, D. Sattlegger, and C. Steger. Mvtec ad–a comprehensive real-world dataset for unsupervised anomaly detection. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 9592–9600, 2019. <sep> [3] R. Chalapathy and S. Chawla. Deep learning for anomaly detection: A survey. arXiv preprint arXiv:1901.03407, 2019. <sep> [4] R. Chalapathy, A. K. Menon, and S. Chawla. Anomaly detection using one-class neural networks. arXiv preprint arXiv:1802.06360, 2018. <sep> [5] H. Choi, E. Jang, and A. A. Alemi. Waic, but why? generative ensembles for robust anomaly detection. arXiv preprint arXiv:1810.01392, 2018. <sep> [6] I. Golan and R. El-Yaniv. Deep anomaly detection using geometric transformations. In NIPS, 2018. <sep> [7] D. Hendrycks, M. Mazeika, and T. G. Dietterich. Deep anomaly detection with outlier exposure. In *CONF*, 2019. <sep> [8] L. Ruff, R. A. Vandermeulen, N. Görnitz, L. Deecke, S. A. Siddiqui, A. Binder, E. Müller, and M. Kloft. Deep one-class classification. In International Conference on Machine Learning, pages 4393–4402, 2018. <sep> [9] L. Ruff, Y. Zemlyanskiy, R. Vandermeulen, T. Schnake, and M. Kloft. Self-attentive, multi-context one-class classification for unsupervised anomaly detection on text. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4061–4071, 2019. <sep> [10] T. Schlegl, P. Seeböck, S. M. Waldstein, U. Schmidt-Erfurth, and G. Langs. Unsupervised anomaly detection with generative adversarial networks to guide marker discovery. In Proceedings International Conference on Information Processing in Medical Imaging, pages 146–157. Springer, 2017.","This paper makes a connection between one-class neural networks and the unsupervised approximation of the binary classifier risk under the hinge loss. An important contribution of the paper is the algorithm to train a binary classifier without supervision by using class prior and the hypothesis that class conditional classifier scores have normal distribution. The technical contribution of the paper is novel and brings an increased understanding into one-class neural networks. The equations and the modeling present in the paper are sound and the paper is well-written. <sep> However, in its current form, as pointed out by the reviewers, the experimental section is rather weak and can be substantially improved by adding extra experiments as suggested by reviewers #1, #2. Since its submission the paper has not yet been updated to incorporate these comments. Thus, for now, I recommend rejection of this paper, however on improvements I'm sure it can be a good contribution in other conferences."
"abstract | weakness | misc  ==> This paper presents a novel method for unsupervised post-processing of pretrained word embeddings that enforces the distributional word vector space to be more isotropic, which in turn improves the expressiveness and quality of the space in terms of similarity. The method is based on the shrinkage of the covariance/Gram matrix and its effects on the input space are evaluated across a range of intrinsic evaluation tasks. While I like the idea overall and this line of work in general, there are still some concerns with the current version of the paper: <sep> * Overall, although its design seems more principled, the proposed method does not seem significantly better than the previous (very similar) method of Mu et al. I would like to see more evidence in the favour of the proposed method, pointing that we should use that instead of Mu's method. Also, the gains over non-processed spaces often seem insignificant, and offer only small benefits. <sep> * The derivation of the method seems too verbose, especially in light of the fact that it is directly inspired by previous work on the shrinkage estimation of covariance matrices and CKA. I would suggest the authors to spend more time on linking their high-level hypotheses to the low-level mathematical implementations instead of flooding the paper with equations - for the interested reader a lot of the derivation process can be put into an appendix, the paper should focus on conveying the key principles instead. This would also offer additional space for more experiments. <sep> * One very relevant paper is not mentioned at all: https://arxiv.org/pdf/1809.02094.pdf (Artetxe et al., CoNLL 2018). I would suggest the authors to cite that work and ideally even compare to it on their set of intrinsic tasks (e.g., word similarity, word analogy), and then discuss the difference in results and their approach to unsupervised post-processing. This shouldn't be so difficult to do as the code from that paper is available online: https://github.com/artetxem/uncovec <sep> * The paper mixes true similarity datasets (such as SimLex) with broader semantic relatedness datasets (MEN, WordSim-353), even mixing true semantic similarity of the same dataset (WordSim353-SIM) with its relatedness subset (WordSim353-REL). In light of the known conceptual differences between the relations of similarity versus relatedness, I would suggest to report the results separately for the two tasks. For instance, another true similarity dataset, which is not used in the evaluation is SimVerb-3500. Along the same line, it is also not clear what type of similarity is meant when the authors state that through post-processing 'the similarity between words can be better expressed'. What does it mean to better express the similarity between words in the first place? Do we talk about true similarity or relatedness or both? However, the two relationships between words support different classes of downstream applications, so therefore it is even more problematic 1) not to distinguish between the two and 2) not to report any results in any downstream (extrinsic) tasks where the post-processed embeddings are used as features (STS is still a semi-intrinsic task imho; BLI is considered as an intrinsic task in cross-lingual settings). <sep> * One of the key reasons to apply post-processing is to mitigate the frequency artefacts: however, such an evaluation that goes towards that direction is never executed. For instance, I would like to see a focused experiment that measures how post-processing affects high-frequency versus mid-to-lower frequency or rare words. A recently developed CARD-660 dataset might be used to this end. <sep> * It is not clear how exactly the authors run Mu et al.'s method, that is, how many top principal components are removed for the input vectors? How is this selected? Are always the optimal results reported for the baseline Mu et al.'s method? Why not reporting the results with removing 2 and 3 at the same time to further prove the point that their method is non-automatic? This would also give the reader a hint how much the results with Mu's method actually differ/decrease if one just decides to make the method 'automatic' by just fixing the method to always remove the same number of top principal components. <sep> Minor remarks: <sep> * The title of the paper is a bit imprecise: in the word embedding literature, the term post-processing is often referred to the methods that fine-tune word embeddings using some external knowledge after (i.e., post) the initial distributional training (e.g., the so-called retrofitting methods). However, in the context of the paper post-processing actually refers to some unsupervised post-training steps on the input space without injecting any external information. This should be made clearer in the paper, and perhaps adding a paragraph which outlines the core difference to other work on retrofitting would be helpful as well. <sep> * I might be missing something while reading Section 3, but it is currently not clear to me how the oracle Gram matrix K is obtained in the first place. Perhaps it makes sense to briefly summarize this in a quite direct way to avoid the reader's confusion? <sep> * It is great to see a summary of the key post-processing steps at the very end of Section 3; this is really helpful for everyone who would like to try out the proposed method off-the-shelf. However, the summary is not self-contained as it is not clear what \\mathcal{L}'' refers to (and the reader must search through the derivations again to find its meaning). <sep> * I like the evaluation on word translation, and I believe that the proposed post-processing methods could actually improve word translation through some pre-alignment perturbations. It is a pity that the method is not evaluated on more distant language pairs, as I believe that the method might have much more effect there than on the already-saturated EN-to-ES/FR/IT bilingual lexicon induction tasks. <sep> * For de-en word translation Mu's method actually beats the proposed method (incorrect number in bold) <sep> * It is not clear why MUSE is used for word retrieval experiments, given the fact that it is known to be unstable (Sogaard et al., ACL 2019), and there are more robust and more effective methods available such as VecMap (Artetxe et al., ACL 2018) or RCSLS (Joulin et al., EMNLP 2019)","This paper explores a post-processing method for word vectors to ""smooth the spectrum,"" and show improvements on some downstream tasks. <sep> Reviewers had some questions about the strength of the results, and the results on words of differing frequency. The reviewers also have comments on the clarity of the paper, as well as the exposition of some of the methods. <sep> Also, for future submissions to *CONF* and other such conferences, it is more typical to address the authors comments in a direct response rather than to make changes to the document without summarizing and pointing reviewers to these changes. Without direction about what was changed or where to look, there is a lot of burden being placed on the reviewers to find your responses to their comments."
"abstract | weakness | rebuttal_process | weakness | rebuttal_process | weakness | decision  ==>  ==> Summary <sep> This paper proposed a new graph pooling method based on the Haar basis on graphs. The authors argued that existing graph pooling methods have drawbacks (ignored node features, ignored the hierarchical structure of a graph or, computationally expensive in terms of space or time) and that the proposed method overcame these drawbacks. The proposed method extracts the low-frequency components of signals in terms of Haar decomposition on a chain of graphs. By the property of the Haar basis, we can compute the pooling matrix with the complexity proportional to the graph size. The paper empirically compared the proposed method with state-of-the-art pooling methods and graph NNs using well-known datasets. <sep> Decision <sep> Although this paper gave a novel pooling method by incorporating the Wavelet theory, I still have questions for the effectiveness of the proposed method in the real datasets and whether the proposed method solved the problems of existing methods the authors mentioned in the introduction (see the Suggestion section). Besides, I think the authors can improve the organization of the paper to maximize the value of the paper. Therefore, I judge the paper as a border, tending to reject for now. <sep> Suggestions <sep> - The main part of the paper is 10 pages long. I think the authors can polish the organization of the paper to fit the recommended page size (i.e., 8 pages). <sep> - I want to know how many times the authors ran experiments for each configuration.  I think the variance of test accuracies are critical since the difference in performance between methods is not significant. Adding error bars to the experiment results are preferrable if authors ran experiments multiple times. <sep> - The authors claimed in the introduction that the drawback of existing methods is their time or space complexity and that the proposed method is computationally inexpensive. I think it is better to emphasize that HaarPooling does solve the questions. Although the authors demonstrated that the proposed algorithm is fast compared to the naive implementation, the comparison with other pooling methods are missing. For example, adding a comparison table in terms of time and space complexity of pooling methods is one idea. Another idea is to demonstrate it empirically by providing time and memory consumption to the experiment results. <sep> Minor Comments <sep> - Introduction <sep> - Haar Pooling is computed following a chain... → Haar Pooling is computed by following ... <sep> - page 1, section 1, paragraph 1 <sep> - Graph classification and regression are a very different kind of task <sep> - → At first sight, I have thought that this sentence discusses the difference between classification and regression tasks. Could you reconsider the sentence? <sep> - page 3, section 2, paragraph 1 <sep> - The authors used the term ""chain,"" whose definition is available in the later section (Sec. 3). I think this terminology is not common in the literature of graph NNs (at least I did not come up with the definition from this term). Could you consider to add the definition (or brief explanation) of the term when you use it for the first time? <sep> - page 5, section 3, Chain of graph by clustering <sep> - Write what w is in the definition of a graph G=(V,E,w). <sep> - page 5, section 3.1, Orthogonality <sep> - Define l2(Gj) (I can imagine its definition, though). <sep> - page 5, section 3.1, Haar basis <sep> - For two consective layers j, j+1 ... <sep> - → I had a little difficulty in understanding this sentence. Could you reconsider the wording? <sep> -  page 13, Appendix B, (10) <sep> -  I think ""1"" in this equation is an all-one vector. It is better to write differently the all-one vector and the scalar one. <sep> -  page 5, section 3.1 and page 7, (8) <sep> -  For the node v in the j-th graph and v′ in the (j+1)-th graph, the authors use both of v∈v′ (section 3.1) and v′∈PaG(v) (equation (8)) to denote the parent-child relationship. It is better to align the notation. <sep> - page 13, Appendix B <sep> - Could you write in the appendix how to construct the compressed Haar from the full counterpart (I suppose we should choose bases corresponding to k=1)? <sep> Questions <sep> - In the paragraph starting with Locality on page 5, the paper says that the number of different values of the Haar basis is bounded by constant. I want the authors to make this sentence more precise. To be more specific, what ""the different values"" means (I imagine that it means the number of scalar values appearing as a component of some basis) and what ""constant"" means (with respect to which variable the ""constant"" is constant?) <sep> - I think that the complexity of the direct matrix product is O(N2C), where N is the graph size and C is the channel size (correct me if I am wrong). Therefore the result of Figure 5 was surprising to me because the direct matrix product takes approximately O(N3) time.  These look inconsistent to me unless the channel size C is proportional to the node size N. <sep> - Equation (5) claimed that the compressive Haar basis approximately keeps the length of an input signal. I want to know how we can justify it (theoretically or empirically).  I think that it is empirically correct for the Fourier basis that signal carries ""information"" in the low-frequency domains and that high-frequency areas are noisy (e.g., NT and Maehara (2019); Ma et al., 2019). If this is the case for Haar transform, too, I understand that equation (5) holds and gives the justification that the proposed method can extract the information from lower frequencies.","This paper presents a new graph pooling method, called HaarPooling. Based on the hierarchical HaarPooling, the graph classification problems can be solved under the graph neural network framework. <sep> One major concern of reviewers is the experiment design. Authors add a new real world dataset in revision. Another concern is computational performance. The main text did not give a comprehensive analysis and the rebuttal did not fully address these problems. <sep> Overall, this paper presents an interesting graph pooling approach for graph classification while the presentation needs further polish. Based on the reviewers' comments, I choose to reject the paper."
"abstract | weakness | decision  ==> Summary: <sep> The authors address the quantization of Generative Adversarial Networks (GANs). The paper first performs a sensitivity study for both the generator and the discriminator to quantization methods. Building upon the conclusion of this study, the authors propose a scalar quantization method (QGAN) and compress models to 1 bit of 2 bits weights and show generated images and metrics by the compressed models. <sep> Strengths of the paper: <sep> - As well stated in the introduction, the compression of GANs (in particular the generator, which is used at inference time) is of practical interest and, to the best of my knowledge, novel. This novelty can be explained by (1) the fact that it takes tome for quantization methods to percolate the entire deep learning field and/or (2) the fact that quantizing GANs has its specificities and own challenges that have not been yet addressed (this is the claim of the authors). <sep> - The sensitivity study is of interest for the community that can build upon this work. The conclusions (discriminator more sensitive than generator to quantization, quantizing both generator and discriminator helps) are sensible and interesting. <sep> Weaknesses of the paper: <sep> - The related work section could be greatly improved, thereby showing the limited novelty of the proposed method (QGAN). Indeed, the authors propose to learn the optimal scaling parameters alpha and beta. Many works perform this already and are currently missing in this section, see for instance the two recent surveys: ""A Survey on Methods and Theories of Quantized Neural Networks"", Guo, ""A Survey of Model Compression and Acceleration for Deep Neural Networks"", Cheng et al. <sep> - Results. The results are not sufficient to justify the performance of the method for two reasons. (1) First, the scale is crucial in assessing the performance of a quantization method. As an example, it is easier to quantize small ResNets on CIFAR-10 than large ResNets on ImageNet. Thus, scales enables to better discriminate between various approaches. I acknowledge that this requires large computational resources but this would greatly strengthen the paper (2) Second, GAN metrics have known shortcomings (see for instance ""How good is my GAN?"", Shmelkov et al.), so the strength of Table 2 is limited. This is in part alleviated by the authors by showing generated images (which is a good practice), but again echoing point (1), larger images would have helped assess better the quality of the quantization. <sep> Justification of rating: <sep> The authors propose a sensitivity study that is interesting for the community. However, the proposed method lacks novelty and the results are not convincing enough. I encourage the authors to pursue in this interesting direction which has important practical implications.","main summary: method for quantizing GAN <sep> discussion: <sep> reviewer 1: well-written paper, but reviewer questions novelty <sep> reviewer 2: well-written, but some details are missing in the paper as well as comparisons to related work <sep> reviewer 3: well-written and interesting topic, related work section and clarity of results could be improved <sep> recommendation: all reviewers agree paper could be improved by better comparison to related work and better clarity of presentation. Marking paper as reject."
"abstract | weakness | decision  ==>  ==> Summary: <sep> This paper generalizes the AGREL biologically plausible learning algorithm to deeper networks. It presents experiments to show that the method can scale to medium-sized tasks with only a modest slow down in training speed and essentially no change in classification performance. <sep> Major comments: <sep> This is an interesting paper which shows that a particular biologically plausible learning method can attain comparable performance to gradient descent on small and mid-size visual classification tasks. The proposed mapping to the biology is of a different flavor from many other recently proposed approaches. <sep> It is striking that the CIFAR10 network trains about as fast as the EBP network, while the CIFAR100 network trains much more slowly. This is presumably because randomly guessing the right answer out of 100 possibilities is the bottleneck. The paper could be strengthened by studying the speed of learning as a function of the number of output classes. For this approach to scale to ImageNet, with a 1000-way classification (or to our human visual recognition abilities with far more classes), this is an important scaling dimension to consider. <sep> It should be noted that other biologically plausible schemes like feedback alignment were able to solve CIFAR and other smaller image classification tasks, but struggled when applied to the larger scale ImageNet problem. The paper could be improved by pointing to this limitation of the present work, the possibility that performance could change on larger tasks, and the need to conduct larger experiments in future work. <sep> Personally I think the statement at the beginning of the introduction that only RL occurs in animals and humans is overstated. Unsupervised learning occurs in some form in critical period plasticity, and in various unrewarded statistical learning paradigms, at a minimum. <sep> I would also caution against categorical statements of biological plausibility, for instance, in saying that shared weights in convolutions are biologically implausible (bottom of pg 6). The same criticism has been leveled at gradient descent learning, and as the present submission shows, these intuitive judgements can be misleading. <sep> The distinction between RL and supervised learning is a bit blurred in the present submission because it is considering a classification setting in which exactly one out of a number of possible outputs is rewarded on each trial. This looks very similar to the supervised learning scenario, and relies on stochastic softmax-like competition to select a single output. This approach is very reasonable for mutually exclusive classification tasks, but this is a small subset of the tasks that full EBP could be applied to. <sep> The paper is fairly clear but the explanation of the algorithm could be further streamlined and condensed. Is DeepAGREL equivalent to stochastically selecting a single unit in a softmax layer and then back propagating only its error? It seems so from what I understand, and this may be a straightforward way to explain the algorithm. <sep> Typos: <sep> The text on page 7 describes MNIST performance as 99.17% but the table has 99.16%.","The paper proposes an RL-based algorithm for training neural networks that is able to match the performance of backprop on CIFAR and MNIST datasets. <sep> The reviewers generally found the algorithm and motivations interesting, but some had issue with the imprecision of the notion of ""biologically plausible"" used by the authors. One reviewer had issues with missing discussion of related work and also doubts about the meaningfulness of the experiments, since the networks were quite shallow. <sep> For this type of paper, clarity and precision of exposition is crucial in my opinion, and so I recommend rejection at this time, but encourage the authors to take the feedback into account and resubmit to a future venue."
"weakness | decision  ==> Overview: <sep> The paper propose an MMD GAN extension via using Random forest Kernel.  Instead of using Gaussian kernel on the top of the learned embeddings from the discriminator, it combines existing deep forests kernels. The theory of being differentiable is carefully studied (to prove zero measure) and the experiments are well conducted. <sep> 1.  Some important  references are missing.  One very related paper is <sep> * Li et al., Implicit Kernel Learning,  AISTATS 2019. <sep> That paper is using the same idea to learn to manipulate the random features on the top of the learned embedding.  The main difference between it and the proposed algorithm is they use MLP parameterization instead of the tree-based model.    Also, the deep forest model can be treated as a sparse neural network, does it have more advantage over Li et al., (2019)? given they use simple dense MLP.   Please at least discuss the similarity and difference in the rebuttal and update the draft correspondingly.  I would even encourage the author to empirically compare with it in the camera ready version.  It would be interesting to see which parameterization is better in this space. <sep> There are also other recent MMD GAN extensions should be cited in the discussion, such as <sep> * On gradient regularizers for MMD GANs. <sep> 2. For the theory part, based on Binkowski (2018), the gradients for the generator parameters should be biased. Could you discuss it with Theorem 2? <sep> 3. For most MMD GAN results, one important property in Li et al., (2017),  Arbel et al., (2018) and Li et al., (2019) is weak* topology.  Does the proposed Random Forest MMD GAN also has that property? In Li et al., (2019), they need some condition to ensure that, how's case in the proposed algorithm?","Reviewers raise the serious issue that the proof of Theorem 2 is plagiarized from Theorem 1 of ""Demystifying MMD GANs"" (https://arxiv.org/abs/1801.01401). With no response from the authors, this is a clear reject."
"abstract | strength | weakness | decision  ==> Originality: <sep> CCA is a generative model that learns a shared subspace based on  two (or multi) views of the data. Being generative, it might not have strong discriminative power for some downstream classification tasks. Previous approaches to infuse discriminative power into the shared subspace estimated by CCA are linear. So, this paper proposes to learn 1) non-linear 2) discriminative subspaces for CCA. The paper accomplishes this by simply adding a task specific term to the optimization objective of DeepCCA (Andrew et. al. 2013), which involves just adding a task-specific MLP on top and minimizing the associated loss-function. <sep> 1). The novelty of the proposed approach is limited. It just adds an extra term (extra neural network layer) with a corresponding weighting hyperparameter to the objective function of a previous method (DeepCCA) without much motivation. <sep> 2). The experimental setup and results are sound but some of the tasks seem contrived to show the improved performance of TOCCA methods. For instance, in the cross-view MNIST classification the authors use only projection from one view at training time and use the other view at test-time. What's the motivation for this setup? Why not split the data into train and test set by splitting observations, then train on both the views at train time and test on the held-out observations at test-time? I hope I am not missing something. <sep> 3). Similarly, for the ""Regularization for Cancer Classification"" task, it's assumed that only one view is available at test time. Why is that? What are the real-world examples of such setups? <sep> Quality: <sep> The paper is technically sound, though it is a trivial extension of a previous method. The experimental setup is somewhat contrived to show the superiority of the proposed method. <sep> Clarity: <sep> The paper is well organized and is well written in general. The supplementary material contains more results and code will be available after the review period. <sep> Significance: <sep> The paper solves an important problem by infusing discriminative power into generative subspaces learned by CCA but the results are not that important in my eyes. Since the empirical setup is a little contrived it is hard to even know whether a simple two-step approach that first estimates CCA subspace and then uses those projections in a SVM or MLP would perform comparable or better if given a fair-chance to compete.","The main contribution of this paper is the training of a supervised model jointly with deep CCA for improving the representations learned in a setting where the training data is multi-view. The claimed technical contribution is modifications to deep CCA to enable it to play nicely with the minibatch gradient-based training used for the supervised loss. Pros: This is an important problem with many applications. Cons: The novelty is minimal. Some previous work has done joint training of supervised models with CCA, and some has addressed training deep CCA in a stochastic setting. The reviewers (and I) are unconvinced that the differences from previous work are sufficient, and the paper does not carefully compare with the previous work. The contribution to the tasks may be quite significant, however, so the paper may fit in well in an application-oriented conference/journal."
"abstract | weakness | rebuttal_process | decision  ==>  ==> This proposes two techniques to replace mixed-precision arithmetic with half-precision training for a large part of the training process. In the first approach, the authors simply switch all mixed-precision operations with half-precision operations, and can achieve performances slightly lower than SOTA. In the second approach, the authors propose to dynamically switch between mixed-operations and half-precision operations during training. The authors claim that this second approach can match SOTA results while using half-precision arithmetic for more than 94% of training. <sep> Overall the paper is fairly well written, and easy to follow. The proposed techniques seem to empirically work well. However, I have a number of concerns about the paper, which explains my score. I list these concerns below. <sep> 1. The proposed approach has a number of additional hyperparameters, which makes it less likely for the algorithm to be widely used if the algorithm is very sensitive to the values of this. For very extreme values of these hyperparameters, I would expect the algorithm to start behaving quite poorly. But it would help a lot to provide some sensitivity analysis to these hyperparameters for reasonable values of these hyperparameters. <sep> 2. How much do the optimal hyperparameters (like numBatchesMP, numBatchesBF16, emaT) vary across problems? <sep> 3. How much do the above-mentioned optimal hyperparameters vary with mini batch size? <sep> 4. How are the other hyperparameters like learning rates selected? Is the learning rate tuned? <sep> 5. Are the experiments repeated multiple times? <sep> 6. It seems a bit weird to call a modification that simply uses half-precision arithmetic for most FMA operations a significant contribution of the paper, especially since it can't reach SOTA performance. <sep> 7. Algorithm 1 should be written out in a better way that shows the training loop. It is slightly confusing the way it is written up right now. <sep> Overall I think the paper would significantly benefit from a more thorough empirical evaluation. <sep> ============================= <sep> Edit after rebuttal: <sep> I thank the reviewers for their response and for running the additional experiments. However, I find the updated version of the paper to be inadequate in fully answering my concerns. While the authors have included a hyperparameter sensititivity analysis, I find the experiment to be unconvincing. Only two of the three hyperparameters are swept over a very small range of values, and the results presented are only for the first 12 epochs, while the actual model is typically trained for 90 epochs. While I appreciate the added experiment and realize that 10 days is too short a time to put in a proper sensitivity analysis, based on the current draft of the paper, I cannot recommend accepting this paper. I am however raising my score to a weak reject.","The submission proposes a dynamic approach to training a neural net which switches between half and full-precision operations while maintaining the same classifier accuracy, resulting in a speed up in training time. Empirical results show the value of the approach, and the authors have added additional sensitivity analysis by sweeping over hyperparameters. <sep> The reviewers were concerned about the novelty of the approach as well as the robustness of the claims that accuracy can be maintained even in the accelerated, dynamic regime. After discussion there were still concerns about the sensitivity analysis and the significance of the results. <sep> The recommendation is to reject the paper at this time."
"abstract | strength | weakness | strength | decision  ==> This paper proves universal approximation theorems for fully connected networks with fixed width and unbounded depth. Unlike recent results focusing on ReLU networks approximating scalar valued target functions, this paper provides universal approximation theorems for a wide range of activation functions and vector valued target functions. <sep> This paper provides a number of universal approximation theorems on different activation functions, but the central result can be stated as follows: <sep> Given input dimension n and output dimension m and activation \\rho, assume \\rho is a continuous function and there exists \\alpha \\in R such that \\rho is continuously differentiable at \\alpha and the derivative \\rho' is nonzero at \\alpha. Then, a fully connected network with layer width n+m+2 and unbounded depth can approximate any continuous function on a compact domain to arbitrary sup-norm accuracy. <sep> I would like to vote for acceptance of this paper because the authors develop nontrivial techniques that extend existing universal approximation results on width-bounded ReLU networks to essentially ""all"" other activation functions. I think this paper is well-written and well-organized; it gives a good overview of the existing results that contextualizes this paper well, briefly summarizes the main results, and then reveals the details of construction. I haven't checked the full details of the proofs in the appendix, but as far as I can tell they look correct. <sep> While I think that Section 4 reveals the proof techniques reasonably well, I believe that proofs of Propositions 4.3 and 4.7 should be covered/sketched in greater detail in the main text. For example, Theorem 4.4 builds upon Proposition 4.3 by approximating identity function locally using \\rho. The main text only reveals approximation of identity function by \\rho and defers the whole proof of Proposition 4.3 to the appendix. I think adding proof sketches for the propositions will be more helpful to the readers, and to this end, cutting down some text in the recurring paragraph ""uniform continuity preserves uniform convergence…"" should be helpful. <sep> According to Remark 4.9, it seems that the proof strategy for polynomials can also be applied to nonpolynomials. This motivates a natural question: why can't you apply the proof strategy for nonpolynomials to polynomials? I believe you can't because Proposition 4.3 relies on the existing universal approximation results which requires \\rho to be nonpolynomial (which is not mentioned in the main text). In my opinion, adding some comments on ""why nonpoly techniques can't be applied to poly activations"" would help readers better understand the proof techniques. <sep> Another question just out of curiosity: can you prove tightness of your construction, e.g., show that a square model with width n+m is NOT a universal approximator?","This article studies universal approximation with deep narrow networks, targeting the minimum width. The central contribution is described as providing results for general activation functions. The technique is described as straightforward, but robust enough to handle a variety of activation functions. The reviewers found the method elegant. The most positive position was that the article develops non trivial techniques that extend existing universal approximation results for deep narrow networks to essentially all activation functions. However, the reviewers also expressed reservations mentioning that the results could be on the incremental side, with derivations similar to previous works, and possibly of limited interest. In all, the article makes a reasonable theoretical contribution to the analysis of deep narrow neural networks. Although this is a reasonably good article, it is not good enough, given the very high acceptance bar for this year's *CONF*."
"misc | abstract | weakness | decision  ==>  ==> This paper asks whether it works to remove task-specific heads and treat classification and regression problems as span extraction, by formatting problems in such a way that a single span extraction model can be used.  This is a reasonable question to ask, and the authors performed a very large number of experiments attempting to answer this question.  The authors claim that using span extractive models instead of task-specific heads yields improved performance over separate heads. <sep> My main concern with this work is actually with something that would otherwise be a strength - the very large number of experiments.  Looking at the results tables, I come to a different conclusion from the authors: there does not appear to be a significant difference between using a single head or using multiple heads (this is still an interesting conclusion).  The numbers presented all appear to be within the bounds of random fluctuations, which are not controlled for at all with standard statistical testing methodologies.  And with the very large number of experiments, there are bound to be some that stand out.  This is especially true given the methodology used for small datasets - even if it was used by prior work, it is still not statistically sound to take the max of 20 samples from a training distribution and report the difference without any other statistical testing. <sep> As an example, look at table 3b.  This is claimed as a big win for SpExBERT when comparing the MNLI fine-tuned version.  But if you look at the other rows in the table, SpExBERT is worse than BERT by a *larger margin* than it is better in the MNLI case (contradicting the claimed result from table 2).  And this general trend is seen across the tables - the differences are small and inconsistent, making it look very much like we are just seeing random fluctuations.  This must be controlled for statistically in order to make any valid conclusions.  The one possible exception here seems to be SST.  Those results on the dev set do indeed seem to be more consistent, which is interesting, and hints at the utility of using ""positive"" and ""negative"" as natural language descriptions, as the authors claim.  It's not very convincing, however, as the test set difference is very small, and SpExBERT had more opportunities to find a good dev set number, as it had more experiments. <sep> My second major concern is with the experimental set up.  The authors want to claim that using a unified span extraction format yields superior performance to having separate heads.  But there are baselines missing to really demonstrate this claim.  The multiple head setup isn't really evaluated as a baseline in most of the experiments (e.g., using SQuAD / other QA datasets as an intermediate task in table 2, using BERT for any of the QA datasets in table 3).  So, even if the above issue of statistical testing were solved, it would still be very hard to evaluate the claim, as the proper comparisons are not present in the majority of cases. <sep> My main conclusion from reading this paper is that it does not appear to matter what head you use for these particular datasets.  This is an interesting result, though it is not the one that is claimed in the paper.  I think it would be very challenging to extend this approach to a broader set of tasks, however, as the authors suggest towards the end of the paper.  How do the authors propose handling cases where it is not feasible to put all possible outputs in the context?  This includes any generative task (including generative QA) and any kind of structured output (like parsing, tagging, etc.), or a regression task that does not lend itself well to bucketing. <sep> Minor issues: <sep> I would be careful about claiming that you've successfully captured regression in a span extraction format.  You have one regression task, and it's one where the original labels were already bucketed, so the bucketing makes sense.  I am very skeptical that this would actually work for more general regression. <sep> Re the paragraph titled ""SpEx-BERT improves on STILTs"": Note that SpExBERT requires additional data preprocessing and hand-written templates when switching between tasks, which is not necessary in the method of Phang et al.  There are always tradeoffs.  Neither using separate heads nor doing your additional processing are very hard, so I wouldn't make as big a deal out of this as you do.  If you want to talk about one of the methods using more ""human inductive bias"", or whatever, the hand-written templates in yours might actually be _more_ ""incidental supervision"" than a very small difference in model architecture.  But again, the difference here is very small, not worth noting in the paper.","(I acknowledge reading authors' recent note on decaNLP.) <sep> This paper proposes a span extraction approach (SpExBERT) to unify question answering, text classification and regression. Paper includes a significant number of experiments (including low-resource and multi-tasking experiments) on multiple benchmarks. The reviewers are concerned about lack of support on author's claims from the experimental results due to seemingly insignificant improvements and lack of analysis regarding the results. Hence, I suggest rejecting the paper."
"abstract | weakness | decision  ==> Summary: This paper proposes to augment crosslingual data with heuristic swaps using aligned translations, somewhat like what bilingual humans do in code-switching. I think this paper investigates a neat extension of the XNLI dataset, which is in fact the sort of thing it was created to enable! It also looks at SQuaD translations (but, I'd have preferred a bit more depth on one of these datasets over having both, but I understand why you made this rhetorical choice). <sep> Your augmentation extension to XNLI also uncovers a bunch of surprising results, like that code-switched utterances help models do better than monolingual ones! My main issue, if i had to find one, is that the paper doesn't try to offer (even possible) explanations for the unexpected results; maybe try to find space for more of these in a discussion section? Finally, this paper is really fun and well written, thanks for the effort! I'm going to leave a bunch of questions: it would be cool to see some in the final, but if they don't fit, you can consider them for a follow up. <sep> Questions: <sep> -Are all ""portions"" full sentences? Did performance change based on which ""portions"" you swapped?  In the human code-switching literature, there are syntactic generalizations about what gets switched. If you analyze the swapping, you could figure out which parts of the sentence (say, verb phrases v. prepositional phrases, beginning v. middle v. end, etc.) mattered more for NLI performance. I'd love to know the answer to that question! <sep> -you say this: ""The BLEU score of the translation system has little effect on a language's performance as a cross-lingual augmentor. "" Any ideas on why? <sep> -you also say this: ""for every language a XLDA approach exists that improves over the standard approach"", what a tantalizing statement! Why did that happen?! <sep> -Are there any generalizations over whether typologically similar languages are better augmentors for each other than they are for really different ones? I feel like if you could redo your XLR method (fig. 4) by adding augmentors in order from most similar to least (or vice versa), and you might find the answer to this. <sep> -for XNLI, I'd love to see if you have differences by label (maybe in an appendix?) <sep> Small Notes: <sep> -the text in fig1 should be bigger. <sep> -too many Ms and Ls, you had me chuckling at all the acronym puns! <sep> -define ""augmentor"" somewhere","The authors provide an analysis of a cross-lingual data augmentation technique which they call XLDA. This consists of replacing a segment of an input text with its translation in another language. They show that when fine-tuning, it is more beneficial to train on the cross-lingual hypotheses than on the in-language pairs, especially for low resource languages such as Greek, Turkish and Urdu. The paper explores an interesting idea however they lack comparison with other techniques such as backtranslation and XLM models, and would benefit from a wider range of tasks. I feel like this paper is more suitable for an NLP-focussed venue."
"abstract | weakness | suggestion  ==> Scheduled Sampling aims to overcome the problems that come with the discrepancy between train and test when training sequence generation models using teacher-forcing. However one of the drawbacks scheduled sampling is that is it is hard to parallelize this is simply because some of the decode input tokens aren't known beforehand and need to be inferred i.e. loss of some tokens are dependent on some previous generations. <sep> Authors introduce a method to perform parallel scheduled sampling by iteratively train using teacher forcing however after each iteration the gold references are mixed with the model predictions with a probability p, this allows parallelization as all decoder input tokens can be known beforehand. The final loss is calculated as before, conditioning on the final mixture. <sep> In theorem 2.1 Authors proof that parallel scheduled sampling can converge to scheduled sampling when P is set to 1 and the number of iterations is larger than the sentence length. <sep> In experiments over summarization, dialogue response generation and image generation authors show that the parallel scheduled sampling can achieve comparable performance to scheduled sampling with high-performance gains reaching 300 times faster and very comparable to teacher forcing. <sep> pros: <sep> The work presents a simple idea that is presented neatly with sufficient experiments. One of the outcomes of this method, that might not been stressed enough in the paper, is a neat way of doing scheduled sampling for the transformers architecture which wasn't straight forward before. <sep> Current proposals include some architecture modifications such as in Mihaylova et al. 2019 https://arxiv.org/pdf/1906.07651.pdf cons: <sep> The only drawback I can find in this paper is it is lacking content. while the idea is interesting and supported by experiments, I find the content is slightly below the amount of content in average *CONF* papers. <sep> The explanation of the proposed scheduled sampling can be much simplified. The idea is quite simple however for example I find the pseudo code in Algorithm 2 is quite hard to grasp maybe due to some typos <sep> Questions to authors: <sep> - We only see performance comparison in dialogue response generation experiments. There are other factors that can affect performance or make parallelization effective.  I wonder what are the performance gains of parallel scheduled sampling on normal scheduled sampling with respect to avg. number of tokens / sentence or batch size. <sep> - I had a hard time grasping Algorithm 2 although I understood the corresponding text could you please verify there aren't any typos. What it says is in the first iteration (k=1) scheduled sampling will be performed which doesn't make sense.","The paper proposes a parallelization approach for speeding up scheduled sampling, and show significant improvement over the original. The approach is simple and a clear improvement over vanilla schedule sampling. However, the reviewers point out that there are more recent methods to compare against or combine with, and that the paper is a bit thin on content and could have addressed this. The proposed approach may well combine well with newer techniques, but I tend to agree that this should be tested."
"abstract | rating_summary | decision | rebuttal_process | suggestion  ==>  ==> The paper proposes a 'potential flow generator' that can be seen as a regularizer for traditional GAN losses. It is based on the idea that samples flowing from one distribution to another should follow a minimum travel cost path. This regularization is expressed as an optimal transport problem with a squared Euclidean cost. Authors rely on the dynamic formulation of OT proposed by Benamou and Brenier, 2000. They propose to learn a time-dependent potential field which gradient defines the velocity fields used to drive samples from a source distribution toward a target one. Experiments on a simple 1D case (where the optimal transport map is known), and on images with an MNIST / CelebA qualitative example. <sep> The use of this dynamic formulation is well known in the OT community. See as a good examples: <sep> Trigila, G., & Tabak, E. G. (2016). Data‐driven optimal transport. Communications on Pure and Applied Mathematics, 69(4), 613-648. <sep> and more generally Chapter 7 of the 'Computational Optimal Transport' book by Peyré and Cuturi. <sep> The novelty arises from the use of neural networks to represent the potentials. However, the claim that the obtained map is the optimal transport map seems wrong to me, because: <sep> The class of potential functions over which the optimization is performed is not the whole class of functions, leading to approximations; <sep> The optimality conditions (a.k.a continuity or preservation of mass equations) are only enforced on sampled trajectories, not on the entire space. <sep> While this claim should definitely be lowered, it is nonetheless still acceptable provided that the proposed model is performing good. On this part, the paper strength could be improved provided that comparisons with existing methods computing a Monge map could be given. Notably, a comparison with the approach from Seguy et al. 2018 is missing. On a same level, a qualitative comparison with cycleGAN  in Figure 4 is missing. <sep> There are some unclear elements in the paper. The final, total, optimization problem is never clearly expressed. I believe a general algorithm presentation could help in understanding the general picture of the method. Notably, for instance, It is still not clear if the generator G is disconnected from the potential definition (following Eq. 10 I assume not). How are the trajectories sampled ? Is the discriminator trained on the same sampled trajectories or different ones ?  In the potential generator, it seems that the time is considered the same way as the feature space. Can you comment on this point ? Is it possible to evaluate the flow of different time stamps that the ones used for training ? <sep> As a summary, <sep> Pros: <sep> An interesting way to represent time-dependent potentials with a network for regularizing generative models <sep> Cons: <sep> Not much theoretical novelties in the paper, nor a good analysis on the source of errors of the model (e.g. impact of discretization on the problem) <sep> There are some unclear aspects in the paper (see comments) <sep> The potential benefits of the approach over the state-of-the-art should be more clearly discussed. <sep> Minor comment: <sep> P3. Uniqueness of Monge problem for the squared Euclidean cost should be attributed to Brenier 91 and his polar factorization theorem. McCann generalized it to Riemannian manifold.","This paper proposes applying potential flow generators in conjunction with L2 optimal transport regularity to favor solutions that ""move"" input points as little as possible to output points drawn from the target distribution. The resulting pipeline can be effective in dealing with, among other things, image-to-image translation tasks with unpaired data. Overall, one of the appeals of this methodology is that it can be integrated within a number of existing generative modeling paradigms (e.g., GANs, etc.). <sep> After the rebuttal and discussion period, two reviewers maintained weak reject scores while one favored strong acceptance. With these borderline/mixed scores, this paper was discussed at the meta-review level and the final decision was to side with the majority, noting that a revision which fully addresses reviewer comments could likely be successful at a future venue. As one important lingering issue, R1 pointed out that the optimality conditions of the proposed approach are only enforced on sampled trajectories, not actually on the entire space. The rebuttal concedes this point, but suggests that the method still seems to work. But as an improvement, the suggestion is made that randomly perturbed trajectories could help to mitigate this issue. However, no experiments were conducted using this modification, which could be helpful in building confidence in the reliability of the overall methodology. <sep> Additionally, from my perspective the empirical validation could also be improved to help solidify the contribution in a revision. For example, the image-to-image translation experiments with CelebA were based on a linear (PCA) embedding and feedforward networks. It would have been nice to have seen a more sophisticated setup for this purpose (as discussed in Section 5), especially for a non-theoretical paper with an ostensibly practically-relevant algorithmic proposal. And consistent with reviewer comments, the paper definitely needs another pass to clean up a number of small grammatical mistakes."
"abstract | weakness | decision  ==> This paper studies the accuracy vs model-size trade-off of quantized CNNs under different channel width multipliers. The authors demonstrated that while all-to-all convolution works well under low bit settings, depthwise conv needs a different sweet spot. The authors then proceed to use the insight to design quantized cnns that have two different schemes for depthwise and normal conv. <sep> Strength <sep> The paper is well written and motivated. By adding network width to the search space,  using the simple heuristics, the authors provide a better results than previously DRL based search method. <sep> Weakness <sep> One of my main concerns is the direct usage of total number of bits as an equivalent measurement between models. While it is useful to measure the storage cost for weights. The choices of bit width will likely affect the computing cost in a non-trivial way, depending on the target hardware platform. It is unclear whether equal model size would mean equal inference latency in practice (most likely they would not be). Providing empirical implementations of these models will shed light into this question. <sep> These weakness makes it a borderline paper. <sep> Question: <sep> How do you handle batchnorm layer, do you use floating points? <sep> How many bits did you use for accumulating the results? <sep> Update after rebuttal: <sep> I have read the authors' response. I would like to keep my current review as it is. Also note that the authors uses floating pt for batchnorm and 32 bit for accumulation, such additional cost might out-weights the benefit of choosing ultra low bits in the low bits regime, making the study less relevant for practice reasons",This paper studies the trade-off between the model size and quantization levels in quantized CNNs by varying different channel width multipliers. The paper is well motivated and draws interesting observations but can be improved in terms of evaluation. It is a borderline case and rejection is made due to the high competition.
"abstract | weakness | suggestion  ==>  ==> # Response to rebuttal <sep> I would like to thank their authors for their rebuttal. <sep> After reading the other reviews, the author response and the revised manuscript, I have decided to keep my score of weak reject for the time being. <sep> In short, while I appreciate the effort the authors put in partly addressing some of the most important comments raised during the review process, I think the paper would greatly benefit from some additional work. In particular: <sep> (1) Given the emphasis on scalability, I still believe the authors should carry out more thorough experiments to characterize the runtime of their approach with respect to different characteristics of the graphs. While the result provided in the response to Reviewer #3 is a first step, I recommend the authors to extend it by (1) varying graph size (in terms of nodes and edges); (2) varying graph type and (3) reporting the speedup with respect to other baselines. <sep> (2) To the best of my knowledge, the ablation experiment in Section A.7.4 does not provide results for the setting in which no graph attention mechanism is used at all, neither for the case where the graph attention mechanism used is identical to GAT (restricted to 1-step neighbourhoods). <sep> (3) While NSPDK might be a reasonable choice, I still am of the opinion that the choice of graph kernel for this purpose is highly arbitrary and, thus, should be investigated further. Given that such a choice is being used to define a performance metric, which moreover is being highlighted as a contribution, the authors should study the robustness of the metric to the choice of graph kernel, as well as its sensitivity to known perturbations. <sep> (4) Finally, I did not see any error bars added to the main results in the paper. <sep> Despite these shortcomings, I would like to reiterate that I believe the proposed approach is promising and, with some additional work, would be a contribution definitely worth publishing. Therefore, I would like to encourage the authors to further revise the manuscript. <sep> # Summary <sep> In this paper, the authors propose an auto-regressive deep generative model for graph-structured data, motivated by the goal of scalability with respect to graph size, graph density and sample size. <sep> In a nutshell, the approach follows closely the ideas in [1, 2], which model graph generation as an auto-regressive process after fixing or sampling an ordering for the nodes. Unlike [1, 2], however, the proposed method makes use of graph convolutions and a graph attention mechanism, closely related to GAT [3], to parametrize the conditional distributions of node/edges given the previously generated graph elements. <sep> The performance of the proposed approach is evaluated in comparison to [1, 2] in several synthetic and real-world datasets, using MMD [4] between generated and held-out test graphs as metric. Unlike [2], which applies MMD on three graph statistics (degree, clustering coefficient and average orbit counts), this manuscript proposes to evaluate MMD using a graph kernel as well [5]. <sep> # High-level assessment <sep> The main contribution in this paper is to combine a graph attention mechanism, which can be seen  as a simplification of GAT [3], with deep autoregressive graph models, such as DeepGMG [1] or GraphRNN [2]. In this way, the manuscript has a large conceptual overlap with the method in [6], which can be nevertheless be regarded as concurrent rather than prior work. From a methodological perspective, I believe the contribution is sound and sufficiently novel, although perhaps slightly on the incremental side. <sep> However, the current version of the manuscript has shortcomings regarding (i) lack of clarity in the exposition of the method's relation to prior work, low-level implementation details and experimental setup and (ii) insufficient experimental results to back up some of the authors' claims. <sep> Nonetheless, I believe the proposed approach is promising, and encourage the authors to address or clarify these issues during the author discussion phase. <sep> # Major points / suggestions <sep> 1. The manuscript presents the proposed approach in a way that does not clearly differentiate between prior work and original contributions. <sep> In particular, I believe that the ideas in Section 3.1 and 3.2 are almost identical to those in [1, 2], the graph attention mechanism in Section 3.3 can be seen as a minor modification of GAT [3], and Section 3.4 also has a strong conceptual overlap with [1, 2]. <sep> I would encourage the authors to be more clear with respect to what is novel and what is borrowed from prior work. Moreover, when slightly departing from prior work (e.g. the modifications applied to the graph attention mechanism in Section 3), I would also encourage the authors to focus on explaining what specifically has changed and what is the rationale behind those design choices, rather than explaining the entire mechanism ""from scratch"", leaving up to the reader to figure out what is novel. <sep> 2. The paper's clarity could be improved, with some parts presented in an unnecessarily complicated manner (e.g. the graph attention mechanism) and others without sufficient detail (e.g. the edge estimator module, the zero-ing heuristic for attention or the generation of graphs based on ""seed graphs"", which is only mentioned in the appendix). <sep> For example, regarding the graph attention mechanism, I would recommend: (i) explaining more clearly what the ""feature vector of node vi"" is exactly in relation to the notation of Section 3.1; (ii) if the query, key and value matrices are identical, as the text seems to imply, I would rewrite the equations directly in terms of X which would simplify the notation significantly; (iii) perhaps most importantly, the bias functions bQ, bK and bV should be defined mathematically and discussed in greater detail and (iv) the output FNN should also be described mathematically. Finally, as mentioned above, I would emphasise the differences between the proposed attention mechanism and GAT. <sep> The edge estimator mechanism is described too imprecisely in Section 3.4.4. While Section A.4 definitely helps, I would recommend defining the entire operation mathematically in Section 3.4.4 as well. Likewise, a precise mathematical definition of GRAM-A in Section 3.5.2 would also be helpful. <sep> Finally, as mentioned in this forum by Prof. Ranu prior to this review's writing, the graph generation procedure described in Section A.7.2 seems unconventional. I would encourage the authors to both clarify what they mean by ""for the convenience of implementation"" and to investigate whether the experimental conclusions are affected by this departure from prior practices. <sep> 3. Key details about the experimental setup, such as the hyperparameter selection protocol for the proposed approach and baselines, as well as the resulting architectures, seems to be missing, making it difficult to assess if the experimental setup is ""fair"". <sep> In particular, all methods should be allowed to use a similar number of parameters or, alternatively, have their hyperparameters tuned equally carefully for each dataset separately. <sep> 4. Most importantly, I believe the experimental results are insufficient to back up some of the claims made in the introduction. <sep> 4.1. Despite the focus on scalability throughout the motivation, there are no experiments systematically exploring how the runtime at train and test time of the proposed approach and the main baselines scales with respect to sample size, number of nodes per graph and graph density. Moreover, no results are provided for large graphs (e.g. ~5k nodes as in [6]). <sep> 4.2. The graph attention mechanism was claimed to be an original contribution. However, no results are provided to evaluate its advantages with respect to the different GAT variants nor ablation studies to see its usefulness relative to a variant of the proposed approach using only graph convolutions. <sep> 4.3 The idea of using MMD in conjunction with graph kernels as a performance metric is interesting. However, there is no investigation of key aspects such as (i) its relation to other metrics and (ii) the impact that the choice of graph kernel, among the many available, and/or of graph kernel hyperparameters has on the resulting metric (see [7] for a comprehensive review on graph kernels). <sep> 4.4. Finally, the results have been reported without error bars, making it difficult to quantify the statistical significance of the observed performance differences between approaches. <sep> # Minor points / suggestions <sep> 1. I strongly believe the authors should adapt the manuscript to mention [6] and related/concurrent work. Ideally, including it as an additional baseline would be even better, but not necessary given the limited rebuttal time. Nevertheless, this point was not taken into consideration when scoring the manuscript, given how recent [6] is. <sep> # References <sep> [1] Li, Yujia, et al. ""Learning deep generative models of graphs."" *International Conference on Machine Learning.* 2018. <sep> [2] You, Jiaxuan, et al. ""Graphrnn: Generating realistic graphs with deep auto-regressive models."" *International Conference on Machine Learning.* 2018. <sep> [3] Veličković, Petar, et al. ""Graph attention networks."" *International Conference on Learning Representations*. 2018. <sep> [4] Gretton, Arthur, et al. ""A kernel method for the two-sample-problem."" Advances in Neural Information Processing Systems. 2007. <sep> [5] Costa, Fabrizio, and Kurt De Grave. ""Fast neighborhood subgraph pairwise distance kernel."" Proceedings of the 26th International Conference on Machine Learning. Omnipress; Madison, WI, USA, 2010. <sep> [6] Liao, Renjie, et al. ""Efficient Graph Generation with Graph Recurrent Attention Networks."" *Advances in Neural Information Processing Systems.* 2019. <sep> [7] Kriege, Nils M., Fredrik D. Johansson, and Christopher Morris. ""A Survey on Graph Kernels."" *arXiv preprint arXiv:1903.11835* (2019).","The paper proposed an efficient way of generating graphs. Although the paper claims to propose simplified mechanism, the reviewers find that the generation task to be relatively very complex, and the use of certain module seems ad-hoc. Furthermore, the results on the new metric is at times inconsistent with other prior metrics. The paper can be improved by addressing those concerns concerns."
"abstract | misc | rating_summary | rebuttal_process | misc | weakness | misc  ==> This paper studies a very interesting topic: automatically grow filters and layers in neural networks and find an ""optimal"" width and depth for neural networks. The method is motivated by SPLITLBI, and its effectiveness is verified by experiments and comparison with AutoGrow. I tend to accept this paper, before the following questions can be answered: <sep> 1. I guess the major issue in this paper is that the method is not clearly explained and rigorously formulated, although it's an extension of SPLITLBI. <sep> -- 1.1. what's Γ? Is it a copy of W or not? What's the exact mathematical function of loss L() w.r.t. W and Γ? How does the neural architecture change after adding Γ? <sep> -- 1.2. why Γ can be approximated by gradients in Eq. (3)? What's the intuition behind? <sep> -- 1.3. why Γ is necessary? How does it compare with enforcing group Lasso on W directly, like what was done in Nonparametric Neural Networks [1]? <sep> Without clarifying those, people can hardly learn from and use this paper. <sep> 2. Experiments <sep> -- 2.1. Include the learned width in Table 1. <sep> -- 2.2. In comparison with AutoGrow, the pairs of ResNet is fair, but the pairs of PlainNet is hard to judge because different neural architectures are used. AutoGrow uses 4 blocks while this paper uses 5 blocks. It's unclear if the benefit comes from the method or just from an additional block. <sep> -- 2.3. In the experiments of layer growing, please clarify if filter growth is also applied or not. <sep> -- 2.4 clarify ""their growing process is not efficient."" If I read the AutoGrow paper correctly, efficiency is one of the their claims and they showed that the growing process is as fast as ""training a single DNN"", and they scaled to ImageNet, which is not covered in this paper. <sep> Minors: <sep> 1. networks with ""20 filters"" and ""100 neurons"" are used as the seeds. How critical are they? <sep> 2. ""To the best of our knowledge, this is the first algorithm for BoN that can simultaneously learn the network structures and parameters from training data."" is over-claimed. Lots of pruning methods can do it, although they start from a large one and prune it down. <sep> [1] Philipp, George, and Jaime G. Carbonell. ""Nonparametric neural networks."" arXiv preprint arXiv:1712.05440 (2017).","This paper considers how to learn the structure of deep network by beginning with a simple network and then progressively adding layers and filters as needed. The paper received three reviews by expert working in this area. R1 recommends Weak Reject due to concerns about novelty, degree of contribution, clarity of technical exposition, and experiments. R2 recommends Weak Accept and has some specific suggestions and questions. R3 recommends Weak Reject, also citing concerns with experiments and writing. The authors submitted a response that addressed many of these comments, but R1 and R3 continue to have concerns about contribution and the experiments, while R2 maintains their Weak Accept rating. Given the split decision, the AC also read the paper. While we believe the paper has significant merit, we agree with R1 and R3 on the need for additional experimentation, and believe another round of peer review would help clarify the writing and contribution. We hope the reviewer comments will hep authors prepare a revision for a future venue."
"abstract | weakness | rebuttal_process | decision  ==>  ==> This work proposes a method to transfer information from PET imaging data from the medical domain, where data is highly available, to the industrial one, where data is scarce, in the context of non-destructive material quality evaluation. The basic idea seems interesting, but unfortunately in the present form he paper is very difficult to appreciate, as it lacks of important details concerning methodology, experimental results, and comparison with respect to the state-of-the-art. Moreover, the manuscript still appears in a draft form. Sentences are often broken, the text presents many typos and grammar mistakes, and the citations are not understandable. <sep> Missing methodological details are grouped in the following parts of this review. <sep> Section 3.2 Encoder <sep> Paragraph 2: The authors claim that they introduce the knowledge of migration learning without explaining it. Where does this concept come from? Is there a literature about it or is this a new concept? <sep> Medical images are fitted though a variational auto encoder (VAE) (citation missing). The encoder description is minimal and lacks of implementation details (see Eq. 3), while the decoder description is missing throughout the paper. <sep> Eq. (2): the authors write that they obtain a distribution p(x) according to Eq. (2), but this equation is just the formula for the sample mean, where p(x) is sampled and not computed. <sep> Eq. (3): f1 and f2 are never made explicit in the paper so we do not know if they are linear or non-linear functions. The prior p(Z) is decomposed as a summation of posteriors p(Z|X) and the choice to have these posteriors equal to N(0,1) (1-dimensional, which is unusual) regardless of the data point X is not explained. <sep> Section 3.3 Feature extraction memory module <sep> Feature extraction from industrial images is done through principal component analysis (PCA). In the same paragraph is written that features are extracted through convolution neural networks (CNN). So it is not clear if there is an arbitrary choice to use PCA or CNN, and what are the conditions when this happens. <sep> Eq. (6): the score between z_t (medical image distribution) and y_s (industrial image feature vector) is computed as scalar product dot(z_t , W_a * y_s). The key link is the linear mapping W_a, which is never made explicit in the paper. How do the authors compute W_a ? <sep> Section 4.1 Implementation details the networks called ""identification network"" and ""front-end network"" are not well defined. They may refer to the  VAE, the CNN, the Adversarial Nets. There is too much ambiguity. A captioned figure can help in resolving the ambiguities. <sep> Section 4.2 Dataset <sep> In the first paragraph the authors cite a dataset of CT images, while the main focus of the paper is on PET images. How the CT images comes into play? <sep> Section 4.3 Evaluation <sep> The authors compare their method with respect to other three methods, namely VAE, DCGAN, and WGAN. <sep> The implementation details of the competing methods are not described so we cannot be sure about the fairness of the comparison. <sep> Other considerations <sep> Introduction, 4th paragraph: ""imaging quality is higher"" With respect to what? Usually PET images have the worst quality in the medical domain. <sep> Introduction, 3rd-to-last paragraph: ""We use the medical CT image ..."". Should be PET images <sep> Related work: This section is a list of works and the relation with the current paper is not highlighted. <sep> ""lung cancer cakes"" what are they? <sep> Paragraph 3.2: ""excessive pursuit of quality"" why is it bad? ""migration learning"" do they mean transfer learning? Equation 2 and 3 in relation to a clustering problem never pointed out before in the paper. <sep> Paragraph 4.1. What is the meaning of ""Adam algorithm(=0.5)"" ? <sep> Figures 1 and 2 have very minimal captions. What do they represent? <sep> Citation formatting problem: name and surname are switched. Journal/conferences often omitted.","The paper studies Positron Emission Tomography (PET) in medical imaging. The paper focuses on the challenges created by gamma-ray photon scattering, that results in poor image quality. To tackle this problem and enhance the image quality, the paper suggests using generative adversarial networks. Unfortunately due to poor writing and severe language issues, none of the three reviewers were able to properly assess the paper [see the reviews for multiple examples of this]. In addition, in places, some important implementation details were missing. <sep> The authors chose not to response to reviewers' concerns. In its current form, the submission cannot be well understood by people interested in reading the paper, so it needs to be improved and resubmitted."
"abstract | ac_disagreement | weakness | decision  ==>  ==> POST-REBUTTAL FEEDBACK <sep> I share the same concerns as that of reviewer 2 in the response to the rebuttal. Hence, my score remains unchanged. <sep> SUMMARY OF REVIEW <sep> This paper motivates the need to ""contextualize"" responses based on the query to bring about stable training in NRG and consequently proposes localGAN to realize this. On the overall, I like the motivation and the proposed approach of this paper. The experimental results also look convincing. <sep> On the flip side, the technical formulation and theoretical results are not presented rigorously and important technical details are missing, as discussed below. As a result, clarifications from the authors are needed to ensure the correctness of their formulation. The authors also need to improve the presentation and proof of the theoretical results; the correctness has to be checked again. In my opinion, these theoretical results do not improve my current assessment of the paper and can be removed to cut down to 8 pages. If the authors like to keep them, they need to revise them based on my concerns above. <sep> It would be good to show some sample queries and corresponding ""meaningful"" responses produced by their proposed LocalGAN that are not considered safe responses which are produced by the other tested methods. <sep> DETAILED COMMENTS <sep> For Lemmas 1 and 2 and Theorem 1, the authors need to present them rigorously by specifying the exact math expressions since they have not defined what it means by sufficient, approximates, small enough, and estimate properly. This will also eliminate any discrepancy in their interpretations. For example, the authors have used Taylor series expansion to approximate the expectation of F(q,r) in equation 19 (instead of bounding it). Hence, one can claim that Lemma 2 does not hold and hence Theorem 1 does not hold as well. <sep> In Section 4.3, the described mechanism is confusing to me: <sep> (a) Are the authors saying that it is performed sequentially from foundation to phase-1, followed by phase-2? Or are the authors saying that these three phases are expected behaviors occurring during the optimization in equation 17? <sep> (b) For the foundation phase, is the DBM pre-trained, that is, prior to optimization in equation 17? <sep> (c) Are there multiple response clusters, that is, one for each q? If so, the second RELU term in the minimizing criterion in equation 17 does not seem to properly reflect this. <sep> (d) How are the response cluster centers r_c exactly determined? The authors vaguely say that they are modeled from training data. Is it one center per response cluster? Are the cluster centers also optimized in equation 17, besides the generator's weights? Can the authors provide the argument under the min operator in equation 17? It is confusing to leave out r_c from the subscript of alpha. <sep> I would have preferred that the authors specify the expression of the evaluation metrics to be self-contained. <sep> In Fig. 2, how exactly do the authors measure stability? If the entropy rapidly increases like that of LocalGAN and Adver-REGS, are they considered stable? <sep> Minor issues <sep> Page 1: Despite of? <sep> Page 3: inequation? <sep> Page 3: Equation 4 and 5? <sep> Equation 6: The first summation should just be over q, unless there are multiple sets of R_q per q. <sep> tilde{R}_q is not used in equation 6. <sep> Page 4: a limited samples? <sep> Page 5: defined in 3.2? <sep> Page 5: To be consistent, mathbb should be applied to E. <sep> Page 9: valid this aspect? <sep> Page 9: from the the?","This paper tackles neural response generation with Generative Adversarial Nets (GANs), and to address the training instability problem with GANs, it proposes a local distribution oriented objective. The new objective is combined with the original objective, and used as a hybrid loss for the adversarial training of response generation models, named as LocalGAN. Authors responded with concerns about reviewer 3's comments, and I agree with the authors explanation, so I am disregarding review 3, and am relying on my read through of the latest version of the paper. The other reviewers think the paper has good contributions, however they are not convinced about the clarity of the presentations and made many suggestions (even after the responses from the authors). I suggest a reject, as the paper should include a clear presentation of the approach and technical formulation (as also suggested by the reviewers)."
"abstract | misc | weakness | strength | weakness | suggestion | misc | decision  ==> This paper presents a grammar-based generation approach for ""slot-filling"" style code generation tasks. Given a context AST with opening non-terminal node, the model completes the opening node by predicting a sequence of child nodes, which forms a sub-AST rooted at the original opening node. The proposed model encodes context ASTs using a path-based approach (Alon et al., 2019a), essentially generalizing the previous model of Alon et al., 2019a from a code-to-sequence setting (e.g., generating natural language comments from code) to a ""code-to-code"" setting (i.e., code completion given contextual snippets). <sep> Strong Points: <sep> * The paper is very well written. The idea of formalizing code completion as structured language modeling and extending Alon et al., 2019a for the task is natural and well executed, with strong models and significantly improved results on two code completion benchmarks for both Java and C#. <sep> * The authors attempted to establish comparisons with most existing code generation models. <sep> Detailed Review: <sep> *Technical Contribution* I have a very mixed feeling with this paper, while the model registers high empirical performance, the technical contribution is a bit limited, as detailed below: <sep> - *Path-based Context Encoding* The most important contribution in this submission is the application of path-based AST encoding model of Alon et al., 2019a to encode context (the given contextual and partially generated ASTs) for code generation. While the path-based encoding scheme is indeed a powerful model that intuitively encapsulates and generalizes over most previous approaches (Section 7), applying the model to a different task without significant task-specific adaptation or in-depth analysis might not sound technically novel. Meanwhile, the core idea of modeling/encoding the information flow in both the given context AST and partially generated programs for opening node expansion has already been explored in Brockschmidt et al. (2019a), albeit using a different encoding approach (GNNs) and in a relatively restricted setting of generating arithmetic expressions. <sep> - *Node-based Tree Generation Model* Apart from the path-based context encoding model, the node-based generation model presented in Section 2 also seems interesting. However, it might take longer time-steps to generate the node sequence instead of the sequence of production rules (composed of multiple child nodes), which could make optimization and inference more difficult. On the other hand, to control arity, the node-based approach need to inject synthetic ""EOS"" nodes to signal end of generating an argument sequence, while existing production rule-based systems could easily generate arbitrary number of argument nodes using either a transition system (e.g., Yin and Neubig, 2018) or a special neural component to compute end-of-argument-list probability (e.g., Rabinovich et al. (2017)), without using separate production rules of different arity. <sep> - *Syntactic Copy Mechanism* While the proposed syntactic terminal token copy mechanism (Section 3.3) could be better than the vanilla sequential one, there have already been syntactic copying models capable of copying both terminal tokens and partial ASTs from the context (Yin et al., 2019). <sep> How to Improve: to better understand the different technical contributions outlined above and their relative impacts, the following ablation studies would be helpful: <sep> - Importance of Path-based Context Encoding: the Seq→Path ablation in Table 3 alone might not be adequate to demonstrate the importance of path-based encoding of AST contexts for code generation tasks. The authors should compare with the GNN-based context encoding approach in Brockschmidt et al. (2019a) as this is the most relevant work. The original GNN→NAG model cited in Table 2 used a much simpler copying mechanism and a vanilla production-based tree generation model, and therefore not directly comparable with a tuned SLM. <sep> - Importance of Node-based Tree Generation Model: If possible, the authors might consider swapping their node-based tree generation model with a state-of-the-art production-based approach (e.g., Yin et al., 2019) to demonstrate its effectiveness. <sep> *Claims* The authors claimed in the beginning of the paper that previous program synthesis approaches are either restricted in domains (e.g., DSLs like SQL) or grammars (e.g., restricted grammar of the full language), therefore coining the proposed approach as ""any-code generation"". However, there are indeed code generation systems (some of them cited in this paper) that synthesize open-domain code snippets in general-porpuse programming languages without restriction on vocabulary or grammar. To give a few examples, Iyer et al. (2018) generate open-domain Java class member functions; Rabinovich et al. (2017) and Zhao et al. (2019) predict full Python classes or partial snippets, while Yin et al. (2019) synthesize open-domain short C# code diffs observed in GitHub commits. In fact, the the proposed ""any-code generation"" benchmark is limited to sub-expressions defined within a function, whose scope is more restricted than other benchmarks like CONCODE. <sep> How to improve: the authors might present more evidence to substantiate the their claim on the novelty of the AnyGen benchmark compared with existing open-domain, general purpose code synthesis benchmarks, or consider revising the claim and the title. <sep> References: <sep> * Zhao et al., Neural Networks for Modeling Source Code Edits. 2019 <sep> * Yin et al., Learning to Represent Edits. 2019","This paper proposes a new method for code generation based on structured language models. <sep> After viewing the paper, reviews, and author response my assessment is that I basically agree with Reviewer 4. (Now, after revision) This work seems to be (1) a bit incremental over other works such as Brockschmidt et al. (2019), and (2) a bit of a niche topic for *CONF*. At the same time it has (3) good engineering effort resulting in good scores, and (4) relatively detailed conceptual comparison with other work in the area. Also, (5) the title of ""Structural Language Models for Code Generation"" is clearly over-claiming the contribution of the work -- as cited in the paper there are many language models, unconditional or conditional, that have been used in code generation in the past. In order to be accurate, the title would need to be modified to something that more accurately describes the (somewhat limited) contribution of the work. <sep> In general, I found this paper borderline. *CONF*, as you know is quite competitive so while this is a reasonably good contribution, I'm not sure whether it checks the box of either high quality or high general interest to warrant acceptance. Because of this, I'm not recommending it for acceptance at this time, but definitely encourage the authors to continue to polish for submission to a different venue (perhaps a domain conference that would be more focused on the underlying task of code generation?)"
"abstract | weakness  ==> Review of ""Unsupervised-Learning of time-varying features"" <sep> This work looks at using a conditional VAE-based approach to model transformations of sequential data (transformations can be spatial, such as rotation of MNIST, or temporal, i.e. screenshots of a car racing game). Like how our own visual system encodes differences in time and space [1], they show that in generative modelling with VAEs, ""there is an advantage of only learning features describing change of state between images, over learning the states of the images at each frame."" <sep> Such an encoding allows the model to ignore features that are constant over time, and also makes it easier to represent data in a rotating curved manifold. They demonstrate this using data collected from CarRacing-v0 task (a task where agents have to learn to drive from pixel observation of a top-down track), and also on MNIST where the digits are rotated around the center of an image. They provide interesting analysis of the latent space learned and show that indeed this approach can handle both stationary and non-stationary features well (in CarRacing-v0). For MNIST, they compare the latent space learned from transformations (z_dot) and show that this approach can encode image geometric transformations quite well. <sep> While this paper is interesting and highlights advantages of modeling transformations of sequential data, I don't think the contributions are currently sufficient for *CONF* conference (right now it is a good workshop paper IMHO). For it to be at the conference level, I can make a few suggestions of things that will bring it there, hopefully the authors can take these points as feedback to help improve the work: <sep> 1) Would be great to see how this approach can compare to existing proposed algorithms (i.e. TD-VAE as cited)? Are there problems where this approach will perform really well that current methods are inadequate? <sep> 2) As the method is based on an RL-task, would the latent representation learned be useful for an RL agent that relies on the latent code across several representative RL tasks (in both sample efficiency, and/or terminal performance)? <sep> I don't mean to discourage the authors (esp as an Anon Reviewer #2...), as I like the direction of the work, and also appreciate that a lot of effort has gone into this work. I hope to see the authors take the criticism to make their work better. Good luck! <sep> [1] Concetta1988, Feature detection in human vision: A phase-dependent energy model","This work proposes a VAE-based model for learning transformations of sequential data (the main here intuition is to have the model learn changes between frames without learning features that are constant within a time-sequence). All reviewers agreed that this is a very interesting submission, but have all challenged the novelty and rigor of this paper, asking for more experimental evidence supporting the strengths of the model. After having read the paper, I agree with the reviewers and I currently see this one as a weak submission without potentially comparing against other models or showing whether the representations learned from the proposed model lead in downstream improvements in a task that uses this representations."
"abstract | rebuttal_process | misc | decision  ==> Summary: <sep> The paper proposes to expand the VAE architecture with a mixture-of-experts latent representation, with a mixture-component-specific decoder that can specialize in a specific cluster.  Importantly, the method can take advantage of a similarity matrix to help with the clustering. <sep> Overall, I recommend a weak accept.  The method seems reasonable, and the paper is well-written, but the results are only marginally better than other methods, and there are several weaknesses with the proposed architecture and experimental setup. <sep> Positives: <sep> * The idea of a more expressive variational distribution seems good, <sep> although it is not novel. <sep> * The ability to have multiple decoder networks seems reasonable. <sep> * The ability to incorporate domain knowledge (in the form of a similarity matrix S) is a plus. <sep> * The experiments are thorough, although the method is generally only slightly better than competing methods. <sep> Negatives: <sep> * It's not clear if the similarity matrix S is already solving the clustering problem - in which case, why do we need the rest of the model?  For example, in your experiments you often used UMAP to cluster data.  How does using UMAP by itself work?  (Along these lines, it was not clear if your GMM experiments clustered data in the original space, or in the UMAP'd space - please clarify this). <sep> A good ablation would be to somehow remove the S matrix, to see if the model can accurately cluster samples. <sep> * There is little variance in the generated samples. <sep> * There is not a one-to-one mapping of clusters to labels, so it is hard to use this method to generate a specific type of data (for example, it is hard to generate a specific digit).  This is a big difference from, say, a conditional sampler as learned by a GAN. <sep> This also arises in Fig. 3, where it is clear that latent cluster assignments do not match human-interpretable cluster assignments.  I <sep> suppose this is to be expected, but taken with the previous point <sep> (little variance in generated samples) I think it seriously weakens the paper's claim that this is an ""accurate an efficient data generation method."" <sep> * The method does not do well when the number of clusters is large. <sep> Regular GMMs seem to outperform it. <sep> * I felt that this paper made excessive use of the appendix.  The paper is not self-contained enough, effectively violating the length restrictions.  Please make an effort to move key results back in to the main body of the paper. <sep> Experiments to run: <sep> An ablation regarding the similarity matrix S. <sep> Clarification of whether GMM experiments are run in data-space, or <sep> UMAP'd space. <sep> MIXAE features prominently in your related works, but is not compared to in your experiments.  It sounds like a natural comparison.  Please run this experiment, or explain why it is not a comparable method.","The paper proposes a VAE with a mixture-of-experts decoder for clustering and generation of high-dimensional data. Overall, the reviewers found the paper well-written and structured , but in post rebuttal discussion questioned the overall importance and interest of the work to the community. This is genuinely a borderline submission. However, the calibrated average score currently falls below the acceptance threshold, so I'm recommending rejection, but strongly encouraging the authors to continue the work, better motivating the importance of the work, and resubmitting."
"abstract | misc | weakness | suggestion | misc  ==> This paper presents a method to infer multi-task networks architecture, more specifically to determine which part of the network should be shared among different tasks. The main idea is to first train a specific, standalone encoder/decoder network for each task. Subsequently, a task affinity factor is computed by looking at the similarity (or, more likely the dissimilarity) of an holdout set of images feature representations. Knowing these dissimilarities (based on RDM), one can cluster the tasks and make similar tasks share more of their layers. Computational budget can also be taken into consideration in the form of the number of parameters. Results on Cityscapes, Taskonomy, and CelebA datasets show, to some extent, improvements against the state of the art. <sep> The paper is well written and addresses a common problem in multi-task learning. The experiments provided are extensive and cover most of the current multi-task learning methods and interesting problems. I especially like the idea of formalizing the dissimilarity between tasks using RDM. There are, however, a few key points that would need improvement. <sep> First, except for CelebA, the experiments provided use ResNet50 with only 4 different ""anchor point"" in the network. In other words, the task at hand is limited to selecting the best sharing point between 4 choices. This is not wrong per se, but in my opinion, it does not tackle the main problem: what to do when brute force / exhaustive exploration cannot be fulfilled? CelebA provides a more complex case, but it also requires to change the method from an exhaustive search to a beam search (end of Sec. 3.2). Doing so get us back to a kind of greedy approach, precisely what was advocated against the paragraph before (in the discussion about Lu et al. 2017). <sep> Second, the fact that task affinities are computed a priori leads to the following conclusion: ""this allows us to determine the task clustering offline"". While I agree that this could be useful, one has to keep in mind that compared to other methods, this one has to first train a network for _each_ task independently, which can take a long time. <sep> Out of curiosity, did you consider other correlation coefficients than Spearman? Why use a rank-based method? <sep> Overall, this is a nice paper, with adequate methodology. It is not groundbreaking, and most of the good results arise when we consider both performance and number of parameters, but it is interesting for the community nonetheless. I am not sure the impact would be very high, since it can probably be replaced by an architecture exhaustive search if the number of tasks and branching points in the network are low, but the formalism of the approach is welcomed.","The authors present an approach to multi-task learning. Reviews are mixed. The main worries seem to be computational feasibility and lack of comparison with existing work. Clearly, one advantage to Cross-stitch networks over the proposed approach is that their approach learns sharing parameters in an end-to-end fashion and scales more efficiently to more tasks. Note: The authors mention SluiceNets in their discussion, but I think it would be appropriate to directly compare against this architecture - or DARTS [https://arxiv.org/abs/1806.09055], maybe - since the offline RSA computations only seem worth it if better than *anything* you can do end-to-end. I would encourage the authors to map out this space and situate their proposed method properly in the landscape of existing work. I also think it would be interesting to think of their approach as an ensemble learning approach and look at work in this space on using correlations between representations to learn what and how to combine. Finally, some work has suggested that benefits from MTL are a result of easier optimization, e.g., [3]; if that is true, will you not potentially miss out on good task combinations with your approach? <sep> Other related work: <sep> [0] https://www.aclweb.org/anthology/C18-1175/ <sep> [1] https://www.aclweb.org/anthology/P19-1299/ <sep> [2] https://www.aclweb.org/anthology/N19-1355.pdf - a somewhat similar two-stage approach <sep> [3] https://www.aclweb.org/anthology/E17-2026/"
"abstract | weakness | rebuttal_process | misc  ==>  ==> This paper presents PrediNet: an architecture explicitly designed to extract representations in the form of three-place predicates (relations). They evaluate the architecture on a visual relational task called the ""Relations Game"" which involves comparing Tetris-like shapes according to their appearance, relative positions, etc.. They show that their architecture leads to useful generalizable representations in the sense that they can be used for new tasks without retraining. <sep> I think this paper contains a number of unusual and interesting ideas but is let down by its presentation. The writing is good, but provides very little intuition for why we should expect this approach to work (aside from its connection to equation 1) - I discuss this in more depth below. The experimental task is interesting (I'm okay with synthetic tasks of this form for unusual new architectures like this), but I'm not sure what it tests that isn't tested in the CLEVR and sort-of -CLEVR datasets which rely on similar relational reasoning to solve. The advantage of those datasets is they are well-established with strong baselines so we can be more certain that a fair comparison is been made. I've voted to reject this paper because I feel its premature in its current form. <sep> Expanding on this - The description of PrediNet covers the basics, but missing detail and intuition for why certain choices are made. For example, <sep> - why is L flattened for the queries (I think it's because the query is independent of pixel location, but flattening seems of when L also includes co-ordinates) but not the key, K? <sep> - Why is the key space shared between heads (this seems more intuitive - keys should have consistent meaning… but if that's the case, make that intention explicit)? <sep> - Also, writing the dimensionality of the matrices, would help (e.g. is W_S in R^{m x j} or R^{m x 1} or something else?). <sep> - What is the meaning of the position features in E_1 and E_2? From the softmax product it seems they should be a weighted sum of the pixels that are addressed - implying that it is the weighted average location? <sep> - The final representation mostly consists of an h x (j) vector (ignoring positions) containing the output of the comparators. Could you provide some intuition for why we would expect such a representation to be useful for the downstream task? This representation seems to differ substantially from what is used in the baseline methods: i.e. attention-weighted sums of the input features.","This paper proposes a model that can learn predicates (symbolic relations) from pixels and can be trained end to end. They show that the relations learned generate a representation that generalizes well, and provide some interpretation of the model. <sep> Though it is reasonable to develop a model with synthetic data, the reviewers did wonder if the findings would generalize to new data from real situations. The authors argue that a new model should be understood (using synthetic data) before it can reasonably be applied to natural data. I hope the reviews have shown the authors which areas of the paper need further explanation, and that the use of a synthetic dataset needs to strong justification, or perhaps show some evidence that the method will probably work on real data (e.g. how it could be extended to natural images)."
"rating_summary | decision  ==> Summary: <sep> The paper aims to develop a more principled framework for choosing between different inference procedures in neural network models employing dropout as a stochastic regularizer. In particular, they posit a family of conditional models and show that the learning objectives of all these models are lower bounded by the usual dropout training objective with L2 regularization on weights. They proceed to show empirically that the deterministic inference procedure (multiplying the node's output by the droput rate) achieves the tightest lower bound. From this observation the authors conclude that deterministic inference should be seen as the best available approximation to the true dropout objective rather than an approximation to Monte Carlo averaging. <sep> Strengths: The paper builds on recent works viewing dropout as a Bayesian approximation to the predictive posterior distribution. Introducing a conditional model and showing that the dropout objective is akin to MAP estimation of the parameters of this model is interesting. The result that dropout simultaneously optimizes a lower bound to an entire family of conditional distributions is novel. <sep> Weaknesses: <sep> The writing is often not clear, ambiguous or misleading and needs improvement. For instance: <sep> In Section 2.1, comments on weaknesses of variational dropout seems out of place. It should either be ommitted or shifted to the previous section where variational dropout is introduced. <sep> In Section 2.1, ""Consider a conditional model p(Y |X, Θ) as a crippled generative model with p(X) constant, X and Θ independent."" Assuming p(X), the input features, to be constant is a very strong assumption. The variational lower bound is still true if p(X) is assumed arbitrary independent of Θ. <sep> In Section 3.3, last paragraph, ""Suppose we pick a base model from the power mean family and have a continuum of subvariants with gradually reduced variance in their predictions but the same expectation."" It is not clear what the authors mean by continuum of subvariants? Is it the dropout rate? <sep> Several statements are made without any citations or explanations. <sep> In Section 3. ""While it is easy to argue in general that objectives of more than one model may share any given lower bound"" How? More explanation needed. <sep> In Section 3.1, ""Notice how with SGD and multiple epochs, for each data point several dropout masks are encountered, and the approximating quantity is the geometric mean of the predicted probabilities"". Citation needed. <sep> In Section 3.2, ""because M_α is monotonically increasing in α"". Proof (in appendix) or citation needed. <sep> In Section 5, ""The construction of a conditional model family with a common lower bound on their objectives is applicable to other latent variable models with similar structure and inference method."" What general structure and inference method are the authors referring to? <sep> Technical Concerns: <sep> Section 3.3, last paragraph. The entire paragraph is extremely convoluted. It is not clear how one achieves deterministic dropout inference by reducing variance of the prediction y keeping its expectation constant. <sep> Section 3.3, ""A similar argument based ... shows that Z monotonically increases... "" How? The authors should deliberate more on this statement since, the Z term is also important in the difference between the true objective of each conditional model and the droput training objective. <sep> In Section 5, ""The gains reported in those works might be explained by reducing the bias of deterministic evaluation and also by encouraging small variance in the predictions and thus getting tighter bounds."" Isn't this contradictory to Section 3.3, where from Eq. 10 and Eq. 11? If the bias is reduced and variance increases, according to Eq. 10, the lower bound would become looser. <sep> How is Fig. 1a and 1b computed? <sep> Conclusions drawn from experiments not convincing. <sep> Section 3.4, last line. ""Having trained a model with dropout, the best ﬁt is achieved by the deterministic model with no dropout. This result isolates the regularisation effects from the biases of the lower bound and the dropout family."" How does this isolate the regularization effects? What biases of the lower bound? Do you mean the difference between the model's true objective and the dropout training objective? <sep> Table 4 indicates that not only AMC, also power with alpha=0.5 is better than deterministic in some cases. Did the authors try other values of alphas? Deterministic seems to be good just for MNIST. This strongly refutes the most important claim of this paper, written in the abstract, ""Together, these results suggest that the predominant view of deterministic dropout as a good approximation to MC averaging is misleading. Rather, deterministic dropout is the best available approximation to the true objective."" <sep> Changing the dropout multiplier and adjusting the softmax temperature of the network output layer, to achieve comparable performance to AMC on several datasets seems to support the existing hypothesis that deterministic dropout is a good approximation to MC average, and not the other way around! <sep> Summary: <sep> The paper introduces some interesting ideas about dropout but suffers from bad writing and presentation of results. One of the most important claims made in this paper, ""dropout trains a deterministic model ﬁrst and foremost and a continuum of stochastic ones to various extents"", is not well-motivated theoretically in Section 3.3. Consequently, this seems to be purely an empirical observation, which is contradicted by further experiments on linguistic datasets. Several conclusions made from experiments seem adhoc.","The reviewers have uniformly had significant reservations for the paper. Given that the authors did not even try to address them, this suggests the paper should be rejected."
"abstract | strength | misc | rating_summary | rebuttal_process | decision  ==> * Note: I highlight I did not assess the model design, which is the main contribution of the paper, and  did not know the background of prior work of system design to really assess the novelty of the work, my score is solely based on the experiments. <sep> I am an expert in machine learning/computer vision, so I could assess the experiments in terms of their validity and relevance from the machine learning/vision perspective, however, I may not be the best person to evaluate the design choices of the system. Therefore I choose the low experience for my background. <sep> * Paper summary: The paper proposes a framework to evaluate machine learning models in a hardware-agnostic way. <sep> To evaluate the models using this framework, the user needs to specify the pre-processing, inference, and post-processing steps and the required software/hardware stack. The authors argue that this is important to consider the  HW/SW stack to allow a fair evaluation and reproducibility. Models are specified using a model specification called manifest. <sep> * The authors assume that SW/HW stack change the results of deep learning models a lot, and this is the main assumption in this work, however, normally in practice HW/SW stack wont change the results. <sep> * I found the experiments either not related to the point of the paper or being very trivial not helping to backing up the arguments of the paper. <sep> * In section 4.1, the authors consider different preprocessing operations and study their impact on the model performance, however, the fact that preprocessing impact the results is trivial in machine learning. In the same section, color layout and data layout, cropping and resizing, where the authors discussed about for instance how changing the data representation from NCHW or NHWC change the results, this is also trivial, because if you change the dimensions, you need to also change the model in a way that handles this change of dimension, therefore, this is clear that the results will change accordingly as well. Such experiments does not back up the main argument of the paper, which argues for fair evaluation between neural models, nor provides informative information to the reader. <sep> On section 4.1, the experiment of type conversion and normalization, again this is mathematically clear that the order would change the results,  let's call imgByte=x, then by substituting given values for mean and standard evaluation, equation (b) is simplified to (b) = (x-127.5)/((127.5)*(255)) <sep> however simplification of (c) results in (x/255-0.5)/0.5 = (x-0.5*255)/(0.5*255)=(x-127.5)/(0.5*255) <sep> the dominator of (b) and (c) are not equal, therefore, this is trivial that the results of these two the expression would not be the same. The author posed it as a new finding, but this is trivial that mathematically these two equations would not be equal. Again, this experiment does not add any value to the paper. <sep> In section 4.2, in Figure 9, the authors show a plot of the CPU latency for different batch sizes and instances, <sep> together with GPU throughput for different batch sizes, i.e., images/seconds. The authors show latency for CPU <sep> instances, versus throughput for GPU instance, since these two measures are not shown for both instances, this is not supported from the text, how actually authors compare this two instance and draw the conclusion that which instance is more efficient since there is no value shown for CPU throughput. Apart from that, I don't see how this section and determining if GPU or CPU instances of  Amazon compute cloud is more cost-efficient is related to the point of this paper which is on reproducibility. Also please have a look at Amazon webpage: <sep> https://docs.aws.amazon.com/dlami/latest/devguide/gpu.html <sep> Here, they explicitly mention that ""A GPU instance is recommended for most deep learning purposes. Training new models will be faster on a GPU instance than a CPU instance. You can scale sub-linearly when you have multi-GPU instances or if you use distributed training across many instances with GPUs."",  so having this experiment again neither back up the arguments in the paper, nor add value to the paper. <sep> * The major issue with this submission is that the experiments are not related to the arguments of the paper, and are not conveying any message towards backing up the arguments of the paper. <sep> * Another crucial problem is that to allow a fair comparison especially in neural models, as shown in several studies(see [1] as a sample), this is important to account for random seeds and study how it impacts the model performance, to allow a fair evaluation of the models this is important to consider this factor, fair evaluation of models is argued to be the main point of this paper, however, the authors does not consider this factor in the paper, nor study it in the experiments. <sep> [1] Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks, Nils Reimers and Iryna Gurevych","The paper proposes a platform for benchmarking, and in particular hardware-agnostic evaluation of machine learning models. This is an important problem as our field strives for more reproducibility. <sep> This was a very confusing paper to discuss and review, since most of the reviewers (and myself) do not know much about the area. Two of the reviewers found the paper contributions sufficient to be (weakly) accepted. The third reviewer had many issues with the work and engaged in a lengthy debate with the authors, but there was strong disagreement regarding their understanding of the scope of the paper as a Tools/Systems submission. <sep> Given the lack of consensus, I must recommend rejection at this time, but highly encourage the authors to take the feedback into account and resubmit to a future venue."
"abstract | weakness  ==> The paper makes a significant attempt at solving one of the practical problems in machine learning -- learning from many noisy and limited number of clean labels. This setting is presumably more practical than the setting of few-shot learning. Noisy labels are often abundantly available and investing in methods that can take the noise into account for building a discriminative model is quite timely. <sep> To be honest, the theoretical contribution of the paper is limited.  The authors make use of the nearest neighbour graph obtained from a reduced-dimensional set of features to compute the weights of the noisy labels that must guide the predictive model. From this perspective, the paper seems like an application of existing tools (such as CNN, graph convolutional network and binary classification). However, that does not undermine the superior results the authors have received in the novel application they have targeted. I appreciate the effort that went validating these ideas with real-world datasets. <sep> In future, I would like to see a joint approach to such training, where the function g(), the nearest neighbour graph loss and the classification loss are all tied in the same objective function and are optimized jointly. <sep> The paper has few really minor grammatical errors and typos. Please fix those before uploading the final draft.",The paper combines graph convolutional networks with noisy label learning. The reviewers feel that novelty in the work is limited and there is a need for further experiments and extensions.
"weakness | misc | weakness | misc  ==> *Summary* : This paper augments the TRPO policy optimization objective with hindsight data, where the hindsight data is generated from goals based on trajectories. The key contribution of the paper is based on deriving an on-policy adaptation of hindsight based TRPO, that can be useful for sparse reward environments. The paper draws ideas from existing papers such as HPG and considers the IS based variant of HPG for on-policy, similar to TRPO, that can achieve monotonic performance improvements. Furthermore, the authors introduce a logarithmic form of constraint, by re-deriving the KL constraint and leading to a f-divergence based constraint, which is argued to have useful effects in terms of lowering the variance. Experimental results are compared with baselines including HPG and its variants on standard sparse reward benchmark tasks. <sep> *Comments* : <sep> - The core contribution of the paper is to introduce hindsight based TRPO where end states from the trajectory treated as goals can be useful for generating pseudo-rewards, such that existing TRPO based approaches can be better suited for sparse reward environments. The claim is that existing policy gradient methods cannot sufficiently explore in sparse reward environments. <sep> - Since incorporating hindsight data can make the approach off-policy in nature, leading to higher variance and instability, the authors propose to augment hindsight approach into on-policy based methods such as TRPO. The key contribution is to develop a theoretically sound approach for hindsight based policy optimization. <sep> - The hindsight TRPO objective is derived based on goal-conditioned policies with TRPO, based on existing recent work on Hindsight Policy Gradient (HPG) (Rauber et al., 2019). The key difficulty that needs to be tackled is when introducing hindsight experience makes the approach off-policy in nature. The authors derive the HTRPO objective based on an importance sampling approach that can also incorporate the hindsight data. <sep> - Equation 7 writes out the key quantity, an IS based approach considering the current goal and alternative goals to derive a similar TRPO objective based on IS based Monte-Carlo estimators, while maintaining the trust region guarantees. Equation 8 further shows the gradient of the objective. Theorem 3.1 and 3.2 follows naturally where the key trick in Thm 3.1 is going from equation 19 to 20 to derive the IS based estimator with goal conditioned trajectories. I am not sure why Thm 3.2 needs to be written out explicitly, given that it follows naturally for gradient of the expectation? Is there any key takeaway from it? <sep> - In TRPO, the expectation is based on states sampled from a state distribution. The authors argue that for hindsight based data, this state distribution in fact can change due to changes in the generated goals (ending states), and hence the KL needs to be in expectation w.r.t to the state occupancy measure. Furthermore, the authors change the KL based constraint into a logarithmic form of a constraint such as to reduce variance and introduce more stability. To achieve this, the paper uses an approximation to the logarithmic form of the constraint, by using an expectation of the square instead of plain differences between the log of policies. The key is that instead of using the KL divergence, the authors introduce f-divergence where the function is convex allowing smooth optimization. <sep> - The overall contribution of the paper can be summarized from equation 15 - introducing a IS based correction, while remaining on-policy for hindsight based TRPO objective. And since hindsight can change the underlying state distribution, leading to more instability, the paper introduces a different form of constraint (based on the f-divergence) which can have lower variance than the KL form of constraint. <sep> - Experimental results are demonstrated for few sparse reward benchmarks, comparing to the standard HPG, TRPO and several variants of the proposed HTRPO approach with weighted IS and with the KL constraint. The advantages of HTRPO on these tasks seems clear, mainly in the Fetch Reach and Fetch Push tasks, significantly outperforming the baselines. Even in the continuous tasks, HTRPO seems to outperform the baselines consistently. <sep> *Feedback/Questions* : <sep> - I am not sure of the significance of Theorem 3.2 - it seems obvious that the gradient of the objective spans out naturally from equation 7? <sep> - The authors mention about the state occupancy measure instead of considering the state distribution for the KL term. However, the discussion of state occupancy measure seemed to have faded away? What was the significance of mentioning it, or why should it be considered even? There are no assumptions being made on whether the state distribution should be the discounted state occupancy measure or the stationary distribution (if infinite horizon is considered). <sep> - The introduction of the logarithmic form of constraint, even though shows theoretically to reduce variance, is not well motivated or demonstrated from the suite of experimental results? From the results, it is not obvious whether this form of constraint is indeed having useful effects in terms of reduced variance? <sep> - The paper seems to adapt from the HPG objective, and does indeed a great job comparing to HPG throughout the suite of experiments. However, in the results, the paper mainly compares to other off-policy based methods including HPG and its variants (official and re-implemented one). I find the comparison of results a bit odd in that sense, since it is comparing the on-policy adaptation of HPG (ie, HTRPO) and the off-policy variants? If run for long enough, does all converge to the same results? If so, then the benefits is mainly in faster learning (assumably due to better exploration in the sparse reward tasks). But then again, these benefits may be because of the on-policy approach compared to the off-policy one? <sep> - I would encourage the authors to compare to more standard goal-conditioned or reward shaping based baselines for TRPO. For example, does the proposed HTRPO approach perform better compared to other goal-conditioned approaches of TRPO, or for example if a form of reward shaping (based on goals) are used in TRPO? It would then be a more likewise comparison? The current results seem to show benefits of HTRPO, but I think there is a need for stronger baselines where TRPO + exploraton (reward shaping or goal conditioning) performs better? <sep> - I am not convinced about the arguments with sensitivity of HTRPO with network architecture and parameters. How is this demonstrated from the suite of results? <sep>  <sep> *Summary of Review and Score* : <sep> Overall, I think the paper has merits, mainly in terms of deriving the on-policy adaptation with hindsight data. The key objectives are derived nicely in the write-up and easy to follow, although there are some confusions that need to be clarified (example : the discussion on state occupancy measure and the significance of it). The paper motivates exploration for TRPO in sparse reward tasks, and considers the hindsight adaptation of existing TRPO. But related works such as HPG have already taken a similar approach for the off-policy case, and this paper's key contribution is in terms of theoretically deriving the objectives for on-policy adaptation. However, I am not convinced about the overall merits and contributions of the paper, especially in terms of demonstrated results and proper comparisons with baselines. I think while the objectives and derivations follow naturally, the contributions of the paper is somewhat marginal. <sep> I would therefore vote to marginally reject this paper - mainly in light of the core novel contribution and due to lack of sufficient results demonstrating the usefulness of the approach. The paper combines several things together - especially discussions of the logarithmic form of the constraint. I doubt whether all these introduced together led to the improvements shown in the experimental results or not. It would be useful to clarify the contributions from each of the components.","The paper pursues an interesting approach, but requires additional maturation. The experienced reviewers raise several concerns about the current version of the paper. The significance of the contribution was questioned. The paper missed key opportunities to evaluate and justify critical aspects of the proposed approach, via targeted ablation and baseline studies. The quality and clarity of the technical exposition was also criticized. The comments submitted by the reviewers should help the authors strengthen the paper."
"abstract | weakness | rebuttal_process | misc | suggestion | decision  ==> This paper concerns the ""information plane"" view of visualizing and understanding neural net training. Kernel-based estimators of Renyi entropy-like quantities are applied to study moderate-sized DNNs. A straightforward extension to CNNs is presented. <sep> Major comments: <sep> I found this paper extremely hard to follow. I think a lot of my difficulty was a lack of familiarity with the papers this work was building on, but I also felt the main line of reasoning was unnecessarily opaque. I've tried to indicate where some clarifying wording might help readers such as myself. <sep> In general I am very skeptical that reasonable entropy-like quantities can be estimated reliably for high-dimensional data using anything along the lines of kernel density estimation, especially based on a single minibatch or small collection of minibatches! The authors provide no experimental evidence that these estimates are even close to being accurate (for example on synthetic datasets where the true entropy / mutual information is known). Clearly the estimated quantities evolve during training, and that may be interesting in itself, but calling the estimated quantities ""mutual information"" seems like a leap that's not justified in the paper. <sep> Throughout the paper, CNNs are referred to as having some special difficulty for entropy estimates because of the channel dimension. This is misleading. For DNNs the activations are vectors, and so a kernel defined over rank 1 tensors is needed. Even without the channel dimension, CNN activations would be rank 2 tensors, and so a kernel would need to be defined over rank 2 tensors. Going from rank 2 to rank 3 doesn't pose any special difficulties. Indeed the most obvious way of defining a kernel over higher rank tensors is to flatten them and use a kernel defined over vectors, which is exactly what the paper does in practice. <sep> Minor comments: <sep> In the introduction, it would be helpful to include some relevant citations after the sentence ""Several works have demonstrated that one may unveil interesting properties... IP"". <sep> In section 3, the multivariate extension proposed by Yu et al. (2019) seems like an interesting side note (since it was used in a previous attempt to estimate information plane for CNNs), but it doesn't seem central to the paper, and I personally found it unnecessarily confusing to have it presented in the main text. What about moving sections 3.2 and 4.2 to the appendix for clarity? <sep> In section 3.1, ""over a finite set"" is probably incorrect (""probability density function"" implies a continuous space for X, as does the integral in (1)). <sep> In section 3.1 (and the appendix), ""The ones developed for Shannon"" seems imprecise. ""Certain approaches developed for estimating the Shannon entropy""? <sep> It's not clear what ""have the same functional form of the statistical quantity"" means. Which statistical quantity? What aspects of the functional form are similar? Please elaborate in the paper. <sep> I think ""marginal distribution"" is incorrect. It's representing a whole probability distribution, not just a marginal distribution. Which marginal would it be in any case? <sep> Section 3.2 states ""The matrix-based... entropy functional is not suitable for estimating... convolutional layer... as the output consists of C feature maps"", but as discussed above, there is no special difficulty caused by the channel dimension. Even if the channel dimension were not present the difficulties would be the same. (Also, it seems like defining a kernel over the rank-3 tensors is an extremely natural / unsurprising thing to try given the set-up so far.) <sep> In section 4.1, ""can not include tensor data without modifications"" seems misleading for a similar reason. One of the great things about kernels is that they can be defined on lots of objects, including things like rank 3 tensors! <sep> Near (8), it would be very helpful to state explicitly what is done for the joint entropy term in (5). It sounds like this term, which involves the Hadamard product, in practice amounts to summing the Euclidean distances between x's and between y's, and it might be helpful to the new reader to point this out. (It also highlights that the method is easy to implement in practice). <sep> The discussion in section 4.2 is only valid for the RBF kernel, but the first paragraph of that section makes it sound like it is true more generally. <sep> At the bottom of section 4.2, if the proposed approach is equivalent to the multivariate approach, then how can one suffer from numerical instability while the other doesn't? Also, numerical stability makes it sound like an artifact of floating point arithmetic, whereas I think the point that's being made is more mathematical? Please clarify in the paper. <sep> In ""enabling training of complex neural networks"", shouldn't ""training"" be ""analysis""? <sep> In section 5, under ""increasing DNN size"", I wasn't clear on the meaning of ""deterministic"" in ""neither .... is deterministic for our proposed estimator"". Random variables can be deterministic or not, but how can a mutual information be deterministic? <sep> Under ""effect of early stopping"", isn't looking at the test set entropies (as is done elsewhere in the paper) much more relevant to overfitting than considering different ""patience"" values? <sep> In the bibliography, two different papers are ""Yu et al. (2019)"". <sep> In appendix A, ""the same functional form of the statistical quantity"", ""marginal"", etc don't seem quite correct, as mentioned above. Also the first equation should not have a comma between f(X) and g(Y) (which if I understand correctly are being multiplied).","This paper considers the information plane analysis of DNNs. Estimating mutual information is required in such analysis which is difficult task for high dimensional problems. This paper proposes a new ""matrix–based Renyi's entropy coupled with ´tensor kernels over convolutional layers"" to solve this problem. The methods seems to be related to an existing approach but derived using a different ""starting point"". Overall, the method is able to show improvements in high-dimensional case. <sep> Both R1 and R3 have been critical of the approach. R3 is not convinced that the method would work for high-dimensional case and also that no simulation studies were provided. In the revised version the authors added a new experiment to show this. R3's another comment makes an interesting point regarding ""the estimated quantities evolve during training, and that may be interesting in itself, but calling the estimated quantities mutual information seems like a leap that's not justified in the paper."" I could not find an answer in the rebuttal regarding this. <sep> R1 has also commented that the contribution is incremental in light of existing work. The authors mostly agree with this, but insist that the method is derived differently. <sep> Overall, I think this is a reasonable paper with some minor issues. I think this can use another review cycle where the paper can be improved with additional results and to take care of some of the doubts that reviewers' had this time. <sep> For now, I recommend to reject this paper, but encourage the authors to resubmit at another venue after revision."
"abstract | strength | weakness  ==> In this paper, the authors propose using federated learning (FL) to train personalized models, which improves the scalability and privacy preservation of the existing personalization techniques. The empirical results show good performance. <sep> However, in general, I think the contribution is limited. The reasons are as follows: <sep> 1. The proposed algorithm, FURL, is a direct and simple combination of personalized model and FL. Although the authors claim that there is significant improvement in the performance, such improvement comes from the personalization. And, the personalization itself is not a novel thing (I think the personalized model used in this paper is similar to [1] or some other references. Please correct me if the personalized model used in this paper is new, since I'm not an expert in personalization.) Thus, in general, this paper simply use FL to replace fully synchronous SGD in the training of the personalized models. All the benefits claimed in the introduction, including scalability, privacy preservation, and improvement of performance, come from either vanilla personalization or vanilla FL. I fail to find any new contribution in this combination. <sep> 2. The authors emphasize a lot on the ""independent aggregation constraint"". Although it sounds like such constraint is designed especially for FL + personalization, it is actually a feature only for personalization, which has nothing to do with FL. Note that when doing inference/prediction, each user uses his/her own private part of the model. Different users' private part of models will never affect each other. It is equivalent to training a global model, which concatenates the private parts of models into a big model, and each user update the global model in a sparse manner. Thus, we can also train such personalized model with fully synchronous SGD with sparse gradients, which also does not synchronize the private parts. The private part is never shared by different users, no matter trained by fully synchronous SGD or FL. <sep> ------------ <sep> References <sep> [1] Jaech, Aaron, and Mari Ostendorf. ""Personalized language model for query auto-completion."" arXiv preprint arXiv:1804.09661 (2018).","This manuscript personalization techniques to improve the scalability and privacy preservation of federated learning. Empirical results are provided which suggests improved performance. <sep> The reviewers and AC agree that the problem studied is timely and interesting, as the tradeoffs between personalization and performance are a known concern in federated learning. However, this manuscript also received quite divergent reviews, resulting from differences in opinion about the novelty and clarity of the conceptual and empirical results. Reviewers were also unconvinced by the provided empirical evaluation results."
"abstract | strength | weakness | rebuttal_process | decision  ==> The paper studies methods for detecting adversarial examples using saliency maps. The authors propose using the method of Dabkowski and Gal (2017) to generate saliency maps and then train a classifier on these maps (or their dot product with the input image) to distinguish natural from adversarial examples. They perform experiments evaluating the white-box and black-box robustness of their detection scheme. <sep> From a technical perspective, the contribution of the paper is rather incremental. The detection of adversarial examples by training a classifier on saliency maps has already been studied in prior work. The only modification proposed in this work is using an (existing) alternative method for producing the saliency maps and utilizing the dot product of maps with images. <sep> From a conceptual perspective, the impact of detecting specific adversarial attacks is not clear. In a realistic setting, an adversary could use a very different attack or even utilize a different set of transformations (e.g. image rotations). Thus, in order to demonstrate the utility of their method in a black-box scenario, the authors would need to evaluate the defense in a variety of different scenarios. At the very least, they should consider generalization to difference attacks (e.g., train against FGSM and BIM, and test against DF). <sep> Moreover, the robustness against white-box adversaries is not sufficiently studied. Firstly, the robustness of the non-adversarially trained detector is suspiciously high. There is little reason to expect that a composition of two neural networks (the saliency map methods and the classifier) would be non-trivially robust. The authors should consider alternative attacks perhaps using more iterations with a smaller step size. Secondly, after adversarial training, only the robustness against the same attack is considered. In order to argue about white-box robustness, the authors would need to evaluate against a variety of diverse adversaries. <sep> Overall, the technical and conceptual contribution of this paper is insufficient for publication at *CONF*, even ignoring the concerns about its experimental evaluation.",This submission proposes a method for detecting adversarial attacks using saliency maps. <sep> Strengths: <sep> -The experimental results are encouraging. <sep> Weaknesses: <sep> -The novelty is minor. <sep> -Experimental validation of some claims (e.g. robustness to white-box attacks) is lacking. <sep> These weaknesses were not sufficiently addressed in the discussion phase. AC agrees with the majority recommendation to reject.
"abstract | weakness | suggestion | decision  ==> This papers proposes DeepEnFM approach for CTR prediction task. In detail, Transformer encoder is applied on top of embeddings to generate new projected embeddings. Such transformer encoder is composed of self-attention with bilinear (to replace dot) and multi-head, which is followed by a mx pooling layer and then a FC layer. Position encoding is utilized then. Besides, some resnet-style trick in placed in the middle. Such encoder output is fed into FM and raw embeddings are feed into DNN part. These two parts are then used for final prediction. Some experimental results show the improvement of the proposed method over other methods. <sep> The major questions are: <sep> *  The assumption of ""The field embedding size is very low in CTR"" is not reasonable. Do we have any study to verify this hypothesis? <sep> * Regarding to above hypothesis, i think it doesn't hold for all the CTR prediction tasks. Computation cost will be dramatically increased when embedding size increases because of bilinear between key and query and the FC on top of self-attention. <sep> * The novelty of the proposed method needs to justified to reach the bar of *CONF*. The major reason is that 1) the proposed method just replaces MHSA with two changes, i.e., bilinear + max pooling, 2) other tricks such as resnet-style connection, layer norm and position encoding have been adopted everywhere. <sep> * The gain of proposed method is not so clear though the author test to remove each component from the architecture. As the change of encoder part is on top of MHSA, but there is no experiment to show the gain compared to using original MHSA instead of newly proposed bilinear + max pooling. I suggest to do this for better understanding the gain of changes.","The authors address the problem of CTR prediction by using a Transformer based encoder to capture interactions between features. They suggest simple modifications to the basic Multiple Head Self Attention (MSHA) mechanism and show that they get the best performance on two publicly available datasets. <sep> While the reviewers agreed that this work is of practical importance, they had a few objections which I have summarised below: <sep> 1) Lack of novelty: The reviewers felt that the adoption of MSHA for the CTR task was straightforward. The suggested modifications in the form of Bilinear similarity and max-pooling were viewed as incremental contributions. <sep> 2) Lack of comparison with existing work: The reviewers suggested some additional baselines (Deep and Cross) which need to be added (the authors have responded that they will do so later). <sep> 3) Need to strengthen experiments: The reviewers appreciated the ablation studies done by the authors but requested for more studies to convincingly demonstrate the effect of some components. One reviewer also pointed that the authors should control form model complexity to ensure an apples-to-apples comparison (I agree that many papers in the past have not done this but going froward I have a hunch that many reviewers will start asking for this) . <sep> IMO, the above comments are important and the authors should try to address them in subsequent submissions. <sep> Based on the reviewer comments and lack of any response from the authors, I recommend that the paper in it current form cannot be accepted."
"abstract | weakness | decision  ==> This paper revisited the a common belief that adaptive gradient methods hurts generalization performances. The authors re-examine this in more depth and provide a new set of experiments in larger-scale, state-of-the-art settings. The authors claimed that with proper tuning, the performance of adaptive optimizers can mitigate the gap  with non-adaptive methods. <sep> - It is great to have someone revisit and challenge the conventional ideas in the community. However, I did not find much insightful information in this paper. As the author mentioned, there are many recent works focusing on further improving the empirical generalization performances of adaptive gradient methods. The main aspects mentioned in the paper, \\epsilon tuning, learning rate warmup and decaying schedule, are not something new and many of which are mentioned or used in the recent advances. This makes the contribution of this paper look like combining all the tricks together. The authors might want to carefully comment the differences with the recent advances <sep> [1] Liu, Liyuan, et al. ""On the variance of the adaptive learning rate and beyond."" arXiv preprint arXiv:1908.03265 (2019). <sep> [2] Loshchilov, Ilya, and Frank Hutter. ""Decoupled weight decay regularization."" *CONF* 2019. <sep> [3] Zaheer, Manzil, et al. ""Adaptive methods for nonconvex optimization."" Advances in Neural Information Processing Systems. 2018. <sep> [4] Chen, Jinghui, and Quanquan Gu. ""Closing the generalization gap of adaptive gradient methods in training deep neural networks."" arXiv preprint arXiv:1806.06763 (2018). <sep> [5] Luo, Liangchen, et al. ""Adaptive gradient methods with dynamic bound of learning rate."" arXiv preprint arXiv:1902.09843 (2019). <sep> - In terms of tuning \\epsilon, the authors mentioned that default setting in Tensorflow is 0.1 for Adagrad which is too high. However, most papers regarding adaptive gradient method usually set \\epsilon as 10^-8. Pytorch set default value as 10^-10. In fact, Yogi paper mentioned above gives some different conclusions. In their experiments, they found that setting \\epsilon to be a bit larger like 10^-3 give better results compared with 10^-8. I wonder if the authors examine the reasons for different conclusions here? <sep> - at the end of page 6, missing reference for histogram","The paper combines several recent optimizer tricks to provide empirical evidence that goes against the common belief that adaptive methods result in larger generalization errors. The contribution of this paper is rather small: no new strategies are introduced and no new theory is presented. The paper makes a good workshop paper, but does not meet the bar for publication at *CONF*."
"abstract | weakness | rating_summary | decision  ==>  ==> Summary: <sep> This paper studies the ""suspended animation limit"" of various GNNs – an important one for how to train a good Graph network. The authors provide sufficient analysis by simplify GNNs as a series of 1-step Markov chains (which is my concern as stated in the section on main issues), while pointing out the limitations quantitatively as in the Theorem 2. Under the assumption, the authors propose several new forms of ResNets for GCNs, which can successfully overcome the limitation. <sep> Overall, the motivation of this work is clear and meaninfgful. The proposed residual architecture is effective, and the presentation is clear and easy to understand. <sep> However, my main concerns are on the initial assumptions for analyzing the suspension of GNNs. See the following comments. <sep> This paper is generally well written and easy to understand. The organization of each part is well-balanced. <sep> Originality: <sep> To the best of my knowledge, numerous methods (i.e., targeting on applications) address this problem by augmenting A [1] or X [2] with similarity of feature representation learned from other sources. However, this paper specifically analyzes the problem in a principle way. I consider this work is generally novel. <sep> [1] X. Wang and A. Gupta. Videos as Space-Time Region Graphs. ECCV 2018. <sep> [2] N. Wang, Y. Zhang, Z. Li , Y. Fu, W, Liu, Y. Jiang. Pixel2Mesh: Generating 3D Mesh Models from Single RGB Images. ECCV 2018. <sep> Significance: <sep> The significance lies mostly in motivation and the proposed GResNet. <sep> Motivation: This paper studies the phenomenal that GNNs tends to not respond to the input data when certain depth of a network is reached, which the authors called as suspended animation limit. Studying problem is fundamental and important, and also unique since different with CNNs where data are independent, the data instance within GNNs are highly correlated. <sep> GResNet: Given the differences, and within the Theorem 2 where the residual formulation of CNNs does not apply to GNNs, the paper also proposes several new formulations, i.e., in figure 2, where the suspension is avoidable and the performance under the same experimental settings is obviously boosted. <sep> Main issues: <sep> My major concern to this work lies in the assumption used throughout section 3 and 4. At the beginning of Section 3.2, the authors assume that W is identity. Since X is assumed to be column-wise normalized, the nonlinearity is removable. However, this is not true in real cases: W is actually learnable and not bounded. When W is learned to be negative, Relu layer is not removable, and the behavior of the network will be completely different with what the paper depicted. Indeed, GNNs contain stacked linear+nonlinear functions, which cannot be simplified as a linear Markov chain. It is analogy to CNNs, which is not possibly be simplified as a group of average poolings. <sep> Minor issues: <sep> 1. I agree that under the assumption, eq. (11) shows that the differences between the learned representations are not discriminative, however the claim ""majority of the nodes are of very small degrees"" is not justified and only apply to the internet topology in Faloutsos et al. (1999). <sep> 2. I feel the ""Raw Feature Coding"" and the ""Network Degree Distribution"" are sort of repetitive, and the eq. (11) is eq. (12) at the stationary point.","This paper studies the ""suspended animation limit"" of various graph neural networks (GNNs) and provides some theoretical analysis to explain its cause. To overcome the limitation, the authors propose Graph Residual Network (GRESNET) framework to involve nodes' raw features or intermediate representations throughout the graph for all the model layers. The main concern of the reviewers is: the assumption made for theoretical analysis that the fully connected layer is identical mapping is too stringent. The paper does not gather sufficient support from the reviewers to merit acceptance, even after author response and reviewer discussion. I thus recommend reject."
"abstract | rating_summary | suggestion  ==>  ==> The authors propose a new univariate time series analysis framework called RISE, which unifies the existing work on adapting RNNs to irregular time series. Building on top of the RISE, they propose a modification called DISE in which the algorithm skips the intervals without any observations. In that sense, DISE can be considered a marked point process analysis algorithm. They quantify the performance of the RISE and DISE on two datasets. <sep> Table 1 is a valuable summary of the existing efforts on adapting RNNs to irregular time series. However, the paper overstates its scope. This work only studies RNNs. There are many alternatives for analysis of irregular time series, including Gaussian processes [1], ordinary differential equations [2], convolutional neural networks [3], neural point processes and spiking neural networks [4]. These references are only notable examples from each category and there are many more. <sep> A major limitation of this paper is that it only applies to the univariate time series. Usually in domains such as healthcare, almost always different variables have different missing rates. Multiple works address the multivariate case, see [5] for example. <sep> The main dataset used for evaluation is the Glucose dataset. However, this dataset is a peculiar and very specific dataset because its goal is to predict glucose level for type-1 diabetics only base on the past glucose measurements. While this task is meaningful, human biology states that forecasting glucose levels without knowledge of insulin injection or carbohydrate consumption is an extremely difficult task. In this setting, the most useful data is the latest data point. This dataset is an extreme forecasting task in absence of major predictors and I do not think it should be the primary dataset for evaluation of a new algorithm. <sep> Finally, the main idea of skipping the intervals without measurements is not very novel given the existing literature on neural point processes. Also, it is not enough contribution for a full conference paper. <sep> [1] Shukla, Marlin (2019) Interpolation-Prediction Networks for Irregularly Sampled Time Series. In *CONF*. <sep> [2] Chen, T. Q., Rubanova, Y., Bettencourt, J., & Duvenaud, D. K. (2018). Neural ordinary differential equations. In NeurIPS. <sep> [3] Nguyen, P., Tran, T., Wickramasinghe, N., & Venkatesh, S. (2016). Deepr: a convolutional net for medical records. IEEE BHI. <sep> [4] Islam, K. T., Shelton, C. R., Casse, J. I., & Wetzel, R. (2017). Marked point process for severity of illness assessment. In Machine Learning for Healthcare Conference. <sep> [5] Che, Z., Purushotham, S., Li, G., Jiang, B., & Liu, Y. (2018). Hierarchical deep generative models for multi-rate multivariate time series. In ICML.","The paper attacks the important problem of learning time series models with missing data and proposes two learning frameworks, RISE and DISE, for this problem. The reviewers had several concerns about the paper and experimental setup and agree that this paper is not yet ready for publication. Please pay careful attention to the reviewer comments and particularly address the comments related to experimental design, clarity, and references to prior work while editing the paper."
"abstract | weakness | rebuttal_process | suggestion  ==> The paper proposes a method to address a known problem for unsupervised disentangling methods that penalises total correlation, namely that while the total correlation of the samples from q(z) (denoted TC(z)) are encouraged to be small, the total correlation of the means of q(z|x) (denoted TC(mu)), used as the disentangled representation in practice, is not necessarily small and can increase with regularisation strength. <sep> In the introduction, I think that the statement ""they concluded pessimistically that it is fundamentally impossible to learn a disentangled representation in an unsupervised setting"" is a wrong interpretation of Locatello et al. They show that optimising marginal likelihood in a generative model (such as a VAE) cannot achieve disentangling without any inductive biases in the model. But there inductive biases in the models used by disentangling methods, along with the loss (that is a variant of the ELBO and not the marginal likelihood), that allow disentangling in practice. There are also theoretical works such as [1] that explain this behaviour. <sep> The theoretical contribution of the paper is Theorem 1, that claims to show the existence of distributions with arbitrarily large TC(mu) but with small, bounded TC(z). Indeed the proof shows that TC(z) is bounded by C, but looking at Appendix A it seems as though C is a function of c1,c2 and R that is used to define p(z|mu), and is lower bounded by (2pi)^{-D/2} (a constant, which confusingly, is also denoted by C in the appendix). It appears necessary to have another line that mentions how small C can be chosen to be via choice of c1,c2,R. Also it seems as though the proof can be largely simplified by having sigma'_j(mu)=c_1 if |mu|<R and sigma'_j(mu)=c_4/|mu| if |mu|>R, removing free parameters l,c_2,c_3. <sep> The methodological contribution of the paper is to propose an extra regularisation term that penalises the variances of q(z|x). While this does introduce another hyperparameter to tune, it has the advantage of being simple to implement and having an intuitive explanation of how it can address the problem; if q(z|x) is encouraged to have smaller variance, the distribution of z will be encouraged to be closer to the distribution of mu, hence helping to address the disparity between TC(z) TC(mu). <sep> I like the simplicity of the idea, however the analysis is lacking in rigour. First of all, when comparing the different methods of estimating TC(z), it's not clear what the mathematical difference of MSS_0 and MSS_1 is. This should be explicitly stated so that one can understand the results in Figure 1. Regarding the following analysis, it's not clear why the off diagonal elements of the cube (i.e. q(z^(i)_k|n^(j)) when i != j ) should be very small compared to the diagonal elements. For example, it could be the case that z^(i) and z^(j) are close to one another, in which case the (i,j)th entries will be non-negligible compared to the diagonal. Hence the analysis is difficult to accept. Also the claim that MSS and MWS prefer to shut down latent dims should be verified by experiments. Further, it's unclear why this is undesirable from a disentangling point of view. Of course we don't want to use fewer number of latent dimensions than the number of ground truth factors, but also we don't want to use more latent dimensions. Also the at the bottom of page 5 is a Gaussian with a correlated covariance matrix, and it's claimed that its TC can be arbitrarily large, but surely this is fixed? Finally the authors list lots of reasons why not to use MSS, but there is no discussion about the density-ratio trick method for estimating TC. For example, it is known that this method suffers underestimates of TC (c.f. Kim & Mnih) - it has its own issues, that can be arguably more severe than MSS. This analysis needs a lot more explanation and rigour. <sep> The experimental results are very weak and sparse, that is nowhere near enough to give a convincing case for the newly proposed method. The method has only been trained on dsprites and 3d shapes, with three choices of beta and a single value of eta, and only estimates of TC(mu) and TC(z) are reported, with no evaluation of disentanglement performance. There is some evidence in the paper that the regularisation makes both TC(mu) and TC(z) close to 0 with eta=10, but it's unclear how this affects disentangling performance, and whether smaller values of eta can give a sweetspot. The experiments should cover a larger range of datasets, with evaluation on how different disentanglement metrics, TC(mu) and TC(z) change for different values of beta and eta, along with a comparison with other disentangling methods, especially DIP-VAE-1, that directly penalises correlation in mu (for an open source library that facilitates this, see e.g. github.com/google-research/disentanglement_lib). Even the authors acknowledge that ""the scale of our experiments is limited"", and it is clear that the paper is not yet ready for publication. <sep> Overall, the proposed idea is simple and easy to implement, which is the main advantage of the paper, but it is evident that the analysis and evaluation lacks rigour, hence the paper will need to undergo significant revision to be in a publishable state. <sep> [1] Rolinek, M., Zietlow, D. and Martius, G., Variational Autoencoders Pursue PCA Directions (by Accident). CVPR 2019. <sep> Minor typos/comments: <sep> Eqn(4): the ||.||_1 should be replaced by trace, since the term inside ||.||_1 is a covariance matrix (although it's diagonal). If A is a matrix, ||A||_1 is the maximum absolute column sum, which is different to what is meant by the paper, the trace (sum of diagonals). <sep> p3: Can MWS also be stated in the paper (or at least in the appendix) to make it self-contained? <sep> p4: followed <- follow, n_{m+1} <- n_{M+1} <sep> p6-7: Section 6.1 should be in a related work section, and not under the Experiment section. The point about modularity is a fair but known issue, closely related to the issue of axis alignment/unidentifiability (see e.g. Rolinek et al)","This paper highlights the problem of penalizing the total correlation of sampled latent variables for unsupervised learning of disentangled representations. Authors prove a theorem on how sample representations with bounded total correlation may have arbitrarily large total correlation when computed with the underlying mean. As a fix, the authors propose RTC-VAE method that penalizes total covariance of sampled latent variables. <sep> R2 appreciated the simplicity of the idea, making it easy to understand and implement, but raises serious concerns on empirical evaluation of the method. Specifically, very limited datasets (initially dsprites and 3d shapes) and with no evaluation of disentanglement performance and no comparison against other disentangling methods like DIP-VAE-1. While the authors added another dataset (3d face) in their revised versions, the concerns about disentanglement performance evaluation and its comparison against baselines remained as before, and R2 was not convinced to raise the initial score. <sep> Similarly, while R1 and R3 appreciate author's response, they believe the response was not convincing enough for them, and maintained their initial ratings. <sep> Overall, the submission has room for improvement toward a clear evaluation of the proposed method against related baselines."
"abstract | weakness | decision  ==> The paper investigates a setting in which the observation function changes while the underlying environmental dynamic stays the same. <sep> In order to re-use the policy which was trained on the old observation function, they propose to learn a mapping function to map the new observations to the old ones. <sep> I believe the work is interesting as generalization and reducing the sample complexity of learning policies, for example through re-use of old policies, is of high current interest.  However, I believe this paper requires more work to show the feasibility of the proposed approach. In particular: <sep> - The proposed method has the problem that matching is done 'locally' and without any guarantee that the mapping function will converge to the correct mapping. This is not a problem of the method itself but of the challenging problem setup. The authors discuss this and propose two approaches to alleviate this. However, no experimental evidence is shown whether and to what extend this prevents wrong local minima, as I don't believe the mountain car has such problems? <sep> - Evaluation is only done on the mountain-car experiment, which is not sufficient to show feasibility in general <sep> - The learning curves in the experiment seem highly volatile and unstable. I'm not sure why this is the case, maybe wrong hyperparameters or a bug in the code? <sep> - Lastly, the paper is over the recommended limit of 8 pages and could, in my opinion, made more concise at many points to shorten it and also make it easier to read. <sep> In summary: I believe this interesting work, but requires more experiments in different environments and additional ablation studies to show the feasibility of the proposed method. Making the writing more concise would, in my opinion, not only shorten the paper but also make it easier to read.","The submission proposes to improve generalization in RL environments, by addressing the scenario where the observations change even though the underlying environment dynamics do not change. The authors address this by learning an adaptation function which maps back to the original representation. The approach is empirically evaluated on the Mountain Car domain. <sep> The reviewers were unanimously unimpressed with the experiments, the baselines, and the results. While they agree that the problem is well-motivated, they requested additional evidence that the method works as described and that a simpler approach such as fine-tuning would not be sufficient. <sep> The recommendation is to reject the paper at this time."
"abstract | strength | weakness | rebuttal_process | misc | decision  ==> Summary: This paper proposes an uncertainty measure called an implied loss. The authors suggest that it is a simple way to quantify the uncertainty of the model. It is suggested that ""Low implied loss (uncertainty) means a high probability of correct classification on the test set."". They suggest that the analysis of an implied loss justifies the maximum confidence value of softmax-cross entropy. They also extend to evaluate Top-k uncertainty (the uncertainty whether our prediction is in the Top-5 maximum values of our confidence score or not). <sep> ======================================================== <sep> Clarity: <sep> I found that this paper content does not seem to be difficult mathematically, however, it is difficult to follow the paper and here I list several parts that can potentially be improved: <sep> 1. INTRO: the sentence ""Our implied loss interpretation justifies both methods, since we demonstrate that ""both these quantities"" are uncertainty measures."". What is both quantities here, the maximum softmax probability and something? <sep> 2. INTRO: first contribution, accurate estimates of the probability that the classification of the model on a test set is correct. What do you mean by this sentence? I couldn't see the accuracy of the estimation in Table 2. <sep> 3. second contribution in INTRO: what is the meaning of ""a consistent manner"" here. If it means ""successfully"", is there a case that your method fail? It would be nice to be precise when describing the contribution. Figure 2 is more like a small example but can be considered difficult to convince to the readers about the capability of the proposed method. <sep> 4. Figure 1 can be much improved. I found the caption is hard to understand. Loss (y-axis) may be written as Kullback-Leibler loss to be more precise in this context (if I understood correctly). <sep> minor comment: Figure 1(a) [colon missing], since Figure 1(b) has "":"" right after. The authors made an effort to explain the color in the caption of Figure 1(b) but the explanation of (yellow) is missing. The explanation of how to train a network to get Figure 1(a) seems to be missing. It seems -log(f_(1)) should be -log(f^{sort}_(1)). Finally, I'm not sure what is P(Correct|x). Is this histogram suggested that when the U_1 is large and the P(Correct|x) is small, we get a correct prediction, or maybe this means the ratio of correct prediction under the value of loss (histogram)? I think it would help the reader to make it more concise. In the description in page 4 -\\log(p_{max}) seems never define before, was it a typo? <sep> 5.1 First page, last sentence: what do you mean by the current setting. Before this point there is no explanation about the problem setting, only we are interested in quantifying an uncertainty measure of the networks. <sep> 5.2 First page, last sentence:  I'm not familiar with Bayes factors, is the last sentence your contribution or it's the finding of the existing work, if it's the latter one, it would be nice to cite them. However, I found this sentence a bit vague: Bayes factor more informative (not sure what is the definition of informative here) better than Brier score under the situation where prob of correct classification is high (is this means high accuracy on the test set?). <sep> 6. PRIOR-WORK: Although the authors suggested many existing works, it would be highly useful if the authors discuss the relationship between existing works and their proposed work, e.g., where to put your work in the literature. And since they proposed an uncertainty function, it would be nice to see a few definitions of uncertainty existing works described (doesn't need to be mathematical formulation, I think just an intuitive explanation is sufficient). <sep> 7. I am confused with the definition of an implied loss. It is first defined in page 3 with a fixed k (as 1) as a loss where the prediction is correct but in Eq. (4), it looks like a set with one element where y is the maximum prediction score, not a correct label. Then there is U_k(x), if k!=1, is this an implied loss? Although in (5) it is a real number, not a set anymore, I read this paper and took it as a ""yes U_k(x) is also an implied loss"". Also it would be better to define Kullback-Leibler here to be concise and kind to the readers. Then in def 4.1, it's mentioned that the uncertainty measure U_k(x) is an implied loss if the event has high expected loss, does that mean if the event has low expected loss, it is not an implied loss? My opinion is that the authors may use a definition environment and define precisely what is an implied loss. For example, given ""\\ell: X x Y \\to R, a correct label y \\in Y, and an integer k <= K (number of classes), an implied loss is defined as"" to avoid confusion. <sep> 8. Def 4.1: U(x) without subscript is undefined (perhaps U_k(x)?). What is an element of the set S^\\eps_k? If it is a set then what is the meaning of the event S has a high expected loss? Does the definition of implied loss after Eq. (6) and the definition of implied loss in Eq. (5) identical when it is Kullback-Leibler loss? <sep> 9. Theorem 4.2: S^\\eps seems to be undefined without k. Moreover, how to interpret the bound in (7), it would be nicer to explain the bound after stating the theorem. It is only an inequality that says the left-hand side is smaller or equal to the right-hand side. And does (7) hold for any y? And how tight is the bound? <sep> 10. Remark 4.3: what is e_k? what is k here if k in U is set to one? And the name of the remark, how to interpret (8) as ""Neural networks are always overconfident?"" Is this about neural networks or this apply to any function? <sep> 11. Sec 5: Figure 6 is not in the main body but the appendix... If this a mistake (and the paper is supposed to be 9 pages without ref.) or it is supposed to be in the appendix? If It's in the appendix, it would be better to mention that this figure is in the appendix. <sep> 12. Sec5: since Bayes factor is highly used in this paper to motivate the use of the measure, I don't think it is a good idea to put the explanation of Bayes factor in the appendix, i.e., it is impossible to understand this paper without knowing Bayes factor. <sep> 13: Fig 6: I think it is better and kinder to use U_1, U_5 in the legend of the plot instead of f. What is the model entropy? <sep> 14. Table 1: why there is ""-"" in CIFAR-10, it is better to clarify it in the paper (or maybe I missed it). I am not sure how to interpret the result, if the higher the better, does that mean the Loss is great?, I'm confused with the experimental results. <sep> 15. Before the beginning of 6.2: Tables 7 and 6 are in the appendix and we should state clearly it is in the appendix. <sep> ======================================================== <sep> Comments: <sep> This paper lacks of clarity and difficult to understand. Although it is claimed to be better than existing measure, I am not convinced about that despite many experiments were conducted unfortunately. <sep> For the criticism of using the maximum confidence of the softmax score from the softmax-cross entropy loss may not quantify the uncertainty, it is known theoretically that the score of softmax-cross entropy corresponds to p(y|x) if our prediction function achieve the global minimizer this loss function and our function class to be considered is all measurable functions (Zhang, JMLR2004: Statistical Analysis of Some Multi-Category Large Margin Classification Method). For other losses, see Williamson+ JMLR2016: Composite Multiclass Losses. However, it may not be accurate empirically when we use a deep network as it is reported in Guo+, 2017. Thus, one direction is to do post-processing or finding a way to modify a network. For U_1(k), I feel that it should suffer from the same problem as using maximum confidence score of softmax. Extending to top-k may have a good point when discussing about uncertainty and I believe it is good to explore that direction. For experiments, I would like to know how many trials did the authors run the experiment? and it would be helpful to see the standard deviation of the reported value. I believe this paper can still be improved a lot. For these reasons, I vote to reject this paper this time. <sep> ======================================================== <sep> Minor comments: <sep> there exists the writing convention of ""Top 5"", ""top 5"", ""top5"". It's better to pick one way to describe it if there is no reason to make it different. <sep> ======================================================== <sep> After the rebuttal: <sep> I have read the rebuttal. <sep> I appreciate the authors' effort to modify the paper. <sep> Also, please let me state that I totally agree that the problem the authors try to solve is indeed important and relevant for using machine learning in the real-world. <sep> I feel that the structure of the modified version is better than the first version. It is a nice to include the explanation of the Bayes factor in the main body. Appendix C is also very useful for everyone to understand the Bayes factor. <sep> As the author requested, I have read through the whole revised paper (including the appendix) carefully . I am aware of the positive sides of this paper. For example, it is interesting that we can find wrongly labeled data. Utilizing the uncertainty information for several applications. However, I found that the paper still requires a lot of modifications. The authors have modified many of my concerns, but still several of them were not addressed. I also emphasized the comments for parts that are unrelated to clarity (please see below). For these reasons, I decide not to change my score. <sep> Below are my comments after reading the rebuttal (which some of them may be overlapped with the issues that were not addressed in the revised version). <sep> ============================ <sep> General comments: <sep> 1. Although the work is about tackling the empirical confidence estimation problem, Theorem 3.4 and Eq. (5) are providing insights about the population, not finite data points for empirical estimation. If we focus on the population case, it is known that the minimizer of the softmax cross-entropy loss must be a conditional probability p(y|x). Thus, it is natural that as we minimize such a loss, the probability of correct classification must be high, since we can pick the best choice for classification, i.e., argmax of p(y|x). But the most challenging part of the research in this direction is that, although the theory suggests we can get nice confidence information (in population), when we use the deep neural networks, the quality of confidence estimation can be very bad (overconfidence in empirical estimation) compared with the high accuracy we can achieve. As a result, a finer theory that can quantify the quality of confidence estimate for the finite sample case is highly needed, but I think Theorem 3.4 fails to capture this. I am aware that this theorem only concerns the KL loss. I believe that even only for KL-loss, the result can be significant if we discuss a finer theory for empirical estimation. <sep> 2. Regarding the Remark 3.5 (Neural networks are always overconfident), in my understanding, the result has no relationship with neural networks at all since it is true regardless of the hypothesis class of interest (e.g., linear models, kernel models, deep networks). This is because it is simply the definition of a loss function (and the result in the paper focus on KL loss, but I believe we can derive for many other losses). We know that the quality of confidence estimation of simple models can be better although the accuracy is worse. Thus, if the objective is to visualize the problem of neural networks, Remark 3.5 does not seem to help and adding neural networks in the remark title can be misleading. Thus, the implication of Remark 3.5 is insufficient to state that Neural networks are always overconfident. <sep> 3. What is the advantage of an implied loss? It seems the paper has two separate stories, the first one is implied loss (Sec. 3) and then move on to Bayes factor (Sec.4). Then, there is an adversarial detection problem using the gradient norm in the last experiment. From Sec. 4, the discussion about implied loss is very limited and if I understand correctly, the −log⁡pmax and −log∑p1:5 (the latter seems to require a superscript sort) are the implied loss, which does not seem to have the clear advantages over other methods. My impression is that the contributions of this paper are unclear and I do not know what is the main point of this paper. While the abstract dedicates most space to highlight the implied loss (nothing about Bayes factor), the conclusion dedicates most space to highlight the Bayes factor. Improving the connection between two parts may help to signify the contributions. Despite all that, I do like the idea of introducing the Bayes factor in this paper. <sep> ============================ <sep> Clarity: <sep> 1. Most importantly, I think the clear and solid definition of implied loss is missing. <sep> According to Definition 3.1, ""The uncertainty measure Uk(x) is an implied loss if the event Skϵ has high expected loss"". I believe the implied loss is one of the most important contributions of this paper. I am not sure what does it means by ""if the event Skϵ has high expected loss"" How can we define ""high expected loss""?. I tried to read the paper many times to understand what exactly is the implied loss, and what is the scope of implied loss (what is and what is not an implied loss). <sep> 2. Following my first issue on clarity, what is the definition of uncertainty measure in this paper? According to the paper, it is defined roughly as Uk, whose statistics allow us to better estimate pk. And in the abstract, it is emphasized that if uncertainty measure is an implied loss, then low uncertainty means a high probability of correct classification. Is there any uncertainty measure that is not implied loss? Also in the introduction, it seems that both softmax and the model entropy are uncertainty measures, are they implied losses? Is MC-dropout an uncertainty measure or even an implied loss in this context? <sep> 3. Eq. (11) left side: I think it is useful to the expectation with respect to which variable, I believe it is B. As a result, is it a typo to have Yi instead of Bi on the left-hand side of (11)? <sep> 4. Example 3.3: If we set k=1 according to the definition of the implied loss at the last sentence of this example. Will it contradict U1(x) defined just right before that sentence? Because yw will become the 2-nd ranked label, which I feel it is different from U1(x) defined in Example 3.3 <sep> 5. I think it is better to clearly state that the figures/tables are in the appendix when referring to them from the main body. I saw the authors refer to Table 5 in Sec. 4.1, Figure 5 in Sec. 4.2 and Tables 6 and 7 in Sec. 4.3. All of them are in the appendix. And it seems some of them are highly needed. For example, the authors said that ""by fine graining the bins we can capture relatively small ... on the order of 20"" (before Sec. 4.3), then suggested the reader to see Figure 5. I feel Figure 5 must be included in the main body because it is hard to understand that without seeing the figure. <sep> 6. I think all figures that have f(1) (Figures 1a, 1b, and 5) must have a superscript sort for all of them. Otherwise, it is wrong. <sep> 7. Kullback-Leibler loss is used extensively here without definition. It is important to clarify the clear definition of it. U1(x) also used extensively for the KL loss case and non-KL loss cases. This can make the paper hard to read. I suggest using U1KL(x) when referring to the uncertainty measure with respect to the KL-loss. <sep> 8. I saw −log⁡(pmax),−log⁡(f(1)),−log⁡(f1sort),U1. Are these all refer to the same thing? And I found that sometimes the argument (x) is ignored in the paper sometimes it doesn't in a quite random way. If they are the same, it would be nice to unify them. <sep> 9. The caption of figure 2(b) is uninformative. The authors may consider improving it. <sep> 10. It would be very helpful to add the implication or interpretation of the theoretical results to help the readers understand the intuition of the proven results. For example, how does Eq. (3) implies that when small uncertainty implies high chance of correct classification. <sep> ============================ <sep> Minor typos: <sep> INTRO: ""uncertainly measures"" -> ""uncertainty measures"" <sep> 5.2: ""on-distribution"" -> ""in-distribution"" <sep> Conclusion: logpmax -> write clearly with $$ should be better <sep> Minor comments in the appendix: <sep> 1. How does Appendix A related to implied loss or Bayes factor in this paper? Did I miss something? <sep> 2. Figure 5: <sep> 2.1 y-axis: is it Bayes ratio or Bayes factor? It seems the Bayes ratio and Bayes factor to be a different thing. Even if it is the same in some literature, I think it's better to use the Bayes factor here for the consistency of this paper. <sep> 2.2 Caption: ""entropy"" -> ""model entropy"". <sep> 2.3 Caption: Is it mistakes or do you want to insist on using U1 and U5 in the caption but using different notions in the figure? (in figure, they are −log⁡(f(1)) and −log⁡(∑f(1:5))). <sep> 3. Appendix C: Eq. 16 and Eq. 19: I believe there is a typo. I think it should be BF(X|Y1),BF(X|Y2),BF(X|Y3), respectively. <sep> 4. Appendix C: Eq. 18-right: does it need to sum to 1 not 0.3+0.5+0.3 = 1.1?","The paper proposes to model uncertainty using expected Bayes factors, and empirically show that the proposed measure correlates well with the probability that the classification is correct. <sep> All the reviewers agreed that the idea of using Bayes factors for uncertainty estimation is an interesting approach. However, the reviewers also found the presentation a bit hard to follow. While the rebuttal addressed some of these concerns, there were still some remaining concerns (see R3's comments). <sep> I think this is a really promising direction of research and I appreciate the authors' efforts to revise the draft during the rebuttal (which led to some reviewers increasing the score). This is a borderline paper right now but I feel that the paper has the potential to turn into a great paper with another round of revision. I encourage the authors to revise the draft and resubmit to a different venue."
"abstract | weakness | rebuttal_process  ==> This paper proposed a method pGAN based on Generative Adversarial Networks to generate poisoning examples in order to degrade the performance of classifiers when trained on the poisoned training data. The authors evaluated pGAN on both synthetic datasets and commonly used MNIST and Fashion MNIST datasets in machine learning. <sep> The paper is self-contained and easy to read. My main concern is on the experiment results. The detailed questions are as follows: <sep> Q1: Has the authors tried more complicated datasets such as CIFAR-10 to evaluate the pGAN method? It would make the paper more convincing to add results on more complex datasets. <sep> Q2: Can the authors structure the experimental results with different sections? Currently it is just a single section which is difficult to read. <sep> Q3: The authors noticed that ""But, as we decrease the value of α, the distribution of red points shifts towards the region where both green and blue distributions overlap"". This observation is interesting as it finds that the poisoned input tends to lie on the overlap of two classes. But this can easily lead to a defense method: remove those training examples that are close to the other class. This defense mechanism can be used together with other sanitization approaches. So I would like to see how would pGAN perform in this case? <sep> Q4: The authors mentioned ""Comparison with existing poisoning attacks in the research literature is challenging: Optimal poisoning attacks as in Munoz-Gonzalez et al. (2017) are computationally very expensive for the size of the networks and datasets used in our experiments in Fig. 2."". <sep> However, I can not agree because you can simply generate poisoned data and train the neural networks on the poisoned data regardless of the underlying approach that is targeted in generating the poisoned data. This would be an effective baseline to compare. (Correct me if I am wrong here.) <sep> I will change my score if the authors can address my concerns here. <sep> ================================================================ <sep> Thanks for the rebuttal. I am more convinced now.","This paper proposes a GAN-based approach to producing poisons for neural networks. While the approach is interesting and appreciated by the reviewers, it is a legitimate and recurring criticism that the method is only demonstrated on very toy problems (MNIST and Fashion MNIST). During the rebuttal stage, the authors added results on CIFAR, although the results on CIFAR were not convincing enough to change the reviewer scores; the SOTA in GANs is sufficient to generate realistic images of cars and trucks (even at the ImageNet scale), while the demonstrated images are sufficiently far from the natural image distribution on CIFAR-10 that it is not clear whether the method benefits from using a GAN. It should be noted that a range of poisoning methods exist that can effectively target CIFAR, and SOTA methods (e.g., poison polytope attacks and backdoor attacks) can even target datasets like ImageNet and CelebA."
"abstract | strength | weakness | rebuttal_process | ac_disagreement | rebuttal_process | rating_summary | decision  ==> This paper introduces a Neural Architecture Search algorithm that attempts to solve the problems of the existing NAS only utilizing manually designed action space (not related to the performance). The paper proposes LaNAS which is based on an MCTS algorithm to partition the search space into tree nodes by the performance in the tree structure. The performance of the method is shown in the NASBench-101 dataset and Cifar-10 open domain search. <sep> I lean to reject this paper because (1) the motivation is not well justified by the experiments, (2) the comparison on NASBench-101 is not convincing, (3) some important explanation of the method is missing. <sep> Main arguments <sep> The main contribution of the paper is using a learned action space in MCTS rather than a manually designed MCTS algorithm for NAS. However, as far as I know, the MCTS approach for the NAS problem is not a standard solution for NAS (which is not proved to be practically useful in other people's papers) which diminishes the contribution of the improvements of MCTS in NAS. <sep> Lack of the main comparison. For the motivation of the proposed method, the authors mention the drawbacks of other NAS methods used fixed action space in their RL or MCTS module. However, the authors only show that using a learned action space in MCTS is better than a fixed MCTS algorithm in the experiments. What about using a learned action space in the RL module such as PPO in NAS comparing to the fixed one? <sep> The comparison of NASBench-101 is not convincing. The authors compared with the BO method and claimed that the method is 16.5x more efficient than BO. However, the recently released paper ""BANANAS: Bayesian Optimization with Neural Networks for Neural Architecture Search"" said that their method is 3.8x more efficient than the one proposed here, which is quite confusing. <sep> Some important explanation of the methods is missing. Throughout the paper, the method to sample from a leaf node is only mentioned in 3.3. However, the corresponding sampling method is unclear. The paper only mentions that MCMC has been adopted to sample from the target subspace. In my opinion, it is not so trivial to use MCMC here and should be elaborated in more detail. Otherwise, people cannot use it. <sep> As given in figure 3, if c is set to be a very small number, the search is similar to simply using a series of predictors and always samples the models with better-predicted accuracies. Is MCTS really useful here? To show the effectiveness of MCTS, it is recommended to experiment on different values of c. <sep> Results given in the upper row of Fig.5 is not useful. In practice, it is already painful to sample about 1000 models and train them for the Cifar-10 dataset. It is more useful to see how this method behaves in the range of (0,1000). However, in Fig.5, different methods are all overlapping in this range and hard to tell whether this method is better than other methods","This paper proposes an MCTS method for neural architecture search (NAS). Evaluations on NAS-Bench-101 and other datasets are promising. Unfortunately, no code is provided, which is very important in NAS to overcome the reproducibility crisis. <sep> Discussion: <sep> The authors were able to answer several questions of the reviewers. I also do not share the concern of AnonReviewer2 that MCTS hasn't been used for NAS before; in contrast, this appears to be a point in favor of the paper's novelty. However, the authors' reply concerning Bayesian optimization and the optimization of its acquisition function is strange: using the ConvNet-60K dataset with 1364 networks, it does not appear to make sense to use only 1% or even only 0.01% of the dataset size as a budget for optimizing the acquisition function. The reviewers stuck to their rating of 6,3,3. <sep> Overall, I therefore recommend rejection."
"weakness | decision  ==> This paper considers a regularization technique, derived from multi-task learning, where multiple models with some shared parameters are jointly trained to solve copies of the same task. The technique is well-motivated as an efficient alternative to ensemble learning. The method is validated for a BiLSTM NLP model, which is applied to several POS tagging and named entity recognition tasks. The power of the technique as a regularizer is also demonstrated in the case of highly noisy labels, surprisingly, even outperforming ensemble learning in this setting. <sep> Despite this, my inclination is to reject the paper because of the substantial overlap with previous work and the limited scope of experiments. <sep> My primary concern is that the proposed technique was previously introduced in [1]. This prior work is not acknowledged in the current paper. Perhaps it was overlooked because it is situated tightly in Multi-task Learning, whereas the present work is motivated mainly with respect to Ensembling. <sep> Although the general method was introduced previously, the paper does have some key experimental differences that would be interesting to see explored further. <sep> (1) The paper uses a hidden layer in the separate classification heads, whereas previous work only used a linear classifier. The intuition that more complex heads will yield more diverse models is clear, but it would be great to see experimental evidence that this complexity helps. The conclusion states that the computational overhead is ""infinitesimal""; does increasing the complexity of the classifier trade cost for performance? <sep> (2) This paper uses Eq. 3 to make predictions, whereas previous work found that this did not improve over simply using the best single prediction model, which makes prediction somewhat more efficient. Is there some experimental evidence that Eq. 3 leads to improvements? <sep> (3) This paper considers the comparison to ensembling, whereas previous work only considered comparisons to single task and standard multitask learning. Additional experiments showing the advantages over ensembling could make this extension a significant contribution. <sep> (4) This paper presents novel investigation of the regularization effects of the method, i.e., the resilience to noisy labels and the analysis of learned weight matrices. Is there a real problem where this resilience to noise will improve over ensembles, i.e., without randomly replacing labels? Such an experiment would make this point more compelling. Also, is there some underlying reason why the method outperforms ensembles in this case? Is it simply because the method is less expressive so cannot overfit? <sep> In effect, if the paper could clearly show that (1) or other practical extensions lead to improvements over ensembling in settings where ensembling is commonly used, or enable ensembling in settings where vanilla ensembling fails (i.e., the case of noisy labels), then it could be a substantial contribution. The current scope of the experiments is too limited to conclusively show these points. For example, the technique can be applied to any architecture, but the experiments in the paper are limited to a single architecture; and additional experiments with architectures and tasks that commonly use ensembling would make the experiments more compelling, ideally with comparisons to external results. <sep> Other minor comments: <sep> - It would be good to see the number of model parameters for easy comparison, especially in table 2 with different value of k. <sep> - It looks like the x and y axis labels are swapped in Figure 3; from the Figure it looks like STL gets higher accuracies. <sep> - Figure 2 should say epochs instead of iterations. <sep> [1] Meyerson, E. & Miikkulainen R. ""Pseudo-task Augmentation: From Deep Multitask Learning to Intratask Sharing---and Back"", ICML 2018.","One of the reviewers pointed out similarity to existing very recent work which would require significant reframing of the current paper. Hence, this work is below the bar at the moment."
"abstract | rating_summary | weakness | rebuttal_process | decision  ==> This paper proposes a reranking architecture with a LogicForm-to-NaturalLanguage preprocessing step for semantic parsing. The authors experiment their method on three datasets and get the state of the art results. <sep> The proposed method is natural. But using neural models to rank (or rerank) is a long-existing technique, regardless of the chosen parametrization of the reranking model. This paper chose BERT. See section-2.6 of this tutorial for more details about using neural models to rank: https://www.microsoft.com/en-us/research/uploads/prod/2017/06/INR-061-Mitra-neuralir-intro.pdf. <sep> Overall, I think the paper is not ready to publish for the following reasons. <sep> 1. The method relies much upon manual designs that seem hard to generalize. <sep> By converting the logic forms to natural languages, the authors can leverage paraphrase datasets and pre-train the critic as a paraphrase model. However, the way they convert the logic forms is different for each dataset and they have to manually design rules for each logic form. <sep> 2. It is not clear how certain experimental designs were made. <sep> The authors chose to not rerank if the candidates' scores are too low or high but close. Such choice and associated thresholds seem arbitrary: how were they actually found out? Were they tuned on a development set? How does the method work if the candidate with the highest score is always picked: in the end, this is what the model is supposed to learn, correct? <sep> Other designs include beam size, whether or not to use a pretrained model, etc. How were such decisions made? Tuned on a development set? <sep> 3. The results are not sound enough. Given the issued pointed out in 1 and 2, I am not sure if the results are really sound as the authors claimed. <sep> For example, what if the authors don't use a LogicForm-to-NaturalLanguage conversion? What is the result if we directly learn to match input and logic forms? <sep> Moreover, the authors better answer questions in 2 so I can gauge if their hyper-parameters were chosen in the principled ways. Once those are answered, a significant test had better be done since the improvement seems small. <sep> 4. Claiming Shaw et al. 2019 in table-3 as ``our methods'' is wrong. It is clear that Shaw et al. (2019) didn't experiment on OVERNIGHT dataset, but setting up the baseline on a dataset should not be classified as ``our method''. <sep> Moreover, I have some comments on the model and experiments. These are not weakness, but I think some work in this direction may help improve the paper. <sep> 1. The model architecture should be better justified. In its current form, the two arguments (input query and output sequence translated from a logic form) are interchangeable. Why so? Why isn't an asymmetric architecture more natural? How can the authors use a pair of logic forms as negative examples (in figure-2)? Why do the authors use the Quora dataset in particular? <sep> 2. The error analysis might be better to be a bit more quantitative. Its current form doesn't seem to give insight on how the proposed method really helps. What the authors can do is: you can sample some sentences from the test/development set and count how many comparative words are misused in the original model, among which how many are corrected by reranking.","This paper presents and evaluates a technique for semantic parsing, and in particular proposes a model to re-rank the candidates generated by beam search. The paper was reviewed by 3 experts and received Reject, Weak Reject, and Weak Reject opinions. The reviews identified strengths of the paper but also significant concerns, mostly centered around the experimental evaluation (including choice of datasets, lack of direct comparison to baselines, need for more methodical and quantitative analysis, need for additional analysis, etc.) and some questions about the design of the technical approach. The authors submitted responses that addressed some of these concerns, but indicated that additional experimentation would be needed to address all of them. In light of these reviews, we are not able to recommend acceptance at this time, but I hope authors use the detailed, constructive feedback to improve the paper for another venue."
"abstract | strength | weakness | suggestion  ==> This paper considers the problem of learning from medical data that is separated both horiontally (across different practices and centers) and vertically (by data type). The contribution is a ""confederated"" machine learning method that learns across these divides. The particular application considered here as means of illustration is that of fall prediction in the elderly. Specifically the authors investigate an ML approach to risk-stratifying elderly patients with respect their likelihood of falling in the next two years. <sep> The basic challenge addressed here is learning in the setting in which different data elements are available only at specific sites, and it is assumed that they do not share data. In addition, it is assumed there are multiple distinct sites that have data corresponding to the respective elementes. However, it is assumed that labels (target vectors) are shared across all sites. This setup is simulated using available data. A simple distributed training scheme is outlined. <sep> This is a potentially important problem worthy of study. I some major concerns with the present work, however. First, I do not think that *CONF* is really the best venue for this work. The machine learning component of this is quite straightforward; basically SGD is performed iteratively on parameters associated with the data types ""owned"" by the respective (simulated) sites. Updates are then averaged over these parameter subsets. This is perfectly reasonable, but not terribly novel. The presentation of this is also much longer than it needs to be for the *CONF* audience. I think this paper, in its current form, would be better suited for an audience more interested in clinical applications specifically (and I say this as someone quite appreciative of work on applied ML; it's just that the audience here will be more interested in methodological innovations.) <sep> With respect to clinical utility: Do we really need ML to tell us about risk of falls? I mean, if we were to ask the MD who had seen these patients to perform a simple stratification (perhaps on an ordinal scale), would they not likely be able to do so reasonably well? The authors mention something like this, discussing the 'clinical screening process' which involves asking about prior falls. This seems like a really strong baseline. The authors argue that this is time-consuming, <sep> In any case, is AUCPR an appropriate or useful metric here? In practice one would need to pick a threshold on which to act; perhaps a simulation that investigated doing so would provide a more meaningful evaluation. Although again a strong baseline here would probably be to ask physicians to risk stratify patients for interventions direclty (I appreciate that this would be a non-trivial experiment to run, but still). <sep> I also have a question regarding the simulation. I *think* the authors have randomly assigned patients to the respective simulated sites; is that right? This seems problematic because in practice patients would not be IID distributed in this way; sites would have their own patient populations which would affect the losses. This should be somehow taken into account in the simulation. <sep> Other comments <sep> --- <sep> - I think I am missing something in the notation here. Ysi is a 'binary label' but seems to vary across 'states' for an individual, is that right? Shouldn't this be constant for an individual? The paper states below that ""The output of the classifier is a binary variable indicating whether the beneficiary had a fall during the follow up period."" <sep> - Labels were derived from ICD codes; was there any effort to spot check these? I am always a bit concerned about deriving labels from ICD and trusting them. <sep> - As far as I understand from 2.1, the authors have not included features extracted from notes in the patient history; is that right? Why not? <sep> Smaller issues <sep> --- <sep> - I would strongly suggest numbering your equations. Also, suggest using \\text while in mathmode for superscripts like `diag'. <sep> - ""Step 1.."" --> ""Step 1."" (p3)"" <sep> - ""The parameter Θof f is randomly initialized"" --> missing space before ""of"" <sep> - On page 4: L(Xdiag, Xmed, Xlab, Θ) is written incorrectly. <sep> - page 4: ""Tstands""  missing space. <sep> - page 5: "" fallst."" --> ""falls."" <sep> - Appendix Tables 2 and 3 both contain the typo ""varaible"" (should be ""variable"") <sep> - In Appendex Table 2, I suggest reporting results with a consistent amount of precision, e.g., 0.002 --> 0.0020 here.","This manuscript proposes a strategy for fitting predictive models on data separated across nodes, with respect to both samples and features. <sep> The reviewers and AC agree that the problem studied is timely and interesting, and were impressed by the size and scope of the evaluation dataset (particularly for a medical application). However, reviewers were unconvinced about the novelty and clarity of the conceptual and empirical results. On the conceptual end, the AC also suggests that the authors look into closely related work on split learning (https://splitlearning.github.io/) which has also been applied to medical data settings."
"abstract | weakness | decision  ==> In this work, the authors studied solving the CMDP problem, in particular to model the constraint propagation using the idea of the backward value functions. Using the time-reversed probability property, they first established Bellman optimality conditions for the backward value function, which accounts for the tail expected return, conditioned on the final state. <sep> Utilizing  the notion of backward value function, they further use that to model the constraint in the CMDPs, and proposed several safe policy iteration algorithms for both cases with discrete actions and with continuous actions. Experiments show that this method has better constraint guarantees than the state-of-the-art algorithms, such as the Lyapunov approach. <sep> The idea of using backward value functions to model constraints in CMDP is interesting and so far I have not seen it in other places. The algorithms developed with this approach also appear to work reasonably well (especially in constraint guarantees) on benchmark domains such as gridworld and mujoco control tasks. The authors also provide several properties such as consistent feasibility and policy improvement, similar to the Lyapunov method,  and derive several versions of safe policy optimization algorithms. However, I found the intuition of using backward value function rather unclear. Unlike the Lyapunov function, which attempts to estimate a near-optimal ""remaining constraint budget"" w.r.t. CMDP constraint, what is the motivation behind using the backward value. Does the backward probability have any connections to occupation measures? Without similar motivations it is unclear how close the performance of the policy computed from this backward value approach with that of an optimal CMDP policy. Also unlike the Lyapunov approach in Chow'18, the consistent feasibility  property in this work appears to be more restricted as it is only limited to a slow policy update. Finally the experimental results look promising, but it would be great to also compare with other state-of-the-art results such as Lagrangian method, and conservative policy improvement (such as CPO).","The paper considers the setting of constrained MDPs and proposes using backward value functions to keep track of the constraints. <sep> All reviewers agreed that the idea of backward value functions is interesting, but there were a few technical concerns raised, and the reviewers remained unconvinced after the rebuttal. In particular, there were doubts whether the method actually makes sense for the considered problem (the backward VF averaging constraints over all trajectories, instead of only considering the current one), and a concern about insufficient baseline comparisons. <sep> I recommend rejection at this time, but encourage the authors to take the feedback into account, make the paper more crisp, and resubmit to a future venue."
"misc | rating_summary | rebuttal_process | rating_summary | misc | decision  ==>  ==> Summary: <sep> This paper performs an empirical evaluation of generalization by TD methods with neural nets as function approximators. To quantify generalization, the paper considers the change in the loss function at similar states to the one where the update rule is being applied (where ""similar"" is usually defined as nearby in time). It comes to a variety of conclusions including that TD(0) does not induce much generalization, that TD(0) does not induce as much generalization as supervised learning, and that the choice of optimizer and objective changes the behavior according to their generalization criteria in various ways. <sep> Decision: <sep> This paper should be rejected because (1) the motivation is unsubstantiated, (2) the main metrics used (""gain"" at nearby states and difference since last visit) are of questionable importance, and (3) the conclusions are often vague and not informative. <sep> Main argument: <sep> Motivation: <sep> - The paper motivates the need for an evaluation of ""gradient update generalization"" by claiming that it is related to sample complexity and brittleness of RL algorithms. While I agree that this is plausible, there is nothing empirical or theoretical to support this claim in the paper or in the references. This is a significant problem since this assumed connection underlies everything in the paper. <sep> - Also, it is this sort of generalization that differentiates the function approximation setting (where there are no convergence guarantees for TD without strong assumptions) from the tabular setting (where there are convergence and sample complexity guarantees). <sep> Metrics: <sep> - The main metric used is TD gain on temporally nearby states. The TD gain is defined as the reduction in the squared TD error at some state s' when an update is applied at some other state s. Note this metric does not capture all update generalization, but only update generalization as it effects the TD error. <sep> - It is not evident, nor supported by the paper, that improvements in this metric at nearby states necessarily improve performance of the algorithm. This is especially true when there is a tradeoff between improvement at very nearby vs. somewhat nearby states, it is not clear which behavior is preferable (and this behavior seems to occur in experiments). As a result there is no clear way to use this metric to determine which algorithms are preferable. <sep> - The other metric used in the paper is the change in value function at a state since the last time that state was sampled from the buffer. It is also not clear whether this measurement is necessarily important for the same reasons as above. <sep> Conclusions: <sep> - In general the results are presented in plots that do not give clear implications and are difficult to read. I understand that we cannot expect completely clean results on such empirical questions, but the results would be potentially much more convincing and clear if the hypotheses were clearly stated and then one plot could summarize the result with respect to each hypothesis. For example, the gain plots are difficult to compare and interpret clearly and the memorization plots do not give such clear results (eg. 3a and 3d look visually fairly similar). Another example is the comparisons between optimizers, which are fine to have as a specific point in one section, but do not need to be in every plot. <sep> - The main result claimed in the paper is that there is little generalization from TD(0) when compared to supervised learning. This seems to be born out by the difference between figure 1a and 2a, but the difference in scale makes it a bit difficult. It is also not clear how to quantify this result or what the implications are. <sep> - The plots are averaged across all states over all of training and all environments, while this is somewhat rationalized in figure 11, I am worried that this may be covering some additional complexity/ambiguity in the results. <sep> - The result about TD(lambda) seems to be born out by figures 3e and f and 5, but is also unsurprising since the objective explicitly averages across temporally nearby states. Again it is not clear how this temporal consistency of updates should be interpreted in terms of the goals of the RL algorithms. <sep> At a higher level, the paper feels like a solid preliminary set of experiments rather than a paper organized around a clear motivating idea with clear hypotheses to test. The results would become more interesting if the metrics used could be connected back (either empirically or theoretically) to an objective. For example, does generalization in the sense defined in the paper give better performance at value estimation or return maximization? Can these results be quantified in a more direct way than the plots presented in the paper? <sep> Additional feedback: <sep> - The plots all have different scales which makes them difficult to compare <sep> - Using ""distance"" to refer to the relative tilmestep of samples in the replay buffer is confusing (distance cannot be negative) <sep> - There are strange visual artifacts (horizontal lines) in Figures 3, 5, 10, 12, 13, and 14 <sep> - Sections 3.7 and 3.8 very briefly present results that seem to distract from the main thread of the paper <sep> - Using one network architecture across all experiments seems like it may have a significant impact on the results. I understand that the architecture chosen is standard and testing different architectures is time consuming, but making broad claims about the algorithms based on only one architecture is potentially dangerous. <sep> - I assume that the replay buffer is always being sampled uniformly, but I could not find this detail in the paper.","This paper received three reviews. R1 recommends Weak Reject, and identifies a variety of concerns about the motivation, presentation, clarity and soundness of results, and experimental design (e.g. choice of metrics). In a short review, R2 recommends Weak Accept, but indicates they are not an expert in this area. R3 also recommends Weak Accept, but identifies concerns also centering around clarity and completeness of the paper as well as some specific technical questions. In their response, authors address these issues, and have a constructive back-and-forth conversation with R1, who remains unconvinced about significance of the empirical results and thus the conclusion of the overall paper. After the discussion period, R3 indicated that they weakly favored acceptance but agreed that the paper had significant presentation issues and would not strongly advocate for it. R1 advocated for Reject, given the concerns identified in their reviews and followup comments. Given the split decision, the AC also read the paper. While the work clearly has merit, we agree with R1's comment that it is overall a ""potentially interesting idea, but the justification and presentation/quantification of results is not good enough in the submitted paper,"" and feel the paper really needs a revision and another round of peer review before publication."
"abstract | rating_summary | decision  ==>  ==> Summary <sep> ------------- <sep> The paper proposes an approach, based on persistent homology (PH), to preserve certain topological structures of the input in the latent representations learned by an autoencoder. This is realized via an additional (i.e., in addition to reconstruction) loss term (optimized over mini-batches) which requires differentiating through the PH computation. While this has been done before (e.g., Chen et al., Hofer et al.), the authors have certainly put an interesting spin on this. The theoretical part of the work deals with the issue of using mini-batches for PH computation and whether this computation is close to the computation on the full point cloud. Experiments and comparisons on multiple datasets are presented to demonstrate that the approach, e.g., preserves nesting relationships in the input. The paper is nicely written and the content is very well presented. There are questions here and there (see below), but I do think they can be answered. <sep> Major comments/remarks: <sep> ------------------------------------- <sep> My first question relates to the issue that the loss only incorporates 0-dim. information. The authors do remark that higher-dim. features can be included, but the results were similar. However, after thinking about this issue quite some time, I am curious if it is possible to obtain ""zero"" of the topological loss (so this term is perfectly optimized), but the encoder introduces, e.g., cavities in the data which were not present in the input (e.g., 1-dim. holes). <sep> Also, can you show formally (maybe this is trivial and I am not seeing it) that L_t = 0 would lead to 0 distance between the corresponding diagrams w.r.t. some common metric? A more formal treatment of the implications of the loss in Eq. (2) would certainly help. <sep> Another question that immediately comes to mind is whether the computation of VR PH in the input space (e.g., CIFAR 10) makes sense, as the authors rely on ||.||_2 if I understood this correctly. I would argue that the topology of the input is basically unknown, especially for images and computing Euclidean distances among images, or vectorized images, does not make sense. For the nice results on the SPHERES data set it does, as the spheres are defined exactly using ||.||_2. If the VR PH in 0-dim. of the input is enforced upon the representations in the AE bottleneck, but the input topology is not captured well, then you might be enforcing something that you possibly do not want. <sep> Apart from that, it is known that the Euclidean distance degenerates quickly in high dimensional spaces, e.g., <sep> Aggrawal et al. <sep> On the Surprising Behavior of Distance Metrics in High Dimensional Space <sep> Maybe this is also contributing to the fuzzy visualization of CIFAR-10 in Fig. 3 (apart from the low-dim. of the bottleneck)? <sep> Also, maybe the authors could work out (in greater detail) the differences between their results from Thm.1/2 and the results of Chazal et al., in ""Subsampling Methods for Persistent Homology"". In my point of view, the results in the paper only hold if you would consider just a single batch, right? I mean, if the loss is computed from the batch, and a gradient update is performend, Z^{m} will changes (as the encoder changes as a result of the update), while the input does not. <sep> Finally, how were the KL divergence measures in Table 1 computed, as you need a density estimate of the input as well, not just for the representation space, right? Is this not a very crucial issue in the input space? If so, how reliable are the numbers presented for KL_{0.01},etc., given that the differences are sometimes extremely small. <sep> Minor comments <sep> ----------------------- <sep> Sec. 6: We presented a topological autoencoders -> We presented a topological autoencoder <sep> Overall, I think this is a nicely done paper, but with quite some question marks at many places. I do think this is always the case for something new, though, and actually a good thing.","This paper introduces a new variant of autoencoders with an topological loss term. <sep> The reviewers appreciated part of the paper and it is borderline. However, there are enough reservations to argue for it will be better for the paper to updated and submitted to next conference. <sep> Rejection is recommended."
"rating_summary | weakness | decision  ==> This paper studies hypergradient descent for precondition matrices. The goal is to learn an adaptable preconditioning for the task while training. Specifically, they take the gradient of the loss wrt the precondition matrix and update the precondition matrix to decrease the loss. They reparametrize the precondition matrix to ensure it is positive-definite and provide low-rank approximations and they provide cheap approximations for CNNs. <sep> Pros: <sep> - Figure 3 and 4 show promising results on cifar10 with a 9-layer cnn. <sep> - Figure 4 shows FOP can improve the accuracy for particular hyper-parameters. In cases improving by 2%. <sep> Cons: <sep> - Results on imagnet are not particularly good. The improvement is not significant. <sep> - Why positive-definite precondition matrix rather than positive-semi-definite? <sep> - Section 5: why is a degenerate precondition matrix bad? Fisher and Hessian for deep networks can be highly ill-conditioned. <sep> - Theo 1 seems to have errors. The term M_t in the update rule should show up in the bound on P as an exponential term in the first upper bound. <sep> - Figure 2: On mnist after 20 epochs the model has not reached 1% test error. Not clear if we can make any conclusions from this figure. <sep> After rebuttal: <sep> I keep my rating as weak reject. I reiterate that results look promising. However, the quality and accuracy of the writing are not acceptable for a paper on optimization. In my original review I only named a few problematic statements. I have to clarify that I do not think fixing only those few is enough. <sep> I am also not convinced about the proof of Theorem 1. Basically, section 6 looks very much like section 5 from Baydin et al. 2018. Even the wording is mostly the same. Theorem 5.1 in Baydin et al. 2018 is based on their update rule in Eq 6 in the form of alpha_t = alpha_{t-1} - beta nabla^T nabla, where alpha does not appear in the second term. However, in this paper, the update rule on line 7 in Algorithm 1 is M_t = M_{t-1} + rho * eps *(.) M_{t-1}, where M_t appears in the second term. Hence, the first bound in Theorem 1 in this paper cannot simply be the same as in Baydin et al. 2018.","This paper has been assessed by three reviewers who scored it as 3/3/3, and they did not increase their scores after the rebuttal. The main criticism lies in novelty of the paper, lack of justification for MM^T formulation, speed compared to gradient descent (i.e. theoretical analysis plus timing). Other concerns point to overlaps with Baydin et al. 2015 and the question about the validity of Theorem 1. On balance, this paper requires further work and it cannot be accepted to *CONF*2020."
"weakness | misc  ==>  ==> The paper considers the problem of sampling points from a constrained set in R^d where the constraints can only be accessed in a zero order fashion. They consider the specific situation where the constraints are a solution of a complicated PDE solver and hence the derivatives or specific functional forms of the constraint cannot be obtained. They repose the problem as sampling from a Gibbs distribution whose potential contains constraints as penalties in a Lagrangian fashion. They now wish to sample from the Gibbs distribution using Langevin diffusion.  The Langevin process requires a derivative of the gradient. The setting does not allow for that and therefore the authors propose two approaches - <sep> 1. Constructing the gradient from zeroth order entries of a gaussian smoothed potential (much like works of Nesterov et al on zero order optimization). <sep> 2. Using a parameteric function class (like an RKHS or a neural network) to learn a function which well approximates the gradient of the constraints as well given zeroth order constraint evaluations. <sep> For the latter, the authors propose two approaches. Hermite learning which directly approximates the error on the gradient as well as the function evaluation. However this involves a separate estimate of the gradient (via a zero order approach such as Gaussian smoothing). An alternative which seems more sample efficient is to do direct learning from zero order samples by penalizing first-order Taylor approximation between given points. <sep> The authors provide a theoretical analysis of the all the approaches and sub-approaches given above and further provide experiments to evaluate this on a problem from material design. I am not at all familiar with the domain of the experiments to comment on competing approaches. As far as I can see the authors also compare only their own proposed algorithms which are many. I will focus on the theoretical analysis which seems to form the bulk of the paper anyway. <sep> The theoretical analysis seems quite rigorous as it begins by first providing a basic guarantee for constrained langevin sampling when gradients are computed with error. The non error gradient part of this analysis has been established before and the authors mention the references appropriately. I have a couple of questions regarding the precise statements of the theorem that i will ask towards the end of the review. Overall its hard to comment on the tightness of the analysis as the non-error versions are also unclear of the tightness of the bounds. Nevertheless the bounds achieved do not look much worse than the non-error counterparts and are easy to implement. The rest of the bounds focus on achieving low error in approximation of the gradients in various settings. Overall the theory in this part seems very loose in terms of bounds as exponential factors in dimensions start to appear and in that regard seems quite preliminary but its hard to comment on whether its natural or can be improved. <sep> Overall the paper is a rigorous treatment of multiple components that would go into the problem of sampling zero order constrained sets and merges many ideas which can all be useful. Nevertheless the paper is a little lacking of novelty in the sense that it brings together many existing ideas and provides an analysis of the effect of bringing them together but none of the theory significantly improves over the existing theory. <sep> Some questions I have regarding the theorem statements <sep> Regarding theorem 1 SPLMC convergence  (and corollaries of the theorem) -  I find it surprising that there is no lower bound assumption on eta in terms of K - only an upper bound. This seems wrong particularly as the theorem as stated then allows eta to be set extremely small while K is finite, in which case there should be no convergence theorems at all. The condition on eta should be a theta(f (K)) for some f type of condition like in the second part of the theorem. I would suggest the authors to look into the theorem - or claify why this is the case. <sep> I am confused by the presentation of the Shi et al results as there is no penalty appearing for the approximation error due to an RKHS, only a finite sample penalty. Does the result assume that phi belongs to the function class of the RKHS in question? Probably yes and in that case that should be specified.","The paper is not overly well written and motivated. A guiding thread through the paper is often missing. Comparisons with constrained BO methods would have improved the paper as well as a more explicit link to multi-objective BO. It could have been interesting to evaluate the sensitivity w.r.t. the number of samples in the Monte Carlo estimate. What happens if the observations of the function are noisy? Is there a natural way to deal with this? <sep> Given that the paper is 10+ pages long, we expect a higher quality than an 8-pages paper (reviewing and submission guidelines)."
"strength | rebuttal_process | decision | suggestion  ==> ## Summary <sep> The authors propose use learned spatio-temporal filtering and a convolutional model to predict the behavior of a turbulent fluid flow. Turbulent flow is a very difficult problem, of great interest for engineers and physical scientists, so the topic of the paper is certainly compelling. <sep> This paper has several significant issues. The baselines the authors compare to are quite weak. Many of them were designed for other purposes, such as video prediction. The authors claim significant improvements (""64.2% reduction in divergence"") that I believe are calculated in a (unintentionally) misleading way. In fact, I think they missed the most important baseline---the ground truth simulation itself. Unless they can argue that the learned model is superior to the classical simulation in a significant way, it is hard to see the benefit of using something like TF-NET. <sep> ## Specific Comments <sep> * Page 2: The claim ""64.2% reduction in flow divergence, compared to the best baseline"" seems misleading. In Figure 5, the constrained TF-NET is as low as ~590 but the ResNet is between ~610 and ~810. I am guessing that the authors meant a 64.2% reduction in *difference* from the Target model, but this should be clarified. <sep> * Page 4: Is 'T' supposed to appear in the denominator of Equation 3? I would have expected this to be a normalizing factor resulting from integrating the filter G over the whole domain. <sep> * Page 4: I would expect filters used for LES are symmetric. Are any symmetry requirements being enforced on the learned filters? <sep> * Page 4: It's not clear what TF-NET is outputting. I would expect it to output the time derivative of the velocity field, but this should be spelled out explicitly. <sep> * Page 5: Since you are using incompressible Navier-Stokes equations, I don't think the Mach number is relevant. IIUC this is only relevant for the propogation of shock waves in compressible flows. <sep> * Page 7: It looks as if the Target model has significant divergence. Why is this? Should we expect this much divergence in the ground truth data? <sep> * Page 8: In homogeneous isotropic turbulent flow, the energy spectrum is governed by Kolmogorov, so we know what the spectrum should look like. Is there an analytic result for RB convection? If so, could you include that on the plot? <sep> * Page 8: The lines for U-Net and TF-NET are nearly indistinguishable in Figure 7. Could you change them? <sep> * Page 8: It seems like it would be worth including the ResNet in the energy spectrum plots as well. <sep> * What did the learned spatial and temporal filters look like? How do they compare to typical 'hand-chosen' filters? <sep> * IIUC, the training, validation and test data all have identical Rayleigh number. Does the learned model generalize to higher/lower energy? This seems critical to making this sort of model useful. <sep> * The paper doesn't describe a tuning process for any of the models. I would expect this to lead to significant improvement. In particular, how do the models' performance change as the weight on the divergence loss term is increased? <sep> ### Baselines <sep> * I think the objective should be to show that TF-NET is superior to the ground-truth method in some way. Can it match the results of the ground truth simulator but do it faster, or with fewer resources? <sep> * The authors say ""we compare our model with a series of state-of-the-art baselines for turbulent flow prediction."" However, most of these models are intended for video prediction or other tasks unrelated to turbulent flows. I wouldn't expect any of the baselines considered to do perform well in this context. <sep> * Since TF-NET gets the benefit of a loss related to divergence, I would have expected the other architectures to get the same treatment. In fact, without this extra loss term, the ResNet architecture is competitive with TF-NET.","The reviewers all agree that this is an interesting paper with good results. The authors' rebuttal response was very helpful. However, given the competitiveness of the submissions this year, the submission did not make it. We encourage the authors to resubmit the work including the new results obtained during the rebuttal."
"rating_summary | weakness  ==> The paper proposes new loss functions for quantization when the task of interest is maximum inner product search (MIPS). <sep> The paper is well written with clear descriptions, fairly comprehensive analysis and empirical exploration, and good results, and in general I agree that learning quantization so as to minimize quantization related errors on task at hand is a good strategy. <sep> Specific comments and suggestions for strengthening the paper are: <sep> a) The proposed loss function in (2) includes a weight function that serves as a proxy for the task objective of giving more emphasis to quantization errors on samples with larger inner product.  Instead, why not use the true task objective which for the MIPS task is stated in the Introduction section?  If this was considered please comment on reasons for not including / discussing this in the paper, otherwise perhaps this'll be good to discuss. <sep> b) Did the authors consider using a task dependent training data set which will capture both 'q' and 'x' distributions and potentially lead to even further improved quantization?  This has the disadvantage of making quantization dependent on query distribution, but in cases where such data is available it will be very valuable to know if incorporating data distributions in quantization process helps performance and to what extent. <sep> c) It will also be valuable to consider the closely related task of cosine distance based retrieval and comment on how that impacts the modifications of loss functions. <sep> d) The idea of learning quantization under objective of interest using observed data distribution has been studied earlier (e.g. see Marcheret et al., ""Optimal quantization and bit allocation for compressing large discriminative feature space transforms,"" ASRU 2009), perhaps worth citing as related work.","While there was some support for the ideas presented, the majority of reviewers felt that this submission is not ready for publication at *CONF* in its present form. <sep> Concerns were raised as to the generality of the approach, thoroughness of experiments, and clarity of the exposition."
"abstract | weakness | strength | rating_summary | rebuttal_process | decision  ==> This paper proposes two new embedding strategies for the task of knowledge graph completion, with special attention on generalizations that support hypergraphs.  The first method, HSimplE, learns an embedding for entities that directly contains multiple positional representations; these are shifted depending on the relation they're used in.  The second method, HypE, disentangles entity embeddings and positional convolutional filters, allowing stronger positional generalization.  Experiments on standard benchmarks demonstrate that the approaches work well. <sep> I think this paper should be accepted.  While the ideas are so simple that they border on being trivial generalizations of previous work, the paper is well written, and the results seem solid.  I think this is work that needs to be done, so I favor accepting it. <sep> On the positive side: <sep> * The ideas underlying HSimplE and HypE are natural and clear - we basically want to learn better embeddings for hypergraphs, and there are a couple of obvious ways to do that.  Both of these seem clear. <sep> * The experiments are nicely done, and show a consistent (if unsurprising) benefit to the approach. <sep> * The paper is well written and well situated in the literature. <sep> * I expect that other researchers in this area will be able to reproduce and build upon this work without any difficulty. <sep> On the negative side: <sep> * The ideas are obvious; the results are unsurprising; the paper lacks ""deep insight"".  It is a contribution in the sense that someone needed to do this work, and I'm glad that it's been done (and done well), but it's not earth-shattering. <sep> * It doesn't seem like this is quite the best version of this idea.  I really like the idea of HypE, but it seems strange that entity embeddings are modulated based on position *only*, without regard to relation -- that is, it seems like a given entity in position #2 might need to be represented in very different ways depending on which relation is being used.","The paper proposes two methods for link prediction in knowledge hypergraphs. The first method concatenates the embedding of all entities and relations in a hyperedge. The second method combines an entity embedding, a relation embedding, and a weighted convolution of positions. The authors demonstrate on two datasets (derived by the authors from Freebase), that the proposed methods work well compared to baselines. The paper proposes direct generalizations of knowledge graph approaches, and unfortunately does not yet provide a comprehensive coverage of the possible design space of the two proposed extensions. <sep> The authors should be commended for providing the source code for reproducibility. One of the reviewers (who was unfortunately also the most negative), was time pressed. Unfortunately, the discussion period was not used by the reviewers to respond to the authors' rebuttal of their concerns. <sep> Even discounting the most negative review, this paper is on the borderline, and given the large number of submissions to *CONF*, it unfortunately falls below the acceptance threshold in its current form."
"abstract | weakness | suggestion | misc | suggestion  ==> This paper sets up a new problem based on a continuous stream of potentially partially labelled data, which the authors call the Unsupervised Progressive Learning problem. The paper also introduces a new model designed to approach this problem, called the STAM architecture, with many concepts applied in a novel way. The STAM architecture is tested on example problems from the UPL problem. <sep> I find this paper to approach a problem that is not well studied in the literature, and it is well-written and easy to read. I am not aware of previous works that tackle this specific problem setup. Many previous works seem to slowly be converging to tackling this kind of problem, but this is the first work that directly tackles this realistic problem. It would be good if the authors made the benchmark (/how they are generated) public. <sep> The STAM architecture is an interesting way of tackling image classification, and it is refreshing to see a technique not dependent on neural networks. The experiments in the paper are extremely detailed, with good figures and ablation studies. <sep> I am recommending this paper be accepted. But I have one main question about this work: what is the memory cost of the STAM architecture? Many 1000s of LTM centroids are stored, what is the memory cost of this? Is this memory cost greater than the cost of just storing all the labelled inputs that have been seen? I would imagine that just replaying these stored labelled inputs (or just training a NN on these stored inputs) would provide extremely high accuracies for the datasets considered in this paper. <sep> I understand that STAM has potential beyond just replaying memory for potentially larger datasets; the authors also make the point that they do not believe that the brain works by replaying memory. However, I would then like to see one (or both) of the following tests: a run with much fewer LTM centroids (with the corresponding memory cost detailed explicitly), and/or a dataset where it is clear that the memory cost incurred by the STAM architecture is a better use of memory than just storing previous data. <sep> I would also argue that the brain *does* replay memory in some manner in order to learn. But that is a debate for another time! <sep> By fixing the LTM centroids that are learnt on previous data, the method is essentially freezing previous knowledge and then learning new knowledge by increasing model capacity. It would be interesting (as future work / other papers) to see an adopted version of eg Progressive Neural Networks and how that compares on the same benchmarks. <sep> I would also be interested to see, as future work, the uncertainty estimates of this new architecture and its robustness to adversarial examples. <sep> Finally, a couple of minor points: <sep> - Figure 4 is small and hard to read, particularly the left column. <sep> - The authors find (in Appendix E) that increasing the \\Delta hyperparameter leads to a growing number of LTM centroids. I wonder if that is still the case if \\theta is also increased along with \\Delta? <sep> - Typos: last line of Section 3.2: 'lowst'; a few places where \\citet{} and \\citep{} are incorrect.","The paper presents a semi-supervised data streaming approach. The proposed architecture is made of a layer-wise k-means structure (more specifically a epsilon-means approach, where the epsilon is adaptively defined from the distortion percentile). Each layer is associated a scope (patch dimensions); each patch of the image is associated its nearest cluster center (or a new cluster is created if needed); new cluster centers are adjusted to fit the examples (Short Term Memory); clusters that have been visited sufficiently many time are frozen (Long Term Memory). Each cluster is associated a label distribution from the labelled examples. The label for each new image is obtained by a vote of the clusters and layers. <sep> Some reviews raise some issues about the robustness of the approach, and its sensitivity w.r.t. hyper-parameters. Some claims (""the distribution associated to a class may change with time"") are not experimentally confirmed; it seems that in such a case, the LTM size might grow along time; a forgetting mechanism would then be needed to enforce the tractability of classification. <sep> Some claims (the mechanism is related to how animal learn) are debatable, as noted by Rev#1; see hippocampal replay. <sep> The area chair thinks that a main issue with the paper is that the Unsupervised Progressive Learning is considered to be a new setting (""none of the existing approaches in the literature are directly applicable to the UPL problem""), preventing the authors from comparing their results with baselines. <sep> However, after a short bibliographic search, some related approaches exist under another name: <sep> * Incremental Semi-supervised Learning on Streaming Data, Pattern Recognition 88, Li et al., 2018; <sep> * Incremental Semi-Supervised Learning from Streams for Object Classification, Chiotellis et al., 2018; <sep> * Online data stream classification with incremental semi-supervised learning, Loo et al., 2015. <sep> The above approaches seem able to at least accommodate the Uniform UPL scenario. I therefore encourage the authors to consider some of the above as baselines and provide a comparative validation of STAM."
"abstract | rebuttal_process | decision  ==> 1. Summary: The authors proposed to alleviate the generic response problem in open-domain dialog generation by generating a response to a prompt from a semantic latent vector. This vector needs to be located close to the latent vector of the corresponding prompt. To this end, they employ canonical correlation analysis and auto-encoder to learn the mapping from a sequence of text to a semantic latent vector. To model the variations and topic shifts that may happen in responses, they also use a separate intermediate vector in the auto-encoder of generating responses. <sep> 2. Overall assessment: While this paper is quite fun to read, it is not innovative enough and it lacks some critical experiments and error analysis to be accepted this time. I'll elaborate on these problems in my comments below. <sep> 3. Strengths: <sep> 3.1 This paper is well written. The motivation and the main idea are well explained and pretty easy to understand. Although there are some details missing, it doesn't prevent readers from understanding and enjoying this paper. <sep> 3.2 The use of human evaluation is a great plus. Subtle characteristics of text, such as readability, coherence, and specificity cannot be well justified by automatic evaluation. Human evaluation is a must for these aspects. It's great to see the authors include this in this paper. <sep> 4. Weakness and questions: <sep> 4.1 It seems the model in this paper can be connected to adversarial learning from some angles. It would be great to see such analysis in this paper. <sep> 4.2 The experiments in this paper do not look thorough enough. There are many more things should be included, such as error analysis, comparisons over different variations of the proposed model and so on. What if we remove Yu in both training and inference but keep the overall model the same? How important is Yu and what's its effect? Many such questions haven't been answered in this paper. <sep> 4.3 Embedding average cosine similarity seems to be too simple to be used for evaluation as it omits the contextual information and semantic meaning of a sentence. <sep> 4.4 It would be interesting to see the Dist-1 and Dist-2 scores of the gold standard responses. It's a good reference to let us know ho diversified the real responses are. <sep> 4.5 I'd like to see some explanation of how the raters rate specificity and coherence. What are the standards they use? What are the instructions the author give to them? <sep> 4.6 Are improvements significant in Table 1? It seems the proposed model is not performing very stably over different metrics. Some anlysis on this would be great.","This paper proposes a response generation approach that aims to tackle the generic response problem. The approach is learning a latent semantic space by maximizing the correlation between features extracted from prompts and responses. The reviewers were concerned about the lack of comparison with previous papers tackling the same problem, and did not change their decision (i.e., were not convinced) even after the rebuttal. Hence, I suggest a reject for this paper."
"weakness  ==> Summary: This paper introduces the task of using deep learning for auto-completion in UI design. The basic idea is that given a partially completed tree (representing the design state of the UI), the goal is to predict or ""autocomplete"" the final tree. The authors propose a transformer-based solution to the task, considering three variants: a vanilla approach where the tree is flattened to a sequence, a pointer-network style approach, and a recursive transformer. Preliminary experiments indicate that the recursive model performs best and that the task is reasonable difficulty. <sep> Assessment: Overall, this is a borderline paper, as the task is interesting and novel, but the presentation is lacking in technical detail and there is a lack of novelty on the modeling side. <sep> In particular, the authors spend a bulk of the paper describing the three different baselines they implement. However, despite the fact that most of the paper is dedicated to the explanation of these baselines. There is not sufficient detail to reproduce the models based on the paper alone. Indeed, without referencing the original Pointer Network and (and especially the) Transformer papers, it would not be possible to understand this paper at all. Further technical background and detail would drastically improve the paper. Moreover, it seems strange that significant space was used to give equations describing simple embedding lookups (i.e., matrix multiplications with one-hot vectors), but the basic technical foundations of Transformers were not adequately explained.  In addition, only the transformer baselines were considered, and it would seem natural to consider LSTM-based baselines, or some other related techniques.  In general, the space that was used to explain the Transformer baselines---which are essentially straightforward ways to adapt transformers to this task---could have been used to give more detail on the dataset. For example, one question is how often a single partial tree has multiple possible completions in the data. <sep> A major issue---mainly due to the lack of technical details and the lack of promise to provide code/data (unless I missed this)---is that the paper does not appear to be reproducible. Given the intent to have this be a new benchmark, ensuring reproducibility seems critical. <sep> Reasons to accept: <sep> - Interesting new application of GNNs <sep> Reasons to reject: <sep> - Incremental modeling contribution <sep> - Lack of sufficient technical detail on models and dataset <sep> - Does not appear to be reproducible","The paper introduces an interesting application of GNNs, but the reviewers find that the contribution is too limited and the motivation is too weak."
"abstract | rating_summary | decision  ==>  ==> This paper empirically demonstrates that DNNs can be trained to be identity mappings for small quantities of samples. It also demonstrates that for many parameterizations, these identity DNNs also have a small number of attractors, iterative fixed points, and can also learn short circular sequences of examples. <sep> The paper is well written and easy to understand, although the presentation could be improved a bit (see comments). Its contents aren't particularly novel in terms of ideas, but they investigate memorization and attractors much further than previous studies. <sep> In a way, the memorization results are unsurprising. We know that DNNs can memorize perfectly, including sequences, so it is natural that by increasing capacity, at some point they should be able to memorize entire images (in fact this is what Zhang et al. (2019)'s Figure 1 appears to be showing). <sep> The more novel and surprising aspect of this is that DNNs would learn such strong (and so few) attractor basins. The fact that deep autoencoders could have attractors centered on the training points has been postulated before (see [1]), but this work makes a stronger case for it. <sep> A crucial aspect that is missing from this paper in order for me to give in an accept is that there is very little about how this paper positions itself in the current literature. There could be much more discussion about related work, and much more discussion about the impacts of these findings. <sep> I have given this paper a 'weak reject' mark but I think with some work this paper could be of interest to many. To reiterate, I am unable to see anything wrong with this paper, but at the same time I am unable to see how impactful these findings are. <sep> Detailed comments: <sep> - It's interesting that DNNs can implement associative memory, but what is the cost of doing that? Should we be using that in practice? Since there is no sense of how costly the presented experiments are, it is hard to tell. <sep> - Again, these results are interesting, but after some time pondering about it, I can't really convince myself that knowing the results of this paper will be beneficial to future research. That being said, there are many areas of Machine Learning that I am unfamiliar with. It should be part of the paper to familiarize readers with areas where these results could be impactful. <sep> - ""the function interpolates the training images"" not sure what this means. Interpolation means making a prediction for a point `u` that is ""between"" two points `x,y` with known values <sep> - ""black and white"" should be ""grayscale"" if values are in [0,1] <sep> - Figure 2b is interesting, but I wonder what happens if e.g. a perturbed version of e.g. Example 6 is fed. Presumably since Example 6 is not an attractor (Jacbian with an eigeinvalue > 1), it should converge to another example. <sep> - Figure 2b's caption numbers, which say you use 1k example, to not correspond to numbers earlier in the text, which say you use 10k examples. <sep> - ""Since overparameterized autoencoders interpolate the training data"", again this is a fairly important assumption and it needs to be defined very clearly, because it could mean many things. <sep> - ""it is essential that we interpolate to numerical precision"", I don't think you are using the word ""interpolate"" correctly, do you mean ""inference""? ""train""? <sep> - Adam citation should be ""Adam: A Method for Stochastic Optimization, Diederik P. Kingma, Jimmy Ba"", not Goodfellow et al., RMSprop should also have a citation, Hinton et al. 2012 <sep> - ReLU citation should be ""Rectified linear units improve restricted Boltzmann machines, Nair & Hinton"", Leaky ReLU should be Maas et al 2013, SELU should be Klambauer et al. 2017. <sep> - The combination of section 3.1 and Figure 3 doesn't make it clear if models trained with Adam and RMSprop have weight decay or not. Can you clarify? <sep> - ""Note that a minimum width of 100 is needed to allow for interpolation."" Again I think you mean ""learning"" rather than ""interpolation"". <sep> - You say that you trained black and white images, but all the images of CIFAR10 in the figures are colored, including the inputs and outputs. Can you clarify why? <sep> - You might be interested in [2], which is much older work about perceptrons, but still relevant to what is studied here. <sep> - The linked supplemental material gives a 404 for me. I replicated the MNIST Figure 6 experiment. I was unable to replicate exactly your results but they were similar enough. In particular, the activation function choice seems to be critical. <sep> [1] The Potential Energy of an Autoencoder, Hanna Kamyshanska, Roland Memisevic <sep> [2] Basins of Attraction in a Perceptron-like Neural Network, Werner Krauth, Marc Mezard, Jean-Pierre Nadal","The paper shows that overparameterized autoencoders can be trained to memorize a small number of training samples, which can be retrieved via fixed point iteration. After rounds of discussion with the authors, the reviewers agree that the idea is interesting and overall quality of writing and experiments is reasonable, but they were skeptical regarding the significance of the finding and impact to the field and thus encourage studying the phenomenon further and resubmitting in a future conference. I thus recommend rejecting this submission for now."
"abstract | weakness | rebuttal_process | decision  ==> This paper proposes to apply the Neural ODE framework (Chen et al 2018) for image segmentation. The method relies on contour delineation through Level Sets. Since contour estimation requires to solve an ODE, this naturally allows to apply the work presented in (Chen et al 2018). The method is here applied in two segmentation tasks: kidney segmentation and salient object detection. <sep> The concept underlying the paper is interesting, and leverages on very recent advances in the field. The idea of learning the dynamics required to evolve segmentation contours is original and certainly appealing. Unfortunately the content of this work seems quite preliminary in terms of presentation and experiments. <sep> First, the methodology is only sketched, while the motivation underlying the modelling rationale is often missing. For example, it is not clear what is the difference between ""image evolution"" and ""contour evolution"" models, besides the implementation details, and what motivates the definition of these two different modelling approaches in parallel. <sep> Second, the experimental paradigm is controversial. The application on medical imaging is overly simplistic, as the authors do not consider the original 3D image stack, but rather the set of corresponding 2D slices modelled independently. The paper seems to ignore the large variety of body organ segmentation methods already available to the community, most of them working in 3D (e.g. [1-4]). The paper should necessarily compare with respect to these approaches and, even more importantly, with respect to standard level sets methods. <sep> From the practical perspective, the proposed method builds upon the results obtained with the UNet, and therefore is characterised by an additional computational burden. Given that the the training of neural ODE is not straightforward and computational expensive, the use of this model for achieving a tiny accuracy improvement seems overkill for this kind of application. Moreover, the segmentation accuracy is still computed slice-by-slice in the 2D images, and no information is available for the consistency of the reconstruction in 3D (regularity over the vertical axis). <sep> Finally, the results reported in Table 3 are not clear, why the metrics of the competing methods are approximated (e.g. ~ 0.7), while for the proposed methods are given up to the 3rd decimal term? <sep> [1] 3D Kidney Segmentation from Abdominal Images Using Spatial-Appearance Models. Fahmi Khalifa, Ahmed Soliman, Adel Elmaghraby, Georgy Gimel'farb, and Ayman El-Baz 1. <sep> [2] Automatic Detection and Segmentation of Kidneys in 3D CT Images Using Random Forests. Rémi Cuingnet, Raphael Prevost, David Lesage, Laurent D. Cohen, Benoit Mory, Roberto Ardon. Medical Image Computing and Computer-Assisted Intervention – MICCAI 2012: 15th International Conference, Nice, France, October 1-5, 2012, Proceedings, Part III <sep> [3] Multi-organ localization with cascaded global-to-local regression and shape prior. Medical image analysis. Gauriau, R., Cuingnet, R., Lesage, D., & Bloch, I. (2015), 23(1), 70-83. <sep> [4] Joint Classification-Regression Forests for Spatially Structured Multi-object Segmentation. Ben Glocker, Olivier Pauly, Ender Konukoglu, Antonio Criminisi. ECCV 2012 pp 870-881","This paper addresses the classic medial image segmentation by combining Neural Ordinary Differential Equations (NODEs) and the level set method. The proposed method is evaluated on kidney segmentation and salient object detection problems. Reviewer #1 provided a brief review concerning *CONF* is not the appropriate venue for this work. Reviewer #2 praises the underlying concept being interesting, while pointing out that the presentation and experiments of this work is not ready for publication yet. Reviewer #3 raises concerns on whether the methods are presented properly. The authors did not provide responses to any concerns. Given these concerns and overall negative rating (two weak reject and one reject), the AC recommends reject."
"abstract | weakness | decision  ==>  ==> Overall the paper is well written and organized and is an application of various paradigms to extract most relevant features for Arrhythmia Subtype classification. <sep> Plan to release the largest (claimed) public ECG data set of continuous raw signals for representation learning containing 11 thousand patients and 2 billion labelled beats. <sep> Our goal is to enable semi-supervised ECG models to be made as well as to discover unknown sub types of arrhythmia and anomalous ECG signal events. <sep> The stated intended goal though is the discovery of new  Arrhythmia Sub types (title), ""automated arrhythmia detection"" (1.1 Objective). <sep> A definition/example of a regular ECG signal, one presenting arrhythmia, various types of arrhythmia signal forms, would be welcome for the paper to fit this audience and be self-contained. <sep> This work suffers from many drawbacks: <sep> - A definition/example of a regular ECG signal, one presenting arrhythmia, various types of arrhythmia signal forms, would be welcome for the paper to fit this audience and be self-contained. <sep> - Given the intended audience in *CONF*, the application domains specificities are not well explained. Many concepts are not introduced clearly: <sep> o ECG models, sub types of arrhythmia, anomalous ECG signal events. <sep> o In related work section: <sep>  Paragraph 2: more ""leads"", RR interval, PR interval, QRS duration, induced myocardial ischemia. <sep>  Paragraph 3: single-lead wearable devices. <sep> o In 3 privacy concerns: heartbeats as biometrics section/ Paragraph 2: <sep>  alternative ways to sense ""cardiac motion"". <sep> o In 4 {companyname} 11k data set section: <sep>  Paragraph 2: ""third line exam"". <sep>  Paragraph 3: ""beats and rhythms"", these are fundamental to understanding graphical results provided. <sep> o In 5.1 quantitative evaluation section: <sep>  Paragraph 3: "" .. irregular RR intervals, no distinct P waves and usually variable intervals between two atrial activations. <sep>  Page 6:  "" .. high-level abnormality labels"" <sep> o 5.2 qualitative evaluation section <sep>  Paragraph 1:  "" .. has a dominant S wave in V1 lead."" <sep> o In 6 conclusion section: <sep>  "" Single-lead heart monitors"" . <sep> - Figures are barely decipherable o In Figure 6: <sep>  What are ESSV and ESV, btype and rtype. Could you give a list of known subtypes? <sep>  Is this for PVC data only? <sep> - Many paragraphs are not clear to us: <sep> o Introduction/Paragraph 2: <sep>  ""While cardiologists are able to see these differences, it is hard to conclude that they are 'real' by finding the same anomalous signal across multiple time points and patients without a data driven approach."" <sep> o In 3 privacy concerns: heartbeats as biometrics section/ Paragraph 3: <sep>  Contrary to 1), 3) & 4), is (2) "".. the expression of environmental variables on the heartbeat data is unique to the individual"", a limitation to ECG to being considered a biometric measure? <sep> o In 4 {companyname} 11k dataset section: <sep>  Paragraph 4:  ""we segment each patient record into segments of 220 +1 signal samples ( ~70 minutes). Care to explain the rationale? <sep>  Last paragraph: "" .. we believe that processing the data with these levels of hierarchy results in some grouping information that could be leveraged to attain better results."". This is a multi-scale approach. But do you have any medical (application domain) knowledge that would justify/hint to using such approach? <sep> o Figure 5: <sep>  Only Beats are labelled. No Rhythm labelling? <sep> o Table 2: <sep>  ""Only 2 types of labels are provided"". Are these beats and Rhythms? What are anomalies considered in the study then? <sep> o In 5.1 quantitative evaluation section: <sep>  Paragraph 2: ""..PAC is an abnormal beat only because it appears too soon and disrupts the rhythm (frequency). Furthermore, a PAC beat has the same shape as a normal beat, so taken alone, you can nearly not make the difference with a normal beat."". A graphical representation would be welcome. <sep>  Paragraph 3: ""..Both require a representation that will compose a representation showing the difference between beats over time."". <sep> o qualitative evaluation section <sep>  Paragraph 2: ""We note that these are easy to see because of the different colors we use to highlight the points, but there seems to be remaining clusters that have not been analysed."". The clusters are not evident to us even after purposefully colored.","This paper introduces a new ECG dataset. While I appreciate the efforts to clarify several points raised by the reviewers, I still believe this contribution to be of limited interest to the broad *CONF* community. As such, I suggest this paper to be submitted to a more specialised venue."
"weakness | decision  ==> Paper summary: <sep> This paper makes the observation that a curriculum need not depend on the difficulty of examples, as most (maybe all) prior works do. They suggest instead a curriculum based on learning one class at a time, starting with one and masking the label of all others as 'unknown' (i.e. treating them as negative examples), and unmasking classes as learning progresses. This is the ""incremental labels"" part. They make another observation, that label smoothing is applied to all examples regardless of difficulty, and propose an alternative ""adaptive"" version where labels are smoothed only for difficult examples. This is the ""adaptive compensation"" part. <sep> Paper contributions: <sep> - the two observations described above are both interesting, and methods addressing these seem like good ideas in light of the observations <sep> - the explanation of the method and experiments are very clear. <sep> - ablation and exploration studies are well-done to further investigate the proposed methods <sep> Review summary & decision: <sep> I like the core of this paper a lot, and recommend acceptance. It makes some insightful observations, reasonable suggestions, explains things clearly, and does reasonable experiments. The observations and proposed methods are both valuable contributions to the field. If the related work and clarity of abstract/intro are improved, along with addressing false claims and some other relatively minor things, I think this could be a very good paper, and I would be happy to increase my score. <sep> Reasons for decision: <sep> - Interesting observations are made and the approaches taken are interesting and well-motivated. <sep> - The paper overall is well structured, easy to understand, and thorough in the experiments. <sep> - Some related work (detailed below) in other fields and just about curriculum learning seems to be missing. Very strange claim (which is false, as far as I know) that curriculum learning has only been used in shallow networks emphasizes that the related work is lacking. <sep> - The abstract and intro summary of contributions didn't do a very good job of conveying what the methods do, although they are clearly explained elsewhere (see suggestions below) <sep> - A lot of time is spent on detail of experiments and long, clear explanations (this is great), but makes it read a bit like a lab report. Figure 2 and the ""properties of LILAC"" section are nice, but could be improved with more discussion and reference to other works/fields to provide insight about _why_ LILAC works, not just _that_ it works. <sep> - Results seem very incremental to me (improvements over other methods are in the decimal places), and I think many people will criticize or dismiss the methods on that basis. I hesitated about this, and in the end decided that the interesting observations are the most valuable part of the paper, not pushing SOTA (while of course it's valuable to report numbers, I think our field should focus less on SOTA numbers in general). <sep> - Misleading results are presented for CIFAR-10; ShakeDrop is not SOTA, and no reason is provided for why the particular citations in that table are there (there are many others that could be...). I don't mind that you don't get SOTA, but I mind being misled. If I've misunderstood something here, please clarify! :) <sep> Feedback/suggestions/nits (not necessarily part of decision assessment): <sep> 1. Briefly review some continual learning work; incremental labels are very similar to open set learning; this is worth mentioning. Would also be nice to mention anomaly detection here. <sep> 2. Briefly review negative mining (cases where this improves learning e.g. hard negative mining in text). <sep> 3. More discussion of motivation and why you think that LILAC works would be nice; connections to the above-mentioned fields could help. <sep> 4. Space could be made for the things I suggest here by decreasing spacing in the ""main contributions"" bullets and reducing the size of Figure 1 (the coloured tiles are very large and there's a lot of unnecessary white space. The size of text is mostly good, although the legend could also be decreased in size) <sep> 5. Incremental labels are well-explained in the dedicated section, but not until then. In the abstract, intro, and first section on ""LILAC"", I didn't really understand what was going on. I would suggest not using the words mask/unmask; this term is overloaded and gives the wrong connotation to me (that you don't use the predictions for the masked out classes). If you can't find a different term, I'd suggest explaining more clearly what it means to ""mask"" in this context in the abstract. It would also be good in the abstract to have a sentence motivating your approach (after ""... learning difficult samples""). The next sentence sort of tries to do this ""... starting point from which"" but I think this is really unclear and makes it sound like you're learning an init. <sep> 6. Adaptive compensation is well explained in the abstract, but very confusingly in the intro summary of contributions. <sep> 7. I think it would be clearer to call it ""adaptive smoothing"". I know LILAC is a nice name, but so is LILAS. <sep> 8. Cite first sentence in abstract or reword <sep> 9. ""in this work, we propose.... which introduces"" propose/introduce is repetitive <sep> 10. ""to the best of our knowledge,"" [insert ""all"" here] <sep> 11. ""curriculum learning approaches have only been tested using shallow networks"" This is just false as far as I know; apart from the misleading presentation of CIFAR-10 results, addressing this is probably the most important suggestion I have. e.g. here's a whole thesis on doing CL with deep convnets that I found from a few minutes googling: http://www.diva-portal.org/smash/get/diva2:878140/FULLTEXT01.pdf <sep> 12. LILACto -> LILAC to <sep> 13. inconsistent use of italics vs. bold for emphasis in the text. Typically use italics; bold is for keywords and subtitles. <sep> 14. Mention tsne hyperparameters in an appendix <sep> 15. Conclusion is more of a summary; the first couple sentences are good but the repeating of the explanation of both methods could be moved to the abstract and/or intro (these sentences are much clearer than the ones used there), making more room to discuss future work and connections to other fields. <sep> 16. CIFAR-10 table should list method names, not just citations (they're referred to by name in the caption, which makes it hard to read the table) <sep> Questions: <sep> 1. What other explorative experiments could you do to investigate properties of IL and AC? <sep> 2. Did I misunderstand something about the CIFAR-10 SOTA presentation (is there some reason for the choice of numbers cited here?)","While the reviewers appreciated the ideas presented in the paper and their novelty, there were major concerns raised about the experimental evaluation. Due to the serious doubts that the reviewers raised about the effectiveness of the proposed approach, I do not think that the paper is quite ready for publication at this time, though I would encourage the authors to revise and resubmit the work at the next opportunity."
"abstract | weakness | decision  ==> The authors propose capacity-limited reinforcement learning and apply an actor-critic method (CLAC) in some continuous control domains. The authors claim that CLAC gives improvements in generalization from training to modified test environments, and that it shows high sample efficiency and requires minimal hyper-parameter tuning. <sep> The introduction started off making me think about this area in a new way, but as the paper continued I started to find some issues. To begin with, I think the motivation in the introduction could be improved. Why would I choose to limit capacity? This is not sufficiently motivated. I suspect that the author(s) want to argue that it *should* give better generalization, but this argument is not made very clearly in the introduction. Perhaps this is because it would be difficult to make this argument formally, and so it is merely suggested at? <sep> Are there connections between this and things like variational intrinsic control (VIC, Gregor et al. 2016) and diversity is all you need (DIAYN, Eysenbach et al., 2019)? These works aim to maximize the mutual information between latent variable policies and states/trajectories, whereas this work is really doing the opposite. I would be interested in understanding the author's take on how the two are related conceptually. <sep> Moving to the connections with past work, this paper seriously abuses notation in a way that actually hinders comprehension. Some of the parts that really bothered me, and should be fixed to be correct: <sep> Mutual information is a function of two random variables, whereas it is repeatedly expressed as a function of the policy. Being explicit about the random variables / distribution here is pretty important. <sep> In Equation 2 (and subsequent paragraph) the marginal distributions p_a(a) and p_s(s) are not well defined, marginalizing over what, what are these distributions? I might guess that p_s(s) is the steady state distribution under a policy pi, and that p_a(a) is marginalizing over the same distribution, essentially capturing the prior probability of each action under the policy. But these sort of things need to be said explicitly. <sep> In KL-RL section there is a sentence with ""This allows us to define KL-RL to be the case where p_0(a, s) = \\pi_0(a_t | s_t)."" What does this actually mean? One of these is a joint probability for state and action, and one is an action probability conditional on a state. <sep> What does \\pi_\\mu(a_t) \\sim \\mathcal{D} mean? <sep> In the block just before Algorithm 1, many of these symbols are never defined. This needs a significant amount of care (by the authors) and right now relies on the reader to simply make a best guess at what the authors probably intend. <sep> Overall in the first three sections the message I would like the authors to understand is that, in striving for a concise explanation they have significantly overshot. These sections require some significant work to be considered publishable. <sep> The experiment in section 4.1 is intended to give a clean intuitive understanding of the method, but falls a bit short here. It is clean, but I needed more explanation to really drive the intuition home. I see that CLAC finds a solution more sensitive to the beta distribution, but help me understand why this is the right solution in this particular case. <sep> I really disagree with the conclusions around the experiments in section 4.2. I do not think these results show that for the CLAC model increasing the mutual information coefficient increases performance on the perturbed environments. First, the obvious, how many seeds and where are the standard deviations? Second, the trend is extremely small and the gap between CLAC and SAC is just as minor. Finally, CLAC has better performance on the training distribution which means that it actually lost *more* performance than SAC when transferring to the testing and extreme testing distributions. <sep> The results for section 4.3 are just not significant enough to draw any real conclusions. The massive temporal variability makes me very suspicious of those super tight error bands, but even without that question, the gap is just not very large. <sep> Finally, in section 4.4 we see the first somewhat convincing experimental results. These look reasonable, but even here I have a fairly pointed question: compared with the results in Packer et al (2018) the amount of regression from training to testing is extremely large (whereas they found vanilla algorithms transfer surprisingly well). Can you explain why there is such a big discrepancy between those results and these? But again, this section's results are in my opinion the most convincing that something interesting is happening here. <sep> Lastly, in section 8.1 the range of hyper-parameters for the mutual information coefficient is very broad, which really makes it hard to buy the claim of requiring minimal hyper-parameter tuning. <sep> All in all there is something truly interesting in this work, but in the present state I am unable to recommend acceptance, and the amount of work required along with questions raised lead me to be fairly confident in this assessment.","This paper presents Capacity-Limited Reinforcement Learning (CLRL) which builds on methods in soft RL to enable learning in agents with limited capacity. <sep> The reviewers raised issues that were largely around three areas: there is a lack of clear motivation for the work, and many of the insights given lack intuition; many connections to related literature are missing; and the experimental results remain unconvincing. <sep> Although the ideas presented in the paper are interesting, more work is required for this to be accepted. Therefore at this point, this is unfortunately a rejection."
"abstract | weakness | rebuttal_process | suggestion  ==>  ==> The paper studies the role of over-parametrization in the student-teacher multilayer ReLU networks. It presents a theoretical part about properties of SGD critical points for the teacher-student setting. And a heuristic and empirical part on dynamics of the SDG algorithm as a function of properties of the teacher networks. Overall, given previous literature, I do not find the presented results novel nor fundamentally very interesting and some parts are hard to understand due to missing details. I tend to vote for rejection at this point. More detailed questions, comments follow. <sep> In related works: <sep> ** Paragraph on ""Teacher-student/realizable setting"": The recent line of works is interesting, but the authors should be clearer about this being a very classical setting dating back several decades. The first paper I know where the teacher student setting appeared is by Garder, Derrida'83 (model B, https://iopscience.iop.org/article/10.1088/0305-4470/22/12/004/pdf). In the classical textbook on neural networks Engel, Andreas, and Christian Van den Broeck. Statistical mechanics of learning. Cambridge University Press, 2001, there is a very detailed account of many results on the setting from 80s and 90s. <sep> ** ""A line of works (Saad & Solla, 1996; 1995; Goldt et al., 2019; Freeman & Saad, 1997; Mace & Coolen, 1998) studied the dynamics from a statistical mechanics point of view, focusing on local analysis near to some critical points."" and ""(Goldt et al., 2019) assumes Gaussian input and symmetric parameterization to analyze local structure around critical points,"" The statements that these works focus on local analysis is not correct. While some formal analysis in these works required an infinitesimally informed start toward the teacher the experiments (in particular all those in Goldt et al., 2019) are run from random initialization and these works show empirically that randomly initialized training converges exactly to the fixed points described in the analysis. <sep> ** ""Local minima is Global"" paragraph: This paragraph seems to neglect the empirically observed fact (e.g. https://arxiv.org/pdf/1906.02613.pdf) that there can be global minima that generalize bad. Hence being global does not ensure good generalization. <sep> Body of the paper: <sep> ** The authors cite: ""Previous works (Ge et al., 2017; Livni et al., 2014) show that empirically SGD does not recover the parameters of a teacher network up to permutation."" but they fail to mention that separate line of work, e.g.  (Saad & Solla, 1996; 1995; Goldt et al., 2019) observed empirically the opposite.The different exiting works have to be reconciles and understood and that may be beyond the scope of the present work. But presenting only one side of the results is not helping. <sep> ** The part on the dynamics with strong and weak directions reminds me on the results on so called ""INCREMENTAL LEARNING"" e.g. in the work: <sep> Andrew M Saxe, James L McClelland, and Surya Ganguli. Exact solutions to the nonlinear dynamics of learning in deep linear neural networks. arXiv preprint arXiv:1312.6120, 2013. <sep> also later: https://arxiv.org/pdf/1809.10374.pdf and others. <sep> It would be useful to understand what is the relation in more detail and comment on it. <sep> ** The experimental part of the paper has numerous flaws that make it hard to be understood. For instance the authors do not specify the distribution of the input data. Some experiments are run with CIFAR and others with ""random"" data, but random in which sense? While generalization is the main focus of the paper the experimental results focus on the alignments of the teacher and students without really being clear how specifically the speed or the generalization error improves when neural networks are overparametrised. I found this information only in Fig. 8 for the test error. In Fig. 11 I do not know what are the different panels. What is the parameter p? So I do not know what to conclude from this figure .... in the first pannel the non-overparametrized loss (blue) decreases fastest. In the last pannel all curves are comparable. But this would suggest that over-parametrizatoin is not really helping which seems to go agains the rest of the conclusion in the paper. <sep> ** A side remark: I note that the paper is on 10 pages and hence according to the paper call higher standards should be applied in the review process.","The article studies a student-teacher setting with over-realised student ReLU networks, with results on the types of solutions and dynamics. The reviewers found the line of work interesting, but they also raised concerns about the novelty of the presented results, the description of previous works, settings and claims, and experiments. The revision clarified some of the definitions, the nature of the observations, experiments, and related works, including a change of the title. However, the reviewers still were not convinced, in particular with the interpretation of the results, and keep their original ratings. With many points that were raised in the original reviews, the article would benefit from a more thorough revision."
"abstract | weakness  ==> This paper aims at solving geometric bin packing (2D or 3D) problems using a deep reinforcement learning framework. Namely, the framework is based on the actor-critic paradigm, and uses a conditional query learning model for performing composite actions (selections, rotations) in geometric bin packing. Experiments are performed on several instances of 2D-BPP and 3D-BPP, <sep> Overall, bin packing problems are challenging tasks for DRL, and I would encourage the authors to pursue this research topic. Unfortunately, I believe that the current manuscript is at a too early stage for being accepted at *CONF*, due to the following reasons: <sep> (a) The paper is littered with spelling/grammar mistakes (just take the second sentence: ""With the developing"" -> ""development""). For the next versions of the manuscript, I would recommend using a spell/grammar checker. <sep> (b) In the related work section, very little is said about Bin Packing Problems. There are various classes of BPPs, and it would be relevant to briefly present them. Moreover, BPPs have been extensively studied in theoretical computer science, with various approximation results. Again, a brief discussion about those results would be relevant. Notably, several classes of geometric bin packing problems admit polynomial-time approximation algorithms (for extended surveys about this topic, see e.g.  Arindam Khan's Ph.D. thesis 2015; Christensen et. al. Computer Science Review 2017). <sep> (c) According to the problem formulation and the experiments, it seems that the authors are studying a restricted subclass of 2D/3D bin packing problems: there is only ""one"" bin, so (it seems that) the authors are dealing with geometric knapsack problems (with rotations). Note that the 2D Knapsack problem with rotations admits a 3/2 + \\epsilon - approximation algorithm (Galvez et. al., FOCS 2017). A. Khan has also found approximation algorithms for the 3D Knapsack problem with rotations. So, even if those results do not preclude the use of sophisticated DRL techniques for solving geometric knapsack problems, it would be legitimate to empirically compare these techniques with the polytime asymptotic approximation algorithms already found in the literature. <sep> (d) The problem formulation is very unclear. Namely, the state representation is ambiguous: sp is obviously not a boolean variable, but a boolean vector (where each component is associated with an item). Nothing is said about actions and transitions and rewards (we have to read the AC framework in order to get a clue of these components). We don't know if it is an episodic MDP (which is usually the case in DRL approaches to combinatorial optimization tasks). Also, it seems that the MDP is specified for a single instance of 3D-BPP. But this looks wrong since it should include the distribution of all instances of 3D-BPP. <sep> (e) The Actor-Critic framework, coupled with a conditional query learning algorithm, is unfortunately unintelligible due to the fact that many notations are left unspecified. For example, in Eq (1) what are the dimensions K and V? In Eq (2) what is d_i? In the algorithm what is n_{gae}? Also in the algorithm, what are l'_i, w'_i and h'_i? Etc. <sep> (f) Even if the aforementioned issues are fixed, it seems that the framework is using many hyper-parameters (\\gamma, \\beta, \\alpha_t, etc.) which are left unspecified. Under such circumstances, it is quite impossible to reproduce experiments.","This paper proposes an end-to-end deep reinforcement learning-based algorithm for the 2D and 3D bin packing problems. Its main contribution is conditional query learning (CQL) which allows effective decision over mutually conditioned action spaces through policy expressed as a sequence of conditional distributions. Efficient neural architectures for modeling of such a policy is proposed. Experiments validate the effectiveness of the algorithm through comparisons with genetic algorithm and vanilla RL baselines. <sep> The presentation is clear and the results are interesting, but the novelty seems insufficient for *CONF*. The proposed model is based on transformer with the following changes: <sep> * encoder: position embedding is removed, state embedding is added to the multi-head attention layer and feed forward layer of the original transformer encoder; <sep> * decoder: three decoders one for the three steps, namely selection, rotation and location. <sep> * training: actor-critic algorithm"
"abstract | weakness | suggestion | weakness  ==> This paper proposes an alternative mechanism of training the attention values of a sequence to sequence learning model as applied to tasks like speech synthesis and translation.  During training they compute two forms of attention: (1) the standard soft-attention from a decoder fed with teacher forced output, and (2) the inference-time attention from a decoder fed with predicted outputs.  Their training objective consists of two terms: The first is the token-wise cross entropy loss but by conditioning on the predicted output  but with teacher-forced attention.  The second is a KL distance between the above two types of attention distributions.   Experiments with mechanical  turks indicate that their attention forcing mechanism is strongly preferred over the existing teacher forced output and attention model.  On translation their method provides little or no improvement. <sep> I am inclined towards rejecting the paper because the experiment and related work section still requires a lot of work before 1. The claimed utility of the idea is established, and 2. The novelty over the many existing attention architectures is established.   I elaborate on each of these next. <sep> Related work: Recently, many papers have directly or indirectly handled the problem of exposure bias that this paper attempts to address.  The paper does not discuss most of these.  Here are some that are missed from the paper: <sep> 1.   Sequence level training with recurrent neural networks <sep> MA Ranzato, S Chopra, M Auli, W Zaremba, 2015. <sep> This paper shows that the scheduled sampling method (discussed in the paper) is much worse than a reinforce-based training mechanism of handling exposure bias. <sep> 2. An actor-critic algorithm for sequence prediction <sep> D Bahdanau, P Brakel, K Xu, A Goyal, R Lowe <sep> 3.  Posterior Attention Models for Sequence to Sequence Learning <sep> S Shankar, S Sarawagi - 2019 <sep> 4. Latent Alignment and Variational Attention <sep> Yuntian Deng, Yoon Kim, Justin Chiu, Demi Guo, Alexander M. Rush 2018 <sep> 5. Attention Focusing for Neural Machine Translation by Bridging Source and Target Embeddings <sep> Shaohui Kuang, Junhui Li, António Branco, Weihua Luo, Deyi Xiong <sep> Experiments:  Their experiments are rather sketchy and limited. <sep> The TTS experiments are only on one dataset.  Their method is compared only with the standard seq2seq learning approach.  Even the scheduled sampling or professor forcing methods are not compared with.  In addition, state of the art TTS methods have gained significantly from hierarchical attention.  As such as far as the TTS task is concerned the significance of the improved quality over a baseline seq2seq method is limited. <sep> For translation they consider only the English-Vietnamese task whereas there are tens of other translation tasks that are used in recent literature. <sep> Overall, the idea proposed seems quite incremental, experiments are limited, and related work discussion incomplete. <sep> ********* <sep> I read the author response but I do not think the paper is ready for publication yet without the thorough comparison with related work.","The paper proposed an attention-forcing algorithm that guides the sequence-to-sequence model training to make it more stable. But as pointed out by the reviewers, the proposed method requires alignment which is normally unavailable. The solution to address that is using another teacher-forcing model, which can be expensive. <sep> The major concern about this paper is the experimental justification is not sufficient: <sep> * lack of evaluations of the proposed method on different tasks; <sep> * lack of experiments on understanding how it interact with existing techniques such as scheduled sampling etc; <sep> * lack of comparisons to related existing supervised attention mechanisms."
"abstract | rating_summary | misc | weakness | rebuttal_process | suggestion  ==> The aim of this paper is to suggest randomized initializations for the various weights of a recurrent neural network (GRUs and various LSTMs are covered), such that training these networks gets to a successful start, when the model is trained on long sequences. Instead of being heuristic, their approach follows first principles of analyzing signal propagation through time, using ideas from statistical thermodynamics (mean field approximations). Some experiments, on toy datasets, validate their approach. <sep> I am quite intrigued by this paper. It is using interesting theory, shapes it to a practically highly relevant and difficult applied problem, and in the end comes up with a computable criterion of how to choose hyperparameters (means and variances of Gaussians to sample initial weights from). While the results in practice are still not too convincing, I am strongly in favour of giving this approach the benefit of doubt, as it could lead to practically very useful downstream work. <sep> The main direction of improvement for this paper (given that experiments are what they are -- somewhat limited to toy situations right now) is to better explain the methodology to researchers not familiar with mean field methods. Most importantly, it is not explained in the main text how hyperparameters are really chosen in the end. Looking at Appendix E, I find some pretty basic choices, and no other alternatives considered. It is not explained why these choices satisfy the theory, why they'd be the only ones, etc. This creates a disconnect between the very nice (and seemingly useful) theory and its implications (they are not really well spelled out). <sep> Here is what I understood (and I am not specifically an expert on stat mech). The authors assume that the dimension of latent states (N) grows large. They assume that weights are sampled independently, and identical distributed in groups k (different cell types, weight vs bias), and that inputs are correlated with each other in each dimension. Based on these assumption, they follow Gaussians statistics through a number of time steps. In the limit, one gets a deterministic dynamical system, and as t -> infty, this may converge to a fixed point. In a very nice argument (which they could explain better), they state that such rapid convergence is bad news, because then information cannot spread across long time scales, so one has to find hyperparameters for which the system behaves ""critically"". A second arguments tries to keep gradient sizes (under MF assumptions) of O(1), so neither -> 0, not -> infty, which is again some ""critical"" range. Under their assumption, these critical conditions can be computed depending on the hyperparameters. <sep> Unfortunately, this is where the paper somewhat stops, it does not give specific methods for finding hyperpars that satisfy the criteria, at least not in the sense of characterising the whole space of such hyperpars (instead, in Appendix E, they just state some few settings that do). As a direction for future work, this would be very important. Another side question is whether for what the authors call ""trainability"", the only point that matters is whether for the initial weights, signals can spread and gradients are O(1). It is certainly necessary, I see that. <sep> Detailed comments: <sep> - Please fix Table 1, the expressions seem broken. What does ""r2"" mean in the GRU column? <sep> - At least for me, (1a) to (1c) really was too short. At least in the Appendix, please do explain how this gives GRU and LSTM <sep> - Please explain the untied weight assumption somewhere. s^t is a map of s^(t-1) and W_k, so how can W_k be independent of all s^t? What are you really assuming here? <sep> - It took some repeated reading until I understood why the expressions in (2a) to (4b) do not depend on i, j, a, b (except whether a = b or a != b). Explain that properly <sep> - The core of the whole approach seems to be first half of page 5. This seems like a very nice argument, but hard to understand. Try making it more crisp. I kind of get the rough idea why fast convergence over t would be bad, but would total divergence over t not also be bad? <sep> - In (12a-c), do you mean ""equal"" or ""approximately equal""? <sep> - In 4.4: ""This motivates the general form of the initializations used in the experiments"": You have to make this more explicit. Why are your choices the only ones? Could there not be other choices satisfying (12a-c) approx, and be better? <sep> - Value of Sigma_z = 1: This seems odd to me, then your covariances are degenerate (rank 1 instead of 2). Please explain <sep> - Standard LSTM harder than GRU or peephole LSTM: Again, this sounds real interesting, but I did not get it from the explanation <sep> - I did not understand Figure 2. How are Theta_0, Theta_1 chosen? <sep> - As said above, the experiments are interesting, but somewhat artificial. Please do at least comment on real-world applications, and whether (and how) the ideas here would apply <sep> - Discussion: ""there is no clear principled way..."": Well, but practitioners need something. I'd disagree, at least one could attempt to navigate this space by global optimization techniques... <sep> ADDITIONAL COMMENT: <sep> I tried to append the following as comment, but the (pretty broken) system would not let me, insisting that ""reader is not valid"" (???). Anyway, here it is. I hope I am allowed to add to my own review. <sep> I've seen the argument in reviews that assumptions made by this paper about independencies between weights and inner states are wrong, and therefore conclusions are not valid. <sep> First, such assumptions are indeed pretty common in such statistical mech analyses of learning methods. Second, you have to distinguish between weights after (random) initialization and after training. Of course, LSTM represents long term dependencies after training, but initialization is a different story. <sep> If I was the AC for this paper, I'd ask somebody with at least some background in statistical mech to provide some additional opinions, as the reviewers (including myself) are not fully qualified.","Using ideas from mean-field theory and statistical mechanics, this paper derives a principled way to analyze signal propagation through gated recurrent networks. This analysis then allows for the development of a novel initialization scheme capable of mitigating subsequent training instabilities. In the end, while reviewers appreciated some of the analytical insights provided, two still voted for rejection while one chose accept after the rebuttal and discussion period. And as AC for this paper, I did not find sufficient evidence to overturn the reviewer majority for two primary reasons. <sep> First, the paper claims to demonstrate the efficacy of the proposed initialization scheme on multiple sequence tasks, but the presented experiments do not really involve representative testing scenarios as pointed out by reviewers. Given that this is not a purely theoretical paper, but rather one suggesting practically-relevant initializations for RNNs, it seems important to actually demonstrate this on sequence data people in the community actually care about. In fact, even the reviewer who voted for acceptance conceded that the presented results were not too convincing (basically limited to toy situations involving Cifar10 and MNIST data). <sep> Secondly, all reviewers found parts of the paper difficult to digest, and while a future revision has been promised to provide clarity, no text was actually changed making updated evaluations problematic. Note that the rebuttal mentions that the paper is written in a style that is common in the physics literature, and this appears to be a large part of the problem. *CONF* is an ML conference and in this respect, to the extent possible it is important to frame relevant papers in an accessible way such that a broader segment of this community can benefit from the key message. At the very least, this will ensure that the reviewer pool is more equipped to properly appreciate the contribution. My own view is that this work can be reframed in such a way that it could be successfully submitted to another ML conference in the future."
"strength | rebuttal_process  ==>  ==> Thank you for an interesting read. <sep> As far as I understand, the paper claims two contributions: <sep> 1. A combination of collapsed variational inference and amortised inference for SNLDS, to make the training pipeline fully differentiable; <sep> 2. An improved loss function upon the variational lowerbound (ELBO) to force the model to use the discrete states. <sep> ======= novelty ======= <sep> The 1st idea is combinatorial: <sep> 1. The forward-backward algorithm is a standard inference method for HMM-like sequence models; collapsed variational inference has been investigated extensively in 2000s when hierarchical Bayes models were actively developed; amortised inference is widely used in variational auto-encoders. <sep> 2. The combination of the above two inference methods on S(N)LDS is new to the best of my knowledge. However, this combination has been proposed on a similar model called Kalman VAE ( Fraccaro et al. 2017), where the sequence model can be viewed as a ""soft"" version of SLDS. <sep> The 2nd idea is interesting but not very well explained to some extent: <sep> 1. The goal of the modified objective function is to encourage the model to use the discrete states (instead of pushing all useful information to the continuous states). It is interesting as it regularises the *exact posterior* of the discrete states conditioned on the *approximately inferred* continuous states. <sep> 2. I believe the entropy regulariser is non-differentiable as it is based on a *histogram* estimate of the temporally averaged discrete state distribution. How exactly is this regulariser implemented? <sep> 3. I agree adding the KL regulariser can avoid the iterating assignment pathology, however, is random assignment of the regime preferred in any case? From the introductory example, I think contiguous segments are preferred. <sep> ======= significance ======= <sep> Experiments consider 3 synthetic examples for sequence segmentation (so that ground truth is available). The proposed approach performs significantly better which is a good sign. The paper also provides useful analysis on the effects of balancing parameter tempering which is always welcome. <sep> However, two baselines are missing: <sep> 1. To claim the significance of the collapsed variational inference approach, a non-collapsed inference version of the proposed SNLDS model needs to be compared. The authors did discuss this and mentioned possible workarounds (e.g. using the Gumbel-softmax trick for discrete state inference), but the comparison is not reported. If compared, this will serve well as an ablation study for the inference method. <sep> 2. The paper also provides comparisons across models, but I do think the Kalman VAE model needs to be compared to SNLDS. Both models are more flexible than the original SLDS, but the complexity is added in different ways. Since I think the inference mechanisms are similar (both using forward-backward inference for top-level latents and amortised inference for bottom-level latents), this comparison would provide a better ablation study on the modelling side. <sep> ======= clarity ======= <sep> 1. The paper presentation is overall clear to me, although I found the many sentences in parenthesis a bit distracted, so I would suggest maybe using footnotes for them instead. <sep> 2. For readers who are less familiar with HMMs/forward-backward algorithms, the papers can be difficult to understand, as it skips all the detailed computation of the gamma terms. I would suggest adding the details in the appendix, and/or visualise the intuition using e.g. message passing on factor graphs. <sep> 3. I found the related work well presented with most relevant papers, although I do think the Kalman VAE approach is highly relevant which needs to be cited and discussed. <sep> ======== references ======== <sep> Fraccaro et al. (2017). A Disentangled Recognition and Nonlinear Dynamics Model for Unsupervised Learning. NeurIPS 2017",This is an interesting paper on an important topic. The reviewers identified a variety of issues both before and after the feedback period; I urge the authors to consider their comments as they continue to refine and extend their work.
"abstract | strength | rating_summary | weakness | decision  ==>  ==> This paper examines the problem of warm-starting the training of neural networks. In particular, a generalization gap arises when the network is trained on the full training set from the start versus being warm-started, where the network is initially (partially) trained on a subset of the training set, then switched to the full training set. This problem is practical, as it is often preferable to train online while data is collected to make up-to-date predictions for tasks (such as in online advertising or recommendation systems), but it has been found that retraining is necessary in order to obtain optimal performance. The paper also mentions active learning, domain shift, and transfer learning as two other relevant and important problems. <sep> The paper attempts to investigate this phenomena from a few different avenues: (1) simple experiments segmenting the training set into two different subsets, then training to completion or partial training on the first subset before switching to training on the full set; (2) looking at the gradients of warm-started models; (3) adding regularization; (4) warm-starting all layers, then training only last layer; (5) perturbing the warm-started parameters. <sep> Strengths: <sep> I believe very strongly in the practical impact of the problems presented in this paper. These indeed are challenging problems that are relevant to industry that have not been given sufficient attention in the academic literature. I appreciate the initial experimentation on this subject, and the clear demonstration of the problem through simple experiments. The paper is also well-written. <sep> Weaknesses: <sep> Some questions I had include: <sep> - Why is the Pearson correlation between parameters of the neural network a good way of measuring the correlation to their initialization? <sep> - Why is it surprising that the magnitude of the gradients of the ""new"" data is higher than at a random initialization? <sep> - Why does this phenomena occur even though the data is sampled from the same distribution? <sep> - Does this work have any relationship with work on generalization such as: <sep> [1] Recht, Benjamin, et al. ""Do CIFAR-10 classifiers generalize to CIFAR-10?."" arXiv preprint arXiv:1806.00451 (2018). <sep> [2] Recht, Benjamin, et al. ""Do ImageNet Classifiers Generalize to ImageNet?."" arXiv preprint arXiv:1902.10811 (2019). <sep> etc. <sep> Although I like the topic of this paper, the investigation seems too preliminary at this point. There is no clear hypothesis towards answering the problems proposed in the paper. There is also no analysis, which places the burden on the numerical experiments to demonstrate something interesting, and the experiments seem sparse and small-scale. For these reasons, I am inclined to reject this paper at this time, but I strongly encourage further exploration into the topic. <sep> Some potential questions or directions could include: <sep> 1. What if only a single epoch of training is used on 50% of the data? Does the gap appear in that setting? I ask because one would expect that a single epoch of training on 50% of the data, then training on new data would be equivalent to training on the full dataset from the start. <sep> 2. How does this gap change with respect to the (relative) amount of new data introduced into the problem? For example, if one were to only add a single datapoint to the training set, would one still observe this behavior? Could one potentially add data more incrementally (rather than half of the training set) and potentially mitigate this drastic change in the problem? <sep> 3. There are optimization algorithms specifically designed for stochastic optimization (with a fixed distribution) versus for online optimization (online gradient, Adagrad). Is the online optimization framework perhaps more ""realistic"" than the stochastic optimization framework in these streaming/warm-starting settings?","The paper addresses the question of why warm starting could result in worse generalization ability than training from scratch. The reviewers agree that increasing the circumstances in which warm starting could be applied is of interest, in particular to reduce training time and computational resources. However, the reviewers were unanimous in their opinion that the paper is not suitable for publication at *CONF* in its current form. Concerns included that the analysis was not sufficiently focused and the experiments too small scale. As the analysis component of the paper was considered to be limited, the experimental results were insufficient on the balance to push the paper to an acceptable state."
"abstract | rebuttal_process | suggestion | decision  ==> This paper studies the problem of differentially private data generator. Inspired by the general GAN framework and the PATE mechanism, the authors propose a new differentially private training algorithm for data generator. The problem of training data generator with privacy guarantee considered in this paper is very interesting, and the proposed algorithm looks novel. However, there are lots of unclear statements in the current paper, and I cannot tell whether the proposed algorithm is indeed better than previous methods. Following are my major concerns: <sep> 1.It is unclear what are the loss functions used in equation (1) and (2). Please define k when introducing equation (2). <sep> 2.The training framework introduced in section 3.1 is different from the traditional GAN framework, and thus my concern is that whether this framework will give us good generated samples. Because the performance of GAN has been proved in both theory and practice. The authors should at least empirically show the performance of the proposed framework in the nonprivate setting. <sep> 3.There is no introduction of the (ϵ,δ)-differential privacy before introducing the Definition 1. <sep> 4.There is no definition of Renyi differential privacy, so the statement of Theorem 2 is unclear. In addition, what is data-dependent Renyi differential privacy? <sep> 5.The privacy guarantee of Algorithm 2 is not very clear. Because there are lots of parameters in Algorithm 1 and 2 which may affect the privacy guarantee, and Theorem 3 does not state such requirements. For example, how to choose σ1,σ2? In Theorem 7, there are some constraints on different parameters, will them be satisfied by your algorithm? <sep> 6.How will the number of teacher models affect the privacy guarantee? <sep> 7.Why you choose random projection matrix with variance 1/k, and what is the projection dimension for different algorithms? <sep> 8.In Table 1, the results of non private GAN are different from the results of non private GAN reported in PATE-GAN paper. Since the baseline results are much better in the current than the results reported in the PATE-GAN paper, it seems to me that the improvements of the proposed method comes from the stronger baseline. <sep> Other comments: <sep> 1.λ>1 in Theorem 3. <sep> 2.Algorithm 2 should be moved to main context. <sep> 3.The last sentence in section 3.2 is not convincing. <sep> 4.Typo ""differnet"" in the caption of Table 1.","This paper addresses the problem of differential private data generator. The paper presents a novel approach called G_PATE which builds on the existing PATE framework. The main contribution is in using a student generator with an ensemble of teacher discriminators and in proposing a new private gradient aggregation mechanism which ensures differential privacy in the information flow from discriminator to generator. <sep> Although the idea is interesting, there are significant concerns raised by the reviewers about the experiments and analysis done in the paper which seem to be valid and have not been addressed yet in the final revision. I believe upon making significant changes to the paper, this could be a good contribution. Thus, as of now, I am recommending a Rejection."
"misc | decision | misc  ==> Summary:  The paper studies the problem of global optimization of high-dimensional sparse estimators regularized by PCP (folded concave penalty). The main result is showing that under certain conditions, with high probability, the desired global solution is an oracle stationary point satisfying the so-called S^3ONC conditions. In light of this result and an existing polynomial-time algorithm for finding an S^3ONC solution, the global solution can be recovered with polynomial computational complexity. Numerical evidence is provided to show the theoretical predictions. <sep> Strong points: <sep> -S1. The global optimization of non-convex (regularized) sparse learning models is a fundamental and challenging problem worth investigating. <sep> -S2. The paper is well organized and clearly presented in general. <sep> Weak points: <sep> -W1. The loss function in Equation (1) is lacking in explanation. It is claimed that ""traditional statistical learning schemes often resort to"" such a formulation. However, it looks like only the logistic loss explicitly admits such a form while some other widely used ones such as squared/hinge/exponential loss do not. So are there any concrete statistical learning models other than logistic loss falling exactly into this framework? It could be beneficial to provide a reference, if any, to this particular problem formulation in machine learning literature. <sep> -W2. The nominal generative model is not clearly defined. Usually, a key component of high-dimensional statistical analysis is to define a nominal statistical model for data generalization. I note the parameter vector of such a model is denoted as βtrue in this paper. However, the related data generation procedure seems completely missing in the statement of problem setups. Particularly, what's the definition of the residuals W appeared in the assumption A2? <sep> -W3. The overall novelty of theory is limited. The main result established in Theorem looks closely related to the excess risk bounds established in [Liu & Ye 2019]. Actually, provided that the risk function L(β) has (restricted) strong convexity in β, it follows immediately based on the first excess bound in Theorem 1 of [Liu & Ye 2019] that ∥β∗−βtrue∥ is well bounded from above. Thus if the minimum non-zero absolute value of βtrue is sufficiently larger than that upper bound, then it is naturally true that β∗ shares the same supporting set as βtrue which in turn implies that β∗ is an oracle solution and also is globally optimal under some more stringent conditions. Unless the author(s) can justify the value-added beyond the results in [Liu & Ye 2019] as sufficient, the degree of novelty of the current theory seems fairly low given that prior work. <sep> -W4. The proof of Theorem 1 has flaws. The proof of main result relies largely on Lemma 5 which basically bounds the cardinality ∥β∗−βtrue∥0. However, when invoking Lemma 5, such a L0 bound shifts to an L2-norm one. This misleading point needs to be clarified. <sep> -W5. The numerical study can be improved. The majority of the reported results is about the advantage of FCP over Lasso and ridge regression, which however is less relevant to the global recovery theory developed in the paper. I think the numerical study needs to be re-designed to put more focus on the effects of some key factors, e.g , the sample size n and the signal strength βmin, on the global optimization performance. <sep> -W6. Concerning the fitness to venue, although somewhat relevant,  I am not sure the main topic of this paper (i.e., statistical analysis of high-dimensional GLM) would gain significant interests in the community of *CONF*. Actually in my opinion, the novelty/importance of this work is more suitable to be evaluated in a high-dimensional learning theory intensive journal or conference rather than in a DL/RL conference. <sep> Minor issues: <sep> - M1. Check the correctness of notation β in Definition 1. <sep> - M2. Equation (3): Why not directly writing out ""0∈1/n∑i=1n…""? <sep> - M3. Statement of Theorem 1: satisfy -> satisfies. The quantities t and t′ appeared in Theorem1 are hard to understand without further explanation. <sep> === update after author response === <sep> Thank you for the response which is helpful for clarifying some of my concerns. However, after reading the revised paper, I am still not quite convinced that the current theoretical analysis is particularly novel given the results in [Liu & Ye 2019]. Also, it seems that the noconvexity of the considered problem mainly arises from the regularization term rather than the loss term, and thus can hard to be claimed as ""central to *CONF*"".  In view of these, I choose to maintain my assessment of this paper as borderline leaning to rejection.","Thanks for your detailed feedback to the reviewers, which clarified us a lot in many respects. <sep> However, the novelty of this paper is rather marginal and given the high competition at *CONF*2020, this paper is unfortunately below the bar. <sep> We hope that the reviewers' comments are useful for improving the paper for potential future publication."
"abstract | strength | weakness | misc | rebuttal_process | decision  ==>  ==> The paper considers generative models and proposes to change the VAE approach in the following way. While VAEs assume a generative model (prior on the latent space + stochastic generator) and aim at learning its parameters so as to maximize the likelihood of the (training) data distribution, the authors propose to derive the learning objective from a different view. In my understanding, they consider the two composed mappings generator(encoder) and encoder(generator) and require the first one to have the data distribution as a fixpoint and the second one to have the latent prior as a fixpoint. Starting from this idea they derive an objective function for the case that the mappings are deterministic and then further enrich the objective by either likelihood based terms or VAE based terms. The new approach is analysed experimentally on MNIST, CIFAR and CelebA datasets. The authors report quantitative improvements in terms of Frechet inception distance (FID) as well as sharper samples (when compared to standard VAEs). <sep> I find the main idea of the paper highly interesting and compelling. Nevertheless, I would not recommend to publish the paper in its present state . The technical part is in my view very hard to comprehend. This is partially due to the disadvantageous notation chosen by the authors. Furthermore, the derivation of the individual terms in the objective is hard to understand and the arguments given in the text are not convincing: <sep> - The two terms in the objective related to the fixpoint of the encoder(generator) mapping seem to enforce a fixpoint that is a mixture of the latent Gaussian prior and the encoder image of the data distribution. It remains unclear to me, why this is a good choice. <sep> - The derivation of the additional likelihood based terms and VAE based terms is in my view hard to understand. <sep> It should be possible (and I believe, is possible) to derive a simpler objective ab initio, starting from the main idea of the authors. <sep> Side note: A close visual inspection of the presented generated samples seems to confirm the known doubts about the appropriateness of the FID measure for evaluation of generative models.","The authors present a new training procedure for generative models where the target and generated distributions are first mapped to a latent space and the divergence between then is minimised in this latent space. The authors achieve state of the art results on two datasets. <sep> All reviewers agreed that the idea was vert interesting and has a lot of potential. Unfortunately, in the initial version of the paper the main section (section 3) was not very clear with confusing notation and statements. I thank the authors for taking this feedback positively and significantly revising the writeup. However, even after revising the writeup some of the ideas are still not clear. In particular, during discussions between the AC and reviewers it was pointed out that the training procedure is still not convincing. It was not clear whether the heuristic combination of the deterministic PGA parts of the objective (3) with the likelihood/VAE based terms (9) and (12,13), was conceptually very sound. Unfortunately, most of the initial discussions with the authors revolved around clarity and once we crossed the ""clarity"" barrier there wasn't enough time to discuss the other technical details of the paper. As a result, even though the paper seems interesting, the initial lack of clarity went against the paper. <sep> In summary, based on the reviewer comments, I recommend that the paper cannot be accepted."
"abstract | rating_summary | rebuttal_process | decision  ==> The paper proposes a novel way to determine automatically the quantization level at each layer of deep neural networks while training. The quantization can result in a more compact network requiring less memory for storing the parameters (weights), with minimal drop in accuracy. <sep> If I understand correctly, the main idea in the paper is utilizing a continuous representation of the quantization using a continuous cursor, and a new differential approach from Neural Architecture Search (NAS), to create a differential learning algorithm for the problem of finding the optimal quantization. <sep> On the positive side, the authors present very good experimental results on popular networks like ResNet, <sep> and improve on other method's compression accuracy, with little to no reduction in accuracy. <sep> Howerver, the paper is not clearly and carefully enough written to convey the author's results which makes it a borderline paper in my opinion. <sep> In my understanding, compressed (quantized) networks can have different advantages besides a more compact representation of the network: <sep> For example, this can also reduce training time because all operations are performed with words with fewer bits - but the authors focus only on the compression ratio and do not discuss this issue. In fact, it seems that their algorithm actually requires more training time than standard training of neural networks, because you start with training w with full 32-bit representation, and then train with different quantizations simultaneously. <sep> It would be good if the authors describe clearly how do different methods (theirs and the others mentioned) compare to each other in terms of savings of different resources (training time, compression, inference/prediction time for the trained networks etc.)  and which of these is more important (e.g. the latter in mobile devices). <sep> Another practical issue is that using a different number of bits for each layer may complicate the design and software/hardware implementation of a network, compared to say allocating 4-bits or 8-bits to each weight. It is correct that you can reduce the overall number of bits, but you may reduce it even further if you allow a different number of bits for each individual weight. I assume that there is a price in power/memory/speed etc. to this non-homogenous property, and it may be better to get a slightly lower compression ratio provided that there is more uniformity in the network weights.  This is not a critique of this particular paper, but of the entire framework of flexible quantization. <sep> The English of the manuscript could be greatly improved. For example: <sep> Page 2: ""..multiple bits for different layers.."" - I assume the authors mean ""..a different number of bits used for different layers .."" <sep> Page 2, bottom: ""The authors (Wang et al. 2018) presented ..."" - the sentence is long and unclear. <sep> Many sentences starting with 'And' - for example: ""And a Gumbel softmax function ..."" (page 3) <sep> There are a few vague and non-informative statements which do not contribute to the understanding of the field and the authors' contribution. For example: <sep> - Page 3: ""A typical search method is random search, however, its efficiency is not ideal."" (does efficiency refer to computational efficiency here?) <sep> - Page 4: ""The reason why we add regularization item to the loss function is because the regularization can prevent overfitting to some extent"" (I thought that here the main purpose of regularization is to get a trade-off between accuracy and compression). <sep> - Page 6: Regarding parameters choice: 'a rather optimal set of them is chosen ..' - this seems quite arbitrary and it is not clear how to choose parameters for the authors' method for future architectures and datasets. <sep> There are also terms that are known to expert but could use a short explanation/reference - for example: <sep> - Adam optimizer <sep> - Cosine annealing <sep> - Gumble Softmax <sep> The mathematical equations are definitions are not clear enough and contain errors: <sep> - Eq. (1): there is (x',y') twice in the equation, but in the sentence afterward there is also (x,y). <sep> There are D_T, D_V with and without tilde. <sep> The space of maximization of w for a given quantization is not defined (I'm assuming all weights vectors w which can be represented using the number of bits in the quantization). <sep> - Eq. (3) - the loss is loosely defined - what is 'parameter size'? the number of bits for all the weights in each layer? <sep> - Eq. (4) - is w or w_k the full precision weights? why are x and y in the interval [0,1]? <sep> - Eq. (6) - c_i represnts the i-th layer, but d_1,d_2, a_1,a_2 are fixed. Are they also different for different layers? <sep> Also, a_1 and a_2 are the boundaries of the cursor - are they set to [0,1]? or [0,32]? <sep> - The authors defined Conv - the convolution operation - but what are W_1 and X? vectors? how is the convolution defined precisely? it is also not common in math papers to use '*' for  product <sep> It is not entirely clear to me how exactly the authors define a differentiable loss and a gradient for the quantization part Loss_q.  When a cursor parameter changes (e.g. c_i from 2.5 to 2.6) then the loss is defined by quantization to the two consecutive integers (in that case 2,3 ) and there is a continuous mixture parameter between them which can be changed smoothly, which is nice. But at some point, the parameter will reach 3, and then the quantization will use the two integers 3 and 4 - the fact that 3 has weight 1 in the loss (and 2,4 have weight zero) makes the loss continuous, but is the loss differentiable at this point?  some explanation is needed here on if this is a problem and how is it avoided/solved. <sep> Algorithm 1: The description in Figure 1 can be made more precise. The algorithm seems to alternate between optimizing the cursor c, and optimizing the weights w for a given value of c. <sep> There are also missing details and parameters like gradient step size.  What does Grad-C * L_C mean? is the gradient multiplied by the loss?? or only applied to it? <sep> In the sentence ""Quantize the network ... to update the loss"" - which loss is updated and how? L_V, L_T? something else? <sep> A detailed description of the algorithm and parameters or the actual code used by the authors would improve the understanding of their method. <sep> Section 4.1: Figure 2 compares the loss of one-integer vs. two-integers quantization scheme. The authors argue that their two integers scheme is better because it is smoother. But the loss for one integer is actually lower - so why wouldn't it be better to use this one? <sep> will the two-integer method eventually reach a lower loss?","This paper presents a method to compress DNNs by quantization. The core idea is to use NAS techniques to adaptively set quantization bits at each layer. The proposed method is shown to achieved good results on the standard benchmarks. <sep> Through our final discussion, one reviewer agreed to raise the score from 'Reject' to 'Weak Reject', but still on negative side. Another reviewer was not satisfied with the author's rebuttal, particularly regarding the appropriateness of training strategy and evaluation. Moreover, as reviewers pointed out, there were so many unclear writings and explanations in the original manuscript. Although we admit that authors made great effort to address the comments, the revision seems too major and need to go through another complete peer reviewing. As there was no strong opinion to push this paper, I'd like to recommend rejection."
"abstract | strength | rebuttal_process | rating_summary  ==> This paper considers the notion of ""no-harm"" group fairness, i.e. trying to reduce the risk gap between minority and majority groups without excessive reduction in performance on the majority groups. Authors formalize the problem by defining a Pareto fair classifier, i.e. one that minimizes the risk gaps between groups and belongs to the family of Pareto classifiers containing the classifier minimizing the empirical risk. Authors suggest an optimization procedure for finding the Pareto fair classifier and demonstrate its performance on multiple datasets. <sep> Pros: <sep> I think that studying ""no-harm"" classifiers is an important topic given the alarming tendency of some of the recent group fairness approaches to achieve fairness by essentially driving down the performance on the majority group without improving on the minority group. Decision making in medical applications is one of the prominent examples where ""no-harm"" is absolutely needed, as authors suggested. The mathematical formulation of the problem around the notion of Pareto optimality also seems reasonable. <sep> Cons: <sep> My concerns are related to counter-intuitive experimental results and lack of clarity in parts of the presentation. <sep> Figure 1 seems important for understanding the ideas in the paper, but is not explained in much detail. Analogous to it Figure 2 is lacking important details. In the upper left plot, what are the decision boundaries of the baselines? What are the baselines risks in the center top figure, particularly for the equal risk classifier? It is hard to see from the right figure if the proposed classifier achieves the ""no-harm"" fairness over the equal risk classifier - numerical summary in a table could help. Finally, why is it necessary to use a neural network (which seems to be the case based on the supplement A.3) for the toy problem? I would recommend working through a toy example in more detail using a linear classifier to verify the correctness of the proposed technique and improve the overall clarity. Further, absence of a toy problem with linear classifier is alarming given there is not much discussion of the algorithm and its convergence properties. <sep> Regarding the real data experiments, none seem to showcase the ""no-harm"" versus ""zero-gap"" fairness tradeoff motivating this paper. <sep> MIMIC-III results seem to contradict the main story of the paper. The minority group appears to be ""D/A/NW"", then there should be a ""harming"" group fairness classifier achieving close to 0 discrimination at the cost of lowering performance on other subgroups. The ""no-harm"" classifier should then achieve a similar or slightly lower performance on ""D/A/NW"", but much better results on other sub-groups. Despite, the ""no-harm"" classifiers seems to outperform other approaches on ""D/A/NW"" by a good margin. Next, it appears that ""Naive+Zafar"" (it also would be helpful to have a brief discussion of the baselines considered) approach was not configured to eliminate A/S disparity as suggested by poor results on the D/A/NW and D/A/W, while it performs very well on other subpopulations. <sep> Skin Lesion classification experiment departs from the problem of fairness and considers the problem of classification with unbalanced classes instead. Results are again counterintuitive. ""Rebalanced Naive"" only mildly improves over the ""Naive"" approach, while proposed algorithm seems to achieve a quite significant mean accuracy improvement. This again does not show the motivating ""harm"" vs ""zero gap"" tradeoff, but it could be interesting as the imbalanced classification is an important problem by itself. Could you please compare to more advanced imbalanced classification algorithms? <sep> Results on the Adult and German Credit datasets are very similar across competing methods. <sep> Acknowledgement and references to the individual fairness line of works are missing when presenting the problem of fairness in machine learning. <sep> Font size in tables and Figure 2 legends is too small. <sep> Typo in the last sentence on page 8: ""have highly impactful"" -> ""are highly impactful"".","This manuscript outlines procedures to address fairness as measured by disparity in risk across groups. The manuscript is primarily motivated by methods that can achieve ""no-harm"" fairness, i.e., achieving fairness without increasing the risk in subgroups. <sep> The reviewers and AC agree that the problem studied is timely and interesting. However, in reviews and discussion, the reviewers noted issues with clarity of the presentation, and sufficient justification of the results. The consensus was that the manuscript in its current state is borderline, and would have to be significantly improved in terms of clarity of the discussion, and possibly improved methods that result in more convincing results."
"abstract | weakness | suggestion  ==> # Review *CONF*20, Visual Hide and Seek <sep> This review is for the originally uploaded version of this article. Comments from other reviewers and revisions have deliberately not been taken into account. After publishing this review, this reviewer will participate in the forum discussion and help the authors improve the paper. <sep> ## Overall <sep> **Summary** <sep> The authors introduce a new RL environment and task, ""Visual Hide and Seek"", in which they analyze how the agent's learned visual representations are impacted by its speed, auxiliary rewards, and opponent behavior. <sep> **Overall Opinion** <sep> This paper presents a thorough analysis and great visualizations of agent behaviors and representations under different conditions. I wish more papers would put this much effort into analyzing their agents. I'd highly recommend this paper get accepted since I believe the analysis carried out here and the conclusions reached are quite novel and the paper is overall well-written. <sep> However, at the same time, the work of [Baker et al., 2019][1] was published with significantly more fanfare. I hope their work does not overshadow this one since they are only related in the general task concept. <sep> [1]: https://arxiv.org/abs/1909.07528 <sep> Some major issues I had with this work: <sep> - In general, please run more random seeds. Just reporting on a single random seed is not enough, as per [Henderson et al., 2018][2]. <sep> - There are some sections of the paper where the order of paragraphs is confusing. You start the introduction by stating what you've done and letting the reader wonder ""why?"". The explanation is only given in the second paragraph. So I'd suggest rotating the second paragraph upwards before the first. Similarly, at the beginning of section 4, you just mention the results - this should either be shorter (1 sentence, as an overview of the work in this section) or moved to the end of that section. <sep> - You're missing a section about future work and flaws/problems of your work at the very end (the latter if which should be in ""Discussion""), which is common to include in *CONF* publications. <sep> [2]: https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/viewFile/16669/16677 <sep> Here are some minor... <sep> ## Specific comments and questions <sep> ### Abstract all good <sep> ### Intro <sep> - Fig.1: What is that white thing that the seeker/hider have on the capsule? <sep> ### Rel. Work all good <sep> ### Method <sep> - What's the speed (FPS) of that Unity engine? Why didn't you use Mujuco/(Py)Bullet/Gym-Miniworld? <sep> - You need to add some measurements and units: the arena size doesn't have a unit, the size (diameter) of the hiders/seekers is unclear, turning left/right is unclear (how far left/right, after action-repeat) <sep> - You mention any real-valued position to be valid - so the agents can step into obstacles? And how about through obstacles? <sep> - ""Affordance Learning"" is usually not used for static environment geometry like obstacles (e.g. [Georgia-Tech course on ""Human-Robot Interaction""][3]) <sep> - Why 4-layer CNNs? Why not 6 or 8 or a ResNet? Would you think the features would be stronger/weaker in an 8-layer CNN? <sep> [3]: https://www.cc.gatech.edu/~athomaz/classes/CS8803-HRI-Spr08/MayaChandan/Site/Affordance_Learning.html <sep> ### Experiments <sep> - 4.1 ""... learned this play game"" -> ""... learned this game"". <sep> - 4.2 ""mid-level features"" -> what's that? The activation of the convolutional kernels after the second layer CNN? What's the dimension? And why did you pick the 2nd layer, not any of the other 3? <sep> - Tab.3: This is averaged over how many frames of rollout? <sep> - ""... case can moves a lot faster."" -> ""... case can move a lot faster"". <sep> - Fig.2 is very interesting. Well done. <sep> - Fig.3: the font is not consistent with other figures <sep> - Fig.4: remove the blueish background to increase contrast. Increase the font size of the ticks on the left. Make the legend color boxes slightly bigger. Add more space or a visual divider between the different states - especially on the right side it's hard to make out where one stops and the next begins. <sep> - Fig.4/5: This analysis is lovely and we need more of this in DRL. <sep> - 4.4 in the text, you sometimes write ""not S"" and sometimes ""¬S"". Please change the ""not s"" <sep> - Fig.5: (suggestion) Merge/sum the 2 columns (in both A/B merge the left and right plot into one by summing); subtract the random policy values as you did with Fig.4. <sep> - ""We summarize representative cases, and put the full results for all combinations in the Appendix"" - no you didn't. <sep> - Fig.6: What is going on in the left third of this diagram? What is this colorful mush? If this is by any chance indicating a change over time, do you maybe want to spread a single, very colorful plot of distance over time into multiple less colorful plots? At least add a legend, please. Also, I'd recommend smoothing (moving average or smoothing spline). <sep> - Also maybe add reward over time plots, as is common in DRL, to show that your policies converged after 8 mil. steps. <sep> ### Conclusion <sep> All good, save for the missing future work and critical analysis of your work. <sep> ### Appendix all good","This paper proposes a technique for training embodied agents to play Visual Hide and Seek where a prey must navigate in a simulated environment in order to avoid capture from a predator. The model is trained to play this game from scratch without any prior knowledge of its visual world, and experiments and visualizations show that a representation of other agents automatically emerges in the learned representation. Results suggest that, although agent weaknesses make the learning problem more challenging, they also cause useful features to emerge in the representation. <sep> While reviewers found the paper explores an interesting direction, concerns were raised that many claims are unjustified. For example, in the discussion phase a reviewer asked how can one infer ""hider learns to first turn away from the seeker then run away"" from a single transition frequency? Or, the rebuttal mentions ""The agent with visibility reward does not get the chance to learn features of self-visibility because of the limited speed hence the model received samples with significantly less variation of its self-visibility, which makes learning to discriminate self-visibility difficult"". What is the justification for this? There could be more details in the paper and I'd also like to know if these findings were reached purely by looking at the histograms or by combining visual analysis with the histograms. <sep> I suggest authors address these concerns and provide quantitative results for all of the claims in an improved iteration of this paper."
"abstract | weakness | decision  ==>  ==> This paper presents an algorithm to generate images for training a student network through distillation. The paper claims the use of the same training data is not necessarily beneficial when it comes to improving the accuracy of the student being trained. <sep> The paper sits on the empirically driven side with sufficient experiments in the context of random forest and CNN. <sep> As a second contribution, the paper proposes a scoring process to evaluate the quality of datasets generated by GAN methods. There are little experiments on this side. <sep> Pros: <sep> - I like the paper and the idea behind being able to improve or even train a student network when the original data is not present. <sep> - Metrics tailored to the problem are relevant. <sep> Negs: <sep> - While there are plenty of experiments, there is a lack of detailed descriptions. <sep> - The scoring for GANS seems to be barely tested. In the scoring GAN, The fact that TSCS drops is enough to be a valid metric (or better than existing?). In the TSC vs inception, it is hard for me to see the unrealistic artifacts and, according to the text, that is not what TSCS is measuring, right? What is the influence of using different student-teacher configuration? (on the time to produce the scores the paper claims NiN and LeNet, is there any difference if using other architectures (ResNet family for instance)? At least, in that case, the time changes. It would be nice to see the stability of this metric to demonstrate that the need for training an inexpensive model is correct. <sep> For the TSC vs Inception, the GANs are subjectively assessed, isn't it (as to select well-trained to Inferior). Would it be possible to see exactly the same images between the two of them? Seems like the difference in quality for IS is significantly larger than for the proposed metric (even in the last gan there is a slight increase in the metric). <sep> It seems to me that IS is just a quality metric based on how a single image looks like. Would it be possible to disentangle the training and the scoring in the proposed metric? What if my hyperparameters for doing the single epoch are totally wrong? <sep> In the general idea of the GAN, while I like it, there is little about how the GAN is actually trained. I guess this GAN is trained using some sort of real data and therefore, the comparison is not totally fair. How many images were used to train this GAN? What would happen if those images are used directly in the distillation framework? <sep> How many images are generated in section 4? Is the influence of pfake related to the dataset? If I have to train using another dataset, how do i set that parameter?","This paper uses GAN for data augmentation to improve the performance of knowledge distillation. <sep> Reviewers and AC commonly think the paper suffers from limited novelty and insufficient experimental supports/details. <sep> Hence, I recommend rejection."
"abstract | rating_summary | strength | weakness | rebuttal_process | decision  ==> The authors propose to use numerical differentiation (using random perturbation) to approximate the Jacobian of a particular update (essentially equations 5~7) which plays an important role in the estimation of HMMs.  To do so, the authors provide first a concise intro to HMM models (well known stuff in S2), presenting the iteration in detail, jump into their model (cryptically presented in my opinion in S3) and then propose a numerical approximation scheme using SPSA (building upon literature from the 90's, with Theo 1 being the main contribution), before moving onto experiments. <sep> I have found the paper poorly presented. Its general motivation stands on a shaky ground (as illustrated by the choice of words by the authors, see below). In terms of presentation, reminders on HMM are welcome, but unfortunately the authors have not kept the same standard for clarity of notation in Section 3, which makes reading and understanding what the authors are doing quite difficult. Not being a specialist in this field, I have struggled a bit to understand the model itself, and the practical motivation of adding a DNN in the middle of what is otherwise an unrolled back-and-forth between k steps of EM estimation of transition parameters and the addition of a DNN layer. Despite the complexity in the story, what the authors propose is essentially to apply a numerical approximation scheme for Jacobians of these EM updates instead of backprop. Since this is the crux of the contribution,  I feel some more numerical evidence that their approach works compared to baselines (e.g. Hinton et al 2018) is needed. For these reasons my assessment is a bit on the lower side. <sep> - parenthesis bug in b_j(... in Eq.4 <sep> - in Eq. 5, index i appears both in numerator (as regular index) and denominator (as sum index) <sep> - what is \\Psi in Eq.8 ? <sep> - ""While HMM is arguably less prevalent in the era of deep learning"": odd way to start an intro. All papers cited date back to more than 2011, 2 in 2006, all the rest in 20th century. This is particularly strange given the few citations to papers >2015 in Section 5. <sep> - the observation sequence o_{t,1:T(u)} is ""weakly"" indexed by u (since T(u) is just a length) <sep> - What is the \\forall u notation below Eq. 9? <sep> - ""the number of nodes required to build the forward and backward probability in the computation graph of an automatic differentiation engine is on the order of O(T^2). Empirically we found this leads to intractable computation cost."" since this is critical, where is this empirical evidence? this seems to be a storage problem and cannot be a complexity issue. There are ways to mitigate this problem by only storing partially information, I feel this comparison would add a lot of value to the authors' claim. <sep> - Where is J^{(k)} defined (as opposed to \\hat{J}^{(k)}) defined in Eq.14?","The authors propose to use numerical differentiation to approximate the Jacobian while estimating the parameters for a collection of Hidden Markov Models (HMMs). Two reviewers provided detailed and constructive comments, while unanimously rated weak rejection. Reviewer #1 likes the general idea of the work, and consider the contribution to be sound. However, he concerns the reproducibility of the work due to the niche database from e-commerce applications. Reviewer #2 concerns the poor presentation, especially section 3. The authors respond to Reviewers' concerns but did not change the rating. The ACs concur the concerns and the paper can not be accepted at its current state."
"abstract | weakness  ==> This paper proposes Ladder Polynomial Neural Networks (LPNNs) that use a new type of activation primitive -- a product activation -- in a feed-forward architecture. Unlike other polynomial architectures that grow in the order exponentially with network depth, the proposed approach gives explicit control over the order and smoothness of the network output and enables training with standard techniques. <sep> The proposed architecture is closely related to a decomposition of a k'th order multivariate polynomial function <sep> [T, x^{\\otimes k}] = \\lambda^\\top (A x \\odot A x \\odot …  \\odot A x) =  \\lambda^\\top (A x)^{\\odot k} <sep> where T is a symmetric tensor of polynomial coefficients and [\\cdot,\\cdot] denotes contraction. This is a shallow (one layer architecture) and sometimes referred as a Waring decomposition. <sep> In this paper, the authors propose a specific chain factorization of the polynomial (Eq 5 in the paper), where they write the factors recursively, that they name as a ladder polynomial neural network. <sep> h^\\ell = (W_\\ell h^{\\ell-1} \\odot V^{\\ell} x) <sep> The ladder architecture is very closely related to tensor trains (https://epubs.siam.org/doi/10.1137/090752286). I found it surprising and somewhat alarming that this literature is not being cited as these methods are also quite well known in deep learning. <sep> I like the smoothness analysis of section 3.1 -- the proof is quite easy to follow and direct. I would be quite surprised if this result would not be known in the literature in some other form but I don't recall seeing it. On the other hand it seems to be inevitably very loose for a deep ladder network unless the network models the zero function. It would have been a valuable addition to the experimental section, if this bound would have been illustrated numerically on synthetic examples. <sep> In 3.2, The authors say that the objective is multiconvex -- I would argue that it is multilinear (apart from the regularization term, that is later introduces). The observation in 3.3, that batch-normalization or dropout can be used for this model is perhaps tangential to the main argument. These is investigated in the experimental section but I don't see a clear conclusion. The section in 3.4 must include links to tensor decompositions beyond factorization machines. <sep> Overall, I think the paper has some merit and could be interesting for some readers, despite the fact that the contribution is not very original and the treatment could be improved in many ways.","This paper proposes a new type of Polynomial NN called Ladder Polynomial NN (LPNN) which is easy to train with general optimization algorithms and can be combined with techniques like batch normalization and dropout. Experiments show it works better than FMs with simple classification and regression tasks, but no experiments are done in more complex tasks. All reviewers agree the paper addresses an interesting question and makes some progress but the contribution is limited and there are still many ways to improve."
"abstract | rebuttal_process | weakness | decision  ==>  ==> The paper proposes a new loss function which adds to the training objective another term that pulls the current parameters of a neural network further away from the parameters at a previous time step. <sep> Intuitively, this aims to push the current parameters further to the local optimum. <sep> On a variety of benchmarks, optimizing the proposed loss function achieves better results than just optimizing the training loss. <sep> The paper is well written and easy to follow.  However, I am not entirely convinced about the intuition of the proposed method and I think further investigation are necessary. <sep> While the method is simple and general, it also seems to be rather heuristic and requires carefully chosen hyperparameters. <sep> Having said that, the empirical evidence shows that the proposed loss function consistently improves performance. <sep> The following details should be addressed further: <sep> - I am a bit confused by the definition of the loss function. In Equation 1 it seems that the term on the left represents the training objective. If that is correct than Equation 2 second case contains the training objective twice? <sep> - F in Section 3 after Equation 2 is not properly defined <sep> - Could it happen that the proposed loss function leads to divergence, for example if the parameter from a previous time step theta^Tp is close to the optimum theta_star? <sep> - What is the motivation to use the L1 norm? How does this choice affect convergence compared to let's L2 norm? <sep> - Section 4.1 typo in first paragraph: K instead of \\kappa <sep> - Section 4.1 the results would be more convincing if all networks were trained multiple times with a different random initialization and Table 1 would include the mean and std. <sep> - Why is no warm-up period used for the GAN experiments? <sep> - Section 4.3: why is \\kappa increase by 1% for the speech recognition experiments where as by 2% for all other experiments? <sep> - I suggest to increase the line width of all figures since they are somewhat hard to identify on a print version. <sep> - Why is the momentum set to 0.5 for SGD in the ablation study? Most frameworks use a default value of 0.9. <sep> - I would like to see the affect of the warm-up period to the performance in the ablation study. <sep> - How does the choice of learning rate schedule, such as for example cosine annealing, affect the loss function? <sep> post rebuttal <sep> ------------------ <sep> I thank the authors for clarifying my questions and providing additional experiments. I think that especially the additional ablation studies and reporting the mean and std of multiple trials make the contribution of the paper more convincing. Hence, I increased my score.","This paper introduces a further regularizer, retrospection loss, for training neural networks, which leverages past parameter states. The authors added several ablation studies and extra experiments during the rebuttal, which are helpful to show that their method is useful. However, this is still one of those papers that essentially proposes an additional heuristic to train deep news, which is helpful but not clearly motivated from a theoretical point of view (despite the intuitions). Yes, it provides improvements across tasks but these are all relatively small, and the method is more involved. Therefore, I am recommending rejection."
"abstract | weakness | misc | decision | misc  ==> * Summarize what the paper claims to do/contribute. <sep> This paper claims to extend existing image translation works, like CycleGAN, to domain pairs that are not similar in shape. It is proposed to do so by using a VGG network trained on classification (I assume on Imagenet), extracting features from the two domains and learn 5 CycleGANs to translate for each level of the feature hierarchy. At each level of the hierarchy the translation from the previous level is used to condition the translation for the current level. During inference, the final image translation is done by ""feature inversion"" (a technique proposed in Dosovitsikiy and Brox, 2016) from the final feature layer. The technique is show on example from a number of pairs of domains like Zebra-to-Elephant (and back), Giraffe-to-Zebra (and back), Dog-to-Cat (and back) and is compared with a number of baselines qualitatively and quantitatively with the FID score. <sep> * Clearly state your decision (accept or reject) with one or two key reasons for this choice. <sep> Weak Reject. <sep> Major reasons: <sep> - The problem itself, as stated in the introduction, seems ill-posed to me. One of the struggles I had while looking through the results was to understand what the images should be looking like. ie What should a zebra translated to a giraffe look like? The motivation for such a problem is also not immediately clear either. <sep> - Most of the resulting images do not seem ""translated"" to me. As stated in the paper (end of p.2) ""one aims to transform a specific type of object without changing the background."" As one can see in eg Fig. 1 the resulting translations are completely different images with the foreground object of the new domain in roughly similar poses. The background in most cases does not persist. What I suspect is actually happening here is that the high-level semantics from the first image are used as some sort of noise to generate new images from the new domain. One question I had, for example: could we be getting similar results if we used the VGG bottleneck as the noise vector in an InfoGAN? Since the VGG network is pretrained and used in the same way in both domains, I imagine we would be seeing something very similar. (and it would be def. preferrable to tuning 10 GANs!) <sep> * Provide supporting arguments for the reasons for the decision. <sep> Some of the decisions made in the paper were unclear and not supported adequately. The questions (in rough order of importance) that made some of the contributions unclear to me: <sep> - Why wasn't a final translator used for the final image, conditioned on the final \\tilde{b}_1? <sep> - Is the VGG network pretrained on ImageNet? Why wasn't another task used that could be retaining more of the relevant features? eg on semantic segmentation <sep> - Could this be used for networks pretrained on other datasets? Presumably ImageNet has information about the animals translated in this paper. Even better, could we somehow learn these features for the domain pairs automatically somehow? <sep> - How meaningful is the FID score really in this case? <sep> - How were the 10 GANs tuned? <sep> * Provide additional feedback with the aim to improve the paper. Make it clear that these points are here to help, and not necessarily part of your decision assessment. <sep> - It is mentioned on p.4 that ""clamping is potentially a harmful irreversible operation"" but that harmful results were not observed. As I was reading that I was wondering how these results would actually look like. <sep> - On p. 6 it is mentioned that the number of images for 2 categories are reported in another paper. I think it'd take less space to actually report the number of images here. <sep> - On p.7 it is mentioned that the number of instances is preserved, however it should be made clear that it's is perserved in some (or most if that is what was observed) of the examples.","The paper addresses image translation by extending prior models, e.g. CycleGAN, to domain pairs that have significantly different shape variations. The main technical idea is to apply the translation directly on the deep feature maps (instead of on the pixel level). <sep> While acknowledging that the proposed model is potentially useful, the reviewers raised several important concerns: <sep> (1) ill-posed formulation of the problem and what is desirable, (2) using fine-tuned/pre-trained VGG features, (3) computational cost of the proposed approach, i.e. training a cascade of pairs of translators (one pair per layer). <sep> AC can confirm that all three reviewers have read the author responses. AC suggests, in its current state the manuscript is not ready for a publication. We hope the reviews are useful for improving and revising the paper."
"abstract | weakness | rebuttal_process | decision  ==>  ==> ## Summary of the Paper <sep> This paper introduces a new architecture for autoencoders based on the concept of *charts* from (differential) topology. Instead of learning a single latent representation, the paper proposes learning charts, which serve as local latent representations. Experiments demonstrate that the local representations perform favourably in terms of approximating  the underlying manifold. <sep> ## Summary of the Review <sep> This is an interesting paper with an original idea. I appreciate the use of concepts from differential topology in deep learning and agree with the paper that such a perspective is required to increase our understanding of complicated manifold data sets. However, I find the following issues with the paper in its current form, which prevent me from endorsing it for acceptance: <sep> 1. I have doubts about the technical correctness of the proposed architecture; specifically, the relevance of the *initial* latent representation, which employs a Euclidean space, is not analysed. <sep> 2. The role of the number of charts, which needs to be specified before-hand, is not analysed in an ablation study. <sep> 3. The experiments do not showcase the *conceptual* improvements of the proposed technique. <sep> I shall briefly comment on each of these points before discussing other improvements. <sep> I want to point out that I really like the ideas presented in this paper and I think it has the potential to make a strong contribution but the issues in their current form require substantial revisions and additions. <sep> # Concern 1: Technical correctness <sep> The paper claims at multiple places that the geometry of Euclidean space is 'trivial' or 'too simplistic' to meaningfully reflect the structure of the data. This claim is double-edged, though: first, there are many methods that use autoencoders based on these spaces that exhibit sufficient reconstruction capabilities. Second, the proposed architecture itself uses a Euclidean latent representation as its initial encoder. The paper states that 'Ideally, this step preserves the topology of the data [...]', but this is never analysed. <sep> I fully agree with the idea that charts are a suitable way to describe complicated manifolds, but the paper needs more precision when terms such as 'topology' and 'geometry' are being used. Likewise, I disagree with referring to Euclidean space as 'trivial'. Again, other methods demonstrate that the space captures high-level phenomena sufficiently well for reconstruction purposes. At the very least, the paper should be more precise here. <sep> Moreover, I would recommend experiments in which the dimensionality of the initial encoder is discussed. <sep> # Concern 2: Number of charts <sep> Selecting the number of charts appears to me as a critical component of the proposed method. While the appendix contains one experiment for <sep> MNIST with different numbers of charts, this concept needs to be fleshed out more. How do we know that we have a sufficient number of charts? <sep> Since in differential topology, the choice of chart should not matter, <sep> how does it behave in these cases? Is there a way to detect that the number of charts must be increased? <sep> I could envision something like a simple 'step size' control procedure: <sep> if a quality measure indicates that there need to be more charts, double the number of charts and re-run the training; if the number of charts is too big, halve it and re-run the training. <sep> I get the idea that increasing the number of charts will probably decrease the reconstruction error, but this comes at the obvious expense of even more parameters. I thus recommend another set of experiments that shows the influence of the number of charts, maybe even on the synthetic data sets used in the paper. <sep> # Concern 3: Conceptual improvements <sep> While I enjoyed the didactic approach of the paper, which first introduces simple test data sets to illustrate the concepts, my main question is about the conceptual improvements that the charts provide in the end. <sep> I see that the reconstruction error for MNIST goes down---but there are also significantly more (!) parameters than in the comparison architectures. The ideas of the sampling or interpolation experiments go in the right direction, but in their present version, they are not entirely convincing. In fact, they even raised more questions for me: <sep> - Figure 6 depicts individual charts but their *covering* of the space is highly non-uniform. The letter '0' is covered more often than the letter '1', for example. How can this be compatible with the claim that the novel architecture learns a suitable set of charts? I could understand some overlaps, but there seems to be a clear difference between the charts generated in the synthetic examples---which do appear to be cover everything in a uniform manner---and the charts for <sep> MNIST. This needs to be elucidated some more, in particular since the paper writes that the charts 'cover [...] in a balanced and regular way'. <sep> - The digit morphing example is not not entirely convincing to me. Is this not something that I can do equally well with a VAE or generative models in general? I am *not* disputing the claims of the paper here, <sep> I am merely stating that *if* the new method is beneficial for this sort of application, a more in-depth experiment is required. <sep> Thus, while I would like to give the paper the benefit of the doubt, it does not show just *why* it is relevant to have a chart-based embedding. <sep> Some suggestions for a set of experiments: <sep> - Do charts help in separating the input space? I would hypothesise that this is the case---it thus might be worthwhile to study low-dimensional embeddings obtained based on each chart and 'stitch' <sep> them together. <sep> - Do charts tell us something about the properties of a manifold? For example, are certain charts 'easier' to embed than others? This could be used to indicate different dimensions in a data set. <sep> ## Experimental setup <sep> I have one major point of critique here, namely the way results are presented without any measures of tendency. Instead of showing a bar plot in Figure 7, I would suggest showing a Table with standard deviations along multiple repetitions of the experiment. It is not clear from looking at this to what extent this results can be replicated. <sep> Moreover, a discussion of the number of parameters is required. To some extent, I find it not surprising that a better reconstruction error is achieved if more parameters are present. <sep> This makes some of the claims in the paper hard to assess. <sep> ## Technical clarity <sep> The papers is generally written well and has a good expository style. <sep> Here are some cases where I find that clarity can be improved: <sep> - To add to what I wrote above: if charts are Euclidean as well, the paper should elucidate why Euclidean charts do *not* suffer from being too simplistic. <sep> - The discussion of homeomorphisms in the introduction is slightly misleading; none of the functions learned later on is a homeomorphism because of the latent space dimensions. <sep> - Homeomorphic mappings of manifolds into a Euclidean space are not necessarily desirable---this is why the definition of a manifold uses the concept of neighbourhoods. I think this should be rephrased in a positive manner, as in: manifolds are complex, so we cannot expect a *single* map to suffice... <sep> - The leading example of a torus embedding needs more details. Why is the structure destroyed? <sep> - The introduction of 'topological features' on p. 2 is slightly abrupt. <sep> It would be sufficient to explain by means of the figure that the mapping obviously does not respect all properties. <sep> - p.2: paths become invariant to _what_ exactly? <sep> - p.2: what is the 'topological class'? <sep> - p.2: would LLE not be a good precursor to the method proposed in this paper? <sep> - p.2: is this paper to be seen as an implementation of Chen et al.  (2019)? <sep> This should be made more clear. <sep> - p.3: the concept of intrinsic dimension slightly varies in literature. <sep> I would propose mentioning the homeomorphism of every chart to some d-dimensional space, and state that if this exists, one calls the manifold d-dimensional. <sep> - p.3: the circle example could be explained in more detail for readers unfamiliar with the concepts. <sep> - p.4: the chart prediction module requires a brief explanation at the point when it is first introduced (1 sentence is sufficient). The method plus architecture is presented but the details come very late; <sep> I would prefer some intuition here <sep> - p.4: N needs to be defined earlier <sep> - p.4: how is the dimension of latent spaces chosen? Please also refer to my comments on the experiments above. <sep> - p.5: Section 4.1 again mixes 'topological' and 'geometrical' concepts; <sep> suddenly, the concept of curvature crops up---this needs to be explained better! <sep> - p.5: Distances can always be measured in connected subsets of real-valued spaces; whether the set is open or closed does not change the fact that a centre exists. Am I misunderstanding this? <sep> - p.5: I like the 'partition of unity' approach, but to me, this reads like a convex combination of predictions. Am I misreading this? If not, I would suggest to rephrase this. <sep> - p.5/6: the goals of the new method need to be stated more clearly; the paper needs to explain better to what extent *reconstruction error* is affected by charts (it does not seem to be, as I outlined above)---and this again raises the question of which quality measure the new method <sep> *can* preserve. <sep> - p.6: the definition of the Lipschitz constant could be more precise; <sep> please specify the requirements f has to satisfy <sep> - Eq. 4 needs more details for me: it seems as if the weights appear twice as a kind of 'decay term' (in the second part, I see the sum but the product appears in both terms). This should be stated more clearly. <sep> - p.6: the pre-training needs more details; how crucial is this step? <sep> - p.6: what does the 'orientation' imply? It is not defined except in the appendix. <sep> - p.6: the jump from the illustrative examples to the non-synthetic ones is large; the uniform sampling of the latent space does not scale to higher dimensions, for example. The paper should comment on this if possible. <sep> - In general, I would recommend giving the employed models more <sep> 'speaking' names. I found it hard to keep track of all of them and had to refer to the appendix constantly. <sep> - For Figure 4, please show the full space, together will all charts <sep> - p.7: please give some ideas (see above) for how to use the covering of the points in practice; I like that the object can be reconstructed with a proper set of charts, but the paper could make the necessity of the technique much more obvious by choosing stronger examples. <sep> - p.7: the object arguably *also* has a complex geometry, not only complex topology. This should be mentioned. <sep> - p.8: the discussion of MNIST is slightly incorrect; as outlined above, <sep> many digits appear to be generated by multiple charts, while some, <sep> such as `1` do not appear on more than one chart. <sep> - The metrics in Section 5.3 should be introduced earlier, maybe at the expense of some exposition in the introduction or the simpler examples; it is not good style to have to refer to the appendix to understand a core experiment of a paper. <sep> - p.8: I do not understand the term 'wholly pyramid'. <sep> - p.11: the decoder should map to xi, if I am not mistaken <sep> - p.11: I would suggest a more consistent terminology to describe the models. The prediction function is replicated multiple times, for example, so why not introduce a shorthand notation for this? <sep> - p.12: '\\cup' and '\\cap' need to be switched: the *intersection* of domains needs to be empty, not their *union* <sep> - p.13: to what extent are the 'faithfulness' and 'coverage' established metrics? It seems that they are developed for this paper, so I would explain them in the main text and also make clear why they are desirable metrics---else, the metrics could be criticised as being fine-tuned for the proposed method. <sep> For example, if *coverage* can measure the phenomenon of *mode collapse*, this needs to be demonstrated. <sep> ## Minor comments <sep> Some typos: <sep> - low dimensional --> low-dimensional <sep> - eigen-functions --> eigenfunctions <sep> - considers manifold point --> considers a manifold point <sep> - paring subnetwork --> pairing subnetwork <sep> - paramterized --> parametrized <sep> - preformed --> performed [occurs multiple times] <sep> - chats --> charts <sep> - Lipshitz --> Lipschitz (in Figure 4) <sep> - evalutation --> evaluation <sep> - seciton --> section","This paper proposes to use more varied geometric structures of latent spaces to capture the manifold structure of the data, and provide experiments with synthetic and real data that show some promise in terms of approximating manifolds. <sep> While reviewers appreciate the motivation behind the paper and see that angle as potentially resulting in a strong paper in the future, they have concerns that the method is too complicated and that the experimental results are not fully convincing that the proposed method is useful, with also not enough ablation studies. Authors provided some additional results and clarified explanations in their revisions, but reviewers still believe there is more work required to deliver a submission warranting acceptance in terms of justifying the complicated architecture experimentally. <sep> Therefore, we do not recommend acceptance."
"abstract | misc | rating_summary | ac_disagreement | decision  ==>  ==> Authors of this paper propose the restricted autoencoder (RAE) framework for selecting features that can accurately reconstruct the rest of features. Authors justify the proposed method via the proof that the reconstruction ability of a set of features bounds its performance in downstream supervised learning tasks. The algorithm that iteratively eliminates features using learned per-feature corruption rates is proposed. <sep> The fundamental of this paper is built on the argument that the optimal approach is to select a set of features that can accurately reconstruct all the remaining features for the settings where they will be used in downstream prediction tasks. Authors studied the performance losses of linear and nonlinear models by using the defined imputation losses.  Some concerns are listed as follows: <sep> 1. the theorem based on strong assumptions that all learned models are optimal. The applicability of the theoretical results to general prediction model is still questionable. <sep> 2. only the prediction problems of least square (linear or nonlinear) are studied. It is not equivalent to the downstream supervised learning tasks. It is just a special case study. <sep> 3. It is unclear how to get the conclusion from Theorem 1 that the linear imputation loss is equal to the sum of eigenvalues. Please clarify it in details. <sep> The RFE-like algorithm is used to solve (7). However, the sensitivity measures used in Algorithm 1 seems to take the different optimization problems since additional regularization terms are added. This is different from RFE where a single SVM optimization problem is used and the ranking score is solely based on the learned SVM classifier. The discussion on the inconsistency of learning h_{\\theta} and the sensitivity measures could be interesting. <sep> In the experiments, authors did not mention the parameter settings of all compared methods. It is known that the unsupervised feature selection methods incorporate priors with usually various parameters. For fair comparisons, it is better to report the properly tuned results since these parameters are often data-dependent.","This paper proposes Restricted AutoEncoders (REAs) for unsupervised feature selection, and applies and evaluates it in applications in biology. The paper was reviewed by three experts. R1 recommends Weak Reject, identifying some specific technical concerns as well as questions about missing and unclear experimental details. R2 recommends Reject, with concerns about limited novelty and unconvincing experimental results. R3 recommends Weak Accept saying that the overall idea is good, but also feels the contribution is ""severely undermined"" by a recently-published paper that proposes a very similar approach. Given that that paper (at ECMLPKDD 2019) was presented just one week before the deadline for *CONF*, we would not have expected the authors to cite the paper. Nevertheless, given the concerns expressed by the other reviewers and the lack of an author response to help clarify the novelty, technical concerns, and missing details, we are not able to recommend acceptance. We believe the paper does have significant merit and hope that the reviewer comments will help authors in preparing a revision for another venue."
"abstract | weakness | rebuttal_process | misc | weakness | suggestion | misc  ==> This paper mainly studies the relationship between the generalization error and mean/variance of the test accuracy. The authors first propose a new score for pruning called E[BN]. Then, the authors observe the generalization error and the test accuracy mean/variance for pruning large score weights and small score weights for VGG11, ResNet18, and Conv4 models. From these experiments, the authors observe that pruning large score weights generates instable but high test accuracy and smaller generalization gap compared to pruning small score weights. The authors additionally study some other aspects of pruning (e.g., pruning as a noise injection) and conclude the paper. <sep> Overall, I am not sure whether the observation holds in general due to the below reasons. <sep> - The authors proposed a new score E[BN] and all experiments are performed on this. However, how it differs from the usual magnitude-based pruning in practice is unclear. I would like to know whether similar behavior is observed for the naïve magnitude-based pruning. <sep> - I think that the author's observation is quite restricted and cannot extend to a general statement since the experiments are only done for pruning small score/large score weights. To verify the generalization and instability trade-off, I believe that it is necessary to examine several (artificial) pruning methods controlling the instability of test accuracies and check whether the proposed trade-off holds. For example, one can design pruning methods that disconnect (or almost disconnect) the network connection from the bottom to the top (i.e., pruned network always outputs constant) with some probability to extremely increase the instability. <sep> - The authors did not report the results for high sparsity. <sep> Besides, I am not sure the meaning of the instability since when the test accuracy of the pruned model is higher than that of the unpruned model, the instability could be large. <sep> Other comments: <sep> - The first paragraph mentions that the generalization gap might be a function of the number of parameters. However, I think that it is quite trivial that the generalization gap is not a function of the number of parameters while it only provides the upper bound. <sep> ---------------------------------------------------------- <sep> I have read the authors' response. Thanks for clarifying the definition of instability and additional experiments with high sparsity. However, I will maintain my score due to the following concern. <sep> The remaining concern is that the current evidence for verifying generalization-stability tradeoff is not convincing as the authors presented only some examples having small and large instability (e.g., pruning smallest/largest weights) under the same pruning algorithm. I think that the results would be more convincing if the authors add a test accuracy plots given a fixed prune ratio, whose x-axis is controlled instabilities (e.g., from 10% to 90%) among various pruning algorithms (other than magnitude-based ones, e.g., Hessian based methods). It would be much more interesting if the same instability results same test accuracy even for different pruning algorithms.","The authors introduce a notion of stability to pruning and argue through empirical evaluation that pruning leads to improved generalization when it introduces instability. The reviewers were largely unconvinced, though for very different reasons. The idea that ""Bayesian ideas"" explain what's going on seems obviously wrong to me. The third reviewer seems to think there's a tautology lurking here and that doesn't seem to be true to me either. It is disappointing that the reviewers did not re-engage with the authors after the authors produced extensive rebuttals. Unfortunately, this is a widespread pattern this year. <sep> Even though I'm inclined to ignore aspects of these reviews, I feel that there needs to be a broader empirical study to confirm these findings. In the next iteration of the paper, I believe it may also be important to relate these ideas to [1]. It would be interesting to compare also on the networks studied in [1], which are more diverse. <sep> [1] The Lottery Ticket Hypothesis at Scale (Jonathan Frankle, Gintare Karolina Dziugaite, Daniel M. Roy, and Michael Carbin) https://arxiv.org/abs/1903.01611"
"abstract | strength | weakness | decision  ==>  ==> This paper starts with a conceptual claim that incorporating a notion of ""empathy"" in language emergency would help agents learn faster.  The paper then proposes a learning mechanism for implementing this, and looks at its empirical effect for the case of a Speaker-Listener game. <sep> The concept at the core of the paper is thought-provoking, somewhat grounded in human communication, and it's interesting to see how this can be translated into a learning mechanism for the multi-agent setting.   The specific implementation proposed seems reasonable at a high level, however there are many technical details missing which really hamper the paper's message & potential scientific impact.   The results are limited to a single game, with just a pairwise comparison (with and without ""empathy""), and provide a narrow view into the effectiveness of the proposed technique. <sep> My main problem with the paper is the clarity & organization problems.  Usually I tend to be lenient on this, thinking poor writing is much easier to fix than poor science.  But in this case the problems are large enough that the paper is just too far from the standard for *CONF* publication.  It also fits in 5 pages, so the authors had lots of space to write a much better paper.   I encourage them to do this for a future submission, in addition to more extensive results, because I think the ideas are worthwhile. <sep> Specific comments: <sep> Design of the empathy mechanism. Can you motivate why it's reasonable to ""achieve a high relation between the hidden states of both agents""? Is this necessary / sufficient for empathy?  What are alternate framings of this?  What are properties and pros/cons of this framing? <sep> Sec.4 needs a lot more detail!   Sections Agent setup, Learning and Empathy Extension in Sec.5 should be moved to Sec.4, since they describe the method, rather than the experiments. <sep> Sec.5 needs better clarity. <sep> o What are m^l_t and M^{<l}_t in Eqn 5?  Define how h_S and h_L are parameterized, and how each is trained. <sep> o Fig.2 gives a high-level view of the approach, but lacks important details.  Do you apply a loss at both the Speaker's Decoder output, and the Listener's Decoder output?  Or just the latter?  What is the loss specifically? I assume combination of Eqn 5 and Eqn 6, but not sure. <sep> o If you train just on the loss of the Listener's Decoder, does this mean this is backpropagated all the way to train the Speaker?  How would this be done in a real system?  It's a very strong assumption to say that the Listener will share gradients with the Speaker.  It seems more realistic to assume they will each observe a loss and train independently. <sep> Results are very brief. <sep> o How robust are the results to the specification of the \\alpha (the loss weight from Eqn 6)?  How much data goes to finding a good \\alpha? <sep> o What is the difference between the left and the right plot?  Is one for the Speaker and the other for the Listener? <sep> o How do you measure ""learning speed"", which is the main metric discussed in the text of Sec.6? <sep> o How do the results change by number of concepts in the game? <sep> o Do you do any pre-training of the encoder/decoder networks? <sep> o Can you show confidence intervals on each curve? <sep> o Can you show test performance? <sep> o Are there other related games to consider? <sep> Many references missing throughout to support statements, e.g. <sep> o ""Natural language is not as rule-based as…"" <sep> o ""These considerations led to the research field of emergent communication…"" <sep> o Sec.2:  Earlier refs to RL in general (e.g. work Sutton in the 1980's). Earlier refs to RL with neural networks (e.g. work of G. Tesauro; work of M. Riedmiller). <sep> o Referential game in Fig.1 caption. <sep> Some minor language issues, e.g. <sep> o ""The field was then alleviated"" -> Do you mean elevated?","This paper introduces the idea of ""empathy"" to improve learning in communication emergence. The reviewers all agree that the idea is interesting and well described. However, this paper clearly falls short on delivering the detailed and sufficient experiments and results to demonstrate whether and how the idea works. <sep> I thank the authors for submitting this research to *CONF* and encourage following up on the reviewers' comments and suggestions for future submission."
"abstract | strength | weakness | rebuttal_process | rating_summary  ==> The paper claims that for invertible neural networks, mathematical guarantees on invertibility is not enough, and we also require numerical invertibility. To this end, the lipschitz constants/condition numbers of Jacobians of both the forward and inverse maps of invertible NNs based on coupling layers are examined mathematically and experimentally. The paper also displays cases that expose non-invertibility in these architectures via gradient-based construction of adversarial inputs, as well as a decorrelation benchmark task, and show that spectral normalization can be a remedy for stabilizing these flows. <sep> I think it's a good point that we need to monitor the Lipschitz constant/bounds of both directions of these invertible functions. It's true that the focus for stabilising NNs by bounding Lipschitz constants was always on the forward function, and for invertible functions we should also ensure that the inverse is numerically stable to compute. <sep> The mathematical contribution of the paper is twofold - 1. deriving bounds on the lipschitz constants of the forward and inverse mapping of additive/affine coupling blocks 2. summarising known lipschitz bounds of forward and inverse mappings of other invertible layers (iResNet, neuralODE, invertible 1x1 convolutions etc). The main contribution lies in 1, and the derivation for the additive coupling block (volume preserving) is neat (although fairly straightforward), but the derivation for the affine coupling layer (NVP) is not useful nor insightful; they are local Lipschitz bounds (so require bounds on all intermediate activations, which is difficult as pointed out by the authors), and the numerical value of this bound was not used at all in relation to the numerical experiments - I imagine the bound is loose. Given that it seems difficult to find a tight global lipschitz bound, I think it would be more insightful to compute a lower bound to the lipschitz constant of the model (with fixed parameter values) by maximising the spectral norm of the Jacobian with respect to the inputs (or outputs if looking at the inverse map) - this will yield a lower bound by Lemma 3. This will be numerical, but more informative since it will give you an indication of where in the input space (or output space if looking at the inverse) there could be numerical instabilities. Also I think the bound on the local lipschitz constant of the inverse for the affine coupling block might be incorrect, because in A.1.1, the inverse map is F^{-1}(y)_I1 = y_I1, F^{-1}(y)_I2 = (y_I2 - t(y_I1))/g(s(y_I1)), so the scale and shift is s'(y_I1) := 1/g(s(y_I1)) and t'(y_I1):=- t(y_I1)/g(s(y_I1)), and hence I think this needs to be taken into account for computing the lipschitz bound of the inverse <sep> I have mixed feelings about the experimental section. In section 4.1, it is interesting to see that we can find inputs where trained flow models can show numerical non-invertibility, evident in the poor reconstructions. It would be a nice addition to investigate whether this is coming from the forward function or its inverse, by examining the norm of the Jacobian of F and F^{-1} at the input x_delta and output F(x_delta) respectively. <sep> However, the decorrelation task introduced in section 4.2 is puzzling. I don't understand why for these invertible models, you are investigating invertibility for parameter values trained to decorrelate, as opposed to parameter values used in the usual task of density estimation with flows (or any other standard application of invertible NNs). The two reasons given in the paper are that 1) decorrelation is a simpler task and 2) it allows both stable and unstable transforms as solutions, but these are not convincing. Point 2) holds for flow-based density estimation as well, and regarding point 1), density estimation is the task we usually care about when using invertible NNs, and this is also computationally plausible/tractable, whereas even if decorrelation is a simpler task, it's not a task that users of invertible NNs are interested in. It is good to know that these invertible NN architectures CAN admit values that are numerically non-invertible, but I would be much more interested to know whether this actually holds when they have been trained for flow-based density estimation. I'm not sure whether the experimental results on models trained for the decorrelation task are useful, because a model that is stable when trained for the decorrelation task may be unstable when trained for flow-based density estimation and vice versa. The observation that spectral normalization can help address numerical instability is useful, but from the perspective of someone who wants to use these invertible NNs for density estimation, I would like to know what is the sacrifice in expressivity/validation performance (if any) when using spectral normalization in these invertible architectures. Also, the results would be more relevant if the architectures resembled the architectures used for invertible models used in the literature (e.g. GLOW) where we not only have coupling layers but they are interleaved with PLU linear flows. <sep> In section 5, the result that Flow-GANs can be numerically non-invertible is more relevant, and it is useful to know that spectral normalisation can help resolve this issue, but again it would be useful to quantify whether this comes at the cost of the quality of generated samples (Figure 3 shows several samples, but a more thorough quantitative & qualitative comparison would be welcome). Also regarding the point about likelihood in Section 5, where the authors state ""it cannot be trusted as true likelihood due to lack of invertibility"", I think it should be emphasised that this point holds specifically for flow-GANs where for F: z -> x, you need a numerically accurate F^{-1} to compute the density, but for standard flow-based density estimation where F:x -> z, you never need to compute the inverse for computing the likelihood, hence if F has a small lipschitz constant then the likelihood will be accurate, regardless of whether the inverse is numerically stable or not. <sep> Overall I believe the experimental section can be largely improved, and given that the motivation of the paper is nice and the paper is clearly written and nicely presented, it would be a shame to leave the experiment section as it is. <sep> Minor typos/Qs: <sep> p2: this problems <- this problem p8: and with maximum likelihood (ML) - should this be removed? <sep> p13: t(x_I2) <- t(x_I1)",This submission analyses the numerical invertibility of analytically invertible neural networks and shows that analytical invertibility does not guarantee numerical invertibility of some invertible networks under certain conditions (e.g. adversarial perturbation). <sep> Strengths: <sep> -The work is interesting and the theoretical analysis is insightful. <sep> Weaknesses: <sep> -The main concern shared by all reviewers was the weakness of the experimental section including (i) insufficient motivation of the decorrelation task; (ii) missing comparisons and experimental settings. <sep> -The paper clarity could be improved. <sep> Both weaknesses were not sufficiently addressed in the rebuttal. All reviewer recommendations were borderline to reject.
"abstract | rating_summary | strength | weakness  ==> This paper proposes and analyzes a new loss function for linear autoencoders (LAEs) whose minima directly recover the principal components of the data. The core idea is to simultaneously solve a set of MSE LAE problems with tied weights and increasingly stringent masks on the encoder/decoder matrices. My intuition is that the weights that touch every subproblem are the most motivated to find the largest principal component, the weights that touch all but one find the next largest, and so forth; I found this idea clever and elegant. <sep> That said, I lean towards rejection, because the paper does not do a very good job of demonstrating the practical or theoretical utility of this approach. As I see it, there are two main claims that one could make to motivate this work: <sep> 1. This is a practical algorithm for doing PCA. <sep> 2. This is a step towards better understanding (and perhaps improving) nonlinear autoencoders, which do things that PCA can't. <sep> Claim (2) might be compelling, but the authors do not make it, and it isn't self evident. <sep> I do not find claim (1) convincing on the basis of the evidence presented. PCA is an extremely well studied problem, with lots of good solutions such as randomized SVD (Halko et al., 2009). A possible advantage of using LAEs to address the PCA problem is that they play nicely with SGD, but again, the claim that the SGD-LAE approach is superior to, say, randomized SVD on a data subsample requires evidence. Also, even if one buys the claim that LAEs are a good way to solve PCA, one can always recover the eigenvectors/eigenvalues by a final decomposition step; the authors claim that an advantage of their approach is that it does not require such ""bells and whistles"", but this seems like a pretty minor consideration; implementing the proposed loss function seems at least as complicated as making a call to an SVD solver, and it's hard for me to imagine a situation where the cost of that final SVD isn't dominated by the cost of solving the MSE LAE problem. <sep> In summary, I think this paper proposes a clever and elegant solution to a problem that doesn't seem to be very important. I can't recommend acceptance unless the authors can come up with a stronger argument for why it's not just interesting but also useful.","Quoting from R3: ""This paper proposes and analyzes a new loss function for linear autoencoders (LAEs) whose minima directly recover the principal components of the data. The core idea is to simultaneously solve a set of MSE LAE problems with tied weights and increasingly stringent masks on the encoder/decoder matrices."" With two weak acceptance recommendations and a recommendation for rejection, this paper is borderline in terms of its scores. <sep> The approach and idea are interesting. The main shortcoming of the paper, as highlighted by the reviewers, is that the approach and theoretical analysis are not properly motivated to solve an actual problem faced in real-world data. The approach does not provide a better algorithm for recovering the eigenvectors of the data, nor is it proposed as part of a learning framework to solve a real-world problem. Experiments are shown on synthetic data and MNIST. As a stand-alone theoretical result, it leaves open questions as to the proposed utility."
"abstract | strength | weakness | rebuttal_process | weakness | decision  ==> Summary <sep> This paper proposes an attribution method, linearly estimated gradient (LEG) for deep networks in the image setting. <sep> The paper also introduces a variant of the estimator called LEG-TV, which includes a TV penalty, and provides a theorem on the convergence rate of the estimator. The paper finds that the LEG attributions pass sanity checks. <sep> My recommendation <sep> Overall, I am recommending this paper as a weak accept. There are several points to address with regards to the exposition and flow of the paper, which is my biggest issue with this paper. I believe the authors can address this point and I am willing to raise my point on this basis. The paper also provide some theoretical analysis of the proposed method, which is typically lacking for most of the interpretation methods in this domain. <sep> Possible Improvements <sep> - The LEG method is not sufficiently motivated. Here, I am specifically referring to the functional form of the estimated itself in definition 1. See the question section  for some of the issues <sep> I raised there. <sep> - From figure 4, we see that the method passes the proposed sanity checks which seem like a key motivation for this work, however, the authors don't give an explanation for why this is the case. <sep> - The paper notes that LEG can be estimated using an LP; it would have been great for the authors to completely spell this out in the appendix or somewhere in the text. What is the exact form of the <sep> LP? What are the constraints? <sep> - As the authors know, the two evaluations presented in the paper: sanity checks, and the zeroing out procedure (in figure 5) don't actually tell us which method is a good method, just rule out a method. <sep> I would encourage the authors to design a toy task where the ground truth attributions are know, then train a model to be 100 percent or so accurate on this task. You can then obtain LEG-TV estimates from this model and compare to the ground-truth. <sep> - I found the proof of lemma 1 confusing, the authors say it follows trivially, but I don't see it. For example, <sep> there should be a factor of 2 somewhere after taking the derivative wrt to vec(g), but I don't see it. It is fine for the authors to spell out the derivation here if possible. <sep> -The paper ends quite abruptly with no conclusion or discussion. It would be great to include a wrap up section that puts the contributions into context. <sep> - I get the sense that this method should be computationally intensive, though the paper says otherwise. <sep> It is fine for a method to be computationally intensive, but can the authors speak to this issue? <sep> Some Questions <sep> Definition 1: I had a difficult time understanding this definition. What is g here? I assume it is the gradient based on the reference to the first order Taylor expansion. In addition, why is the estimand squared? Further, What does it mean to take expectation wrt F+x0. I was particularly confused by the last point, because F is a continuous distribution, while presumably x0 is the point of interest. The paper notes in several places that it can sample from F+x0, <sep> is this equivalent to sampling from F and adding point x0? <sep> What is LEG0 in figure 5? <sep> Is κ in your theorem 1, the condition number of the covariance matrix of the perturbation? <sep> Conclusion <sep> Overall, the paper provides a nice method along with analysis on convergence rates and other statistical properties. Several of the key issues/questions I have about the paper are raised above. None of these should be dealbreakers but would require the authors to flesh out more details and possibly justify certain choices. In general, I think more effort should be put into the flow and writing of the paper. <sep> Overall, this is an interesting contribution. <sep> ## After reading author responses <sep> I believe the authors have clarified and improved the readability of the paper and clarified several of the questions that I had. I am raising my score to an accept. While I believe this is a valuable contribution to the sea of attribution methods that have now been published, like the authors noted, it is still not clearly if attribution methods as a whole are useful of decision making or understanding of a model by either a generic end user or the model developer. <sep> This is a huge problem in this area that deserves significant attention. This said, the goal of this current paper is to take a step towards developing a principled method, so this is a step in that direction perhaps.","This submission proposes a statistically consistent saliency estimation method for visual model explainability. <sep> Strengths: <sep> -The method is novel, interesting, and passes some recently proposed sanity checks for these methods. <sep> Weaknesses: <sep> -The evaluation was flawed in several aspects. <sep> -The readability needed improvement. <sep> After the author feedback period remaining issues were: <sep> -A discussion of two points is missing: (i) why are these models so sensitive to the resolution of the saliency map? How does the performance of LEG change with the resolution (e.g. does it degrade for higher resolution?)? (ii) Figure 6 suggests that SHAP performs best at identifying ""pixels that are crucial for the predictions"". However, the authors use Figure 7 to argue that LEG is better at identifying salient ""pixels that are more likely to be relevant for the prediction"". These two observations are contradictory and should be resolved. <sep> -The evaluation is still missing some key details for interpreting the results. For example, how representative are the 3 images chosen in Figure 7? Also, in section 5.1 the authors don't describe how many images are included in their sanity check analysis or how those images were chosen. <sep> -The new discussion section is not actually a discussion section but a conclusion/summary section. <sep> Because of these issues, AC believes that the work is theoretically interesting but has not been sufficiently validated experimentally and does not give the reader sufficient insight into how it works and how it compares to other methods. Note also that the submission is also now more than 9 pages long, which requires that it be held to a higher standard of acceptance. <sep> Reviewers largely agreed with the stated shortcomings but were divided on their significance. <sep> AC shares the recommendation to reject."
"abstract | strength | weakness  ==>  ==> This paper presents a SIFT-feature inspired modification to the standard convolutional neural network (CNN). Specifically the authors propose three innovations: (1) a differences of Gaussians (DoG) convolutional filter; (2) a symmetric ReLU activation function (referred to as a truncated ReLU; and (3) a projected normalization layer. The paper makes the claim that the proposed CNN variant (referred to as the EVPNet) demonstrates superior performance as well as improved robustness to adversarial attacks. <sep> Clarity: Overall, the paper is not particularly well written. There are multiple missing articles and other grammatical errors that make it a bit arduous to read, though I do not believe they have obstructed my ability to understand the contributions. The section describing the projected normalization layer (second half of page 5) is a bit confusing. Figure 2(c) is not helpful in shedding light on the details, though I think a more detailed figure could be quite helpful. Beyond these issues, the paper is relatively clear in the presentation of the material. <sep> Novelty: Over the last few years there have been many, many proposals for how to vary the basic CNN architecture to improve performance. Some of these have lead to genuine performance gains and have become part of the standard CNN specification. ReLUs, ResNets and Batch Normalization are particularly prominent examples of contributions that have been shown to lead to improvements in performance. Yet the vast majority of these sorts of proposals ultimately make little or no impact on the field. In light of this, I would rate the novelty of the basic goal of this paper as relatively low, though the specific proposal is novel to me and seems reasonable. <sep> Impact: The impact potential for this paper lies with the performance offered by the proposed innovations. With respect to overall performance improvement the proposed method has not been shown to perform quite at a state-of-the-art level, as given by these resource: <sep> SVHN: https://paperswithcode.com/sota/image-classification-on-svhn <sep> CIFAR-10: https://paperswithcode.com/sota/image-classification-on-cifar-10 <sep> The authors compare the performance of their proposed EVPNet against a fair baseline - a squeeze-and-excite ResNet model. These sorts of controlled experiments are useful, but the actual reported performance for both models are somewhat off of the state-of-the-art and it's not clear that the relatively small benefit the authors show over their baselines are maintained for higher performing architectural configurations. Can this architecture be competitive with the state-of-the-art? The current paper in it's current <sep> Most of the results relate to the claim that the proposed model is robust to adversarial examples. Unfortunately, this is not a particular area of expertise for me, so it's difficult for me to provide a confident assessment of the contribution here, though I will say two things: (i) the method seems to provide a significant increase in adversarial robustness across the baseline architectures investigated. (ii) the authors demonstrate that the benefit provided by the proposed architecture seems to persist even when training for adversarial defence is introduced. <sep> I would have liked to see more datasets explored in Experiments section. I  especially would have liked to see results on ImageNet. <sep> My current rating is weak reject based on the weakness of the writing and the lack of strong empirical evidence in support of the effectiveness of the proposed contributions.","This manuscript proposed biologically-inspired modifications to convolutional neural networks including differences of Gaussians convolutional filter, a truncated ReLU, and a modified projected normalization layer. The authors' results indicate that the modifications improve performance as well as improved robustness to adversarial attacks. <sep> The reviewers and AC agree that the problem studied is timely and interesting, and closely related to a variety of recent work on robust model architectures. However, this manuscript also received quite divergent reviews, resulting from differences in opinion about the novelty and importance of the results. In reviews and discussion, the reviewers noted issues with clarity of the presentation and sufficient justification of the approach and results. In the opinion of the AC, the manuscript in its current state is borderline and could be improved with more convincing empirical justification."
"abstract | weakness | decision  ==> This work proposes a way to reduce the latencies incurred in inference for neural machine translation. Basic idea is to train a model with softmax attached to each output of decoder layers, and computes a loss by aggregating the cross entropy losses over the softmaxes. During inference, it could either use one of the softmax or train an additional model which dynamically selects softmaxes given an input string. Experimental results show that it is possible to reduce latencies by trading off the translation qualities measured by BLEU. Dynamically selection did not show any gains in latencies, though, this work empirically shows potential gains in oracle studies. This work further shows that the model could be compressed further by knowledge distillation. <sep> I have several concerns to this work and I'd recommend rejecting this submission. <sep> - One of the problems of this paper is presentation. This work basically combines three work together as a single paper, i.e., section 3 for the basic model, section 4 for dynamic selection and section 5 for distillation, with each section describing a separate experiment. I'd strongly suggest the author to focus on the main point, e.g., dynamic selection, and present the basic model and dynamic selection. Experiments should be presented in a single section for brevity. <sep> - Similarly, this work should have been submitted when meaningful gains were observed in the dynamic selection method, given that the proposal is somewhat new. Otherwise, I don't find any merits to see this accepted in *CONF*, given the rather negative results in section 4. <sep> - The description in section 4.2 is totally messed up. x^i and y^i_k are strings since they are an input sentence and an output sentence, respectively,. However, they are treated as scalars in Equation 3 by multiplied with \\delta_k, subtracted from 1 and taking sigmoid through \\sigma. I strongly suggest authors to carefully check variables used in the equations and the description in the section. <sep> - The authors claim that the use of knowledge distillation is novel. However, it is already widely known in the research community and I don't think it is worthy to keep it as a single section. It could have been described as a yet another experiment in a single experimental section. <sep> Other comment: <sep> - Although this paper claims that attaching a softmax for each output layer is new, there is a similar work in language modeling, though the motivation is totally different. <sep> Direct Output Connection for a High-Rank Language Model, Sho Takase, Jun Suzuki and Masaaki Nagata, EMNLP 2019. <sep> - In section 3.4, this paper claims that the training of all 36 models took 25.5 more time, but took 9.5 more time for a tied-model when compared with a basic 6-layer Transformer. It is not clear to me whether this comparison is meaningful given that it might be possible to employ multiple machines to train 36 models.","The paper proposed a method for training multiple transformers with tied parameters and enabling dynamic choice of the number of encoder and decoder layers. The method is evaluated in neural machine translation and shown to reduce decoding costs without compromising translation quality. The reviewers generally agreed that the proposed method is interesting, but raised issues regarding the significance of the claimed benefits and the quality of overall presentation of the paper. Based on a consensus reached in a post rebuttal discussion with the reviewers, I am recommending rejecting this paper."
"abstract | strength | rebuttal_process | decision  ==> In a standard multi-stage neural network such as a ResNet, the resize operation between stages typically reduces the spatial resolution by 0.5.  (input ---> stage 1---> 0.5 reduction ---> stage 2 --> 0.5 reduction ---> ....). In this paper the authors apply a variety of reduction factors in between these stages for different training examples (input ---> stage 1---> variable reduction ---> stage 2 --> variable reduction ---> ....). They demonstrate that simply training in this way is a powerful form of augmentation. It also produces a network which may be scaled depending on a FLOP budget. <sep> I think this is a really neat idea, and as far as I'm aware it is novel.  It is similar in spirit to EfficientNet, although more flexible. The experimental results are good. However, the paper is let down by poor writing and a lack of detail. <sep> The paper needs a rewrite as there are many grammatical errors, which cause a bad impression: <sep> - ""performs arbitrary resize operation"" ->  ""performs arbitrary resize operations"" <sep> - ""Scale is the fundamental component of the physical world"" ---> *a* fundamental component! <sep> - ""i.e the acuracy of NISP"" --> e.g. <sep> - ""compare with baseline"" -> ""compared to the baseline"" <sep> - There are several instances of a space missed out between a letter and an open bracket <sep> - ""The Figure 2(a)"" --> ""Figure 2(a)"" <sep> The comparison to weight-sharing NAS methods is unnecessary. Those entail searching for architectures and sharing weights through the search process, whereas in this work it is having a network that can take different sized inputs at different stages. On that note, it doesn't feel right to me to refer to there being many ""subnetworks"". What there really is, is just a single network that is robust to multi-scaled inputs (which is a good thing!) <sep> Figure 1 is nice! <sep> EfficientNet-B0 is a base-network that can be scaled up with a compound scaling approach found through grid search. What happens if you scale up your Resizeable net to the same FLOPs as e.g. EfficientNet-B7? <sep> I like the old-school vision citations, although referring to object detection makes me wonder why there are no experiments on it. For distillation, I recommend you cite https://arxiv.org/abs/1312.6184, as the Hinton paper is really just an extension of this. <sep> The fair sampling seems important to the method. Could a detailed explanation be included in the appendix? Do you given the performance as a result of naive sampling? What do you mean by ""Some convolutional layers can go through more training issues than others""? <sep> A big problem in this paper is that (as far as I can tell) the scaling factors considered aren't given (but we are told that they lie between 0 and 1). It isn't possible to sample these arbitrarily as you indicate that a batch-norm layer is needed for each one, so these must be discrete (because of this, it isn't correct to say that you have infinite networks contained within).  It therefore isn't clear e.g. in Table 1 which permutation of scaling factors were used in upsampling the networks. Are the results given representative of the best-case selection of these scaling? I hope this isn't the case. I am assuming it is uniform scaling in the case of the ""individually trained counterparts"". It would be interesting to know more generally which combinations worked well. <sep> In Section 3.5 I'm not sure what is meant by ""It is inefficient to process all images to one-subnetwork, as the algorithm spends equal energy at each sample"".  I assume energy use is proportional to the number of FLOPS, which in turn depends on spatial resolution. <sep> The legend in Figure 2 is close-to unreadable and needs changing. <sep> The results are impressive, but error bars would be appreciated if possible. As ImageNet is the only dataset considered, this would give some needed clout. <sep> Pros <sep> ------- <sep> - Nice, novel method <sep> - Good experimental results <sep> Cons <sep> -------- <sep> - Paper is poorly written <sep> - Very few details of the scaling factor variations <sep> - Only one dataset considered <sep> Although the paper is written badly and the narrative is muddled, the underlying idea is a nice one, which is executed well experimentally. Because of this I would like to recommend a Weak Accept, subject to the authors (i) doing a rewrite and (ii) including more information regarding the scaling permutations.","This paper offers likely novel schemes for image resizing. The performance improvement is clear. Unfortunately two reviewers find substantial clarity issues in the manuscript after revision, and the AC concurs that this is still an issue. The paper is borderline but given the number of higher ranked papers in the pool is unable to be accepted unfortunately."
"abstract | strength | decision  ==> * Summary: <sep> The paper proposes an IL method based on the f-divergence. Specifically, the paper extends f-VIM (Ke et al., 2019), which uses the f-divergence for IL, by using a sigmoid function for discriminator output's activation function. This choice of activation function yields an alternative objective function, where the reward function for an RL agent does not directly depend on the convex conjugate of the function f; the paper claims that this independency improves stability of policy learning. This proposed method is named f-VIM-sigmoid. The paper extends f-VIM-sigmoid to the setting of IL with observation and proposes f-VIMO-sigmoid. Experiments on Mujoco locomotion tasks show that f-VIM-sigmoid and f-VIMO-sigmoid perform better than existing methods. <sep> * Rating: <sep> The paper proposes a simple but interesting approach to improve stability of adversarial IL. However, the paper has issues regarding baseline methods, motivation, supports of the claim, and experiments (see below). These issues should be addressed. At the present, I vote for rejection. <sep> * Major comments: <sep> - Discussion and comparing against a simple baseline method based on swapping distributions: <sep> To make the reward function be independent of the convex conjugate f*, it is possible to simply swapping the distributions P and Q in the definition of the f-divergence. More specifically, instead of minimizing D_f(P||Q), we can minimize D_f(Q||P), where P is a data distribution and Q is a generator. In this case, pi* and pi_theta in Eq. (7) swap, and the RL agent minimizes the cost function g_f(V_w(s,a)). This cost function does not directly depend on f*, similarly to the reward function r(V_w(s,a)) in Eq. (8). This swapping is simpler and more flexible than re-parameterizing, while achieving the same goal as f-VIM-sigmoid. This swapping method should be discussed and compared against the proposed methods. <sep> - Need stronger baseline methods for ILfO: <sep> The paper should evaluate f-VIMO-sigmoid against stronger baselines, e.g., forward adversarial IL (Sun et al., 2019) which outperforms GAIL-based methods in the ILfO setting. <sep> [1] Wen Sun, Anirudh Vemula, Byron Boots, and J Andrew Bagnell. Provably efficient imitation learning from observation alone. ICML, 2019. <sep> - Using the f-divergence for ILfO is not well motivated: <sep> The paper does not provide good motivations for using f-divergence in ILfO. This makes the paper quite difficult to follow, since there is no connection between f-divergence and ILfO. <sep> - The experiments focus on evaluating existing methods rather than the proposed methods: <sep> Specifically, the proposed methods are evaluated with only one choice of the divergence (TV) in Figure 2. Meanwhile, most of Section 6 and results (Figure 3 and 4, and additional results in the appendix) focus on evaluating the existing methods (f-VIM and f-VIMO) with different choices of divergence. <sep> - The experiments in Figure 2 do not support the claim regarding stability: <sep> The paper claims to improve stability of IL by using the proposed re-parameterization. However, the experimental results do not support this claim, and the questions asked in Section 5 are not related to this claimed. Instead, it seems that re-parameterization helps avoiding local optima (possibly due to a biased reward function, see below), while stability is improved by regularizing the discriminator. I could not see how the re-parameterization improves the policy stability as claimed. <sep> - The experiments in Figure 2 seem unfair, since TV-VIM-sigmoid incorporates priors about survival bonuses: <sep> Specifically, TV-VIM-sigmoid uses sigmoid which yields strictly positive rewards, while TV-VIM uses tanh which yields positive and negative rewards. As discussed by Kostrikov et al., 2019, using strictly positive rewards incorporate strong priors about the survival bonuses, which exist in the locomotion task used in the experiments. Therefore, TV-VIM-sigmoid uses strong priors while TV-VIM does not. In order to make the comparison fairer, I suggest the authors to evaluate TV-VIM with sigmoid reward output, or include environments that do not have survival bonuses. <sep> [2] Ilya Kostrikov, Kumar Krishna Agrawal, Debidatta Dwibedi, Sergey Levine, and Jonathan Tompson. Discriminator-actor-critic: Addressing sample inefficiency and reward bias in adversarial imitation learning. *CONF*, 2019 <sep> * Minor comments: <sep> - The abstract is long and could be shorten. <sep> - Figures are too small and difficult to see, especially the legends. <sep> - Table 1 should describe the form of f in addition to its conjugate. <sep> - The title of the Algorithm 1 should be f-VIMO-sigmoid instead of f-VIMO. <sep> ** Update after response. <sep> I read the response. I thank the authors for clarifying the claims as well as the new experiments with the swap formulation. However, improving clarity of the claims is considered a major revision. I still keep the vote of rejection. <sep> Regarding reward bias. As the authors acknowledge, the improvement achieved by using reparameterization+sigmoid can be explained by two equally-plausible reasons: 1) reparameterization+sigmoid improves stability (as claimed) and 2) sigmoid gives biased rewards. The issue here is that we do not know which is the actual reason, given the current experiments in the paper. As I commented, evaluating TV-VIM with sigmoid but without reparameterization will help address this issue.","The submission performs empirical analysis on f-VIM (Ke, 2019), a method for imitation learning by f-divergence minimization. The paper especially focues on a state-only formulation akin to GAILfO (Torabi et al., 2018b). The main contributions are: <sep> 1) The paper identifies numerical proplems with the output activations of f-VIM and suggest a scheme to choose them such that the resulting rewards are bounded. <sep> 2) A regularizer that was proposed by Mescheder et al. (2018) for GANs is tested in the adversarial imitation learning setting. <sep> 3) In order to handle state-only demonstrations, the technique of GAILfO is applied to f-VIM (then denoted f-VIMO) which inputs state-nextStates instead of state-actions to the discriminator. <sep> The reviewers found the submitted paper hard to follow, which suggests a revision might make more apparent the author's contributions in later submissions of this work."
"abstract | strength | weakness | rebuttal_process | suggestion | decision  ==> The authors propose R2D2 layers, which are trained to reduce and re-use existing parameters of a neural network layer, and apply this to Transformer and LSTM architectures. The authors conduct experiments on various NLP tasks, including NLI and NMT. <sep> The main benefit of the proposed R2D2 layer is that the number of parameters can be reduced, as the existing parameters can be reused. I find this motivation compelling, particularly as it is well known Transformer networks are largely overparameterized. <sep> Comments: <sep> 1. There is no analysis on the specific choices made for dynamic weight diffusion- the way the partitioning is done could have a large effect on the end result. There's also little comparison to other ways to share weights across a model besides the proposed weight diffusion method. <sep> 2. Sharing parameters contributes a regularization effect - it is difficult to untie the contributions of increased regularization from the proposed method. This is particularly problematic as the majority of the datasets used are ""small"" by current standards. WMT en-de (authors do not include the sizes of the datasets, but this is 4.5 million sentences) is the only large scale dataset, and the BLEU drop is quite large on this dataset compared to the smaller ones such as IWSLT. <sep> To tie my points #1 and #2 together, I feel the authors did experiments on a variety of different tasks, but these style transfer and subject verb agreement tasks are not particularly interesting or realistic - instead this space should be devoted to discussions of the advantages of their method and analysis on its performance, which is quite lightly covered. <sep> 3. The authors claim that the R2D2 Transformer outperforms standard Transformer models on 5 out of 7 NMT tasks. This appears true if up-sampling with a factor of 2 is used to make the models larger again. The authors should compare to factorized/quaternion baselines which have a larger quantity of parameters as well. <sep> 4. Table 3, where results are reported on the competitive WMT en-de benchmark, lacks comparison for number of parameters and decoding speed. This table would probably have the most compelling and impactful results for this paper as this is the most competitive task (aside from the pre-training regime on MNLI/QNLI as part of GLUE). Can the authors complete this table so readers can understand the parameter reduction and inference speed possible from this method on this benchmark? <sep> (As an aside, the technique should be applicable to the DynamicConv model, which is a Transformer variant?) <sep> 5. The related work section is quite light on other approaches to reducing model size, such as knowledge distillation or quantization? While the approach taken in this paper leverages parameter sharing, the motivation is similar and I feel acknowledging this entire area of work would be relevant. <sep> 6. I'm not clear on why we see inference time decoding speed improvements based on the description of the method. Can the authors clarify this point for me?","This paper proposes a very interesting alternative to feed-forward network layers, based on Quaternion methods and Hamilton products, which has the benefit of reducing the number of parameters in the neural network (more than 50% smaller) without sacrificing performance. They conducted extensive experiments on language tasks (NMT and NLI, among others) using transformers and LSTMs. <sep> The paper appears to be clearly presented and have extensive results on a variety of tasks. However all reviewers pointed out that there is a lack of in-depth analysis and thus insight into why this approach works, as well as questions on the specific effects of regularization. These concerns were not addressed in the rebuttal period, instead leaving it to future work. My assessment is that, with further analysis, ablation studies, and comparison to alternative methods for reducing model size (quantization, etc), this paper has the potential to be quite impactful, and I look forward to future versions of this work. As it currently stands, however, I don't believe it's suitable for publication at *CONF*."
"rating_summary | weakness | decision  ==> *Revision after author response* <sep> I thank the authors for the comments on my questions. <sep> Unfortunately, I do not feel that these comments addressed my main concerns. For all my experimental analysis questions, the authors promised some analyses for future versions, but I was hoping to see at least a minor preliminary analysis at this point, to see if indeed my concerns are valid or not. <sep> Moreover, for my question number 1 about the optimization problem, the authors referred me to Corollary 1 from the paper, but that didn't really help me because, as the other reviewers also point out, the writing is quite hard to follow. <sep> Because of all these, I have decided to revise my score to a weak reject. While I believe the paper has merit, it requires revisions at many points in order for a reader to truly understand the method and trust the experimental results. <sep> -------------------------------------------------------------------------------------------------------------- <sep> The paper proposes a curriculum learning approach that relies on a new metric, the dynamic instance hardness (DIH). DIH is used to measure the difficulty of each sample while training (in an online fashion), and to decide which samples to train on next. The authors provide extensive experiments on 11 datasets as well as some theoretical motivation for the use of this approach. <sep> ---- Overall opinion ---- <sep> Overall I believe this paper is an interesting take on curriculum learning that is able to achieve good results. I believe this approach is a combination of core ideas from multiple sources, such as boosting, self-paced learning, continual learning and other curriculum learning approaches, but overall it seems different enough from each one of them individually. Because of the resemblance with these many different methods, the method itself does not surprise through the novelty of a new idea, but the authors seemed to have found something that was missing from these methods and that leads to very good results. The experimental results look great, but I believe the paper is missing some ablation studies to assess the importance of certain components (see details below). I also had some trouble understanding certain arguments, which I hope the authors can clarify. <sep> ---- Major issues ---- <sep> 1. I find the arguments section 2.1 quite difficult to follow. In particular, under the assumption stated in the paper that r_t(i) = f(i|S_{1:t−1}) =  f(e_i + S_{1:t−1}) − f(S_{1:t−1}) , why does it follow that r_t(i) can be used instead of f in the minimization problem (2). <sep> 2. Based on the method itself, it seems to me that the parameter k_t could would have a lot of influence on how well the method doing.  The authors mention in the experimental section what values they use, but there is no indication on how one would choose this value. Moreover, it would be good to see an analysis of how sensitive the results are to this choice. <sep> 3. In Figure 1, it is not clear whether the figure on the right shows the actual loss, or the smooth loss using Equation (1) with instantaneous instance (A). If it is the former, then if the loss is so smooth, why do we need DIH? If it is the latter, then what does the instantaneous loss look like? This actually raises the question of how important the smoothing component is -- could we achieve the same results with an instantaneous loss (i.e. set gamma to 1 in Eq. 1)? <sep> ---- Minor issues ---- <sep> 1. How do you choose T0, gamma and gamma_k? <sep> 2. In the conclusions, the authors state that "" The reason [why  MCL and SPL are less stable] is that, compared to the methods that use DIH, both MCL and SPL deploy instantaneous instance hardness (i.e., current loss) as the score to select sample"". Since there are so many other differences in the way training progresses, I think we don't have enough evidence to attribute this to merely the ""instantaneousness"" of the loss. In fact, it would be interesting to see how SPL does if you use DIH as a metric (just smoothing the loss over time), but their approach of scheduling samples (easy to hard, and not the opposite and in DIHCL). <sep> 3. Appendix C shows some interesting results regarding wall time comparison. I was surprised to see that, despite the extra computations, DHCL is comparable to random mini-batches. This makes me wonder what the stop criteria was, because when you stop matters a lot for run time comparisons. It would also be interesting to see a more ample discussion on this in the main text. <sep> 4. In Figure 1, the axes are barely readable. <sep> 5. The authors oftentimes reverse the use of \\citet and \\citep, for example ""has been called the ""instance hardness"" Smith et al. (2014) corresponding to"" should have a bracket, whereas ""Our paper is also related to (Zhang et al., 2017)"" should not have brackets. <sep> 6. This is not an issue, but I just wanted to say I appreciated Appendix B. <sep> ---- Suggestions ---- <sep> 1. It would be interesting to make a connection between the DIH and what other papers have discovered about example forgetting (e.g. Toneva et. al, that was mentioned in the paper). <sep> 2. Major issues 3 -> a study on the effect of k and how to choose it. <sep> 3. While I understand that the models chosen in the experiments are expensive to train, it would be good to report standard deviations in Table 1. <sep> 4. Based on Table 1 and Figure 3, there is no concrete winner among the DIHCL methods. It would be good to include some recommendations in your conclusion on which one to choose and when. <sep> ---- Questions ---- <sep> 1. ""On average, the dynamics on the hard samples is more consistent with the learning rate schedule, which implies that doing well on these samples can only be achieved at a shared sharp local minima."" -> can you please explain why this is so? <sep> 2. See Major issues 3. <sep> 3. In Table 1, on some datasets, the authors apply lazier-than-lazy-greedy, and on some not.Why, and how does one decide this for a new dataset? <sep> 4. How did you choose T0, gamma and gamma_k, as well as the schedules in Appendix C (page 17)?","All three reviewers, even after the rebuttal, agreed that the paper did not meet with bar for acceptance. A common complaint was lack of clarity being a major problem. Unfortunately, the paper cannot be accepted in its current form. The authors are encouraged to improve the presentation of their approach and resubmit to a new venue."
"rating_summary | weakness | suggestion  ==> ===== Update after author response <sep> Thanks for the clarifications and edits in the paper. <sep> I recommend acceptance of the paper. <sep> Other comments: <sep> Definition 1 in the updated version is still too vague (""difference of what?"" -- function values? distance in norm between iterates?) -- this should be clarified. <sep> ======== <sep> This paper considers the problem of sparsity-constrained ERM and asks whether one can design a variant of the stochastic hard thresholding approaches where the hard-thresholding complexity does not depend on a (sparsity dependent) condition number, unlike all previous approaches (Table 1). It proposes a method which combines SVRG-type variance reduction, with block-coordinate updates, leaving the hard thresholding operation outside the inner loop, to accomplish this goal. It provides a convergence analysis which significantly improves the previous best rates (by having both the sparsity level shat which is significantly lower (kappa_shat vs. kappa_stilde^2) as well as a condition number independent hard thresholding complexity (Table 1). An asynchronous and sparse (in the features) variant is also proposed, with even better complexity. Some standard experiments on sparse linear regression and sparse logistic regression is presented showing an improvement in both number of iterations as well as CPU time. <sep> I think the clarity of the paper should be quite improved (see detailed comment), hence why I think the paper is borderline, but I am leaning towards an accept given the significant theoretical improvements over the past literature (and positive empirical results), even though the algorithmic suggestion is somewhat incremental. <sep> The proposed Algorithm 1 seems very close to the one of Chen & Gu (2016), the paper should be more clear about this. There seems to be mainly two changes: a) extending the support projection of the gradient to the union of the sampled block with the one of the support of the reference parameter wtilde (vs. just the sampled block in Chen & Gu (2016) and b) moving the hard-thresholding iteration outside of the SVRG-inner loop. These small tweaks to the algorithm yield a significant theoretical improvement, though. <sep> == Detailed comments == <sep> Clarity: the number of long abbreviations with only one letter change make it hard to follow the different algorithms; perhaps a better more differentiating naming scheme could be used. Moreover, I think more background on the sparse optimization setup should be provided in the introduction or at least in the preliminaries, as I do not think the wider *CONF* community is very familiar with it (in particular, no cited paper was at *CONF*). For example, define early the separation in optimization error and statistical error; and point out that F(w_t) might even be lower than F(w*) as the sparsity threshold s might be much higher than s*. This will make Table 1 more concrete and less abstract for people who not are not yet experts on this particular analysis framework. <sep> - Table 1: I would suggest to put the rate for S2BCD-HTP instead on the last row and mention instead that the rate for ASBCD is similar under conditions on the delay; as it is interesting to already have a better gradient complexity for S2BCD vs. SBCD. <sep> ** Questions:  ** <sep> 1) In Corollary 1, how is the gradient oracle complexity defined or computed? And more specifically, how does one compare fairly the cost of doing a gradient update in Algorithm 1 on the *bigger set* S = Gtilde U G_jt vs. just G_jt for the Chen & Gu ASBCD algorithm? Is this accounted in the computation? <sep> 2) In Figure 1, which ""minimum"" is referred to and how is it found? I suspect it is not F(w*) (as it could be higher than F(w_t)), i.e. it is *not* the minimum of (1) with s*. One natural guess is that it might be min_w F(w) s.t. ||w||_0 <= s, though I do not see any guarantee in the main paper that running the algorithm would make F(w_t) converge to such a value (i.e. all we know from Thm 1 is that F(w_t) might be within O(||nabla_Itilde F(w*)||^2) of F(w*) ultimately. Please explain and clarify! <sep> == Potential improvement == <sep> The current result in Theorem 1, which is building on a similar proof technique as the original SVRG paper, has the annoying property of requiring the knowledge of the condition number in setting the size of the inner loop iteration. I suspect that this is an artifact of using an outdated version of the SVRG algorithm. This has been solved since then by considering a ""loopless"" version of SVRG which implicitly defines the size of the inner loop in a random manner using a quantity *which does not depend on the condition number*. This was proposed first by Hofmann et al. [2015], and then re-used by Lei & Jordan [2016] and more recently by Kovalev et al. [2019] e.g. Note that Leblond et al. (2017) that you cited profusely also used this variant of SVRG. I suspect that this technique could be re-used in your case to obtain a similar result with a loopless variant (which also gives cleaner complexity results). (Though I only skimmed through your proof.) <sep> Caveat: the sensibility of the theory in the main paper seems reasonable, but I did not check the proofs in the appendix. <sep> = References: <sep> - Hofmann et al. [2015]: Variance Reduced Stochastic Gradient Descent with Neighbors, Thomas Hofmann, Aurelien Lucchi, Simon Lacoste-Julien and Brian McWilliams, NeurIPS 2015 <sep> - Lei & Jordan [2016]: Less than a Single Pass: Stochastically Controlled Stochastic Gradient Method, Lihua Lei and Michael I. Jordan, AISTATS 2016 <sep> - Kovalev et al. [2019]: Don't Jump Through Hoops and Remove Those Loops: SVRG and Katyusha are Better Without the Outer Loop, Dmitry Kovalev, Samuel Horvath and Peter Richtarik, arXiv 2019","All the reviewers reach a consensus to reject the current submission. <sep> In addition, there are two assumptions in the proof which seemed never included in Theorem conditions or verified in typical cases. <sep> 1) Between Eq (16) and (17), the authors assumed the 'extended restricted strong convexity' given by the un-numbered equation. <sep> 2) In Eq. (25), the authors assume the existence of \\sigma making the inequality true. <sep> However those assumptions are neither explicitly stated in theorem conditions, nor verified for typical cases in applications, e.g. even the square or logistic loss. The authors need to address these assumptions explicitly rather than using them from nowhere."
"rating_summary | suggestion  ==> Summary of the paper: The authors propose a latent variable model RaDOGAGA, a generative autoencoding model. The model is trained via a tradeoff between distortion (the reconstruction error) and the rate (the capacity of the latent space, measured by entropy). The paper provides an analysis of theoretical properties of their approach, and presents supporting experimental results. <sep> Review tl;dr: weak reject, for three main reasons: <sep> (i) While the existing literature around VAEs, beta-VAEs,  and Rate-Distortion theory is mentioned in the related work, the connections are not nearly discussed sufficiently. <sep> (ii) On top of (i), the derivation of their loss function and architecture is not sufficiently motivated. This is in astonishing contrast to 1.5 pages of main text and 8 pages of (much appreciated!) analysis of properties. <sep> (iii) Given the paper is clearly related to existing approaches in the literature, the experiments would require a much more careful comparison to existing models. It remains unclear why an interested user should favor your model over conceptually simpler generative models with fewer hyperparameters. <sep> Detailed review: <sep> Nota bene: This review is a late reassignment. While I reviewed the paper to the best of my ability, time constraints did not allow me to review parts of the paper in depth.  I am open to reassess my review during the second stage. <sep> Connection to prior art: As a probabilistic, neural autoencoding model, the connections to the family of VAE models are obvious. The loss function (eq. (4)) still looks very much like the ELBO, where the typical conditional log-likelihood was split into two distortion terms. How is this different from e.g. a beta-VAE? Particularly, what is the connection between the rate-distortion analysis of beta-VAE by Alemi et al. and yours? These things need to be discussed explicitly, with more than a sentence or two in the related work section. <sep> A lesser, but still important omission in your discussion of prior work: The Jacobian of the generator has also been studied, even for the VAE, cf. e.g. [1]. I believe this deserves more attention in your assessment of prior art. <sep> Motivation: You use two distortion terms: actual sample vs. undistorted reconstruction. Why is that? What is the interpretation of the multipliers? How do I choose them? Why is a large part of your architecture (the pipeline from x to \\hat(x)) actually deterministic? Why are you using the entropy of the prior over the latents, rather than the KL divergence between encoder and a prior? I think an interested reader could learn much more from your paper if you discussed your model embedded in th related work rather than in isolation. <sep> Theory: Due to aforementioned time constraints, I was not able to review the extensive theoretical analysis in depth. Still, I would strongly recommend structuring the respective sections more clearly. Separate model and architecture description from the theoretical analysis; precisely formulate your claims. In particular, state your assumptions clearly. For instance, you assume ""that each function's parameter is rich enough to fit ideally"" (and similar e.g. in Appendix A). Does this only mean that the true distributions are part of the parametric family? What if this is not the case? Do your parameters need to be in the optimum for your analysis to hold true? <sep> Given that the full 20-page manuscript spends 10 pages on theory, I think this contribution is not given appropriate space in the main text. <sep> Experiments: There are three experiments: a simple 3D proof of concept; anomaly detection; analysis of the latent state in CelebA. As mentioned in my review of the methods section, I believe the approach to be very similar to established models. None of the experiments provides convincing evidence why I should prefer the new, arguably more complex model. <sep> For instance, I would have much preferred that you investigate properties of your model against alternatives over the anomaly detection experiments, which did not further my understanding of the proposed model. <sep> Summary: The paper tackles an important problem, namely the lack of control over the latent embedding in autoencoding generative models. I believe the author's contribution can be valuable, and I particularly appreciate the effort to investigate theoretical properties. As is, the case is not sufficiently convincing to be accepted, but I encourage the authors to improve the paper. <sep> Minor comments: <sep> 1. While I appreciate a pun, I would recommend to rename the model along with the acronym to a more concise name. <sep> 2. Please revise your notation and typsetting. Examples: x1 instead of x_1, f of f(\\cdot) instead of f(), \\log instead of log. <sep> 3. Introduce acronyms before using them (e.g. VAE, MSE, SSIM), even when they seem obvious to you. <sep> 4. Please carefully check the manuscript for typos, missing articles, missing spaces etc. <sep> 5. Your citations are inconsistent, in that they sometimes use first names, sometimes first name initials, and sometimes no first names. <sep> 6. To my knowledge, the term scale function does not have an obvious definition. I think you are simply referring to monotonically increasing functions. Please clarify! <sep> 7. Your figures should be understandable without too much context, they need more detailed captions. <sep> [1] http://proceedings.mlr.press/v84/chen18e.html","Agreement by the reviewers: although the idea is good, the paper is very hard to read and not accurately enough formulated to merit publication. <sep> This can be repaired, and the authors should try again after a thorough revision and rewrite."
"abstract | weakness | rating_summary | decision  ==> The paper describes a method for word normalization of Amharic text using a word classification system followed by a character-based GRU attentive encoder-decoder model. <sep> The paper is very short and lacks many important details, such as where the data is collected from, how it is processed and split into training and evaluation sets, and how the initial token classification is performed. The paper also doesn't adhere to the conference paper template, which is grounds for desk rejection. <sep> The authors should revise the paper with this information and consider submitting to a different venue, as the task considered, while interesting, seems far from the core focus of *CONF*.","The paper proposes a text normalisation model for Amharic text. The model uses word classification, followed by a character-based GRU attentive encoder-decoder model. The paper is very short and does not present reproducible experiments. It also does not conform to the style guidelines of the conference. There has been no discussion of this paper beyond the initial reviews, all of which reject it with a score of 1. It is not ready to publish and the authors should consider a more NLP focussed venue for future research of this kind."
"abstract | rebuttal_process | weakness | rating_summary  ==> This paper proposes a sequence-to-sequence model for mapping word sequences to relation-argument-tuple sequences. The intermediate representation (output of the encoder) is a fixed-dimensional vector. Both encoder and decoder internally use a tensor product representation. The experimental results suggest that the tensor product representation is helpful for both the encoder and the decoder. The paper is interesting and the experimental results are positive, but in my opinion the exposition could use some substantial work. Fixing the most substantial flaws in the exposition would be sufficient to warrant an accept in my view. <sep> Major comments: <sep> I found the mix of levels of detail in the model specification in section 3 confusing. It would be extremely helpful to have a straightforward high-level mathematical description of the key parts of the encoder, mapping (which could be considered part of the encoder), and decoder in standard matrix-vector notation. While equations (7), (8), (9), (10), (11) and appendix A.2 go some way toward this, key high-level details seem to be missing, and I feel like the exposition would benefit from simply stating the matrix-vector operations that are performed in addition to describing their interpretation in terms of the semantics of the tensor product representation. Specific examples are noted below. <sep> It would be helpful to be explicit about the very highest-level structure of the proposed model. If I understand correctly, it is a probabilistic sequence-to-sequence model mapping a word sequence to a probability distribution over relation-argument-tuple sequences. It uses an encoder-decoder architecture with a fixed-dimensional intermediate representation, and an autoregressive decoder using attention. Both the encoder and decoder are based on the tensor product representation described in section 2. Stating these simple facts explicitly would be extremely helpful. <sep> Especially for the encoder, the learned representation is so general that there seems to be no guarantee that the learned roles and fillers are in any way related to the syntactical / semantic structure that motivates it in section 2. There doesn't seem to be any experimental investigation of the learned TPR in the encoder. If I understand correctly, the way encoder roles and fillers are computed and used is symmetric, meaning that the roles and fillers could be swapped while leaving the overall mapping from word sequences to relation-argument-tuple sequences unchanged. This suggests it is not possible to interpret the role and filler vectors in the encoder in an intuitive way. <sep> Minor comments: <sep> In section 2, ""R is invertible"" should strictly be ""R has a left inverse"". <sep> In section 3.1.1, the claim that ""we can hypothesize to approximately encode the grammatical role of the token and its lexical semantics"" is pretty tenuous, especially given the apparent symmetry between learned roles and fillers in the encoder and given the lack of experimental investigation of the meaning of the learned encoder roles and fillers. <sep> In section 3.1.2, my understanding is that the relation-argument tuple (R, A_1, A_2, A_3), say, is treated as a sequence of 3-tuples: (A_1, R, 1), (A_2, R, 2), (A_3, R, 3). Each of these 3-tuples is then embedded using learned embeddings (separate embeddings for argument, relation and position). If correct, it would be helpful to state this explicitly. <sep> In section 3.1.2, it is stated that contracting a rank-3 tensor with a vector is equivalent to matrix-vector product, which is not the case. <sep> In section 3.1.3, both high-level and low-level details of the MLP module are omitted. High-level, I presume that the matrix output by the encoder is reshaped to a large vector, the MLP is applied to this vector to produce another vector, then this is reshaped to a rank-3 tensor to input to the decoder. It would be helpful to state this. Low-level, the number of layers, depth and activation function of the MLP should be specified somewhere, at least in the appendix. <sep> Did the authors consider using a bidirectional LSTM for the encoder? This might improve performance. <sep> In section 3.1.2 and appendix A.2, why use the LSTM hidden state for subsequent processing rather than the LSTM output (which would be more conventional). The LSTM output is defined in appendix A.2 but appears not to be used for anything. Please clarify in the paper. <sep> Did the authors consider passing the output of the reasoning MLP into every step of the tuple LSTM instead of just using it to initialize the hidden state? <sep> It would be helpful to state the rank of the tensors H, B, etc in section 3.2.2. <sep> In section 3.2.2, what does ""are predicted by classifiers over the vectors..."" mean? This seems quite imprecise. What is the form of the classifier? My best guess is that the vector a_i^t is passed through a small MLP with a final softmax layer which outputs a probability distribution over the 1-of-K representation of the argument. The main text says ""more details are given in the appendix"", but appendix A.2 just has ""Classifier(a_1^t)"". Please clarify in the paper. <sep> What is the attention over in equation (9)? Attention needs at least two arguments, the query and the sequence being attended to. It seems that (9) only specifies one of these. It would also be helpful to be explicit about the form of attention used. <sep> What is f_linear in (11)? <sep> It seems unnecessarily confusing to switch notation for the arguments from A_1 in section 3.1.2 to a r g_1 in section 3.2.2, and similarly for the relations. <sep> For the decoder tuple LSTM, how exactly is the previous relation-argument tuple (R, A_1, A_2, A_3), say, summarized? Are each of R, A_1, A_2 mapped to a vector, these vectors concatenated, then passed into the LSTM? Or is the positional decomposition into (A_1, R, 1), ... used? Please clarify in the paper. <sep> Based on section 3.3, it seems that the model assumes that, in the decomposition of (R, A_1, A_2, A_3) into a sequence (A_1, R, 1), (A_2, R, 2), (A_3, R, 3) of 3-tuples at each decoder output step, the three 3-tuples are conditionally independent of each other and the three entries of each 3-tuple are conditionally independent of each other. Is this indeed assumed? If so, it would be helpful to state this explicitly. It seems like this is likely not true in practice. <sep> Section 3.3 refers to ""predicted tokens"". Where are these predicted tokens in (9), (10) or (11)? <sep> In section 3.3, it seems the loss at each decoder step is the log probability of the relation-argument tuple at that step. Thus, by the autoregressive property, the overall loss is the log probability of the sequence of relation-argument tuples. If so, it would be helpful to state both these facts explicitly. <sep> Section 3 seems to be missing a section, which is how decoding is performed at inference time. For the output of the decoder at each step, is random sampling used, if so with a temperature, or is greedy decoding (selecting the most likely class, equivalent to a temperature of 0) used? Also, what is done if decoding outputs different R's for (A_1, R, 1), (A_2, R, 2), (A_3, R, 3)? The three R values here should be equal in order for this to represent a relation-argument tuple (R, A_1, A_2, A_3), but there is no guarantee the model will respect this constraint. <sep> Unless I missed it (apologies if so), many experimental architecture details were omitted. For example, how many hidden cells were used for the LSTMs, etc, etc? These should at least be stated in the appendix. <sep> It would be interesting to investigate how long input / output sequences need to be before the fixed-dimensional internal representation breaks down. <sep> In section 4.1.1, it was not clear to me what ""noisy examples"" means. Does this mean that the dataset itself is flawed, meaning that the reference sequence of operations does not yield the reference answer? Please clarify in the paper. <sep> In table 1, please state the total size of the fixed-dimensional intermediate representation for all systems. This seems crucial to ensure the systems can be meaningfully compared. <sep> In figure 4, left figure, the semantic classes don't apper to be very convincingly clustered. (And it seems like K-means clustering could easily have selected a different clustering given a different random initialization.) <sep> In appendix A.2, mathematical symbols are essentially meaningless without describing what they mean in words. Please explain the meaning of all the symbols that are not defined in terms of other symbols, e.g. w^t, T_{t-1}, ..., f_s m (is this softmax???), f_l i n e a r (what does this mean?), C o n t e x t, C l a s s i f i e r, etc, etc. C o n t e x t in particular doesn't even have a hint of a definition. <sep> In (19) and (27), why would a temperature parameter be helpful? This can be absorbed as an overall multiplicative factor in the weight matrix of the previous linear layer. Is this temperature parameter learned during training (I presume so)? Please clarify in the paper. <sep> Usually * is used for convolution, not simple multiplication (e.g. equation (17)). <sep> Throughout the main body and appendix, there are lots of instances of poor spacing. For example, flinear should be written as something like flinear in latex to avoid it coming out as l i n e a r (which literally interpreted means l times i times n times e, etc). Please fix throughout.","The paper proposed a new seq2seq method to implement natural language to formal language translation. Fixed length Tensor Product Representations are used as the intermediate representation between encoder and decoder. Experiments are conducted on MathQA and AlgoList datasets and show the effectiveness of the methods. Intensive discussions happened between the authors and reviewers. Despite of the various concerns raised by the reviewers, a main problem pointed by both reviewer#3 and reviewer#4 is that there is a gap between the theory and the implementation in this paper. The other reviewer (#2) likes the paper but is less confident and tend to agree with the other two reviewers."
"abstract | weakness | misc | weakness  ==> # Summary <sep> - The paper proposes a UCB-inspired algorithm for a contextual LQR problem. The problem itself is introduced in this paper and is similar in spirit to CMDPs, with the difference that instead of learning a mapping from context to transition matrix, a mapping from context to matrices [A, B] figuring in the system dynamics of LQR is learned. <sep> - The proposed algorithm is an online-learning algorithm shown to have sublinear regret in the number of experienced environments. A toy experiment with a 2D moving mass is presented to illustrate the theory. <sep> # Decision <sep> Although the problem setting is interesting and it is encouraging to have a guarantee, several important unclear points in the paper and a missing comparison to a straightforward baseline stop me from recommending it for publication in its present form. I detail my concerns below. <sep> # Concerns <sep> 1) First, a conceptual question. I can see a straightforward algorithm that can learn the linear mapping \\theta from context to [A, B] as follows. <sep> - In episode k, obtain trajectory (x_{1:H}, u_{1:H-1}) <sep> - By least squares, find [A, B] from the obtained trajectory <sep> - Since context [C, D] is observed, find \\theta : [C, D] -> [A, B] again by least squares <sep> One can do this using data from K episodes if needed, one can sequentially update the controller for collecting data, etc. <sep> =>  A comparison to such a basic approach should be definitely included in the paper, in my opinion. <sep> 2) The authors might argue that the algorithm suggested above has no guarantee. I would be curious to hear in this regard a comment on the practical implementation suggested in the paper. Namely, after deriving the bounds etc., the authors make further approximations and modifications in the practical algorithm. From my point of view, these modifications defeat the purpose of the bounds, because then only empirical evaluation can confirm that these approximations have not destroyed the analysis. Alternatively, one needs to incorporate the introduced approximation errors in the analysis. In more detail, <sep> - Eq. (9) is not solved exactly but by random sampling. In the 2D toy task, it may be OK, but in higher-dimensional spaces, a significant error can be introduced which is not accounted for. <sep> - More importantly, the UCB bound \\beta in Eq. (11) is not used at all in the experiments. <sep> => To my understanding, it is the crucial point of UCB to use the UCB-bound. If it is not used, how should one judge the resulting algorithm? <sep> 3) This is a concern regarding clarity. I didn't get (i) if matrices [Q, R] are context-dependent or not and (ii) if the agent observes them or not. This is not clearly communicated in the text. <sep> => Clarify whether [Q, R] are context-dependent and observed. <sep> # AFTER REBUTTAL <sep> After authors' clarifications and improvements on the paper, I update my score to weak reject. The reason I am still against acceptance is the lack of stronger empirical evaluations. As R4 pointed out, some clarifications on the side of the algorithm are also required.","This work considers the popular LQR objective but with [A,B] unknown and dynamically changing. At each time a context [C,D] is observed and it is assumed there exist a linear map Theta from [C,D] to [A,B]. The particular problem statement is novel, but is heavily influenced by other MDP settings and the also follows very closely to previous works. The algorithm seems computationally intractable (a problem shared by previous work this work builds on) and so in experiments a gross approximation is used. <sep> Reviewers found the work very stylized and did not adequately review related work. For example, little attention is paid to switching linear systems and the recent LQR advances are relegated to a list of references with no discussion. The reviewers also questioned how the theory relates to the traditional setting of LQR regret, say, if [C,D] were identity at all times so that Theta = [A,B]. <sep> This paper received 3 reviews (a third was added late to the process) and my own opinion influenced the decision. While the problem statement is interesting, the work fails to put the paper in context with the existing work, and there are some questions of algorithm methods."
"abstract | weakness | decision  ==> Title: Good work, requires some edits. <sep> 1. Summarize: <sep> This paper proposes a new problem setting in visual relation detection which is called ""Predicate Zero-shot Learning (PZSL)"". They provide a clear motivation and description of this setting. They propose a solution to this problem which leverages linguistic priors and knowledge bases. Furthermore they propose an unbalanced sampled-softmax to tackle the long tail distribution of predicates. <sep> 2. Clearly state your decision. One or two key reasons for this choice. <sep> I will go for a weak accept for the paper at this stage. (+) I think the proposed problem setting is well-motivated and useful. Also, (+) the proposed initial solution to this problem is interesting. However, (-) they propose a ""fast graph convolution network"" which seems to be precisely equivalent to a PinSage.  Also, (-) the paper requires to be polished as it lacks clarity. <sep> 3. Main discussion <sep> My first argument is: I'm not sure why the authors have changed the name of PinSage and just mentioned that ""their"" ""Fast Graph Convolution Network"" is ""inspired"" from PinSage. To me it looks exactly the same. If there are any differences, it should be stated clearly. In fact, I would not be against using PinSage as a part of their approach. However, trying to rename it without clear reasons is not a good idea. <sep> My second argument is that the paper lacks clarity in writing (for detailed suggestions please refer to comments and feedbacks). Specially the evaluation section lacks details and clarity: a) In the beginning of this section (page 7), the authors talk about ""generalized"" and ""traditional"" settings without properly defining them. b) The descriptions for Table 1 and Table 2 fail to provide enough details to help understand the difference between the results in these two tables (one of them states ""Accuracy of unseen predicate recognition"" and the other one ""Accuracy of recognition of triplets with unseen predicates""). <sep> 4. Comments and feedback. <sep> Introduction: <sep> Paragraph one in the: <sep> 1. The relationship recognition methods are mainly supervised ""that"" → ""to"". <sep> 2. last line: …. and do not study ""on generalizing"" → ""the generalization of"". <sep> Paragraph two: <sep> 1. no manual annotations or ""real samples"" → ""image samples"". (a real sample is ill-defined) <sep> 2. For example, no instance of chew → For example ""given"" no instance of chew. <sep> Paragraph three: <sep> 1. … is difficult since predicates are often abstract not as specific →   is difficult since predicates are often abstract ""and"" not as specific. <sep> 2. Furthermore, unlike many object ZSL methods … → This line to the end is very complicated and hard to understand. <sep> Related Works: <sep> 1. Visual Relationships: I would cite ""Graph R-CNN for scene graph generation"" since it is the most relevant work regarding the similarity of pipeline (using GCNs). <sep> 2. External Knowledge bases  (KB): I would cite ""Improving Visual Relationship Detection using <sep> Semantic Modeling of Scene Descriptions"" since it is one of the most relevant works using knowledge graph modelings to improve visual relation detection. <sep> Problem Setup: <sep> Do you plan to provide the proposed dataset splits so others can work on this setting? I consider this very important given your paper's contribution. Maybe it is better if it is also mentioned in the paper. <sep> Pipeline: <sep> 1. Paragraph 2: … the output of which is fused with …: Given Figure 2, it does not seem like Vp is being created by fusing Vs and Vo. It looks more like it is extracted directly from the image (union of bounding boxes). <sep> 2. In Figure 2: In the representation of Pipeline (A), the graph is colored by dark blue for objects and light blue for predicates. The represented graphs show Object to Object and Predicate to Predicate connections which I'm not sure if it is correct. Shouldn't we always have a light blue between every pair of dark blue connections? <sep> Evaluation: <sep> 1. Please consider the mentioned points in the Main Discussions. <sep> 2. In Table 1, I suggest re-naming ""embedding"" to ""initial embedding"". <sep> 3. In Table 1, Hit@k should be Hits@k. <sep> 4. Please define the metrics clearly (Hits@k). <sep> Extra: I have a question regarding the ablation studies with GloVe, Normal and InferSent initialization. The question is whether this initialization is necessary? It seems like in the setting ""W/O KG"", even though the embeddings are initialized with GloVe, there is no gain (all of the Hits@k values are 0.0). So GloVe embedding without KG bring no external semantic knowledge? Then why use them? Regarding that, I can see that a GCN, initialized with normally distributed embeddings (row 14 in Table 1) has given 0.0 accuracies, but I find this very counter intuitive, as graph convolution layers already have trainable weights capable of compensating for the lack of 'proper' initialized embedding and getting 0.0 does not make sense to me. <sep> Conclusions and Future Work: <sep> 1. two lines before the last: please use ""\\citep"".","The paper proposes a new problem setting of predicate zero-shot learning for visual relation recognition for the setting when some of the predicates are missing, and a model that is able to address it. <sep> All reviewers agreed that the problem setting is interesting and important, but had reservations about the proposed model. In particular, the reviewers were concerned that it is too simple of a step from existing methods. One reviewer also pointed towards potential comparisons with other zero-shot methods. <sep> Following that discussion, I recommend rejection at this time but highly encourage the authors to take the feedback into account and resubmit to another venue."
"abstract | weakness | rebuttal_process  ==>  ==> The paper introduces a new approach to generate adversarial examples for deep classifiers. As opposed to the majority of work on adversarial attack models, which generally limit the attacker on pixel-space distortions measured with respect to an Lp norm, the authors here consider a slightly more general attack model that is a combination of an affine transformation and additive L2 perturbation of the input example. <sep> Finding optimal attacks for this model can be non-trivial (standard due to the highly nonlinear coupling between the affine parameters and the additive perturbation), so the authors instead propose training a surrogate neural network that generates the attack affine-transformation and distortion- parameters sequentially. This can, in principle, be done in a traditional supervised training setup; however, to force the adversarial images to look perceptually close to natural looking images, the authors throw a discriminator loss on top, and train the attack generator network adversarially. <sep> The paper is well-written in general, the idea is intuitive, and the experiments are well-described. However, I have a few concerns that lead to me to give a low score (at least in the first round of reviews). <sep> - Novelty. <sep> Leveraging spatial distortions (or other visually meaningful transformations) to generate adversarial attacks is not a new idea, but the authors seem to have been unaware of this very large body of work. See, for example: <sep> ** Engstrom et al, ""Exploring the landscape of spatial robustness"", ICML 2019 <sep> ** Poovendran et al, ""Semantic adversarial examples"", CVPR 2018 <sep> ** Ho et al, ""Catastrophic Child's Play"", CVPR 2019 <sep> ** Joshi et al, ""Semantic adversarial attacks"", ICCV 2019 <sep> among many others. <sep> Using GAN-like transformation models to generate attacks is also not a new idea. A few of the above papers use this approach, and the authors refer to a few other such papers as well. <sep> So as such, the conceptual novelty of the contribution seems to be low (beyond the specific choice of combining affine and L2 perturbations). <sep> - Experimental evaluation. <sep> The authors do a commendable job thoroughly laying out the experimental setup. However, a couple of red flags emerge in the experiments. First, why not look at L-infty perturbations (as opposed to L2)? Second, why not test on more challenging datasets (CIFAR, CelebA, etc) as opposed to simple black/white datasets such as MNIST/Fashion-MNIST? One would imagine that the smaller, simpler datasets are easier to optimize for, and therefore the ""amortized"" attack generator networks are not necessary here. <sep> - Weakness of theoretical part. <sep> I am not sure the theorem is saying anything strong or useful (since the underlying transformer neural network is assumed to possess infinite capacity). I would suggest just removing it.","The method proposed and explored here is to introduce small spatial distortions, with the goal of making them undetectable by humans but affecting the classification of the images. As reviewers point out, very similar methods have been tested before. The methods are also only tested on a few low-resolution datasets. <sep> The reviewers are unanimous in their judgement that the method is not novel enough, and the authors' rebuttals have not convinced the reviewers or me about the opposite."
"abstract | weakness | suggestion | weakness | decision  ==>  ==> ### Summary <sep> This paper described a model-based RL method which uses learned dynamics models to augment the replay buffer used when training a PPO agent. <sep> Specifically, the agent learns an ensemble of dynamics models, and then performs a PPO updates using a mixture of trajectories sampled from the true environment and trajectories sampled from teh learned dynamics models. <sep> The fictitious trajectories are sampled from different dynamics models in the ensemble, which helps prevent the policy from exploiting the errors of a single model. <sep> The proportion of real/fake trajectories are adapted using the ratio of the real vs. predicted rewards: if the average returns are similar, then the proportions will be approximately equal, but if they are very different then more trajectories from the real environment will be used. <sep> Overall, I think there are some interesting elements to this paper but it is currently not ready for publication. I recommend weak reject for three reasons: limited novelty, the experiments as they stand are not very convincing and the writing needs work. <sep> #### Novelty <sep> The two stated contributions are: <sep> 1. the use of principled uncertainty estimates in model-based RL (an anchored ensemble) <sep> 2. the routine which adjusts the proportion of real vs. simulated trajectories for learning the policy. <sep> In my opinion, 1. is not much of a contribution since a number of works have already made use of different types of model uncertainty in the context of model-based RL. Here they use a slightly different version (the anchored ensemble, which is similar to the randomized priors used in DQNs) but this is more of a choice of implementation than a contribution in itself (a number of methods for estimating uncertainty exist, such as regular ensembles, dropout masks, BNNs, etc). The manner in which they use the uncertainty is to perform simulations with several different models to avoid overfitting to one, and this was already proposed in the ME-TRPO paper. <sep> The idea of 2. is interesting and in general, adapting between model-based and model-free RL regimes during execution seems to have potential. However the current method seems currently under-explored and heuristic. The fact that it is sensitive to the \\alpha hyperparameter (Figure 2) is a bit troubling. Is there a principled way which does not require tuning this hyperparameter? Or reformulating things so that it is less sensitive? <sep> #### Experiments <sep> Although the experiments show some improvement, they are not that convincing. The improvements do not seem statistically significant for Swimmer and Walker, and the asymptotic performance is worse than PPO for Hopper. There is only half-cheetah where the algorithm shows a clear improvment. <sep> Since one of the claimed contributions is the use of the anchored ensemble, there should be an ablation experiment showing this gives an improvement over a standard ensemble, but this is not included. <sep> #### Writing: <sep> One issue with the paper is that it spends *much* too long discussing related work and preliminaries. <sep> The first 5 pages are devoted to this and the proposed method is only introduced on page 6! <sep> Some detailed comments: <sep> - Paragraph 1 in the intro should be drastically cut. The sentence ""Recently...challenging tasks"" could be followed by references and then a sentence discussing the sample inefficiency and then moving on to the next paragraph. Most readers will be familiar with DQN, actor critic etc. <sep> - Same for paragraphs 2 and 3. A short discussion of the general idea behind model-based RL and its improved sample efficiency, the issue of the policy exploiting model errors, and some references are sufficient. <sep> - Similarly, section 2.1 is mostly unnecessary. It isn't necessary to detail the updates for standard algorithms such as PPO/TRPO unless they are useful for the proposed algorithm. <sep> - Section 2.2 is also too long. They main idea is very simple and is summarized in Equation 13.","The authors propose a hybrid model-free/model-based policy gradient method that attempts to reduce sample complexity without degrading asymptotic performance. They evaluate their approach on a collection of benchmark tests. <sep> The reviewers raised concerns about limited novelty of the proposed approach and flaws in the evaluation. The authors need to compare to more baselines and ensure that the baseline algorithms are performing as previously reported. Even then, the reported improvements were small. <sep> Given the issues raised by the reviewers, this paper is not ready for publication at *CONF*."
"abstract | weakness | strength | decision | rating_summary  ==> The paper presents a multi-lingual multi-way pseudo-parallel text corpus automatically extracted from Wikipedia. <sep> The authors use a variety of pre-existing techniques applied at large scale with substantial engineering effort to extract a large number of sentence pairs in 1620 language pairs from 85 languages. <sep> In the proposed method 1) raw sentences are extracted from a Wikipedia dump, 2) LASER sentence embeddings and language IDs are computed for each sentence, 3) for each language pair candidate sentence pairs are extracted using a FAISS approximate K-nearest neighbor index on the cosine distance between sentence embeddings, 4) sentence similarity scores are computed between the candidate pairs using the ""max margin"" criterion of Artetxe & Schwenk, 2018 and finally 5) sentence pairs are selected according to a language-pair-agnostic threshold on the similarity scores. <sep> The extraction method is symmetric w.r.t. language directions for each language pair. <sep> Structural metadata of Wikipedia, such as cross-lingual document alignments, is deliberately not exploited (some discussion is provided but I would have preferred an empirical comparison of local vs global extraction). <sep> The similarity threshold is determined by evaluating training corpora extracted at different thresholds on a machine translation task on De->En, De->Fr, Cs->De and Cs->Fr translation directions, evaluated on WMT newstest2014, and manually selecting the threshold based on BLEU scores. The paper also reports that combining the automatically extracted corpora with Europarl results in strong BLEU improvements over training only on Europarl. BLEU scores on TED test sets obtained using only the automatically extracted corpus are also reported. The corpus has been released. <sep> Overall the methodology presented in the paper is strong and the corpus is likely going to become a valuable tool to build machine translation systems and other multi-lingual applications. However, I am concerned that *CONF* 2020 may not be the appropriate venue for this paper, as in my understanding dataset release papers are not explicitly solicited in the Call for Papers https://*CONF*.cc/Conferences/2020/CallForPapers . The corpus generation method is based on existing techniques, and to the extent that the engineering effort is innovative, it might not necessarily transfer well to data sources other than Wikipedia, thus limiting its broad scientific value. Therefore I suggest to submit the paper to a different venue.","The authors present an approach to large scale bitext extraction from Wikipedia. This builds heavily on previous work, with the novelty being somewhat minor efficient approximate K-nearest neighbor search and language agnostic parameters such as cutoffs. These techniques have not been validated on other data sets and it is unclear how well they generalise. The major contribution of the paper is the corpus created, consisting of 85 languages, 1620 language pairs and 135M parallel sentences, of which most do not include English. This corpus is very valuable and already in use in the field, but IMO *CONF* is not the right venue for this kind of publication. There were four reviews, all broadly in agreement, and some discussion with the authors."
"abstract | weakness | rebuttal_process  ==> In the article, the authors solve the problem of anomaly detection using a fully unsupervised approach. They try to deal with the main challenge of anomaly detection: a lack of certainty on what defines normal data and anomaly one. For this purpose, the authors iteratively use: 1) autoencoders to learn the representation of the data; 2) applying in the latent space clustering to get a new training set and retrain autoencoders. The experimental results show that the author's method performed better results than such a baseline model as one-class SVM and one-class NN. <sep> The proposed algorithm looks robust and well-motivated, but the text of the article and the experiments can be improved. As the proposed approach is a heuristic, the experiments should be done more persuasively, including more metrics used and more alternative algorithms considered. <sep> The key comments are the following: <sep> 1. The formatting of the article needs to be improved e.g.: <sep> 2. there is no comma between rows in the equation (1) ; <sep> <<to be accepted into the ""training"" set, .>> - there is an extra comma; <sep> 3. round brackets in the equation (6) should be bigger; <sep> 4. Table 3 is bigger than the page sizes. <sep> 5. The quality of the pictures should be improved: <sep> - Increase the captions font size in Figure 2; <sep> - The captions and the legends in Figure 3 are practically not visible; <sep> 6.  Is the DAGMM method SOTA in anomaly detection with deep autoencoder? There are many other methods with similar ideas. We expect that we should provide a comparison with other methods: <sep> https://arxiv.org/pdf/1809.02728.pdf - IGMM-GAN <sep> https://papers.nips.cc/paper/7915-generative-probabilistic-novelty-detection-with-adversarial-autoencoders.pdf - GPND AE <sep> 6. Also, DAGMM works badly according to the experiments in the article with max AUROC in Table 1 only 50.3 (so it seems that it is no better than the coin-flipping) <sep> 7. Why was the only selected digit for analysis 4? Usual for comparison anomaly detection on MNIST dataset apply the following procedure: for each figure in dataset consider corresponded class as anomaly data, and the rest of the digits are used as normal data, e.g.: <sep> https://arxiv.org/pdf/1802.06222.pdf https://arxiv.org/pdf/1906.11632.pdf <sep> 8.  Class imbalances can affect the value of the AUROC metric. Possibly, the other metrics like AUPRC, F1-scores will better reflect the work of the algorithms for comparison. Also, AUROC is not representative when it comes to the selection of the threshold for anomaly detection. Precision and Recall can help to get more insights. <sep> 9. In Table 3, the result of applying the proposed algorithm presented with standard deviation, but other methods are represented by one metric value. Why? The explanation is required.","The authors propose an approach for anomaly detection in the setting where the training data includes both normal and anomalous data. Their approach is a fairly straightforward extension of existing ideas, in which they iterate between clustering the data into normal vs. anomalous and learning an autoencoder representation of normal data that is then used to score normality of new data. The results are promising, but the experiments are fairly limited. The authors argue that their experimental settings follow those of prior work, but I think that for such an incremental contribution, more empirical work should be done, regardless of the limitations of particular prior work."
"abstract | weakness | decision  ==>  ==> Summary :  <sep> The paper proposes an exploratory objective that can maximize state coverage in RL. They show that a formal objective for maximizing state coverage is equivalent to maximizing the entropy of a goal distribution. The core idea is to propose a method to maximize entropy of a goal distribution, or a state distribution since goals are full states. They show that the proposed method to maximize the state or goal distribution can lead to diverse exploration behaviour sufficient for solving complex image based manipulation tasks. <sep> Comments and Questions : <sep> - The core idea is to maximize the entropy of the state visitation frequency H(s). It is not clear from the paper whether the authors talk about the normalized discounted weighting of states (a distribution) or the stationary distribution? The entropy of the state visitation distribution only deals with valid states - but I am not sure what it means to maximize the entropy of this term exactly in terms of exploration, since it is neither the discounted weighting of states or the stationary distribution for an infinite horizon task? <sep> - The authors do mention that maximizing the entropy of H(s) is not sufficient - so instead suggests for maxmizing entropy of H(s|g). But why is this even sufficient for exploration - if I do not consider new tasks at test time but only the training task? How is this a sufficient exploration objective? Furthermore, since it is the conditional entropy given goal states, the fundamental idea of this is not clear from the paper. <sep> - Overall, I am not convinced that an objective based on H(s|g) is equivalent to an maximizing H(s), and why is this even a good objective for exploration? The meaning of H(s) to me is a bit vague from the text (due to reasons above) and therefore H(s|g) does not convince to be a good exploration objective either? <sep> - The paper then talks about the MI(S;G) to be maximized for exploration - what does this MI formally mean? I understand the breakdown from equation 1, but why is this a sufficient exploration objective? There are multiple ideas introduced at the same time - the MI(s;g) and talking about test time and training time exploration - but the idea itself is not convincing for a sufficient exploration objective. In light of this, I am not sure whether the core idea of the paper is convincing enough to me. <sep> - I think the paper needs more theoretical insights and details to show why this form of objective based on the MI(s;g) is good enough for exploration. Theoretically, there are a lot of details missing from the paper, and the paper simply proposes the idea of MI(s;g) and talks about formal or computationally tractable ways of computing this term. While the proposed solutuon to compute MI(s;g) seems reasonable, I don't think there is enough contribution or details as to why is maximizing H(s) good for exploration in the first place. <sep> - Experimentally, few tasks are proposed comparing skew-fit with other baselines like HER and AutoGoal GAN - but the differences in all the results seem negligible (example : Figure 5). <sep> - I am not sure why the discussion of goal conditioned policies is introduced rightaway. To me, a more convincing approach would have been to first discuss why H(s) and the entropy of this is good for exploration (discounted weighting or stationary state distribution and considering episodic and  infinite horizon tasks). If H(s) is indeed a difficult or not sufficient term to maximize the entropy for, then it might make sense to introduce goal conditioned policies? Following then, it might be convincing to discuss why goal conditioned policies are indeed required, and then tractable ways of computing MI(s;g). <sep> - Experimentally, I think the paper needs significantly more work - especially considering hard exploration tasks (it might be simple setups too like mazes to begin with), and then to propose a set of new experimental results, without jumping directly to image based tasks as discussed here and then comparing to all the goal conditioned policy baselines. <sep> Overall, I would recommend to reject this paper, as I am not convinced by the proposed solution, and there are lot of theoretical details missing from the paper. It skips a lot of theoretical insights required to propose a new exploration based objective, and the paper proposes a very specific solution for a set a very specific set of experimental setups.","This paper tackles the problem of exploration in RL. In order to maximize coverage of the state space, the authors introduce an approach where the agent attempts to reach some self-set goals. The empirically show that agents using this method uniformly visit all valid states under certain conditions. They also show that these agents are able to learn behaviours without providing a manually-defined reward function. <sep> The drawback of this work is the combined lack of theoretical justification and limited (marginal) algorithmic novelty given other existing goal-directed techniques. Although they highlight the performance of the proposed approach, the current experiments do not convey a good enough understanding of why this approach works where other existing goal-directed techniques do not, which would be expected from a purely empirical paper. This dampers the contribution, hence I recommend to reject this paper."
"abstract | strength | misc | weakness | rebuttal_process | strength | decision  ==> The paper proposes a spectral non-local block, which is a generalized method of the non-local block and non-local stage in the literature. The proposed spectral non-local block can be plugged into a neural network to improve its effectiveness. The paper also provides theoretical analyses of the stability of the proposed method, and also extend the method by including more  Chebyshev polynomial terms. Experiments are conducted on image classification and action recognition tasks, and they valid the effectiveness of the proposed method. <sep> The idea is well-motivated, and it is a generalization of existing works in the literature. I do like this idea. However, I am afraid that the idea is not well explained and supported, thus I gave a weak reject to encourage the authors to further improve the paper. <sep> The major concern I have is the reasonability of the experiments.  The experiments in the paper show relative performance gain with respect to a baseline method. It seems that there is a lack of comparison with state-of-the-art methods in the literature. For example, in Table 8, a performance gain is observed when compared with I3D. However, the recent STOA models can achieve much higher accuracy than the baseline. and also the proposed method. Since the proposed method is generic to all neural nets, it makes more sense to compare with SOTA and make improvements based on SOTA.  What is the conclusion from Table 4? Are you trying to demonstrate that the best configuration is DP3, and increasing the number of consecutive non-local blocks (from SP3 to SP5) doesn't work? It is awkward since the paper gives a stable hypothesis for deeper nonlocal structure, but experimentally the deeper structure doesn't work well. Figure 4 is abrupt without much background descriptions. Are the images randomly chosen? Ours here means SNL or gSNL? Is the colored superimposition the attention map (I believe so but the paper doesn't indicate so) and how to interpret it? What is the relation of the coverage of the critical parts on birds and the long-range dependency? More background descriptions and interpretations of the results are needed. <sep> Another concern I have is the clarity of the writing. There are quite a number of informal use of English, mismatched descriptions, undefined acronyms, etc. For example, in the caption of Fig. 1, it is said self-attention and self-preserving are taken effect by W1 and W2, which is contradictory to what is illustrated in the figure. Also, the terms self-attention and self-preserving, and other terms such as CGNL, A2, Hadama (Hadamard?) product, are not formally defined or described.  A lot of grammar errors and informal use of English are present, such as ""which lead to"", ""the weight means"", ""when using in the neural network"", ""fig. 4"", ""Figure. 2"", ""more liberty for the parameter learning."", etc.","This paper proposes a new formulation of the non-local block and interpret it from the graph view. The idea is interesting and the experimental results seems to be promising. <sep> Reviewer has two major concerns. The first is the presentation, which is not clear enough. The second is the experimental design and analysis. The authors add more video dataset in the revision, but still lack comprehensive experimental analysis for video-based applications. <sep> Overall, the idea of non-local block from graph view is interesting. However, the presentation of the paper needs further polish and thus does not meet the standard of *CONF*"
"rating_summary | weakness | decision  ==> This paper shows that there is a one-to-one correspondence between pixel-shift based data augmentation and average pooling operations in CNN-NNGP/NTK based ridge regression. Interestingly, the authors show that standard average pooling + flatten can lead to a better performance than simple global average pooling. This paper further shows that using the data pre-processing step proposed in (Coates et al., 2011) can boost performance of CNN-NNGP/NTK based ridge regression by ~7% which allowed the authors to achieve classification accuracy in high 80s which is AFAIK SOTA on CIFAR-10 when not using learned representations. <sep> My current assessment of the paper is ""weak accept"". There are two main reasons why I am on the verge of recommending rejection of this paper: (1) I believe that the experiment evaluation is not done entirely correctly leading to inflation of the reported results (my guesstimate is by ~0.5-2%)---please see my ""Major comments"". If this is not fixed, I am very likely to downgrade my score. (2) While the observation of the relationship between pixel-shifts and average pooling is very nice (which is why my current score is ""weak accept""), it seems that most of the improvement comes from application of the pre-processing step of Coates et al. (2011) (seems like a ~7% improvement!). Given the large computational cost of CNN-NNGP/NTK (authors say about 1000 GPU hours), I wonder whether a simpler algorithm like some of the newer variants of boosting combined with the Coates et al. algorithm wouldn't also perform at around 87-88% like CNN-NNGP/NTK (given the baseline 85-86% accuracy of the Coates et al. (2011) algorithm reported by the authors). <sep> Major comments: <sep> - Can you please clarify why you decided to give a new name (Box Blur) to standard average pooling? Why not just use the existing name? <sep> - I believe that the way you report results in all the tables (i.e., tables 1-6) and the text based upon them is flawed. The right approach would be to select the hyperparameters ""c"" and ""d"" on a validation set, and then report the performance with these hyperparameters on the test set. While the experiments are somewhat rescued by the fact that you report results for (almost) all the possible hyperparameter settings (which allows us to see samples from the population distribution of the generalisation error), type-setting the best results in boldface and thus implying that these are valid estimates of the generalisation error is not appropriate since you are effectively selecting the best hyper-parameters on the test set! Unfortunately, I cannot accept these results to be published ""as-is"". While re-running the experiments with hyperparameter selection on validation set is already a somewhat imperfect solution, I am not sure I can see a better way forward. However, I do understand that this could be prohibitively expensive in which case I would like to ask you to suggest an alternative solution please (of course, other reviewers are welcome to chime in as well)?! <sep> - While most of the paper is about Local Average Pooling (LAP) and the equivalence between averaging and pixel shifts, the experimental results seem to show that most of the improvement comes from the use of Coates et al.'s preprocessing step. Could you please run the experiments in tables 3 and 4 with c=0 and c=32 to see what the effect of the preprocessing is without LAP? <sep> - In sect.6.3, you say ""Our experiment illustrates that even with a fixed last FC layer, using GAP could improve the performance of CNN, and challenges the conjecture that GAP reduces the number of parameters in the last fully-connected layer and thus avoids overfitting."" I am not sure I see why fixing the last FC layer should provide more convincing evidence than training it? I do not know the conjecture to which you refer but from your description, the overfitting without GAP should occur because the FC layer has more parameters than with GAP?! If this is true, then the overfitting would happen in the last layer (due to the large number of parameters) which you have (at least partially) prevented by not training it?! Can you clarify and also report the results of this experiment with all the layers trained please? <sep> - In Appendix D, you say that you have used lambda = 10^{-5} for all configurations. How have you selected this particular value please? Do you have a sense of how far from optimal this value is for all the different configurations (or at least for NTK vs NNGP models---in my experience, the optimal setting between the two can differ quite a bit)? <sep> Minor comments: <sep> - In the abstract and throughout the paper, you claim that the cost of kernel regression is quadratic. AFAIK without any approximations, the cost is cubic (or O(n^{2.67}) to be more precise). Please clarify. <sep> - In par.1 on p.1, you say ""convolutional neural networks (CNNs) whose width (number of channels in convolutional layers) **goes to infinity**"" (emphasis mine) and cite the Jacot et al. (2018) paper. AFAIK this paper only works with infinite networks but does not actually prove that **deep** networks of finite width (in each layer) converge to the NTK limit; IMHO you should cite the Allen-Zhu et al. (2018) and Du et al. (2018) papers from your references for that result. Based on p.2 (end of par.2 in sect.2), you seem to be aware of this distinction but cite Arora et al. (2019) instead of these two; I would suggest either citing Allen-Zhu et al. and Du et al. only, or citing all three as the Arora et al. paper came out later than the first versions of the other two paper which AFAIK already contained all the necessary derivations (even if the words ""Neural Tangent Kernel"" were not spelled out there). <sep> - Also in par.1 on p.1, you say that Arora et al. (2019) was the first to provide an algorithm to compute the CNTK kernel which is a bit of a stretch given that both Garriga-Alonso et al. (2019) and Novak et al. (2019) have implemented the CNTK kernel in their experiments. AFAIK the claim in (Arora et al., 2019) is that they provided first **efficient** implementation of the CNTK-GAP kernel which should be made clearer in the next revision of your paper. <sep> - On p.2, you say ""These kernels correspond to neural networks where only the last layer is trained."" In reality, the correspondence is not exact for finite networks because the induced kernel will not be exactly equal to the one at the limit. <sep> - Bottom of p.2, ""Global Average Pooling (GAP) is proposed"" -> ""... was proposed"". <sep> - Top of p.3, ""..., and GAP is more robust"" -> ""..., and that GAP is more robust"". <sep> - On p.3 in the ""Padding Schemes"" paragraph, do you mean to assume that the input image has only a single channel (not necessary later)? <sep> - On p.4, I am slightly confused by your definition of the ""augmented kernel"". Specifically, it does not seem K^G (x , x') = K^G (x', x) holds in general. Can you please clarify? If there's no symmetry, I do not think it necessary to use a different name, but perhaps a clarifying note would be beneficial to the reader?! <sep> - On p.5, fig.1 is too small when printed and one needs to use the computer screen to see what is depicted; given the amount of white space around, can you please try to make the images larger (you can perhaps also only include 2 or 4 images instead of 16 which will give you additional space)? <sep> - On p.5, you say that for small ""c"", circular padding will not create unrealistic images. Looking at fig.1b, it seems like the images are not as unrealistic as in fig.1a but human eye can still tell they are not realistic (potentially even more so with other images than the one selected for this figure). I am not sure whether there is a reason to assume this issue does not affect CNNs too?! Further, I am not convinced the motivation is correct in the first place given that the optimal ""c"" for CIFAR-10 is 12 which will presumably create clearly unrealistic images; perhaps it would be best to omit this motivation?! <sep> - On p.6, you claim ""Another advantage of LAP is that it **does not incur any extra computational cost**"" (emphasis mine) while at the next line you say that there is a constant additional computational cost. Perhaps say that the extra computational cost is relatively small? <sep> - It might be nice to swap tables 3 and 4 so at least the results for NNGP are next to each other. Even better would be the current table 3 was closer to table 1 to achieve the same effect for NTK. <sep> - I am not sure I fully understand the description in sect.6.3: isn't the number of channels on the input irrelevant after computation of the kernel in the first layer? In other words, why have you opted to use only 2,048 patches in your experiments and not 32,000 or 256,000 as used by Recht et al. (2019)? Do you have an estimate of how different could the performance of NNGP/NTK be with the larger number of features? Do you know what is the performance of Coates et al.'s algorithm with only 2,048 features? Relatedly, do you know how AlexNet would perform if its PCA data augmentation was replaced by the Coates et al.'s feature extractor?","This paper was assessed by three reviewers who scored it as 6/3/6. <sep> The reviewers liked some aspects of this paper e.g., a good performance, but they also criticized some aspects of work such as inventing new names for existing pooling operators, observation that large parts of improvements come from the pre-processing step rather than the proposed method, suspected overfitting. Taking into account all positives and negatives, AC feels that while the proposed idea has some positives, it also falls short of the quality required by *CONF*2020, thus it cannot be accepted at this time. AC strongly encourages authors to go through all comments (especially these negative ones), address them and resubmit an improved version to another venue."
"abstract | weakness  ==>  ==> After Responses: <sep> I understand the differences that authors pointed to the relevant literature. However, it is still lacking comparisons to these relevant methods. The proposed method has not been compared with any of the existing literature. Hence, we do not have any idea how does it stand against the existing approaches. Hence, I believe the empirical study is still significantly lacking. I will stick to my decision. Main reason is as follows; I believe the idea is interesting but it needs a significant empirical work to be published. I recommend authors to improve empirical study and re-submit. <sep> ------- <sep> The submission is proposing a method for multi-objective RL such that the preference of tasks learned on the fly with the policy learning. The main idea is converting the multi-objective problem into single objective by scalar weighting. The weights are learned in a structured learning fashion by enforcing them to approximate the Pareto dominance relations. <sep> The submission is interesting; however, its novelty is not even clear since authors did not discuss majority of the existing related work. <sep> Authors can consult the AAMAS 2018 tutorial ""Multi-Objective Planning and Reinforcement Learning"" by Whiteson&Roijers for relevant papers. It is also important to note that there are other methods which learn weighting. Optimistic linear support is one of such methods. Hence, this is not the first of such approaches. Beyond RL, it is also studied extensively in supervised learning. For example, authors can see ""Multi-Task Learning as Multi-Objective Optimization"" from NeurIPS 2018. <sep> The manuscript is also very hard to parse and understand. For example, Definition 2 uses but not define ""p"" in condition (2). Similarly, Lemma 1 states sth is ""far greater"" than something else. However, ""far greater"" is not really defined. I am also puzzled to understand the relevance of Theorem 1. It is beyond the scope of the manuscript, and also not really new. <sep> Authors suggest a method to solve multi-objective optimization. However, there is no correctness proof. We do not know would the algorithm result in Pareto optimal solution even asymptotically. Arbitrary weights do not result in Pareto optimality. <sep> Proposing a new toy problem is well-received. However, not providing any experiment beyond the proposed problem is problematic. Authors motivate their method using DOOM example. Why not provide experimental results on a challenging problem like DOOM? <sep> In summary, I definitely appreciate the idea. However, it needs better literature search. Authors should position their paper properly with respect to existing literature. The theory should be revised and extended with convergence to Pareto optimality. Finally, more extensive experiments on existing problems comparing with existing baselines is needed.","The paper considers planning through the lenses both of a single and multiple objectives. The paper then discusses the pareto frontiers of this optimization. While this is an interesting direction, the reviewers feel a more careful comparison to related work is needed."
"abstract | weakness | decision  ==>  ==> This paper studied the problem of universal adversarial attack which is an input-agnostic perturbation. The authors proposed to use the top singular vector of input-dependent adversarial attack directions to perform universal adversarial attacks. The authors evaluated the error rates and fooling rates for three attacks on standard benchmark datasets. <sep> - The paper is generally well-written and easy to follow. My main concern towards this paper is about the experiments part from several aspects. First, the proposed method needs quite large L2 norm (50 on ImageNet) to work, while common adversarial attack experiments on ImageNet are usually conducted with L2 perturbation strength of 5 or less. I totally understand that performing universal attack would be much more difficult, yet having such loose L2 norm constraint still seems impractical. Second, the authors did not compare with any other baselines such as  (Moosavi-Dezfooli et al. 2017a) arguing that their universal attack is different for different perturbation strength and pixels are normalized. I do not think normalized pixel will be a problem as you can simply scale the perturbation strength accordingly. And because (Moosavi-Dezfooli et al. 2017a) uses different attack vectors for different perturbation strength, some comparison between these two types of universal attacks should be presented in order to mark the difference and demonstrate your advantages. I would suggest the authors to compare with several mentioned baselines in the paper to show the superiority of the proposed method. <sep> - Theorem 1 seems interesting, yet it needs a special assumption. The authors argue that this is a reasonable assumption in a small neighborhood of x. I wonder if the authors could conduct some demonstrative experiments to verify this? Because the definition of S_x depends on the attack function, does it mean that the assumption need to be held for any attack function? Also regarding the choice of \\delta, it seems that \\delta is different for different x? If so, since u is also depend on \\delta, this attack vector seems not universal? <sep> Detailed comments: <sep> - In proof of Theorem 1, all S should be G? <sep> - In proof of Theorem 2, how to get \\|v - \\hat v\\|_2 \\leq \\epsilon/(\\gamma - \\epsilon)? Directly applying the Theorems seems to get \\epsilon / (\\gamma) only? <sep> Depending on whether the authors can address my concerns, I may change the final rating. <sep> ====================== <sep> after the rebuttal <sep> I thank the authors for their response but I still feel that the assumption is not well-justified and there is still a lot to improve in terms of experiments. Therefore I decided to keep my score unchanged.","The paper proposes to get universal adversarial examples using few test samples. The approach is very close to the Khrulkov & Oseledets, and the abstract for some reason claims that it was proposed independently, which looks like a very strange claim. Overall, all reviewers recommend rejection, and I agree with them."
"abstract | weakness | decision  ==> Summary: <sep> Many prior works have found that the features output by the final layer of neural networks can often be used as informative representations for many tasks despite being trained for one in particular. These feature representations, however, are learned transformations of low-level input representations, e.g. RGB values of an image. In this paper, they aim to learn useful feature representations without meaningful low-level input representations, e.g. just an instance ID. Instead, meaningful representations are learned through gathered triplet comparisons of these IDs, e.g. is instance A more similar to instance B or instance C? Similar existing techniques fall in the realm of learning ordinal embeddings, but this technique demonstrates speed-ups that allow it to scale to large real world datasets. <sep> The two primary contributions of the paper are given as: <sep> - a showcase of the power of neural networks as a tool to approximately solve NP-hard optimization problems with discrete inputs <sep> - a scalable approach for the ordinal embedding problem <sep> After experimentation on synthetic data, they compare the effectiveness of their proposed method Ordinal Embedding Neural Network (OENN) against the baseline techniques of Local Ordinal Embedding (LOE) and t-distributed Stochastic Triplet Embedding (TSTE). The test error given by the systems is comparable, but there are clear speed benefits to the proposed method OENN as the other techniques could not be run for a dataset size of 20k, 50k, or 100k. <sep> Then, they gathered real-world data using MTurk applied to a subset of ImageNet and applied OENN to learning embeddings of different image instances using only the MTurk triplet information rather than the input RGB input features. <sep> Decision: Weak Reject <sep> 1. Interesting technique to take advantage of neural networks to efficiently learn ordinal embeddings from a set of relationships without a low-level feature representation, but I believe the experiments could be improved. One of the main advantages of this approach is efficiency, which allows it to be used on large real-world datasets. The MTurk experiment gives a qualitative picture, but it could be improved with comparisons to pairwise distances learned through alternative means using the RGB image itself (given that images would permit such a comparison). By this I mean, that you may be able to use relationships learned using conventional triplet methods which use input RGB features as ground truth, and test your learned relationships against those. However, since quantitative exploration of large real-world datasets may be challenging and expensive to collect, the synthetic experiments could have been more detailed. The message of synthetic experiments would be stronger if more of them were available and if the comparison between LOE, TSTE, and OENN was made on more of them. <sep> 2. I think that the claim that the use of neural networks with discrete inputs can approximately solve NP-hard optimization problems is an exciting one, which likely necessitates more experiments (or theoretical results), but as it stands I don't think it is a fundamentally different conclusion from the fact that this method provides a great scalable solution for the ordinal embedding problem. This claim can be made secondarily or as motivation for continued exploration along this direction, but I think listing them as two distinct contributions is necessary. <sep> Additional feedback: <sep> Since quantitative real-world results are challenging to obtain, improved presentation of the qualitative results would be helpful as well. You may be able to show more plots which help display the quality of the embedding space varying with the number of triplets used. For example, an additional plot after Figure 5 (b) which shows a few scatter plots of points (color coded by class) for training with different numbers of collected triplets. Also, since it should be fairly easy to distinguish between cars and animals or cars and food, it may be more interesting to focus on the heat-maps from along the block diagonal of Figure 5 (a) and talk about what relationships may have been uncovered within the animal or food subsets. <sep> Very minor details: <sep> In Figure 5, a legend indicating the relationship between color intensity and distance would be helpful. <sep> In Figure 6 there seem to be unnecessary discrepancies between the y-axis and colorbar of subplots (a) and (b), and keeping those more consistent would improve readability.","The authors demonstrate how neural networks can be used to learn vectorial representations of a set of items given only triplet comparisons among those items. The reviewers had some concerns regarding the scale of the experiments and strength of the conclusions: empirically, it seemed like there should be more truly large-scale experiments considering that this is a selling point; there should have been more analysis and/or discussion of why/how the neural networks help; and the claim that deep networks are approximately solving an NP-hard problem seemed unimportant as they are routinely used for this purpose in ML problems. With a combination of improved experiments and revised discussion/analysis, I believe a revised version of this paper could make a good submission to a future conference."
"abstract | rating_summary | weakness | decision  ==>  ==> Summary: This paper hypothesizes that even though we are able to achieve very impressive performance on benchmark datasets as of now (e.g. image net), it might be due to the fact that benchmarks themselves have biases. They introduce an algorithm that selects more representative data points from the dataset that allow to get a better estimate of the performance in the wild. The algorithm ends up selecting more difficult/confusing instances. <sep> This paper is easy to read and follow (apart from some hickup with a copy of three paragraphs), but in my opinion of limited use/impact. <sep> Comments: <sep> 1) There is a repetition of the "" while this expression formalizes.."" paragraph and the next paragraph and the paragraph ""As opposed to .."" is out of place. Please fix <sep> 2) I am not sure <sep> - What applications the authors suggest. They seem to say that benchmark authors should run their algorithm and make benchmarks harder. To me it seems that benchmarks become harder because you remove most important instances from the training data (so Table 4 is not surprising - you remove the most representative instances so the model can't learn) <sep> - how practically feasible it is.  Even if in previous point I am wrong, the algo requires retraining the models on subsets (m iterations). How large is this m? <sep> 3) Other potential considerations: <sep> -  When you change the training size, the model potentially needs to be re-tuned (regularization etc) (although it might be not that severe since the size of the training data is preserved at t) <sep> - How do u chose the values of hyperparams (t, m,k eta), how is performance of your algorithm depends on it <sep> 4) I don't see any good baselines to compare with - what if i just chose instances that get the highest prediction score on a model and remove these. How would that do? For NLP (SNLI) task i think this would be a more reasonable baseline than just randomly dropping the instances, <sep> 5) I wonder if you actually retrain the features after creating filtered dataset, new representation would be able to recover the performance. <sep> I read authors rebuttal and new experiments that show that the models trained on filtered data generalize better are proving the point, thanks. Changing to weak accept","This paper proposes to address the issue of biases and artifacts in benchmark datasets through the use of adversarial filtering. That is, removing training and test examples that a baseline model or ensemble gets wright. <sep> The paper is borderline, and could have flipped to an accept if the target acceptance rate for the conference were a bit higher. All three reviewers ultimately voted weakly in favor of it, especially after the addition of the new out-of-domain generalization results. However, reviewers found it confusing in places, and R2 wasn't fully convinced that this should be applied in the settings the authors suggest. This paper raises some interesting and controversial points, but after some private discussion, there wasn't a clear consensus that publishing it as is would do more good than harm."
"abstract | weakness  ==> The paper describes a ""layer"" that aims at producing embeddings for discrete objects by using fewer parameters than classical embeddings layers. Indeed, the model proposes, instead of learning an embedding matrix of size VxN, to learn a matrix of embeddings of anchors (AxN) and a transformation matrix (VxA) such that the embedding of any object can be found by multiplying A with T. On top of that, they propose different regularization techniques to improve the quality of the learned embeddings, and particularly a proximal gradient method over a L1 normalization on T to reduce the number of parameters. They propose also different ways to initialize A and also a method for incorporating a priori information (e.g knowledge) into the model.  They evaluate this model on different tasks: text classification and language modeling and show that they can achieve good performance while using fewer parameters than Sota methods. <sep> First of all, the paper is well written, and the description is very detailed and understandable. It was a pleasure to read such a paper! <sep> One point which is unclear is the interest of using such a method, and more precisely in which cases, this method can be useful. Indeed, the overall number of parameters of ANT is AxN + VxA (N being the size of the embeddings, A the number of anchors and V the size of the vocabulary) while classical methods are VxN parameters. Said otherwise, we need to have V<N to really have less parameters to train in the model -- knowing that classical embeddings spaces size is usually between 256 and 1024, it means that we have to target a task where the number of anchors is quite low. I agree that the sparsity term on T is here to encourage to decrease the number of parameters but first, the same sparsity could be applied on the original VxN embedding matrix, and also, even if, at the end, the T matrix is sparse, during learning one has to maintain a large matrix in memory.  I would like the authors to discuss more on this point which is crucial? Particularly, I am not sure to understand what the #Emb value is in the table (AxN + AxV or just AxN), and how to compare the models. (There is a discussion in Section 3, but the argumentation does not explain why having so many parameters at train time is not a problem).  Also, since this is the crucial point in the paper, I would be interested in having a discussion about the use of neural models compression techniques after learning that could also ""do the job"" (even if they are not trained end-to-end). <sep> One other remark concerns the different ""components"" added into the model (e.g sparsity, orthogonality, Relu...). It is difficult to measure the interest of each of them, and I would recommend the authors to provide an ablation study to make the effect of the different choices more understandable by the reader. <sep> The notion of anchors also is misleading since it gives the impression that the A matrix will store embeddings for particular objects, while there is no constraint of that type. Each line of the A matrix is an embedding, but this embedding is not associated with one of the objects seen at train time (no direct mapping from anchors to words in the vocabulary). This has to be made more clear at the beginning of the paper. <sep> Concerning the initialization of A by K-means, it assumes that the space of objects has a particular metric. The authors say that this metric can come from a pretrained embedding space, but in that case, the problem in the number of parameters (which is the main justification of this work) is invalid (i.e if you already have an embedding matrix, then just let us fine-tune it). Could you clarify ? <sep> The fact that the method would allow incorporating knowledge is certainly the most interesting point. The way it is done has to be better explained (I do not understand why positive pairs are taken into account by not enforcing sparsity on T at this particular point, the way negative pairs are handled seem more natural) <sep> The paper is interesting and proposes a new simple model that could be used to keep good performance while reducing the number of parameters of the final model. Discussions have to be added to discuss the relevance of the approach since it still needs a large number of parameters at train time, and the role of each component could be studied more in depth.","The paper proposes a method to produce embeddings of discrete objects, jointly learning a small set of anchor embeddings and a sparse transformation from anchor objects to all the others. While the paper is well written, and proposes an interesting solution, the contribution seems rather incremental (as noted by several reviewers), considering the existing literature in the area. Also, after discussions the usefulness of the method remains a bit unclear - it seems some engineering (related to sparse operations) is still required to validate the viability of the approach."
"abstract | misc | strength | weakness | decision  ==> This paper proposed a white-box (known network architecture, known network weight) data free (without need to access the data) adversarial attacking method. The main idea is to find a perturbation that maximizes the activations at different layers jointly. But the optimization is done sequentially, treating each layer's activation (before ReLU) as a linear transformation output. <sep> The method is compared with existing methods (only one existing approach for the problem, GDUAP by Mopuri et al. 2018) in terms of the fool rate. It shows significant improvement. Ablation study is carried out to compare with baselines like perturbation maximizing only first layer activation, only last layer activation, etc. Also on some other settings (black-box testing, less data) the proposed method outperforms GDUAP. <sep> The problem of data-free white-box attack is very interesting and does make sense. The proposed method achieve significant improvement over the previous one (GDUAP). I do have the following concerns though. <sep> 1), the novelty of the proposed idea seems relatively limited. The proposed idea seeks perturbation maximizing activations over all layers. It incur perturbation before ReLU. But overall, the flavor of the idea is not significantly different from GDUAP, despite the significant performance boost. <sep> 2), it was mentioned that compare with GDUAP, this paper has more theoretical analysis. But this is not very convincing to me. There are many steps of approximation/relaxation from the original problem (Equation (1)) to the final formula (Equation (10)). Many assumptions are made over the steps. It is OK to use these steps to derive a heuristic. But these steps can hardly be called ""theoretical analysis"". <sep> I am particularly uncomfortable with Equation (5), which is the basis of the main idea. It assumes that all data in W1X are in the same orthant as W1p. But this is unrealistic as different data in X will for sure incur different activation patterns. Did I misunderstand anything? <sep> 3) I do like the experimental results. It looks impressive. But the baselines are really limited (granted, there are not many existing approaches). There is only one task (image classification). How about other tasks like segmentation etc shown in Mopuri et al. 2018? Also it would be nice to also show the results of other UAP methods, as it gives us a better sense of the gap between with and without data. <sep> 4) I wonder how will the attack affect some model which has been trained with some defense mechanism, e.g., adversarial training. <sep> Typo: <sep> Equation (5), RHS missing a max","This paper focuses on finding universal adversarial perturbations, that is, a single noise pattern that can be applied to any input to fool the network in many cases. Further more, it focuses on the data-free setting, where such a perturbation is found without having access to data (images) from the distribution that train- and test data comes from. <sep> The reviewers were very conflicted about this paper. Among others, the strong experimental results and the clarity of writing and analysis were praised. However, there was also criticism of the amount of novelty compared to GDUAP, on the strong assumptions needed (potentially limiting the applicability), and on some weakness in the theoretical analysis. <sep> In the end, the paper seems in current form not convincing enough for me to recommend acceptance for *CONF*."
"abstract | weakness | decision  ==> Summary <sep> ======== <sep> This paper proposes a defense against adversarial examples that detects perturbed inputs using kernel density estimation. The paper uses a combination of known (and often known to be broken) techniques, and does not provide a fully convincing evaluation. <sep> I lean towards rejection of this paper. <sep> Detailed comments <sep> ================= <sep> The idea of increasing robustness by maximizing inter-class margins and minimizing intra-class variance is fairly natural, but the author's discussion of their approach (mainly in sections 1 and 2) is very hand-wavy and relies on a lot of general intuitions and unproven claims about neural networks. <sep> For example, in the introduction, the authors claim: <sep> ""A trained deep classification model tends to organize instances into clusters in the embedding space, according to class labels. Classes with clusters in close proximity to one another, provide excellent opportunities for attackers to fool the model. This geometry explains the tendency of untargeted attacks to alter the label of a given image to a class adjacent in the embedding space as demonstrated in Figure 1a."" <sep> First, a t-SNE representation is just a 2D projection of high-dimensional data that is useful for visualization purposes, and one should be careful when extrapolating insights about the actual data from it. For example, distances in the 2D projection do not necessarily correspond directly to distances in the embedding space. <sep> The claim that untargeted attacks lead to a ""nearby"" cluster are hard to verify given just Figure 1. First, the colors of the labels between 1a and 1b do not seem to match (e.g., Dog is bright green in 1b but this color does not appear in 1a). If the other colors match, then this would seem to suggest that trucks (purple) often get altered to ships (orange). Yet, the two clusters are quite far apart in 1a. It seems hard to say something qualitative here. An actual experiment comparing distances in the embedding space and the tendency of untargeted attacks to move from one class to another would be helpful. <sep> The color scheme in Figure 1b is also unclear. A color bar would help here at the very least. <sep> These observations are then used to justify increasing cluster distance while minimizing cluster variance, but it would be nice to see a more formal argument relating these concepts to the embedding distance. <sep> The technique proposed in Section 3.2. to reduce variance loss estimates each class' variance on each batch. Would  this still work for a dataset with a large number of classes (e.g., ImageNet)? For such a dataset, each class will be present less than once in expectation in each batch, which seems problematic. <sep> The plots in Figure 2 don't give much of a sense of how the combination of the different proposed techniques is better than any individual technique. The evaluation compares PDM to RCE, but from Figure 2 one could guess that variance reduction alone (2c) performs very similarly to PDM (2e). An ablation study showing the contribution of each of the individual techniques would be helpful. <sep> The evaluation section could be improved significantly. FGSM, JSMA, and to some extent BIM, are not recommended attacks for evaluating robustness. The gray-box and black-box threat model evaluations are also not the most interesting here. Instead, and following the recommendations of Carlini et al. (2019), the evaluation should: <sep> - Propose an adaptive attack objective, tailored for the proposed defense in a white-box setting. The authors do this to some extent, by re-using the attack objective from Carlini & Wagner 2017, which targets KDE. It would still be good to provide additional explanations about how the hyperparameters for this attack were set. <sep> - Optimize this objective using both gradient-based and gradient-free attacks <sep> - As the proposed defense is attack-agnostic, I also suggest trying it out on rotation-translation attacks, as the worst-case attack can always be found by brute-force search <sep> Other <sep> ===== <sep> - The citations for adversarial training in the 2nd paragraph of the intro are unusual. Standard references here are for sure the first two below, and maybe some of the other three as is relevant to your work <sep> - Szegedy et al. 2013: ""intriguing properties of neural networks"" <sep> - Goodfellow et al. 2014: ""Explaining and harnessing adversarial examples"" <sep> - Kurakin et al. 2016: ""Adversarial Machine Learning at Scale"" <sep> - Madry et al. 2017: ""Towards deep learning models resistant to adversarial attacks"" <sep> - Tramer et al. 2017: ""Ensemble Adversarial Training"" <sep> - The Taylor approximation in (1) does not seem to be well defined. The Jacobian of F is a matrix, so it isn't clear what evaluating that matrix at a point x means. <sep> - The ""greater yet similar"" symbol (e.g., in equation (4)) should be defined formally.","A defense against of adversarial attacks is presented, which builds mostly on combining known methods in a novel way. While the novelty is somewhat limited, this would be fine if the results were unequivocally good and other parts of the problematic. However, reviewers were not entirely convinced by the results, and had a number of minor complaints with various parts of the paper. <sep> In sum, this paper is not currently at a stage where it can be accepted."
"abstract | weakness | rebuttal_process | suggestion  ==> 1. Summary <sep> The authors employ a multi-agent learning approach for learning how to set payoffs optimally for crowdsourcing contests and auctions. Optimality means e.g. incentive alignment (the principal problem) between the principal (e.g. the organizer) and participants (e.g. bidders), assuming e.g. that participants can be strategic about their behavior. In this work the principal uses ReLU-log utility. <sep> First, the authors use fictitious play and multi-agent RL to train agents on a distribution of payoffs. Then, a neural net is fitted to the samples (payyoffs, expected principal utility), and finally iteratively attempts to improve the payoffs using mirror ascent within the convex set of admissable payoffs. <sep> The authors compare the payoffs with theoretically known solutions and in situations where the optimal solution is not known. <sep> 3, 4-agent all-pay auction (Nash eq known). <sep> Same as above, but with noise added to bids (Nash eq not known). <sep> The authors analyze in some detail how the principal's utility and bidder ranking behave as the participants' bids change. <sep> 1. Decision (accept or reject) with one or two key reasons for this choice. <sep> Reject. Although the high-level approach is interesting (use learning to design auctions for cases where no theoretical solution is known), the actual experimental results and methodological improvement over e.g. Dutting 2017 are weak. The authors only consider 3, 4-agent auctions. There are no other learned baselines (e.g., constrained optimization without neural nets) that the authors could consider. <sep> 3. Supporting arguments <sep> See above. <sep> 4. Additional feedback with the aim to improve the paper. Make it clear that these points are here to help, and not necessarily part of your decision assessment. <sep> M-EMA and M-EMD: ascent and descent? in Algo 1, 2? <sep> --- <sep> I've read the rebuttal, but still lean towards reject. The scope/analysis of the experiments (e.g. auction type), still seems limited, even though both agents and mechanism are adaptive.","This paper demonstrates a framework for optimizing designs in auction/contest problems. The approach relies on considering a multi-agent learning process and then simulating it. <sep> To a large degree there is agreement among reviewers that this approach is sensible and sound, however lacks substantial novelty. The authors provided a rebuttal which clarified the aspects that they consider novel, however the reviewers remained mostly unconvinced. Furthermore, it would help if the improvement over past approaches is demonstrated in a more convincing way, for example with increased scope experiments that also involve richer analysis."
"abstract | weakness | decision | misc  ==> Summary: The paper raises an alarm that state-of-the art change-point detection methods in the ML literature do not handle important practical aspects arising in time-series modeling, namely seasonality. Indeed, methods designed to detect changing distribution under an i.i.d. setting can fail dramatically when the assumption is violated, when the change happens in the seasonal component.  The paper proposes to use an auto-encoder to find the ""main pattern"" within each seasonal window, and to use total variation penalty (l1-norm on the change) of the hidden state in the auto-encoder to encourage a smooth state-sequence which allow breaks.  They use k-means clustering to partition data-points, and detect a change-point if two consequent hidden states don't end up in the same cluster. <sep> While the proposal is sensible and the paper is reasonably readable, I find the paper lacking in several respects, and recommend to reject it. My main concerns are <sep> (a) novelty: despite the claims in the paper -- the importance of seasonality is well known and appreciated in time-series literature, and the proposal to look for changes in seasonality is fairly obvious when dealing with practical time-series. I would suggest to do a comprehensive literature search and re-evaluate the novelty of the paper. <sep> I believe that recent ML papers e.g. kernel two-sample tests and such, focus on the i.i.d. setting and ignore seasonality (and other messy aspects of practical TS) -- as it is the more challenging statistical problem. <sep> (b) The paper considers a setting where the time-series consists of a seasonal component and an i.i.d. component (combined additively or multiplicatively). It doesn't attempt to model any kind of stochastic dynamics -- e.g. at least a simple auto-regressive model instead of iid, and non-stationarity (trends) in the time-series. So despite aiming to look at practical time-series, the paper still considers a simplified model. <sep> (c) The paper's presentation is often sloppy in language use, assumptions, mathematical details, and simulations and needs to be significantly improved to be considered for *CONF* (or related ML conferences). <sep> Detailed comments: <sep> a) The references are severely lacking. There is an extensive literature in modeling time-series with seasonality and classical methods such as SARIMA (seasonal ARIMA), or exponential smoothing can track the evolution and changes in seasonal components. Various nonlinear DL-approaches to TS with seasonality have also started to appear. Once time-series is decomposed into trend, seasonal and stochastic part (using any linear or nonlinear or deep model), it is straightforward to apply anomaly detection algorithms to each component separately. Please take a look at e.g. https://anomaly.io/blog/index.html (from salesforce.com), to see practical change-point or anomaly detection in time-series in practice which does pay attention to seasonality. Also papers by Rob Hyndman pay close attention to seasonality, see e.g. https://otexts.com/fpp2/. <sep> ""Changepoint Detection in Periodic and Autocorrelated Time Series"", https://journals.ametsoc.org/doi/full/10.1175/JCLI4291.1 <sep> https://cran.r-project.org/web/packages/trend/vignettes/trend.pdf  (which has a section on seasonal change-point detection) <sep> Harvey, Koopman, Penzer, ""Messy Time Series: A Unified approach"", Adv. in Econometrics, Vol. 13, pp. 103-143., https://www.stat.berkeley.edu/~brill/Stat248/messyts.pdf <sep> Perhaps there's relatively less focus on these practical details of change-point detection in recent ML literature and the focus is on the stochastic component, as it is the most challenging for prediction. The use of l1-norm of differences in time-series to detect changes is a natural idea, and has been suggested many papers e.g.in: http://eeweb.poly.edu/iselesni/lecture_notes/TV_filtering.pdf, <sep> ""Time Series Clustering using the Total Variation Distance"", <sep> Stephen Boyd's trend filtering, https://web.stanford.edu/~boyd/papers/pdf/l1_trend_filter.pdf . <sep> While I am not aware of a specific prior work on auto-encoder with temporal smoothness for CPD, most of the main ideas are well known, and in my view the contribution is very limited in novelty. <sep> b) You're ignoring any memory or dynamics in the stochastic component of the time-series -- e.g. allowing something like a simple AR model rather than iid would be a good step. Detecting changes in the dynamics or correlation structure (temporal or cross-sectional) would make the paper more interesting.  Something closer to switching linear dynamical systems, see for example https://arxiv.org/abs/1610.08466. <sep> c) The presentation has many issues in language / math / simulations and needs to be improved: <sep> 1. The setting is not described clearly / formally -- are you trying to detect change-points online or offline, what assumptions are you making on the segments after removing seasonality -- are these just iid / stationary, can they include trends, outliers, e.t.c. <sep> 2. Baseline methods for detecting seasonal patterns are naive -- clearly applying methods that are not aware of seasonality will fail when there is strong seasonal components. There is one basic attempt at removing the seasonal component by averaging, and applying iid kernel CPD methods -- where it does help.  I believe doing something a bit more realistic (like doing a seasonal decomposition) will make the baselines much stronger. <sep> 3. Citation format is inconsistent with *CONF*. <sep> 4. ATR-CSPD is undefined in the abstract. <sep> 5. Intro:  i, j, k notation inconsistent -- you seem to use i both for i = j*p + k, and also to refer to window id. <sep> 6. What is a ""generative function"" of time-series? Do you mean the pdf / cdf? What do you mean by a product of generative functions (which is additive or multiplicative), do you mean adding / taking products of random variables coming from independent distributions? What do you mean that you do not differentiate between additive / multiplicative? Do you claim to handle both within the same model? <sep> 7. Definition 2 -- do you look for x_jo,k ~ Gk',  or x_j for j> j0 ~ Gk'? <sep> 8. You claim a multi-variate extension is easy -- but is it? How would you tackle e.g. changes in correlation structure? <sep> 9. ""Autoencoders attempt to copy input to output"" - isn't this trivial by using an identity function? You should mention some compression / bottleneck as well. <sep> 10. How do you optimize the total-variation (l1-norm) penalty in your formulation? Just throw it into SGD in keras? <sep> 11.  The discussion in 3.2. is confusing -- you talk about weekly series, but use daily-seasonality, however you then describe detecting weekdays vs. weekends? How can you associate separate weekends without a weakly seasonal model? <sep> 12. London electricity data-set -- why do you average all weeks within the time-series to find average customer week? This was very surprising. Don't you loose most of the interesting anomaly data this way? <sep> 13. Figures are not explained well. While there's nice use of color -- it's often hard to understand what is the description pointing at. <sep> typos: Person -> Pearson,  Autencoder -> Autoencoder, and many others.","The paper proposes ATR-CSPD, which learns a low-dimensional representation of seasonal pattern, for detecting changes with clustering-based approaches. <sep> While ATR-CSPD is simple and intuitive, it lacks novel contribution in methodology. It is unclear how it is different from existing approaches. The evaluation and the writing could be improved significantly. <sep> In short, the paper is not ready for publication. We hope the reviews can help improve the paper for a strong submission in the future."
"abstract | rebuttal_process | misc  ==> The authors propose a new gradient-based method (FAB) for constructing adversarial perturbations for deep neural networks. At a high level, the method repeatedly estimates the decision boundary based on the linearization of the classifier at a given point and projects to the closest ""misclassified"" example based on that estimation (similar to DeepFool). The authors build on this idea, proposing several improvements and evaluate their attack empirically against a variety of models. <sep> I found the proposed method quite interesting and intuitive. All the improvements made to the core method are well-motivated and clearly explained, while the ablation experiments are relatively thorough. <sep> However, I did find the presentation of experimental evidence quite misleading. <sep> Specifically, reporting mean accuracy over models, datasets, and epsilon constraints in Table 2 does not give the full picture. Going through the appendix tables, we can see the following: <sep> -- The step size used for PGD is quite large---eps/4 for the L2 case---which is quite uncommon when using 150 iterations. Based on prior work and my own personal experience, a step size of 2 * eps / #steps (i.e., eps / 75) would seem more suitable. I wonder if this is the reason for PGD performing worse than FAB for large epsilon values on CIFAR10. The authors mention that they chose this parameter using grid search but do not provide concrete details. <sep> -- The adversarially trained MNIST model of Madry et al. 2018 learns to use thresholding filters as the first layer (observed in the original paper). This causes issues for most gradient-based methods (e.g., PGD performs worse than the decision-based attack of Brendel et al. 2018, also observed in other prior work). While it is encouraging that FAB is robust to such gradient obfuscation, this is arguably not the ideal setting to compare gradient based methods (especially when averaging performance over models). <sep> -- For MNIST and Restricted IN, PGD performs comparably or even better than FAB (modulo larger epsilon values for which the large step size used could be an issue for PGD and the Linf-trained model with the thresholding filters). <sep> -- For the L1-norm setting, EAD performs similarly or better compared to FAB (again modulo the Linf-trained model). <sep> Based on these observations, I am not fully convinced that FAB outperforms PGD (for L2 and Linf) and EAD (for L1) by as much as Table 2 suggests. <sep> Moreover, the runtime comparison performed in not exactly fair: <sep> -- It is not clear how many restarts where included in the runtime of PGD. Its runtime should be in the same ballpark as FAB but the time reported is ~20x higher. <sep> -- PGD is known to produce quite accurate estimates when run with much fewer (say 15) steps. Thus in order to make a fair comparison one would also need to look at the entire #steps vs robust accuracy curve to get a better picture of the efficiency of these two methods. Choosing an arbitrary number of steps for each method is not very enlightening. <sep> -- It is not necessary to run PGD 5 times to evaluate the robust accuracy at 5 thresholds. One can perform binary search for each input in order to find the smallest epsilon for which a misclassification can be found. This will result in at most 3 (sometimes 2) evaluations per point (instead of 5). <sep> Despite these shortcomings of the experimental evaluation, I still believe that the paper has merit. After all, the method is clean and well-motivated,  performs comparably to the best of PGD and EAD in a variety of settings, and is robust to a certain degree of gradient masking. In that sense, it could potentially be a valuable contribution and could be of interest to a subset of the adversarial ML community. <sep> In the sense, while my initial stance is to recommend (weak) rejection, I would be open to increasing my score and recommending (weak) acceptance should my concerns be addressed. <sep> UPDATE: I appreciate the response and the additional experiments performed by the authors. The authors have addressed my concerns in their response. I am increasing my score to a weak accept. <sep> One thing that would be nice to add in the next version of the manuscript is a note inviting the reader to consider the appendix tables since average robust accuracy can be inconclusive.","This work presents a method for generating an (approximately) minimal adversarial perturbation for neural networks. During the discussion period, the AC raised additional concerns that were not originally addressed by the reviewers. The method is an iterative first order method for solving constrained optimization problems, however when considered as a new first order optimization method the contribution seems minimal. Most of the additions are rather straightforward---e.g. using a line search at each step to determine the optimal step size---and the reported gains over PGD are unconvincing. PGD can be considered as a ""universal"" first order optimizer [1], as such we should be careful that the reported gains are substantial and not just a question of tuning. Given that using a line search at each step increases the computational cost by a multiplicative factor, the comparison with PGD should take this into account. <sep> The AC notes several plots in the Appendix show PGD having better performance (particularly on restricted Imagenet), and for others there remain questions on how PGD is tuned (for example the CIFAR-10 plots in Figure 5). One of two things explains the discrepancies in Figure 5: either PGD is finding a worse local optimum than FAB, or PGD has not converged to a local optimum. There needs to be provided experiments to rule out the second possibility, as this is evidence that PGD is not being tuned properly. Some standard things to check are the step size and number of steps. Additionally, enforcing a constant step size after projection is an easy way to improve the performance of PGD. For example, if the gradient of the loss is approximately equal to the normal vector of the constraint, then proj(x_i+ lambda * g) ~ x_i will result in an effective step size that is too low to make progress. <sep> Finally, it is unclear what practical use there is for a method that finds an approximately minimum norm perturbation. There are no provable guarantees so this cannot be used for certification. Additionally, in order to properly assess the security and reliability of ML systems, it is necessary to consider larger visual distortions, occlusions, and corruptions (such as the ones in [2]) as these will actually be encountered in practice. <sep> 1. https://arxiv.org/pdf/1706.06083.pdf <sep> 2. https://arxiv.org/abs/1807.01697"
"misc | decision | weakness | decision  ==>  ==> This paper attempts to learn a preconditioner for optimization, specifically for the Dual space preconditioned descent (DPGD). <sep> - The techniques used to learn the preconditioner are heuristic, not scalable and without justification or ablation studies. <sep> - It does not compare against ""standard"" optimization techniques that construct data-driven preconditioners such as Adam or Adagrad or even to more Newton, natural gradient methods that use the Hessian or the Fisher information matrix as preconditioners. It shows ad-hoc synthetic experiments in dimensions 1 and 50. This is clearly not enough. <sep> Detailed review below: <sep> - Section 2: Please explain why Legendre functions are useful in ML. For assumption 1, 2; it needs to be explained why these hold for a given f*. What constraints do you need on f? What functions satisfy these? Please explain this explicitly. <sep> - Section 3: What is the number of points x_i needed in high dimensions to learn? Is it even possible to scale up this method to high dimensions? <sep> - Constructing \\mu requires computing the determinant of the Jacobian. What is the computational complexity? Moreover, it seems that we need access to the \\nabla f(x) for all x in D(f)? <sep> - Please state all the assumptions in the beginning rather than introducing one at a time in the propositions. <sep> - Remark 1: It is unclear that the cost of an inverse Hessian matrix is more than the procedure proposed in this paper. <sep> - Section 3.5: Please explain what is the advantage of this learned optimizer compared to other methods? Note that there is literature on non-smooth optimization and methods like sub-gradient descent can be used in this case. <sep> - What is the justification for the selection of the loss function and log-rescaling? <sep> - The result of Lemma 1 is standard. Please acknowledge this. <sep> -  Section 4: ""The step-size is set to 1"". It seems that the optimizer has been overfit and engineered to work on this specific problem. Either these decisions need to be justified, there needs to be an ablation study or there needs to be a larger set of experiments.","Thanks for the detailed replies to the reviewers, which significantly helped us understand your paper better. <sep> However, after all, we decided not to accept your paper due to weak justification and limited experimental validation. Writing should also be improved significantly. We hope that the feedback from the reviewers help you improve your paper for potential future submission."
"abstract | weakness | suggestion | weakness | suggestion | weakness | suggestion | misc | decision  ==> Summary: <sep> This paper proposes a relation-based ZSL model which can effectively alleviate the domain bias problem. To this end, first, the paper claims that a good relation-based ZSL model should consider two requirements -- modality invariance and class separability. And the paper designed Modality-invariant and Class-separable Multimodal VAE (MCMVAE) based on VAEs to meet the two aforementioned requirements. Next, the paper hypothesizes that the domain bias problem is due to the overlap between seen and unseen classes in the shared space, and explicitly introduced a discriminator to separate the two domains. The paper performs experiments on ZSL benchmark datasets and shows that the proposed method outperforms other relation-based methods. Besides, the domain discriminator which can be applied to other models demonstrates its effectiveness in reducing domain bias given the experimental results. <sep> +Strengths: <sep> 1. Clear writing logic. The author clearly depicts how to get the final loss of the method step-to-step and the relationship with existing methods. <sep> 2. The version without the domain discriminator (i.e. MCMVAE) is similar to PSE and CADA-VAE as the author acknowledges. However, the domain discriminator has certain novelty and can be applied to other methods. The overlap among seen and unseen classes is an important problem (domain bias problem named by the author) and the add of the domain discriminator to distinguish whether a sample is from seen classes or unseen classes is reasonable, which can provide better class separability (among seen and unseen classes). <sep> -Weaknesses: <sep> 1. Although the author claims that the proposed method is a relation-based method, it is strange that the proposed method is called xxVAE but in Table 2 it doesn't fall into synthesis-based methods (as CVAE-ZSL and CADA-VAE do). Although it is derived from VAE, the current method doesn't seem to be called a VAE any more (some of the regularizations of the VAE are relaxed). Also, are the two terms -- relation-based and synthesis-based -- first proposed by the author? Is there a clear boundary between those two groups of methods? <sep> 2. It is recommended that an additional figure that depicts the framework is added (similar to Figure 2 in CADA-VAE) to promote better understanding. Currently, the method part only contains formulas with many parameters, making it difficult to grasp the idea of the whole framework at first glance. <sep> 3. The novelty of this paper is somewhat limited while missing some relevant works, e.g.[r1, r2]. [r1] learns a latent space where the compactness within class and separateness between classes are considered. [r2] uses a two-stage prediction for GZSL. <sep> [r1] Jiang et al. Learning Discriminative Latent Attributes for Zero-Shot Classification. In IEEE ICCV 2017. <sep> [r2] Zhang et al. Model Selection for Generalized Zero-shot Learning. In arXiv 2018. <sep> 4. It is a question whether the seen and unseen classes can be separated (Whether a two stage process is correct?). The key for ZSL is knowledge transfer and the base is that seen and unseen classes are related [r3]. If they are separated, can one use the model trained on seen classes to recognize the unseen classes? This is quite problematic. Besides, in Tab.2 there lacks of necessary comparisons with recent relation-based approaches e.g.[r3][r4], which makes the evaluation less sufficient. <sep> [r3] Jiang et al. Transferable Contrastive Network for Generalized Zero-Shot Learning. In IEEE ICCV 2019. <sep> [r4] Li et al. Discriminative Learning of Latent Features For Zero-Shot Recognition. In IEEE CVPR 2018. <sep> 5. Some unclear/incorrect descriptions of the method: <sep> 5.1) The formulation of GZSL is incorrect. Y= union(y_s  y_u), but not intersection(y_s  y_u) <sep> 5.2) How is the class separation formulated in the framework? <sep> 5.3) In Sec.3.2, why is the log-likelihood of the generative models can be obtained by the L1 loss? <sep> Minor issues: <sep> 1. Better use vectorgraphs for clear view (especially for Figure 3 and 4). <sep> 2. Incomplete reference: for Probabilistic semantic embedding (PSE), the reference should add the conference information. <sep> 3. Grammar and spelling mistakes: <sep> [1] Content in Figure 2 (not caption): unseen class -> unseen classes <sep> [2] Last line in 4.1: MCVAE-D -> MCMVAE-D <sep> [3] Last paragraph in 4.2: close -> stay close <sep> [4] Last model name in Table 1: MCMVAE -> MCMVAE-D <sep> 4. The color bar for the contours at the rightmost of Figure 3 is not clear (not the standard way to draw a color bar, better refer to what a color bar is usually drawn). <sep> 5. If possible, better reduce the main text to 8 pages as recommended by the submission instructions (e.g. some content of the method part can be moved to the appendix?).","This paper proposes a relation-based model that extends VAE to explicitly alleviate the domain bias problem between seen and unseen classes in the setting of generalized zero-shot learning. <sep> Reviewers and AC think that the studied problem is interesting, the reported experimental results are strong, and the writing is clear, but the proposed model and its scientific reasoning for convincing why the proposed method is valuable is somewhat limited. Thus the authors are encouraged to further improve in these directions. In particular: <sep> - The idea of using a variant of the widely-used domain discriminator to make seen and unseen classes distinguishable is somewhat contradicted to the basic principle of zero-shot learning. How to trade off the balance between seen and unseen classes has been an important problem in generalized ZSL. These problems need further elaboration. <sep> - The proposed model itself is not a real ""VAE"", making the value of an extensive derivation based on variational inference less prominent. <sep> - There is also the need to compare with the baselines mentioned by the reviewers. <sep> Overall, this is a borderline paper. Since the above concerns were not addressed convincingly in the rebuttal, I am leaning towards rejection."
"abstract | weakness | decision  ==>  ==> Overview: This work proposes to learn sentence embeddings using both contrastive learning and multiple ""views"" of sentences.  This work largely builds off of [1], including using the same objective, but uses a multi-view approach to modeling. <sep> - They apply the concept of multi-view models, specifically combining tree and linear LSTMs to learning sentence representations. <sep> - They prepare a new, large-scale book dataset, which is useful because the previously commonly used book dataset was taken down for legal reason. <sep> - They provide a fairly broad set of analyses on their model, both quantitative and qualitative, performance-driven and analysis-driven. <sep> - Review: The ideas and models presented in this paper are not new, while the supporting experiments are not very well done or convincing. Overall, I recommend rejecting this work. <sep> - The models are contrastively learned in that they are trained to embed ""similar"" sentences nearby in the embedding space, and ""dissimilar"" sentence far away, where ""similar"" sentences are defined as consecutive sentences. This method of learning textual representations is well-established in the NLP literature, mostly prominently in recent years with word embedding models like Skip-Gram and in sentence embedding models like in [1], [2], [3] (the next sentence prediction task), and several more. <sep> - In practice, the multiple views of each sentence that this paper considers boils down to encoding the sentence with a bidirectional LSTM and a TreeLSTM and concatenating the representations from each encoder. This idea again has been established in the literature ([4], [5], [6]). <sep> - The experiments don't seem setup to demonstrate that the multiple views are beneficial over a single view. In Table 1, there are rows for just an LSTM or just a TreeLSTM, but they seem to be trained with labeled data whereas the proposed method is trained self-supervised. A more informative comparison to demonstrate the value of using multiple views would be to train the LSTM and TreeLSTM with the same objective (and ideally model size). Overall, I don't think the claims in the paper are well-supported by the model proposed or the experiments. <sep> - I have a number of concerns about the experiments. <sep> - ""Models are trained on a single epoch on the entire corpus without any train-test split"": so there is no early stopping? Why stop training after one epoch? Was there any indication you were overfitting the data? <sep> - ""The training phase was stopped after 33 hours of training"": Why stop there? Computational constraints? Later comments suggest this is quite premature (""training phase was completed on only 4.6M sentences among the 78M available""). <sep> - The results seem to indicate that this method underperforming recent work significantly. <sep> Areas of improvement <sep> - Some of the language in the introduction and conclusion are a bit of a stretch. Using a linear and tree LSTM (based on dependency parses) doesn't really represent a ""diversity of linguistic structures"". <sep> - Related work: There's no mention of pretained language models, which could be seen as a form of representation learning for language, and have been hugely impactful in NLP. <sep> - Method <sep> - Missing negative in the log likelihood <sep> - Why do you use inner product if other works ""report excellent results"" with other scoring functions? <sep> - ""assumes the underlying structure of the sentence to be a sequence, while allowing for long term dependencies"": If anything, the treeLSTM more easily allows for long-term dependencies than the linear LSTM. <sep> - ""Negative examples are obtained using the dependency Tree LSTM"": I'm not totally sure how the negatives are obtained here. <sep> - ""The target sequence is encoded using the sequential Tree LSTM, while the positive and negative samples are encoded using the ChildSum Tree LSTM"": why are the sentences not all encoded with the same encoder? <sep> - It looks really odd that most of Table 1 is empty. Given your model, I imagine it can't have been that difficult to evaluate more baselines (BiLSTM and TreeLSTM) on the rest of the tasks. <sep> - It'd be nice if you could clearly indicate in Table 1 which method is yours. <sep> - Results and Analysis <sep> - The standard evaluation setting for sentence embeddings would be GLUE or SuperGLUE. <sep> - A glaringly missing baseline is BERT (or any of its relatives), which is also self-supervised. <sep> - The results are underwhelming, and as the author admits, somewhat premature as training didn't seem to finish. <sep> - 5.2: what are the contrastive LSTM and Tree LSTM? Are those the learned encoders from the ""Contrastive Tree"" in Table 1, or are they trained from scratch? <sep> - I don't think the analyses in Sections 5.2 and 5.3 or Figure 2 are particularly useful. <sep> - There are a noticeable number of typos. For example, in the abstract: ""this linguist[ic] diversity"" and ""better capture semantic[s]"". It'd be worthwhile to look over the paper closely for typos. <sep> [1] AN EFFICIENT FRAMEWORK FOR LEARNING SENTENCE REPRESENTATIONS. Lajanugen Logeswaran and Honglak Lee <sep> [2] Discourse-Based Objectives for Fast Unsupervised Sentence Representation Learning. Yacine Jernite, Samuel R. Bowman, David Sontag <sep> [3] BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova <sep> [4] Enhancing and Combining Sequential and Tree LSTM for Natural Language Inference. Qian Chen, Xiaodan Zhu, Zhenhua Ling, Si Wei, Hui Jiang. <sep> [5] Enhanced LSTM for Natural Language Inference. Qian Chen, Xiaodan Zhu, Zhenhua Ling, Si Wei, Hui Jiang, Diana Inkpen. <sep> [6] Improving Sentence Representations with Consensus Maximisation. Shuai Tang, Virginia R. de Sa.","This paper proposes a method to learn sentence representations that incorporates linguistic knowledge in the form of dependency trees using contrastive learning. Experiments on SentEval and probing tasks show that the proposed method underperform baseline methods. <sep> All reviewers agree that the results are not strong enough to support the claim of the paper and have some concerns about the scalability of the implementation. They also agree that the writing of the paper can be improved (details included in their reviews below). <sep> The authors acknowledged these concerns and mentioned that they will use them to improve the paper for future work, so I recommend rejecting this paper for *CONF*."
"weakness | decision  ==>  ==> ######### Rebuttal Response: <sep> Thanks for the clarifications and especially for updating the formatting. The current state does not convince me to rate the paper as weak accept but I increased my rating to weak reject. <sep> ""Pereira et. al. has shown that a recurrent network architecture using LSTM outperforms the fully connected networks at every time step proposed by Han et. al. in task completion, space and time complexity. Therefore, in this paper we choose to use an LSTM-based network architecture."" <sep> -> Yes it might be true that a recurrent function approximator does in practice perform better than a feed-forward function approximator. However, in theory a feed-forward network should be sufficient as the value function does not depend on the previous states. Therefore, the question is, why does the LSTM perform better? Does the recurrent nature of the LSTM make the predictions smoother compared to a feed-forward network?  Can any other regularizing scheme be introduced s.t. the feed-forward networks performs equally well? <sep> ######### Review: <sep> Summary: <sep> The paper builds on the work of Pereira et. al. and uses forward backward stochastic differential equations to learn the Hessian of the Value function Vxx and \\partial _t V_x + 1/2 tr(\\partial_{xx} V_x CC^T). In contrast to the prior work, this paper introduces multiplicative noise for the control and uses second order optimization. The performance is evaluated on different control tasks, e.g., linear system, cartpole, quadcopter & human arm actuated by tendons. <sep> Conclusion: <sep> All in all, I like the proposed research of combining theoretical approaches and deep learning to perform trajectory optimization and I would like to see much more of this research like this within the *CONF* community. Furthermore, I think that the paper has a contribution and that the paper was improved compared to the initial Neurips submission (i.e., adding ILQG as baseline). However, the writeup and formatting is still very much sub-standard and must be improved to make this paper worth publishing. The current write-up is not accessible for the *CONF* community and the understandability must be significantly improved (Details are provided below). Therefore, I currently rate this paper as a clear rejection but I am happy to improve the score to 7-8 if the write up is improved during the rebuttal. <sep> Theoretical Structure: <sep> I like the introduction, which covers the topic but might be a bit too long. Maybe you want to shorten the introduction and add an additional related work section at the end. The stochastic control introduction is nice and has the correct level of abstraction for the reader. However, the paper introduces many complex concepts which are not essential for understanding the paper (e.g., filtered probability space etc.). One might want to trade off understandability vs. mathematical rigor especially, if the paper does not rely on these concepts. Furthermore, you might want to make eq 1 more explicit as the multiplicative action noise is not visible from eq 1. Section 3 'A FBSDE Solution to the HJB PDE' is the most problematic section of this paper, which is not understandable for the common *CONF* reader. Eq. 6, which just appears without any derivation, is not understandable and the reader has no intuition how to derive this eq. Furthermore, Eq 6 (page. 4) uses notations which is only clearly introduced later within the paper or even the appendix (e.g., Y being the propagated value function, Z being the propagated gradient of the value function is only mentioned in the appendix, i.e., page 12. \\Gamma is only introduced in page 5. Yes, Eq. 7 defines these variables but the style of definition is not standard and one does not expect the variables to be defined in this style.). Could the authors please provide an intuitive derivation of these equation and use clearer notation (Why would one want to abstract V, V_x, V_x, \\mathcal|{H}(V_x) in the first place as these are intuitive for the *CONF* community and sufficiently short?) Especially, as this section highlights the difference to the prior works of Pereira et. al., this section should be very clear. Section 4 is clear but should include the loss function as the loss is not trivial and essential for the optimization. Currently, the loss description is buried in the appendix. All in all, the theoretical explanation and the bloated notation should be simplified and every equation should be embedded into an intuitive derivation. Currently these explanations are not understandable without reading the appendix and prior work. <sep> Experiments: <sep> The experiments apply 2FBSDE to 4 different control tasks (Linear system, quadcopter, cartpole & human arm) and compare the performance to the prior work of FBSDE and iLQG. The number of baselines and systems is sufficient. However, the paper should provide more evaluations: <sep> (1) Plot the histogram of the obtained cost distributions. <sep> (2) Plot a single state- and action trajectory (and the action distribution). Using these plots, the level of noise and smoothness and hence the applicability to physical systems can be evaluated. <sep> (3) Plot the noise free trajectories and show that these mean trajectories reach the desired solution. <sep> (4) Specify the exact cost function for every experiment <sep> Further Comments to the individual experiments: <sep> Cartpole: <sep> The Cartpole iLQG seems to perform much better (swing-up the pendulum faster, don't deviate so much from x=0, much more coherent velocity compared to 2FBSDE, FBSDE). Could the authors please discuss these aspects in more detail and present experiments with longer time-horizons to check whether the proposed method can stabilize the cart at [0, 0, 0, 0]. The current plots don't reach this target state. Your plots also hint that the cartpole does not need to pre-swing the pendulum, which is most likely due to the very low action cost. This selection of action cost significantly simplifies the problem. Could the authors please include a cartpole with higher action cost and show that 2FBSDE can learn to pre-swing the pendulum. <sep> The quadcopter: <sep> Could the authors please specify the exact quadcopter dynamics. What kind of abstraction did you model? What are the control inputs? Furthermore, the citation for the dynamics is wrong and puts the supervisor of the master thesis as first author. Furthermore, can the authors please provide longer plots to highlight, which method can stabilize the system. <sep> Human Arm: <sep> For the human arm neither iLQG or 2FBSDE reach the desired target location. Can you explain why no trajectory optimization does reach the desired position. <sep> Formatting: <sep> Please rework the formatting such that the inline math does not cause the formatting issues of different line spacings (e.g., sec. 2.1, sec. 5) and irregular whitespaces (e.g., last line of paragraph 2.1 Preliminaries). Please remove the color coding of text for the experiments and make sure that the legends are sufficiently large and include all lines. Currently the legends are missing the target state. You can also extend the figure captions to highlight the conclusion of the plots. Please rework the figures such that the figures do not cause so much whitespace (e.g., Figure 3, 4 & 5). When reconfiguring the plots, you gain space, which can be used for further explanation of the theory. Furthermore, you might want add dotted lines to the confidence intervals as the confidence intervals are important but the differences are not clearly visible from the plots. Also, the labelling in figure 3 seems wrong, the axis labeled cart velocity should be pendulum angle and the pendulum angle axis should be cart velocity. All in all, the formatting can be significantly improved, which is especially bad as this paper is most likely a resubmission from Neurips. <sep> Minor Comments / Questions: <sep> - 'where l :Rnx×Rnu→R+ is the running cost and C1,23 \\phi :Rnx→R+ is the terminal state cost.' Is l and \\phi of class C^{1,2} or only one of them? The notation is confusing and should be simplified. <sep> - Can you comment on how important this multiplicative noise in physical system? ¬¬¬ <sep> - Why are you using a LSTM instead of a simple feed-forward neural network as the ff-nn should be sufficient to model V(x) as the value function is not recurrent. Have you tried using a simple ff-nn?","A nice paper, but quite some unclarities; it's unclear in particular if the paper improves w.r.t. SOTA. Esp. scaling is an issue here. Also, the understandability is below par and more work can make this into an acceptable submission."
"abstract | rating_summary | weakness | suggestion | misc  ==> The authors consider the relation between Frechet distance of training and test distribution and the generalization gap. The authors derive the lower bound for the difference of loss function w.r.t. training and test set by the Wasserstein distance between embedding training and test set distribution. Empirically, the authors illustrate a strong correlation between test performance and the distance in distributions between training and test set. <sep> The motivation to find the relation between generalization gap and the Frechet distance of training and test distribution is sound. However, I am not sure that the lower bound as in Equation (1) is enough. I am curious that one can derive the upper bound for the relation or not. The finding about choosing a training data distribution should be close to the test data distribution seems quite trivial in some sense. I am not clear about its important since it is quite popular that the distribution shift affects the performance and many learning approach assumes same distribution for training and test data. Overall I feel that the contribution may be quite weak, and I lean on the negative side. <sep> Below are some of my concerns: <sep> 1) About the lower-bound in Equation (1), it seems unclear to me that when the W_2(p1, p2) = 0, we can inference any information about the test performance (It seems quite trivial for this case, the left hand side time is greater than or equal 0?) In my opinion, the upper-bound is more important which one can inference much information about the difference of generalization gap. <sep> 2) In the proof of Theorem 1, it is quite hard to follow with the current notation, for the integral in (i), (ii) as well as in the proof using the intermediate value theorem, which variables are used? I am confused which one is variable, which one is constants in those integrals. <sep> 3) In page 5, at the interpretation (1), for W2(p1, p2) = 0, the learned function fits training distribution perfectly and is not ill-conditioned ==> why one can deduce that the test distribution is fit perfectly? What we have in Theorem 1 is the lower-bound only?","The authors discuss how to predict generalization gaps. Reviews are mixed, putting the submission in the lower half of this year's submissions. I also would have liked to see a comparison with other divergence metrics, for example, L1, MMD, H-distance, discrepancy distance, and learned representations (e.g., BERT, Laser, etc., for language). Without this, the empirical evaluation of FD is a bit weak. Also, the obvious next step would be trying to minimize FD in the context of domain adaptation, and the question is if this shouldn't already be part of your paper? Suggestions: The Amazon reviews are time-stamped, enabling you to run experiments with drift over time. See [0] for an example. <sep> [0] https://www.aclweb.org/anthology/W18-6210/"
"misc | strength | weakness | rebuttal_process  ==> The authors propose a model for learning local pixel motions between pairs of frames using local image representations and relative pixel displacements between agents and objects.  The model learned is compared to the ability of the primary visual cortex where adjacent simple cells share quadrature relationships and capture local motion. <sep> ""The representation theory underlies much of modern mathematics and holds the key to the quantum theory (Zee, 2016)."" <sep> Can the relevance of this claim be elaborated on? <sep> ""Figure 1 illustrates the scheme of representation."" <sep> Please provide more detail here on what is happening in the figure.  The caption and reference here are not informative to what the figure is representing. <sep> ""We obtain the training data by collecting static images for (It) and simulate the displacement field ...  We refer to this method as self-supervised learning"" <sep> This is not self-supervised learning.  In self-supervised learning the training label/signal is generated by the system.  In this case artificial data is being generated as the displacement between images is sampled. <sep> Since the motion between images is artificially generated what guarantees are there that the model is learning to capture realistic motion behavior?  Why not use adjacent video frames? <sep> ""Note that those methods train deep and complicated neural networks with large scale datasets to predict optical flows in supervised manners, while our model can be treated as a simple one-layer network, accompanied by weight matrices representing motions."" <sep> Is there a comparison on execution times of the different approaches? <sep> ""by obtaining the pre-trained models and testing on V1Deform testing data"" <sep> Is this a fair comparison if the proposed approach was trained on V1Deform training data and the comparison methods were not.  A more appropriate comparison would be to apply all the methods to infer the displacement fields between video frames which is also a more natural application.  This can be controlled to contain small motions if needed.  Why nt use the MUG dataset here? <sep> ""Displacements at image border are leaved out"" -> left out <sep> Sections 5.4, 5.5 and 5.6 show only qualitative results with no comparison methods.  Can the authors provide reasons that other methods could not be used for evaluation? <sep> I am not sure I understand the motivation for the approach.  Why do we need this over other methods that can better capture larger motions.  This needs to be more clear from the introduction.  Why do we care if the approach captures aspects of V1 for the tasks presented? <sep> The work is sensible and the approach is clear but I found the evaluation and motivation lacking in key areas that I mention above.  The authors should revise and make it clear to the reader why we should care about this problem.  Aligning with V1 is interesting but it does not come into play in the applications of the approach or the analysis so I am not sure why I should care.  The evaluation also needs to be much more convincing before I could recommend acceptance.","The paper received mixed reviews. On one hand, there is interesting novelty in relation to biological vision systems. On the other hand, there are some serious experimental issues with the machine learning model. While reviewers initially raised concerns about the motivation of the work, the rebuttal addressed those concerns. However, concerns about experiments remained."
"abstract | rebuttal_process | weakness | rebuttal_process | decision  ==> This paper proposes to pre-train policies on some goal-reaching tasks, and then leverage the associated successor features to improve the learning of a new task. The method heavily draws from the Generalized Policy Evaluation/Improvement framework without adding much to it. The only relevant point would be showing (as the title indicates) how to obtain disentangled cumulants, and whether they help transfer to new tasks. Nevertheless, both the definition, the full method, and the claimed benefits are quite ambiguous. <sep> Among other concerns showing that the theory needs more formal treatment, the pillar definition of ""Optimal independent controllability"" is very confusing because it seems to depend on ""a trajectory generated by following \\pi_i^*"". But what If the environment is stochastic? Then following that policy might give different trajectories! This definition needs to be revisited. More concerning examples are given at the end of this review. <sep> On the experimental side, Fig. 4 is the only reported result, and it has an x-axis that is not clearly explained. What are the ""steps (min)""? <sep> It is also not clear what they mean by the ""off-diagonal trick"", which seems so important for the good performance. <sep> Furthermore, it seems that their method doesn't really learn anything new in most of the tasks, it just stays at the same performance that is started with after the whole pre-training steps. It is not clearly stated how much computation effort is required to obtain the desired cumulants, and this invalidates quite strongly any result they report. Even if there's no ""reward"" needed during the pre-training, which arguably is not even true because you do need the rewards related to whether you have achieved a specific change in a feature! <sep> In fact, it would be greatly appreciated if the ""final"" tasks could be expressed in a similar notation than the rest of the pre-training tasks, or vice-versa. As far as I understand, the pre-training tasks consist of making a certain feature fall into a certain subset of its possible values. Can't the final tasks, like ""move the agent to the top right"" be also expressed in that form. The link between the two kinds of tasks needs to be much more explicit to be able to assess the relevance of this work. <sep> Finally, they only test their algorithm on Spritworld, which is a small discrete state-action space environment. Even if they try different kinds of tasks in this environment, more detailed analysis or more extensive experiments are needed to assess the benefits of the proposed approach. <sep> This is particularly timely because their method relies on a discretization of some given features that represent the state, which will probably not be very practical in higher dimensional environments. <sep> Finally, I would like a comment on how this method interacts with discrete versus continuous action-state spaces. <sep> Misc comments: <sep> - Why do the authors introduce the terminology ""Endogenous RL"", and then say it's the same as doing RL with intrinsic motivation? This seems like introducing a new name for the same concept, which seems pointless and confusing. <sep> - The connection with ""latent learning"" of Tolman 1984 is very unclear. <sep> - There's a ""Representation Learning"" section, but it's not clear at all whether any features are actually learned, or whether the features are actually hand-defined. Is the number of features n also hand-defined? <sep> - There might be a typo in the first sentence after equation (9): ""While \\phi_w is not guaranteed to be optimal with respect to \\phi_w"". <sep> Because of all these concerns, I suggest the paper to be weakly rejected.","The author propose a method to first learn policies for intrinsically generated goal-based tasks, and then leverage the learned representations to improve the learning of a new task in a generalized policy iteration framework. The reviewers had significant issues about clarity of writing that were largely addressed in the rebuttal. However, there were also concerns about the magnitude of the contribution (especially if it was added anything significant to the existing literature on GPI, successor features, etc), and the simplicity (and small number of) test domains. These concerns persisted after the rebuttal and discussion. Thus, I recommend rejection at this time."
"abstract | weakness | decision  ==> Pros: <sep> This paper proposed a new method for zero-shot transfer learning under the reinforcement learning setting. The use of attention weights to regularize the latent states was fairly interesting. <sep> Cons: <sep> Limited applicability of the proposed methods <sep> - The paper was restricted in a setting where rewards, actions, and true states were identical between source and target environments, and only the observed states differed due to differing renderers. Working under such a restricted setting was interesting in its own right, but it might also lead to limited applicability of the proposed method in the real-world setting. <sep> - The proposed method focused on solving a very specific problem: learning a dis-entangled latent representation for images. As a result, the potential impact of the proposed methods could be minimal. <sep> Limited technical novelty <sep> - The proposed method, SADALA, was built on top of Higgins et al., 2017 (DARLA). The only difference was an added attention layer to the learning of latent states. As a result, the novelty of the proposed method was very incremental and limited from a technology perspective. <sep> - Even with additional attention layer, the paper could have performed a more thorough study to help the readers understand and appreciate the idea. For example, this paper didn't discuss the tradeoff between training SADALA over separate stages, versus training it from end to end. For example, why the weights of the pre-trained beta-VAE had to be frozen and used as weights in the state representation stage. <sep> Insufficient experiments <sep> -More thorough discussion of the qualitative results should be helpful to understand whether the attention weights helped the model to focus on the right thing. For example, this paper did study the quality of reconstruction in Figure 3-5 of the proposed method. When comparing Figure 3 and Figure 5, it appeared to me that the reconstructed the angle of the pole was different from the original one. And it seemed like attention weights did successfully ignored the color of the cart and pole, but it ignored the angle of the pole, which should be important to the learning task. Unfortunately, the paper didn't further explain the implication of such misrepresentation. <sep> -Quantitative results <sep> * It would be interesting to all compare the proposed methods against model-agonistic methods like MAML <sep> * It would be useful to include confidence intervals over different tasks. <sep> * It would be useful to compare different methods with different parameter settings <sep> * The authors mentioned ""Visual Pendulum tasks"" but didn't include them in the paper <sep> Reproducibility <sep> - It's unclear to me how reproducible the research conducted in this paper was, and it would be useful to open source the code used to conduct the experiments.","This paper proposes a new method for zero-shot policy transfer in RL. The authors propose learning the policy over a disentangled representation that is augmented with attention. Hence, the paper is a simple modification of an existing approach (DARLA). The reviewers agreed that the novelty of the proposed approach and the experimental evaluation are limited. For this reason I recommend rejection."
"abstract | strength | weakness | decision  ==> This paper studies the impact of embedding complexity on domain-invariant representations. By incorporating embedding complexity into the previous upper bound explicitly, the authors demonstrate the limitations of previous theories and algorithms. Based on their theoretical findings, the authors propose to control the embedding complexity with implicit regularization. Specifically, aligning source and target feature distributions in multiple layers controls both embedding complexity and domain discrepancy. The proposed algorithm can achieve similar performance as DANN with manual selection of embedding depth. <sep> By noting that the hypothesis space can be decomposed in to the feature extractor and the classifier, the authors propose to address the domain discrepancy separately. D_H\\DeltaH is termed latent divergence, which the algorithm attempts to minimize. D_G\\DeltaG is treated as embedding complexity, which is the intrinsic property of the feature extractor. Thus, domain-invariant representations should seek a proper tradeoff between those two terms. <sep> The paper is well-written and the contributions are stated clearly. The exploration on the layer division is really insightful. <sep> However, I have several concerns: <sep> 1. The proposed upper bound is insightful, but it has several limitations. Compared to the version applied to the feature space in equation (3), the proposed upper bound is looser. The embedding complexity terms includes two encoders, which are deep neural networks in practice, thus it can be excessively large. As the authors point out, in equation (3), the embedding complexity is not addressed explicitly, but it is implicit in the adaptability \\lambda in a more reasonable way. Previous works [1], [2], [3] have already taken them into consideration. Proposition 5 is a direct application of proposition 1 in [1]. <sep> 2. On the claim of implicit regularization. By applying domain adversarial training to multiple layers, the authors claim that the encoder in higher layers is implicitly restricted. However, they do not validate this regularization effect. Is the embedding complexity controlled? Theoretical analysis or experimental results would be helpful. <sep> 3. The proposed MDM method seems to be incremental. [4] has probed into the effect of multi-layer adaptation strategy. Besides, applying domain adversarial training to many layers leads to more computational cost and may slow down training significantly. <sep> [1]Fredrik D Johansson, Rajesh Ranganath, and David Sontag. Support and invertibility in domain- invariant representations. arXiv preprint arXiv:1903.03448, 2019. <sep> [2]Han Zhao, Remi Tachet des Combes, Kun Zhang, and Geoffrey J Gordon. On learning invariant representation for domain adaptation. arXiv preprint arXiv:1901.09453, 2019. <sep> [3] Hong Liu, Mingsheng Long, Jianmin Wang, and Michael Jordan. Transferable adversarial training: A general approach to adapting deep classifiers. In International Conference on Machine Learning, pp. 4013–4022, 2019. <sep> [4] Mingsheng Long, Yue Cao, Jianmin Wang, and Michael I. Jordan. Learning transferable features with deep adaptation networks. In Proceedings of the 32nd International Conference on International Conference on Machine Learning, volume 37, pp. 97–105, 2015.","This paper studies the impact of embedding complexity on domain-invariant representations by incorporating embedding complexity into the previous upper bound explicitly. <sep> The idea of embedding complexity is interesting, the exploration has some useful insight, and the paper is well-written. However, Reviewers and AC generally agree that the current version can be significantly improved in several ways: <sep> - The proposed upper bound has several limitations such as looser than existing ones. <sep> - The embedding complexity is only addressed implicitly, which shares similar idea with previous works. <sep> - The claim of implicit regularization has not been explored in-depth. <sep> - The proposed MDM method seems to be incremental and related closely with the embedding complexity. <sep> - There is no analysis about the generalization when estimating this upper bound from finite samples. <sep> There are important details requiring further elaboration. So I recommend rejection."
"decision | abstract | weakness | decision  ==> The authors describe a method for adversarially modifying a given (test) example that 1) still retains the correct label on the example, but 2) causes a model to make an incorrect prediction on it. The novelty of their proposed method is that their adversarial modifications are along a provided semantic axis (e.g., changing the color of someone's skin in a face recognition task) instead of the standard Lp perturbations that the existing literature has focused on (e.g., making a very small change to each individual pixel). The adversarial examples that the authors construct, experimentally, are impressive and striking. I'd especially like to acknowledge the work that the authors put in to construct an anonymous link where they showcase results from their experiments. Thank you! <sep> Overall, I think that this is interesting work that can help to broaden the study of adversarial examples and make them more applicable even in non-adversarial settings (e.g., by making models more robust to the changes in semantic attributes that the authors consider). There has been quite a bit of interest in the community in adversarial examples that are not just Lp perturbations, and I believe that the authors' approach will encourage a good deal of follow-up research. <sep> However, my main concern with the paper is that in my opinion, it does not sufficiently address why it is important to generate adversarial examples in the way they do. For example: <sep> 1) Is the argument that this is a more powerful attack surface, so adversaries should take note (and defenders should figure out how to defend against this)? If that is the case, what is the attack model under which these attacks are realistic? For example, the original L∞ attacks are motivated in the sense that the adversarial examples are visually imperceptible, so they might not be noticed by the end-user. What is the equivalent argument for these semantic attacks? <sep> 2) Is the argument that these semantic attacks somehow capture a more realistic part of the data distribution over all natural images, and therefore it is good to have models that perform well on these semantic adversarial examples even if we're not concerned about an adversary (e.g., because the model might generalize better to other tasks or be more causally correct)? If that's the case, then I think this needs to be explored more. For example, what about the following straw man baseline: use a controllable semantic-attribute-based generator to generate semantically different images without any notion of an adversarial attack, and then do standard Lp attacks on that generated image? How would that be better or worse than the proposed method? <sep> 3) Or is the argument that it is just good to be able to generate examples that models get wrong? If so, why, and why is this method better than other methods? <sep> I think the paper would be significantly stronger if the importance and implications of their work were explicated along the above lines. For this reason, my current assessment is a weak reject, though I'd be open to changing this assessment. <sep> === Less critical comments, no need to respond or fix right away === <sep> While the overall concept and approach was clear, I generally found the notation and mathematical exposition difficult to follow. Please be more precise. Here is a non-exhaustive list of examples from section 3: <sep> a) I'm not sure what's the difference between xtgt and xadv, or between xnew and  x∗. These seem to be used somewhat interchangeably? <sep> b) Equation 3 is the central optimization problem in the paper, and should be written out explicitly using α as the optimization variable, instead of referring to equations 1 and 2 (in which x∗ doesn't even appear). <sep> c) I didn't understand equation 4. What does assuming M(xtgt)=ytgt mean? What happens when that is not true? <sep> d) Equation 5: Why is y in the right hand side by not in the left? <sep> e) Equation 6: Lsmooth is missing an argument.","I had a little bit of difficulty with my recommendation here, but in the end I don't feel confident in recommending this paper for acceptance, with my concerns largely boiling down to the lack of clear description of the overall motivation. <sep> Standard adversarial attacks are meant to be *imperceptible* changes that do not change the underlying semantics of the input to the human eye. In other words, the goal of the current work, generating ""semantically meaningful"" perturbations goes against the standard definition of adversarial attacks. This left me with two questions: <sep> 1. Under the definition of semantic adversarial attacks, what is to prevent someone from swapping out the current image with an entirely different image? From what I saw in the evaluation measures utilized in the paper, such a method would be judged as having performed a successful attack, and given no constraints there is nothing stopping this. <sep> 2. In what situation would such an attack method would be practically useful? <sep> Even the reviewers who reviewed the paper favorably were not able to provide answers to these questions, and I was not able to resolve this from my reading of the paper as well. I do understand that there is a challenge on this by Google. In my opinion, even this contest is somewhat ill-defined, but it also features extensive human evaluation to evaluate the validity of the perturbations, which is not featured in the experimental evaluation here. <sep> While I think this work is potentially interesting, it seems that there are too many open questions that are not resolved yet to recommend acceptance at this time, but I would encourage the authors to tighten up the argumentation/evaluation in this regard and revise the paper to be better accordingly!"
"abstract | weakness  ==>  ==> ========================= <sep> Update review <sep> After reading the authors response I would like to keep my score as is. <sep> I still see many unclear statements, and most importantly I feel that more analysis of the proposed method should have been done here. <sep> ========================= <sep> This paper proposed a Top-Down method for neural networks training based on the good classifier hypothesis. In other words, after obtaining a classifier that performs well on the test set, keep fine-tuning / re-learning the data representation. <sep> The authors provide character error rate results for the task of Automatic Speech Recognition using WSJ and CHiME-4 datasets. <sep> Although being an interesting research idea, several issues in this paper make it not yet ready for publication at *CONF*. <sep> First, the paper is poorly written; there are many claims the authors are making without providing experiments/proofs/citations. <sep> For example: ""...since the feature extractor learns more slowly, then potentially the classifier may overfit the feature extractor before the feature extractor is able to learn much about the underlying pattern of the data..."". <sep> Or: ""...We suggest that the reason for this is that when all layers are trained on the noisy dataset jointly, the middle layers overfit the bottom-most layers much faster than the bottom-most layers are able to learn input features..."" <sep> Next, since there is no theoretical/mathematical explanation of the proposed approach, I expect the authors to run an analysis on the results to better understand the effect of using such an approach. For instance, under which settings this method is most efficient? In what layer should  I start the fine-tuning? Is it better to reinitialize the bottom layers or fine-tune them? Does the proposed approach applicable to different domains? i.e. vision/nlp/other speech/signal processing tasks?  Does the proposed approach applicable to different models or only for the proposed one? <sep> Lastly, although it is not the main point in this paper since all results are reported on ASR, did the authors tried to compute WERs too? That way, people can compare results with other ASR models. The baseline seems relatively weak, at least in Table 1. <sep> Minor comments: <sep> The complexity of the algorithm is written to be O(n). However, this assumes training the model takes O(1) or did I miss something? <sep> Can the authors provide more details/insights regarding the delta differences in Table 1? Did the authors use the same initializations? Did the authors try different ones?","The paper proposes a top-down approach to train deep neural networks -- freezing top layers after supervised pre-training, then re-initializing and retraining the bottom layers. As mentioned by all the reviewers, the novelty is on the low side. The paper is purely experimental (no theory), and the experimental section is currently too weak. In particular: <sep> - Experiments on different domains should be performed. <sep> - Different models should be evaluated. <sep> - Ablation experiments should be performed to understand better under which conditions the proposed approach works. <sep> - For speech recognition, WER should be reported - even if it is without a LM - such that one can compare with existing work."
"rebuttal_process | suggestion  ==> The paper proposed a method to model the graph topological evolution from the spectral domain by developing a new generalized graph kernel. The new graph kernels cover many existing graph kernels as well as their combination and composition as special cases. The idea of spectral graph translation and its integration with deep learning is interesting, especially considering that most previous spectral graph neural networks only transform the graph signal instead of graph structures. However, I do have some concerns of papers. <sep> 1. My major concern is the soundness of keeping eigenvectors unchanged in the evolution. Although the authors claim that in previous studies eigenvectors are found stable in evolution, it is very counter-intuitive, and I am not sure it is the case for all types of graphs. Let us look at the proof of Lemma 3.1, obviously L′ does not necessarily have the same eigenvectors, and UTL′U is not a diagonal matrix, so this loss is actually very large in many cases. That is to say, the evolution model does not have enough expressive power to recover the L′. <sep> 2. Spectral graph translation looks interesting, but the main idea comes from Kunegis et al. (2010). Despite of a new designed graph kernel and adding nonlinear activations, the contributions seem not so significant. <sep> 3. In Kunegis et al. (2010), they consider the evolution of adjacency matrix A, but in this paper the authors use the Laplacian matrix L. If there any reason to make this choice? Also, I think some of the conclusions (e.g. stable eigenvectors) in Kunegis et al. (2010) may not work since L is used instesd of A. <sep> 4.    The correlation metric is acceptable, but it will be much better if the authors can do more analysis. For example, why not add the link prediction task as in Kunegis et al. (2010)? BTW, is correlation analysis used before in previous graph evolution papers? (if so, please add a reference)","The reviewers kept their scores after the author response period, pointing to continued concerns with methodology, needing increased exposition in parts, and not being able to verify theoretical results. As such, my recommendation is to improve the clarity around the methodological and theoretical contributions in a revision."
"abstract | strength | rebuttal_process | weakness | rebuttal_process | suggestion | decision  ==> The paper proposes a novel way to formulate intrinsic reward based on optical flow prediction error. The prediction is done with Flownet-v2 architecture and the training is formulated as self-supervision (instead of the ground-truth-based supervised learning in the original Flownet-v2 paper). The flow predictor takes two frames, predicts forward and backward flows, then warps the first/second frame respectively and compares the warped result with real frame. The comparison error serves as the intrinsic reward signal. The results are demonstrated on 7 environments: SuperMario + 5 Atari games + ViZDoom. On those environments, the proposed method performs better or on-par with ICM and RND baselines. <sep> I am leaning towards rejecting this paper. Two key factors motivate this decision. <sep> First, the motivation for this work is not fully clear: why would the error in flow prediction be a good driving force for curiosity? Optical flow has certain weaknesses, e.g. might not work well for textureless regions because it's hard to find a match. Why would those weaknesses drive the agent to new locations? <sep> Second, the choice of tasks where the largest improvement is shown (i.e. 5 Atari games) seems not well-motivated and rather crafted for the proposed method. Those 5 Atari games are not established hard exploration games. <sep> Detailed arguments for the decision above: <sep> [major concerns] <sep> * Analysis is need on how the method deals with known optical flow problems: occlusion, large displacements, matching ambiguities. Those problems don't fully go away with learning and it is unclear how correlated corresponding errors would be with state novelty. <sep> * ""Please note that ri is independent of the action taken by the agent, which distinguishes FICM from the intrinsic curiosity module (ICM) proposed in Pathak et al. (2017)"" - but would it then be susceptible to spurious curiosity effects when the agent is drawn to motion of unrelated things? Like leaves trembling in the wind. ICM was proposed to eliminate those effects in the first place, but what is this paper's solution to that problem? Furthermore, the experiments on BeamRider show that this concern is not a theoretical one but quite practical. <sep> * ""CrazyClimber, Enduro, KungFuMaster, Seaquest, and Skiing"" - none of those Atari environments are known to be hard exploration games (which are normally Gravitar, Montezuma Revenge, Pitfall!, PrivateEye, Solaris, Venture according to Bellemare et al ""Unifying count-based exploration and intrinsic motivation""). I understand that every game becomes hard-exploration if the rewards are omitted but then there is a question why those particular games. Moreover, if you omit the rewards the question remains how to select hyperparameters of your method. Was the game reward used for selecting hyperparameters? If not, what is the protocol for their selection? This is a very important question and I hope the authors will address this. <sep> * ""These games are characterized by moving objects that require the agents to concentrate on and interact with."" - this looks like tailoring the task to suit the method. <sep> * Figure 6 - those results are not great compared to the results of Episodic Curiosity: https://arxiv.org/abs/1810.02274 . Maybe this is because of the basic RL solver (A3C vs PPO) but that brings up another question: why are different solvers used for different tasks in this paper? PPO is normally significantly better than A3C, why not use throughout the whole paper? <sep> [minor concerns] <sep> * Figures are very small and the font in them is not readable. Figure 2 is especially difficult to read because the axes titles are tiny. <sep> * ""complex or spare reward"" -> sparse <sep> * ""However, RND does not consider motion features, which are essential in motivating an agent for exploration."" - this is unclear, why are those features essential? <sep> * ""We demonstrated the proposed methodology and compared it against a number of baselines on Atari games, Super Mario Bros., and ViZDoom."" - please state more clearly that only 5 out of 57 Atari games are considered, here and in the abstract. <sep> * ""Best extrinsic returns on eight Atari games and Super Mario Bros."" - but only 5 games are shown, where are the other 3? <sep> Suggestions on improving the paper: <sep> 1) Better motivating the approach in the paper would help. Why using the flow prediction error as a curiosity signal? <sep> 2) Better motivating the choice of the environments and conducting experiments on more environments would be important for evaluating the impact of the paper.","This paper proposes a method for improving exploration by implementing intrinsic rewards based on optical flow prediction error. The approach was evaluated on several Atari games, Super Mario, and VizDoom. <sep> There are several strengths to this work, including the fact that it comes with open source code, and several reviewers agree it's an interesting approach. R1 thought it was well-written and quite easy to follow. I also commend the authors for being so responsive with comments and for adding the new experiments that were asked for. <sep> The main issue that reviewers pointed out, and which I am also concerned about, is how these particular games were chosen. R3 points out that these 5 Atari games are not known for being hard exploration games. Authors did conduct further experiments on 6 Atari games suggested by the reviewer, but the results didn't show significant improvement over baselines. <sep> I appreciate the authors' argument that every method has ""its niche"", but the environments chosen must still be properly motivated. I would have preferred to see results on all Atari games, along with detailed and quantitative analysis into why FICM fails on specific tasks. For instance, they state in the rebuttal that ""The selection criteria of our environments is determined by the relevance of motions of the foreground and background components (including the controllable agent and the uncontrollable objects) to the performance (i.e., obtainable scores) of the agent."" But it doesn't seem like this was assessed in any quantitative way. Without this understanding, it'd be difficult for an outsider to know which tasks are appropriate to use with this approach. I urge the authors to focus on expanding and quantifying the work they depict in Figure 8, which, although it begins to illuminate why FICM works for some games and not others, is still only a qualitative snapshot of 2 games. I still think this is a very interesting approach and look forward to future versions of this paper."
"abstract | strength | rebuttal_process | suggestion | decision | misc  ==> Summary: <sep> The work is focused on classification of irregularly sampled and unaligned multi-modal time series. Prior work has primarily focused on imputation methods, either end-to-end or otherwise. This paper approaches the problem as a set function mapping between the time-series tuples to the class label. The proposed method is uses a set encoding of a multi-modal time series input, followed by mode-specific encoding of the tuples which are then aggregated in multiple ways prior to classification. An attention mechanism is attached in order for the model to automatically weigh the relevance of tuples for the classification. The model is compared to imputation based baselines on clinical ICU time series classification tasks. The performance mostly appears comparable across baselines but the proposed method has much better run-times. <sep> The paper is for the most part well written, and related work well characterized. The formulation is interesting and clinically relevant as well so the choice of data-sets makes some sense. I have a few concerns about the architecture formulation and lack of clarification and intuition in what appears to be the main contribution of the paper (Sec 3.2 and 3.3) which I will detail below: <sep> a. In the evaluation, I really want to see a decoupling between the ""time encoding step"" and ""attention based aggregation"" on the performance to figure out to isolate different sources of performance improvements. That is can there be a SEFT without time encoding? If not, why not? I encourage more ablation like studies that look at different sources of performance gains and demonstrate them in experiments. <sep> b. The description of Sec 3.3. is really missing key motivation for the choices made around how the attention formulation is designed. For example why does the dot produce include the set elements? What if it doesn't? What is Q supposed to capture? <sep> c. Is a_{j,i} shared across instances? Then irrespective of the number of observations per instance, the jth tuple gets similar weights? If not appropriate indexing will help clarify this. <sep> d. It would be useful to provide how exactly a label is inferred for a *new* test instance. <sep> I have some minor additional feedback (just for presentation and motivation purposes): <sep> 1. Authors make a claim in the introduction which should likely be qualified with a citation - ""Furthermore, even though a decoupled imputation scheme followed by classification is generally more scalable, it may lose information that is relevant for prediction tasks"". How does decoupled imputation imply loss of relevant information? By losing information about which observations are missing and relying on that for prediction? Does this clinically make sense? Or even generally? <sep> 2. In Sec 3.3, you probably mean Wi∈R(im(f′)+|sj|)×d. That is parenthesis are missing? <sep> 3. What are the +- std errors indicating? Is it cross validation error on a held-out test set? <sep> 4. Initially i is indexing samples and by equation (3), (4) i indexes time(?) and in Sec 3.3 i indexes observations? How are observations defined here? is it measurement of specific modality at a specific time instance? Can you clear this in the introduction itself? <sep> ----------------------------------------------------------------------------------------------------------------------------------------------------------------------- <sep> I have read the authors updated draft and response. The experiments section looks much better now. <sep> 1. The overall contribution has less clinical utility in my opinion as generally a patient likely deteriorates over time before an adverse outcome and therefore -- to give the model too much flexibility w.r.t. time ordering doesn't make quite as much sense. This is reflected in the fact that experimental results are not drastically better than other baselines. The authors might be able to show the utility of the method on other time series classification datasets where this is not a limitation of the data itself. However in those settings, it may be a bit hard to beat transformers. Do the authors have a sense of where the benefits of this method really are? <sep> 2. Mortality tasks are generally on the simpler side of clinical prediction problems as well. Nonetheless I think the contribution has some utility to the community. I do encourage the authors to try non--clinical datasets for a comparison <sep> 3. Please have a discussion that includes limitations and to discuss where the benefits of your methods really lie. A clear and thoughtful discussion is currently missing in your conclusions. <sep> With that said, I am updating my score to a 6.","The paper investigates a new approach to classification of irregularly sampled and unaligned multi-modal time series via set function mapping. Experiment results on health care datasets are reported to demonstrate the effectiveness of the proposed approach. <sep> The idea of extending set functions to address missing value in time series is interesting and novel. The paper does a good job at motivating the methods and describing the proposed solution. The authors did a good job at addressing the concerns of the reviewers. <sep> During the discussion, some reviewers are still concerned about the empirical results, which do not match well with published results (even though the authors provided an explanation for it). In addition, the proposed method is only tested on the health care datasets, but the improvement is limited. Therefore it would be worthwhile investigating other time series datasets, and most important answering the important question in terms of what datasets/applications the proposed method works well. <sep> The paper is one step away for being a strong publication. We hope the reviews can help improve the paper for a strong publication in the future."
"rating_summary | decision  ==> Edit after author rebuttal and author additions: <sep> I have updated my score from a weak reject (3) to a weak accept (6). <sep> Justification: <sep> 1. The authors have pointed out that I misunderstood one of their contributions: pointing out that they are demonstrating that the univariate case is an insufficient setting to test causal discovery methods because it can be done without even looking at the conditional distributions (just the marginals). This contribution seems important to orient future work. They have made this more clear in their recent upload and would likely make it even more clear in a camera-ready version. <sep> 2. The authors have made their contribution to the multivariate setting more substantial by adding evaluation on the MOUS-MEG real-world dataset and have better positioned their work relative to others by adding comparisions to multivariate extensions of PNL and CGNN. <sep> ==================================================================================================== <sep> Original Review: <sep> Summary: The authors focus on the problem of inferring whether the causal structure X —> Y or Y —> X. They first consider the case where both X and Y are scalar (univariate) random variables and then consider the case where X and Y are vector-valued (multivariate) random variables. In the scalar case, motivated by the idea that the effect could be less entropic than the cause (due to data processing inequality), they introduce a method based on comparing reconstruction losses of X and Y and show competitive results in Tables 1 and 2. They establish that this method is not sufficient for the multivariate case in Lemma 2 and move to a new method for the multivariate case. They prove identifiability for this new method in for the multivariate case in Section 4.2 and claim state-of-the-art (SOTA) results in Table 3.  <sep> Main contributions: <sep> - Presents a causal discovery technique for the univariate cases that only examines the marginal distributions of X and Y and seems fairly competitive (Tables 1 and 2) <sep> - Extends the post-nonlinear identifiability analysis of Zhang & Hyv¨arinen (2009) from scalars to vectors and proved that their method will actually identify the correct causal direction <sep> - Demonstrates competitive experimental results for both their univariate method <sep> - Claims SOTA results for their multivariate method <sep> Decision: I lean toward rejecting this paper because 1) I have several questions about the univariate case (see below) that would need to be resolved before I lean toward accept, 2) although I am not too familiar with the literature, I believe that this paper may be missing key related work that also uses independence testing for causal discovery (see, e.g., Heinze-Deml et al. (2017)'s Invariant Causal Prediction for Nonlinear Models), and 3) I am not yet convinced that the comparison done in Table 3 is fair and exhaustive.   <sep> Sufficient reason to accept: If the theorems in Section 4.2 are found to checkout, and the SOTA results in Table 3 are found to be fair, exhaustive comparisons to the previous SOTA, their contribution to the multivariate case would seem to be sufficient for acceptance. I believe more discussion between the authors and reviewers is necessary here. <sep>  Questions about univariate case: <sep>  1. The motivation for the first method (entropy decreasing along a Markov chain due the data processing inequality) seems to only be valid when Y := f(X), but not necessarily when Y := f(X) + E. For example, let f be the identity function and E be independent to X. How did you resolve this argument against the intuition?  <sep> 2. Also, I thought the data processing inequality relates mutual information between variables, not necessarily their entropies. Can you make this connection more clear?  <sep> Context for questions 3 and 4: In Section 3.1, you write, ""estimating the entropy of each random variable from its samples does not present a consistent difference between the entropies h(X)  and h(Y). Our method, therefore, computes an alternative complexity score for X and, independently, for Y."" You then go on to link the entropy to the reconstruction error (your method) in Lemma 1 and show competitive results in Tables 1 and 2. <sep> 3. Why do you want to link the reconstruction error to entropy if you found a purely entropy-based method did not work? <sep> 4. Why did the purely entropy-based method not work while your method worked if the two are linked? <sep> Questions about multivariate case: <sep> 5. Are you certain that BivariateFit and ANM are the only models that you should be comparing against for this multivariate setting? <sep> 6. What is CGNN's runtime? Would you be able to compare against CGNN in time for a potential camera-ready version of this paper?","The author response and revisions to the manuscript motivated two reviewers to increase their scores to weak accept. While these revisions increased the quality of the work, the overall assessment is just shy of the threshold for inclusion."
"abstract | rebuttal_process | decision  ==> The paper addresses the problem of using multiple modalities for learning from demonstration. Approaches that take in task or joint space data to learn a policy for replicating that task are numerous. Doing the same with multiple modalities involved, in particular vision, language and motion, has only been recently considered, so this is a timely paper. <sep> The core contribution is pretty well summarised by the architecture in figure 1, which involves a combination of encodings of the words and sentences, images and parameters of a DMP in order to generate movement commands from a high level instruction. <sep> Unless I have missed something in the experimental setup, all of the considered task variations are movement commands of the form <Move> to <Object>. The network setup allows for synonyms of two kinds, so <Move> can be replaced by numerous verbal synonyms such as advance and go, and the object can be specified in terms of shapes, colors and so on, but otherwise this is the only specification of the task. This has been addressed in the recent literature using neural network architectures similar to the one being proposed here, e.g., see the following papers. These papers already solve the proposed problem and provide similar explanations. It would be helpful to see comparative discussion with respect to those methods and a clear statement of novelty with respect to such prior work: <sep> [R1] M. Burke, S. Penkov, S. Ramamoorthy, From explanation to synthesis: Compositional program induction for learning from demonstration, Robotics: Science and Systems (R:SS), 2019. <sep> [R2] Y. Hristov, D. Angelov, A.Lascarides, M. Burke, S. Ramamoorthy, Disentangled Relational Representations for Explaining and Learning from Demonstration, Conference on Robot Learning (CoRL), 2019. <sep> An interesting feature in R2 that the authors do not explicitly address here is the issue of relational specifications in the language, e.g., in addition to saying ""move to the red bowl"", we may also wish to say ""place on top of red block"". In the way that MPN is currently set up to map from the language input directly to hyperparameters of the DMP, and considering the embedding structure, it is not clear if MPN is capable of handling such specifications. If so, the claim of generalisation on the language input should be stated more clearly. <sep> The ablation study is setup somewhat differently than what I would have expected. The authors consider the effect of changing the training set size and if the language input includes synonyms or not. Those two aspects seem to produce the expected results. It would also be interesting to see an ablation study in the sense of replacing or removing aspects of the architecture to see its relative effect on the overall model performance. So, for instance, if one did not have a DMP with the hyperparameters being estimated by a network and instead had a more straightforward encoding of where to move to - does it make a difference and how much? Likewise, how much performance benefit, if any, is being derived from an uninterpreted image I being combined as described in the embedding as opposed to an alternative that detects an objects and combines that position differently. The paper would have been stronger if such architectural choices were better justified and also demonstrated in the experiments.","The present paper addresses the problem of imitation learning in multi-modal settings, combining vision, language and motion. The proposed approach learns an abstract task representation, and the goal is to use this as a basis for generalization. This paper was subject to considerable discussion, and the authors clarified several issues that reviewers raised during the rebuttal phase. Overall, the empirical study presented in the paper remains limited, for example in terms of ablations (which components of the proposed model have what effect on performance) and placement in the context of prior work. As a result, the depth of insights is not yet sufficient for publication."
"abstract | weakness | decision  ==> The paper proposes an extension of learning graph structure and GNN concurrently, by considering that real-world graphs are often noisy and incomplete. The idea of optimizing the intrinsic graph structure iteratively for down-stream prediction tasks is interesting. Experimental results demonstrate the effectiveness of proposed method. <sep> Strengths: <sep> 1）the paper proposes a learnable similarity metric function and a graph regularization for learning an optimal graph structure for prediction. <sep> 2）Besides raw node features, the paper attempts to optimize graph structures via learned node embeddings in an iterative manner. <sep> 3）The paper is easy to read, and experiments show that the proposed method performs well. <sep> Weaknesses: <sep> 1）Compared with LDS [1], this work seems to overlook the bi-level optimization problem for learning model parameters based on the optimal graph structure. The reason behind this method is expected. <sep> 2）Although the paper claims that the dependence of raw node features for learning graph structure has been weakened,  empirical analysis on this point is not given. The feature matrices in experiments are not strictly independent with graph structures. <sep> 3) As shown in Appendix B, too many hyper-parameters are involved. I conjecture it will be difficult to reproduce the experimental results. <sep> 4) Eqs.(2), (3) and (10) are problematic. Node embeddings Z should be included in them. Eq.(10) does not have theoretical proof. According to Eq.(10), the method cannot handle graphs with noisy edges. In experiments, there are edge deletions, but no edge addings. Experiments with attacked graph are expected. <sep> 5) Although this method is claimed efficient, it is indeed slower than the classic GNNs due to the iterative operation. The details of training time comparison between this method and GNNs such as GCN and GAT will be helpful. I was wondering why this method is faster than LDS. Is it due to removing the bi-level optimization problem ? <sep> 6) Although the method can handle inductive training, it is hardly scale to big networks. Pubmed is an open citation network with around 20,000 nodes similar to Cora and Citeseer. Those three datasets are popularly used in GNNs as testbed. However, Pubmed is not used in this work. I conjecture that the new method cannot handle such a big dataset efficiently. <sep> Overall, this proposed method is well motivated, but the technical novelty is limited.","The submission proposes a method for learning a graph structure and node embeddings through an iterative process. Smoothness and sparsity are both optimized in this approach. The iterative method has a stopping mechanism based on distance from a ground truth. <sep> The concerns of the reviewers were about scalability and novelty. Since other methods have used the same costs for optimization, as well as other aspects of this approach, there is little contribution other than the iterative process. The improvement over LDS, the most similar approach, is relatively minor. <sep> Although the paper is promising, more work is required to establish the contributions of the method. Recommendation is for rejection."
"abstract | weakness  ==> This paper proposes a method to predict future trajectories by modeling partial and full occlusions. Although it is well-written and the topic sounds interesting, I failed to catch why this approach is required for this setting. So, to strengthen the message of this paper, I listed a couple of suggestions and comments below (from the most important to the least important): <sep> 1. It is a bit hard to catch how this model handles ""diversity."" Specifically, when predicting the futures, it should be able to generate stochastic outputs. However, I failed to find how diverse the output of the model is. If the output is not that stochastic, then it would be tough to believe that the model can ""predict"" the future; instead, it may ""extrapolate"" the current condition only. To reassure such concerns, I recommend reporting how diverse your output is. (One easy way is to report the variance of the predicted center mass values between multiple samples while reporting the l2 distance.) <sep> 2. For the future prediction task, it would be much better if it is compared with various state-of-the-art future prediction approaches [1, 2, 3, 4, 5, 6]. For some of the models, it could not be able to compare directly with this approach (e.g., lack of 'center of mass' information). However, it would be still okay once it is compared with other state-of-the-art results without feeding some 3D information (e.g., provide projected 2D video as an input). By doing so, I believe the readers can easily catch (1) why it is better to predict physical interaction in 3D space (instead of directly predicting from a 2D space), and (2) also why predicting occlusion is essential in this problem setting. <sep> 3. Minor comments: <sep> (a) It is a bit hard to catch how the author computes the ""aggregate pixel reconstruction error"" in Table S1. I recommend adding an equation number there to make it clear. <sep> (b) There are a couple of missing references: the last sentence on page 4, the first paragraph in Supplementary, the last sentence in Supplementary page 3, etc. <sep> (c) \\citep is often misused. Please replace some inappropriate \\citep with \\citet. <sep> (d) Please check the format of the reference, as well; currently, it has various styles even for the same source/conference. <sep> ------------------------------------------------------------------ <sep> [Some comments based on the authors' rebuttal] <sep> I thank the authors for their thorough comments and detailed explanations for each question. I carefully read the whole (not just my part), but it didn't change my mind; it would be much better if the claim comes with a more directly comparable result. <sep> Some additional comments: <sep> Q1-comment) I think the limitation of ""learning to extrapolate""-style video prediction approach is partially presented in Reviewer #2's claim as well. Therefore, in this context, I recommend the author to show a better result to reassure the reader's concern. <sep> Q2-comment) I at least strongly recommend to add more experiments with other baselines, rather than relying mainly on the original model of the dataset. Although the input condition of a model could be different, I at least do believe that it will help the readers to catch the benefit of your setting more clearly. <sep> I hope this review phase would make your paper more powerful. <sep> [1] Liang et al., Dual Motion GAN for Future-Flow Embedded Video Prediction, in ICCV, 2017 <sep> [2] Denton and Fergus, Stochastic Video Generation with a Learned Prior, in ICML, 2018 <sep> [3] Wichers et al., Hierarchical Long-term Video Prediction without Supervision, in ICML, 2018 <sep> [4] Wang et al., Video-to-Video Synthesis, in NeurIPS, 2018 <sep> [5] Heish et al., Learning to Decompose and Disentangle Representations for Video Prediction, in NeurIPS, 2018 <sep> [6] Minderer et al., Unsupervised Learning of Object Structure and Dynamics from Videos, in NeurIPS, 2019","The paper studies the problem of modeling inter-object dynamics with occlusions. It provides proof-of-concept demonstrations on toy 3d scenes that occlusions can be handled by structured representations using object-level segmentation masks and depth information. However, the technical novelty is not high and the requirement of such structured information seems impractical real-world applications which thus limits the significance of the proposed method."
"abstract | strength | rating_summary | rebuttal_process | rating_summary | decision  ==> This submission studies losses at local minima of a set of neural networks trained on an XOR-like synthetic dataset, finds that local minima are of varying quality, and proposes a network pruning method to find better local minima. The pruning method is evaluated on XOR-like datasets as well as real-world datasets. <sep> The use of an XOR-like dataset to study loss landscapes is interesting, making for a controlled and analyzable setting to carry out the study. The way the authors set it up, the XOR-like problem involves nuisance variables that naturally introduce suboptimal local minima into the loss landscape (this is my observation as a reviewer -- I am not sure if the authors were aware of this). I am unsure if Section 2 of the paper was intended as a core contribution or as a motivation for the pruning algorithm proposed in Section 3. Given the set-up's simplicity, a short theoretical argument (maybe even a theorem) about the quality and number of local minima one would expect to find could have been more concise and compelling than the empirical analysis from the paper. The findings from Section 2 may not be surprising enough to warrant two full pages. <sep> Section 3 proposes a network pruning method to find better local minima. The authors cite a paper by Adrian Barbu as the inspiration for their pruning algorithm with annealing, and use it ""to improve the capability of NNs to find a deep local minimum even when there are irrelevant variables"". The cited paper by Barbu as well as https://arxiv.org/pdf/1805.01930.pdf (also by Adrian Barbu, not cited, maybe because it appeared) explore feature selection and regularization with (nearly) the same annealed pruning algorithm in some detail. I would be grateful if the authors could highlight the differences between their work and Barbu's. <sep> I vote to ""weak reject"" this paper. The paper discusses interesting ideas, but other *CONF* submissions present deeper and more novel material, and there appears to be some (unintentional, I believe) overlap with already-published work. I recommend that the authors cite and discuss https://arxiv.org/pdf/1805.01930.pdf , and possibly submit the paper at a less competitive conference. <sep> Further comments / questions / advice <sep> ================================= <sep> - It would be helpful if the authors made more clear what they consider the key contributions of their paper. If contributions build directly on earlier work, it's helpful to highlight the differences. <sep> - Section 4.2 states that datasets were ""carefully selected"" in what sounds like a case-by-case basis, probably with the goal of finding data sets on which CPNA outperforms networks trained with vanilla gradient descent methods. This process would have selection bias and surface data sets on which CPNA outperforms. I could be grateful if the authors could clarify if this was indeed the process, or if a less biased criterion was used. For example, one could have chosen data sets on which a 1-layer fully connected neural network achieves between 50% and 90% F-1. <sep> - A reader of the paper might wonder for what data sets they should use CPNA in order to train network that achieves low out-of-sample loss. I could be grateful if the authors could comment on this. Following up on the previous point: it would be great the authors could include data sets where CPNA does not outperform. <sep> - Could the authors include information on how long training takes for the experiments from Table 3? <sep> - https://openreview.net/pdf?id=HkghWScuoQ should probably be cited <sep> - https://arxiv.org/pdf/1805.01930.pdf should definitely be cited","This paper provides empirical evidence on synthetic examples with a focus on understanding the relationship between the number of ""good"" local minima and number of irrelevant features. The reviewers find the problem discussed to be important. One of the reviewers has pointed out that the paper does not present deep insights and is more suitable for workshops. The authors did not provide a rebuttal, and it appears that the reviewers opinion has not changed. <sep> The current score is clearly not sufficient to accept this paper in its current form. Due to this reason, I recommend to reject this paper."
"abstract | strength | weakness | misc  ==>  ==> On the basis of existing topic modelling approaches, the authors apply a transfer learning approach to incorporate additional knowledge to topic models, using both word embeddings and topic models. The underlying idea is that topic models contain a global view that differs on a thematic level, while word embeddings contain a local, immediate contextual view. The combination of both local and global view transfer to enhance a topic model is the main contribution of this paper, especially when using multiple sources (therefore the title: multi-source multi-view transfer). <sep> Given a document collection, DocNADE is used to generate the topic-word matrix. In the local view transfer step, the pre-trained WordPool is used, from which knowledge is transferred on the target document. The global view transfer is done by transferring knowledge from the pre-trained TopicPool to the target. As described in Algorithm 1 in the paper, both Word- and TopicPool are jointly used in the transfer learning process. <sep> For evaluation, three different measures are taken into account: Perplexity, Topic Coherence and Precision (Information Retrieval). In comparison to a DocNADE only approach, all values are better in the settings that use the transfer learning approach. Compared to DocNADE + word embeddings, the results are competitive as well. In both experiments, the multi-source setting evaluates best overall. <sep> In conclusion, the paper shows that exploiting multiple sources and views in transfer learning leads to an overall improvement in the given tasks. The main contribution is the usage topic models in a transfer learning framework. Additionally the use of multi-source word embeddings is novel too, especially in the joint setting with the topic model transfer. The paper shows how the DocNADE approach is enhanced to make use of both local and global view transfer and how this enhancement leads to improved performance on various related tasks. <sep> Still, the overall contribution is mostly in combining existing methods and can be judged as rather incremental. <sep> Minor note: A small mistake has been found in Table 5. The best perplexity value in the first column is not the bold 638, but the 630 in the local-view transfer setting. <sep> Edit after rebuttal: In my review I did not value the contribution of the transfer learning approach enough. So, when also considering the extensive evaluation I am now leaning towards accept.","This paper presents a transfer learning framework in neural topic modeling. Authors claim and reviewers agree that this view of transfer learning in the realm of topic modeling is novel. <sep> However, after much deliberation and discussion among the reviewers, we conclude that this paper does not contribute sufficient novelty in terms of the method. Also, reviewers find the experiments and results not sufficiently convincing. <sep> I sincerely thank the authors for submitting to *CONF* and hope to see a revised paper in a future venue."
"abstract | weakness  ==>  ==> The authors study continual, lifelong learning. They suggest a new algorithm, named Adaptive Online Planning (AOP) that combines model-based planning with model-free learning. AOP decides how much additional planning is needed based on the uncertainty of the model-free learner and the performance of the planner. Experiments are carried out on three tasks, i.e. Hopper, Ant and a Maze. <sep> This paper should be rejected. The main reason is that the experiments were only performed for 3 different seeds and are therefore not statistically relevant (see Henderson et al. ""Deep reinforcement learning that matters."" Thirty-Second AAAI Conference on Artificial Intelligence. 2018.). <sep> Besides the issue of significance of the results section, there are other concerns. Some of them are: <sep> - Page 2: 'The dynamics model is updated immediately at world changes.' - Is this a reasonable assumption? Where does an accurate model come from? Given a perfect model, it is not surprising that a learner that is combined with such a model achieves a superior performance. <sep> - Although the authors state that the 'ability to perform well in old tasks (backward transfer)' is important, they don't explicitly show their algorithm to achieve this goal. Backwards transfer might be included into the experimental section, but I could not find a statement that addresses this explicitly. <sep> - I would like the authors to crisply define their use of the word 'online learning'. Does online learning simply mean to process each sample as it is available or does the term include real-time? <sep> - How is \\sigma_{thres} chosen? What is the influence of this parameter? <sep> - The statement that 'AOP uses only 0 - 25% of the number of timesteps as MPC-8, but achieves generally comparable or stronger performance.' is wrong (see Fig 4, d and e). This statement is especially difficult, as results are only averaged over 3 runs. <sep> There are furthermore a few minor concerns: <sep> - the interval for \\gamma should exclude 1 in this setting, as the return would otherwise be unbounded. <sep> - In the background section, the authors confuse the definition of the return with reward. <sep> - the term 'deep exploration' is used but not defined <sep> - There are two figures between the subsection header for 4.4 and the text - this is highly confusing","A new setting for lifelong learning is analyzed and a new method, AOP, is introduced, which combines a model-free with a model-based approach to deal with this setting. <sep> While the idea is interesting, the main claims are insufficiently demonstrated. A theoretical justification is missing, and the experiments alone are not rigorous enough to draw strong conclusions. The three environments are rather simplistic and there are concerns about the statistical significance, for at least some of the experiments."
"abstract | weakness | decision  ==> -- This paper seeks to combine several ideas together to propose an approach for image classification based continual learning tasks. In this effort, the paper combines previously published approaches from generative modeling with VAEs, mutual information regularization and domain adaptation. <sep> I am a making a recommendation for reject for this paper with the main reason being that I believe the primary derivations for their method appear flawed. <sep> --In the main section describing the approach (Section 4), the authors start with a claim that Equation 1 and 2 are equal; I don't believe 1 and 2 are equal. <sep> --In Section 4.1, it appears that they are instead making a claim about Equation 2 being a bound for equation 1; but even this derivation appears to have a problem. The following is the concern: <sep> --In the second line of Equation 5, the KL term appears to be measuring a distance between distributions on two different variables; z|c and c|z. If one were to interpret the second one as the unnormalized distribution on z defined via the likelihood for c given z; even this has an issue because then the expression for KL where we plug the unnormalized density in place of the normalized need not be positive which is something they need to derive their bound. <sep> --Another issue is that the regularization lambda should apply to both the terms in the bound but in Equation (7) only appears selectively for one of the two terms. <sep> It is also not clear how the loss function proposed differs from that of the CDVAE, etc.  If the novelty is in applying to continual learning and new datasets, it is not clear that this is sufficient. <sep> Additional feedback for authors (not part of the main decision reasoning): <sep> - What is dt in Algorithm 1 description? <sep> Figure 1: <sep> -typo ""implmented"" <sep> -What's the 3d plot supposed to represent? <sep> Doesn't the classification loss have a dependency on the input condition? <sep> --What does a ""heavy classifier"" imply concretely? <sep> --""Redundant weights"" seems like not a very strong constraint especially for a small cardinality label space (like 10, in the case of this paper). <sep> --The notation for the proposed parameters theta, theta', phi, phi' are not consistent with the notation in the intro section, where phi was used for the encoder and theta for the decoder. In later sections they use theta and theta' for encoder/decoder resp. <sep> -- ""When the encoder and decoder networks are sufficiently complex, it is enough to implement each the prior and classification network as one fully-connected layer"" → what do the authors mean ""when … networks are sufficiently complex"" or do they actually mean when the ""when the problem is simple enough""?","The paper presents a method for continual learning with a variant of VAE. The proposed approach is reasonable but technical contribution is quite incremental. The experimental results are limited to comparisons among methods with generative replay, and experimental results on more complex datasets (e.g., CIFAR 100, CUB, ImageNet) are missing. Overall, the contribution of the work in the current form seems insufficient for acceptance at *CONF*."
"abstract | misc | weakness | decision  ==> This paper proposes an actor-critic method that tries to aid learning good policies via learning a good representation of the state space (via a latent variable model). In actor-critic methods, the critic is learnt to evaluate policies in the latent space, which further helps with efficient policy optimization. The proposed method is evaluated on image-based control tasks, with baseline evaluations against both model-based and model-free methods in terms of sample efficiency.  <sep> - The key argument is that learning policies in the latent space is more efficient, as it is possible to learn good representations in the latent space. There are quite a few recent works (e.g DeepMDP, Gelada et al., 2019;  Dadashi et al., 2019) that talks about representation learning in RL, and yet the paper makes no relations or references to previous works. I find it surprising that none of the past related works are mentioned in the paper.  <sep> - I find the arguments on solving a POMDP instead of a MDP a bit vague in this context. I understand that the goal is to solve image based control tasks - for which learning good representations via a latent variable model might be useful, but it does not explicitly require references to a POMDP? In most ALE tasks, we have pixel based observations too, which makes the ALE environments a POMDP in some sense, but we use approximations to it to make it equivalent to a MDP with sufficient history. The arguments on POMDP seems rather an additional mention, with no necessary significance to it? <sep> - The paper mentions solving RL in the learned latent space, which is empirically proposed to be a good approach without theoretical justifications. There are several recent works that tries to understand the representation learning in RL problem from a theoretical perspective too - it would be useful to see where this approach stands in light of those theoretical results? Otherwise, the contribution seems rather limited : solving RL in latent space is useful, but there are no justifications to it? Why should this approach even be adapted or what is the significance of it? <sep> - The proposed actor-critic method in the latent space is built on top of Soft Actor-Critic (SAC). I understand this is a design/implementation approach building from previous works - but it would have been useful to add more context as to what it means to learn a critic in the latent space. If the critic evaluates a policy in the latent space - then is this a good policy evaluation for actor-critic itself? Why or why not? I do not understand why the critic evaluation in the latent space is even a good approach?  <sep> - My first impression was that the paper proposes a separate auxilliary objective for learning good representations based on which actor-critic algorithms can be made more efficient. However, this does not seem to be the case directly? Following on previous point - I find the argument of solving a critic in the latent space rather vague.  <sep> - The sequential latent variable model proposed is based on existing literature. This can be any latent variable model (e.g VAEs), but I understand, as mentioned in the paper, the design choice of using sequential models to capture the temporal aspect.  <sep> - The proposed algorithm is in fact a combination of SAC and sequential latent variable models, both of which are well-known in the literature. The SLAC algorithm combines these to solve image-based control tasks. As per equation 10, which is the regular policy optimization objective with max entropy - the only difference is that the critic is evaluated in the latent space. This appears to me as more of an engineeering choice, and experimentally one that perhaps give good results - but the lack of justifications of why equation 10 is even the right objective to solve makes the paper rather less appealing.  <sep> - I think overall the contribution of the paper is rather limited. It is more of an experimental design and engineering approach that combines previous known techniques. The paper mentions learning good representations for RL, without any references or justifications - and it appears that overall there are bold claims made in the paper but it lacks significant scientific contribution.  <sep> - Experimental evaluations are made on image based control tasks. Experimental results are compared to few baselines - but it is not clear whether these are even the right baselines. For example, it would have been good to include analysis of the proposed model with different latent variable models (including VAE) to perhaps justify the choice of the latent variable model. Results in figure 5 appear a bit concerning to me - these are mostly the standard Mujoco tasks from the OpenAI suite. Are these all image based benchmarks too, or the standard baselines? It is not clear from the text. Assuming they are standard baselines, the comparisons made are rather unfair (for example : SAC and MDP performs much better on tasks like HalfCheetah-v2). Why are the baselines performing so poorly in the results? <sep> - Overall, I think the paper needs more work in terms of writing and justifying the choice of the approach. There are significant references missing in the paper. Most importantly, there are quite a few claims made in the paper which are not properly justified, that makes the overall contribution and novelty of the paper rather limited. I would tend for a rejection of this paper, as it requires more work - both in terms of theoretical justifications (including references) and experimetnal ablation studies and more simpler benchmarks explaining the choice of the approach.","An actor-critic method is introduced that explicitly aims to learn a good representation using a stochastic latent variable model. There is disagreement among the reviewers regarding the significance of this paper. Two of the three reviewers argue that several strong claims made in the paper that are not properly backed up by evidence. In particular, it is not sufficiently clear to what degree the shown performance improvement is due to the stochastic nature of the model used, one of the key points of the paper. I recommend that the authors provide more empirical evidence to back up their claims and then resubmit."
"abstract | rebuttal_process | weakness | decision  ==>  ==> The manuscript proposes a method for model explanation and two metrics for the evaluation of methods for model explanation based on robustness analysis. More specifically, two complementary, yet very related, criteria are proposed: i) robustness to perturbations on irrelevant features and ii) robustness to perturbations in relevant features. Moreover, different from existing works which defined the perturbation values following different somewhat-fixed procedures, the proposed method aims at allowing perturbations in any directions. <sep> A greedy algorithm optimizing these criteria is proposed in order to produce a method able to highlight important features of the input and justify/explain model predictions. <sep> In addition, the proposed robustness criteria are used as metrics to assess the performance of methods for model explanation. <sep> Experiments on models addressing image classification and text classification, shows the performance of the proposed method w.r.t. existing work. <sep> The manuscript has a good flow, and its content is easy to follow. The proposed method is sound, well motivated, and very well founded. The formal presentation of the proposed method is good. I appreciate the fact that evaluation covers different modalities of data, i.e. text and images. <sep> My main criticism over the manuscript is the following: <sep> In Sec. 3.3, when describing the first criterion, it is stated that the size |S_r|, i.e, the amount of anchors, could be defined by the used. In my opinion, this may not be applicable since in theory the amount of relevant/irrelevant features is unknown before hand. In that case the proposed AUC-based method seems more adequate. Could you comment on this? <sep> In  Sec. 3, a pre-defined size K is introduced. Later in Sec. 3.1 it is stated that the greedy algorithm uses this size as a stopping criterion for the optimization of the proposed robustness criteria. Could you indicate how this size K is defined in practice? Is there a principled way to define it? What is the effect of this parameter on the performance of the proposed method? An ablation study focused on this parameter would provide further insights into the inner workings of the proposed method and would improve the manuscript. <sep> In Sec.4 it is stated that only 50 random examples are considered when reporting results. When comparing the performance across the different methods, are these random 50 examples fixed or always re-sampled? Also, given the size of the considered datasets, where the number of images in their test sets is in the order of the thousands, it is hard to grasp how representative are the reported results? <sep> In the same paragraph discussed above, it is mentioned that the GRAD method performs competitively on the proposed criteria. It might be interesting to further positioning the proposed method w.r.t. GRAD. Given the comparable performance achieved by GRAD and its relative simplicity, it would be hard to motivate why not choose GRAD instead of the proposed method? Could you provide some discussion on this? <sep> In Sec.4 (pag. 6) it is stated the the proposed regression-greedy method outperforms other methods in these criteria. In my opinion this trend shouldn't be surprising given the fact that the proposed method is specifically optimized on such criteria as it is clearly stated by the title of Sec.3. <sep> Fig.3 and Fig.4 display binary images indicating the top-n features selected by different methods. Perhaps it would be more informative to have a heatmap highlighting/grading the entire input space. This may throw more light on the performance of the compared methods. <sep> In Adebayo et al., NIPS'18 (and very related efforts), there are presented a set of sanity checks to be applied to explanation methods to ensure their predictions are relate to the class and model being predicted. Could you provide any indication on whether the proposed method passes these checks? <sep> In Sec.4 (visualization) it is stated that the proposed method effectively highlights crucial positive pixels as well as pertinent negative pixels. A similar capability has also being reported earlier in Samek et al., Trans NNLS'16 and Oramas et al. *CONF*'19. Since the visualization analysis (discussed in pag.6) focuses exclusively on this capability. There should be a comparison between the proposed method and the two mentioned works.","The paper proposes an approach for finding an explainable subset of features by choosing features that simultaneously are: most important for the prediction task, and robust against adversarial perturbation. The paper provides quantitative and qualitative evidence that the proposed method works. <sep> The paper had two reviews (both borderline), and the while the authors responded enthusiastically, the reviewers did not further engage during the discussion period. <sep> The paper has a promising idea, but the presentation and execution in its current form have been found to be not convincing by the reviewers. Unfortunately, the submission as it stands is not yet suitable for *CONF*."
"abstract | strength | weakness | rating_summary  ==> In this paper, the authors proposed an automatic piano fingering algorithm, that accepts YouTube videos and corresponding MIDI files and outputs fingering prediction for each note. The claimed contribution is two-fold: First, they proposed the algorithm, and second, they claim that the algorithm can be used to automatically generate large datasets for piano fingering problems. The motivation is clearly stated and convincing. The overall algorithm is mainly described. <sep> However, I would like to reject this paper. Major issues: <sep> * Some key information is missing in Section 3.6, which is the only section that shows technical details: What is X_{n_k}? How is that related to the estimated finger poses? What is the function f in the definition of function g? (Also, it would be helpful to label the equations for clarification.) Are you doing Bayesian inference? With the key information missing, it is hard to fully understand the remaining technical details in this section. <sep> * Their experimental results cannot properly support their claims. In Section 4.2, the authors try to show the strength of their proposed piano fingering algorithm by comparing their automatically annotated dataset APFD with an existing manually annotated dataset PIG. The authors showed the evaluation results of models trained and fine-tuned with different datasets. However, this is not an acceptable comparison for me, due to several reasons. <sep> First, in order to show the strength of automatic piano fingering prediction, it is much better to directly run the prediction algorithm on datasets with known labels. According to the related work section, there is at least one existing work by Takegawa et al. that uses videos and MIDI files to detect piano fingering. Can you compare your algorithm with theirs? <sep> Second, it is essentially unreliable to compare two datasets by comparing the performance of two prediction models, as there are too many implementation details that are almost impossible to control. <sep> Third, it is not clear how we should compare the testing errors in Table 2. Yes, a model initially trained on PIG and fine-tuned on APFD may perform better than a model trained merely on APFD, but does that suggest anything (and the advantage is just 0.4%)? Similarly, the experimental result that an MLP model initially trained on APFD then fine-tuned with PIG works better than an HMM model that is trained with PIG data alone cannot prove anything. There are too many possible reasons that may lead to this experimental result. <sep> * How is this method more attractable than the existing ones? There are neither experimental comparisons nor high-level justifications of why the existing algorithms are not applicable to the given scenario. In Section 2, although the authors described a good number of existing work on piano-fingering and their drawbacks, they failed to point out the strength of their paper as a comparison. As a result, the strength of this paper is still unclear after reading this section. How does this paper avoid the drawbacks of these previous papers? <sep> * The writing of this paper needs to be greatly improved. It takes a lot of effort to literally understand this paper: There may be missing parts, misplaced clauses, and broken logic between sentences. I have listed several examples in the minor issues part. <sep> Minor issues: <sep> * In the first paragraph of Section 1: The sentence before 'In practice ...' is incomplete. <sep> * In the last paragraph of Section 1: Missing brackets for \\textsection 3.3 and \\textsection 3.4. Also, 'on A new dataset we introduce' should be 'on THE new dataset we introduce'. <sep> * On page 3, the sentence 'In this work, we continue the transition of search-based methods that optimize a set of constraints with learning methods that ...' is not making sense to me. Do you mean that your work is an extension of search-based methods, or do you mean that your work is not a search-based method? Also, are you optimizing a set of constraints, or optimizing with a set of constraints? <sep> * On page 3, the last sentence in Section 2: '... and adapt their model to compare TO our ...' should be '... and adapt their model to compare WITH our ...'. The last part of this sentence is also a bit confusing: How do you compare a model with a dataset? <sep> * On page 4, the paragraph starting with 'MIDI files': The first two sentences are almost the same; the period between them is missing. I guess one of them should be deleted. The following sentences in this paragraph are also subject to grammatical errors. For example, the sentence 'It consists of a sequence of events ... to carry out, WHEN, and allows for ...' is not a complete sentence. 'We only use videos that come along with a MIDI file' -> 'We only use videos that come along with MIDI files'. <sep> * On page 5, last paragraph in Section 3.3: 'highest probability defections' -> 'highest probability detections'. <sep> * The last paragraph on Page 5: 'Using off-the-shelve ...' -> 'Use off-the-shelf ...'. <sep> * In Section 4.2.1, the corresponding result is Table 2, instead of Table 1.",The paper shows an automatic piano fingering algorithm. The idea is good. But the reviewers find that the novelty is limited and it is an incremental work. All the reivewers agree to reject.
"abstract | weakness  ==>  ==> Motivated by recent development of attack/defense methods addressing the vulnerability of deep CNN classifiers for images, this paper proposes an attack framework for adversarial text generation, in which an autoencoder is employed to map discrete text to a high-dimensional continuous latent space, standard iterative optimization based attack method is performed in the continuous latent space to generate adversarial latent embeddings, and a decoder generates adversarial text from the adversarial embeddings.  Different generation strategies of perturbing latent embeddings at sentence level or masked word level are both explored. Adversarial text generation can take either a form of appending an adversarial sentence or a form of scattering adversarial words into different specified positions. Experiments on both sentiment classification and question answering show that the proposed attack framework outperforms some baselines. Human evaluations are also conducted. <sep> Pros: <sep> This paper is well-written overall. Extensive experiments are performed. <sep> Many human studies comparing different adversarial text generation strategies and evaluating adversarial text for sentiment classification/question answering are conducted. <sep> Cons: <sep> 1) Although the studied problem in this paper is interesting, the technical innovation is very limited. All the techniques are standard or known. <sep> 2) There are two major issues: lacking a rigorous metric of human unnoticeability and lacking justification of the advantage of the tree-based autoencoder. I think the first issue is a major problem that renders all the claims in this paper questionable. The metrics used to define adversarial images for deep CNN classifiers are indeed valid and produce unnoticeable images for human observers. But in this paper, the adversarial attack is performed in the latent embedding space, and there is no explicit constraint enforced on the output text. It's unconvincing that this approach will generate adversarial text that seems negligible to humans. Therefore, the studied problem in this paper has a completely different nature from the one for CNN image classifiers and it is hard to convince readers that the proposed  framework generates adversarial text legitimate to human readers. <sep> 3) It is unclear why tree-structured LSTM instead of a standard LSTM/GRU should be chosen in this framework for adversarial text generation. If this architecture is preferred, sufficient ablation studies should be conducted. <sep> 4) In section 3.3, the description about adversarial attacks at word level is unclear. More detailed loss function and algorithms along with equations should be provided. <sep> 5) In section 5.2, it is unclear that the majority answers on the adversarial text will, respectively, match the majority answers on the original text. Moreover, it seems that there is a large performance drop from original text to adversarial text. Therefore, it is valid to argue that whether the proposed framework can generate legitimate adversarial text to human readers or not. <sep> 6) It's better to include many examples of generated adversarial text in the appendix. <sep> 7) Missing training details: It is unclear how the model architectures are chosen, and learning rate, optimizer, training epochs etc. are also missing. All these training details should be included in the appendix. <sep> 8) Minor: Figure 1: ""Append an initial sentence..."",  section 3: ""map discrete text into a high dimensional..."",  section 3.2.2: ""Different from attacking sentiment analysis..."" .... <sep> In summary, the research direction of adversarial text generation studied in this paper is interesting and promising. However, some technical details are questionable, and the produced results without rigorous metrics seem to be unconvincing.","This paper proposes a method for generating text examples that are adversarial against a known text model, based on modifying the internal representations of a tree-structured autoencoder. <sep> I side with the two more confident reviewers, and argue that this paper doesn't offer sufficient evidence that this method is useful in the proposed setting. I'm particularly swayed by R1, who raises some fairly basic concerns about the value of adversarial example work of this kind, where the generated examples look unnatural in most cases, and where label preservation is not guaranteed. I'm also concerned by the fact, which came up repeatedly in the reviews, that the authors claimed that using a tree-structured decoder encourages the model to generate grammatical sentences—I see no reason why this should be the case in the setting described here, and the paper doesn't seem to offer evidence to back this up."
"abstract | strength | weakness | rebuttal_process | decision  ==>  ==> This paper introduces and studies modifications to gating mechanisms in RNNs. <sep> The first modification is uniform gate initialization. Here the biases in the forget and input gate bias are sampled such that after the application of the sigmoid the values are in the range (1/d, 1-1/d) where d is the dimensionality of the hidden space for the bias. The second modification is the introduction of a refine gating mechanism with a view to allow for gradients to flow when the forget gates f_t in the LSTM updates are near saturation. The idea is to create an effective gate g = f_t +/- phi(f_t, x_t). The paper proposes using phi (f_t, x_t) = f_t(1-f_t) * (2*r_t-1) where (r_t is between 0 or 1). The effect of such a change is that g_t can reach values of 0.99 when the value of f_t is 0.9 allowing gradients to flow more freely through the parameters that constitute the forget gate. Overall the change corresponds to improving gradient flow for the forget gate by interpolating between f_t^2 and (1-f_t)^2. i.e. the authors note that the result of these changes is that it corresponds to sampling biases from a heavier tailed distribution while the refine gate (by allowing the forget gate to reach values close to 0 and 1), allows for capturing information on a much longer time scale. <sep> The paper studies various combinations of the two changes proposed to gating architectures. Other baselines include a vanilla LSTM, a Chrono initialized LSTM, and an ordered Neuron LSTM. The models are trained on several synthetic and real world tasks. On the copy and add tasks, the LSTMs that contain the refine gate converge the fastest. A similar story is observed on the task of pixel by pixel image classification. The refine gate was also adapted to memory architectures such as the DNC and RMA where it was found to improve performance on two different tasks. <sep> Overall, the paper is written well, I like the (second) idea of the refine gate and the contributions are explained in an accessible manner. While I'm not entirely convinced about the proposed initialization scheme but across the many different tasks tried, the use of the refine gate does appear to give performance improvements that lead me to conclude that this aspect of the work is a solid contribution to the literature. <sep> Questions and comments: <sep> * This manuscript already quite long and has several formatting issues. Several of the figures are unreadable when printed. For example, every piece of text on Figure 2(d) is unreadable on paper. Figure 3 and 5 are difficult to read; they contain too many alternatives with a colour scheme that makes it difficult to distinguish between them -- consider displaying a subset of the options via a plot and using a table to display (# steps to convergence) as a metric instead. It also appears as if the caption for Table 6 is deleted? <sep> * I think that for this approach to work, two conditions need to be satisfied (a) there must be foreseeable improvements in the use of a forget gate that can reach values close to 0/1 for the task at hand and (b) r_t needs to function well despite not being too close to 0 or 1 (lest its parameters suffer from gradient flow issues). <sep> * Was there any visualizations done on whether (a) happened? i.e. for the URLSTMs that performed well, were the values of the forget gate closer to 0/1 than the baselines? <sep> * What were typical values of r_t, did the models need the refine gate to reach values close to 0 or 1 for the overall approach to work?",This submission proposes a new gating mechanism to improve gradient information propagation during back-propagation when training recurrent neural networks. <sep> Strengths: <sep> -The problem is interesting and important. <sep> -The proposed method is novel. <sep> Weaknesses: <sep> -The justification and motivation of the UGI mechanism was not clear and/or convincing. <sep> -The experimental validation is sometimes hard to interpret and the proposed improvements of the gating mechanism are not well-reflected in the quantitative results. <sep> -The submission was hard to read and some images were initially illegible. <sep> The authors improved several of the weaknesses but not to the desired level. <sep> AC agrees with the majority recommendation to reject.
"abstract | rating_summary | rebuttal_process | decision | suggestion  ==>  ==> ###Summary### <sep> This paper tackles unsupervised domain adaptation in a decentralized setting.  The high-level observation is that the conventional unsupervised domain adaptation has two bottlenecks, namely excessive centralization and poor support for distributed domain datasets. The paper proposes Multi-Step Decentralized Domain Adaptation (MDDA) to transfer the knowledge learned from the source domain to the target domain without sharing the data. <sep> The paper also explores explore a proposition: in addition to adapting from the labeled source, can uDA leverage the knowledge from other target domains, which themselves may have undergone domain adaptation in the past. <sep> The proposed MMDA method contains a feature extractor (E), a domain discriminator (D) and task classifier (C) for each domain. The target domain components are initialized with the respective source components. The source domain discriminator D_s target domain discriminator D_t are synchronized by exchanging and averaging the gradients. The paper also proposes Lazy Synchronization to reduce the communication cost of the algorithm. <sep> The paper also proposes Wasserstein distance guided collaborator selection schema to perform the domain adaptation task. <sep> The paper performs experiments on five image and audio datasets: Rotated MNIST, Digits, and Office-Caltech, DomainNet and Mic2Mic. <sep> The baselines used in this paper include ""Labeled Source"", ""Random Collaborator"", and ""Multi-Collaborator"". The experimental results demonstrate that the proposed method can outperform the baselines on some of the experimental settings. The paper also provides a detailed analysis of the model and experimental results. <sep> ### Novelty ### <sep> This paper does not propose a new domain adaptation algorithm. However, the paper introduces some interesting tricks to solve the MMDA task such as the lazy synchronization between the source domain discriminator and the target domain discriminator. <sep> ###Clarity### <sep> Several critical explanations are missing from the paper: <sep> 1) When training the source domain discriminator D_s and target domain discriminator D_t, if the features between the source domain and target domain cannot be shared with each other, how to train the D_s and D_t. For example, the D_s cannot get access to the features from the target domain, how to train D_s? <sep> 2) How is the target classifier C_t updated when there are no labels for the target domain? <sep> 3) As far as I understand, the domain discriminator is this paper is trained adversarially. The detailed adversarial training step is unclear. <sep> ###Pros### <sep> 1) The paper proposes an interesting transfer learning schema where the data between the source and target domain can not be shared with each other to protect the data-privacy. <sep> 2) The paper provides extensive experiments on multiple standard domain adaptation benchmarks, especially the most recent dataset such as the DomainNet. <sep> 3) The paper provides detail empirical analysis to demonstrate the effectiveness of the proposed methods. <sep> ###Cons### <sep> 1) The most critical issue of this paper is that some explanations are missing, e.g. how are D_s, D_t, C_t trained? Refer to the #Clarity. <sep> 2) The presentation and writing of this paper need polish. The author should do more relative surveys to motivate the authors. One critical relevant reference of this paper is: <sep> ""Secure Federated Transfer Learning"", Yang Liu et al https://arxiv.org/pdf/1812.03337.pdf <sep> 3) The baselines used in this paper is also trivial. It is desirable to compare the proposed method with state-of-the-art domain adaptation methods. <sep> Based on the summary, cons, and pros, the current rating I am giving now is ""reject"". I would like to discuss the final rating with other reviewers, ACs. <sep> To improve the rating, the author should explain the questions I proposed in the #Clarity","This paper proposes a solution to the decentralized privacy preserving domain adaptation problem. In other words, how to adapt to a target domain without explicit data access to other existing domains. In this scenario the authors propose MDDA which consists of both a collaborator selection algorithm based on minimal Wasserstein distance as well as a technique for adapting through sharing discriminator gradients across domains. <sep> The reviewers has split scores for this work with two recommending weak accept and two recommending weak reject. However, both reviewers who recommended weak accept explicitly mentioned that their recommendation was borderline (an option not available for *CONF* 2020). The main issues raised by the reviewers was lack of algorithmic novelty and lack of comparison to prior privacy preserving work. The authors agreed that their goal was not to introduce a new domain adaptation algorithm, but rather to propose a generic solution to extend existing algorithms to the case of privacy preserving and decentralized DA. The authors also provided extensive revisions in response to the reviewers comments. Though the reviewers were convinced on some points (like privacy preserving arguments), there still remained key outstanding issues that were significant enough to cause the reviewers not to update their recommendations. <sep> Therefore, this paper is not recommended for acceptance in its current form. We encourage the authors to build off the revisions completed during the rebuttal phase and any outstanding comments from the reviewers."
"abstract | misc | strength | suggestion  ==> Summary: <sep> The authors propose using non-Euclidean spaces for GCNs. This is inspired by the recent work into non-Euclidean, and especially hyperbolic, embeddings. A few papers have recently tried to go past embeddings into building non-Euclidean models, requiring the lifting of standard operations in Euclidean space to non-Euclidean settings. This has been done in particular in hyperbolic space, but some datasets benefit from more complex spaces. The authors combine the mixed-curvature product formalism that uses products of Euclidean, hyperbolic, and spherical spaces for embeddings, but use these for GCN operations. <sep> Doing this requires, in particular, developing a reasonable way to perform these operations in spherical space (since Euclidean is trivial and hyperbolic has been recently worked on). The authors do a nice lifting via complex operations, and both the hyperbolic and spherical spaces can devolve into the flat Euclidean space when their curvature goes to 0. The authors implement these GCNs, train the curvatures, and demonstrate performance improvements over Euclidean only versions on node classification on benchmark datasets. They also give a fairly nice introduction to all of these ideas in an extended appendix. <sep> Strengths, Weaknesses, Recommendation: <sep> This paper is reasonably interesting---it joins an effort to produce non-Euclidean models in a tractable way, which is fairly challenging, but could have a good impact. On the plus side, it's great that the authors added the nice development for the spherical operations, since that will come in handy for many models. The experiments are also good. On the downside, everything here is an extension of existing work, and the body of the paper is hard to read (though this may be inevitable, there's a lot of background to go over here). Overall I recommend accepting it; I think it's a solid contribution. <sep> Comments: <sep> - I don't understand why the authors say that their space ""interpolates smoothly"" just because the limit in the curvature is the same from the left and right side. For example, the absolute value function has the same limit from the left and the right at 0, but it's not differentiable there. Is it actually true that if we take the derivatives of the piecewise hyperbolic/spherical distance function that it's differentiable at c=0? <sep> - There are a couple of recent papers that also consider hyperbolic GCNs, and in fact use  similar ideas for the aggregation and update steps (i.e., same lift to hyperbolic space). However, these were recently NeurIPS papers, and the text is not yet out, so I don't think this should affect the authors' independent work (and also the product part is new). I do recommend that the authors compare against those results in a future update of this work. The papers are ""Hyperbolic Graph Convolutional Neural Networks"" by Chami et al and Hyperbolic Graph Neural Networks by Liu et al. <sep> - One thing that I didn't see discussed by the authors is that there are subtle difference between hyperbolic and spherical spaces. For example, the weighted midpoint of Def. 3.2 doesn't immediately extend to spherical space (or at least won't be unique). As an example, consider S^2 and the mean of two antipodal points on it---there's many choices for the midpoint. You probably have to limit the operation to a half-sphere (there's some ideas for this in Gu et al). <sep> - For the synthetic tree, why is the number of edges 2(|V|-1) rather than |V|-1? <sep> - Are the curvatures the same for each layer for the GCNs? This is an interesting point to discuss (some of the NeurIPS papers I mentioned train the curvature for each layer). Also, how do you select the number of factors of each type? <sep> - Minor, but some of these citations can be updated. The ""De Sa"" et al 2018 arxiv citation is really Sala et al and is an ICML '18 paper. Similarly, Gulcehre et al is a 2019 *CONF* paper, and so on. It's always good to get these right. <sep> - Is there any actual empirical importance from recovering the Euclidean case exactly for 0 curvature? The reason I ask is that my experience is that the hyperboloid is typically easier to work with. <sep> - One useful thing to point out in B.3.3 is that in general, it need not be a diffeomorphism for all of M for any manifold, which leads to non-uniqueness. In differential geometry, the ""cut locus"" is the region beyond which there is this non-uniqueness. <sep> - In the appendix, the statement ""Sarkar (2011) show that a similar statement as in Theorem 2 holds for a very general class of trees"" is confusing to me. The ""general class"", as far as I know, is actually *all* trees, weighted or unweighted.","This paper proposes using non-Euclidean spaces for GCNs, leveraging the gyrovector space formalism. The model allows products of constant curvature, both positive and negative, generalizing hyperbolic embeddings. <sep> Reviewers got mixed impressions on this paper. Whereas some found its methodology compelling and its empirical evaluation satisfactory, it was generally perceived that this paper will greatly benefit from another round of reviewing. In particular, the authors should improve readability of the main text and provide a more thorough discussion on related recent (and concurrent) work."
"abstract | weakness | rebuttal_process | weakness | decision  ==>  ==> This paper addresses the problem of constructing a sentence embedding using a generative transformer model which encodes semantic aspects and language-specific aspect separately. They use transformers to encode and decode sentence embedding, and the objective reconstructs input with a latent variables (language variables for each language and semantic language).  These latent variables are sampled from multivariate Gaussian prior, and the learning uses evidence lower bound (ELBO) for variational approximation of the joint distribution of latent variables and input. <sep> The method is evaluated on two tasks: sentence similarity task and machine translation evaluation metric tasks. Both tasks evaluates how similar are two sequences, and the metric is correlation score with score's from human judge. The model shows promising results on the first task, but weaker results on the second task, especially when compared against pretty naively built sentence embedding from BERT model. I'm not expert in sentence embedding literature, so a bit tricky to evaluate, but baselines seem strong and experimental results on semantic textual similarity task. <sep> In terms of evaluation, I appreciated how they defined harder subset of the evaluation dataset and showed a larger improvements on those portions of the dataset. The The paper also includes analysis on what is captured by their language-specific latent vector and semantic latent vector. While I'm not totally convinced this distinction between language-specific characteristics and semantics of the sentence, it makes it easier to understand what's going on in the model. <sep> One of my question is, why not test this method in more popular benchmark such as MNLI or other classification tasks? MNLI evaluates how each sentence pair relates to one another, thus would be a good benchmark for sentence embeddings as well. Having to encode all the information about a sentence into a single vector will make these sentence embedding model weaker than other models which can do cross sentence attentions and etc, but I think that's the genuine limitation of sentence embedding research and has to be clarified as such. I recommend discussing and clarifying these points. <sep> I'm a bit unclear how these sentence embeddings are translated into a score that decides the degree to which sentences have the same meaning. Is it just cosine similarity of two sentence embedding vectors? <sep> While the purpose of these references is to generate sentences instead of building a sentence embedding, the method is related and comparison and discussion would be worthwhile. <sep> Generating Sentences from a Continuous Space <sep> Samuel R. Bowman, Luke Vilnis, Oriol Vinyals, Andrew M. Dai, Rafal Jozefowicz, Samy Bengio https://arxiv.org/abs/1511.06349 <sep> Toward Controlled Generation of Text <sep> Zhiting Hu, Zichao Yang, Xiaodan Liang, Ruslan Salakhutdinov, Eric P. Xing https://arxiv.org/abs/1703.00955 <sep> Comments & Questions: <sep> - Methods using a large amount of unsupervised monolingual data shows very strong performance in a panoply of NLP tasks these days. If I understand correctly, this model is constrained by the amount of bitext — some analysis on this would be interesting. <sep> - Figure 1 mentions about ""Section 3, 4"" but I don't think they are correct references? <sep> - BERT baseline seemed not to allow fine-tuning of the LM parameters. I think this makes the baseline significantly weaker? <sep> - It seems odd that only English semantic encoder is used to downstream application. <sep> - Does table 3 covers all the data? What proportion of the data is covered by each row? <sep> - Given the similarity of English and French, I'm not sure how ""language-specific"" such latent vectors are. It would be much more interesting analysis if it studies distant language pairs.","This paper presents a model for building sentence embeddings using a generative transformer model that encoders separately semantic aspects (that are common across languages) and language-specific aspects. The authors evaluate their embeddings in a non-parametric way (i.e., on STS tasks by measuring cosine similarity) and find their method to outperform other sentence embeddings methods. The main concern that both reviewers (and myself) have about this work relates to its evaluation part. While the authors present a set of very interesting difficult evaluation and probing splits aiming at quantifying the linguistic behaviour of their model, it is unsatisfying the fact that the authors do not evaluate their model extensively in standard classification embedding benchmarks (e.g., as in GLUE). The authors comment: ""[their model in producing embeddings] it isn't as strong when using classification for final predictions. This indicates that the embeddings learned by our approach may be most useful when no downstream training is possible"". If this is true, why is it the case and isn't it quite restrictive? I think this work is interesting with a nice analysis but the current empirical results are borderline (yes, the model is better on STS, but this is quite limited of an idea compared to using these embeddings as features in a classification tasks). As such, I do not recommend this paper for acceptance but I do hope that authors will keep improving their method and will make it work in more general problems involving classification tasks."
"abstract | weakness  ==>  ==> The paper proposes a method that aims at encoding trajectories (described as a sequence of actions) into a set of discrete codes with a hierarchical structure. The principle of the algorithm (as far as the article allows me to understand) is to apply multiple iterations of classical sparse coding over the trajectories. The experimental section on simple (deterministic) tasks shows that the SSC method is able to extract interesting options, which can then be used to learn faster on some close domains. <sep> In terms of positioning, I find the idea of the paper interesting (i.e encoding trajectories through discrete symbols) since it uses sparse coding approaches which, as far as I know, are not classical in the RL domain. This type of approach can give us both a meaningful insight about the ""nature"" of the learned policy (as it is the case in the paper that compresses expert trajectories), and can also become a manner to constraint an RL algorithm to force it to exhibit behaviors that could seem more natural to humans. <sep> But the way it is done in this article is disappointing. First of all, the article is badly written, and I am still not sure to fully understand how the algorithm exactly works. Indeed, many notations are not well defined (see at the end of the review), and it makes the algorithm 1 difficult to catch. Then, the authors consider that trajectories are represented as sequences of actions (using one-hot encoding) and do not discuss this hard choice: representing trajectories as a sequence of actions usually rely on the assumption that both the environment is deterministic, and the initial state is always the same. Is it the case in this paper? If it is, it clearly restricts the applicability of the technique. If it is not, then I don't see how it could work well... As far as I understand, all experimental environments are deterministic. So the algorithm description would clearly need to be rewritten, and the authors have to discuss the assumptions they are doing mainly: deterministic environments and also the fact that the ""options"" can only be extracted once a first policy has be learned (or by using expert traces) which limits its applicability. <sep> In terms of experiments, the assumption made is that we have access to a set of 'good' trajectories (which is easy in the proposed environments, but may be difficult in the real-life). It is compared to the option-critic architecture which simultaneously learns the options and the policy and I think that the comparison is somehow unfair. Since SSC is more a ""sequence compression"" algorithm, I would prefer to compare with existing sequence compression algorithms like hierarchical recurrent neural networks for instance.  The results are illustrated in very simple environments and the article would gain by using more complex ones (for instance the Atari grand challenge dataset could be used for such a study). So it is difficult to understand if the approach as it is is really interesting and efficient for general RL purposes. <sep> Summary: A good idea, but not well described, with strong assumptions not discussed, and with low-quality experimental results. <sep> Some other minor remarks: <sep> The introduction is a little bit messy and does not well allow one to understand the focus on the paper, mixing some notions of neuroscience with classical reinforcement learning aspects, the connection between the two domains being not trivial. <sep> Equation 2 versus Equation 3: What is the difference? <sep> s notation appears in 2.1 and 2.2 while it corresponds to different things. The variables are not defined and we don't know in which domain they rely on. <sep> Articulation between sparse coding and MDL not clear (since sparse coding is directly a way to minimize the MDL). MDL never used after that. <sep> section 3, paragraph 3: I do not understand what is described here. The description has to be rewritten to allow the readers to understand the algorithm e.g ""the size of the dictionary elements is set to 2-timesteps. "" ?  ""The dictionary element a which has the highest explained variance is then selected and assigned an integer code n + 1 "" Variance on what ?  what is T_i ? <sep> [cite] appears in the introduction","The paper proposes an interesting idea of identifying repeated action sequences, or behavioral motifs, in the context of hierarchical reinforcement learning, using sparsity/compression. While this is a fresh and useful idea, it appears that the paper requires more work, both in terms of presentation/clarity and in terms of stronger empirical results."
"abstract | weakness | decision  ==>  ==> Summary <sep> ======== <sep> This paper proposes a framework for privacy-preserving training of neural networks, by leveraging trusted execution environments and untrusted GPU accelerators. <sep> The system builds heavily on the prior Slalom system, and uses standard MPC techniques (three non-colluding servers, multiplication triplets) to extend Slalom's inference-only protocol to privacy-preserving training. <sep> This is a valuable and hard to reach goal. Unfortunately, the paper's evaluation fails to deliver on its strong promises, by ignoring the high network communication between the non-colluding servers. <sep> Specifically, all experiments were conducted with three servers co-located in a public cloud's LAN. In this setting, it is hard to argue that non-collusion is a valid security assumption as the cloud provider controls all servers (alternatively, if the cloud provider is trusted, then there is no need for any trusted execution or cryptography). If the same experiments were conducted on a WAN, the communication costs would alleviate any savings in computation time. <sep> For these reasons, I lean strongly towards rejection of this paper. <sep> Detailed comments <sep> ================= <sep> Extending the ideas in Slalom to support privacy-preserving training is a good research question, and Tramer and Boneh had discussed some of the challenges and limitations towards this in their original paper. <sep> Getting rid of the pre-processing stage for blinding factors by leveraging non-colluding servers is a well-known trick from the MPC literature, but it does not seem easily applicable here. <sep> The problem is that the servers need to communicate an amount of data proportional to the size of each internal layer of the network, for each forward and backward pass. If the servers communicate over a standard WAN, the communication time will be much too high to be competitive with the CaffeScone baseline. <sep> In a LAN, as in this paper's experiments, the network latency is low enough for the network costs to be dominated by computation. But this begs the question of whether servers running in a same LAN (e.g., hosted by a single cloud provider) can really be considered non-colluding. In the considered setup, the cloud provider (Google in this case), could just observe the communication between all servers, thereby breaking privacy. <sep> Another security issue with proposed scheme is the lack of computation integrity. This corresponds to the so-called ""honest-but-curious"" threat model which often appears in the MPC literature, and this should be acknowledged and motivated. <sep> On the experimental side, the considered baseline, CaffeScone, seems pretty weak. In particular, any optimizations that the authors implement for Goten (e.g., better paging) should also be added to their baseline for a fair comparison. <sep> The numbers in Figure 3 show that the baseline could be optimized a lot further.  A gap between hardware/simulation modes of ~6x seems indicative of sub-optimal paging. Even the single-core, simulation mode throughput numbers seem low for CIFAR10. <sep> The experimental setup is quite confusing. Running the baseline and Goten in different environments (e.g., different CPUs) and then re-normalizing throughputs is somewhat convoluted and prone to mistakes. Why not run all experiments on the same setup? <sep> Similarly, converting between results in SGX's hardware and simulation modes is also not very indicative. The authors note (p. 8) that in SGX's simulation mode ""code compilation is almost the same as hardware mode except that the program is not protected by SGX, which is fine for our purpose since the DNN training and prediction algorithms are publicly known"". This is fundamentally incorrect! <sep> SGX's simulation mode provides absolutely no security guarantees. It simply compiles the code using the SGX libraries and ensures that the enclaved code performs no untrusted operations, but it does not provide any hardware protections whatsoever. In particular, code running in simulation mode will not be affected by the overhead of SGX's paging, as the memory is never encrypted. <sep> As a result, performance results in simulation mode are usually not indicative of performance in hardware mode. Trying to convert runtimes from simulation mode to hardware mode by comparing times of specific layers is also prone to many approximation errors. <sep> Finally, I had some trouble understanding the way in which Goten quantization works. Section 3.3. mentions that values are treated as floats, but then mentions the use of 53 bits of precision. Did you mean double-precision floats here? But then, aren't modern GPU optimized mainly for single-precision float operations? Section 3.3. also says that the quantization ensures that there are nearly no overflows. What happens when an overflow occurs? I guess that because of the randomized blinding, a single overflow would result in a completely random output. How do you deal with this during training? <sep> Minor <sep> ===== <sep> - Typo in abstract: Slaom -> Slalom <sep> - I don't understand the purpose of footnote 3 in Appendix B.2. First, the bibliographic entry for (Volos et al. 2018) explicitly says that the paper was published in OSDI 2018, a top-tier peer-reviewed conference. Regardless, claiming a date for a first unpublished draft of your paper is a little unusual and somewhat meaningless. I'm sure Volos et al. had a draft of their paper ready in late 2017 or even earlier if they submitted to OSDI in XXX 2018. If you want to timestamp your paper, post in to arXiv or elsewhere online.","This paper proposes a framework for privacy-preserving training of neural networks within a Trusted Execution Environment (TEE) such as Intel SGX. The reviewers found that this is a valuable research directions, but found that there were significant flaws in the experimental setup that need to be addressed. In particular, the paper does not run all the experiments in the same setup, which leads to the use of scaling factor in some cases. The reviewers found that this made it difficult to make sense of the results. The writing of this paper should be streamlined, along with the experiments before resubmission."
"abstract | weakness | rebuttal_process | decision  ==> The paper presents a weakly supervised method for segmentation of trajectories into sub-skills inspired by multi-instance learning (MIL) in image classification by Andrews et al. (2002). This is done via training a classifier to label each observation per time-step with the probability of skills corresponding to that observation. These predictions are then accumulated throughout the trajectory to compute the probability of the skill in that trajectory. There is only a trajectory level supervision provided which specifies which skills are present with no specification of the order in which they appear. They empirically show that their model can achieve decent skill level classification scores on multiple environments provided that there is a large variety of demonstrations provided. <sep> In its current form, I would recommend this paper to be rejected because 1) the framing and motivation of the paper does not correspond to the results and experiments reported and hence seems misleading 2) the paper is limited in scope 3) further experiments and comparisons to relevant baselines are needed to support the claims made in the paper. <sep> The problem that they are proposing is interesting and is of clear value, however, the paper falls short in addressing this problem. In particular, the paper is framed as a way to learn re-usable useful sub-skills that can help generalize to new situations in control. However, the method presented provides a per time-step labelling of each observation with the associated most likely sub-skill. Having the per time-step labelling of the trajectory provides no indication that the data could be useful for learning reusable skills for downstream tasks later. One very basic experiment could be to train a behaviour cloning (BC) agent on the observation-action pairs conditioned on the sub-skill (or a separate network per sub-skill) and show that the learned policies can be leveraged in solving the tasks presented. For instance, one can train a meta-controller that can switch between these learned sub-skills to successfully perform the task. Training such sub-skills from weakly supervised skill annotations has been successfully done by Shiarlis et al. (2018). It should be noted that in their setting, the annotations are ordered which simplifies the problem to some extent. <sep> Ignoring the motivation and focusing only on the problem setting addressed which is annotation of trajectories with skill labels, the experiments seem very restrictive to me. To my understanding there are at most 4-6 primitive skills present in each environment investigated. Looking at the videos linked, it feels like there is some overfitting to the trajectories provided as for example in the case of the video with the red bowl (Reach and Stir inside Cup), the classifier is predicting 'Stirring' from the first frames where there is not much information present in the scene and 'reach to object' could also be plausible for example. It would have been nice to see the logits of predictions per classes for these examples to understand the confidence of the model particularly when some of the skills could be equally likely (e.g. first few frames). <sep> I found the experimental setup unclear. Particularly, details regarding the task setup, how many demonstrations are needed per task/skill, architectural choices and hyper-parameters are lacking and makes experiments hard to follow and understand. I would have also liked to see more analysis regarding the segmented trajectories, particularly how consistent these predictions are through time. To my understanding there is nothing that keeps a skill annotation consistent over some period of time (e.g. the model could keep switching its prediction every time-step). This would be quite unsatisfactory if one would like to use this to actually segment sub-trajectories associated with a given skill that could be useful for training policies. They report applying Gaussian smoothing to filter out noisy predictions but there are no details provided on how this is tuned and how much this affects the quality of segmentations. <sep> Overall, the paper seems to me very limited in its scope and experimental results. The claims made throughout the paper are not supported empirically or theoretically. There is not enough evidence for me to assess the significance of the proposed method and know whether this is indeed useful in practice.","The authors present a multiple instance learning-based approach that uses weak supervison (of which skills appear in any given trajectory) to automatically segment a set of skills from demonstrations. The reviewers had significant concerns about the significance and performance of the method, as well as the metrics used for analysis. Most notably, neither the original paper nor the rebuttal provided a sufficient justification or fix for the lack of analysis beyond accuracy scores (as opposed to confusion matrices, precision/recall, etc), which leaves the contribution and claims of the paper unclear. Thus, I recommend rejection at this time."
"abstract | weakness | rebuttal_process | decision  ==> *Edit: original score was a weak reject (3), updating to a weak accept (6) in light of revisions.* <sep> This work implements a hierarchical control scheme for a high-dimensional control problem (locomotion using a humanoid body).  The hierarchy consists of a high-level module that plans in an abstract space of ""intention"", and the intention variables serve as inputs, along with state, to a low-level controller that actually executes the movements.  The premise is that a lower-level controller should be usable for multiple tasks, and should be able to be commanded by a lower-dimensional intention input.  I find the basic ideas presented clear, the literature reviewed reasonably well, and the motivation and setting to be very interesting.  The video summary is valuable. <sep> My main concerns have to do with presentation, but I think they are relatively significant concerns.  As the draft currently stands, I would, somewhat regrettably, be inclined to reject the submission (marginally).  I think revisions could seriously improve this paper and incline me towards acceptance. <sep> Algorithm 1 indicates that the learning of the low-level controller will be done jointly with the learning of the latent model and planning using the high-level, learned intention space.  In the experiment, it is indicated that the low-level controller is pretrained.  This points to a couple issues that are not clear in the draft: <sep> (1) Presumably this pretraining is necessary and things do not work without it.  Indeed, it is hard to imagine that the movements will be well grounded to human motion capture movements without this pretraining.  Does the algorithm work as written or is pretraining a fundamentally essential step?  There are no settings, even toy settings, where the algorithm as written is shown to be effective. <sep> (2) The authors should be clearer how they conduct the pretraining which involves learning the low-level controller. <sep> (3) I'm not clear how updating the low-level controller is effective in the algorithm.  While I understand why it makes sense to plan in the intention space of the pre-trained controller, and I understand why learning a model is a core part of planning, it would seem like fine-tuning the low-level controller could make the movements deviate considerably from the initial movement space and maybe even eliminate the ability of the low-level policy to express movements that are not used early in training. So essentially, while the planning in the low-D space makes sense and the learning of the model makes sense, the low-level controller update seems possibly to not make sense, and there aren't experiments showing that step helps. <sep> Is it just a coincidence that the intention space (h) is one-of-three and the low-d state space (z) is 3-dimensional as well?  Or are these both selected with sort of going straight vs turning left or right in mind? <sep> The experiment section is generally very unclear, though details are made a little clearer from the video.  In the paper, there are a few points that need to be clearer: <sep> (1) ""ref"", ""plan"", and  ""true"" are not well defined and it is unclear what these distances in Table 1 refer to precisely.  Clearly introduce what each of these refers to.  The authors simply say that there are imitation tasks but do not walk through what these terms refer to. <sep> (2) In 4.1, the different structures are not adequately introduced.  The pointers to the figure 2 diagrams are essential, but there is no pointer for zaz', the pointers are only in the table (not in the text), there are grammar issues in the text and the text could be verbally clearer about the variants. <sep> (3) shs' setting is a bit unclear. Basically, clarify briefly how planning is performed in this case.   Is a forward model still trained, but the model opperates with the full state space?  If so, presumably the forward model is much worse and then the planning approach is correspondingly bad, hence the poor rollouts? <sep> (4) 4.2 is I think obviously inadequately described in the text and I can only assume was the result of rushing for the deadline?  The second experiment is essentially not presented in the text all aside from a still image. <sep> (5) And for all of the experiments that rely on planning with a particle filter, details such as how reliable the filter is in generating useful control, how many samples are required, and possibly elements of compute speed would make much clearer how well the approach actually works.  Does the choice of planner matter at all?  A common, albeit relatively weak, baseline planning approach is CEM...would CEM work here?  I'd like to understand if the choice of particle filter is the author's default choice, which is fine if so, or if there is a positive assertion being made that the particle filter is particularly valuable. <sep> I hope the authors will generally improve the exposition in the experiment section (4) during the revisions. <sep> Overall, I find the paper well motivated in framing the problem (i.e. using model-based approaches to control the latent space of a low-level controller).  I also appreciate the scale of the problem (humanoid control is challenging, so this is not a toy problem).  I find the results a bit unclear, perhaps due to hurriedness in writing, so I find them a bit difficult to fully appreciate.  Nevertheless, the core contribution that I take away from this work is that there is a value to learning the low-dimensional state representation (z, via the LVM), relative to planning using a forward model on the full state (?...I'm still unclear on the presentation of this result, due to unclear exposition). Slightly more broadly, this is a good demonstration of using a planner jointly with a learned high-level command/intention representation, for a high-dimensional problem. <sep> If I've understood this correctly, I'd be reasonably interested in this result. If the authors can both clarify the core results and communicate that the choices made in the algorithm are well thought through, I would be happy to adjust my score. <sep> Relatively minor: <sep> Abstract says 90-dimensional humanoid system, but later it is stated ""34 degrees of freedom, <sep> 197 state features, and 36 action parameters"". Where the 90 dimensions comes from is unclear.  Often people refer to number of actuators or DoFs.  Please adjust this or be more explicit. <sep> In equation 9, f() is not very clearly specified. Is f() a nonlinear function (e.g. a neural network) or is it a linear function? It seems like it might as well be a linear function, since the authors propose to learn a latent dynamics model that is nonlinearly related to the state. <sep> Typos in Fig 3 caption.","The authors present a self-supervised framework for learning a hierarchical policy in reinforcement learning tasks that combines a high-level planner over learned latent goals with a shared low-level goal-completing control policy. The reviewers had significant concerns about both problem positioning (w.r.t. existing work) and writing clarity, as well as the fact that all comparative experiments were ablations, rather than comparisons to prior work. While the reviewers agreed that the authors reasonably resolved issues of clarity, there was not agreement that concerns about positioning w.r.t. prior work and experimental comparisons were sufficiently resolved. Thus, I recommend to reject this paper at this time."
"abstract | weakness | rating_summary | rebuttal_process | decision  ==>  ==> Comments: <sep> -This paper considers codeswitched hate speech texts from an NLP perspective.  The dataset considers mixed languages. <sep> -Focuses on kenyan presidential election. <sep> -Paper has severe formatting issues as well as simple issues like capitalization.  Additionally many plots are rather unattractive (seem to be produced using Excel or Google Sheets, whereas generally something like matplotlib or seaborn is preferred). <sep> -The paper puts a lot of examples into the main text, whereas these are usually put into the appendix, or only a few examples are placed in the main text.  Usually the main text focuses more on higher level analysis. <sep> -Figure 2 is formatted incorrectly (the caption runs on to the next page) <sep> -I appreciate the effort that went into data annotation as well as the disclosure of the demographics of annotators. <sep> -Table 1 should make the metric much clearer (it's mentioned in the main text, but it should be in the caption too, also the best performance usually should be bolded)!  Generally TF-idf features or PDC features seem to have the best performance.  The performance of the CNN does not seem very strong.  I think a simple RNN based approach might also be worth considering.  It would also be worth analyzing if the differences between the methods is attributable to underfitting or overfitting. <sep> -If the paper is proposing a new task with many baselines, it's also important to release the dataset and code in my opinion (I believe *CONF* allows this to be done in a de-anonymized way). <sep> Review: <sep> This paper deals with an important problem in social media analysis.  With the spread of hate speech and hate crimes by rioting separatists in Hong Kong as well as equally hateful attacks on Chinese people in the west, I think that this is an issue that deserves more attention in our community.  Unfortunately this paper needs more polish to be appropriate for *CONF*.  It also might be better suited to an NLP focused conference (such as ACL, EMNLP, or NAACL) although I think if the technical contribution is clear enough it could be suitable for *CONF* as well. <sep> I think the big things to focus on would be including more baselines, improving polish in the paper, and providing a clearer high-level analysis of the dataset (with specific examples mostly left for the appendix).","This paper focuses on hate speech detection and compares several classification methods including Naive Bayes, SVM, KNN, CNN, and many others. The most valuable contribution of this work is a dataset of ~400,000 tweets from 2017 Kenyan general election, although it is unclear whether the authors plan to release the dataset in the future. <sep> The paper is difficult to follow, uses an incorrect *CONF* format, and is full of typos. <sep> All three reviewers agree that while this paper deals with an important topic in social media analysis, it is not ready for publication in its current state. The authors did not provide a rebuttal to reviewers' concerns. <sep> I recommend rejecting this paper for *CONF*."
"abstract | strength  ==> Nonsymmetric determinantal point processes (NDPPs) received some attention recently because they allow modeling of both negative and positive correlations between items. This paper developed scalable learning and MAP inference algorithms with space and time complexity linear in ground set size, which is a huge improvement compared to previous approaches. Experimental results show that the algorithms scale significantly better, and can roughly match the predictive performance of prior work. <sep> This is a well written paper and I recommend its acceptance. Scalable learning and MAP inference algorithms are important for the application of the NDPPs model, which seems promising compared with its symmetric counterpart in experiments. <sep> I have some (minor) comments listed below. <sep> In Lemma 1, the result is only proved for skew-symmetric matrices with even rank. Does it hold for odd rank matrices? This is important to support the claim that the new decomposition covers the P0+ space. <sep> Equation (3) uses notation λi, which is already used in Lemma 1. This could cause confusion. <sep> In the paragraph after Theorem 1, it is proposed to set B = V and relax C. Is this used in Section 4? If not, I would suggest moving it to the experiments section, and adding some comparison in Table 2 to show the impact of this simplification. <sep> I cannot quite understand the last sentence before Lemma 2. What can be computed in O(K2) time? <sep> The footnote in Table 1 might cause confusion because it can be mis-interpreted as a square. <sep> In G.1, the first sentence after equation (13), do you mean when M is odd or when ℓ is odd? <sep> In equation (24), X should be BTXB <sep> The inverse of C appears in the gradient of Z. Is C guaranteed to be invertible in the learning algorithm? And how are V, B, C initialized in the algorithm? <sep> In equation (31), please double check if we need the reciprocal in the denominator.","This paper proposes a technique of decomposing the nonsymmetric kernel of determinantal point processes, which enables inference and learning in time and space linear with respect to the size of the ground set. This substantially improves upon existing work. The proposed method is well supported both with theory and experiments. All of the reviewers find that the contributions are significant, and no major flaws are identified through reviews and discussion. The determinantal point process might not be one of the most popular topics in the *CONF* community today but certainly is relevant."
"abstract | misc  ==>  ==> Summary: <sep> This paper proposes a Rao-Blackwellized version of the straight-through gumbel-softmax gradient (STGS) estimator. <sep> The Gumbel-Rao estimator remains single-evaluation (but multiple sample), does not have higher variance than the original straight-through estimator. <sep> The estimator exhibits lower variance at lower temperatures in the experiments. <sep> Contributions: <sep> Proposes a single-evaluation estimator that cannot have higher variance than the STGS gradient estimator. <sep> Demonstrates effectiveness of proposed estimator in terms of the variance of the gradient estimator and the ELBO on a toy task, a simple parsing task (ListOps), and a mixture model for MNIST. <sep> Strengths: <sep> The method is simple and the computational overhead is very small compared to the original STGS estimator. <sep> The empirical results support lower variance claims and effectiveness at lower temperature. <sep> Weaknesses: <sep> I am not convinced that the relative gains from training at lower temperatures are significant. <sep> The overall gains over ST-GS seem to be modest on MNIST as well as the L <= 50 setting in ListOps. <sep> In the ListOps experiments, lower temperatures barely achieved better accuracy. <sep> Decision: Marginally below acceptance threshold <sep> Improving gradient estimators for discrete latent variable models is an important problem. <sep> The method is straightforward and the claims of performing better at lower temperatures are supported by empirical evidence. <sep> However, the overall performance on the ListOps dataset is lower than related work [1], and there does not appear to be a large gain from low temperatures. <sep> Questions: <sep> The main argument of this paper hinges on the claim that lower temperatures result in lower bias of the gradient estimator. This claim seems reasonable, and is supported by figure 2b. Is there a proof or citation for it, and do we know more? It would be nice to know how variance and bias are traded off, as that would tell us how much (or how little) we could gain from training at lower temperatures. <sep> Is there an explanation for the difference in performance between the 99% accuracy obtained in Havrylov et. al. 2019 [1] and the performance obtained at low temperatures in this paper? <sep> How does this method perform versus the estimator proposed in Pervez et. al. [2], which is also single-evaluation? <sep> Suggestions: <sep> The GR estimator is not guaranteed to have lower variance than ST-GS, just not higher. <sep> Is there an application where lower temperatures are necessary for training? That would strengthen the argument. <sep> [1] Serhii Havrylov, German Kruszewski, and Armand Joulin. Cooperative Learning of Disjoint Syntax and Semantics. In Proceedings of NAACL 2019. <sep> [2] Pervez, A., Cohen, T., & Gavves, E. 2020. Low Bias Low Variance Gradient Estimates for Hierarchical Boolean Stochastic Networks. ICML 2020. <sep> Edited score after author comments.",The paper presents a variance reduction technique to the Straight-Through version of the Gumbel-Softmax estimator. The technique is relying on the truncated Gumbel of Maddison et al. I share the excitement of the reviewers about this work and I expect this technique to further influence the field.
"abstract | strength | rebuttal_process | rating_summary  ==> Summary <sep> The authors propose EATS, a method for TTS from unaligned audio and text data, directly to the waveform. Previous work either use aligned phonetic features, or output spectrograms that are later converted to a waveform by a deep vocoder model. <sep> In order to achieve this, the authors had to use several tricks, some already existing, for instance taken from the GAN TTS architecture, and some novel. The three key novelties are: <sep> differentiable monotonic attention with gaussian kernel and length prediction. <sep> dynamic time warping for the spectrogram loss. <sep> using both spectrogram and waveform domain discriminators <sep> The authors provide a comprehensive ablation study with MOS score, although their model is under the state of the art by a significant margin. <sep> Review <sep> This paper builds on GAN TTS, and tries to make it trainable end-to-end without aligned features. <sep> The two main contributions, namely dynamic time warping and monotonic attention with gaussian kernel are both elegant, and can likely be used for many other applications related to time series with heterogeneous time scales. In particular, the time warping loss allows to accomodate both for the natural irregularities in spoken speech, as well as providing sufficient signal for the monotonic attention to work. <sep> The rest of the architecture is very similar to GAN TTS except for the spectrogram domain discriminator that was added. <sep> While the model is under the state of the art for TTS, the samples are already quite convincing. The authors conduct a thorough ablation study, both with MOS and audio samples. <sep> Overall I think this is a really good paper, that is likely to prove quite useful for the development of end to end speech synthesis solution. As I already mentioned, I also believe that the approach of using dynamic time warping and monotonic attention can be used for other kind of time series. <sep> Remarks and questions to the authors <sep> Table 1, MOS for Tacotron 2 would be very informative. All the baselines are trained on aligned data while Tacotron is a legitimate contender for EATS as it can be trained on the same data. The point of the authors is that their methods is simpler because the training is in one stage. However, given the large number of losses and components in their model, with their respective hyper-parameters to tune, I'm not entirely sold on the simplicity argument. The tacotron 2 paper reports a MOS of 4.5 but on a private dataset. <sep> Section 3, [1] used the same simple L1 + log spectrogram loss as used here. <sep> I was surprised by the bad performance of the transformer attention, in particular in the audio samples, the output for this model is garbage towards the end of the signal. Any clue on why this would happen? <sep> It would be interesting to have a benchmark, in particular, can the model generate speech in real time on GPU and on CPU? <sep> [1] SING: Symbol-to-Instrument Neural Generator, Defossez et al. Neurips 2018.","This paper investigates a speech synthesis approach that directly generates raw audios from text or phoneme inputs in an end-to-end fashion. The approach first maps the input texts/phonemes into a representation sequence that is aligned with the output at a lower sampling frequency by a differentiable aligner and then upsamples the representation sequence to the full audio frequency by a decoder. A number of techniques including adversarial training and soft DTW are applied to improve the training. The experimental results are good. There are raised concerns from the reviewers which are mostly cleared by the rebuttal of the authors. After the rebuttal and discussion, all reviewers are supportive on accepting the paper."
"abstract | strength  ==> The paper studies the controlled sequence generation problem based on pretrained language models, i.e., controlling a generic pretrained LM to satisfy certain constraints, e.g., removing certain biases in language models. Specifically, the paper proposes a distributional view and imposes constraints based on collective statistical properties. The problem is formalized as a constraint satisfaction problem, minimizing a divergence objective. The paper proposes to use KL-Adaptive DPG algorithm for approximating the optimal energy-based model distribution. Experiments were conducted over both pointwise constraints and distributional constraints, showing the effectiveness of the model over the compared baselines. <sep> Pros: <sep> The problem under study is an important problem and can have extensive impact on many downstream language generation applications. <sep> This paper makes solid contributions by proposing a formal view on generation controlling. It provides a framework to handle pointwise, distributional, and hybrid constraints. <sep> The method proposed to sample from the sequential EBM makes sense and is empirically vilified to be effective. <sep> The experiments and analyses support the claims and conclusions. <sep> Overall, the paper is well organized and easy to understand. <sep> Cons: <sep> The paper may benefit from some human evaluation for text generation. <sep> It is somehow not easy to tell which model is better from figure 2, GDC or Ziegler. It seems that Ziegler is superior in generating attribute-related sentences while inferior in diversity. The sentence quality might be similar as the converged values of (π, a) are close. <sep> The current submission contains a number of typos, grammatical and other style issues, in both the main sections and appendixes, but these are rather easy to fix. <sep> Questions: <sep> For real-life applications, whether the proposed framework has scalability issue; e.g., if a task has a large number of constraints to consider or if the constraints are more complicated than what are tested in Section 3? <sep> Assuming one has already got an adjusted LM with some attributes based on GPT2, which would be better if she/he wants to add a new attribute to generation: starting scratch from GPT2 or continuing with the adjusted model?","The paper studies the problem of being able to control text generated by pre-trained language models. <sep> The problem is timely and important. The paper frames the problem as constraint satisfaction over a probability distribution. Both pointwise and distributional constraints can be imposed. The proposed algorithm, Generation with Distributional Control (GDC), is elegant, and is an interesting new addition to this line of work. Overall, the paper brings forth news ideas, and could have impact."
"abstract | weakness | rebuttal_process | decision | suggestion  ==> This problem is well-motivated --- estimating dose-response is a challenging and practically important problem. <sep> The paper is extremely well written.  It explained complex (poor written) ideas in the semiparametric literature clearly. <sep> The comparison against existing works is clear. <sep> The theory, as far as I can tell, is sound. It improves the existing results in targeted regularization and can be adapted to analyze one step TMLE. <sep> The experiment shows that the model outperforms existing benchmarks on the task it set out to do. <sep> My main suggestion to improve the paper is to use some datasets that actually have continuous treatment in evaluating the method.  In particular, I would be interested in seeing an application of the methods on real-world datasets. That being said, while it will improve the paper, it's probably asking too much for an 8-page conference submission. I believe the paper at its current state is sufficient for acceptance. <sep> I want to thank the authors for writing such an elegant paper. I really enjoyed reading it.","The paper designs a new way (in some sense a new perspective) on how neural networks can be used to model intervention variables when the goal is to estimate ADRF. Basically, the idea is to emphasize the importance of the intervention variable by ensuring that it appears not just in every layer but also in every neural of a neural network. <sep> Reviewers mostly agree that this is a good paper with varying degrees, although there are some criticisms on e.g., assuming away the confounders. However, I believe the authors address the criticisms of R4 satisfactorily. <sep> Overall I find the idea new and interesting and the experimental results strong, hence I happily recommend accepting the paper. <sep> I do have a few quips myself and some comments that may help the authors to further improve the paper. <sep> Re: the design that models each parameter as a spline. <sep> This is equivalent to introducing additional parameters (coefficients for spline basis) and adding a fixed linear layer (spline basis themselves) to every layer of the neural networks. t is taken as an input in all layers thus it makes sure that the model prioritizes on learning the impact of t. <sep> If you use a B-spline basis (that comes with kernels of bounded support), then the proposed method is very similar to stratifying the data according to different bins of t, and then fitting a separate model for each t. The only difference is that the different bins are now smooth kernels and they overlap somewhat. As a side note, the authors should clearly write out how they are choosing the knots to specify the basis functions. Otherwise the paper will not be reproducible. <sep> I am not sure how this method would compare to naive (non-deep) baselines. Maybe this was considered in a prior work? If not, then I tend to side with Reviewer 4 that the evaluations are mostly ablation studies and they are not really comparing to representative work in this domain. Given that there is a large body of work on this before deep learning takes over, it is important to somehow compare with the right baselines."
"abstract | strength | decision  ==> Summary: <sep> This paper proposes a new intrinsic objective for RL agents: surprise minimization. This may come as a surprise, as other related works usually propose to maximize surprise, or to maximize novelty. The authors present motivations and conduct an empirical study on several environments to support their idea. <sep> Strong points: <sep> Overall, I think it is a good paper. Let me list some strong points: <sep> The idea is simple, novel and well motivated. The paper positions this new intrinsic objective with respect to variants of novelty maximization objectives and brings a new perspective. <sep> It's well written and organized <sep> The algorithm is tested on a relevant selection of environments and against state-of-the-art algorithms using intrinsic objectives. <sep> The empirical evidence seems to support the claims. <sep> The related work is quite complete. <sep> I liked the discussion about stable vs unstable environments. It is the first time I see it discussed. <sep> The website brings visualizations of trained policies. <sep> Weak points: <sep> I will now list a few weak points of the paper. <sep> Some descriptions of the results are missing. In figures, what does the shaded area represent? (std, sem, confidence intervals, etc). In Table 1, what are the numbers? (what is the central tendency, what is the error, how many seeds?). Same for Appendix D. <sep> The paper does not provide all necessary information to reproduce the results. There is no detail about the RL algorithms (TRPO and DQN), no description of the architectures, and no hyperparameters. This is important and should be contained somewhere in the Appendix. <sep> Will the code be released? If not, why so? Same questions for the environments, are they accessible somewhere? <sep> It seems to me that this approach could potentially tackle harder problems, but the paper is limited to a simplification of the tetris game, planar humanoid variants and Doom. The x-axes of the figures also tell us that only a few episodes were needed to solve them. I am not saying that I need the hardest games solved to find an algorithm interesting, but I am wondering whether it would scale. Could you tell us whether you attempted to tackle harder environments, and if so, why do you think SMiRL failed? I think we can gain a lot of understanding by looking at negative results. <sep> For example, I feel like testing this algorithm on a 3D humanoid would better demonstrate the power of this approach. <sep> It seems to me that there might be a confounding factor that could partially explain the success of the surprise reward. Indeed, it seems that all environments presented here can terminate when the agent dies. This is a guess, as I could not find this information in the paper (please add it). If so, then the expected cumulative rewards is an increasing function of the lifetime of the agent in the game. Because of this, maximizing the cumulative surprise might be a good idea because it goes in the same direction (by construction) as maximizing survival. <sep> A counter-argument from the paper can be the performance of SMiRL + reward in Walk, that is superior to the performance of reward alone. However, this is unclear as I could not find the description of the reward function in the paper (please add it). <sep> Another way to disprove this hypothesis would be to compare the performance of SMiRL to the performance of an agent maximizing a reward function that gives +1 whenever the agent is alive (survival bonus). If it performs better, then surprise maximization brings something to the table, if it does not, then it might work because it is correlated to the survival time. <sep> Another way could be to have episodes that do not include death-related resets (fixed length episodes). <sep> Recommendation and justification: <sep> This is overall a strong paper. However I'm concerned about the potential confounding factor of the survival time. I'll give a score of 6, but I would happily increase that score if <sep> The authors convince me that the success of the surprise maximization is not due to the survival confounding factor. <sep> The authors include all necessary details for reproducibility and/or release the code. <sep> Discuss the scalability to harder problems (e.g. 3D Humanoid). <sep> Feedback to improve the paper (not part of assessment) <sep> I would move the discussion about how surprise minimization and novelty maximization can be complementary to the intro. These two approaches seem to go in opposite directions and, as a reader, I would be happy to read this discussion early. <sep> It would be interesting to discuss how it plays out in natural agents. The intuition is that minimizing surprise leads to finding a stable configuration and staying there. In practice, it is probably balanced with other driving needs like the need for food. I guess it is discussed in related papers like Friston 2009. In natural agents, surprise minimization must also be model-based. Indeed, animals do not need to jump out of cliffs several times to know that it's bad. <sep> Not sure I understand the inequality in Eq1. Maybe I missed something. <sep> Discuss the surprise maximization approach of Achiam et al and whether it differs from yours. <sep> Table 1: the legend seems to disagree with the results. It seems to me that the entropy difference is as low in your environment as in the others, but the caption says ""note the clear negative  entropy gap on our tasks, whereas this clear trend is absent on the Atari games"". <sep> Fig 4. Is it really training in 80 episodes? There are very few images to train a VAE, especially if the episode resets when the agent falls (does it?) How many steps per episode? <sep> How do you get demos for humanoid tasks? I guess there are not human demos but previously trained agents? <sep> Results in Fig. 6 are not super satisfying, they are quite far from the target (although I guess it is a difficult task). I am not even sure the second example is achievable. In the traditional Tetris, wouldn't cubes fall due to gravity? <sep> Typos: <sep> ""our results are available online"" → missing full stop. <sep> ""In such environments, which we believe are more reflective of the real world"" → previous sentences do not discuss environments but intrinsic objectives. <sep> ""unexpected events don't happen"" → ""do not"" <sep> ""deep DQN"" → ""Deep Q-Networks"", or ""DQN"" <sep> Update post-rebuttal <sep> The authors addressed most of my concerns, especially the one about the confounding factor. I am updating the score from 6 to 7.","The paper is studying a new intrinsic motivation RL setup in a dynamic environment, where the authors minimize the state entropy instead of the common approach of maximizing it. The resulting idea is simple but also surprising that it works so well. All reviewers appreciated the new problem formulation of using dynamic environments and found the idea very promsing. In addition, they identified the following strengths of the paper: <sep> The experiments are exhaustive, identifying many domains where the approach can be applied <sep> The presented results are compelling <sep> The paper is well written <sep> The paper introduces a new problem setup that has not been studied before <sep> I agree with the reviewers that this paper contains many interesting contributions and therefore recommend acceptance."
"abstract | strength | weakness | rebuttal_process | decision | strength | weakness  ==> Principal component analysis (PCA) is a well-known dimensionality reduction and feature learning technique in the literature that leads to uncorrelated features. While there are a plethora of algorithms for PCA, along with accompanying analysis, a majority of these works have been developed from an optimization perspective. This paper differs from existing works in that it motivates the k-PCA problem, which involves learning the k-dominant eigen vectors of the sample covariance matrix, as a competitive game between k players in which each player is supposed to estimate one of the eigen vectors and the PCA solution is the unique strict-Nash equilibrium. The main contributions of the paper in this regard are the following: <sep> Setting up the PCA problem as a competitive game between k players and showing that the Nash equilibrium corresponds to the PCA solution (Theorem 2.1) <sep> Development of two games (algorithms), with one a sequential algorithm and the other a decentralized algorithm, for solving the PCA problem (Algorithms 1 and 2) <sep> Convergence analysis of the sequential algorithm under a restrictive set of assumptions (Theorem 4.1) <sep> Establishment of the equivalence between the decentralized algorithm and the Generalized Hebbian Algorithm (GHA) of Sanger (Proposition H.1) <sep> Overall, this is a novel paper in that it offers an alternate view of the PCA problem, which might lead to further advances in our understanding of PCA-type algorithms in the future. I therefore have a favorable view of this paper. There are however several important aspects of this paper that need to be clarified by the authors in a subsequent revision before it becomes ready for publication. <sep> Major Comments <sep> Theorem 2.1 is based on the assumption of the top-k eigenvalues being distinct. Algorithms such as Orthogonal Iteration (""subspace"" power method), to the best of my understanding, only require an eigen gap between the k and k+1 eigenvalues and do not require the first k eigenvalues to be distinct. This needs to be discussed clearly in the paper. <sep> While Theorem 4.1 for the sequential game does not explicitly state it, it appears that it also requires the eigenvalues to be distinct (Theorem L.4, e.g.). This, once again, is a major assumption that is neither discussed clearly in the paper, nor compared to other works that do not seem to have this limitation. <sep> Majority of the works in the PCA literature require the initialization subspace to not be orthogonal to the k-PCA subspace. This work, however, requires the stringent assumption that each eigenvector is initialized to within π/4 radians of the original eigenvector. Not only is this a strict probabilistic assumption in the case of random initialization, but it also becomes harder to satisfy as k increases (as the authors also discuss). In light of this strict condition, this reviewer is confused by the claim in the paper that ""these theoretical findings are strong relative to other claims."" I would also have liked the authors to discuss this assumption up front in the paper. <sep> The sequential game appears to be very similar to other approaches that have been proposed in the literature that estimate an eigenvector, subtract its contributions from the data, and then estimate the next eigenvector (see, e.g., Allen-Zhu and Li, 2017 and Raja and Bajwa, 2020). Such approaches of course suffer from the fact that they require distinct eigenvalues, but they don't require any QR decomposition. There is however no discussion of the connections between such approaches and the proposed sequential game. <sep> Why is the decentralized game being called ""decentralized""? Is there a distinction the authors are making between distributed and decentralized? What's the topology being considered by the authors in relation to this game and what exactly does ""broadcast(v^i)"" mean in terms of reaching out to other nodes? Some discussion of this would be useful. <sep> While the decentralized game has not been analyzed in this paper, the distributed variant of GHA has been analyzed in the literature; see ""Fast and communication-efficient distributed PCA"". It would be helpful for the authors to comment on the differences between their decentralized algorithm and this distributed GHA work. <sep> Why is ""the longest streak of consecutive vectors with angular error less than π/8 radians"" the right metric for the experiments? <sep> The claim in Figure 3 that ""We omit Krasulina's as it is only designed to find the top-k subspace"" is not clear to this reviewer. <sep> It would be useful for the authors to discuss the use of ∇vR, rather than ∇vi, for updates in both algorithms. <sep> Minor Comments <sep> In my opinion, it is incorrect to say that PCA leads to interpretable features. <sep> The claim ""An exponential convergence rate in the full-batch setting is possible using Riemannian acceleration techniques"" is perhaps too ambitious, unless the authors are confident that this is doable, in which case one wonders why this was not shown in the paper. <sep> The experimental plots in the paper is too hard to see clearly. It might be a useful idea to add them to the appendix also, where they can be shown in larger sizes. <sep> Post-discussion period comments <sep> The authors have satisfactorily addressed all of my comments as well as, in my opinion, comments of other reviewers. Based on the latest revised version of the paper, I am increasing my score to 8 (from 7). I believe this paper is worthy of publication in proceedings of *CONF* 2021 and I recommend it as such.","This paper introduces a novel game-theoretic view on PCA which yields an algorithm (EigenGame; Algorithm 2) that allows evaluation of singular vectors in a decentralized manner. The proposed algorithm is significant in its scalability, as demonstrated in the experiment on a large-scale dataset (ResNet-200 activations). This paper is generally clearly written, and in particular Section 2 provides an easy-to-follow reasoning leading to the proposed game-theoretic reformulation of PCA. I felt that the later sections are a bit condensed, including the figures. In the authors response major concerns raised by the reviewers have been appropriately addressed. I would thus recommend acceptance of this paper. <sep> What I found particularly interesting in their game-theoretic reformulation is that in the utility functions shown in (6) the orthogonality constraints u^j⊤u^i=0 have been removed and replaced with the soft constraints represented as the regularizer terms encouraging the orthogonality. Although several alternative forms for the regularizers would be possible, it is this particular form that allows an efficient gradient-ascent algorithm which does not require explicit orthonormalization or matrix inversion is straightforwardly parallelizable. <sep> Pros: <sep> Provides a novel game-theoretic reformulation of PCA. <sep> Proposes a sequential algorithm and a decentralized algorithm for PCA on the basis of the game-theoretic reformulation. <sep> Provides theoretical guarantee for the global convergence of the sequential algorithm. <sep> Demonstrates that the proposed decentralized algorithm is scalable to large-scale problems. <sep> Cons: <sep> The latter statement of Theorem 4.1 requires conditions on the initialization, which are hard to satisfy in high-dimensional settings. <sep> Significance of the proposed game-theoretic formulation in the context of game theory does not seem to be well explored."
"abstract | strength | rebuttal_process | decision  ==> Edit: <sep> I have read the authors' response as well as the other reviews. Based on the additional results and added feature selection details, I now agree that ESP is generally applicable. <sep> Summary: <sep> The authors present ESP, an RL system that can then explain action choices in terms of future feature values. Generalized value functions (GVFs) are used to learn an estimate of total future discounted feature values. These estimated total feature values are then used to estimate each actions Q-value. Since Q-values are based on GVF outputs, these intermediate values can be used as an explanation. The authors select a subset of these values to form a Minimal Sufficient Explanation (MSX). The proposed system is evaluated using three domains. The authors show that performance is comparable to non-GVF agents. <sep> Reasons for score: <sep> Though the authors present a novel explanation format, the applicability of the method is uncertain. The results appear to rely on specific GVF feature choices. Non-general methods are still of interest, but the limited information on feature construction prevents a fair comparison to other approaches. Additionally, the explanations are not evaluated quantitatively. <sep> Pros: <sep> -The use of GVFs for explanations in terms of future feature values is a novel line of work. MSXs are a natural way to then produce more concise explanations, and the authors extend MSX to their non-linear use case in a well-justified way. <sep> -The analysis of the Tug of War explanations was thorough. It clearly showed how ESP explanations would be used to investigate agent behavior. <sep> Cons: <sep> -ESP is built upon the GVF features, but the choice of GVF features is suspect. Each environment uses a different style for its features. Lunar Lander and CartPole both have continuous features, yet the authors use ""deltas"" for Lunar Lander and region discretization for Cart Pole. Tug of War uses a bunch of features, including information about feature values when a game ends, and non-linearities are applied to the outputs of the GVF features. Note that these non-linearities are used only for Tug of War, and different non-linearities are used for different features (Table 2 in Appendix D). Effectively, each environment appears to use carefully engineered features. Given that DQN-Full performs very poorly for specific settings (i.e., in some cases, the agent cannot learn without the GVF features), the choice of features seems to be important. The authors should indicate the process used for selecting them and how these features should be chosen for other environments. ESP may not be robust to GVF feature choice, but this is insufficiently addressed in the paper. <sep> -In Section 6.3, the authors present potential conclusions that can be drawn from an ESP agent. These conclusions can be evaluated to determine whether valid conclusions can be drawn from the explanations. Such an evaluation would allow the hypotheses of the authors to be tested. <sep> -A substantial reorganization of the paper would improve clarity. Various definitions and descriptions are provided a few sections after the terms/methods are first used. Terms are unnecessarily over-loaded (such as ""sound""). <sep> Questions During Rebuttal Period: <sep> -Please address and clarify the ""Cons"" above. <sep> -In particular: <sep> a) How were the GVF features chosen? Why does each environment use different features? <sep> b) What happens when Lunar Lander is given ""CartPole-style"" (region discretization) features? What happens when CartPole is given ""Lunar-Lander-style"" (change in features) features? <sep> Minor comments: <sep> -This work would benefit from another editing pass for tense/plurality matching between subject and verb. <sep> -The proofs in Appendix B would benefit from a re-write; currently, they are hard to parse.","This paper tackles the important problem of endowing deep RL agents with added interpretability. Action values are decomposed as the combination of GVFs learned on externally-specified features, offering action explanations in terms of discounted future returns in the space of interpretable quantities. Reviewers praised the approach, as well as the level of detail for reproducibility purposes. R3 had concerns about the generality of the method but follow-up experiments have allayed these concerns. Given the reviewer response and the central importance of the problem considered to the field, I can wholeheartedly recommend acceptance."
"abstract | strength | abstract | strength | decision  ==> Paper summary <sep> The paper gives theoretical proof showing that the recently proposed data augmentation technique Mixup can indeed improve generalization and help in robustness. The theorems cover GLMs and certain classes of neural networks. The paper also contains numerical experiments supporting some aspects of the theory. <sep> Strengths <sep> Currently, there is only a limited theoretical understanding of why Mixup works. This paper shows that Mixup is essentially equal to regularizing the first and second derivatives (with respect to the input x). Intuitively, this means that changing the training samples slightly shouldn't change the output of the model much. Further, the paper proves that the mixup loss is an upper bound on the 2nd order Taylor approximation of the adversarial loss, and hence reducing mixup loss reduces adversarial loss. Finally, the paper proves that mixup helps in reducing the Rademacher complexity and hence improves generalization. <sep> The results seem fairly general and apply to many models such as GLMs and neural networks. <sep> The paper supports its approximations and claims by numerical experiments. <sep> Concerns <sep> The regularizing term R3 looks like it is minimizing zT∇fθ(xi)z (for some z). This promotes the Hessian (wrt x) to have negative eigenvalues in the direction of z. Ideally, we would want the Hessian (and also the gradient) to be 0 around the training samples so that perturbing the input doesn't change the output much. Thus, I don't see how the R3 term helps regularize the Hessian properly. <sep> The paper claims that Assumption 3.1 holds when the minimizers are not too dispersed. Does it still hold for practical neural networks where the minimizers can possible be fairly far apart? <sep> Comments <sep> Although the paper seems well written, I have a few suggestions: <sep> The notation cos(θ,x) which refers to ⟨θ,x⟩|θ||x| should be explained in the preliminary section. <sep> On page 6, the statement fθ(x)=∇fθ(x)Tx should be proven. It will save the reader some time if the proof is provided. <sep> In Remark 3.1, I think Theorem 3.2 should actually be Theorem 3.4 <sep> Score justification <sep> There isn't much prior work on the theoretical understanding of Mixup. This paper provides theoretical guarantees for Mixup on two fronts - robustness and generalization; for both GLMs and ReLUs.","This paper provides theoretical justifications on why the data augmentation technique, Mixup (convex combinations of pairs of data examples) , can help in improving robustness and generalization of GLMs and ReLUs. The authors rewrote a Mixup loss function as the summation of a standard empirical loss and some regularization terms regularizing gradient, Hessian and some higher order terms. Using the quadratic approximation of the Mixup loss (ignoring the higher order terms), the authors proved that the quadratic approximation of the Mixup loss was equivalent to an upper bound of the second order Taylor expansion of an adversarial loss, providing justifications for why Mixup loss training could improve robustness against small attacks. Using the same quadratic approximation of the Mixup loss, the regularization term controlled the hypothesis class to have a smaller Rademacher complexity. <sep> Overall, the paper provides insightful theoretical interpretations for a commonly used data augmentation technique in DL. The paper also supports its claims by numerical experiments. Although there is some minor concerns on using the quadratic approximation of the Mixup loss, as well as R3 term's regularization effect on a broader family of models, the paper provides unique and novel insights on Mixup.; all reviewers acknowledge the authors applying the existing proof techniques to analyze Mixup's effect on robustness and generalization. <sep> Therefore, I recommend accepting this paper."
"abstract | suggestion  ==>  ==> Summary <sep> This paper proposes an improvement to how tokens are selected for masking in pre-training large masked language models (BERT and family). Specifically, it stipulates that purely random choice of words (or word pieces) makes the MLM task insufficiently hard. It then goes on to propose a data-driven approach for selecting n-grams to mask together. The approach, based on an extension of pointwise mutual information for n-grams, is shown to outperform random token and random spans masking strategies on performance of downstream tasks. <sep> Strong and weak points <sep> Strong points: <sep> The paper is very well written throughout, and easy to follow. <sep> The problem is well motivated with empirical evidence. I think Section 2 demonstrates well the case for random masking being too easy. <sep> The multivariate version of PMI proposed is simple and well motivated. <sep> The evaluation experiments are convincing the results are robust across the tasks shown, <sep> Weak points: <sep> The one main drawback in this study is the lack of comparison with entity-based techniques for masking.  In particular [1] has recently defined ""salient span masking"" based on named entity recognition and dates. Salient span masking has been adopted in [2] where it is shown to boost performance of open-domain question answering by 9+ points (Table1 of [2], models tagged with ""+ SSM""). <sep> I think it would be extremely interesting to compare these techniques (SSM specifically, but entity-based techniques in general) with PMI-Masking. Specifically, it is currently unclear whether the PMI based n-gram masking vocabulary simply ends up rediscovering popular named entity mentions, or whether there are more interesting sub-phrases (e.g., idiomatic sub-phrases) that a NER system would not select. Finally, it would be interesting to empirically test whether these extra non-entity n-grams provide further performance boost over the entity-based ""salient span masking"". <sep> [1] REALM: Retrieval-Augmented Language Model Pre-Training (https://arxiv.org/abs/2002.08909). <sep> [2] How Much Knowledge Can You Pack Into the Parameters of a Language Model? (https://arxiv.org/abs/2002.08910) <sep> Recommendation <sep> I recommend this paper for acceptance. The analysis and ideas throughout the paper are well executed. I also think the topic should be of high interest to the *CONF* and NLP communities, given the importance of MLM pre-training on most state-of-the-art models at the moment. Despite the lack of comparison with entity-based techniques, having a statistically principled alternative, solely based on co-occurrence, without linguistic grounding, seems interesting. <sep> Questions for authors <sep> The main question here related to the entity-based approaches discussed above. I think it would be interesting to address this issue given how closely related it is to this work, and the good performance the cited papers demonstrate using it. I can think of a couple of ways to address this comparison: (1) qualitative analysis of the masking vocabulary to better understand the differences between ""PMI ngrams"" and entity mentions, (2) experimental analysis incorporating some entity-based masking into the experiments in the paper. <sep> Irrespectively of how you choose to address the entity-based comparison, I was interested in some analysis, or sampling, of the PMI-Masking vocabulary to understand what type of n-grams are being selected (entity mentions, idiomatic phrases, noun-phrases, etc.). Would be interesting if you could make this vocab available or add a small sample in the appendix. <sep> Throughout the paper there is an assumption that contiguous words are considered for masking. This was not immediately clear in the beginning of the paper (I realized it only in Section 3.2 with - ""What about contiguous spans of more than two tokens?"").  But one question came to mind: what about correlated non-contiguous spans? For example ""eigenvalue"" and ""eigenvector"" are unlikely to be present in the same n-gram, but have reasonably high chances of showing up together in the same passage. Have you considered extending this work to non-contiguous spans? Is there any expectation that this would help learning, or is it just a bad idea? <sep> Was there an attempt to mix masking strategies during pre-training? Although Table 6 is convincing in demonstrating that single-token perplexity is not correlated with performance of downstream tasks, the differences seem curious. One is left wondering if there is any benefit in adding a small number of ""easy"" masking cases (i.e., random-tokens or random-spans)?","This paper describes a new and experimentally useful way to propose masked spans for MLM pretraining, by masking spans of text that co-occur more often than would be expected given their components - ie that are statistically likely to be non-compositional phrases. <sep> The authors should make some attempt to connect their PMI heuristic with prior methods for statistical phrase-finding and term recognition, eg https://www.aaai.org/Papers/IJCAI/2007/IJCAI07-439.pdf or https://link.springer.com/chapter/10.1007/978-3-540-85287-2_24 in the final paper."
"abstract | rebuttal_process  ==> The paper is generally a good contribution especially in the field of machine learning for physics. However, I do have some questions about the novelty of this work and what makes it different from other physics informed approaches. Below are the pros and cons and the I list out a set of comments and additional results that I believe would make the contribution of the paper much more clear. <sep> Pros: <sep> -- A physics informed neural architecture that optimizes the governing PDEs to perform forward integration of the system. Generally a great approach since it does not require high-res numerical simulations <sep> -- Generalizes to new geometries <sep> Cons <sep> -- I might be missing something, but I generally feel that there is a lack of novelty. Approach is very similar to certain papers I point out in the comments. That however, does not take anything away from the approach proposed and it is still a good contribution once certain concerns (in the comments) are resolved <sep> -- The authors claim that their network generalizes but I do not see an explanation for it. Even an intuitive explanation would be good. <sep> The introduction section of the paper does talk about different approaches to physics informed neural networks especially the Raissi et al., papers and mention that Raissi's approach (https://arxiv.org/pdf/1711.10566.pdf and https://arxiv.org/abs/1711.10561) does not generalize to new domains. Can the authors explain why they believe that their network would automatically generalize to new geometries? Does the introduction of Ω and ∂Ω in their input features make their framework generalizable? <sep> The use of vector potentials to ensure divergence free by construction inside neural architectures using fixed convolution operations has been shown before By Mohan et al., (https://arxiv.org/pdf/2002.00021.pdf). The authors should cite this paper. <sep> In Figure 2, I'm concerned about how well the network satisfies BCs. If possible, I would like to see a plot showing velocity with time during inference near the edges of the obstacle. <sep> There is a complete absence of baselines. Sure, the a-net outperforms v-net when it comes to the loss. However, the loss term is 10−3 for v-net and 10−5 for a-net. Both are pretty low. How much improvement in inference would one expect?  It is evident that constraining divergence by construction definitely helps in reducing the loss and this has been shown in https://arxiv.org/pdf/2002.00021.pdf. <sep> I would like to see a comparison between the a-net prediction and a numerical simulation and possibly a comparison with general PINNs (https://arxiv.org/pdf/1711.10566.pdf)  for the rectangular obstacle. It should clearly show that their approach is more accurate in terms of RMSE error between a-net and numerical simulation as compared to PINN and numerical simulation. What would be more interesting to see is how well the authors satisfy BCs as compared to PINN. Moreover according to the authors PINNs would not generalize to the airfoil shaped obstacle while their network would . This would clearly show an advantage of the proposed mechanism. Without a baseline however, there is not much to prove that this is a better approach. <sep> The problem solved in this paper is a very simple problem. Even numerical solvers are not expensive when it comes to solving this problem. This physics informed approach is very promising because it can reduce the computational cost immensely (during inference). Would it be possible for the authors to show this for a real turbulent flow, e.g. the system considered in the Mohan et al., paper (https://arxiv.org/pdf/2002.00021.pdf) or the Kolmogorov system as shown in this paper (https://advances.sciencemag.org/content/advances/3/9/e1701533.full.pdf) or even the 2D Kraichnan system shown in https://arxiv.org/abs/1808.02983. Unless these approaches generalize to fully turbulent flow, the use for such architectures are limited for real applications <sep> This is still an important contribution in the field of deep learning for computational physics, specifically in fluid dynamics.","The paper introduces a learning framework for solving incompressible Navier-Stokes fluid using a physics informed loss formulation. The PDE is solved on a grid, and the model, implemented via convolutions and a U-Net, is trained to minimize the NS residual. The model is trained on a variety of randomized contexts, in a way that allows training to explore a large number of configurations. The paper presents original contributions compared to previous Physics informed framework (discrete formulation, conditioning on the domain conditions, …). All the reviewers agree that the detailed rebuttal provides answers to their questions and that the contribution is significant, they all have a positive assessment of the paper."
"abstract | strength  ==>  ==> ########################################################################## <sep> Summary: <sep> This paper investigates data augmentation in the context of RL and proposes a novel augmentation algorithm to enabling robust learning directly from pixels without the need for auxiliary losses or pre-training. The authors propose to average both the Q function and its target over multiple image transformations. The experiments on DeepMind control suite and Atari 100k benchmark show that their method outperforms previous model-free, model-based and contrastive learning approaches. <sep> ########################################################################## <sep> Pros: <sep> This paper tackles a valuable problem of improving RL by data augmentation. It will have a broad impact on the area of both representation learning and reinforcement learning. <sep> The idea of averaging both the Q function and its target over multiple image transformations is interesting and promising. This approach is easy to use and can be combined with any model-free RL algorithm. <sep> The paper is well written and the results section is well structured. They outperform baseline methods on two popular benchmarks and conduct ablation studies to verify the contribution of each component. <sep> ########################################################################## <sep> Cons: <sep> The proposed idea is very similar to RAD, a concurrent work by Laskin et al. The performance is also similar between these two approaches. More discussions and comparisons will help the readers better understand the difference. <sep> They claim data augmentation is all you need. To support this strong claim. I think the authors should conduct more experiments on more base algorithms. Can we get the same conclusion if we add the data augmentation techniques to other model-free RL algorithms (e.g., PPO or TD3) or a model-based RL algorithm (e.g., PlaNet or Dreamer)? <sep> ########################################################################## <sep> Taken both pros and cons in to consideration, I vote for an acceptance because of the novelty of the proposed idea and large-scale comparisons to previous model-free, model-based and contrastive approaches. However, the experiments in the paper are not sufficient to support their claims ""data augmentation is all you need"" and I do suggest the authors to include more experiments to make it clear.",The paper describes a new data augmentation approach for image based RL. The approach is both simple and effective. It improves significantly the performance of several algorithms across a number of tasks. The reviewers were unanimous about the benefits of the proposed technique. This represents an important advance for RL.
"abstract | rating_summary | misc | weakness | decision  ==> [EDITED AFTER DISCUSSION: My concerns are largely addressed and the paper is now stronger. I very much hope to see it at the conference, and have updated my review and rating accordingly.] <sep> The paper proposes a new algorithm for early classification of sequential data, exploiting approaches to density ratio estimation to enable applying a sequential probability ratio test-type algorithm on perceptually rich data with no explicit likelihood. The algorithm is trained using a novel density ratio estimation loss alongside more standard cross-entropy, and shows strong performance on a number of provided benchmarks, including on a new variant of sequential MNIST. The paper claims ability to control speed-accuracy tradeoff in early classification, and applying the Wald SPRT on arbitrary sequential data. <sep> I enjoyed reading this paper: it combines two ideas (sequential likelihood ratio testing and density ratio estimation) in a way that appears obvious after the fact, but as often with these sort of post-hoc obvious ideas, is novel to my knowledge, and elegant. Trying to get something like the SPRT working beyond pairs of simple hypotheses has been under investigation for over half a century, and this paper is a worthwhile attempt. The empirical results look pretty good as well. <sep> At the same time, I think the paper does overstate the benefit of the theoretical connection to the classical SPRT and previous non-i.i.d. extensions, and doesn't engage sufficiently with the challenges of stopping rules. I discuss these issues in more detail next, and conclude with some minor points about presentation, background work, and analysis. <sep> Connections to the classical SPRT <sep> The paper claims to extend Wald's SPRT to non-i.i.d. data, and in particular its optimality properties. This includes claims in section 1 (on approaching Bayes-optimality and extending the Wald SPRT to arbitrary sequential data), at the beginning of section 4 (on how the LLLR enables performing the SPRT, which is provably optimal), and in section 5 (on approaching if not reaching Bayes-optimality). As far as I can tell, the paper does not enable performing the Wald SPRT on arbitrary data, and does not provide a provably optimal sequential test. The claim that it approaches optimality in any formal sense is likewise not supported as far as I can tell. Broadly, a sequential test consists of an update rule and a decision rule -- for the SPRT the update rule and decision rule are both optimal. For most extensions (non-i.i.d., multi-hypothesis, deadlines, etc), the update still follows Bayes' rule, and the optimal decision rule can only be found numerically (if at all), so some heuristic is given. This heuristic is often a fixed threshold, with asymptotic optimality guarantees). SPRT-TANDEM seems to be in the family of such extensions: it still applies Bayes' rule sequentially, and the stopping is given based on a fixed threshold. Thus, its optimality is asymptotic at best, and stronger claims are not supported. Furthermore, the paper doesn't make it clear whether the standard conditions for asymptotic optimality apply to the SPRT-TANDEM either: in my rough understanding, the standard asymptotic result is as risk goes to 0 (or equivalently, the LLR goes to infinity, and the threshold goes to infinity). I'm not sure that we know the SPRT-TANDEM LLR to grow in this way, and empirically, it seems like the LLR saturates to some fixed value, especially with high-order N, which means high threshold values are not achievable and risk cannot go to 0. We should also expect the SPRT-TANDEM to depart further from optimality as the model approaches the end of the video (since the optimal thing to do there is to gradually collapse the decision boundary, as the appendix reminds us). I recommend moderating these claims regarding optimality, and / or strengthening the results if possible. <sep> Stopping rules <sep> The paper does not provide guidance on stopping rules, which limits practical use, and does not report on the thresholds used to generate the speed-accuracy tradeoff figures. Presumably, the simplest thing is to set the threshold to the desired accuracy (which I think will do the right thing in the no-overshoot case?). Does this work for SPRT-TANDEM to achieve a given accuracy? If not, is there another heuristic that applies? I recommend addressing this question in more detail. Relatedly, the paper criticizes Mori et al. 2018 and Hartvigsen et al. 2020 for using a separate objective for determining stopping and accuracy, but in fact SPRT-TANDEM would likewise need some dynamic programming or RL solver to have an optimal stopping policy, similarly to that prior work. I recommend providing explicit guidance about stopping rules, and moderating the claims relative to prior work. Solving for an optimal stopping policy would also strengthen the paper. <sep> Presentation issues <sep> The paper tries to cram a lot into the short *CONF* format, supported by an extensive appendix. I appreciate the inclusion of classical SPRT results in the appendix, which may be unfamiliar to the *CONF* audience. At the same time, the main text does not provide much intuition about the novel LLLR loss, which is given very little explanation considering it is presented as one of the paper's major contributions. The relationships and improvements relative to KLIEP are presented too tersely, and a reader not familiar with that precise method will not know what to make of them. The paper would do better to provide more exposition there, perhaps in favor of moving the results tables to the appendix (since they show the same information as figure 3 as far as I can tell).In addition, the SAT curves are too busy, small, and hard to read. For the main document, I would recommend increasing font and symbol sizes, and presenting fewer orders of SPRT-TANDEM models (e.g. just order-1 and best-order), and fewer hitting times per model (e.g. there is no need to present the accuracy at every one of the last 10 frames if the accuracies are all the same there). Finally, the LLR trajectory figures can use partial transparency to make the individual traces easier to see. <sep> Additional/minor points <sep> Background work. I appreciated the fairly detailed review of past work related to the SPRT. A few notable missing pieces related to neuroscience are work predating Kira et al. 2015 in applying the SPRT to neural data (e.g. Gold & Shadlen 2002) and to human decision making more broadly (e.g. Stone, 1960; Edwards, 1965, Ashby 1983, and others). Missing work related to practical applications of the SPRT includes Johari et al. 2017, Ju et al. 2019, and others from the domain of internet experimentation. None of these are critical omissions, I only bring them up considering the already-broad review. <sep> Statistical analysis.  Given that the paper has a clear hypothesis (that SPRT-TANDEM outperforms competitors), it seems more sensible to perform repeated measures regression with planned contrasts rather than post-hoc testing for significance. This is a minor issue.","This paper presents a density ratio estimation approach to make the early decision for sequential data. The main contribution of this paper is the mathematical soundness of the proposed algorithm and all reviewers are unanimously positive about this paper with pretty good scores (7, 8, 6, 9, 7). However, despite the good scores, the verbal comments by the reviewers are not very strong except for one reviewer (R2); the reviewer with the highest score (9) did not provide detailed information about his/her rating. Also, the evaluation of this work is relatively weak because synthetic or simple datasets were employed for the experiment and the baseline methods are too straightforward. Also, it is not clear how the proposed algorithm can handle the data with sparse observations (data with idle times in the middle). Moreover, it does not provide rigorous stopping criteria although the authors proposed a simple method to determine the threshold, which is contradictory to the main objective of the proposed algorithm---making early predictions on sequential data---because the method requires ""plotting the speed-accuracy tradeoff curve on the test dataset."" This response implies that it at least requires a withheld dataset. Although this issue can be regarded as a separate problem, the paper could have provided an ablation study with respect to the criteria. <sep> Considering all these facts--high scores but relatively low supports and confidences, and practical limitations, I would recommend accepting this paper as a spotlight presentation."
"abstract | strength | rebuttal_process | decision  ==>  ==> Summary <sep> The paper introduces a new method for learning approximately sufficient summary statistics in the context of likelihood-free inference. To do so, neural networks are trained with a loss maximising mutual information, using a JSD surrogate estimator previously proposed in the literature. The authors show how their approach can for example be combined with SMC-ABC or SNL, leading to marked performance improvements on three problems. <sep> Score <sep> I appreciate the core idea of the paper as well as the empirical improvements over SMC-ABC and SNL. As detailed below, my main concern is the comparison to existing LFI approaches that are able to automatically reduce data dimensionality (SRE, SNPE). I hope the authors will address the weaknesses and conceptual question during rebuttal. As it stands, I view the paper as marginally below acceptance. <sep> Strengths <sep> The proposed method of learning summaries is novel and very relevant to many applications of LFI <sep> SMC-ABC+ and SNL+ show marked performance improvements <sep> Comparison of different mutual information estimators <sep> Concise discussion of their approach versus posterior-as-mean statistic used in some previous work <sep> Comparison to hand-crafted summary statistics (SMC-ABC', SNL') <sep> Weaknesses <sep> As mentioned above, I see the main weakness in the comparison to SRE and SNPE, which falls short in several ways:  <sep> The authors include comparison to SNPE-B rather than SNPE-C arguing that ""We select to compare with SNPE-B (Lueckmann et al., 2017) rather than the more recent SNPE-C (Greenberg et al., 2019) due to its equivalence to SRE shown in (Durkan et al., 2020)"". While Durkan et al. 2020 indeed show an equivalence in the loss functions of both approaches, this does not mean that they are the same and comparison is not warranted: While SNPE-C/APT trains a density estimator to estimate the posterior directly, a classifier architecture is used for SRE/AALR, combined with MCMC sampling. While these two approaches share similarity in the loss, they perform differently in practice, due to different training problems/inductive biases/algorithmic steps. Therefore, results using the newer SNPE-C version should be reported (which generally outperforms SNPE-B) <sep> No details on how SRE and SNPE-B were used are reported in the paper, i.e., it is unclear which network architectures were used, and whether these were appropriate for the problems at hand. I briefly checked the code in the supplementary material, but could not find hyperparameters for these methods in the respective notebooks either. In any case, this is central information which needs to be reported properly in the papers' appendix <sep> OU process: Judged by MMD instead of JSD, the performance of SRE and SNPE seems comparable to SNL+ on the OU model (Figure 6 Appendix), while this is not the case when looking at JSD results in Table 3. How can the discrepancy be explained? <sep> More generally, regarding empirical comparisons: <sep> In the figures reporting results it is not defined what error bars represent <sep> Have the authors checked whether JSD calculation is stable, i.e., yields similar results, when using a finer grid and more than 500 samples? <sep> Results on the MMD metric for the Ising model are missing from the appendix <sep> It would have been good to include a truly high-d problem, e.g. involving images <sep> Conceptual question <sep> In the appendix, the authors show the posterior directly extracted from the critic. How exactly were the plots shown for comparison and the JSDs calculated, and more specifically, could the difference in JSD be explained by calculating the normaliser on coarse grid? As a control, I would like to see a sample-based comparison, i.e., a comparison of samples from the approximate posterior extracted from T (e.g., by MCMC) versus samples generated from the NDE refit. If it turns out that results are comparable, wouldn't it make sense to think of the algorithm as a new LFI algorithm in itself rather than a pre-processing step reducing dimensionality of data for other algorithms? <sep> Additional comments <sep> As a suggestion: It might make sense to first compare and discuss SMC, SMC+, SMC' as well as  SNL, SNL+, SNL', and second, go on to compare SMC+, SNL+ versus SRE, SNPE <sep> Minor comments <sep> Figure 5 and 6 (Appendix): SNP should be SNPE for consistency with main paper <sep> Figure 6 (Appendix): OP should be OU for consistency with Figure 3 (main paper) <sep> Ticks in Figures 2c and 3c are too small to be legible <sep> The reference section should be checked carefully, to name a few examples: Hermans et al. (2019) has been published, Durkan et al. (2020) has been published, Kingma et al. (2014) has been published, Aaron Van Den Oord et al. (2016) misses a venue, Aaron van den Oord (2017) has been published, Rippel et al. (2014) misses a venue, Thomas et al. (2016) has been published <sep> Update <sep> The authors have addressed most of the above criticisms. Even though part of the definite answer about directly extracting the posterior from the MI network is left to future work and no test of their method on a high-d example was included, I believe that the paper is a valuable addition to the literature. I have increased my score accordingly to acknowledge the authors' replies.","This paper addresses a central problem in inference in implicit models-- classical approaches on such problems ('ABC') rely on computation of summary statistics, and multiple methods for automatically finding summary statistics have been proposed. This paper provides a fresh take on this classical problem, by providing a methods for finding information-maximising summary stats. The work is original, likely impactful, and carried out rigorously and carefully. The reviewers flagged some issues with empirical comparisons, as well as discussion or relevant work-- those issues mainly seem to have been resolved in the review process. Moreover, given the originality of the approach, and provided that the description of empirical comparisons and relationship with other work are carefully and conservatively worded, I believe this will be worth publishing even if it is not always the 'best' method on all problems."
"abstract | strength | weakness | strength  ==>  ==> This paper aims to indicate a new direction to conduct an adversarial attack: to delay the inference time/raise the computational costs of a ''multi-exit'' model. <sep> However, both the motivation and novelty are lacking, I will argue for the rejection if other reviewers wish to accept it. <sep> From the perspective of the industry: <sep> We do not actually use the so-called ''multi-exit'' architectures, because they could be difficult to optimize, quantize, assemble, and so on. For mobile devices, we deploy different ''single-exit'' architectures to meet their computing capacity and balance the performance. And it is even trivial to maintenance these architectures because we have developed automation tools. <sep> On the cloud servers, the deployment is redundant, such that the service will still work stably, even all the queries require the maximal flops. <sep> After all, it is easy to limit one's Queries Per Second (QPS), and nothing need to be worried about. <sep> From the perspective of academia: <sep> This paper only employs the PGD attack and the adversarial training on a specific task, the novelty is lacking. <sep> In section 5.2, if one knows the architecture and the training set and the task, how could this configuration being called ''black-box''. <sep> UPDATE: <sep> In the rebuttal, the authors emphasize two facts: <sep> There exist such IoT scenarios and real-time systems that employ multi-exit architectures (including those that employ cloud computation.). <sep> The slowdown attack is effective in these scenarios. <sep> However, to prove this method works in practice, it is not a simple ""1 then 2"", you need to show us ""1 and 2"". That is, you do actually deploy any system described in [1,2,3,4,5,6,7,8], and provide a feasible approach to attack with your method, and report the actual damage caused by your method, and convince the readers the damage is significantly severe compared to the efforts spent for causing the damage. <sep> Otherwise, it is only an application of PGD with a different loss function.","The paper discusses a new threat model for multi-exit DNNs: attacks against efficiency of inference. The proposed attack increases the inference time of such networks by the factor of 1.5-5, while at the same reducing the accuracy of attacked networks. Unlike classical adversarial examples, the new type of attack cannot be thwarted by adversarial training. <sep> Overall, the paper exhibits a novel contribution, is well written and methodically sound. Its practical motivation is somewhat weak, as it is currently unclear for which applications such attacks may be feasible. However, the novelty of the threat model addressed by this paper makes it an interesting methodical contribution."
"abstract | rebuttal_process | decision  ==> Title before Rebuttal: Novel idea for accelerating verification hampered by concerns on correctness of algorithm <sep> Summary of Contributions <sep> The paper proposes a method to verify local robustness networks with piecewise-linear activation functions, providing tighter bounds than existing approximate verification approaches while scaling to deeper networks than existing complete verifiers. <sep> Furthermore, unlike other approximate verification techniques, the method is able to show the existence of adversarial examples (in some cases). <sep> The method works by exhaustively checking each of the activation regions (that is, the convex polytopes on which the network is linear) that are fully or partially within the ε-ball for the presence of a decision boundary. <sep> If there exists a region containing a decision boundary for which the projection p of x to the decision boundary is within the ε-ball: <sep> An adversarial example exists if p is in the activation region <sep> The robustness status is unknown if p lies outside of the activation region <sep> Otherwise, if no such region exists, the sample is robust to local perturbations. <sep> Score Recommendation <sep> The method in this paper is novel and the results presented are compelling. However, it is currently marginally below the acceptance threshold (5) for me, as I am concerned about the correctness of the optimized tree-based exploration variant of FGP presented. Notwithstanding the other issues identified, if my concerns around the correctness are addressed and the presented results stand, this paper would be among the top 50% of accepted papers (8). Otherwise, it would be a reject (3). <sep> For the tree-based exploration approach to be correct, we need to prove that regions filtered out are unreachable or explored through a different path. The proof in A.2 needs to be expanded upon, and does not currently provide sufficient detail to demonstrate that this is the case. <sep> Specifically, consider the potential counterexample in the image linked: https://www.geogebra.org/geometry/ajzcrves. We have four polytopes A_prev, A, A_next, A bounded by ECG, DCE, FCD, GCF respectively. Every region that differs by one neuron is adjacent to each other. A_next is discarded when exploring from A, it will not be reached by any other path since A' will not be explored from A_prev (the boundary GC does not pass inside the ε-ball). I expect that some other property of the network means that this is not an actual counterexample, but do not see which specifically. <sep> Strengths <sep> Novelty: The paper presents a novel approach for verification that can be accelerated on GPUs. While other work to accelerate verification on GPUs exist (e.g. predicting dual variables for optimization using a neural network or expressing SDP solver algorithms as passes through the network being verified), the method presented simply exhaustively searches (very efficiently) each activation region. <sep> Results: To the best of my knowledge, the method: <sep> Advances the state-of-the-art in certifying local stability over perturbations of bounded l2-norm, with significant improvements in runtime (1-2 orders of magnitude) over GeoCert. <sep> Advances the state-of-the-art in providing tight certified lower bounds (up to 2 orders of magnitude) on l2-robustness radius <sep> Overall Clarity: This paper was a pleasure to read, and I wanted to make special note of this. I appreciated that thought had been put in in naming and defining terms (e.g. activation pattern, activation region), and found the diagrams in particular helped me understand the algorithm. Results were organized clearly <sep> Reproducibility: The authors took pains to provide details that make their results more reproducible (architecture, training hyperparameters and method, time budget, computational setup for verification). <sep> Weaknesses <sep> (The relative length of this section should not be taken as an indictment of this paper.) <sep> In addition to the issue with the optimized variant of FGP, here are some additional weaknesses that should be addressed. <sep> Clarity of Algorithm Description <sep> With reference to Figure 2b and focusing on C1, the penultimate paragraph of page 3 says ""For each constraint at distance less than ε from x, we enqueue the corresponding activation region if it has not been searched yet"". It would be helpful to emphasize in the paper that we are checking if the line C1 intersects the ε-ball, not just the line segment between A11 and A01. <sep> I could not find an explanation in the paper for how the decision boundaries are computed; this seems core to the efficiency of the approach. As far as I can tell, the method described linearizes the network f about the activation region (obtaining f′), and computes the decision boundary in f′ (not f), and then checks the distance to x. If this is the case, it should be explicitly specified, and a concise proof that this is correct added. <sep> Generalization to Other Norms <sep> The time comparison (""up to 5x faster than ERAN on models trained with PGD, and one to two orders of magnitude faster than ERAN on models trained using MMR"") is misleading without noting that the FGP returns ""unknown"" for a significant proportion of samples (6-15%) while ERAN+MIP returns a result in all cases. This should be noted in Section 3.3, rather than readers having to head to the appendix to determine this. <sep> While ERAN is a reasonably competitive verification method, I'd be interested to see a comparison with nnenum (CAV '20: https://github.com/stanleybak/nnenum) if possible, as I expect that nnenum may provide stronger results than ERAN. <sep> I would also love to see a comparison with VeriNet (ECAI '20 but this is not necessary as ECAI '20 did occur after Aug 2, 2020. <sep> Certified Lower Bounds <sep> Given that FastLin is designed to be quick but provide loose bounds, it would be reasonable to provide the mean runtime for FastLin (presumably ~1s) and FGP_LB (60s) in Table 1b, so that someone scanning the paper can understand that FGP provides better bounds at the cost of longer runtime) <sep> CROWN provides better certified lower bounds than FastLin without a significantly higher cost to runtime (see, for example, Table 4 of the original paper). Computing the mean bounds in Table 1b using CROWN (rather than FastLin) would be a better comparison of FGP to the state of the art. <sep> Reproducibility <sep> The first paragraph of page 6 states that ""measurements are obtained by evaluating on 100 arbitrary instances"". It would be helpful for reproducibility to provide the indexes of these instances. <sep> It would be nice to have the relevant code released. <sep> Questions for Authors <sep> With reference to Figure 1b: it seems possible to distinguish between the left and right cases by computing the projection of point p onto the ε-ball, p′, and checking whether p′ is within the activation region. This would allow the algorithm to avoid returning unknown in cases like the right. Is this incorrect, or was it merely challenging to implement? <sep> How did you handle the presence of multiple decision boundaries within a single activation region (e.g. one between category 1 and 2, and one between category 1 and 3) or does this provably not occur? <sep> Additional Feedback <sep> Figure 1 combines two related but different ideas (an illustration of the basic FGP algorithm, and an example of a case requiring FGP to return unknown when analyzing a boundary constraint) into a single figure. As a result, the figure was slightly confusing for me when I initially looked at it. I understand that this is probably done for space constraints, and the authors do attempt to use the spacing between the squares to distinguish between the two groups of squares, but perhaps a light vertical line (or some other visual aid) would help to distinguish more clearly. (I have a similar recommendation for Figure 2 is even less clear). <sep> The use of ""enriched"" in third paragraph of Page 5 is slightly confusing (is the queue somehow enriched, or is it the regions that are?). What about ""Instead, it is sufficient to use a regular queue of regions, augmenting each region with a field storing the layer the last flipped neuron belongs to""? <sep> The paragraph on ""depth scalability"" discussing Figure 3 seems to emphasize the ratio of regions searched between FGP and GeoCert. Given this, showing the y-axis of regions searched on a log scale might be appropriate since it enables readers to do the comparison themselves. <sep> The layout of the paper means that the first introduction in text of verified robust accuracy occurs after its use in Table 1. Given that it looks like there is enough space in the caption, it might be worth it to spell out what VRA is in the caption. <sep> For reproducibility, it would be helpful to indicate which commit / release of the MIP code you used to obtain the results. <sep> Post-Rebuttal Comments <sep> I've increased the rating for the paper from 5 -> 8 as the authors have addressed all my substantive concerns. <sep> Most critically, the authors demonstrated that, even without the tree-based exploration variant, FGP's performance improves significantly over the state of the art particularly for l2 networks. <sep> The authors have also significantly improved the clarity of the algorithm description, made the comparison in the section on generalization to other norms more clear, provided information in the paper allowing readers to understand that FastLin is significantly faster (but also significantly looser), and provided details that make reproducing these results far simpler.","The paper presents a sound and efficient (but not complete) algorithm for verifying that a piecewise-linear neural network is constant in an Lp ball around a given point. This is a significant contribution towards practical protection from adversarial attacks with theoretical guarantees. The proposed algorithm is shown to be sound (that is, when it returns a result, that result is guaranteed to be correct) and efficient (it is easily parallelizable and can scale to large networks), but is not complete (there exist cases where the algorithm will return ""I don't know""). The experiments show good results in practice. The reviewers are positive about the paper, and most initial concerns have been addressed in the rebuttal, with the paper improving as a result. Overall, this is an important contribution worth communicating to the *CONF* community, so I'm happy to recommend acceptance."
"abstract | strength | rating_summary | decision  ==> ##################################################################################### <sep> Summary: <sep> This paper proposes a new formulation of knowledge distillation (KD) for model compression. Different from the classic formulation that matches the logits between student and teacher models, this paper suggests to match the output features of the penultimate layers between student and teacher models, based on L2 distance. Two complementary variants are introduced, with one directly minimizing the distance of the original feature vectors and with the other minimizing the distance of the feature vectors projected on the teacher classifiers. The approach is evaluated in a variety of scenarios, such as different network architectures, teacher-student capacities, datasets, and domains, and compared with state-of-the-art results. <sep> #################################################################################### <sep> Pros: <sep> The proposed knowledge distillation formulation is simple. The paper is well written. Experimental evaluations clearly demonstrate the effect by directly matching the output features of the penultimate layers. <sep> #################################################################################### <sep> Cons: <sep> While the proposed approach is interesting, its novelty seems limited, with formulations being special cases of existing methods. In particular, as the authors also mentioned, the proposed L_FM loss is a simplified FitNet loss in Romero et al., which focuses only on matching the final representation without considering the intermediate representations. The proposed L_SR loss is similar to the standard KD formulation in Hinton et al., with the only difference that the pre-trained teacher's classifier is used for both teacher and student models. <sep> From the result tables, the performance improvement of the proposed approach is marginal, which is on par with existing works. <sep> As the authors mentioned, the L_SR loss is inspired by that only using the L_FM loss ignores the inter-channel dependencies of the feature representations h^S and h^T. So L_SR-CE is introduced. However, empirically, L_SR-CE is worse than L_SR-L2 which directly minimizes the distance between projected features. It seems that the empirical formulation is inconsistent with the motivation. <sep> In Eq 8, I assume that a separate classifier is also trained for the student model using the L_CE loss. How is the performance if we completely remove the student classifier? That is, the student model only consists of a feature extractor, and during inference time, we directly insert the pre-trained teacher classifier on top of the student model. <sep> It would be interesting to show the results when h^T and h^S have different dimensionality. <sep> In the ablation studies, the authors investigated ""where should be losses be applied"". How is the classifier generated for the intermediate layers? <sep> How does the performance change with respect to different settings of the hyper-parameters alpha and beta? <sep> The notation is inconsistent throughout the paper. For example, h^T and h^S are used in the first half of the method section, while h_T and h_S are used later. <sep> #################################################################################### Updated: <sep> The authors' rebuttal addressed my concerns and I lean toward acceptance.","This paper proposes a new idea for performing knowledge distillation by leveraging teacher's classifier to train student's penultimate layer feature via proposing suitable loss functions. Reviewers appreciate the simultaneous simplicity and effectiveness of the method. A comprehensive set of studies are performed to empirically show the effectiveness of the method. Specifically, the proposed distillation method is shown to outperform state-of-the-art across various network architectures, teacher-student capacities, datasets, and domains. The paper is well-written and is easy to follow. All reviewers rate the paper on the accept side (after the rebuttal) and believe the new perspective this work provides on distillation and its simplicity to implement can lead it to gain high impact. I concur with the reviewers and find this submission a convincing empirical work, and thus recommend for accept."
"abstract | strength  ==> Summary <sep> The paper describes excess risk bounds for ERM in the few-shot learning setting where one has access to n1 samples from each of the T source tasks and n2 samples from the target class. Under the assumption that there exists a ground-truth representation map, shared across all tasks, and a ground-truth task-specific linear map, that maps from the representation space to output space, the authors derive bounds that make use of all n1T data samples from the source tasks to bound the excess risk of the ERM. <sep> The bounds that are derived are for the following three settings: 1) low dimensional linear representation, 2) general linear map with ℓ-2 norm-based capacity control for the representation, and 3) linear map with a Relu non-linearity based representation. The key innovation is the structure imparted onto the input distribution of the source and target tasks where the assumption is that the input distribution of all the tasks covers the target task. The assumptions get progressively stricter for each of the three cases where for the low dimensional linear case, the assumption is that covariance of the target tsk is dominated by the covariance of all the source tasks, for the high dimensional case, the mean covariance is shared across tasks and finally for the neural network cases where the tasks share the same input distribution. These assumptions allow the improvement of the bound in Maurer et al. 2016. <sep> Strengths: <sep> Overall the paper is well written and motivated. The implication of the results are well discussed and makes for a good read. <sep> Under the assumptions on input distribution, results are an improvement over the iid task case and provides theoretical motivation for few-shot learning. <sep> Generalizes the Excess risk bound for the ERM in [1] for the low dimensional linear and the high dimensional linear setting. Extends the result to the ERM of 2 layer Relu network. <sep> Weaknesses <sep> The bound is valid when one has access to oracle ERM. This is not the case for most non-convex optimization. Especially in the case of the neural network extension. <sep> As pointed out by the authors, many results are very similar to concurrent work of [1] <sep> The paper lacks discussion with some prior work  [2] where the authors obtained results for task averaged excess risk. The result in this work is not for new task but since the authors make strong assumptions on input data distribution and task model the result seems like a natural extension of the results in [1]. <sep> Discussion and concerns <sep> Both Theorem 5.1 and 6.1 make the coherence assumption on the target task model. I understand that it allows us to get a bound in terms of |Θ∗| but the implications of this assumption is not well discussed in the paper. This looks like assumption 4.3 for Theorem 4.1. Any understanding of how the results would change if we don't make the assumption 4.3 and the coherence assumptions? <sep> EDIT: Added references <sep> [1] Nilesh Tripuraneni, Chi Jin, and Michael I Jordan. Provable meta-learning of linear representations. arXiv preprint arXiv:2002.11684, 2020 <sep> [2] Pontil, M., & Maurer, A. (2013, June). Excess risk bounds for multitask learning with trace norm regularization. In Conference on Learning Theory (pp. 55-76).",The paper considers the problem of learning a new task with few examples by using related tasks which can exploit shared representations for which more data is available. The paper proves a number of interesting (primarily theoretical) results.
"abstract | strength | weakness | decision  ==>  ==> Summary <sep> This paper propose to study the Lipschitz constant of convolutional layers and to give an easy to compute and differentiable upper bound. The upper bound is composed of 4 different bounds, based on tensor unfolding of the Jacobian, and taking the min of these 4 values. This upper bound is then used to train networks with spectral norm regularization and compared with network trained with singular value clipping from Sedghi et al. (2019). The proposed bound gives similar performances with much cheaper computational time. <sep> Overall assessment <sep> The title is misleading a bit. The bound is not on all the singular values but on the largest one. I would update it to Fantastic four: differentiable upper bound on the Lipschitz constant of convolutional layers. <sep> The derivations for the 4 bounds are very straightforward from the results from Sedghi et al. (2019) as it is simply different unfolding of the hollow tensor that is obtained with the Jacobian of the convolution layer in the Fourier domain. The Jacobian is given in the preceding paper so the technical contribution is not very large. <sep> My main concern with this paper is that the proposed evaluation is not sufficient to show the benefit of the proposed method IMO. <sep> First, I don't understand why the author choosed to compare singular value clipping with the spectral norm regularisation. Indeed, computing all the singular value will be much more expensive than computing a bound on the largest one. A proper evaluation with the same regularisation, using for instance the low number of power iterations proposed in Ryu et al. (2019) would be more convincing to show the computational benefit as well to compare the potential accuracy loss due to the approximation. <sep> Then, the results seems to be provided on single run of the method and not average on multiple random init. Averaging (and giving the standard deviation) would allow to asses the difference in stability of the proposed result. <sep> The weight decay is ""selected using grid search"" but there is no mention of validation set or the score used to select this parameter. This feels like the weight decay has been selected to maximize the test error, which is not a proper method to select such parameter. <sep> I don't understand why the running time of the proposed method does not change between layer of size 64x64x3x3 and 512x512x3x3 in table.1. As the complexity of the bound is at least linear in the total shape of the filter, the running time should be mutliply by 64, which would give similar runtime as the one in Ryu et al (2019). <sep> Minor comments, nitpicks and typos p.3: The corresponding Jacobian matrix -> relative to the input. An introduction of the layer as a function ϕ(x)=w∗x and the definition of the Jacobian J=∂ϕ∂x(x) would probably help in this regard. <sep> Table.1: this table could probably be reduced to make more space for experiements p.4: the notation section introduce many notations that are not used in the core of the paper. This should be trimmmed down to the minimum. For instance, I don't think that ϕ′(z),ϕ″(z) or the per layer z(I),a(I) are used in the core of the text. <sep> p.5: O(1/p)bound is obtained on the error: which error are the authors referring to? This should be properly introduced and discuss. <sep> p.7: In the last paragraph of 4.2, the results are first given in order ""no weight decay/ with with decay"" and then in reverse order. This is confusing to read and changing the order would improve the readability. <sep> Sedghi et al. (2018): The paper was pucblished in *CONF* 2019, the proper citation is:@inproceedings{Sedghi2019, <sep> title={The Singular Values of Convolutional Layers}, <sep> author={Hanie Sedghi and Vineet Gupta and Philip M. Long}, <sep> booktitle={International Conference on Learning Representations}, <sep> year={2019}, <sep> url={https://openreview.net/forum?id=rJevYoA9Fm}, <sep> } <sep> Ryu et al. (2019): The paper was published in ICML 2019:@InProceedings{pmlr-v97-ryu19a, <sep> title = {Plug-and-Play Methods Provably Converge with Properly Trained Denoisers}, <sep> author = {Ryu, Ernest and Liu, Jialin and Wang, Sicheng and Chen, Xiaohan and Wang, Zhangyang and Yin, Wotao}, <sep> pages = {5546--5557}, <sep> year = {2019}, <sep> editor = {Kamalika Chaudhuri and Ruslan Salakhutdinov}, <sep> volume = {97}, <sep> series = {Proceedings of Machine Learning Research}, <sep> address = {Long Beach, California, USA}, <sep> month = {09--15 Jun}, <sep> publisher = {PMLR}, <sep> pdf = {http://proceedings.mlr.press/v97/ryu19a/ryu19a.pdf}, <sep> url = {http://proceedings.mlr.press/v97/ryu19a.html} <sep> } <sep> Other references should be checked. <sep> Section D: The complex part here is not to compute the gradient of the convolution (which is simply computed with the correlation) but the shape of the gradient of the spectral norm of the Jacobian. This should be updated. <sep> p.14: will give a{n,} desired. <sep> p14/15/26: Eq.6/10/13/17 are the same (multiplication is associative and l/m are inverted in eq 10). The repetition is misleading as the reader search for the difference which is simply the ordering of terms. All these bounds would be much easier to derive/read if using tensor notation. Basically, what is done here seems to simply be computing the spectral norms of tensor slices unfolded along different dimensions.","The authors provide four rigorous upper bounds on the operator norm of the linear transformation associated with a 2D convolutional layer of a neural network. One of these is a heuristic proposed in earlier work by Miyato et al, and widely used, so, among other things, their result provides theoretical context for that method which will be of broad interest. All four of their bounds can be efficiently computed and have easily computed gradients, so they propose using the minimum of the four bounds for various purposes. Since, for standard architectures, the Lipschitz constant of a network can be bounded above by the product of the operator norms of its layers, there are a variety of applications of differentiable bounds on these operator norms. They show that their new bound is sometimes much tighter than the bound of Miyato et al, and can be computed much more efficiently than two known methods for exact computation. The paper is written well, which will facilitate future work building on this work. The analysis builds on earlier work, but insight was required to obtain the new results; the fundamental novelty of the mathematical development was confirmed by an expert reviewer. <sep> While they experimentally compared the accuracy of their approximations to those of the method of Miyato, et al, the case for the practical utility of their method would have been stronger if they had shown that their regularizer led to better results for some tasks. However, I believe that the paper should be accepted purely on the basis of its theoretical contribution, which enhances our understanding of this important topic, and, even if it cannot be directly applied, seems like to inspire practically useful methods in the future."
"abstract | strength  ==> The authors propose a framework for learning a commentary or teacher model that provides helpful information during training. Under this framework, a teacher model learns to provide meta-information, referred to as a commentary, <sep> on a given training example, with the goal of improving the student network <sep> (e.g., improving convergence speed or robustness). The learning of the teacher model is done by minimizing the validation set loss of the student model under parameterization from a training loop that used the commentaries. <sep> The optimization of the teacher parameters involves backpropagating through gradient descent steps. The authors propose doing this directly in small toy examples, or using an approximation using the Implicit Function <sep> Theorem for more realistic problems. <sep> The authors then demonstrate that the commentary learning framework can be used in a variety of ways: <sep> learning to provide example weights, learning a blending policy for data augmentation, and learning to provide an attention mask for image classification. <sep> In the latter case, they show that using the attention masks leads to a more robust student when the backgrounds are modified to spuriously correlate particular class labels on the training/validation sets but not on the test set. <sep> This paper has many strengths. It introduces an interesting meta-learning framework and demonstrate its flexibility to implement three different kinds of commentary for improving a student network. Additionally, it supports its statements with a variety of interesting experiments. While I would like to see this paper accepted, it could be improved in its organization and explication. <sep> For example, it is not immediately clear that the commentary network is learned with equations 1 & 2, but the student network in this process is ultimately discarded and only after the finished teacher network is obtained, is a student network actually trained for evaluation purposes. An ""illustrative example"" of commentaries is put in the appendix instead of the main paper which feels like a lost opportunity to holistically explain the algorithm and clear up these misconceptions. <sep> In the example weighting curricula it is not clear what is meant by curricula. <sep> In an ablation study, example weights produced by a teacher network with and without curricula are compared, but what constitutes the curricula here is never defined. I'm inferring that in the curricula case, the teacher network produces weights using both the example x and the current training iteration i, while the non-curricula case only uses x. This should be stated explicitly somewhere. <sep> In equations 1 and 2 the authors write optimize(phi/theta)[loss func] <sep> when argmin_phi/theta loss func would convey what the authors intend while using standard notation. <sep> On the masking experiments, the masks are used both at training and test time which seems somewhat odd. The commentary formalism proposed is supposed to be limited to learning a commentary model to facilitate training. In this case, <sep> the commentary model seems less like a separate teaching model and more like another component of the classification model, similar to ""Recurrent Models of Visual Attention"" where a component of the classifier determines where to focus in an image. How well does the student network trained with masks provided by the teacher do on the validation/test sets without masks?","This paper proposes an interesting unified framework for meta-learning with commentaries, which contains information helpful for learning about new tasks or new data points. The authors present three kinds of different instantiations, i.e., example weighting, example blending, and attention mask, and show the effectiveness with the extensive experiments. The proposed method has a potential to be used for a wide variety of tasks."
"abstract | strength | decision  ==> Nice paper. The main idea in this paper is to use a specific kind of data augmentation, Mixup (Manifold Mixup), in order to improve the effectiveness of the KD process and obtain better performing student models, especially in cases where not enough data is available on the target dataset and task. <sep> While the idea is interesting in itself, I think what is proposed in this paper, is very relevant to this paper (noisy student): https://arxiv.org/abs/1911.04252 <sep> So, methodologically it's not super novel, but it should also be taken into account that many of the developments in this area have happened very recently. Moreover, the paper still has an added value since it is applying these techniques for a different input modality and in a slightly different setup. <sep> Summary of the experiments: <sep> -12 layer BERT is fine tuned as the teacher (different fine-tuned teacher for each task). <sep> Two different architectures are used for student models (6 layer BERT and 3 layer BERT). <sep> Student models are initialised by copying the lower layers of the teachers. <sep> The teacher is used to provide pseudo labels for the student model on the target dataset, while the target dataset is augmented with the manifold mixup approach on the embedding layer (Proposed Approach). <sep> The proposed approach is compared with plain KD and no KD (just fine-tuning on target). <sep> My Questions: <sep> During KD, do you feed original examples from target to the teacher then apply mixup, or do you first apply mixup then feed the generated examples to the teacher and get the pseudo labels from the teacher? <sep> Most importantly, It is not clear to me from the experiments whether there is still an advantage of doing this, if the teacher is trained well enough e.g., with the same type of augmentation (intuitively, if the teacher is trained well enough and KD process is well tuned, all the information that the model can gain by doing additional data augmentations should already be transferrable through the soft targets from the teacher.) <sep> Some positive point about the paper: <sep> Ablation experiments are conducted to separate the gain from simple KD with KD+Mixup. <sep> Some of my concerns: <sep> The improvements in the accuracies reported seem marginal and I think there are indication of the significancy of the results in the paper (e.g., mean and variance over several trials?). <sep> Some minor points: <sep> Figure 2: Mixup and Non Mixup examples (circles and triangles) are not clearly differentiable (just hard to see). <sep> I think the type of data augmentation applied is a special case of Manifold Mixup, so it would be nice to cite this paper as well:  https://arxiv.org/abs/1806.05236","This work explores the distillation of language models using MixUp for data augmentation. Distillation with MixUp seems to be novel in the narrow context of distilling language models, although it has been used before in different contexts as the reviewers point out. The results of the experimental validation are encouraging, and the application is valuable and of wide interest to the *CONF* audience. I therefore recommend accepting this paper for a poster presentation."
"abstract | strength | ac_disagreement | rebuttal_process | decision  ==> This paper introduces the over-squashing problem in GNNs. The problem is more evident in the graph-structured data where both local neighbors and long-range interactions are required. The paper designs experiments to show that the existing extensively-turned, GNN-based models suffer from the over-squashing problem in long-range prediction tasks. It also shows the improvement of breaking this bottleneck by adding a fully-adjacent layer. <sep> Developing deep GNNs is a popular topic and attracts significant efforts. However, training a deep GNN may face with several problems. Besides the over-smoothing, this paper introduces a new type of bottlenecks: over-squashing. It happens when the GNN needs to pass the information of interactions from long-range neighbors. In this case, exponentially increased information is squashed into a fixed-length vector. I personally believe this bottleneck is quite crucial when we develop deep GNNs. <sep> The overall quality and clarity of this paper are good. It clearly and detailedly explains the over-squashing in long-range problems, and distinguishes it from other similar bottlenecks. It also tries to show the existence of the over-squashing in four different types of datasets. The training details and results are also included. Meanwhile, to my best knowledge, this is the first paper that highlights and analyzes the over-squashing problem in GNNs in detail, and I believe this bottleneck is of vital importance when we design deep GNNs. Therefore, I think the originality and significance of this work are also quite high. <sep> This paper proposes a solution to the over-squashing, which is to add a fully-adjacent layer to the extensively-tuned state-of-the-art model. By comparing the performance of the modified models and the state-of-the-art models in the four datasets mentioned above, we can clearly see an improvement in terms of the accuracy and the error rates. However, although the authors say that this solution is very simple and mainly for the demonstration purpose, I believe there may be a potential problem within this solution. When we add the fully adjacent layer to the original model, we actually change the data directly. That means the modified model and the original model are trained on different datasets. In this case, I am concerned that we cannot compare these two models directly.  Although the authors clarify that the improvement is not from the under-reaching by considering the graph's diameter in the QM9 dataset, I am still not fully convinced that we can safely rule out the possibility that the improvement is simply because the changed data makes the tasks easier for GNNs. Since the paper shows the existence of the over-squashing by comparing the performance of these two types of models, I believe it would be much better and more convincing if it can propose a new solution that can avoid changing the data directly. <sep> Overall, this paper has the following pros and cons. I believe this paper is of high quality and clarity with supporting experiments and analysis. The bottleneck of over-squashing is also a new problem and worth considering when we develop deep GNNs. But the solution proposed by the authors may be potentially problematic and hence, weaken the results and the conclusions.","The paper identifies the phenomenon of oversquashing in GNNs and relate it to bottleneck. While this phenomenon has been previously observed, the analysis is new and insightful. The authors conclude that standard message passing may be inefficient in cases where the graphs exhibit an exponentially growing number of neighbors and long-range dependencies, and propose a solution in the form of a fully-adjacent layer. While the paper does not offer much methodologically, it is the observation of bottleneck that is of importance. <sep> We therefore believe that the criticism raised by some reviewers of the observation not being novel and the solution ""too simple"" rather unsubstantiated. The authors have well addressed these issues in their rebuttal. The AC recommends accepting the paper."
"abstract | rating_summary | weakness | decision  ==> The present paper aims to understand the generalization capability of self-supervised learning algorithms that fine-tune a simple linear classifier to the labels. Analyzing generalization in this case is challenging due to a data re-use problem: the same training data that is used for self-supervised learning is also used to fit the labels. The paper addresses this issue by implicitly conditioning on the training covariates x and then deriving generalization bounds that depend only on (hypothetical) noise to the labels y. The paper show that, empirically, the dominant factor in generalization error is a certain quantity called the ""memorization gap"", which can also be upper-bounded via theoretical analysis (the theoretical bound seems to be loose by about a factor of 4 compared to the empirical measurement, but is still non-vacuous in many cases). Interestingly, this is not the case for standard supervised learning, likely to the higher-complexity models used to fit the labels; in that case the memorization gap is high, but a different gap (called the ""rationality gap"") is large in magnitude but negative. <sep> Overall, the paper is clearly presented, innovative, and has interesting empirical and theoretical results. It seems like a clear accept to me, with my only uncertainty that I am not completely familiar with the related literature. I am also not sure why the authors could not use the Rademacher complexity---are there theoretical obstacles to using it to upper-bound generalization error in this setting, or is the problem that it is too large? If the latter, then have you considered using your approach in settings other than just the self-supervised setting in order to improve on Rademacher complexity bounds? <sep> ==== <sep> Framing comments / questions: <sep> -I don't like the word rationality, since it has a technical meaning in Bayesian statistics that is not the same as the usage here (i agree they are somewhat similar in flavor, but I think it's confusing to conflate them). <sep> -I'm not sure it's correct to say that SS+S is a dominant methodology. In practice we would almost always do full fine-tuning on the self-supervised representation, rather than just the final layer. Still, starting with final layer fine-tuning is a reasonable start for analysis. <sep> -It seems an important point of your analysis is that we can condition on x and then just look at label noise for measuring generalization. It seems like empirical Rademacher complexity bounds also condition on x, so is there a fundamental difference here? (I think you try to address this in Remark 3.3 but I didn't understand your point there.) <sep> ====== <sep> A few presentation comments: <sep> -I didn't understand this claim: "" An optimal Bayesian procedure would have zero rationality gap, and indeed this gap is typically zero or small in practice."" <sep> -Drawing lines between the dots (and shading the area under the curve) in Figure 1 is inappropriate, since the different points don't follow a logical linear progression (it's really just a scatter plot). <sep> -In Fact I, why do we need to take the max with zero? The result is still true even without the max, I believe. <sep> -In Fact I, it would be helpful to comment on the effect of changing eta. Do we expect certain of these quantities to get bigger or smaller in that case? Any heuristic intuition for how to choose the best eta? <sep> -Section 2.1 is a bit dense. <sep> -I liked Figure 2 a lot.","The paper offers a new take on generalization, motivated by the empirical success of self-supervised learning. Two reviewers found the contribution novel and interesting, and recommend acceptance (with one reviewer championing for it). Two reviewers remain skeptical about the value of the paper, and the authors are encouraged to add a discussion about the points made in these reviews. <sep> I agree with the positive reviewers and would like to recommend acceptance."
"strength | weakness | suggestion  ==> In this paper, the authors study the problem of training fair machine learning models through the lens of bi-level optimization. In particular, they propose a method, denoted FairBatch, that adaptively selects different batch-sizes for different protected groups to impose a certain measure of fairness. This is achieved by solving an outer optimization problem that imposes fairness by computing a batch-size ratio for different groups. This batch selection is then used to train the original model. The proposed approach can impose several prominent fairness measures, among which: equal opportunity, equalized odds, and demographic parity.  To demonstrate the efficiency of FairBatch, the authors conducted several experiments on synthetic and real datasets. The results show the proposed method is easily implementable, scalable and can achieve comparable performance when compared to existing methods. <sep> Pros: <sep> The authors approached the problem of fair machine learning by iteratively adapting the batch-size ratio for different protected groups. The approach is novel and was implemented through a novel bi-level optimization formulation. <sep> The proposed method is easy to implement, scalable, and can readily improve fairness for any pre-trained model. <sep> Clarity: The main body of the paper is concise and clearly written with very few typos. <sep> Cons: <sep> The theoretical part of the paper requires a more careful and detailed analysis. First, the inner minimization problem in section 3.1 has wλ appearing in the objective. I think this should be w instead. This also affects the proof of Lemma 2 in the supplementary material. Moreover, the author computes the gradient of the objective of the outer optimization problem, F(λ). Being a finite max, this function might not be differentiable. <sep> It seems that positive definiteness assumptions in sections A.1 and A.4 are not easy to check. The authors did not comment on the practicality of these assumption nor they provided a method to check whether they hold. Moreover, I believe these assumptions should be moved to the main body of the paper. <sep> I believe the paper is worth publishing after re-working the theoretical part. <sep> Minor Comments: <sep> Page 3: In defining the bi-level optimization problem, the objective function of the inner minimization is a function of w. This should be explicitly included in the formulation (it is currently hidden in yi^) <sep> In the proof of proposition 2 in Appendix A, should it be ℓ(|1−y|,⋅)=1−ℓ(y,⋅)? <sep> In Sections A.2 and A.4, the subscript of the functions fi and gi are missing in some expressions (also in Lemma 1). <sep> The related material are referenced and well-discussed in the paper. The authors clearly positioned their work in the related field and discussed their contributions in comparison to other similar works. <sep> Update after the author's response: <sep> I was not convinced by many of the authors' responses. <sep> While the author(s) agree that F(λ) is non-differentiable, they keep the gradient updates in the paper with a footnote referring to sub-gradient methods in (Boyd et al. 2004)? Moreover, it is not clear what the sub-gradient would refer too when the loss function is non-convex, what would Hλ refer to in section 3.2? One possible way to solve this issue might be through a mild smoothing technique. But as presented the theoretical part is not rigorous. <sep> The positive definiteness assumption in Lemma 1, although will most probably hold when the three functions are convex; this limits the application of the results as typical ML loss functions involve non-convexity. <sep> My comment on the subscript of the functions fi and gi in Lemma 1 was addresses by adding a comment mentioning that the subscript will removed for the simpler notation. I believe one subscript does not make the notation complex. Rather defining two notations might confuse the reader. <sep> Despite my tendency to have this novel idea and work published, I believe the authors need to be more careful in writing and dealing with the theoretical section of the paper. I will hence decrease the score to 4.","All the reviewers and I agree that the proposed approach is interesting and the paper is overall well written. However, I agree with R3 that the paper need further re-working the theoretical part (see the post-rebuttal comments of R4). Thus, I would encourage the authors to carefully address the comments of the reviewers in the revised version of the paper, which would ultimately improve the quality of the paper."
"abstract | weakness | rebuttal_process | decision  ==> I stand by my initial review that this is a strong submission, and having read through the other reviews and author responses, I am raising my confidence level as well (I think I have a solid grasp of this work's potential import). I disagree with critiques of the paper's novelty and practicality -- I think it provides new insights into OOD problems with substantive theory (not common) and provides actionable insights to boot. Also, the the revised manuscript is much improved. I hope this gets accepted. <sep> This submission presents a rigorous analysis of a subset of ways in which machine learning models can fail when encountering out-of-distribution (OOD) samples (often referred to as train/test skew or as train/scoring skew in industry). As the paper notes, the topic has received a great deal of attention, particularly under other guises (""domain adaptation""). However, much of that attention has aimed at pragmatic or heuristic solutions (various tricks to design or learn ""invariant"" features), while our fundamental understanding of what goes wrong in OOD situations remains incomplete. This paper aims to fill those gaps in understanding by studying simplified settings, and asking the question: why does a statistical model learn to use features susceptible to shift (""spurious"" features) when the task can be solved using only safe (""invariant"") features. After formulating five constraints (guaranteed to hold true for easy-to-learn tasks), they go on to show that failures come in two flavors: geometric skew and statistical skew. They analyze and explain each in turn, while also providing illustrative empirical results. <sep> I like this work a lot (though I am more lukewarm on the paper itself, see below), and barring discovery of a fatal flaw during the discussion, I would advocate with some enthusiasm for its inclusion in the conference. The paper's claims are stated at the bottom of page 2 as: <sep> Careful design and articulation of ""easy-to-learn"" settings in which there are few, if any, unmeasured variables that could confound the findings (a weakness in previous work on this topic). <sep> Identification of two (but not the only two) distinct types of OOD failures that occur even in easy-to-learn settings, in the form of necessary and sufficient data ""skews."" <sep> Experimental evidence to illustrate and support the analyses from (2.), along with enlightening discussion. <sep> I agree with the paper's claims, though I admit that I was not previously familiar with, e.g., Sagawa 2019 or Tsipras 2019, and so cannot confidently situate this work amongst related research. I also feel my understanding may still be somewhat superficial -- I buy its arguments but don't have a particularly strong intuition yet for the two flavors of skew (particularly in non-toy settings). <sep> This work has a very strong scientific flavor (not always true of machine learning research): I would liken the restriction to carefully designed ""easy-to-learn"" settings to a well-designed laboratory experiment in which there are few, if any, unmeasured variables that could confound the findings. It is very elegant and satisfying to read and think about. I would anticipate that this paper will inspire a lot of follow-up work, in which other researchers adopt the ""easy-to-learn"" and ""skew"" framework and terminology and even utilize the specific experimental designs in this paper. After all, machine learning researchers love adopting intellectual frameworks and benchmarks that they can build upon rapidly. <sep> The ""easy-to-learn"" constraints articulated in Section 3.1 are sensible and clearly stated, and I am unable to find fault in them thus far. I agree with this statement on page 5: ""any algorithm for solving OoD generalization should at the least hope to solve these easy-to-learn tasks well."" <sep> The experiments were thoughtful and well-designed, and their results are presented effectively: each plot, it seems, illustrates a particular point or supports a specific argument in the paper. For example, I like how Figures 2 and 3 serve as visual summaries of the geometric and statistical skew sections, respectively. A reader (particularly a savvy one familiar with the relevant related work) could probably skip Sections 4 and 5 (three pages total!) and still get the high level idea simply by skimming the plots and reading the captions of those two figures. <sep> The largest weakness I perceive concerns the clarity and accessibility of the writing: for example, the connection drawn in Section 4 to the work on norms in over-parameterized neural nets is very interesting, but I'm not sure the text fully succeeds in further connecting it to OOD settings. In particular, certain details of the ongoing discussion of majority and minority groups aren't entirely clear (to me, at least)...are minority group samples available during training, just in smaller number? In that case, what is the OOD ""shift"" -- the prevalence of the minority group at test time? <sep> Likewise, I'm not sure I really connected with the takeaway in Section 5 -- is it that early in the optimization, the ""spurious"" weights get updated repeatedly by an amount proportional to the spurious correlation, and that it then takes a long time to undo these updates, if at all? The statistical skew section is definitely more abstract and perhaps a little harder to connect to practical settings, vs. geometric skew where the bridge is the previous work on ""norms."" <sep> My recommendation is to accept this submission, and at the moment, I am willing to advocate for it. However, it is entirely possible I am missing (or misunderstanding) key details, and so I am eager to discuss with the other reviewers.","This paper studies the reasons for failure of trained neural network models on out of distribution tasks. While the reviewers liked the theoretical aspects of the paper, one important concern is about the applicability of these insights to real datasets. The authors added an appendix to the paper showing results on a real dataset that mitigates this concern to an extent. Further, there are interesting insights in the paper to merit acceptance."
"abstract | weakness | rebuttal_process | strength | rebuttal_process | decision  ==>  ==> Summary: <sep> This paper tackles the problem of improving exploration in deep RL for procedurally-generated environments, where state-of-the-art exploration techniques typically fail. In the proposed approach, called RAPID, each agent-generated episode is evaluated with respect to its local exploration score (for the given episode), global exploration score (across all previous episodes), and extrinsic reward obtained. Episodes with high scores are stored in a replay buffer, and a policy is trained via behavioral cloning on batches of state-action pairs from this buffer. This policy is also used to produce the agent-generated episodes. <sep> Recommendation: <sep> The approach is simple and intuitive, and empirically improves exploration in procedurally-generated environments. Training on procedurally-generated environments is becoming more common and useful, for instance in domain randomization for sim-to-real transfer in robotics, so this approach would be relevant for the *CONF* audience. I have some concerns and questions (detailed below), but overall I recommend acceptance. <sep> Pros: <sep> The approach itself is simple and easy to implement. <sep> The paper is clearly writtten and well-motivated. <sep> The empirical evaluation is thorough: RAPID is compared against a suite of state-of-the-art baselines for exploration bonuses, exploration in procedurally-generated environments, and a self-imitation approach; there are also ablation studies and hyperparameter sensitivity studies. The ablation study comparing behavioral cloning versus exploration bonuses (where the per-episode exploration scores are given as part of the reward to the agent) is particularly interesting, given that existing approaches typically rely on the latter. <sep> The empirical evaluation is on a variety of domains, including both discrete-action and continuous control tasks. <sep> Cons: <sep> I would like to see an empirical comparison against Never Give Up (NGU; Badia et al. 2020), which also uses episodic novelty and global novelty to guide exploration. Although NGU uses the same environment for training and testing, because it takes into account how controllable a state is, it wouldn't suffer from the limitation highlighted in Figure 1 (where a state is randomly generated, regardless of the agent's action), and may do well in procedurally-generated environments. <sep> It seems strange to me that single state-action pairs are stored in the replay buffer, rather than keeping all state-action pairs from an entire episode together. It's possible that a particular state-action pair may only be good in terms of exploration, in the context of the rest of the agent's trajectory in that episode. <sep> The continuous control experiments for MuJoCo locomotion tasks are contrived, since the extrinsic reward for forward progress is summed and given at the end of the episode, which just doesn't make sense for locomotion tasks, and unnecessarily hampers the baseline approaches. Using a goal-reaching continuous control task would be more relevant: e.g., Swimmer in a procedurally-generated maze. <sep> I would like to see a discussion of limitations and failure cases for RAPID. <sep> There are minor grammatical errors and typos throughout the paper. <sep> Questions: <sep> Can the local score and global score be applied to observations (e.g., image observations) directly, instead of states? <sep> Are the scores associated with state-action pairs in the replay buffer taken into account during sampling of batches, or ignored? <sep> How were the weights chosen for the total score, that is a weighted sum of the extrinsic reward, local score, and global score? The weight for the global score is very small compared to the others (i.e., 0.001 versus 0.1 and 1). Why is this the case? What happens when this weight is higher (e.g., is there some type of suboptimal behavior that emerges)? <sep> Why is it necessary to anneal the imitation learning to zero for some environments, but not others? <sep> What is the state space for each of the tasks, that's used to compute the local and global scores? It would be useful to have a table in the Appendix that summarizes this.","In order to learn good exploratory behaviors in settings where agents encounter diverse environments, the authors propose an approach which involves learning from episodes that exhibit good episode-level exploratory behaviors. The innovation is in the scoring and learning from these episode-level behaviors rather than trying to come up with shorter timescale proxies of exploration. In making this concrete, the authors propose to score trajectories based effectively on state coverage within an episode (i.e. good exploration corresponds to good state coverage) as well as by scoring episodes relative to one another and giving preference to episodes that explore less often encountered states. To learn, the core algorithm interleaves standard RL updates with behavioral cloning updates using the best episodes of data, thereby training the policy to both solve the task and explore well at the episode level. <sep> A weakness is that the paper uses low-level state in grid worlds and there is some ambiguity in applying this to settings with continuous states. The authors discuss general strategies for dealing with these limitations as potential future work. <sep> The reviewers were positive about the clarity of the text and felt the core idea that was proposed was simple and effective. The authors put in solid effort to address reviewer concerns. The most salient remaining concern, which I share, is that there will be challenges in scaling this approach to more complex environments with continuous state/observation spaces. <sep> Overall, this paper had a consensus ""accept"" rating (7,7,7,6), and I endorse this as my decision."
"abstract | strength | weakness | rebuttal_process  ==> ########################################################################## <sep> Summary: <sep> This paper studies the problem of dense text retrieval, which represents texts as dense vectors for approximate nearest neighbors (ANN) search. Dense text retrieval has two phases. The first phase learns a representation model to project semantically similar texts to vectors of large similarity scores (e.g. inner products or cosine similarity scores). The second phase adopts an ANN search algorithm to index these vectors and process queries. The paper claims key contributions at the first phase. Specifically, (1) The paper introduces a better negative sampling method to sample good dissimilar text pairs for training.  (2) The new method enables faster converge of model learning. (3) The new method leads to 100x faster efficiency than a BERT-based baseline, while achieving almost the same accuracy as the baseline. <sep> ########################################################################## <sep> Reasons for score: <sep> Overall, I like the idea of this paper and opt for a weak accept. A carefully designed negative sampling method should be able to outperform baselines that use simple heuristics. The efficiency improvement 100X is very promising. However, the paper can be better in experimental comparison and presentation. For experimental comparison, a stronger baseline using dense vectors should be included to strengthen the performance claim. For presentation, many important terms require clear definitions, without which the performance gain is not understandable. It will be good if the authors can address the above two issues in the rebuttal. <sep> ########################################################################## <sep> Pros: <sep> The paper proposes a novel negative sampling method. Based on the method, the paper proposes a new dense text retrieval framework ANCE. ANCE introduces an asynchronous index refresh to select the most dissimilar text pairs for training in a timely manner. <sep> The proposed ANCE achieves faster model training and equally accurate text retrieval when compared with a number of baselines. In a TREC 2019 task, ANCE achieves the best NDCG score against 11 baselines. <sep> The authors promise to make code open source. That will greatly improve the reproducibility of the work.  The code, together with its performance, will serve as a new state-of-the-art for future study. <sep> ########################################################################## <sep> Cons: <sep> An important baseline is missing. In section 5, the paper describes Baselines. According to the descriptions, all baseline use BM25 to retrieve samples for training. BM25 may not be the best for a strong baseline since it relies on sparse word tokens. An alternative is to use BERT [CLS] dense vectors of all texts and a similarity search algorithm such as locality sensitive hashing as the retriever. It will be good if the authors can add the baseline to the paper. <sep> The paper does not explain clearly why the proposed method runs faster than baselines. The experimental results support that the proposed method outperforms several baselines. However, the paper does not explain the performance superiority. I am not sure about which of dissimilar text pairs selection or index refresher or others in the proposed negative sampling leads to the superiority. The reason for uncertainty may be due to the lacking of definitions in the paper. For example, ""BERT rerank"" refers to a baseline but it has not a definition in the paper. The input and output of ""BERT rerank"" remain unclear. Similarly, ""TREC 2019"" is an important benchmark but it has not definitions related to the inputs and outputs. It is necessary to explain important concepts for the best readability of the paper. <sep> ########################################################################## <sep> Questions during rebuttal: <sep> I would like to see some experiments or discussions to clarify the above cons.","The paper explores how to effectively conduct negative sampling in learning for text retrieval. The paper shows that negative examples sampled locally are not informative, and proposes ANCE, a new learning mechanism that samples hard negative examples globally, using an asynchronously updated ANN index. <sep> Pros • The problem studied is important. • Paper is generally clearly written. • Solid experimental results. • There is theoretical analysis. <sep> Cons • The idea might not be so new. The contribution is mainly from its empirical part. <sep> During rebuttal, the authors have addressed the clarity issues pointed out by the reviewers. They have also added additional experimental results."
"abstract | suggestion | strength | weakness | suggestion  ==>  ==> Summary <sep> This work proposes a new stochastic algorithm for distributed optimization. I really enjoyed reading the paper. It has a strong theory, nice experiments with several objectives and parameter-sensitivity tests, and the writing is sufficiently good. This authors did a great job comparing both theoretically and numerically to the most relevant literature. The paper builds on top of the recent successes in distributed optimization and two particularly popular approaches: quantization and decentralized topology of the network. The paper's main weakness, in my opinion, is that the algorithm is a bit hard to understand at first, and it has a number of extra parameters, but this seems to be a common issue for decentralized algorithms, and the experiments show that their tuning is not difficult. Another issue is that the paper might be too technical for an unprepared reader, and the theorem statement is hard to parse. <sep> Detailed comments <sep> The paper presents a clear motivation for developing a new algorithm by explaining the shortcomings of the existing ones. For instance, D-PSGD is explained to not converge precisely when the data is heterogeneous; error-feedback has a slow rate due to the delayed compensation of errors; DCD-PSGD is mentioned in Remark 1 to be too aggressive and to perform worse numerically. I think that the thorough comparison to the prior work is one of the main strengths of this paper. <sep> I think the algorithm is not explained well. Its design consists of multiple pieces: a consensus algorithm based on NIDS/D^2, a compression scheme based on Diana, and the gradient updates of SGD, all of which are given together immediately. The authors present the algorithm without any warm-up, and there are several ways to fix it. Firstly, the meaning of the variables that are presented in the algorithm could help to understand it. Before presenting the full algorithm, it makes sense to mention that D^k is a dual variable needed to ensure stationarity at the optimum and that its role is to estimate the gradient there. Similarly, the communication procedure is not obvious and the meaning of H^k is unclear when just looking at the algorithmic steps. I do think that the paper would benefit from introducing some of the concepts before the algorithm. My first impression was a lot of confusion, mainly because of the communication procedure. Maybe it's best if the authors explain what happens when the algorithm is fully centralized, what it boils down to, and why it works in that case. <sep> Theorem 1 assumes that the gradients are bounded uniformly over the space. A number of recent papers showed that SGD works even if the variance is bounded only at the optimum, for example, (Gower et al., ""SGD: General Analysis and Improved Rates""). Do the authors think that it's possible to relax the assumptions for LEAD as well? <sep> Can the authors present a complexity bound based on Theorem 1 that would show explicitly every term? I think it should be something O((C*kappa+beta)log(1/eps) + kappasigma^2/eps) and I'd hope to see a comparison to the bound of (Koloskova et al., 2019). <sep> The authors should provide a reference for the claims around Assumption 1, such as the eigenvalue bounds. For instance, (Xiao and Boyd, ""Fast linear iterations for distributed averaging"") or any other material where the properties of mixing matrices are explained. <sep> The convergence rate recovers that of NIDS as mentioned in Remark 3. Does it also recover the right rate in other special cases, for instance, when the algorithm is fully centralized (W=I)? <sep> In figure 4, please clarify if you use the running average of the losses or the full train loss. <sep> Typos: <sep> Corollary 1: the expression for condition number has u instead of \\mu in the denominator. <sep> p.7, ""before uniformly partitioned"": should be ""before being uniformly partitioned""","The paper introduces LEAD, a decentralized optimizer with communication compression that can achieve linear convergence rate in the strongly convex setting. In terms of novelty, the authors should still add a discussion of Magnusson et al., 2019, On Maintaining Linear Convergence of Distributed Learning and Optimization under Limited Communication, which is a related linear convergence result in the deterministic (full gradient) case, and relates to the analysis here which is stochastic but also exploits the deterministic case. Nevertheless, reviewers reached consensus-with communication compression in the given time-that the paper in its current form is well written and the results are presented clearly in both experiments and theory (which builds up on the earlier NIDS algorithm). The presentation of the algorithm can be slightly improved. We hope the authors will incorporate the remaining smaller open points such as mentioned by R1, such as making the constants in the convergence bounds explicit when comparing with other methods."
"abstract | strength | rating_summary  ==> Sample-dependent dropout for prediction and confidence improvements <sep> Quality: <sep> Pros. The authors assume that the sample-dependent dropout rate is necessary to improve model performance in aspects of prediction and confidence. In the Bayesian framework, they propose a contextual dropout module that is carefully designed considering computational efficiency since it is prone to increase model capacity and computational cost. <sep> Cons. Figures and tables are intentionally resized to fit the smaller areas. Some text is hard to read. Please increase the font size of the text in the figures for a printed paper. <sep> Clarity: <sep> Pros. The manuscript is well-written and straightforward to understand the proposed method. <sep> Cons. In Section 2.3, why do we have a multi-dimensional tensor for a fully-connected layer, instead of two-dimensional U^l? (in ""Contextual dropout module for fully-connected layers"" paragraph.) <sep> Originality: <sep> Pros. It explores the context-aware dropout method, while the other dropout-related works do not, with a few extra parameters. <sep> Including the ImageNet and the VQA experiments is helpful to illustrate the applicability of the proposed method. <sep> They show that the contextual dropout can successfully apply to attention networks. <sep> Cons. The sample-dependent dropout has a resemblance to the squeeze-and-excitation module (the authors also mentioned in Section 2.3). In the related work section, more discussion on it would be helpful to recognize its novelty. <sep> Sample-dependent dropout inevitably needs a sub-network to model a conditional distribution q of z for a given x. For this matter, it would be fair to compare with the squeeze-and-excitation module or self-attention models (with a small hidden dimension for parsimony; with the weights from sigmoid function). <sep> In the proposed work, if the output of a sigmoid function is used for the real-valued weight (without the ARM estimator), instead of binary-sampled weight, does it significantly underperforms? <sep> What expected in rebuttal: <sep> (1) I would like to know that the ARM estimator in the proposed work is a critical factor, where the real-valued output of a sigmoid function is readily used for the dropout mask (as a weight). One could see this work as a variant of self-attention networks instead of a new dropout technique. In this viewpoint, the authors should defend with more persuasive logic (and experiments if possible) for their originality. <sep> (2) For the VQA experiment, MCAN (Yu et al., 2019) gets 67.2 in the original paper with a standard dropout module with the same setting as described in the Appendix. To be fair, this report should be included. Then, the improvement from a standard dropout is reduced to 0.22 (Bernoulli contextual dropout gets 67.42 with extra parameters.) <sep> Minor comments: <sep> Footnote 1 has extra space before the period.","This paper proposes an input-dependent dropout strategy, using variational inference to infer the rates. While the idea is a fairly straightforward variant of recent probabilistic dropout methods, the paper demonstrates consistent improvements across several types of NN layers (dense, convolutional, and attention) in large-scale experiments (e.g. ImageNet). The reviewers unanimously agreed on accepting the paper."
"abstract | strength | rebuttal_process | strength | weakness  ==> In the context of neural text generation, the authors study how perplexity varies with top-k and top-p sampling and propose a sampling algorithm that uses Zipf's law to dynamically adjust k in order to control per-sequence perplexity. <sep> Overall, the theoretical analysis and relationship between log probability and repetition was interesting, but there are several concerns with the method and experimental evaluation, detailed below. The idea is interesting and I hope the authors continue down this line, but in its current form I would not recommend acceptance. (edit: see discussion below, I have adjusted the score to above the acceptance threshold) <sep> Pros <sep> The theoretical analysis of cross-entropy growth with top-k versus top-p was interesting (e.g. summarized in Figure 1). <sep> Nice empirical demonstration of repetition correlating with log probability. <sep> Clarity <sep> The presentation in Section 2 could be simplified or made more concrete - overall it seems like this section is building up to a standard definition of perplexity in a complicated way. <sep> are these generic definitions of cross-entropy rate and perplexity (defined using the Shannon-McMillan-Breiman theorem) needed? I don't see them used in the main text, so it would be helpful to shorten this section, or concretely say how each step corresponds to a language model. <sep> Equation (3) assumes that PN is a stationary ergodic source. Why can a neural language model be considered a stationary ergodic source? <sep> Why surprise instead of information content? Surprise rate is not used again in the text. <sep> In the abstract you say ""target value of perplexity"" but then τ is called the ""target surprise value"", and in Appendix A it reports ""target average surprise value"". In this review I'll use 'perplexity', but it would be helpful to check whether there are inconsistencies in the paper. <sep> Method <sep> Dependence on hyperparameter. The authors mention that for low values of k and p, perplexity drops, leading to repetition, while for high values of k and p, perplexity increases, leading to incoherence. The authors claim that Mirostat avoids both traps. However it requires setting a target value (τ). What is the difference between having to choose k or p versus choosing τ? Wouldn't Mirostat fall into the traps with low τ or high τ (e.g. Figure 4.d)? Since you showed that perplexity grows linearly with p (Fig 1), why is Mirostat needed versus using top-p? <sep> Fixed perplexity per continuation. Mirostat enforces the average token log-probability of each individual continuation to be near a hyperparameter τ. However, won't the ""ideal"" perplexity vary based on the prefix? I.e. for some prefixes there may be low conditional entropy in the true distribution, meaning a small number of high-probability continuations are much more reasonable than others. In this case, generating a sequence with perplexity based on τ would filter out these high-probability continuations. Could the authors comment on this issue? The underlying assumption of the method is that it is a good idea to have a fixed perplexity for all continuations. <sep> Zipf's motivation. While I understand that the Zipf's law assumption was needed to derive the theoretical results, it's unclear why Zipf's law is used to motivate the practical method (Algorithm 1). Why would we want to estimate the zipf's exponent on the top-100 words at each timestep, and choose k using (7)? This motivation, and a comment on the guarantees it gives us on full sequences, should be more clearly stated. <sep> Experiments <sep> Simple 'perplexity target baselines'. Related to the ""Zipf's motivation"" comment above, what's missing is evaluating different ways of controlling the perplexity, in order to evaluate that the proposed method based on Zipf's law is the best (for some definition of best). If the goal is to control the perplexity of each sequence, why not sample several sequences with top-k and choose one that has perplexity close to a target τ? What is the performance of adjust k or p based on a different heuristic, e.g. absolute difference between the perplexity of the sequence-so-far and τ? <sep> Human evaluation. Human evaluation is required to get a full measure of the generated text's quality; currently the paper just argues for quality/coherence by showing a few examples. It's possible that this form of dynamic k adjustment introduces some artifacts. For instance, the behavior of the ""surprise"" in Figure 5.d under Mirostat doesn't resemble that of humans, so it's possible that some odd behavior is introduced. <sep> Misc. In figure 5, why is mirostat preferable to top-k or top-p? Why are k,p of 0.4 and 1.0 and τ=1.8 selected here?","This work presents a novel approach to improving text decoding. This is backed up by a solid analysis of cross-entropy growth with top-k vs top-p and an interesting demonstration of repetition correlating with probability. The paper is well written and well organized. The authors' rebuttal was effective in convincing the reviewers. The human evaluation (added during the rebuttal phase) is a good demonstration of the effectiveness of the approach and so this paper's proposed decoding algorithm is likely to be impactful. <sep> Pros: <sep> Well written. <sep> Solid theoretical analysis of cross-entropy and its relation to top-p and top-k decoding. Good demonstration of how repetition is related to probability. <sep> Interesting, novel and effective decoding algorithm. <sep> Human evaluation of the algorithm's output. <sep> Cons: <sep> The approach has not been tested with a variety of language models. <sep> Decoding quality still depends on a target perplexity which may need to be tuned. <sep> Unnecessary dependence on Zipf's law in the basic decoding algorithm."
"abstract | strength | weakness | suggestion | strength  ==> This paper presents a new adversarial attack targeting the votes derived from the primary capsules of a capsule network. It demonstrates that this is a stronger attack against capsules networks than an attack directly optimizing the output logits of a capsule model. This paper has some novel contributions to the literature but can be improved in a few areas. <sep> The paper spends a fair bit of time discussing the efficiency of their attack but does not meaningfully motivate this focus. They claim that the time it takes to compute adversarial attacks against capsule networks may have contributed to the claimed robustness, but all the papers they cite which discuss the adversarial robustness of capsule networks measures the success rate with respects to the number of attack optimization steps as opposed to optimization time. Furthermore as shown in table 3, their new attack does not significantly decrease the attack creation time for most optimization strategies. I believe the focus on attack optimization time should be removed from the paper entirely, and the space can be dedicated to some of the results relegated to the appendix such as black box attacks, or the class conditional reconstruction detection. <sep> By creating an attack that specifically targets the votes of the primary capsules this paper is able to increase the success rate of adversarial attacks against capsule networks. It does this however, by effectively not attacking the capsule part of a capsule network. Rather they have created an attack which optimizes the features extracted by the CNN feature extractor used to derive primary capsules. This is an interesting finding, and illustrates that capsules networks are more vulnerable to adversarial attack than perhaps previously believed due to their reliance on CNN feature extraction, but it is important to note that it does so by optimizing for activations of non-capsule components of a capsule network. This is not a major flaw of the paper, but i believe it warrants some discussion, space providing. <sep> This paper also presents the success rate and undetected rate of their new attack against the class conditional reconstruction attack presented by Yao et al. This section improves the paper, but there are some omissions. Namely they do not in the main text nor in the appendix visualize the resultant attacks. This is an issue as both Yao et al (2020) (Detecting and diagnosing adversarial images with class-conditional capsule reconstructions) and  Yao et al (2020) (Deflecting Adversarial Attacks) show that the undetected attacks often resemble the target class, even under small epsilon bounds. This paper also does not address the additional defense mechanisms presented in Deflecting Adversarial Attacks, which were shown to drastically increase the attack detection rate, specifically in colour datasets such as cifar10. <sep> This paper would be improved by addressing those 3 main issues. <sep> smaller issues, <sep> the text describing figure 2 is confusing, and i am not entirely sure what the point of the figure it. perhaps more space could be dedicated the distinguishing the b and c plots and discussing why this motivates the work. <sep> there is a missing equation link in section 4 <sep> there is a missing table link in appendix E <sep> ----POST AUTHOR RESPONSE -------- <sep> 1.) Efficiency of our Vote-Attack: <sep> It is clear that the vote attack is more time efficient than other attacks, but there is no clear motivation for this improvement. To my knowledge the attack creation time has never been a barrier to adversarial research, nor has it prevented real world adversarial attacks. As a result this focus of the paper simply confuses the reader, be spending time addressing an issue that is not important in research or practical settings. <sep> 2.) Optimizing for activations of non-capsule components of a capsule network: <sep> In the authors response they discuss the semantic meaning of the votes of capsules. This too is a bit of a red herring. Although when discussing the motivation behind capsules, the potential for semantically meaningful capsule votes is invoked, there is nothing in the training procedure that ensures that the activations of the capsules correspond directly to features that humans would find semantically meaningful. In my original review i mentioned that by not attacking the output of the capsules after the routing procedure, this attack was simply optimizing for representations extracted from a standard neural network. In this way this work is similar to the representation attacks first presented by [1] which showed the success of representation attacks on standard neural networks. <sep> 3.) The undetected attacks often resemble the target class, under the class-conditional capsule reconstructions detection: <sep> Th addition of the attack visualizations is an improvement but the authors do not specify which attacks are successful and undetected and a few of the visualized attacks do indeed resemble the target class. <sep> 4.) The additional defense mechanisms presented in Deflecting Adversarial Attacks: <sep> The authors are right to point out the scope of the paper, and it is perhaps unreasonable to expect this paper to address these defence mechanisms, but their inclusion would greatly strengthen the paper. <sep> [1] Sara Sabour, Yanshuai Cao, Fartash Faghri, and David J Fleet. Adversarial manipulation of deep representations. In *CONF*, 2016.","This paper studies the robustness of CapsNets under adversarial attacks. It is found that the votes from primary capsules in CapsNets are manipulated by adversarial examples and that the computationally expensive routing mechanism in CapsNets incurs high computational cost. As such, a new adversarial attack is specially designed by attacking the votes of CapsNets without having to involve the routing mechanism, making the method both effective and efficient. <sep> Strengths: <sep> This is the first work which proposes an attack specifically designed for CapsNets by exploiting their special properties. <sep> The proposed vote attack is more effective and efficient than the other attacks originally proposed for CNNs rather than CapsNets. <sep> The paper is generally well written. <sep> The experimental study is quite comprehensive. <sep> The code will be made available to facilitate reproducibility. <sep> Weaknesses: <sep> The study is mostly for only one type of CapsNets. It is not clear whether the observations in this paper still hold generally for other types of CapsNets even after some additional experiments have been added. <sep> The presentation of the paper has room for improvement. <sep> The authors are recommended to proofread the references thoroughly to ensure style consistency such as the consistent use of capitalization, e.g. <sep> ""Star-caps"" -> ""STAR-Caps"" <sep> ""ieee symposium on security and privacy (sp)"" -> ""IEEE Symposium on Security and Privacy (SP)"" <sep> Despite its weaknesses especially those pointed out by Reviewer 2, this paper would be of interest to other researchers as it is the first paper that studies adversarial attacks on CapsNets."
"abstract | ac_disagreement  ==> Overview <sep> Inspired by DeepEMD's obersvation that compositional representations generalize better for few-shot image classification, this paper (COMET) introduces ""Concepts Embeddings"" components to the Prototypical Networks. ""Concepts Embeddings"" are part-based representations and are learnt by a set of independent networks {fθi} (can also share weights). <sep> For each novel class, its N concept prototypes are computed using the support set, and a query example is classified by measuring its distances to each novel class's N concept prototypes. <sep> Evaluations and experiments are carried out on 3 datasets: CUB, Tabula Muris and Reuters. The proposed COMET approach obtains favorable performances on all 3 datasets. <sep> Pros <sep> This work explore ways to learn compositional representations in a semi-supervised fashion.  It takes Prototypical Networks as baseline and adds a novel ""Concept Embedding"" component.  The authors show the proposed approach can significantly improve ProtoNet baseline on 3 datasets. It also outperforms the recent approach DeepEMD. The following observation on the CUB dataset is very interesting: ""Strikingly, by adding just one most frequent concept corresponding to a bird's beak on top of the whole image concept, we improve ProtoNet's performance on CUB by 10% and 5% in 1-shot and 5-shot tasks, respectively."" <sep> Cons <sep> Using multiple prototypes per class has been explored in ""Infinite Mixture Prototypes for Few-shot Learning"" (ICML'19), maybe the authors should mention it and highlight differences. <sep> It's unclear where does C comes from by just reading section 2.2.  How to compute C is later elaborated in experiments section 3.1. Essentially C are hand-crafted for each dataset. Maybe the authors should hint this in section 2.2. <sep> The math symbols are a bit confusing in section 2.2. For instance, inconsistent notation fθ(j) in section 2.2 and fθj in Figure 1, also D's definition is not given (is it equal to the number of pixels × number of input channel?). <sep> I appreciate the authors provinding many interesting ablation studies, but the main evaluation (Table 1) misses 2 important image few-shot classification dataset/benchmarks: miniImageNet, and MetaDataset.  How does the COMET with ""Unsupervised concept annotation"" performs on the above two datasets? <sep> When compared with ensemble, why specifically 5 ProtoNets? How many concepts are used in COMET and COMET (shared weight)? This ablation should be included in main paper, as network capacity can improve accuracy significantly, see: ""A Closer Look at Few-shot Classification""(*CONF*'19). <sep> Reason for the decision <sep> This paper explores using compositional representations in Prototypical Networks. The authors introduce a novel concept embeddings idea, and show that it can significantly improve ProtoNet baseline on 3 datasets. However using multiple prototypes per class is not a new idea and the authors should highlight the differences with prior works (what's the strength of this particular formulation?). For the evaluation, I highly suggest the authors to include standard mini-ImageNet and Meta-Dataset FSL image classification benchmarks and report performances on them. <sep> I find the following observation on the CUB dataset  very interesting: ""Strikingly, by adding just one most frequent concept corresponding to a bird's beak on top of the whole image concept, we improve ProtoNet's performance on CUB by 10% and 5% in 1-shot and 5-shot tasks, respectively."" That means human-validate concepts are complimentary to the representations learnt by a 4-layer network, but how about using a deep/stronger backbone, would it narrow the margin? I suggest the authors add this ablation study: How much does adding a concept help w.r.t to the depth of the backbone.","The paper introduces ""Concept Embeddings"" to Prototypical Network, which are part-based representations and are learnt by a set of independent networks (which can share weights). The method first computes the concept embeddings of an input, and then takes the summation of the distances between those concept embeddings and their corresponding concept prototypes in each class to estimate the class probability. The experiments validates the proposed methods on 4 benchmarks in three different domains, including vision, language and biology. For the biology task, the authors also develop a new benchmark on cross-organ cell type classification. The key novel idea of transferable concepts results in significantly improved generalization ability over the existing few-shot learning methods. <sep> Although some reviewers raised concerns about not using other few-shot image classification datasets such as MiniImageNet these are not appropriate benchmarks, as the method requires the ""part-based concepts"" to reasonably span the space of all images which is a characteristic of fine-grained image classification problem. Although this does limit the scope of the method, the fact that it is applicable for multiple tasks is a strong counteragument to the claim that it is too limited, so overall I disagree with the assessment of one reviewer that the choice of benchmarks is insufficient."
"abstract | strength | rebuttal_process | strength | decision  ==> The paper provides a number of adversarial attacks on hybrid neural-symbolic systems. The systems are recommender and QA systems which use an underlying knowledge-graph (KG) such as ConceptNet. Previous work has suggested that the KGs are important for good performance, and moreover that the use of KGs lends the system a degree of interpretability. The attacks are successful - maintaining performance whilst seriously degrading the KG - throwing doubt on these claims. <sep> Two main approaches to attacking the systems are followed: a simple heuristic method in which labels in the KG are modified randomly, and a more sophisticated method in which deep reinforcement learning is used to learn an optimal policy to change the KG whilst maintaining good performance on the task. <sep> Overall I have a lot of sympathy for the motivation of this paper. There is increasing evidence that the attempted use of symbolic methods in hybrid systems, and in particular the use of symbolic methods to provide explanations, is just picking up on incidental properties of the data and exploiting the power of the deep network in ways unrelated to the symbolic representations. This paper provides further compelling evidence. <sep> However, the paper is currently an extremely frustrating read, and it took me a number of attempts to get through the paper to write this review. Part of the problem is that the authors have tried to cram in too much material. I would have preferred to have seen fewer experiments described, but in more detail, with the remainder briefly mentioned in a paragraph or two, or perhaps in an appendix (if that's allowed for *CONF*). <sep> The other problem is that the presentation in the paper is poor. Part of that is down to the non-native English in parts (which is not the fault of the authors), but part of it is also just down to sloppiness in the presentation. <sep> More detailed comments <sep> A KG is denoted as G = (E, R, T ), where E and R are the entity set and relation set respectively - this is a little confusing since T is also the set of relations (tuples over E). R is the set of relation labels? <sep> Finally, both c and k are concatenated for calculating the plausibility score - [presuambly the concatenation is then put through an MLP?] <sep> In both graph encoders, the subgraph G(q,a) together with the aggregation weights - [neither of the descriptions mention aggregation weights] <sep> while the task is to predict the unobserved interaction (yuv = 0). - <sep> [does the zero here mean that the interaction is unobserved, or that the user has not engaged with the item? (or both?)] <sep> where the aggregation weights are personalized for u - [you need to define how the aggregation works] <sep> We randomly choose two triples (edges) in the KG and swap their relations. - [it would be good to see some actual examples here from one of the applications/KGs.] <sep> where sG(·) is a KG scoring function trained on G.  - [is this defined anywhere?] <sep> The font in table 1 is really small. If you get more room in a final version please turn this into two tables. <sep> in-house test accuracy - what's an ""in-house"" accuracy? <sep> We then compute the relation specific clustering coefficient vector c^r - do you define this anywhere? <sep> OpenBookQA (OBQA) - do you give a reference? <sep> We randomly select 10 questions - this seems like a small sample on which to peform the human evaluation. This evaluation would be more persuasive with a larger sample (even 20 or 30). <sep> Typos etc. (not exhaustive) <sep> despite a deceptive symbolic structure - [I'm not sure that <sep> ""deceptive"" is the right word here (and lots of places elsewhere), <sep> just something like ""incorrect"" may be better.] <sep> that KG -> the KG <sep> The preliminary results indicate that KG can be easily manipulated and lost its benefit -> lose <sep> It brings more worrisome scenario - [rephrase] <sep> without noticeable performance drop -> without a noticeable performance drop <sep> In specific -> More specifically e.g., commonsense question answering (QA) and recommender system, -> <sep> i.e. <sep> aggregating its neighbors' embedding -> aggregating its neighbors' embeddings <sep> This heuristic changes the semantic -> semantics <sep> This heuristic also does not perturb KG's structure but its semantic <sep> -> This heuristic also does not perturb KG's structure but its semantics on both of commonsense QA and recommendation system tasks -> on both commonsense QA and recommendation system tasks since one of our goal -> goals have captured the information of KG. -> the KG <sep> sequentially to LSTM -> sequentially to the LSTM <sep> Evaluating the KG on downstream task -> Evaluating the KG on a downstream task <sep> Relation Swapping(RS) -> Relation Swapping (RS) <sep> but keep the relation distribution -> keeps <sep> We also leverage the validity scores given by human -> humans ranging from gradient based(Chen - [space] <sep> AutoEncoder based(Chen - [space] <sep> which can lead to corrupt explanations -> which can lead to incorrect explanations our RL-RR method always yield -> yields between entities instead of the semantic -> semantics that investigate into the problem -> that investigate the problem","The paper's main message is that some existing NLP techniques that claim to improve performance by the use of a knowledge graph may not achieve this improved performance because of the knowledge graph or at least the explanation given may be questionable. This is thought provoking and it will incite the community to think more carefully about the real factors of improved performance. The initial version of the paper was not well written, but the authors improved the writing significantly. The paper includes a thorough empirical evaluation to support the main message. I have read the paper and I believe that this work will be of interest to a diverse audience."
"abstract | strength | decision  ==> Many Information Retrieval systems rely on two components: a retriever that identifies a small set of ""support"" documents from a large corpus, followed by a reader that re-scores these support documents more finely. For retrievers, metrics like BM25 were once common but they are increasingly replaced by machine learned components. However, most datasets do not provide direct supervision information for the retriever. <sep> Assuming a Transformer reader, this paper proposes to train the reader and retriever iteratively, using cross-attention scores from an increasingly better reader as a way to identify increasingly better sets of support documents. The novelty of this paper is the use of reader cross-attention scores as a proxy to train the retriever. The models are otherwise standard. <sep> This paper is well motivated and clearly written. It seems to be improving the state-of-the-art on three datasets and over several recent baselines. The training procedure (including hyper-parameters) is detailed, facilitating reproducibility. <sep> Questions and comments: <sep> Since DPR plays a central role in this paper, it deserves a longer introduction, either in Section 2 or later. Further discussion on the use of already trained models as starting point would be interesting. <sep> Similarly, it would be nice to summarize the negative example sampling strategy in Section 3.5. <sep> On the NarrativeQA dataset, the authors mention ""it is not straightforward to train the retriever with heuristics using Q-A pairs"". Isn't BM25 (used for the initial set of documents) a heuristic? Can this be clarified? <sep> I assume the gains against the baselines are statistically significant, but it would be nice to mention in writing. <sep> An iterative training procedure is going to be slower than other approaches. Some discussion on this point would be interesting. Or referencing a discussion in a previous paper would also be OK. <sep> Are there any other downsides to this approach? <sep> Minor comments: <sep> In the third paragraph of the first section, it sounds like the authors introduce the retriever/reader architecture, which is standard (and otherwise clear from the rest of the paper). <sep> Typo, page 2: well know -> well known <sep> Please introduce d (embedding dimensionality) in page 3. <sep> Please add a citation in the first row of Table 3.","The paper attempts to improve retrieval in open domain question answering systems, which is a very important problem. In this regards, the authors propose to utilize cross-attention scores from a seq2seq reader models as signal for training retrieval systems. This approach overcomes typical low amount of labelled data available for retriever model. The reviewers reached a consensus that the proposed approach are interesting and novel. The proposed approach establish new state-of-the-art performance on three QA datasets, although the improvements over previous methods are marginal. Overall, reviewers agree that the paper will be beneficial to the community and thus I recommend an acceptance to *CONF*."
"decision | strength | decision  ==>  ==> == Summary == <sep> The paper proposes a contrastive learning approach for self-supervised learning in which multiple heads are trained to be invariant to all but one type of data augmentation. The rationale is that different downstream tasks may require different types of invariances (e.g. we may want to be rotation invariant for pictures of flowers, but not for pictures of animals), and one does not know a-priori which kind of invariances will be required. After training multiple representation heads, one can later concatenate them or use the general embedding (the input to all the variant-specific heads) for the downstream task. <sep> == Pros == <sep> The authors try to tackle an important problem of self-supervised approaches: how does one decide which data augmentation strategies to use when the downstream task is not known in advance? This question has not been properly addressed in the literature, and can be of great important for the real application of self-supervised strategies beyond academic benchmarks. <sep> The method that the authors introduce scales well with the number of data augmentations used (linear), and avoid a combinatorial explosion that could arise. <sep> The paper is generally well written and the algorithm is explained quite clearly, and illustrations are used appropriately to help the reader understand the proposed approach (Figure 1 and Figure 2). <sep> == Cons == <sep> Once the self-supervised pre-training finishes, one has to decide whether to use the general embedding space in the downstream task, or a concatenation of the different variant-specific embeddings. However, from the results reported in Table 2, 3 and 4 it's not clear which approach is better, and this is downstream task-dependent. This is unfortunate since it basically introduces another hyperparameter to tune for each downstream task. <sep> Many experiments do not report any measure of variance or statistical significance, and do not follow an an ""standard"" setting. This makes really hard to tell whether the observed increase in accuracy is statistically significant or not. For instance, the authors use IN-100 and ON-13, two subsets of the ImageNet-1k dataset which this reviewer has never seen before, and thus the results are really hard to interpret. The only results that show some measure of variance (standard deviation) are with Flowers-101. <sep> Update after discussion: Authors pointed out that IN-100 is in fact used in other works. During the discussion they also provided additional standard deviation measures for CUB-200. Although I believe that reporting standard deviation of multiple runs and/or confidence intervals for the results should be the standard practice, I acknowledge the effort made by the authors running additional experiments to accommodate this demand at least for some of the datasets that they use. <sep> When training using all (3) data augmentations, the results in Table 4 don't suggest that LoCo improves upon MoCo in any significant way. The same applies for LoCo++ vs. MoCo++. Some of the reported accuracies are indeed slightly higher than the baseline, but the differences are small. In addition, for the only dataset for which the authors report confidence intervals, these are greatly overlapping in most of the cases. <sep> The authors restricted their experiments to a ResNet-50. A few experiments showing that their approach works for other modern architectures would be appreciated (e.g. DenseNet, EfficientNet, Inception). <sep> Update after discussion: The authors provided additional results using a ResNet-101. I appreciate the effort of the authors running these extra experiments. <sep> There are plenty works using contrastive losses for doing self-supervised learning, yet the authors decided to compare only against the MoCo baseline. Additional baselines would be also appreciated. This reviewer acknowledges that using MoCo would probably be sufficient if the results were significantly better. <sep> Update after discussion: The authors argue that MoCov2 is the strongest baseline at the moment of submission, which is publicly available and can run in affordable resources. This is a perfectly valid point. <sep> == Reasons for score == <sep> I believe that the authors aim to tackle a very important question in self-supervised learning. However, the proposed approach fails to deliver the expected results, according to the results shown in Table 4. Other tables show LoCo and LoCo++ shine on top of the baseline (MoCo), but this is only ""constrained"" settings. The truth is that Table 4 contains the best numbers for the baseline, thus these are the results that LoCo(++) should improve, but it is not clear that it does so. As mentioned before, the differences are too small in most of the cases, and no measure of statistical significance is reported for most datasets. Only with Flowers-102 some measure of variance is reported (it's not clear whether the +/- show standard deviation or confidence intervals), and the intervals largely overlap in most cases. <sep> Update after discussion: The authors have addressed most of my concerns, although not always satisfactorily. They made a considerable effort running additional experiments to provide with additional standard deviations of the accuracy in CUB-200, as well as provided results achieved using a ResNet-101. They also clarified some of my concerns regarding the datasets used. In some situations, the benefit of the proposed approach versus already existing methods is not clear, but in others the experimental evaluation shows clears benefits. Given this, and the fact that the paper is well written and motivated, I am increasing my score.","There was a predominantly positive feedback from the reviewers so I recommend acceptance of the paper. It is well-written and well-motivated tackling an important problem: That in self-supervised learning one might encode different invariances by default, even if some of these invariances are useful for downstream tasks (e.g. being rotation invariant may be detrimental to predicting if an image has the correct rotation on a phone). For this, they propose a simple, yet elegant approach and validate it on many downstream tasks. Given the recent interest in self-supervised learning, this appears to be a relevant and interesting paper for the *CONF* community."
"rating_summary | weakness | rebuttal_process | decision  ==>  ==> The authors introduce global reference into the entropy model for deep image compression. <sep> They also develop a reference algorithm to ensemble local context, global reference and hyperprior. <sep> This causes the algorithm to be robust to background noise. <sep> Also, the authors develop GSDN module to handle mean-shifting issue. <sep> The proposed method demonstrates good quality and memory usage gain. <sep> This paper propose to take into account the global information as well as the local information to perform better image compression. <sep> The authors also demonstrate comparison to popular image compression standards and recent deep learning approaches. <sep> I think this work is a nice work, however I have two main concerns. <sep> The dataset used for evaluation is rather outdated. Have the authors tried evaluating on recent image compression datasets, or custom data and compare with the state of the art? <sep> Have the authors compared computational complexity? The main reasons why industry standards are not enthusiastic about deep learning approaches to compression is due to the computational complexity, not so much memory. Have the authors compared FLOPS? Moreover, since this work is dealing with global image information, it seems the complexity would increase rapidly with image size, while standard jpeg will relatively be not as severe. Have the authors experimented computational time with UHD, QHD, or 4k? <sep> I am leaning towards accept but not by a lot. <sep> I would like the authors to discuss upon <sep> Empirical results on more recent datasets <sep> Computational complexity and in terms of image size <sep> FLOPS <sep> Computational complexity and time with high resolution like UHD to 4k <sep> After these comments, I would like to adjust the rating","This paper received moderately good reviews, 3 positives (6, 6, 7) and 1 negative (5). The reviewers are generally positive about the main idea but identified several limitations; performance improvement is marginal compared to existing approaches, the proposed method incurs higher computational complexity, and the presentation is not clear enough. Some of these issues are addressed in the rebuttal, though. Overall, the merits of this work outweigh the drawbacks and I recommend accepting this paper."
"abstract | ac_disagreement | decision  ==>  ==> Strength: <sep> The paper provides an interesting application of Influence functions that were introduced in Koh et al. for studying data poisoning attacks. The main idea of the paper given in Section 3 is that an approximation of the influence or impact of a training sample on the test set can be obtained using second order Taylor's series expansion at the optimal model parameter -- a bilinear form where the matrix of the form is given by the hessian inverse and the vectors are gradients evaluated using training and test sample. However, since this approximation only holds when the Hessian positive definite, the paper identifies certain settings in which the influence functions can be estimated to reasonable approximation. <sep> Weaknesses: <sep> The ""retraining"" phase to get the ground truth seems like an interesting idea, which in my understanding has not been looked at. However, this retraining phase has been directly borrowed from Koh et al., and in fact, Koh et al propose the method also as an approximation. The paper fails to clarify why such an approximation will provide any insights on the performance of neural networks. Including a summary, and intuition of the approach will be helpful to the reader. <sep> Most of the neural networks used have a relu activations (with linear layers), making them a piecewise linear function with respect to the parameters, more importantly, nonsmooth. In fact, in it possible to count the number of linear regions, see (http://proceedings.mlr.press/v80/serra18b.html). In this light, it is unclear what the main message of the paper from an intuitive perspective is. Moreover, the paper misses an important line of work in statistics called Experimental Design in which the goal is to approximate a dataset using a subset with respect to various statistical criterion on the estimators, see https://www.jstor.org/stable/2290334?seq=1. <sep> Related to the previous point: It is unclear what the take away is from the experimental results in the paper. Some of the experiments are performed using very shallow networks (width of 5, depth of 1 and relu activations) whereas others have a few more layers. However, two test points are used in all the experiments. Why? All of the datasets used in the experiments have thousands of points in each class, and it is also possible to augment them by simple rotations, translations etc.. The paper will benefit by adding robust estimates of Pearson and Spearman using a fraction of test samples around the highest and 50% test points. <sep> Finally, none of the results are provided with confidence intervals over runs. Are the experiments averaged over a few runs? If so, I suggest you include them in the figures to provide an accurate summary of the experiments. <sep> After response: Thanks for the clarifications especially the piecewise linearity of deep network as a function of input and not as parameters (like I incorrectly suggested in my review). I think the empirical results have some merit, but are preliminary and I fail to see the bigger picture. For example, more carefully designed ablation studies are needed to determine the practical utility i.e., when the approximations suggested are reliable and when they are not.","This paper examines under what conditions influence estimation can be applied to deep networks and finds that, among of items, that influence estimates are poorer for deeper architectures, perhaps due to poor inverse Hessian vector approximations for poor for deeper models. The authors provide an extensive experimental evaluation across datasets and architectures, and demonstrates the fragility of influence estimates in a number of conditions. Although the reviewers noted that these issues are now ""folk knowledge"", there has been less scientific effort in identifying these failures. <sep> Of course, more theoretical understanding would help the community better understand where these fragilities lie, but the experimental evaluation is sufficiently strong to be of broad interest to the community."
"rating_summary | weakness | rebuttal_process | decision  ==> --- Update --- <sep> The authors have addressed several concerns that I had regarding the work.  While this is largely an application of a previous method, they have made some application-specific decisions in order to achieve the significant boost in performance on colorization that they saw.  While the metrics for this task are much improved, there are still some things to be desired on the qualitative results (i.e. the diversity of results tends to be in blocks, as opposed to high within-image color variability).  Nevertheless, I think the improvement from this approach my guide future work in this area.  Given the author's responses and changes made, I have amended my recommendation accordingly. <sep> 1.  Summary <sep> The authors propose a method for image colorization based on self-attention largely following the architecture of the Axial Transformer (Ho et al., 2019b).  This approach outperforms several SOTA colorization models on FID and human evaluation. <sep> 2a. Strong Points <sep> The motivation for this work is clear.  Image colorization has many applications and while past approaches have significantly advanced in the past few years, there is certainly much left to be explored in this space. <sep> The recap/explanation of the Axial Transformer is clear and concise.  My concern (see below) is not with the articulation of this section, but more on the reliance of an approach that hasn't been accepted via peer review. <sep> The performance of this method using both FID as well as using human evaluators is compelling. <sep> Breaking the problem of colorization into two intermediate low resolution images is a nice approach for enabling larger models.  One question would be how well a single model would perform if smaller images were all that was required. <sep> The ablation studies show how different components impact the performance. <sep> 2b. Weak Points <sep> All three modules of this approach are based on method of (Ho et al., 2019b), which is available on arxiv, but was rejected from *CONF* 2020.  The current work is focused on the application of that method. This makes for a bit of a tricky situation.  The description of the Axial Transformer is given in section 4, but it is only textual and refers the readers back to the pre-print for more detail.  Since this is the central method of the current work, at a minimum I think it requires more explanation/justification as opposed to pointing to a work that has not been accepted via peer review. <sep> While the language of the paper is fine, the overall flow of the paper is lacking a bit of narrative.  Overall I found myself having to jump around to find the definition and explanation of important things.  Particularly within the description of the model, it would be good to add some language to help the sections flow- currently they feel very independent.  Alternately, if maybe help if in the beginning part of the model, the different model components (fc, fs, etc.) are named there.  Related, the Architecture Section feels out of place after the Model description.  There are references to the attention layers in the model description which are not explored until the Architecture section.  Perhaps it makes sense to put the Architecture section first because it's addressing layers/mechanisms that span all aspects of the model.  Or perhaps combining the two sections?  Right now it feels like there are two methods sections. <sep> Some of the text around Eqn(7) seems to be missing because the sentence structure doesn't make sense. <sep> It's not clear what some of the labels in Figure 3 mean.  You have to go into the text to find out what MLP 4x means, for example, and then when you find it in section 5.2, you have to go back to section 4.3 to actually understand what it means. <sep> The ablation studies feel like they're done in relative isolation.  It would be useful to know, for example, how the lower performance of using the standard Axial Transformer vs. the conditional Axial transformer impacts the final results, not just that portion.  The section ""Conditioning Details"" in 5.2 just feels like a results dump.  It's unclear what motivates those particular ablation choices and what those results tell the reader more generally about this approach.  Some kind of context or discussion would be useful.  In general, this section feels like it's being included just to show that ablation studies were performed without providing any greater understanding as to the approach (to potentially motivate future work or other examples, for instance).  The descriptions are also very terse.  If these experiments add meaningful insight to this approach, then they belong in the main text with additional explanation and discussion.  If they are merely a justification that this approach works, then I would suggest moving most of this section to the appendix and using the space to give better explanation of the methods and results which are central to the application. <sep> Some of the models which the current method is compared to (Table 2) are not referenced to the best of my knowledge.  What does ""CNN"" mean in this case?  Do all of these methods use a combined spatial and color upsampling method?  If not, how were they implemented?  This is actually a pretty significant issue as it limits the reproducibility of the comparative experiments. <sep> 3. Recommendation <sep> Reject.  While the results are compelling, the work largely relies on a method which has not been accepted via peer review.  That in and of itself does not warrant rejection, but I believe it contributed to some of the difficulties in explaining the approach, the motivation behind the approach, the results of the ablation studies, etc., which make the paper extremely difficult to follow, likely difficult to build upon, and potentially difficult to reproduce. <sep> 4. Recommendation Explanation <sep> I would argue that the main goal of this paper is to show a novel application of the Axial Transformer approach of Ho et al 2019b and this is done by adapting that method to the task of Image Colorization.  I would argue the focus is around applying that method, not exclusively doing better Image Colorization, because there is no discussion around how this advances our understanding of image colorization broadly.  Nevertheless, that (showing the usefulness of an approach to a new task) is a valid objective, but because Ho et al 2019b has not been formally accepted, it also somewhat then requires this work to explain and justify approaches of that work.  I believe that challenge has a lot to do with some of the difficulties in the paper around the methods and experiment explanation. <sep> While the (within sentence) language is clear, the overall flow of the paper is  difficult to follow.  It feels like the authors were strongly up-against the page limit, so important explanation and discussion was omitted or made very terse.  For example, the ablation studies, while thorough, sort of feel dumped there.  There's no discussion as to why those and not other experiments were run and what the results of those experiments tell us more broadly.  Similarly, the model and architecture section seem like they should be more intertwined.  As another example, some of the methods in Table 2 are not referenced anywhere and it's not clear how they were used in this context (did they start with a low res image, or high-res image).  That calls the reproducibility of the comparison studies into question. <sep> 5.  Questions <sep> Overall it seems like every generated image has a red, green, and blue variant.  Were they sampled in a particular manner to guarantee this?  Obviously it is possible to draw other samples, but do they all largely fall into one of these three coarse categories?  When the performance is poor for a given sample, it usually because entire swaths of the image are being painted in with a very non-natural color (like someone's face being green, or the entire picture having a blue-ish exposure).  Can you speak to this and other common ""mistakes"" that are observed?  How do these compare with some of the other methods you compared yours against?  Are there simply fewer ""mistakes"" (i.e. non-natural images), or are the types of imperfections created by this approach different that would warrant different use-cases? <sep> It seems like a lot of compute (16 TPUv2) was used and the batch size was relatively large.  Is the large batch size necessary for obtaining these results, or could a smaller amount of compute and smaller batch size be used? <sep> Why does training baselines with 2x and 4x wider MLP dimensions make ""a fair comparison""?  Is ""Baseline"" in Figure 3, x1 (standard) MLP but no conditioning?  Why would x1 be better than x4, but worse than x2? <sep> The caption of Figure 2 feels a bit imbalanced.  ColTran core is called out specifically, but then the ColTran Upsamplers are not referenced.  Is the ""Axial Transformer"" just the right branch of the ColTran Core (which the figure seems to suggest) or the entire ColTran core, as the caption seems to suggest. <sep> On pg. 3 ""ColTran Core"" it is stated that ""we also train a parallel prediction head which we found beneficial for regularization"".  I think it would be useful to given additional explanation here as it's a fairly significant architectural choice.  If results of not including this head exist, perhaps it would be useful to show this in the appendix.  Otherwise a brief explanation as to why this additional head aids the regularization would be useful.  Since this is an instantiation of the Axial Transformer, is this prediction head added to that approach for this particular task, or is this already a part of the standard Axial Transformer (and therefore maintained here for consistency)?  Ah, this is explored further in section 5.3…. It would be helpful to the reader to reference this section when you introduce the prediction head (i.e. that the impact will be explored in section 5.3). <sep> In 4.2 it says they ""adapt the Axial Transformer model for colorization"".  Can you elaborate on the adaptation?  It's not clear (without looking up that reference), what belongs to the original approach vs. what was added/changed here for this specific task. <sep> It feels odd to mention the number of axial attention blocks in the training section as opposed to the model or architecture.  This is a fundamental architectural choice, is it not? <sep> Why are the set of models compared via FID and Human Evaluation different? <sep> 6.  Feedback <sep> The demonstrated colorization scores and output are compelling, however, I believe the structure of text is very detrimental.  I think it would potentially be feasible to fully rework the text to make it more readable and reproducible and therefore a solid publication because the result is compelling, but as it stands, there is substantial rewritting which would need to be done in my opinion. <sep> In ""Model: ColTran Core"" fc is described as a conditional, auto-regressive axial transformer.  While the definition of pc and pc~ are stated thereafter, there is not any further description as to what this means and/or a citation.  The Ho et al. citation is provided in the Figure 2 caption.  At a minimum that citation should be given here as well, but it would be good to give a textual description as to what an ""a conditional auto-regressive axial transformer"" is since it is not a commonly used architecture. <sep> The second paragraph under ColTran Upsamplers (In our experiments…) is slightly confusing.  It seems to suggest that parallel upsampling is sufficient and advantageous for a number of reasons, but that prediction is chosen to reduce color inconsistencies.  Then it seems to go back to again say that Parallel upsampling has a huge advantage of being fast.  This is perhaps also confusing because there is a ""Sample"" label in Figure 2.  The confusion is less about the validity of the approach and more that the language (in conjunction with the figure) is difficult to follow for someone not already familiar with Guadarrama 2017. <sep> While not necessary, it would be interesting to see how this approach performs on out of domain images (i.e. not from ImageNet). <sep> In 5.5, it's stayed you follow the protocol used in PixColor.  It would probably be best to additionally include the citation here, or the citation in place of ""PixColor"" even though that work is cited near the beginning of the paper (when the reader comes to this section, they may be unfamiliar with this approach and would like to go directly to that reference as opposed to having the search for ""PixColor"" and then go find the reference).","The paper initially received a mixed rating, with two reviewers rate the paper below the bar and two above the bar. The raised concerns include the need for an autoregressive model for upsampling and the effect of batch sizes. These concerns were well-addressed in the rebuttal. Both of the reviewers that originally rated the paper below the bar raise the scores. After consulting the paper, the reviews, and the rebuttal, the AC agrees that the paper has its merits and is happy to accept the paper."
"abstract | misc | weakness | rebuttal_process | rating_summary | decision  ==> The paper studies how initialization and the implicit regularization of SGD affect the training dynamics of neural networks in terms of minimality and sufficiency of learned representations. The main findings are that 1) SGD with random initialization learns almost minimal and sufficient representations and 2) SGD with an initialization that contains information about irrelevant factors fails to converge to minimal representations, increasing the chance of overfitting. These findings are interesting, useful for understanding neural networks, relevant to the *CONF* community, but lack evidence of generality. <sep> Task choices. <sep> Most of the experiments in this paper are done on the checkerboard task. In this task one is given a checkerboard, each cell of which is either red or green. One of these colors appears more (is dominant) and the task is to decide which color is dominant. The key aspect is that the input also contains 2 targets: left and right, with one being green and one being red. Instead of directly predicting the dominant color, the subject should pick the target whose color matches the dominant color. Importantly, the color arrangement of targets is picked at random. The authors also consider the task of predicting whether an MNIST digit is odd or even. These two tasks are too simple, which restricts the generality of conclusions. I suggest to consider harder tasks, for example classifying CIFAR-100 images, where the target is the superclass (1-20) and the irrelevant factor is the exact class (1-5). Additionally, it would be interesting to consider cases when the training data is such that there is a small mutual information between the irrelevant factor and the target. Will SGD with random initialization find a solution that has even smaller mutual information with irrelevant factors (i.e. sacrificing sufficiency for minimality)? <sep> Network. <sep> Throughout the paper only fully connected networks are considered. Additionally, the last hidden layer always has <= 20 units. For generality, it would be better to consider also larger and more modern networks, such as ResNets. <sep> Activation function. <sep> Saxe et al. [1] showed that the choice of activation function is crucial when judging about compression in late stages of training. The presented paper can be improved by considering other choices of activation functions. <sep> The role of SGD. <sep> The implicit regularization of SGD arises from its stochasticity. The findings of this paper suggest that this stochasticity has a key role in finding minimal representations. This should be verified by comparing to standard gradient descent (i.e. batch size = number of examples). <sep> Minor comments <sep> The description of the checkerboard task starting at the last paragraph of the first page can be improved. <sep> Did you consider using a more powerful decoder? In Fig. 2c for example, we see that later layers have more usable information about the direction. This means that there is a room for strengthening the decoder. <sep> P.S. I am willing to increase the score if the authors address the above concerns about generality. <sep> Update <sep> Thanks for the rebuttal, it addressed my main concerns. The new results on CIFAR-10 and CIFAR-100 with fine and course labels match the results on the checkerboard task. This increases the generality of the main claims. The new experiments also confirm that the level of noise in SGD has a key role in finding minimal representations. Furthermore, they show that when training with SGD with enough amount of noise, the usable information with fine labels increases initially and then decreases. This improves our understanding of the phenomenon introduced by [2], which was later debated by [1]. For these mentioned reasons, I updated the rating from ""5: Marginally below acceptance threshold"" to ""7: Good paper, accept"". <sep> References[1] Saxe, Andrew M., et al. ""On the information bottleneck theory of deep learning."" Journal of Statistical Mechanics: Theory and Experiment 2019.[2] Ravid Shwartz-Ziv and Naftali Tishby. Opening the black box of deep neural networks via information. CoRR,  abs/1703.00810, 2017.","This paper proposes that we can understand the evolution of representations in deep neural networks during training using the concept of ""usable information"". This is effectively an indirect measure of how much information the network maintains about a given categorical variable, Y, and the authors show that it is in fact a variational lower bound on the amount of mutual information that the network's representations have with Y. The authors show that in deep neural networks the usable information that is maintained for different variables during training depends on the task, such that task irrelevant variables (but not task relevant variables) eventually have their usable information reduced, leading to ""minimal sufficient representations"". <sep> The initial reviews were mixed. A common theme in the critiques was the lack of evidence of the generalization and scalability of these results. The authors addressed these concerns by including new experiments on different architectures and the CIFAR datasets, leading one reviewer to increase their score. The final scores stood at 3, 7 ,7, 7. Given the overall positive reviews, interesting subject matter, and relevance to understanding learned representations in deep networks, this paper seems appropriate for acceptance in the AC's opinion."
"abstract | misc | weakness | misc | strength | weakness | strength  ==> There has been an increase of works using deep neural networks to heuristically predict solutions to constrained optimization problems. However, these methods cannot generalize to arbitrary constraints.  In this paper, the authors propose a method to build neural networks that output vectors that satisfy hard equality and inequality constraints. They do this by first having the network predict the underdetermined part of the system defined by the equalities, then doing a series of gradient steps to project the solution onto the space delineated by the inequalities. They evaluate on synthetic quadratic programs and problems derived from a AC power flow application. <sep> Prior to this work I could see two ways of doing what they propose. <sep> A) If the constraints are linear, then Frerix et al. (2020) propose to compute the Minkowski-Weyl decomposition of the polyhedron of constraints, and to have the neural net output the coefficients in the resulting basis. Computing the MW decomposition is very expensive (both in theory and in practice), but once it is computed, enforcing the constraints is automatic. <sep> B) Alternatively, if the constraints are (disciplined) convex, one could use a differentiable convex layer as last layer, e.g. using the method of Agrawal et al. (2019) with a somewhat arbitrary choice of objective function. For example, it could be a quadratic program layer (as in the preceding work of Amos and Kolter (2017), where a neural net with a final QP layer is used to output soft solutions to Sudokus, which the authors do cite). In fact when constraints are convex something very similar to this paper can be done with this approach, where a neural net would predict a solution, and a final QP layer would find the point that satisfies the constraints that is the closest in L2 distance with the initial neural net guess. <sep> The current paper discusses briefly works related to method B in the ""Implicit layers"" paragraph of Section 2, without discussing how it can accomplish what they aim to do, nor compare against it. It is a direct competitor when constraints are convex. As for method A, it is neither cited nor compared against. Against, this is a direct competitor when constraints are linear. <sep> So in what way does the current approach compare to these previous ones? The main advantage I see is that the method can be applied to nonconvex problems. Of course, this doesn't mean it will necessarily do well - the Newton's method step for the equality constraints or the first-order projection for the inequality constraints might not converge. There is no miracle and probably this method won't be workable on nonlinear constraints that are too wild. But at least, it seems it does converge for the nonconvex ""ACOPF"" problem of the experiments, so there seems to be some use-case. I think this advantage (the method can be applied to arbitrary nonlinear constraints) should be emphasized more, and a discussion of the probable limitations should be included as well. In particular, when more assumptions are put on the constraints (e.g. linearity), there are probably better options than first-order methods. <sep> Regarding the timings in the experiments: all the machine learning methods use a GPU, so it would be interesting to compare against a baseline solver that can use a GPU when possible as well. For example, for the QP, the qpth library of Amos and Kolter (2017) has the entire QP solver implemented on CUDA - this might make a difference in speed. <sep> Overall, my opinion is that the paper does nothing groundbreaking, but has the quality of being the first method (as far as I know) to propose enforcing hard equality-inequality constraints on the output of a neural net, which is an obstacle right now in extending recent work on learning heuristics for optimization problems. The literature review definitely has to be expanded however, to explain that when constraints are linear and convex respectively, the works of Frerix et al. (2020) [method A] and Agrawal et al. (2019) [method B] are competitors. Moreover, more discussion on the advantages and inconvenients of the approach is necessary (i.e. that the method is very general, but probably inefficient on more structured problems, and yet probably can't work on constraints that are too wild). But otherwise, the core concept of the paper sounds sound to me, and the experiments, although minimal, are I think correctly made (several seeds, etc.) I also appreciate that code was provided. So I tend towards weak acceptance of the paper. <sep> In case this paper is rejected, my recommendation to the authors would be strengthen the experimental section. First, since comparisons on QPs are presented, I would have liked to see comparisons against these methods. I think comparisons on another nonconvex problem would be a must as well. Also comparisons in a ""predict-then-optimize"" setup (e.g. see Elmachtoub and Grigas 2017, Donti et al. 2017, Wilder et al. 2019, Vlastelica et al. 2019) would be very welcome, since this is a setup where neural networks have tremendous potential. This would make for a much stronger paper. <sep> References <sep> [1] Frerix, T., Nießner, M., & Cremers, D. (2020). Homogeneous linear inequality constraints for neural network activations. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (pp. 748-749). <sep> [2] Agrawal, A., Amos, B., Barratt, S., Boyd, S., Diamond, S., & Kolter, J. Z. (2019). Differentiable convex optimization layers. In Advances in neural information processing systems (pp. 9562-9574). <sep> [3] Amos, B., & Kolter, J. Z. (2017). Optnet: Differentiable optimization as a layer in neural networks. arXiv preprint arXiv:1703.00443. <sep> [4] Elmachtoub, A. N., & Grigas, P. (2017). Smart"" predict, then optimize"". arXiv preprint arXiv:1710.08005. <sep> [5] Donti, P., Amos, B., & Kolter, J. Z. (2017). Task-based end-to-end model learning in stochastic optimization. In Advances in Neural Information Processing Systems (pp. 5484-5494). <sep> [6] Wilder, B., Ewing, E., Dilkina, B., & Tambe, M. (2019). End to end learning and optimization on graphs. In Advances in Neural Information Processing Systems (pp. 4672-4683). <sep> [7] Vlastelica, M., Paulus, A., Musil, V., Martius, G., & Rolínek, M. (2019). Differentiation of blackbox combinatorial solvers. arXiv preprint arXiv:1912.02175. <sep> Edit after rebuttal: I am satisfied with the changes to the paper and increase my score to an accept.","The paper proposes an approach for solving constrained optimization problems using deep learning. The key idea is to separate equality and inequality constraints and ""solve"" for the equality constraints separately. Empirical results are given for convex QPs and for a non-convex problem that arises in AC optimal power flow. <sep> There was much discussion of this paper between the reviewers and the area chair. THe key question was whether the empirical evaluation is sufficient to convince that the method is more effective than existing solvers. The current experiments do not show that the method achieves better solutions than existing solvers. For the convex case this is to be expected since solvers are optimal. But in the non-convex case, it would have been nice to see that the method indeed can find better solutions. <sep> This leaves the advantage of the method in its speedup over existing methods. However, as the authors acknowledge, it is possible that this speedup is due to better use of parallelization than the methods they compare to. It is true that deep learning is particularly easy to parallelize, but this is not impossible for other methods (e.g., for linear algebra operations etc). <sep> Thus, taken together the empirical support for the current method is somewhat limited. The method itself does make sense, and this was indeed appreciated by the reviewers."
"abstract | rating_summary | decision  ==> ############## Summary ############## <sep> This submission asks the question of how the minima found by batch multi-task learning compare to those of continual learning. It empirically finds the they are connected via linear interpolation through a manifold of low error, and leverages this fact to come up with a clever new algorithm for continual learning that performs better than various existing continual learning baselines on three benchmark data sets. <sep> ############## Strengths ############## <sep> The question of how to connect multi-task to continual learning solutions is well motivated via simple introductory experiments. <sep> The answer to this question, that there is a linear mode connectivity, motivates a simple, elegant, and effective algorithm. <sep> ############## Weaknesses ############## <sep> The paper could benefit from substantial editing to make it clearer and easier to follow. I found myself having to re-read various sections to properly understand how the different parts of the paper were connected. <sep> The empirical evaluation is done only on three benchmarks. It could be valuable to add evaluations on additional data sets, like Omniglot (https://github.com/brendenlake/omniglot). <sep> ############## Recommendation ############## <sep> I recommend this paper for acceptance, but urge the authors to substantially revise their manuscript to make it more approachable. I believe this paper to be self-contained, with a clear question being asked, which hadn't been asked before: how are the solutions to multi-task and continual learning methods connected. The authors find that there is linear connectivity between these solutions, and use this fact to motivate a simple yet effective continual learning algorithm. <sep> ############## Arguments ############## <sep> The question of whether and how the solutions to multi-task and continual learning are connected is highly relevant. While most prior literature had assumed that some distance metric in the parameter space was the correct way to measure their connection, this work is motivated by the experiments in Fig. 2, which show that these metrics are not quite appropriate. Instead, the authors show that linear mode connectivity better explains how multi-task and continual learning solutions are related. <sep> The manuscript then deviates to an analysis of when this type of connectivity holds by analyzing second-order Taylor approximations. I had to re-read this section (Section 3) multiple times in order to find what the relevance of it was to the submission. My conclusion was that the point is that the fact that the parameter vectors move in directions of low curvature means that interpolation in those directions doesn't increase the loss by much. This fact seems to be somewhat hidden in the text. I encourage the authors to place emphasis on what their analysis is attempting to find before diving into it in depth, as it is easy to lose the reader if they are not aware of where the analysis is going from the start. <sep> The proposed algorithm is clever and simple: it leverages past data not only to approximate the loss of the previous tasks on the new solution, but also to add a regularization encouraging a low-loss linear path between the solutions. Although the authors experiment with very few data sets, I believe they sufficiently show the applicability of their method and the fact that it performs well. It would be interesting to see how differently the method would perform if instead of the MC regularization, the authors used the EWC one. This would help avoid conflating the claim ""regularization + replay is best"" from ""MC regularization + replay is best"". Similarly, it would be relevant to reproduce Figure 7 with the solutions found by baselines, to assess whether they also find linear connectivity solutions. The claims would be stronger if the authors showed that baselines don't find linearly connected solutions. <sep> ############## Additional feedback ############## <sep> The following points are provided as feedback to hopefully help better shape the submitted manuscript, but did not impact my recommendation in a major way. <sep> Intro <sep> It seems like the authors interchangeably used en-dash and em-dash. They also used en-dash to open, but not close, a statement. <sep> What confounding factors are removed by doing w1 --> w1,w2 other than initialization? The text makes it sound like there's more but no other is discussed. <sep> Contribution 3: benchmark --> benchmarks <sep> I believe compressed related work sections or those pushed to the appendix make it hard to place the contribution in context. I encourage the authors to expand this section in the main paper. <sep> Sec 3 <sep> I had two main questions when reading this section: <sep> Why doesn't EWC find such a low curvature path, if it precisely penalizes deviations in directions of high curvature? <sep> Why can't we just use the proper Taylor expansion instead of Euclidean distance then, to measure forgetting, instead of mode connectivity? <sep> These two questions were answered towards the end of the section by showing that this is not a sufficient condition, and are then explicitly addressed by suggesting that second-order approximations are a promising direction for future work. I encourage the authors to clarify this before diving into the analysis, so the reader knows what to look for when reading this section. <sep> The caption for Fig. 5 doesn't explain difference between b and c, which is only somewhat explained in text later. <sep> Sec 4 <sep> Regularization only considers low-loss path between the solution to the immediately previous task and the current solution (but not the solutions to all past tasks), assuming that the immediately previous solution contains sufficient information. Was this empirically tested? The EWC authors claim that using only the previous model in their setting is insufficient [1], so it would be interesting to see if there's a similar effect here. <sep> Appendices <sep> Very complete: additional results, justification of experimental setting. <sep> Style, grammar: <sep> appendix X --> Appendix X <sep> second order Taylor --> second-order Taylor expansion/approximation minima is often used as a singular, which should be minimum regularization based --> regularization-based rehearsal based --> rehearsal-based <sep> Inconsistent italization of i.e. <sep> few shot learning --> few-shot learning <sep> [1] Kirkpatrick, J., Pascanu, R., Rabinowitz, N., Veness, J., Desjardins, G., Rusu, A. A., ... & Hassabis, D. (2018). Reply to Huszár: The elastic weight consolidation penalty is empirically valid. Proceedings of the National Academy of Sciences, 115(11), E2498-E2498. <sep> Chicago","The paper is presenting an important empirical finding. When the learning algorithms are initialized with the same point, the continual and multitask solutions are connected by linear and low-error paths. Motivated by this finding, the paper proposes a new continual learning algorithm based on path regularization. The paper received unanimously good scores. I agree with the reviews and recommend acceptance."
"rating_summary | rebuttal_process  ==>  ==> This paper proposes a simple way to increase the robustness of the learned representations in a network perform a series of object recognition tasks by adding a random convolution layer as a pre-processing stage, thus ""filtering the image"" and preserving the global shape but altering the local `texture' of the newly transformed image. Here, the hope is that  -- analogous to Geirhos et al. 2019 that induces a shape bias by transforming the image distribution into a new one with altered global textures that induce a shape bias and increases general robustness to o.o.d distortions --  the authors here go about doing something similar at the local level given the small size of the receptive field of the filter, thus preserving the shape and slightly altering ""the texture"". <sep> Pros: <sep> While the innovation is simple and efficient, this data-augmentation scheme works, and I can see how other future works may use this as well as a data-augmentation technique for object recognition. I am not sure however if no one else has explored the effects of random convolutions for robustness. It sounds too good to be true, but then again -- there is always beauty in simplicity and it is possible that the authors have hit the nail on the head on finding a somewhat 'contrived' filtering process as a bonus rather than a limitation. Simple, yet counter-intuitive findings like these are relevant for *CONF*. <sep> Authors provide lots of experiments that to some degree prove the success of their augmentation strategy (although see Cons). <sep> Cons: <sep> Biological Inspiration: What is the biological mechanism linked to the success of using random convolutions. One could argue that this point is 'irrelevant' to the authors and the readers, but as there is a plethora of different data-augmentation techniques to choose from, why should computer vision and machine learning practitioners choose this one? (See Missing Reference for a suggestion) <sep> Insufficient/Incomplete Baseline: The model is inspired loosely by Geirhos et al. 2019; but how does the model compete with Geirhos' et al.'s Stylized ImageNet? I would have wanted to see a baseline between the authors proposed model and other texture-based augmentation strategies. This would elucidate the Global vs Local advantages of ""texture""/style transfer on learned representations. I think this is where authors could capitalize more on. <sep> The word `texture' in the paper is a mis-nomer. Here what is really done is 1st order filtering via a convolution operation with a filter that does not happen to have a Gabor-like shape. ""Texture"" in other contexts going back to vision science and even computer vision and image processing (style transfer included), is usually computed by a set of cross-correlations between outputs of a filtered image (analogous to the Gramian Matrix of Gatys et al. 2015), or the principled Portilla-Simoncelli texture model from 1999. <sep> Missing references: <sep> Excessive Invariance increases adversarial vulnerability by Jacobsen et al. *CONF* 2019. The augmentation procedure proposed by the authors shows robustness to common distortions, but how about adversarial robustness? Is this relevant? Was this tried? I'd love to hear more about the authors thoughts on this to potentially raise my score. <sep> Emergent Properties of Foveated Perceptual Systems (link: https://openreview.net/forum?id=2_Z6MECjPEa): An interesting concurrent submission to this year's *CONF* has shown that the biological mechanism of visual crowding (that resembles texture computation for humans in the visual periphery) is linked to some of the operations introduced in the paper by the authors. It would be great if the authors potentially cite similar (and/or the before-mentioned) works to provide a link to a biological mechanism that may support why their data-augmentation procedure works and/or should be used; otherwise it seems contrived and could be seen as ""yet another data-augmentation procedure that increases robustness but we don't know why"". <sep> Implementing a Primary Visual Cortex in the retina increases adversarial robustness by Dapello, Marques et al. 2020 (NeurIPS). This recently published paper in a way shows almost the opposite of what the authors are proposing here. Rather than using random convolutions, they actually mimic the gamut of spatial frequency tuning properties of Gabor filters in the first stages of convolution as done in human/monkey V1. The authors should discuss how their results fit with Dapello, Marques et al. 2020 and how they can reconcile their somewhat opposing views. <sep> Final Assessment: <sep> I am on the fence of having this paper accepted at *CONF* given the limitations expressed above, but I do like it's simplicity that should not take away it's merit -- thus my slight lean towards acceptance. I am willing to raise my score however if authors address some of the cons/limitations, and am also curious to see the opinion from other reviewers, it is possible that I may have missed a key reference regarding data-augmentation that may weaken my assessment.",Reviewers concurred that this is an interesting paper with contributions worthy of publication. The authors also provided many details in the rebuttal which makes the paper even more strong.
"abstract | weakness | ac_disagreement  ==>  ==> The given paper carries two main contributions: 1) theoretical study of the pruning at initialization (i.e. before training); 2) proposing a new rescaling trick to avoid issues (namely, entire layer pruning) that are common for such pruning mechanisms. <sep> Major concerns:  . Theoretical contribution: First of all, I would like to mention that I am not an expert in one-shot pruning and pruning at initialization. However, I strongly believe that the layer pruning problem is commonly observed and studied phenomena (which is stated by authors as well) and it was theoretically studied before. For example, I suggest authors refer to the recent work from [1] where they call it ""layer collapse"". Furthermore, it was shown that layers with the smaller size have more likelihood of getting entirely pruned. Here, we observe similar behavior (called ""layer ill-conditioning"") but due to EOC.  . Methodological contribution: again, as it is done in [1], the main propose of the rescaling trick is to make sure that the sensitivity score is uniformly distributed to avoid layer pruning. I believe, there are other ways to achieve this goal and thus, contribution is marginal. <sep> [1] Tanaka et al. Pruning neural networks without any data by iteratively conserving synaptic flow. June, 2020. <sep> Major advantageous:  . Authors theoretically justify their proposed method;  . Experiments show a consistent improvement over the SoA baselines (Snip, Grasp). The improvement is even drastic for ResNet104; However, it will be nice to have a comparison with [1]. <sep> ---- update after authors' response ----Thanks for clarification and providing additional experiments. I'm changing my final evaluation to weak accept. Yes, this paper does provide some interesting insights, but I still think that it has a limited potential impact (see above for major drawbacks).","The paper proposes a sensitivity-based pruning method at initialization. For fully connection and and convolutional neural networks, it shows that the model is trainable only when the initialization satisfies Edge of Chaos (EOC). The paper also provided a rescaling method so that the pruned network is initialized on the EOC. For Resnet, the paper shows that the proposed pruning satisfies the EOC condition by default and further provides re-parameterization method to tackle exploding gradients. The experiments show the performance of the proposed method on fully connected and convolution neural network, as well as ResNet. There were some concerns about the contribution of the paper compared to that of [1]. I read the two papers carefully and while both papers aim at addressing a similar problem, i.e., pruning at initialization while avoiding layer collapse, the paper provides a different perspective on the problem, and provides enough theoretical contribution and insights to be found helpful and interesting by the community."
"abstract | strength | weakness | misc | rebuttal_process | strength | decision  ==> Brief summary of your review: <sep> I like the main idea of having an ""active memory"" where each slot can choose which operation to perform. It is similar to the notion of variables and functions, which should be advantageous for systematic generalization, which is one of the most significant issues of current neural networks. <sep> The paper focuses on the visual domain and is motivated by extracting objects and their behavior. However, the model does not use attention over the input. The only way that the proposed model can use different object files for different instances of the same object type is to have separate schemata (because all other weights are shared), which defeats the purpose. <sep> Thus, sadly, I must reject the paper. <sep> Review: <sep> The authors focus on a very important question of current neural networks: systematic generalization. Their method is interesting: they factorize modeling the scene of objects into object files (memory slots) and schemata (different learned operations). This is related to but different from memory networks, which use a single controller (instead of the multiple schemata), and also to routing networks, which use a single state (instead of multiple object files) and multiple sets of weights describing the different operations. <sep> The authors introduce their method as a way of modeling multiple objects, some of which may share behavior. This can be seen clearly from Figure 1, but also the tone of the whole paper is organized around this goal. However, the way the input image is handled makes this very implausible. Specifically, the problem is in step 2 of Algorithm 1: <sep> The object files compete for the input. In terms of the Pacman example, this implies that multiple moving ghosts, each with its own OF, would have to compete with each other to see the input, which makes no sense: all of them should see the change in their respective object. <sep> The other, perhaps even more significant issue regarding input handling is that there is no attention over the input image. I cannot see how this architecture can focus on a different subset of the input corresponding to different objects. To focus on objects separately, they necessarily have to have different schemata, because the rest of the weights are shared between OF. That is the only way of having a different transformation of the input (assuming the same initial hidden state). This defeats the purpose of schemata, which is to reuse computation when possible. The current setup would make more sense in other input domains, for example for textual input, where different ""objects"" are not represented by different subsets of a single input z_t. <sep> In Figure 3 the authors analyze which schema the model uses for a single object slot. The result is very nice and it shows that the model learns to use different schema for different types of motion/rooms. However, especially because of the aforementioned issues, it is unclear how the model uses the object files. I would like to see a plot similar to Figure 3, just showing which OF is used. The experiment could be to add/remove objects, and see the number of object files, or to corrupt specific files and see which object becomes unpredictable, thus identifying which OF corresponds to which object. <sep> I still consider the paper interesting. But the issue above must be fixed, e.g., by changing the tone of the paper and shifting away from focusing on objects or switching the attention in stage 2 to attend to image regions and not over the object files. <sep> More minor issues/questions: <sep> In step 3, the attention is based on comparing the updated state to the old one. Why does this make sense? Is there an intuition behind this? Wouldn't it make more sense to have fixed keys identifying the schemata? <sep> In step 4, why do the queries use state from the previous time step as opposed to the current? <sep> On page 4, step 2, κk is mentioned, however, it should be κt, as κ is independent of the object file. <sep> On page 4, saying that Gumbel softmax ""softens the output"" is misleading. The goal is to have a hard, but differentiable output instead of a soft one. <sep> In the related work section, you compare to CNNs. The same argument of weight sharing can also work for RNNs. However, differently from SCOFF, CNNs can actually ""attend"" to different parts of the input image, thus in theory focus on different objects differently. <sep> Related work: Routing networks are [1,2,3] are also related. Page 2 and 4: GRU is a variant of vanilla LSTM with forget gates (Gers et al, 2000). <sep> On page 5, the last paragraph, it is described how the output is produced from the OFs. However, it is not clear how this attention works. What is the query? Is it a transformed state? If the transformed stats are concatenated to form the query, then it is not symmetric anymore, so it will not ""ensure exchangeability of OFs"". It can just focus on a single OF, based on its index. <sep> In the experiments section, page 6, for the ""Single object with switching dynamics"" why does the network need markers? Can't it infer the schema to use just from the motion itself? <sep> Page 6, last paragraph, it should refer to Figure 3 instead of Figure 4. <sep> On page 7, ""Multiple objects with multiple dynamics"", the authors use the dataset proposed by ""Van Steenkiste et al."", and claim that they outperform their baselines. But they do not compare to the original method proposed in the same work. How does it compare? <sep> Could the 1.3 lines long appendix E be a footnote instead? <sep> [1] Routing Networks: Adaptive Selection of Non-linear Functions for Multi-Task Learning <sep> [2] Kirsch et al: Modular Networks: Learning to Decompose Neural Computation <sep> [3] Chang et al: Automatically Composing Representation Transformations as a Means for Generalization <sep> After rebuttal: <sep> Glad to see this was just an error in the decription of the algorithm! Score increased from 4 to 7. We'd even increase our score to 8 if the authors added an analysis similar to the one of Figure 3, just showing which OF is used. As stated in the original review: The experiment could be to add/remove objects, and observe the number of object files, or to corrupt specific files and see which object becomes unpredictable, thus identifying which OF corresponds to which object.","This paper proposes a modular RNN architecture called SCOFF. The work was inspired by cognitive science(object file and schema) and was built upon previous work RIMs. The method is validated on tasks having multiple objects of the same type. <sep> Pros: <sep> It addresses an important problem in DNN -- systematic generalization. <sep> The proposal makes sense and is more flexible than RIM. <sep> Experimental results outperform baselines. <sep> Cons before rebuttal: <sep> The presentation of the algorithm is not very clear due to some confusing notations and missing details of algorithm steps. <sep> The comparison with baselines might not be fair due to extra parameters. <sep> The novelty is limited, because the only difference from RIM is weight sharing. <sep> The reviewers raised concerns listed in Cons. The authors successfully addressed concerns: they indicated that the comparison was fair with the same input to both; SCOFF is more flexible than RIM, and there is spatial attention to input. <sep> The authors added the missing details in the revised version. <sep> All reviewers agree that the problem is important and the idea is interesting. Since the authors' rebuttal was very helpful in clarifying the questions raised, I recommend accept."
"abstract | strength | rebuttal_process | strength | weakness  ==> Summary: <sep> The authors propose to apply the idea of Bayesian ensemble methods to (tree-based) gradient boosting methods so as to be able to measure the knowledge uncertainty (e.g., to detect anomaly or out-of-domains samples) while typically only data uncertainty (e.g. related to noise in the data) is considered. <sep> Overal comment: <sep> The paper is well-written and the amount of experiments is impressive. However, the proposed approach raises several concerns addressed below and regarding the novelty mainly (comment (b)) and the actual performances (comment (d)). I think the paper could be accepted if comments (b) and (d) are addressed. <sep> Major comments: <sep> (a) The motivation is well explained and I agree that not knowing when a machine learnt predictor makes a prediction with certainty or not is the key of the trust in such models. <sep> (b) It is unclear how the approach for estimating uncertainty is novel with respect to what was made with other ensemble approaches as Section II refers to preliminary works but Section III (except virtual ensembles) refers to ensemble of GB models and I am not sure it is an actual contribution of this paper. Please clarify what's new and what already exists in Section III and how much your approach differs / improves / modifies what has already been suggested by the uncertainties estimation via Bayesian Ensembles. <sep> (c) As suggested by Figure 1, it appears that a virtual ensemble is built recursively by adding the one tree model to the previous ensemble and making hence a new object of the virtual ensemble. This suggests that models are not independent and intrinsically give a much higher weight on the first model for instance. The motivation behind the building of the virtual ensemble (the choice of T/2 in the text for instance) is not very clear either. Please clarify (a) if Figure 1 is not overly simplistic and (b) the motivation of the making of a virtual ensemble (especially the value T/2). <sep> (d) It would have been interesting  (and more convincing) to have results (of Table 1) for other non-gradient boosting techniques to convince the soundness of the proposed approach and how it compares with other techniques. <sep> Minor comments: <sep> typo : ""it's (applications)"" in the first section. <sep> Typically, loss functions are defined as function R2→R+ justifying the ""negative"" log-likelihood. Please check.","The authors design a framework to estimate the uncertainties in the predictions of gradient boosting models, for both classification and regression. The framework contains several methods, some that use sub-sampling on data to calculate the estimation, and some that use sub-sampling on the trees within one single gradient boosting model (i.e. virtual ensemble) to calculate the estimation. The different methods reveal the trade-off between faster calculation and good uncertainty estimation. The authors conduct extensive empirical study to demonstrate the validity of the designed framework. <sep> The reviewers agree that the paper is well-written on a very important topic of machine learning in practice. The authors have done a great job addressing the comments from the reviewers, including the comparison to random forest, and adding more motivating examples. The reviewers believe that the work marks a good starting point for addressing this important topic. Nevertheless, the reviewers have some concerns that the results are promising but not impressive yet, and the performance of the virtual ensemble is a bit discouraging."
"abstract | decision | weakness  ==> Update during review period <sep> The reproducibility of the paper is now much better. It's great that the authors promised to release the LTA code. I hope that this includes the code for the experiments. <sep> Based on the above, I changed my review score to 7. <sep> Summary <sep> The paper presents a novel activation function (Leaky Tiling Activation - LTA) to produce sparse activations, which have been found to stabilize learning in continual learning and RL settings. The new nonlinearity and its theoretical properties are described well, and the authors present convincing experiments demonstrating that the method yields practical benefits on synthetic datasets and RL games (e.g. Atari). <sep> Reasons for score: <sep> The paper should certainly be published somewhere, but maybe in a workshop that focuses on continual learning or RL. <sep> While the proposed new activation function may be useful in some settings, there is not enough evidence in the paper that it would become a go-to solution, or significantly change the way we think about interference in continual learning and RL. In particular, the authors compare LTA variants of DQN, but it might have been useful to compare with e.g. Rainbow too. While I agree that interference is still an issue in modern RL with function approximation, it would be useful for potential users of LTA to know whether LTA provides benefits when used in conjunction with an existing state-of-the-art RL algorithm. Adding an experiment to this effect to the appendix would make this paper stronger. <sep> Pros <sep> The paper addresses a key problem in continuous learning and RL: interference and catastrophic forgetting. It presents a novel method to combat this problem, and demonstrates its usefulness in a number of experiments. <sep> The paper is generally well-written. <sep> The experiments on synthetic data were compelling, and made for a very nice controlled experiment. <sep> Cons <sep> The authors could have benchmarked against stronger baselines. The authors might be able to do this during the review process. <sep> The precise set-ups that the authors used for their experiments should be described more clearly. As it is, the paper's work is not reproducible. See my the section ""Questions during rebuttal period"" below for details. <sep> Questions during rebuttal period: <sep> Have the authors considered to benchmark using stronger (state-of-the-art) baselines? Someone who considers using LTAs would likely use a state-of-the-art method already, not DQN, which the authors benchmarked against. The question is then whether LTA yields benefits when used in conjunction with state-of-the-art methods, not when used in conjunction with DQN. <sep> I did not fully understand what architecture the authors used in their experiments. The architecture appears to be described mostly in this single sentence: ""All the algorithms use a two-layer neural network, with the primary difference being the activation used on the last layer. "" <sep> Between what and what is this difference? Between baselines and the LTA experiments? <sep> Later on the page from which I quoted above, the authors mention that they experimented with DQN-like networks. The original DQN paper used 3 layers (https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf). Did the authors mean to write """"All the algorithms use a two-layer neural network""? <sep> Do the authors insert the LTA before the third layer? <sep> More generally, it would be helpful if the authors could describe the architectures used in more detail and maybe ask a colleague who is not yet familiar with the paper to review for clarity and reproducibility. <sep> Some suggestions <sep> I would rephrase the first sentence of the abstract in order to introduce ""interference"" in a gentler way. While interference is an active research topic in the RL community, many members of the *CONF* community might wonder ""what kind of interference""? A half-sentence like ""where updates for some inputs degrade accuracy for others"" (copied from the paper's introduction)  could suffice here. <sep> In section 5.1, the authors have lines beginning with bolded ""DQN"", ""DQN-LTA"", ""DQN-Large"" et cetera. For readability's sake, it might be useful to format these as bulleted lists. <sep> A small grammatical issue: ""This issue is usually called gradient vanish"". Maybe rephrase this as ""This issue is known as the vanishing gradient problem"". <sep> The authors list ReLUs as an example of an activation function with vanishing gradients. As vanishing gradient problems go, ReLU is a bit different from tanh and other nonlinearities that saturate, so I would avoid listing it here to avoid unnecessary debates. <sep> The authors write ""Mnist"" in a number of places. The correct spelling is ""MNIST"": this is an acronym for ""Modified National Institute of Standards and Technology"".","This paper proposes a new sparsity-inducing activation function, and demonstrates its benefits on continual learning and reinforcement learning tasks. <sep> After the discussion period, all reviewers agree that this is a solid paper, and so do I. I am thus recommending it for acceptance as a poster. Hopefully, such visibility (combined with the open source release of the code) will encourage other researchers to try this new technique, and we will see more evidence confirming its usefulness in more varied settings and versus stronger baselines (that remain somewhat limited in the current work: this is the main weakness of the paper)."
"abstract | rebuttal_process  ==>  ==> The paper focuses on detecting ""individual unfairness"" in supervised learning. <sep> The main contributions are: <sep> A method to generate ""adversarial"" examples, that is, the examples that are very close to the original input, but get a very different outcome. An optimization problem is used formulated by leveraging the DRO framework. The authors then point out the difficulty in solving the problem (specially with continuous features) and propose an ODE based solution to find the adversarial examples. <sep> After finding the adversarial examples, a hypothesis testing framework is proposed to test the model for individual unfairness. The main idea here to compute the mean and variance of the test statistic on a given set of data points and then construct the confidence intervals using the Normality assumption. <sep> On a high level, the idea of testing for individual unfairness is an interesting one, specially given that there aren't many metrics of it individual unfairness out there. However, it feels like many important design choices are not very clear. For these reasons, this reviewer is split between a weak accept and a weak reject. See the detailed comments below: <sep> Intuitively, it seems like the need for hypothesis testing arises when one is working with a small test set (if the test set is large enough, then assuming IID samples, one could already be quite confident of the point estimate of the amount of individual unfairness as measured in Eq. 3.2). However, the paper then assumes that the distance metric is learnt from the test set. Now if the test set is already quite small, how good a metric do we expect to learn? It is not clear how to reconcile these two problems. <sep> How much ""interpretability"" does the hypothesis test really add? The test statistic does really provide a whole lot more interpretability on on top of the quantity measured in Eq. 3.2 (which itself it very closely related to that in Eq. 2.4). Essentially, the main insight seems to be to monitor the change in loss from an example to an adversarial version of it. So that additional benefit does the hypothesis testing bring here, specially when working with reasonably sized test sets (also, see the point about the need for hypothesis testing above)? Given that the paper does not offer any discussion into the tradeoff between type I and II errors, it is not clear that advantages does the hypothesis testing bring for us. <sep> Perhaps it would be worth discussing how the scale of the loss (<<0) might make the ratio in Eq. 3.2 unstable in practice. <sep> This reviewer is a bit confused about the Normality result derived in Theorem 3.1. True that the distribution here tends to a Normal when n→∞. However, with small test sets, how well does this assumption hold? If it does not, the interval constructed in Eq. 3.6 may not be very accurate. In general, an empirical analysis of the intervals as in (https://arxiv.org/pdf/2007.05124.pdf) would be a great addition to the paper. <sep> The paper seems to take the assumption that the distance metric specified by the user is differentiable (for solving the problem in Section 2). Is that true? It is quite possible for domain experts to specify distance metrics with discontinuities in them (e.g., if education level of x1 is higher than education level of x2, upweigh the distance by 1). Can such user-specified metrics be handled by the methods in the paper? <sep> This reviewer is not very sure about the usage of four-fifth rule for the hypothesis test. While I am not a legal expert, the four-fifth rule seems to apply to groups instead of individuals as suggested by the paper. Moreover, applying the four-fifth rule on well-understood and well-bounded quantities (acceptance rate of the two groups), as is done in the group fairness literature, indeed makes sense. However, applying the same ratio threshold on a quantity such as loss that can be arbitrarily high or low might not be very interpretable (also see point 3). For instance, if the original loss is very low (<<0) or very high (in the order of 10's), does it make sense to apply the ratio test. Similarly, the test might lead to very different behavior when the loss changes from say hinge loss to squared loss to logistic loss. Is this behavior indeed desirable? Some explanation here would be greatly helpful in convincing the readers of the usefulness of the ratio test. <sep> Post-rebuttal comments: Thanks for the detailed answers. Many of my concerns were addressed, and I am increasing my score as a result. A follow-up thought: It would be nice to add some discussion on the runtime of the proposed framework.","This paper studies how to statistically test if a given model violates the constraint of individual fairness. This is an interesting and novel problem, and the paper leverages the technique of gradient flow to identify a ""witness"" pair for individual fairness violation. <sep> During the rebuttal, the authors have addressed many concerns raised in the reviews. The author should also consider discussing the runtime and improving the exposition to resolve some of the presentation issues raised in the reviews."
"misc | abstract | decision  ==> Pros: originality, clarity, technical correctness. <sep> Cons: experiments need some clarifications. <sep> Calibration typically relies heavily on binning the data, both for the calibration itself  (Histogram Binning) and how to measure its quality (ECE). Thus both operations suffer from sampling issues that are a cause for both bias and variance. The idea to rather perform the analysis using cumulative distributions would seem obvious, as it has been tried on so many other problems, but I have not seen it used for calibration. They do it for both calibration and its measure: <sep> The use of the Kolmogorov-Smirnov test to measure calibration between the target and the output distributions. <sep> Spline fitting of the cumulative distribution to compute its derivative <sep> As the implementation details are far from obvious , there are several original contributions, especially in implementing the splines. <sep> I found the description both very clear and concise, switching between intuition and equations. <sep> The only part I found confusing is the second paragraph of section 4.2 (""One method of calibration..""). Fortunately, the next paragraph gives a very simple intuitive explanation by just stating how it is implemented. <sep> Experiments are very comprehensive and show improvements over Temp scaling and other methods. However, there are also some results that contradict previously reported experiments and need to be clarified: <sep> Besides Temp scaling, the main other methods are borrowed from Kull et al, so one could expect some consistency. However ECE results from Table 6 look very different from Table 3 in Kull et al. I assume these are different types of ECE: confidence vs. class-wise? In Table 1, KS result for the ODIR methods of Kull et Al are much worse than Temp scaling, in particular for CIFAR-100 and Imagenet. This contradicts results reported in Table 2 (this paper) and Table 3 (Kull et al) and should be explained. <sep> I am no expert in image classification,  but the 70% accuracy reported for CIFAR-100 seems way below current numbers, which have exceeded 80% since 2017, in particular for the proposed architectures (for instance Wide Resnet or DenseNet) https://benchmarks.ai/cifar-100.  However these numbers seem to be consistent with what is reported by Kull et al (Table 18 in https://arxiv.org/pdf/1910.12656.pdf), so I assume the issue comes from borrowing their architecture and scores. While this should not impact comparative results, it would have been more satisfactory to use baseline architectures that match the state-of-the-art. <sep> Additional experiments or discussions on the following would greatly help: <sep> As the spline method is not always better than Temp scaling, how do they compare from a computational viewpoint? How long does the binary search over thousands of calibration examples take compared to the DNN feed-forward? <sep> From Tables 1,2 and 6, KS error and ECE rank methods quite differently. One reason one should trust KS more is that it does not depend on binning choices, and rely on a time-proven test. But could one come up with an experiment that shows that KS is provably more reliable? <sep> Post rebuttal: I have read the authors responses with to my 4 questions, and appreciate how detailed and honest they are. They satisfy my concerns.",Many papers have been written on calibrating neural networks recently. This paper presents a definition of calibration that is more robust than the popular ECE measure while also being more discerning than the Brier score. Then it proposes a practical spline-based method of post-editing the output softmax scores to make them more calibrated. The method is shown to be better than existing methods both on their measure and established measure (thanks to reviewer's questions on that.). <sep> The paper should be of much interest to the community.
"abstract | rating_summary | rebuttal_process | suggestion | weakness | rebuttal_process | strength | suggestion | strength | decision  ==> Summary <sep> The paper introduces the Graph Information Bottleneck (GIB) which aims to learn the most-informative compressed representation Z given graph G with associated label Y. Further, it defines GIB-Subgraph which aims to learn the compressed representation as the subgraph Gsub which maximizes the mutual information within the family of subgraphs Gsub of G. The paper introduces bi-level optimization objective which has the following parts: <sep> (a) optimizing the mutual information loss Lcls between the subgraph representation Gsub and the graph label Y using the backbone GNN followed by aggregation of subgraph node embeddings Xsub and cross-entropy loss when comparing to graph labels. <sep> (b) approximates the mutual information LMI between the original graph and a subgraph I(G,Gsub) using statistics network fϕ which uses the backbone GNN to obtain graph embeddings (using mean/sum or pooling over node embeddings) followed by MLP over concatenated embeddings of G and Gsub. <sep> The procedure retrains the graph-subgraph mutual information estimator in the inner loop for each step (eqn. 10) before updating the parameters of the backbone GNN and the subgraph selection MLP and finally updating the subgraph-label MI estimator (Lcls). In order to obtain compact subgraphs, the paper introduces a regularization term Lcon closely related to graph cut. <sep> The papers shows empirically on downstream task of graph classification that adding the GIP objective improves classification accuracy. Further, on graph interpretation task, the authors show that the GIP objective improves the similarity of the retrieved subgraphs using domain-specific metrics. The authors also evaluate on graph denoising on the MUTAG dataset. <sep> Recommendation <sep> I vote for a strong accept. This paper is well-written, makes a clear theoretical contribution to the field as well as provides sufficient empirical evaluation. <sep> Questions to the authors <sep> I would have liked to see in the supplementary material an example of the algorithm on a toy graph example (similar to case study A). <sep> I wonder does the initialization have an influence on the final chosen subgraph nodes. Does S (node-assignment) (always/almost always?) saturate  as mentioned on page 5? <sep> What is the influence of the Lcon on the size of the final chosen subgraph. A table showing the size of final subgraphs (in term of output of MLP θ2 in Figure 1) might be helpful, though this is partially addressed in Table 4. <sep> For completeness, it would be good to provide in the supplementary material the properties of the datasets used e.g., number of graphs, mean/max/min number of nodes, edges, dimension of node features, dimension of edge features (if any), etc. <sep> It would have been good to see plots showing the convergence of the different losses as part of the bi-level optimization iterations. <sep> [optional] On the graph denoising experiment, it might be good to add more concrete evaluation both on larger graphs e.g. on graph families such as Power-Law, SBM as well as non-uniform edge addition.","This paper proposes a graph information bottleneck (GIB) framework for subgraph recognition, including the proposal of a MI objective as well as a bi-level optimization scheme for minimizing said objective. The paper receive mixed reviews, with two reviewers in favor of acceptance and two reviewers in favor of rejection. <sep> One negative reviewer was too short to judge and had low-confidence. I think most of the concerns arise from lack of understanding of the work and the authors adequately address this on the rebuttal. The authors are encouraged to make minor modifications for clarity. In particular, classical IB considers random variables x, z, y, and learns latent representation z that is maximally informative about output y and sufficiently informative about input x. Therefore, it is natural to expect that the input to GIB is a random graph. <sep> The other negative reviewer finds the paper lacks novelty and points to multiple references. The positive reviewers also ask about the connection with additional references. Im my opinion, the authors do an excellent job at clarifying the differences with all prior work mentioned by the reviewers, including the closest one, a GIB paper in NerurIPS 2020. In my view, the present submission contains sufficient novelty relative to prior work, specifically as it focuses on a different problem (sub-graph) and proposes a different optimization method. That being said, I think it is absolutely essential that the author responses be added to the paper. In other words, the final version must add citations to the relevant work mentioned by the reviewers and clarify the differences. <sep> All other comments from the two remaining reviewers are very positive: the reviewers find the paper contributes with ""quite interesting information theoretic objective functions that actually work on multiple graph learning tasks"" and ""makes a clear theoretical contribution to the field as well as provides sufficient empirical evaluation."" I share the views of the positive reviewers and recommend acceptance, subject to the authors incorporating their responses to the reviewers' comments."
"misc | rebuttal_process | decision  ==>  ==> #Summary <sep> This paper proposes a robust curriculum learning method that interpolates a regular loss and a consistency loss, aiming at a smooth transition from learning from clean data and then to noisy data with pseudo labels. <sep> #Pros <sep> The paper is well-written, and the results over several benchmark datasets seem to be strong. <sep> Some of the insights provided in this paper, seem rather interesting, like by transitioning from supervised learning, to self-supervised learning of noisy data, can better benefit the learning process. <sep> #Cons <sep> The authors should perform an ablation study of the RoCL method. Currently the final proposed method mixes too many components, and it is hard to disentangle the true contribution of each component. For example, in section 3.4, the authors mentioned additional techniques were added, like class-balance regularization, label-smoothing, and mix-up, a further analysis is required to understand the true contribution for each individual part. <sep> Similarly, for all the baselines used, the authors should do a better categorization of each baseline method, to ensure a fairer comparison. For example, does any of the baselines use model averaging, mix-up, label-smoothing, or data augmentations? How is RoCL without mix-up compared to baselines that don't use mix-up? <sep> If I understand correctly, one of the key contributions is the interplay between the regular loss and the consistency loss, but the scheduling part is not super principled and seems to involve a lot of ad-hoc tuning of the balancing parameter λ and the temperatures. Is there a principled way to balance the two losses? <sep> The RoCL algorithm seems to involve a lot of parameters. The averaging parameter γ for EMA, λ for the trade-off between loss and consistency, and tempature τ1,τ2 (and the additional params required for scheduling the temperatures properly). In practice, how are those parameters picked? Is there a lot of careful tuning required? <sep> The paper seems to have combined a lot of existing techniques. The paper would be stronger if the authors can provide further analysis to better understand how/why RoCL works. E.g., how important is EMA/data augmentation/data sampling respectively? <sep> #Overall recommendation <sep> Overall I'm on the fence but tend to reject. I think this paper can benefit a lot from better organizing of the methods and results, with a clearer focus on its major contributions. Currently the experimental result is a mixture of multiple existing approaches and the proposed RoCL method, it is hard to know what the role of each approach is and whether the proposed RoCL method indeed improves curriculum learning. I think this paper has the potential of providing some great insights, but the current set of results are rather noisy. <sep> #Minor comments <sep> Figure 1 is really small and hard to read. <sep> The paper needs to be better organized. Currently it seems like the authors run out of space and rushed through the experimental results. <sep> Can you clarify if the γ is the same for Eq. 1, Eq. 3, and for computing θ¯t? If they are different please use different notations. <sep> I don't quite get Eq. 9, why is the temperature defined in this way? <sep> How important is the sampling part in Algorithm 1? It involves another parameter bk and how sensitive is RoCL to the choice of that parameter? <sep> The consistency loss over augmented examples is also a commonly-adopted technique in semi-supervised learning, citations to the use of those methods in existing literature are missing, e.g., [1] Mehdi Sajjadi, Mehran Javanmardi, and Tolga Tasdizen. Regularization with stochastic transformations and perturbations for deep semi-supervised learning. NeuIPS 2016. <sep> [2] Samuli Laine and Timo Aila. Temporal ensembling for semisupervised learning. *CONF* 2017.","This paper has been thoroughly evaluated by four expert reviewers and it had received one public comment. The authors provided extensive explanations and added technical updates to the contents of their submission in response to constructive critiques from the reviewers. Even though some minor issues have not been fully resolved in the discussion between the authors and the reviewers, I consider this paper worthy of inclusion in the program of *CONF* 2021 since, albeit marginally, the apparent strengths outweigh its outstanding limitations."
"rebuttal_process | strength | decision | suggestion | decision  ==> In this manuscript, the authors present a method for predicting residue-residue contacts within protein structures using the attention layers learned by transformer language models. Using the largest transformer language models trained to data, the authors show good performance for contact prediction. The paper is clearly written and easy to follow. <sep> The general concept of fine tuning protein language models for contact prediction has circulated for some time which lessens the core contribution, but the authors approach is surprisingly data efficient and accurate. Overall this is an interesting work, though there is quite a bit of background on contact prediction missing. This paper is also very application specific and may not present new machine learning methods of general interest to the *CONF* community. The existence of previous language model-based contact prediction methods reduces the novelty of this work, especially given that the model used here is from Rives et al. 2019, who already look at contact prediction. Furthermore, no comparisons with state-of-the-art evolutionary coupling-based or language model-based contact prediction methods are performed. With this in mind, the manuscript may be better suited to submission at a biology specific venue. <sep> Additional specific comments follow below. <sep> Major comments: <sep> Missing related work: there are a number of highly relevant prior works that are not mentioned/discussed. In particular, ""Deep generative models of genetic variation capture the effects of mutations"" – Riesselman et al. 2018 was, as far as I know, the first paper to show that deep generative models capture structure information (see Figure 6). Following that, ""Learning protein sequence embeddings using information from structure"" – Bepler & Berger 2019 was, to my knowledge, the first paper to propose deep language models (alignment free) for learning protein sequence representations and used those unsupervised representations for contact prediction. Furthermore, there has been extensive work in improving contact prediction using sequence + co-evolutionary features. See, for example, ""Enhancing Evolutionary Couplings with Deep Convolutional Neural Networks"" Liu et al. 2018 and ""Accurate De Novo Prediction of Protein Contact Map by Ultra-Deep Learning Model"" Wang et al. 2017. Other papers looking at protein structure prediction from sequence with deep learning, though they are less directly relevant, include ""End-to-End Differentiable Learning of Protein Structure"" AlQuraishi 2018 and ""Learning Protein Structure with a Differentiable Simulator"" Ingraham 2019. <sep> Before this work, others have looked at fine tuning language models for contact prediction. How do those approaches compare with the approach presented here? Rives et al look at contact prediction in their manuscript describing the transformer model (which is the same model used here) on CASP 11-13 (see Table 5 in their manuscript). How does that approach compare with this one? Likewise for Bepler & Berger <sep> Many methods have surpassed GREMLIN for contact prediction using evolutionary couplings. How do those approaches compare with this one? It would be helpful to see how this approach compares with truly state-of-the-art contact prediction methods. Reporting results on the CASP data would help to make this comparison. <sep> Minor Comments: <sep> Although multiple sequence alignment methods have challenges especially as related to evolutionary coupling prediction, these methods have been heavily optimized for decades. The authors should provide citations for claimed failings such as ""failure to find an optimal alignment"" and ""suboptimality of the substitution matrix and gap penalty."" Certainly, these may be sources of error in alignments, but I am not aware of any studies of the frequency or impacts of these errors on evolutionary coupling analysis. If these studies exist, I encourage the authors to cite them. If they do not exist, I suggest the authors focus on well known sources of error here (namely, alignment depth) and provide references. <sep> The authors use the language model without fine tuning, but the model could be fine tuned for each protein using its MSA. It's great that contacts can be predicted without fine tuning, but it would be interesting to investigate whether additional gains can be made. <sep> Eight iterations of jackhmmer is a lot. In my personal experience, jackhmmer often diverges at 3+ iterations. By this I mean, the set of sequences and HMM learned by jackhmmer drift far away from the original sequence/family. Did the authors perform and quality checks of these alignments to ensure jackhmmer did not diverge? <sep> How are sequence depths in Figure 3 calculated? Is this the raw number of sequences in each MSA or is it after applying some sort of neighborhood weighting to calculate an effective number of sequences? <sep> Things that would improve my rating: <sep> Provide a more comprehensive background review. <sep> Compare with state-of-the-art evolutionary coupling-based contact prediction methods. <sep> Compare with other language model-based contact prediction methods. <sep> What should interest the general machine learning community about this paper? What can we learn that might lead to better ML methods in the future? Convince me that this doesn't belong in a bioinformatics venue!","The authors have done a very thorough job of responding to the comments from reviewers. The paper has a clear contribution, namely that attention maps predict contacts as well as existing unsupervised pipelines. This paper deserves to be published. <sep> In the final version, the authors should discuss briefly ""BERTology Meets Biology: Interpreting Attention in Protein Language Models""(https://openreview.net/forum?id=YWtLZvLmud7) and ""Improving Generalizability of Protein Sequence Models via Data Augmentations"" (https://openreview.net/forum?id=Kkw3shxszSd). However, the authors should also make sure that the final version respects the *CONF* length limits. <sep> I am recommending poster acceptance because the results are anticlimactic given the recent success of Deepmind at CASP 2020."
"abstract | strength | abstract | rebuttal_process  ==> This paper introduces a new algorithm for learning adversarially robust models in the semi-supervised setting, where a small amount of labeled data is available together with a sizeable unlabeled dataset. The proposed approach BYORL adapts an existing self-supervised learning method BYOL by introducing a new adversarial augmentation technique based on maximizing the cosine similarity between representations. BYORL is evaluated on CIFAR-10 and compared against a recent pseudo-labelling based approach UAT-FT for the semi-supervised setting, and is shown to outperform UAT-FT in terms of robust accuracy under ℓ2 and ℓ∞ attacks under the low-labelled data regime. The representations learnt by BYORL are also shown to be better than that of UAT-FT when transferred to other datasets. Finally, robust representations are shown to be more important than learning a robust linear classifier on top. <sep> Strengths: <sep> Significant improvements in robust accuracy in the low-label regime <sep> Interesting observations regarding transferability of robust representations and importance of final robust linear classifier <sep> Paper is generally clear and well-written <sep> Weaknesses: <sep> RoCL as introduced by ""Adversarial Self-Supervised Contrastive Learning"", NIPS 2020 can also be applied to the semi-supervised setting, which somewhat compromises novelty. Ideally this method should be included in the experimental evaluations. This paper was cited but not discussed in the context of related work, but should be. <sep> Label budgets claimed do not include sizeable validation set used for early stopping, which can sometimes exceed the label budget (e.g. result with 500 labeled images also uses validation set of 1024 examples). <sep> Experimental evaluation could be strengthened in a few ways: <sep> Clean accuracy comparison for Fig 3/4 - does UAT-FT do better in terms of clean accuracy? <sep> It would be informative to include the robust accuracy of a model trained directly on STL-10 and CIFAR-100 in Table 2 to show how good the transferred representations are. <sep> Experimental results seem to be from a single run; multiple runs would give an idea of the variance. <sep> Overall, the robust accuracy improvements achieved in the low-label regime by BYORL are significant and the method is somewhat novel. The paper could be further strengthened by improving the experimental evaluation as described above. The highly related work RoCL should be discussed and ideally compared with. <sep> Other comments: <sep> The paragraph after Eq 8 was confusing to me. What is meant by symmetrizing the loss? What is the final loss used to train the model after symmetrization? And why does the argument about batch-norm statistics not apply to the online network as well if the loss is symmetrized (which I took to mean that both models would be separately attacked and the losses added up)? <sep> Some analysis into the representations learnt would be helpful to give some insight into why the method works. <sep> Why does the method work better under ℓ2 attacks as compared to ℓ∞ attacks? <sep> What is the effect of different transformations on robustness? Are the transformations important for robustness different from those important for clean classification? <sep> * Post Response Comments * <sep> I thank the authors for addressing the points raised. I am raising my score accordingly to 7. <sep> Nit: The y-axis labels on Figure 7 and 8 should probably say ""Clean accuracy"" instead of ""Robust test accuracy"".","The paper considers the use of adversarial self-supervised learning to render robust data representations for various tasks, in particular to integrate the Bootstrap Your Own Robust Latents (BYOL) with adversarial training, where a small amount of labeled data is available together with a sizable unlabeled dataset. Especially the low-data regime is of interest. It extends a previous method with a new adversarial augmentation technique, it is compared against several methods, and the robust representations are shown to be useful more generally. There were some confusing presentations and questions that were resolved in a detailed discussion with the reviewers."
"abstract | strength | rebuttal_process | decision | suggestion  ==>  ==> Summary <sep> The paper leverages two methods for improving generalization in standard training, logit smoothing and stochastic weight averaging, and show that these results can mitigate robust overfitting and improve generalization for adversarial training methods. <sep> Overall, the paper was clear and easy to follow. There are a number of ablation studies showing the marginal effects of the two methods, as well as experiments demonstrating how the approaches vary with certain choices in methodology. My initial impression is positive, though there are certain changes described below which would help solidify the paper and its claims. <sep> Comments for discussion <sep> By improving upon the results in Rice et al. 2020, the authors purport to have state of the art results. However, there's be a plethora of new work since then which have improved these numbers even further. Fortunately, since the submitted work handles the standard CIFAR10 setting, there are a number of public benchmarks that can be used here. It would be great if the authors could train a comparable model and evaluate it using one of these benchmarks (e.g. the autoattack framework at https://github.com/fra31/auto-attack). <sep> To be clear, since a number of these approaches on the benchmark are quite recent, I am not requesting that the authors directly compare to these new methods in their work. However, the baseline that they do compare to (e.g. Rice et al. 2020) is evaluated in this framework (and had a not-insignifcant drop in robust accuracy), so it would be of significant utility to also evaluate the approach using the improved attack. Performing this evaluation would serve two purposes: <sep> This should alleviate most concerns on the validity of the result <sep> This makes the work easily comparable for future work <sep> Note that reaching the top of the benchmark is not a requirement for publication. As long as it is consistent with the claims of the paper, that the approach reduces robust overfitting for PGD training and improves upon the PGD baseline within this benchmark, then this is fine. If the authors can report how their approach performs under this improved evaluation or a comparable alternative, then I am happy to adjust my score accordingly. However, the authors probably shouldn't claim state-of-the-art performance without doing this evaluation first. <sep> Minor comments <sep> In section 3.2, it is mentioned that Table 2 supposedly shows differences when a robust self-teacher is pretrained, but this does not seem to be the case. <sep> Update <sep> I have looked through the response and edited version. The updated evaluation looks solid and provides a potential solution to a robust overfitting problem. Although the work is primarily empirical in nature, it may inspire directions for future work to look into more theoretical explanations of robust overfitting.","This paper focuses on the problem of robust overfitting. The philosophy behind sounds quite interesting to me, namely, injecting <sep> more learned smoothening during adversarial training. This philosophy leads to two simple yet effective methods: one leveraging knowledge distillation and self-training to smooth the logits, and the other performing stochastic weight averaging to smooth the weights. <sep> The clarity and novelty are above the bar of *CONF*. While the reviewers had some concerns on the significance, the authors did a particularly good job in their rebuttal. Thus, all of us have agreed to accept this paper for publication! Please carefully address all <sep> comments in the final version."
"abstract | rating_summary | strength | weakness | rebuttal_process  ==> The proposed BlendedSearch (BS) presents an intuitive next step in the combination of global and local search schemes for hyper-parameter optimization (HPO). Global search schemes are widely used for HPO but can suffer from large HPO times since their vanilla forms do not account for function evaluation costs. Local search schemes are usually not widely used for HPO but seem useful if the goal is to restrict the search to a region of the search space where the function evaluation costs do not grow drastically. The proposed BS interleaves global and local search steps to ensure that the global search does not go into regions of high evaluation costs while also avoiding being stuck in local minima. <sep> The empirical evaluation of BS against various baselines demonstrate that BS has a better anytime performance relative to both local and global schemes. BS is also able to leverage multi-fidelity HPO schemes for reduced evaluation times and further improve upon them. The empirical evaluations also highlight the effect of the different parts of the proposed algorithm through an ablation study. <sep> However, the proposed algorithm raises a lot of questions that are not addressed in this paper. <sep> Most importantly, in my opinion, the empirical advantage of this proposed scheme is tied to how low-cost (and maybe low-loss) the initialization is from where the global and the local searches start. The initializations for the empirical evaluations are chosen to be low-cost (and maybe low-loss) for BS -- it is not completely clear from the main paper how these initializations were chosen and how (if at all) the considered baselines leveraged these initializations. It is not clear how BS would perform if initialized with a high-cost, high-loss point. It seems to me that having low-cost initialization(s) would also benefit something like GPEIPS -- instead of picking a single initial point, the GPEIPS could be initialized (similar to the proposed scheme) with a set of low-cost evaluations -- this would counter the issue in GPEIPS of requiring high-cost evaluations initially. <sep> Another unknown is the interplay between the global and local search -- it is not clear whether the proposed scheme really leverages both global and local searches. In this current setup, where the local search threads focus on local ""low-cost"" regions, it seems plausible that the global search constantly get rejected repeatedly and never really used much (beyond initiating a few local threads). Given the ""exploitation"" evaluations obtained from the clearly favored local search threads, the current surrogate model (such as the GP regressor) would probably be in the ""exploration"" mode (trying out regions of the search space not part of the valid bounding box), which will often just get classified as invalid. For points to be valid, the Δi for the local searches need to be large, but the choice of these Δis are not discussed in this paper. <sep> Moreover, the MBB of the union of the all the local thread regions does not imply that the evaluation costs stay low for anything in that region for the global thread to pick without additional assumptions on the cost structure. It might be good to justify such a choice. The ablation study shows that the anytime performance is heavily reliant on such a ""ConfigValidator"" that keeps the search in low cost regions, but this ties back to the question about how the search gets initiated and how that would affect the performance of BS. It is not technically clear how the current algorithm can recover if started from high-cost region. <sep> Beyond this, there are various other questions which makes me think that we are not understanding the technical reason for the strong empirical performance, which makes me score this as a borderline paper. <sep> Further questions: <sep> The Δi in the local search drives where the global search can look, so it is not clear how best to set that value (especially given the varying types and ranges for the hyper-parameters). <sep> Also it is not clear how local search is accomplished with mixed continuous-integer-categorical hyper-parameters. <sep> Why is ""Trust region BO"" (Eriksson et al., 2019) not applicable where the local models have ""low-cost"" initializations similar to the low-cost initializations used for BS? <sep> As per notation, given a budget B, the goal of the problem is to minxP.LossFunction(x) subject to G(π)≤B, <sep> not necessarily minimizing G(π). <sep> In Fig 4(a), where evaluations are not costly, how is it that BS is outperforming the BO? What is the intuition behind that? Shouldn't BO be the best? Fig 1(b) makes more sense -- eventually BO and BS have similar performance but BS has improved anytime performance. <sep> Regarding ""performance improvement speed S.s"", it is unclear why speed is set to highest when the formula suggests it should be 0. This needs better clarification. <sep> Again, pertaining to the definition of performance improvement speed, if S.l2nd=S.l1st, isn't S.c1st=S.c2nd by definition? What does that mean for the definition of speed? <sep> Formula in eq(2) could use more justification -- intuitively using the notion of ""speed"", we would have S.s=(S.l1st−l)/(S.CostImp(l)+S.c−S.c1st), implying S.CostImp(l)=(S.c1st+(S.l1st−l)/S.s)−S.c. <sep> It appears that the ""Priority(S)"" is some form of a UCB over a local thread -- it would be good to formally discuss how (if at all) this priority is connected to well established notions of UCB/EI. <sep> In ""Step 3: Search thread creator...."", what is precisely meant by ""incumbent of a LS thread is reachable by another LS thread"" <sep> The paper does not clearly discuss how the fidelity of the global search (handled by HyperBand, BOHB, etc) translates to the fidelity of the local searches when leveraging multi-fidelity schemes","This paper presents a hyperparameter optimization (HPO) method in which two search strategies: global and local optimizations, are effectively combined. All reviewers evaluated the proposed method positively. The experimental results clearly show the effectiveness of the proposed method, and it could be an important contribution to the AutoML research community. On the other hand, since there is no theoretical justification for the proposed method, it is not clear why the performance of the proposed method is improved so much. The author's rebuttal has alleviated some of our concerns on this point, but the further theoretical analysis is desirable."
"abstract | rating_summary  ==> Significance <sep> I view this paper has high significance level. As more and more OPE methods arise, papers rely on different datasets and metrics, which makes them hard to compare, and especially see the ""success"" and ""failure"" mode of this method. We need a benchmark dataset on this domain and this paper moves an important step towards this by providing standardized logged dataset, and various policies in different challenging, complex domains. <sep> Novelty <sep> There is a very similar work by Voloshin et.al (2019), which gives the empirical evaluation of popular OPE methods across various domains. Though not providing the logged data, they give the logging policy to logged the data, I am concerned about the novelty of this work compared with Voloshin et.al (2019). It would be great if the author could discuss the novelty/improvement over that work. <sep> Quality & Clarity <sep> This paper has a great structure, metrics/choice of datasets/design of policies are well motivated and clearly stated, it is a very easy to read paper. <sep> This work gives a clear description of the logged datasets/policies/and the performance of various off-policy method. However, I feel it would be great if the authors could also think, and if possible make the following discussions along with this paper: <sep> (1). it would be great if the authors could give some statistics to measure the property of the datasets, such as a measure of the missing support of logging policy, how difference are the logging policy and target policy? <sep> (2). I know the authors discuss some properties of the environment in words, such as sparse/dense rewards, stochasticity of the environment, etc. It would be great to summarize these environment info into a single table. <sep> (3). A very important class  of estimators in OPE is these kind of interpolated estimators between value-based and weighting-based, which contains a trade-off of bias-variance, such as the work by Phillip and Emma (2016). It also shows in Voloshin et.al (2019)'s empirical evaluations and shows very promising performance. However, it is missing in this paper and it would be excited to see how it works in the D4RL tasks. <sep> (4). A further extension from (1) is that the authors did a great job in doing lots of empirical evaluations in various domains. I saw aggregated performance in main paper and individual task performance is listed in Appendix. It would be great to classify them into groups which depends on  the MDP property, the metric measuring difference of logging/target policy, and etc... This will definitely give some further directions like which method perform well in which setting. This is also shows in Voloshin et.al (2019)'s paper but kind of missing in this work. <sep> (5). One difference compared with Voloshin et.al (2019) is that it seems in the logged dataset, they know the propensity. However, I feel the dataset released along the paper does not have propensity. Will is possible to also add some datasets with propensity included? <sep> Overall, I feel this paper addressing an important problem, however, I feel there is still room for improvement to make them have more impact. I am willing to adjust my score if the authors address some of these issues. <sep> Ref: <sep> Philip S. Thomas, Emma Brunskill  Data-Efficient Off-Policy Policy Evaluation for Reinforcement Learning. <sep> Cameron Voloshin, Hoang M. Le, Nan Jiang, Yisong Yue Empirical Study of Off-Policy Policy Evaluation for Reinforcement Learning.","This paper proposes benchmark tasks for offline policy evaluation. The proposed benchmark tasks evaluate the policy with batch data with respect to three metrics, including the standard mean squared error. The paper also evaluate several baseline offline reinforcement learning methods with the benchmark tasks, which will serve as standard baselines. All of the reviewers are in favor of the paper."
"abstract | rating_summary | decision  ==>  ==> Summary Of The Manuscript: <sep> This manuscript focuses on the problem to protect the users/humans from unauthorized facial recognition systems. To tackle the issue mentioned, the author proposes a customized adversarial filter to work against industrial/government facial recognition systems. In addition, the author claims that their easy-to-use web tool helps to significantly degrade the accuracy below 1% of AWS Service - Amazon Rekognition and Microsoft Azure Face Recognition API for face recognition and similar systems. <sep> Strength Of The Manuscript: <sep> Clarity: <sep> ++ The paper reads very well and provides a very good description of related work and background, motivating the problem. Even outside of the contribution of this paper, I would recommend this paper to people getting started with protecting users' information from industrial systems as it provides a thorough description of the part of the pipelines it deals with. <sep> Novelty: <sep> There are mainly two points are novel which are presented very well and are as follows: <sep> ++ The author designs a custom black-box adversarial attack on facial recognition models where the proposed algorithm changes the representation of the features in such a way that it will preserve the image quality. <sep> ++ The in-depth analysis with the commercial - Amazon Rekognition and Microsoft Azure Face Recognition APIs shows the practical usage of the author's proposed adversarial attack. <sep> Experiments: <sep> ++ There are a number of experiments performed across datasets that are extensive and fair. The fact that the proposed adversarial attack achieves better results while preserving the image quality, makes me confident in the result as the implementation and experiments are sufficient and presented in a good manner. Additionally, the improvements are fairly consistent. Besides, in-depth analysis/ablation studies are done on the robustness of the approach, and comparison with the commercial APIs provides a thorough analysis of where the benefits of the author's approach are obtained. <sep> Reproducibility: <sep> ++ First of all, I would like to thank the author for providing code in the supplementary material and because from the code, most of my doubts have been solved and I can say this that the code is written in a very good manner and helped me to understand the whole pipeline of their work and I appreciate the authors for their effort. <sep> Weakness Of The Manuscript: <sep> Overall, apart from the contribution of the approach, I have some concerns regarding the manuscript. <sep> -- Can the authors provide a brief description of the core difference between their proposed adversarial attack (LowKey) and attack generated by any good GANs based model (i.e. Style GAN.). I believe that the standalone contribution of the manuscript is to create such an attack to hide the user's identity. Thus what makes a user believe that the proposed webtool will be helpful to hide their identity if it only applies a LowKey adversarial attack. <sep> Justification Of The Review: <sep> -- In the reviewer's opinion, in its current form, the paper provides in-depth analysis for protecting the user's identity from any industrial/government surveillance facial recognition system and the authors did an excellent job to provide a brief insight on their complete pipeline. I appreciate the author's efforts to provide a code in supplementary material which has nullified my doubts in such a manner that makes me believe that the work is incremental and should reach the Computer Vision community. Therefore the current rating of the paper will be 7 in reviewers' opinion because of fairly consistent work and practical usage.","This paper presents a method named LowKey, which is designed to protect user privacy. This is done by taking advantage of adversarial attacks to pre-process facial images against the black-box facial recognition system in social media, yet the processed facial images remain visually acceptable. The paper experimentally illustrates that it is effective against two existing commercial facial recognition APIs. <sep> The reviewers unanimously agree that this is an interesting and important problem, and recommend the paper for acceptance. The ACs agree."
"strength | suggestion | decision  ==> The authors propose a two step procedure for generating molecular graphs that optimize some desirable properties. The method consists of a rationale extraction phase, where the subgraph ""important"" for the desired property is identified and a graph completion step. <sep> When reading the paper for the first time, I found it a bit hard to follow the approach. The paper might be easier to read when the individual model components are introduced directly with the E- and M-step. Currently, the graph completion model is introduced in Sec 3.1, then the E- and M-steps are described in Sec. 3.2, while the ""explainer"" is only mentioned and referred to Sec. 3.3. An easier to follow structure might be: 1) Rationale extraction 2) Graph completion 3) E/M iteration. <sep> Beyond that, the terms ""evolution"" and ""explanation"" might be misleading here: The method is not an evolutionary algorithm, as the title might suggest. Even if subgraphs are extracted that lead to the generation of promising molecules in the graph completion step, this does not show that these subgraphs responsible. To warrant the term explaination, a  thorough analysis of the extracted rationales would be necessary, in particular since they might not even be connected graphs. <sep> The evaluation of the method in Table 1 and Fig. 3 show that the method outperforms previous appraoches and is capable of shifting the generated distribution shifts to higher scores. The human evaluation is weak, since only a single expert was asked. A panel of experts with reported agreement among the panel would improve the paper. However, since the scores of the molecules are available, it does not become quite clear to me, how human experts benefit the performance evaluation under those same criteria. <sep> Pros <sep> Interesting approach to update the pool of rationales <sep> Outperforms previous approaches, Fig 3. show that desired properties improve over rounds <sep> Ablation study demonstrates improvement by proposed procedure <sep> Cons <sep> Structure of the paper could be improved <sep> The paper states that in the explainer a ""subgraph s of k vertices"" is extracted, therefore I assume the size of the rationale is fixed. This would severely restrict the space of rationales. <sep> The notion of explainability is not sufficiently discussed in the paper and the claim(?) that the rationales are somehow meaningful is not examined. <sep> Update: I read the reply and thank the authors for the clarifications.","The authors appreciated this submission because (a) the aspect of explainability is novel, (b) its strong performance, (c) the clarity of the paper. I urge the authors to double check all of the reviewer comments to make sure they are all addressed in the updated version. I vote to accept."
"abstract | strength | rebuttal_process  ==> This paper addresses an important symmetry in meta-learning.  Namely, the context data consists of a set of datapoints in arbitrary order.  The model should thus be permutation equivariant to their order.  At the same time, the data itself may have its own symmetries, e.g. rotation, which the network should likewise be equivariant to.  The authors follow a theory-driven approach, proving in Thm 2 that a function with these two types of symmetries may be factored and represented by a composition of functions reflecting each symmetry individually.  They then design a Neural Process (NP) model, EquivCNP, which reflects this result.  Other works have used permutation equivariance and translation equivariance in NPs, but this is the first to incorporate other symmetry groups. <sep> As a weakness, the method description in Section 4 is often imprecise and unclear as noted in the specific points below.   The method is somewhat novel; it may be described as a combination of two existing methods: permutation-equivariant NPs (Garnelo 18) and LieConv (Finzi 20).  The experiments achieve good results, but could be much more convincing.  They concern fitting only functions on low-dimensional space, report only one metric, and have limited comparison to baselines.  In particular, it does not seem necessary to frame image reconstruction as a meta-learning task.  However, the strength of this paper is the theory-guided model design.  The theorem proved is wholly appropriate to the task and informs the model design very nicely.  In particular, it makes use of the realization that DC→C(X,Y) can be G-equivariant; that is, the symmetry transformation can be inferred from context and transferred onto the prediction function.  Thus I suggest acceptance (6). <sep> Specific Point and Questions <sep> Sec 2.1 ""Weiler & Cesa ... is costly"" I do not agree.  However cost is defined this method is on par or better than CNNs <sep> Sec 2.1 ""learning the objective from the data."" I do not follow.  Can you explain further how this applies to EquivCNP? <sep> Sec 2.1 How do the symmetries in this paper compare to those considered in (Maron et al 2020, On Learning Sets of Symmetric Elements) which also considers permutation and Lie group equivariance together? <sep> Defn 3, It seems here we are giving all the orbits the same weight? Should there be a measure on orbits?  Considering the SO(2) action on the plane, should not the circular orbits be weighted by circumference? <sep> Sec 4.2 ""LieConv … can handle Lie groups"" This is true, but many other architectures such as steerable CNN can as well. <sep> Sec 4.2 ""we define distance in the Lie group."" Since we have lifted from the space X, is it important to consider differences between measuring distance on X vs. G? <sep> Sec 4.2 |qi−qj|, how do you define distance between orbits? <sep> Sec 4.2 Since compact groups have a bi-invariant metric, why not use that instead of d(u,v)? <sep> Page 5, first Eqn, According to which distribution are (ui,qi) sampled? <sep> Page 6, para 1, it is not clear exactly what precisely is meant by discrete or continuous data. <sep> Page 6 ""although we want to convolve E(Z)..."" I do not follow this sentence.  Can you clarify it? <sep> Page 7 ""MNIST already contains diverse group equivariance"" If this is the case then the equivariant model should still have good performance, but perhaps with less improvement over the baseline, correct? <sep> Page 8, Why not test SO(2)-equivariant model on test data which has only been rotated? Does not the success of R×SO(2) over the other groups simply show that the others have misspecified (or perhaps underspecified) inductive biases? <sep> Page 11, second equation, should be ψ(g−1x,g−1x′) since it is a left-action.  This will not change the rest of the proof. <sep> Page 11, proof of (II) implies (I) feels incomplete.  Can you define ρ,H here? <sep> Minor Points <sep> Sec 3, first equation, πZn:=…, the subscripts should be π−1(i), not π(i). One can see this in a simple example or from the fact π is a left-action. <sep> Sec 3, by mult(Z′):= is sup over Zm but contains Zn. <sep> Sec 4.1, ""lifting into K elements"" How do we know the stabilizer is finite? <sep> Sec 4.2, note that not all Lie groups are matrix groups as seems to be implied here. <sep> Sec 4.2, Last Eqn on Page 4, the integral can be written over G since the support of gθ limits to the neighborhood of u <sep> Bottom Page 4, footnote, triangle, not trigonometric <sep> Page 5, first Eqn, sum should be over (vj,qj′). <sep> Since the support of gθ is limited to a neighborhood of 1, it may not be necessary to assume exp is surjective since exp is always surjective in a small neighborhood of 1. <sep> Sec 4.3, first Eqn, the first line is a product of density functions, but the second is a product of distributions.  This is unclear. <sep> Sec 4.3 EquivDeepSet is not introduced <sep> Sec 4.3 ""a single output per one input"" Is this backwards? <sep> Page 6, line 3, ϕ with ψ <sep> Sec 5 ""two questions"" should ""three questions"" <sep> Page 11, Proof switches between X and S, make consistent <sep> Page 11, What do the brackets mean? This notation should be defined. <sep> Update Based on Author Feedback <sep> I am grateful to the authors for their detailed replies to my questions. I understand some of the minor points better and am happy they have revised some unclear parts. Overall, I think this paper has some real strengths: theory-guided design, important problem, novel methods. I do feel (similar to R3) that the experiments and applications could have been far more convincing. Weighing these strengths and weakness, I still tilt slightly towards accept (6).","This paper provides a natural combination of conditional neural processes with LieConv models. It is a good step forward for stochastic processes with equivariances. While there is still room to improve the experiments, the authors provided a good response to reviewers, and the paper is a nice contribution."
"abstract | strength | weakness | rebuttal_process  ==> This paper proposes a method for learning representations for relation statements as well as classes (prototypes) for relation extraction tasks. I think the key insight is that the relation statements and classes (corresponding to relation types) can be learned jointly using contrastive training objectives. The paper also proposes to use the similarity metric as 1 / (1 + exp(cos(u, v))) and claims that this will provide a clear geometric interpretation and more interpretability. In the experiments, they first train this framework on distantly-supervised data constructed from Wikidata + Wikipedia and then fine-tune it on several downstream tasks: FewRel, SemEval, and a new dataset they created focused on identifying false positives in distantly supervised data. <sep> Overall, I think the proposed approach is quite reasonable and also seems to work well in the evaluation tasks (learning prototype embeddings instead of taking the average of instance embeddings and adopting contrastive losses between relation statements and prototypes). However, I think the paper has many clarity and presentation issues that make it difficult to evaluate the significance of the work. <sep> First of all, I think this pre-training stage on weakly-supervised data is very crucial and the details of the data collection (which relations and how many instances have been used) should be moved to the main body of the paper instead of the Appendix. In realistic few-shot scenarios, you only have a very small number of examples for a new relation k so it is difficult to learn the r_k embedding from only a few examples. My interpretation is that for the FewRel evaluation, the relations must have been already seen in the pre-training stage (given both the weakly-supervised data and FewRel are collected based on Wikidata) unless I have misunderstood something (especially that the model can achieve a good accuracy when there is no training data used in Figure 3 & 4). For the SemEval evaluation, I assume the prototype embeddings must have been learned from the training data but it has 6k training examples so it is fine. I think this point really needs to be clarified and can be a weakness of the approach, especially in few-shot settings. <sep> For the results in Table 1 & 3, it is not very clear to me whether the numbers of previous approaches are from their papers or re-run by the authors. This should be clarified. My main concern is the pre-training data used in this paper can be different from what has been used in (Soares et al, 2019 -- they didn't use Wikidata and only consider Wikipedia and the links) and it makes the comparisons unfair. <sep> The authors claimed that this similarity metric is crucial but there is no ablation study or comparison to other alternatives… How if you just compute the dot product between the two embeddings? I think the comparisons to commonly used similarity metrics need to be added to justify why this design choice is important. <sep> I also don't know what the L_{CLS} training loss is used for. If z_k is just a set of learnable embeddings (one embedding per relation) and if it is used to predict the relation k, isn't it just multiplied by another set of K embeddings? What is the benefit here? <sep> Also, I don't understand the fixed prototype baseline (denoted as IND). What does ""fixed prototypes z that are pre-computed by vectorize extracted patterns for each relation"" mean? <sep> To sum up, I can't recommend the acceptance of the paper if the above issues cannot be addressed. I am also concerned whether the highlight of the approach is the contrastive losses and prototype embeddings, or it has to be coupled with some type of pre-training (or even specifically pre-training on distantly-supervised data). <sep> Minor: <sep> Equation (7): The second S2Z should be S2Z'.","This paper proposes a method for regularizing the pre-training of an embedding function for relation extraction from text that encourages well-formed clusters among the relation types. Experiments on FewRel, SemEval 2010 Task 8, and a proposed FuzzyRed dataset show that the proposed prototype method generally outperforms prior state-of-the-art, including MTB (Soares et al., 2019), which was the strongest. The key, novel idea is to model prototype representations for target relations as part of the learning process. A contribution of the work is to show that learning prototype representations are useful in supervised deep learning architectures even beyond few-shot learning. This additional learning objective is useful as an inductive bias, and is perhaps of interest even beyond relation extraction research. <sep> Reviewers generally found the proposed method sound and intuitive, and the original set of experiments promising. Some of the reviewers raised concerns about the setup of the experiments, including the relationship between the pre-training and target tasks, and the need for several additional baselines. The authors were able to address these concerns, and the reviewers did not raise any follow-up concerns."
"abstract | decision | suggestion  ==> Summary: <sep> In this paper, the authors present a technique for refining the output of generative models by using gradients from discriminator to update the latent variables from which the generator produces a sample. The technique is more versatile than previous work since it can be applied to vector-valued discriminators, and does not restrict itself to scalar discriminators. The technique itself isn't particularly complicated, yet delivers impressive results. <sep> Pros: <sep> This work provides some sound theoretical foundations on which to build sample refinement, which isn't specific to a single GAN architecture, but is applicable to a wide range of generative techniques. Although I'll admit that I'm not familiar enough with stochastic differential equations to understand every aspect of the author's theoretical justification, the idea of leveraging gradients from the discriminator to update not the generator (which lives in a high dimensional parameter space) but the latent variable (which lives in a much lower dimensional space) seems plausible. <sep> In addition, one sees that the authors invested much thought and effort into the experiments section. I especially liked that they did language modeling, a task that shows a generative model works on domains other than just images. All told, after reading the experiments one is satisfied with the evidence of the effectiveness of the author's technique. <sep> Cons: <sep> The only major concern I have is that by only sampling images generated by updated latent variables, one loses diversity of generated images. A common experiment in some GAN papers was to choose two latent vectors z1 and z2, and show how the generator is able to smoothly interpolates between these two generated samples g(z1) and g(z2) as one generates samples with g(λz1+(1−λ)z2),λ∈[0,1]. It would be interesting to see if the latent refinement technique presented here also allows such smooth interpolation, or if as one starts with λz1+(1−λ)z2 and then refines, if the generated samples fall into a few ""modes"" of good samples while not generating the samples between the modes. <sep> In 5.2 and 5.3, I would appreciate it if the authors could specify, as the authors did in 5.1, how many times the authors did the train a base model and then refine pipeline in order to obtain the means and standard deviations reported. <sep> I would also appreciate it if the authors would report the JS-6 scores in the character level language modeling experiment. One gets the scores more or less for free, if runtime concerns are an issue, I can recommend this: https://github.com/cseward/ngram_language_model which does the same thing as the evaluation metric the authors used (https://github.com/igul222/improved_wgan_training) but is much faster by using C++ under the hood (there's a python API) and saving the n-grams in a tree structure. <sep> One last note: In our community, to get papers published one must generally demonstrate that a method works on some standard datasets, and authors are unfortunately yet understandably hesitant to discuss situations where a method fails to produce the desired results, as such a discussion could add arrows to the quiver of unduly critical reviewers. Such situations include datasets where the method doesn't work, sensitivity to hyperparameters and  Yet it is exactly these counter-examples which often help further understanding of the method and pave the way for further advancements. Therefore I would humbly request that, if this paper is accepted, the authors also discuss any failure cases they may have found, as these would make the paper an even more compelling read. <sep> Room for improvement: <sep> One of your claims is that ""By refining inferior samples, our technique avoids expensive sample rejection used by previous methods."" Please consider reporting the runtime (i.e. wallclock time) of your method vs the methods you're comparing against (such a report is fine in the appendix). <sep> It's a fact of life that most readers don't want to delve into the mathematical details in order to understand a method, but like a few pretty pictures that give them at least an idea of what's going on. I think the author's paper would gain more traction in the community if in the introduction they could come up with some ""eye candy"" showing how the method allows the gradient to flow all the way from a discriminator to the latent variable, resulting in a better good generated sample.","This work improves deep generative models by applying Langevin dynamics to sample in the latent space. The authors test their method under different configurations (different loss functions) and various generative models (VAE, flow, besides GAN). Experimental results demonstrate the benefits of the proposed method in different generative tasks. <sep> I tend to accept this solid work. I just have two suggestions: 1) the authors should discuss the connections and the differences between the proposed method and the energy-based methods like (Arbel et al., 2020) in-depth; 2) it may be more suitable to replace ""Wasserstein gradient flow"" with ""Discriminator gradient flow"" in the title."
"strength | decision | rating_summary | rebuttal_process  ==> Considering a continuous time RNN with Lipschitz-continuous nonlinearity, the authors formulate sufficient conditions on the parameter matrices for the network to be globally stable, in the sense of a globally attracting fixed point. They provide a specific parameterization for the hidden-to-hidden weight matrices to control global stability and error gradients, consisting of a weighted combination of a symmetric and a skew-symmetric matrix (and some diagonal offset). The authors discuss numerical integration by forward-Euler and RK2, and thoroughly benchmark their approach against a large set of other state-of-the-art RNNs on various tasks including versions of MNIST and TIMIT.  Finally, they highlight improved stability of their RNN against parameter and input perturbations. <sep> What I like about this paper is that it provides a solid theoretical basis and a principled, insightful parameterization that let's one control separately the size of the real and the imaginary parts of the eigenvalues of A and W. The paper contains a number of interesting thoughts, and a really extensive comparison to other state-of-the-art models on several benchmarks. <sep> In general, I feel however that this work is relatively close to the 2019 *CONF* paper by Chang et al.; in several ways it feels like a more or less straightforward extension of this previous work. <sep> It also remains a bit unclear to me how it's ensured in practice that the matrices obey to the required conditions in the training process. Sect. 5 is not really about training, but just about numerically solving the ODE. From Appendix C it seems the scalar parameters β,γ controlling the influence of the symmetric vs. skew-symmetric parts and the offset are not learned at all but just fixed after grid-search? The component matrices B, C, on the other hand it seems are not restricted at all but just initialized such that the theoretical conditions are likely, but not necessarily, met? This seems somewhat unsatisfying as there are in fact no guarantees that the global stability conditions will be met in practice, and tuning the model may require (potentially extensive) meta-parameter search? <sep> Another drawback in my mind is that enforcing global fixed point dynamics is quite restrictive. For instance, this rules out cycles and many other interesting dynamics in the model's intrinsic behavior. Apparently, from the authors' empirical tests, this seems not to be required for solving this particular set of tasks. Which is somewhat puzzling to me as it appears this assumption should strongly curtail the model's expressiveness. <sep> In the tables and figures I missed statistics. No standard errors or confidence bands were provided, or how many runs were performed. If it's all from a single run, can I be certain the numbers are not just lucky draws? <sep> Nevertheless, given the overall convincing and extensive empirical results, I'm slightly leaning toward acceptance. <sep> Minor issues: <sep> The authors sell the additional linear term in their RNN as a novelty, while in fact it's rather standard in continuous RNN (older papers by Barak Pearlmutter, Song & Wang 2016, arxiv.org/abs/2006.02427, arxiv.org/abs/1910.03471) <sep> RK2 is generally not sufficient for more involved (stiff) dynamical problems; so the reason it works well here may lie in the fact that the model's intrinsic dynamic is indeed very simple <sep> What is an unstable unit? I guess the authors mean that the RNN is not globally stable? <sep> Sect. 2,  dynamical systems inspired RNN: It may be important to note that formulating a RNN as ODE does not solve the exploding/vanishing gradient or stability problem per se (nor is it immediately clear to me why it should actually make it easier). <sep> Theorem 1: The σ refer to the matrix eigenvalues in this case? <sep> What is a superset of skew-symmetric matrices? <sep> Second-to-last pg. of Sect. 7 was unclear to me, i.e. what exactly was done and evaluated here, maybe cos I'm not familiar with some of the cited methods.","Solid work on extending AntisymmetricRNN and expanding its expressivity while controlling the global stability of the recurrent dynamics. It contributes to the growing interest in continuous-time RNN formulations that can deal with exploding gradient problem, and worthy of *CONF* poster presentation. Three reviewers were positive and one was slightly negative. Authors added additional experiments and strengthened the manuscript significantly during the review process."
"abstract | rebuttal_process | decision  ==> The paper proposes GONs which seek to build a generative model with an ""implicit"" encoder that comes essentially for free with the use of a few re-parameterization tricks. The main idea being that existing generative models with an encoder are ""redundant"" in that the decoder itself has the ability to compute the gradient with respect to a latent vector, z, which itself can be thought of as the ""encoding"". Since the choice of what initial latent vector to choose arises here, the paper advocates for simply choosing a z_0 which is a zero vector. In addition to the ""explicit"" formulation, there is also an implicit GON which is proposed that can generalize implicit generative models  (like SIREN) to entire distributions as opposed to a single data point, as they are currently used. <sep> Overall, I think this is very interesting work but incomplete. Considering GONs are a completely new category of generative models, it would greatly help to study each piece in more detail (theoretically or empirically) to establish what makes GONs successful, different, and how this improves our understanding of implicit representations in neural networks. <sep> Strengths: <sep> An interesting and novel formulation of encoding schemes from decoders that do not need any additional training or networks. <sep> The paper explores several different variants of GONs — from a variational alternative, implicit, and a classifier. Which greatly expands its scope of application in new problems. <sep> GONs generalize implicit generative models like SIRENs to work with an entire data distribution with very few parameters, which I think is a great benefit. This also naturally allows for variational alternatives, meaning we can sample from complex high dimensional distributions using very simple networks. <sep> The implicit GON also enables finer grid sampling in the input space, enabling its use in applications like super resolution naturally — but to any image from the training distribution. <sep> Weaknesses: <sep> The paper is very dense in terms of ideas, and as such falls short in thoroughly evaluating all of them. For example, the paper contributes several ideas like GONs, implicit GONs, variational GONs, which is great but it would help if each one of those pieces were studied in some more detail so they can be compared and contextualized better with existing approaches. For example, in the formulation itself the GON loss is presented ""as is"", but I think it warrants some more study. <sep> For example, why is just a single step ""sufficient"" to estimate ""z""? Does the quality of ""z"" improve if you take multiple smaller steps? How stable is this for different datasets? The empirical studies show promise, that indeed this can work reasonably well in reconstructing different datasets, but it would greatly help to justify some of these choices further. <sep> In the explicit case, how important is the choice of ""F"" ? The choice of activation function is explored but what about the architecture/ number of parameters for a given dataset? <sep> In all the experiments, the reconstruction losses are shown are for the training set, how do the validation set samples get reconstructed?  It's not clear if GONs are so effective in reconstructing because they are memorizing the data? <sep> How does the performance of GONs change as the size of the output space grows larger? For e.g. 128x128 or 256x256? <sep> Some of the terminology is also confusing. What does it mean when you ""overfit"" to an entire distribution? I understand its usage for a single image, but it's not clear what this means for an entire dataset. Are the samples from Figure 4 all from the same trained GON? <sep> Is Figure 7 from an explicit GON or an implicit GON? If its explicit, how are the number of parameters comparable to an implicitGON? Clearly an explicit model will have a lot more number of parameters. esp as the size of the images increase? <sep> I really like and appreciate the variationalGON experiments. How do they compare with  standard VAEs? Can they recover CelebA 64x64 images? How would they compare on quantitative metrics like FID etc.? <sep> In the super resolution experiment, can it super resolve any image from the distribution it was trained on? For e.g. in figure 5. is it just a matter of resampling the grid to 256x256 and running them through the pre-trained model for any sample from p(x)? <sep> ---------- Update on the revised manuscript ---------- <sep> I have read the new version of the paper and it reads a lot better. The new expanded methods section, and the definitions for different variations of GONs makes the paper much stronger and easier to understand. I appreciate and like the new experiments that show GONs capabilities on LSUN, comparisons with VAE on ELBO. <sep> Most of my concerns have been addressed in this version. I think this paper makes an interesting and novel contribution and I will raise my score accordingly.","This paper presents a new inference mechanism for latent variable models, by taking the derivative of log-likelihood with respect to a zero-valued vector. Initially, the reviewers raised concerns mostly regarding the limited experimentation and missing baselines. However, in the revised version, the authors addressed most of these concerns. <sep> Given that most reviewers are positive after the revision and since the proposed method is simple and interesting, I recommend accepting this paper."
"abstract | rating_summary | strength  ==> In non-autoregressive neural machine translation (NMT), learning from the predictions of autoregressive teacher models through sequence-level knowledge distillation (Kim and Rush, 2016) has been an important step to improve the performance of the non-autoregressive student models. Despite the success and prevalence of this knowledge distillation (KD) technique, this paper hypothesises---and empirically validates---that this KD procedure has a detrimental side-effect of propagating lexical choice errors that the autoregressive teacher model makes by mistranslating low-frequency words, into the non-autoregressive student model. <sep> To overcome this issue, this paper proposes a way to incorporate lexical choice prior knowledge from the raw parallel text (as opposed to the autoregressive teacher's output that may propagate lexical choice errors for low-frequency words). More concretely, this work specifies two prior distributions: (i) a word alignment distribution that specifies a list of plausible target words for each token in the source sentence, as obtained from automatic word alignment tools, and (ii) a target distribution from an identical non-autoregressive teacher model trained on the raw data (i.e. self-distillation or born-again network), which does not contain the same lexical choice error propagation that the autoregressive teacher's model output has. The student model is then trained to minimise KL divergence with respect to these prior distributions, in conjunction with the standard sequence-knowledge distillation loss that learns from the autoregressive teacher model's output, as determined by an interpolation coefficient with logarithmic decay (Eq. 6). <sep> Pros: <sep> This paper does a great job of motivating the problem of lexical choice error propagation on low-frequency words from the autoregressive teacher to the non-autoregressive student. The paper clearly states its hypothesis, provides a nice motivating example, and runs extensive preliminary experiments that convincingly confirm the existence of the lexical choice problem for low-frequency words. <sep> The proposed prior knowledge approach is simple to implement, and yet provides decent improvements across all four datasets. The improvements are also consistent across different autoregressive teacher model sizes and language pairs, and the two kinds of prior knowledge can be combined to produce stronger results. <sep> The paper features a fairly comprehensive analysis (including in preliminary experiments) and ablation studies that help better understand where exactly the improvements are coming from. <sep> Cons: <sep> Despite the positive aspects above, I still have two major concerns regarding the paper: <sep> In Eq. 6 (page 5), the interpolation rate λ controls how much weight is assigned to learning from the prior knowledge vs from the autoregressive teacher. But the proposed logarithmic decay function does not make sense to me. Let i be the number of steps. At the very beginning of training, i=0, so based on Eq. 6, λ=1. This makes sense since, at the beginning of training, the model only learns from the prior knowledge. But according to Eq. 6, λ will actually get larger as training progresses (up to i≤I/2). In the case where i=I/2−1, Eq. 6 will translate into λ=2. This does not make sense for three reasons. First, λ is an interpolation coefficient that therefore should be between 0 and 1. Second, λ=2 means that the interpolation weight assigned to distilling the autoregressive teacher is -1. Third, based on the description, λ is designed to get smaller as more training steps are done, instead of getting larger. <sep> My second concern is that there is a much simpler way of injecting the prior knowledge. For instance, what if we simply provide a decaying learning schedule (i.e. a curriculum) where, at the beginning of training, the non-autoregressive student is trained to learn more from the raw dataset, while at the later stages of training, the non-autoregressive student is trained to learn more from the autoregressive teacher's output? This can potentially accomplish the exact same goal of learning the prior knowledge from the raw dataset first, and then move on to learn more from the teacher model's output. This simpler baseline should at least be compared against. <sep> This is a minor concern, but there are some grammatical mistakes and presentational suggestions in the paper that can be modified to improve clarity, as detailed below. <sep> Question <sep> In page 4, it is mentioned that: ""The chosen procedure [to get the ""gold"" lexical choice for each word] is as follows: ..."". How accurate is this procedure? Did you examine the output and double check whether the ""gold"" lexical choice corresponds well to human judgment or a dictionary entry for each word? <sep> Presentation/Typos/Grammatical Errors: <sep> In page 3, section 2.2, paragraph ""Datasets."", ""... to avoid unknown works..."" -> ""words"". <sep> In page 4, just under Eq. 3, it is mentioned that ""V is the source side vocabulary in test set"". If I understand correctly, f is a token on the source sentence, so shouldn't Vsrctest be the list of word tokens (rather than source side vocabulary) on the source sentence? <sep> In page 5, ""Through fine-grained analyzing"" -> ""analysis"". <sep> In page 5, ""...becomes significantly accurate (AoLC).."" -> ""significantly more accurate"". <sep> In page 6, ""...signal of the priority information"" -> ""prior"". <sep> In Table 3, I would suggest displaying the AoLC performance just on rare words (rather than overall AoLC), since that is the problem that the paper is trying to solve. <sep> In Table 5, mention what the column ""Iter."" means. <sep> In page 7, ""... It is worthy noting ..."" -> ""worth"". <sep> In page 7,  ""... by substantially increase the lexical choice accuracy ..."" -> ""increasing"". <sep> In section 4.3, I suggest saying a bit more about how the human judgment is collected. <sep> In page 8, ""For AT models, ..."" -> remove ""For"". <sep> -----Update after the authors' response----- <sep> Thank you for the detailed authors' response, and for meticulously taking the feedback into account. The response has addressed most of my concerns. <sep> Some further comments: <sep> In Eq. 6, I think ""up to i≤I/2"" should be replaced with ""up to i≤(I/2−1)"", since λ would be negative with when i=I/2. Other than this, the equation looks good to me. <sep> I look forward to the addition of the results with the ""decay curriculum"" into the main paper. It is encouraging that the proposed approach outperforms this simpler ""decay curriculum"" baseline. <sep> Since the authors have addressed most of my concerns, I am therefore raising my recommendation to a ""6"".","This paper investigates knowledge distillation in the context of non-autoregressive machine translations. All reviewers are supportive of acceptance, especially after the thoughtful author responses. A well motivated and simple to implement approach that is giving good empirical results."
"strength | rebuttal_process | suggestion  ==> Sequential decision-making is investigated in this paper. A number of demonstrations (i.e. trajectories) is provided to the algorithm that has to lean a policy using only data. Any exploration using sampling from the environment is not possible. To allow for a better understanding of the domain, the algorithm also has to recover the reward that is optimized by the agent that provides demonstrations. In this paper, the authors propose two neural networks that are expected to resemble an autoencoder. One of the neural networks learns a policy that can recover the decisions made in the demonstrations, and the second neural network learns the reward of the policy that is learned by the first neural network. The empirical results show that the new method outperforms alternative methods with respect to the ""action matching"". The reward learned by the algorithm is not compared with any other approaches. <sep> The overall quality of writing in this paper is at a high, professional level. The authors know related literature, and citations are relevant and sufficient. I noticed very few types that are below. <sep> The approach proposed in the paper is very interesting, and this work could be significant because the medical application requires this kind of method. I am not in position to give a strong recommendation to accept this paper because I feel that the core algorithm is not explained sufficiently in the current version of the paper, and a few important experiments are missing. This position is justified by my major concerns below. <sep> Major concerns: <sep> The biggest problem that I have is that I cannot easily link Fig. 1 with Eq. (11) because the current explanations in the paper leave many details implied. In particular, Fig. 1, shows two neural networks, one for qϕ and one for Qθ, but the paper does not explain what are the inputs and what are the outputs in both networks, and which components of Eq. (11) are used to optimize every network. It seems that the learning objective in Qθ uses terms 1 and 3 in Eq. (11); this is possible when ϕ are frozen at the time when θ are updated. This must be a coordinate ascent algorithm then. So, the reward predicted by qϕ is not the input to Qθ; it is used only as a penalty term in Qθ. This is confusing because Fig. 1 indicates that R is the input. <sep> Next, let's consider qϕ. It takes (s,a) tuples as input, and it predicts (μ,σ) that define the normal distribution over R for every (s,a). I guess that terms 2 and 3 in Eq. (11) are used as objectives. The KL divergence wants the reward to be close to the prior, and the 3rd term, wants the predictions made by qϕ to be close to the real differences encoded by Qθ. So, again, when θs are fixed, the ϕ can be updated. The parameters of one of the networks can be updated, when the parameters of the other network are frozen. <sep> Thus, this summary of the two networks indicates that this procedure is not an autoencoder. There are two independent NNs here, that are trained using coordinate ascent. In particular, if Qθ is the decoder, then its error should be backpropagated to the second network, i.e., to qϕ, or at least the decoder should be decoding using the low-dimensional embeddings instead of receiving entire input. This is definitely not the case here. There are two networks that are trained independently, i.e. they don't share the error information during their own backpropagations. I think that these nuances of the algorithms should be made clear; it took me some time to identify these details. <sep> I would further claim that qϕ is not even required to learn Qθ with good predictive accuracy. It would be sufficient to use Qθ with the first term in Eq. (11) and then penalize the differences in Q(s,a) that are in the second term without the use of qϕ at all. Qθ would be to learn the same policy, I believe, using simpler regularization. The paper should show the result of such a simplification of the method presented in the paper. This simplification would still use reward to regularize Qθ, but without a use of a probability distribution over rewards, i.e., without the use of a second NN. <sep> The addition of qϕ is sensible overall if one wants to have a (normal) distribution over rewards for every (s,a). But still, the impact of having qϕ should be demonstrated empirically as I described in the paragraph above. <sep> There is something wrong about this sentence: ""where given the graphical model in figure 2 the reward can be seen as a latent representation of the policy."" In RL, the reward is myopic and instantaneous, and the Q-values have to learned to represent a policy. The Q-values take into account long-term consequences of actions, whereas R(s,a) does have this information in MDPs. <sep> The goal of this paper is to learn reward, but on p. 7 the authors said that in their experiments the performance is measured with respect to action matching"", and one of the baselines mentioned on the same page can recover the reward function as well. I feel that the rewards extracted by those methods should be compared with the rewards learned by the new method. This is a missing exploration. <sep> Minor problems: <sep> Introduction is of high quality, but this sentence: ""This redirection to obtain policies in a manor we shall refer to as apprenticeship learning (AL) introduces its own challenges, particularly in the offline setting."" does not explain apprenticeship learning. Please provide a link with IL and IRL that precede this sentence. <sep> One page 4, ""they still"" -> ""there still"" <sep> In bibliography ""bayes"" -> ""Bayes"" or ""gans"" -> ""GANs"" <sep> Summary: <sep> The authors learn the network Qθ, and they use another network qϕ to regularize it. It seems that the same empirical results could be achieved without the second network because the first network could be regularized directly. The experiments showing the results of the simpler method would be highly desirable. The link with the autoencoder is not clear because the two networks that are used in this paper are leaned separately and the original inputs are given to the decoder, i.e. the decoder does not decode using a low-dimensional representation. It is not clear why this approach is equated with an autoencoder. The decoder does not decode using the low-dimensional embeddings, and it uses the low dimensional embeddings for regularization only. The low dimensional embeddings are not even given as input to the decoder. <sep> This is interesting work with a good potential, but the above reasons don't allow me to suggest acceptance of the current version of the paper. <sep> Added after the discussion period: Thank you for answering my questions and the lively online interaction. You explained a few things to me, and I could see that you understood my point about VAEs. That was a great outcome. I hope that that you will address the  points that we discussed, where showing an honest link with VAEs will be highly desirable for your future readers.","The reviewers agree that the submitted paper is of high quality and provides a promising approach/framework for Bayesian IRL. Certain concerns regarding details of the implementation and evaluation have already been addressed by the authors during the rebuttal phase, and also the title of the paper was adjusted in line with discussions with the reviewers. For the final paper, the authors should make sure to clearly highlight the advances of inferring a distribution over rewards (this is already partly done by the added grid world experiments) and discuss relations to VAEs as the initially had in mind and even in the paper title. Beyond that, the should of course also address other reviewers' comments."
"rebuttal_process | decision  ==>  ==> Summary: <sep> The paper proposes Skip-Window or SkipW an abstraction encapsulating RNN cells to actively skip updates similar to some earlier works like Skip-RNN, Skim-RNN, and ThrRNN. The novelty of the method comes is in having control over the total updates to control the overall computational budget compared to previous methods which didn't provide deterministic upper bounds and varied depending on the inputs. The idea is very simple and straightforward and can be looked at as a logical extension to the Skip-RNN line of work combined with a windowed approach on time series as used in ShaRNN (Dennis et al., NeurIPS 2019). The entire time series is divided into Windows of length L (which is a tunable parameter) and each window has a precomputed (from the final hidden state of the previous window) per-time step (update inside the window) importance vector which can be used as an indicator to update or not to update following the binarization as done in previous methods. The strict sparsification of this per-window importance vector to have only K non-zeros per window helps reduce compute to an upper bound ratio of K/L. The method further uses another threshold term over the sparsified importance vector to control finer budget requirements if needed. The experimentation is done on 2 tasks HAR-2D-Pose (with 32 time steps) and Adding task with 50 timesteps. The evaluation shows that Skip-Window shows good performance./accuracy compared to previous flexible RNNs with a reduction in the total number of updates. Finally, the impressive part of the paper is the real-world evaluation on Jetson Nano with a more complex workflow involving pose estimation from images for HAR-2D-Pose. <sep> Pros: <sep> Simple and elegant idea. SkipWindow solution is in theory generalizable to multiple RNNs without much hassle. The abstraction could be thought of as an independent outer layer over RNNs similar to most other works in the space. <sep> Related work section is very thorough and the claims are grounded. <sep> The architecture is easy to understand along with the aspects of training <sep> The experimentation on both datasets reveals interesting insights and showcase the advantage of SkipW over other methods in both accuracy and computational budget. <sep> I highly commend the deployment experiments and evaluating it with complex workflow and showing how skipping updates and inputs can help compute latency and resources. <sep> Plots are very clean and appendix and ablation are good. <sep> Weaknesses: <sep> While I feel the idea works well and is elegant. It still is a combination of a couple of known techniques (mentioned in summary) which limits the novelty somewhat. But that doesn't stop the method from being useful. <sep> Figure 2 is not required or needs redesigning as the equations were much clearer than the figure and the gates just made it hard to understand. <sep> I understand while K/L upper bound is guaranteed, the authors want to use thr for finer control, I just feel, it might not be required for super long sequences. - Just a comment. <sep> My major issues are with experiments. While I like the work on HAR-2D-Pose and agree it is a good choice of dataset. I also want to see the success of Skip W on long term dependency tasks. I wouldn't count adding tasks as it can be trivially solved using good initialization (Henaff et al., ICML 2016). While I appreciate the efforts, I would like to see results on at least one or two (at least two preferred but I understand the time limitations, so one good dataset also works for the discussion phase) more real-world datasets to check the generalization. It can be Keyword spotting, phoneme detection, or even noisy-CIFAR. Something around 100 timesteps or more would be great <sep> I am asking the authors to add these things during rebuttal or give reasoning corresponding to it. This is a key question to be addressed during the discussion. Note that I do not need deployment for these datasets but would like to see these numbers in the paper for more datasets to make the paper stronger. The reasoning for the long-range is because the gains would be more profound there than in 32 time steps. <sep> While the K/L based on importance makes sense per window. Sometimes, RNNs tend to identify signatures randomly. I suggest the authors add a comparison with random K choice instead of top K choice per window and have multiple variants of it like periodic sampling or all inputs being from start or end (these are deterministic for prediction time). This experiment will help us determine if the importance vector actually contributes to the decision making on skipping updates. The rationale behind this comes from the action recognition literature where people have shown that random sub-sampling works decently well compared to intelligent sub-sampling. Another thing to add here is because the importance vector per window is chosen based on the previous window it might not be optimal and maybe random selection might work fine. If random sampling (even not so random like periodic etc.,) works well, then I am not sure about using importance vectors anymore. <sep> Without these baselines, it is hard to argue otherwise. I strongly suggest authors pursue this to make the claims more solid. This is another thing that needs to be addressed during the discussion. <sep> Decision: <sep> Even though the novelty is slightly limited, I like the idea for its ease, decoupled natured and potential generalizability along with the control on computational budgets. I appreciate the authors for ablation and on-device experiments which are very thorough. The only issue I found with the paper is the experimentation and baselines. I want to see at least one long term dependency task (real-world) and baselines that evaluate if the importance vector is even needed with the simple sampling strategies among each window. <sep> I am very much willing to increase the score based on discussion and the improvements on the experimentation front. <sep> Edit after rebuttal and discussion. <sep> I thank the authors for extra experimentation to showcase the effectiveness of SkipW. While most reviewers here agree that the novelty is limited (that doesn't stop it from being useful), I strongly think the impact due to SkipW will be translated to the real-world. There has been some discussion on the datasets, which I agree are not extensive making the initial experimentation weak. However, the new experiments compensate to an extent and I would like to recommend a weak acceptance with a score of 6 (I am still between 6 and 7, waiting for other reviewers to pitch in).","The authors did a good job responding to reviewer concerns. While the reviewers still consider the method described in the paper to not be especially novel, at least one is impressed by the practicality. imo the authors' attention detailed ablations and analysis post-review makes the paper worth including in the conference."
"abstract  ==> Summary: The paper extends Poisson-Gamma models for non-stationary sequences, in a manner that allows partitioning the counts according to a binomial model to account for multiple resolutions. This generalisation is motivated well with a biological application of practical relevance, and the proposed method is particularly strong in enabling linear computational scaling required for analysis of large genome data. <sep> Reasons for score: I am leaning towards rejection in the current form.  The contribution itself is worth publishing and the method is likely to be valuable for the application, but the presentation would need to be improved (especially regarding the GP+CNN part; see the detailed feedback) to better communicate the technical contributions for the *CONF* audience. Conditional on improved presentation, I would be leaning towards acceptance. <sep> Detailed feedback: The proposed split Poisson-Gamma (SPG) process seems reasonable, but in technical terms is a fairly straightforward hierarchical structure and can be constructed using standard properties of Poisson-Gamma. It is well motivated by the resulting efficient inference and this specific application, and may find uses in other applications as well, but does not provide a very clear theoretical contribution that would open immediate follow-up research directions for more general modelling questions. <sep> My main problem with the paper concerns the structure of the presentation. While the authors motivate the model well and provide very clear illustrations for the application, the method sections are disconnected and I had trouble following the technical contributions. In particular, the connection between Section 2 (SPG) and the technical algorithm required for using it (Section 3.3) is unclear. To me it seems the GP+CNN part is an integral part of the overall solution and a contribution in itself (and, in fact, an important one -- the SPG alone is not quite sufficient as theoretical contribution). It provides a concrete algorithm using SPG and is general, but now the description is provided only after talking about specific data and looks more like a minor technical detail with no proper theoretical justification. For me, the paper would be more natural if Section 3.3 (and possibly some other parts of Section 3) would be described after Section 2 as description of how SPG is used in practice, and if they would use shared notation and terminology. This would make the practical approach easier to follow and the contributions more clear. <sep> The empirical experiments and illustrations for the application are well carried out, and serve as good demonstration of the method. However, a reader uninterested or uneducated in this specific application will have some trouble figuring out how well the method works; this could be improved by complementing the results with clear artificial data of slightly simpler nature. <sep> Modifications after discussion: <sep> Increased score by one as the presentation in the revised version has clearly improved, along the lines requested in the original review.",Reviewers agree that the paper excels in providing a principle pipeline that combines CNNs and GPs with a Poisson-Gamma distribution to provide a generic approach for multiresolution modelling of tumour mutation rates. As a whole such combination of techniques addresses a key challenge in computational biology that also scales to large datasets.
"abstract | strength | weakness | ac_disagreement  ==> Summary <sep> This paper proposes a theoretical framework for understanding consistency-based semi-supervised learning. While establishing this framework based on the Hidden Manifold Model, this paper frames the SSL in the context of Manifold Tangent Classifiers. <sep> Pros <sep> Formal understanding of SSL is indeed currently limited and theoretical works are needed. <sep> The formalization of minimizers as harmonic function leads to the non-obvious prediction that SSL methods are insensitive, or at least very robust, to the weighting of the consistency loss λ. To me, this is the most important result of the paper. <sep> The bibliography is well documented. <sep> Cons <sep> The experimental verification of insensitivity to the weighting of consistency loss is unconvincing: it is done on a trivial low-dimensional toy dataset. It would be more convincing to take the GitHub code of one or more, possibly recent, SSL methods and vary λ on real datasets. <sep> I fail to see any other takeaway from the theoretical framework than the predicted insensitivity to λ. <sep> Technically, if I understand correctly, the insensitivity to λ is not a takeaway of the Hidden Manifold Model but of the Minimizers are Harmonic Functions. <sep> I didn't see where the claim ""... demonstrate that the quality of the perturbations is key to obtaining reasonable SSL performances"" has indeed been demonstrated. <sep> Questions and nits <sep> ""Several extensions such as the Mean Teacher (TV17) and the Virtual Adversarial Training (VAT) (MMIK18) schemes have been recently proposed and have been shown to lead to state-of-the-art results in many SSL tasks."" They are far from the current state of the art, especially so for few labels, this would have been true before the advent of MixMatch but the state of the art changed drastically with its introduction. <sep> ""... consider as well the tangent plane Tx to M at x."" Here I have a hard time visualizing it. Say the manifold M is a 2D Gaussian point cloud for example, what would be the tangent plane for a point x in that cloud? <sep> Following on question 2, how is the tangent plane property exploited other than by defining an orthonormal basis on it? And why can't an orthonormal basis of the Manifold space itself be enough if instead we assume it is dense? <sep> ""Enriching the set of data-augmentation degrees of freedom with transformations such as elastic deformation or non-linear pixel intensity shifts is crucial to obtaining a high-dimensional local exploration manifold."" This seems in direct contradiction with MixMatch results which does not use any sophisticated augmentation: just pixel shift, mirroring and random pixel-wise linear interpolation between samples and labels (MixUp). <sep> Proposition 4.1: Here I was hoping for a prediction for your method. You mention the ""sequence of processes converges weakly"" and I was hoping it would explain why SSL techniques are much slower than fully supervised to converge. But then, it seems this statement is not exploited in any way other than saying that it does indeed converge to the solution of the ODE. <sep> ""This indicates that the improved performances of the Mean Teacher approach sometimes reported in the literature are either not statistically meaningful, or due to poorly executed comparisons, or due to mechanisms not captured by the η → 0 asymptotic."" This is vague, considering Mean Teacher outperforms VAT, would this mean that the last option holds (due to mechanisms not captured by the η → 0 asymptotic)? Just to clarify what the last option actually means: does it mean the proposed analysis relies on assumptions that don't capture the reality of the phenomenon? <sep> ""These results are achieved by leveraging more sophisticated data augmentation schemes such as ... Mixup"". It seems odd to see MixUp being referred to as sophisticated (as in domain specific). MixUp is domain agnostic, it simply linearly combines a pair of samples and their labels. So in fact, it seems even simpler than a pixel shift since it's linear. <sep> ""... with a neural network with a single hidden layer with N = 100 neurons"". What non-linearity was used? I didn't seem to find it, or I may have missed it. <sep> ""Figure 3 (Left) shows that, contrary to several other types of regularizations such as weight-decay, this method is relatively insensitive to the parameter λ"". I didn't see it. It indeed shows that (a) the method is relatively insensitive to the parameter λ in the context of the toy task but it doesn't show that (b) the method is sensitive to other types of regularization such as weight-decay. <sep> Note: Ultimately I must admit a lot of the maths are above my head and I have no idea whether they are correct or not, therefore I didn't comment on them and only focused on the parts that I understand. On the other hand, I'm pretty confident in my understanding of SSL techniques and MixUp. <sep> =====POST-REBUTTAL COMMENTS======== <sep> I thank the authors for the response and the efforts in the updated draft. Most of my queries were clarified and I raised my rating accordingly. However, unfortunately, I still think a more realistic validation (e.g. on non-toy dataset) would benefit the paper.","This paper provides some theoretical perspective on the use of data augmentation in consistency regularization-based semi-supervised learning. The framework used in the paper argues that high-quality data augmentation should move along the data manifold. This generic view allows the paper's ideas to be applied across datasets (as opposed to image-specific data augmentation used in state-of-the-art semi-supervised learning algorithms). I am not aware of any other work raising these points, and indeed this paper is significant in that it provides a new and potentially useful perspective on the most performative semi-supervised learning approach. Reviewers agreed that the paper was clear and useful. The main concern was that the paper only included experiments in toy settings. Indeed, it would have been much more impactful to apply these ideas to state-of-the-art semi-supervised learning methods, but I think it can be excused given the theoretical focus of the work."
"abstract | strength | weakness | rating_summary | rebuttal_process  ==> Summary <sep> This paper proposes a new variant of local SGD algorithm to make it be more realistic. In particular, (1) it allows workers to perform different number of local steps, depending on their computational resources; (2) workers are organized in a multi-level structure. Workers connected to one central hub can synchronize frequently and hubs are communicated in an infrequent and decentralized manor. <sep> The authors provide convergence analysis under non-convex, iid data partition settings, and conduct preliminary experiments to validate their theoretical findings. <sep> Pros <sep> This paper is easy to follow and very well-written. <sep> It makes a non-trivial extension of (Wang & Joshi, 2018). By introducing probability of taking local updates, the framework allows different workers to take different local steps within a given time interval. This is a realistic setting typically ignored by related literature. <sep> The analysis of the proposed algorithm is not trivial. It is nice to see how the authors model the complex algorithm in a simple way, although the formulation is roughly the same as (Wang & Joshi 2018). <sep> Cons <sep> It seems that the results of MTL-SGD cannot recover local SGD? In particular, the additional error terms (the last two terms in (13)) increase with q2τ2. However, in local SGD, the additional error terms increases linearly with τ, as shown in (Wang & Joshi, 2018). I didn't find any discussions on this discrepancy. <sep> The experimental results are too limited. Especially, it is hard to see the advantages of MTL-SGD over other two baselines. I encourage the authors to redefine the x-axis in experiments to time slots. Within one time slot, each worker has a probability pi to perform one local step. In local SGD, in order to finish one round of τ local updates, the time slots required for one worker is τ/pi, and hence, the time slots used for one round is maxiτ/pi. However, in MTL-SGD, the time slots used per round can be exactly τ by allowing workers to have different number of local steps. By doing this, MTL-SGD might have much faster convergence than local SGD, in terms of loss versus time slots. <sep> Post-rebuttal <sep> I've read the authors' response and other reviewers' comments. The response and the updated version clarify my concerns. So I slightly increase my score.","The paper studies a hierarchical or multi-level version of local SGD, extending earlier work by (Wang & Joshi, 2018), (Lin et al, 2018) and (Jiang et al. 2019) among others. It gives novel convergence rates in relevant settings, such as by allowing different workers to take different numbers of local steps within a given time interval. The current analysis is restricted to the IID data case, but still insightful, and might serve as a useful building block for follow-up research in the future. <sep> Smaller concerns remained that the presented multi-level results cannot exactly recover local SGD as a special case. Nevertheless the consensus remained that the overall contributions and relevance of the paper remain above the bar. In the discussion phase, several concerns were clarified and additional deep learning experiments have been added to the paper, which is appreciated."
"abstract | strength | rebuttal_process | decision  ==> Summary <sep> This paper introduces a novel application of normalizing flows to speech synthesis, allowing direct optimization of spectrogram log-likelihoods which results in more natural variation at inference compared to L1/L2 losses that model the mean. This setup also allows more control over non-textual information and interpolation between samples and styles. <sep> Recommendation <sep> Accept <sep> The idea is good and experiments are good. There are some concerns about the clarity of the paper but those can be worked on. <sep> Positives <sep> The paper introduces a novel architecture and demonstrates improved output variation and more controllability, which is an important current issue for TTS research. <sep> Many experiments investigating controllability are described, as well as some ablation studies for the model architecture in the appendix. <sep> Negatives <sep> Figure 1: Please provide more informative captions. In the text, timbre and F0 are mentioned but it is not clear how that is relevant in the images. It is not clear what the colors mean in 1b, and if 1b is supposed to show a separation between male and female speakers the colors make it worse. Finally, are the points in 1b cluster centers or a single random sample per speaker? <sep> Figure 2: Would it be possible to make it clearer that there is an autoregressive dependency in the Attention and Decoder blocks due to the LSTM cell memory? The way the figure is currently drawn makes it seem as if each attention/decoder block can be computed in parallel from only the inputs from the previous flow iteration. <sep> In section 2.2 NN and f are described as acting on the latent variable frames zt, but Figure 2 applies the NN and flow on the mel spectrogram frames x in order to produce z. Similarly, there is text saying ""we take the mel-spectrograms and pass them through the inverse steps of flow"" and ""Inference, [...], is simply a matter of sampling z values [...] and running them through the network"" but Figure 2 marks the block as ""Step of Flow"" rather than ""Inverse Step of Flow"". It would be good to smooth out the consistency. <sep> More discussion about the end of sequence prediction would be appreciated. As the entire sequence of z must be used for inference, I assume there is some constant max inference length z used to obtain a final x, and the end of sequence prediction only happens to the final x rather than at each step of the flow? How well does this model adapt to inference samples that are much longer than any of the inputs seen during training? <sep> 3.3.2 Interpolation Between Samples: I don't understand why there was a need to sample z to find z_h and z_s. Is it not possible to take z_h and z_s from a random training example for the speaker? Or does that mean there is a correlation between the latent space z and the content that is being spoken? If the latter, I would like to see more discussion on that. <sep> Table 2: I assume bold means closest to ground truth? It's really hard to know how to interpret this table and I don't think it supports saying that FTA Posterior is more effective. FTA did not capture the increase in pitch mean from expressive -> high pitch, nor the decrease in std from expressive -> surprised. A better visualization may be to express this table as a bar graph (3 separate groups of 3 bars each) and conclude FTA Posterior is able to produce much more variation than Tacotron 2 GST? <sep> Misc <sep> Abstract <sep> ""varation"" -> ""variation"" <sep> spell out IAF <sep> ""We provide results on speech variation"" etc. sounds weak. eg. ""Flowtron produces output with far more natural variation compared with Tacotron 2 and enables interpolation over time between samples and style transfer between seen and unseen speakers in ways that are either impossible or inefficient to achieve with prior works."" <sep> 1 Introduction <sep> ""Their assumption is that variable-length embeddings are not robust to text and speaker perturbations"" citation needed <sep> ""Flowtron learns an invertible function that maps a distribution over mel-spectrograms to a latent z-space parameterized by a spherical Gaussian."" mention IAF and cite Kingma et al., 2016 here instead of the previous paragraph. <sep> ""Finally, although VAEs and GANs provide a latent embedding that can be manipulated, they may be difficult to train, are limited to approximate latent variable prediction"" While the IAF approach allows for direct optimization of log-likelihood, the latent variable encoding part is still approximate, just like in a VAE. The original Kingma paper even states that it is an approximate posterior. <sep> 2.2 Invertible Transformations f−1 wouldn't be applied to zt. Maybe f−1(xt) or f−1(f(zt)). <sep> 3.1. Training Setup <sep> ""progressively adding steps of flow on the last step of flow has learned to attend to text"" on->once? <sep> 3.4.2 Seen Speaker with Unseen Style <sep> ""Flowtron succeeds in transferring not only the somber timbre, the low F0 and the long pauses associated with the narrative style"" -> ""not only the somber timbre, but also"" <sep> Appendix <sep> IAF is known to be quite inefficient. Is there any noticeable impacts on training or inference time? Or is this not a problem due to only using 2 layers of flow? It would be great if the ablation study in the appendix regarding layers of flow also covers this, but this is quite optional.","This paper proposes an autoregressive flow-based network, Flowtron, for TTS with style transfer. It integrates the Tacotron architecture with the flow-based generative model. Extensive experiments are carried out in a controlled manner and the results show that the proposed Flowtron framework can achieve comparable MOS scores to the SOTA TTS models and is good at generating speech with different styles. All reviewers consider the work interesting. There are concerns raised on technical details which mostly have been cleared by the authors' rebuttal. The exposition also has been greatly improved based on the reviewers' suggestions and questions. Overall, this is an interesting paper and I would recommend acceptance."
"abstract | strength | decision  ==> The paper disproves the hypothesis that addressing shape bias improves robustness to corruptions of neural networks, which has been stated by the previous studies [1, 2]. The paper demonstrates that the degree of shape bias of a model is not correlated with classification accuracy on corrupted images via experiments. For the experiments, this paper presents two novel methods to encourage CNNs to be shape-biased: 1) edge dataset and 2) style randomization (SR). In the experiments, the authors train CNNs to be shape-biased to various degrees based on the proposed methods. Additionally, they compare test accuracies of the models to evaluate shape bias and robustness to corruption. In addition, this paper shows that through fine-tuning the affine parameters of the normalization layers, a CNN trained on original images can achieve comparable, if not better, performance than a CNN trained with data augmentation. <sep> Pros: <sep> The paper clearly demonstrates that shape bias is not correlated with robustness to corrupted images. The test accuracy on texture-shape cue conflict images effectively indicates the degree of a model's shape bias. Also, the authors visualize the results that explicitly compare the models' accuracy on corrupted images. <sep> The authors present an interesting analysis towards Stylized ImageNet (SIN) dataset by separating the dataset into different factors that are used to generate SIN. According to this paper, the newly made datasets based on each factor leads a CNN to be shape-biased to various degrees. This provides insightful perspectives on shape bias and corruption robustness. Specifically, it is intriguing that a model trained on a stylized dataset using styles from in-distribution images achieves comparable performance compared to what uses styles from out-of-distribution images, such as paintings. <sep> Cons: <sep> The novelty of the two proposed methods, edge dataset and SR, is limited. First, the edge dataset is created based on an existing model. The authors simply convert the non-binary edge map into a binary one. Furthermore, there are not enough experiments or evidence that validate the effectiveness of this method compared to the original method. Additionally, SR is also similar to existing methods, except that they change the target distribution from training samples' distribution to a uniform distribution. <sep> According to this paper, a CNN trained on edge dataset is more shape-biased but achieves lower accuracy on corrupted images compared to a CNN trained on SIN. The authors state that the result indicates that shape bias is not correlated to corruption robustness. <sep> However, it seems to be unfair to compare edge dataset and SIN to disprove the correlation between shape bias and corruption robustness. <sep> Edge dataset, unlike SIN, does not contain any information except edges. Therefore, a CNN trained on edge dataset 'learns only shape information, while a SIN-trained CNN learns other information as well, but 'focuses more on the shape'. Since the edge dataset provides much less information of the original images, it is trivial that a CNN trained on edge dataset achieves lower performance compared to a SIN-trained CNN. <sep> The previous approaches [1, 2] mentioned in this paper also imply that shape bias is basically related to how much the model focuses on shape information, not to how much shape information the model is given. <sep> Therefore, it would be more reasonable to compare a SIN-trained CNN with other CNNs that are trained on datasets containing a similar amount of information, but have different levels of concentration on shape information. <sep> [1] Geirhos et al., ""ImageNet-trained CNNs are biased towards texture; increasing shape bias improves accuracy and robustness."" *CONF*'19 <sep> [2] Michaelis et al., ""Benchmarking Robustness in Object Detection: Autonomous Driving when Winter is Coming."" *CONF*'20 <sep> After rebuttal: <sep> Thank you for the additional explanation. <sep> The comments by the authors effectively address my concerns. Although the edge dataset and SR are quite similar to the existing methods, the authors clearly present the usefulness of them as well as their additional contributions. Also, additional training details support the adequacy of the comparison in the experiments. Therefore, I would like to increase my final score to 7: Good paper, accept.","This work investigates the recently proposed hypothesis that enhanced shape bias improves neural network robustness to common corruptions. Several interesting experiments are performed to better understand the contributing factors that lead to improved robustness of models trained with texture randomization. Of particular note, the authors design a data augmentation strategy that verifiably increases the shape bias of model, but for which corruption robustness is not improved. Reviewers agreed that this is an interesting counter-example to the shape-bias hypothesis and improves our understanding of why stylization improves robustness. Given the carefully designed experiments investigating an important topic I recommend accept."
"abstract | strength | weakness | rebuttal_process | decision  ==> The paper presents a new mapping of Euclidean vectors to bit vectors (quantization), along with a de-quantization method. The method is closely related to recent work by Huynh & Saab (2020), but instead of using a random rotation DBx on the input vector x, a sparse, Gaussian random projection is used. Under the assumption that the mass of x is ""well spread out"" this mapping also preserves Euclidean distances, but is quicker to compute (assuming random access to entries of x). After this mapping, a so-called Σ∆ quantization method is used, following Huynh & Saab. <sep> The main issue I have with the paper is that there is no adequate comparison with quantization methods other than Σ∆ quantization. Notably, there is no comparison with product quantization (IEEE transactions on pattern analysis and machine intelligence, 2011). (Though it is presented as a data dependent method, it also makes sense as a data-independent method.) Also, LSH-based methods like simhash, and binarized E2LSH are not considered. It would also be helpful for the reader with a comparison to data dependent schemes, e.g. ""Practical Data-Dependent Metric Compression with Provable Guarantees"", NeurIPS 2017. Finally, there are related works in the theory literature that should probably be cited, e.g. ""Optimal Compression of Approximate Inner Products and Dimension Reduction"", FOCS 2017 (relevant for the case of normalized vectors). <sep> Another claimed contribution, a speedup from O(n log n) to O(m) time, is not surprising (or new): It is known that for well spread vectors, even random sampling is an optimal dimension reduction method. (I think this goes back at least to Ailon and Chazelle.) <sep> The writing can be made clearer. Even though I worked on related things, I found several parts of the paper hard to make sene of. For example: <sep> The abstract suggests that the results apply to all vectors that are not sparse, but in fact they apply only to vectors that satisfy a hard-to-satisfy L-infinity norm restriction. <sep> The abstract does not make clear that random access to the input vector is assumed, making it hard to understand how time less that O(n+m) is possible. <sep> Algorithms 1 and 2 in the introduction are impossible to understand without reading section 3 first. Maybe give a special case, with explicit details, and the general case later or in appendix? <sep> I suspect some assumption is missing in Theorem 1.1: The set of possible distances d_V(q_x,q_y) is finite, yet is supposed to be able to express the distance between arbitrary vectors x and y in R^n ...! <sep> The start of section 5 seems to imply that one can assume that vectors are well spread without loss of generality, by applying a random rotation. However, for random rotations the l-infinity norm will exceed the required norm by a factor sqrt(log n) with high probability. <sep> Is it important that L1 distance is used in algorithm 2, rather than L2 distance? After all, these distances are similar up to scaling for ""spread out"" vectors. <sep> Out of curiosity: Did you consider Kashin's representations of vectors?","The paper provides a new distance preserving embedding based on a recent result called sigma-delta quantization. The authors notice that in many realistic scenarios, the input vectors are well-spread and under assumptions regarding the spreadness provide a fast technique to convert the input vectors into binary vectors, possibly of lower dimension. For completeness, the authors analyze the setting where the vectors are not spread and show that by using a randomized Walsh-Hadamard transform, their results still apply. <sep> The authors do not provide a completely novel approach, to quote R2 ""On a technical level the results in this paper are hardly too surprising for the JL community, but it is nice to see this analysis worked out in detail"". That being said, they show that a natural idea indeed works out by providing both a theoretical analysis and experimental results. The experiments can be more thorough but do convey the point that the result indeed works and moreover is somewhat robust in that it works well even when the formal requirements do not entirely hold. <sep> There are a few issues mentioned by the reviewers that should be addressed: A clearer exposition of the guarantees and assumptions, some comparison with previous papers. However given the responses and discussions these seem minor and fixable towards a camera ready version. I recommend accepting the paper"
"abstract | strength | rating_summary | strength | decision  ==> General <sep> This paper proposes an adaptive temporal fusion network called AdaFuse for action recognition, which adaptively removes temporal redundancy and reuses past features for accuracy and efficiency. <sep> I listed the Pros and Cons I found in the paper below as well as some questions to clarify some of the details. <sep> Pros <sep> The idea of learning a decision policy to dynamically determine whether channel-wise features at time t are calculated normally, reused from t−1, or skipped, is interesting and reasonable. <sep> The experimental results show that the proposed method achieves good accuracy with reasonable computational budget. <sep> The ablation study in Table 4 reveals that the performance is greatly affected by the policy and it is important to fuse the futures from different frames to captures the temporal dependencies. <sep> Cons <sep> The propsoed method is not compared with some of the recent methods such as [1-3] ([4] is optional because the publication date is very close to the *CONF* 2021 submission deadline). Especially for Jester and Mini-Kinetics dataset, the proposed method is compared with only TSN, which is old and weak as baseline as it does not incorporate the temporal information. <sep> In Table 3, it seems that the proposed method achieves good accuracy, but I am afraid that it is just because of the strong base network, TSM. Merely adding AdaFuse to TSM indeed saves some computation but degrades the performance as described in the paper. The proposed remedy indeed slightly improves the accuracy but it requires much more parameters compared to the vanilla TSM. Overall, I find it benefitial to use the proposed method on top of simple base networks such as TSN, but the benefit of using the proposed method on top of strong base networks such as TSM may be marginal. Combined with the point 1 above, I am not well convinced of the effectiveness of the proposed method. <sep> Some of the important details are not clear. I would appreciate if the authors could answer the questions I listed below. <sep> Questions <sep> Is it necessary to use Gumbel softmax? I think there are two kinds of tricks involved in Gumbel softmax. One is a trick for sampling from a categorical distribution, and the other is a trick for making the opperation differentiable. In my understanding, which may be wrong, the required characteristic for the present method is the latter one, and the sampling from the categorical distribution is not necessarily required. In this case, I think simply using q instead of log⁡r+G in equation (7) is enough. <sep> Related to the point above, please clarify the type of output (hard or soft) of the policy net. The sentence after equation (2) says the output is integer values (0, 1, or 2), while the sentence before equation (7) says it is a real-valued vector. <sep> Suppose pti=1 (reuse) and pt−1i=1 (reuse again). In this case, is yti copied from yt−2i ? Or is the feature map of i-th channel at time t−1 calculated on the fly for ""reusing"" at time t? In other words, if the policies for a channel is ""reuse"" n consecutive times, does the method take the feature from n frames before? <sep> Other comments <sep> Figure 1 may be incorrect or misleading. I think pt, the output of the policy net, should go to the 2D Conv. block. Otherwise the block never knows which channel to compute at time t and which channel to reuse or skip. <sep> [1] Sudhakaran+, Gate-Shift Networks for Video Action Recognition, CVPR 2020 <sep> [2] Martinez+, Action recognition with spatial-temporal discriminative filter banks, ICCV 2019 <sep> [3] Jiang+, STM: SpatioTemporal and Motion Encoding for Action RecognitionSTM: SpatioTemporal and Motion Encoding for Action Recognition, ICCV 2019 <sep> [4] Kwon+, MotionSqueeze: Neural Motion Feature Learning for Video Understanding, ECCV 2020","This paper presents a model for video action recognition. The reviewers appreciated the development of a novel dynamic fusion method that examines channels from feature maps for use in temporal modeling. After reading the authors' responses, the reviewers converged on an accept rating. The solid empirical results and analysis, the fact that is is a plug-in method that could be used in other models, and the clear exposition were deemed to be positives. As such, this paper is accepted to *CONF* 2021."
"misc | decision | suggestion  ==>  ==> This paper presents Hopper, a method that performs multi-hop reasoning to address the problem of object permanence in videos. <sep> Strengths: <sep> Very good results in the Snitch Localization task. Good and sensible baselines. <sep> Good ablations, experiments and visualizations. Also good justification of results both for Hopper and for baselines. <sep> They introduce a method that is modular, and clearly separates several stages of processing (while still making everything end-to-end trainable). This provides good understanding of the system, and makes ablations and comparisons with baselines easier (for example, baselines can work a the per-module level). <sep> The multi-hop transformer reasoning module they introduce is intuitive, and it is very well executed. <sep> Overall well written and good positioning among state of the art literature. <sep> Contribution of a new CATER-h dataset that corrects some biases of CATER for the specific task the paper is solving. Good explanation of the need for this dataset, both by analyzing the dataset statistics, and by looking at the results of the baselines on the two datasets. <sep> Weaknesses: <sep> A lot of supervision and limited dataset. <sep> Object annotation class and bounding boxes are available. <sep> Fixed set of objects and attributes, with no variation across samples (synthetic data). <sep> The tracker learns based on the class. If there are two objects belonging to the same class, it will probably not distinguish them (see Figure 15a). Probably humans could watch a video and follow the snitch even if all objects looked exactly the same, just by following their movements, and this method is, by construction, not doing this. Therefore, Figure 15a is not just some very hard case, but a case that the system as it stands cannot solve. This implies that the tracking done by the system is a very weak form of tracking, and it is more a relabeling of objects. Actually, to solve the task the system only needs to know 1) where the snitch was before and 2) where did the container move (just reidentify by classification). While this can be improved, the main contribution of the paper is in the step after this, so it is OK to leave it for future work. <sep> In some cases the method feels very specific for the presented task and dataset. This is acceptable to some extent because it is a hard problem and it doesn't have to scale right away, but I am concerned it is too specific about this formulation and cannot be used. There are a lot of specific intuitions for this specific problem and setting (not interesting for any other case). See for example page 6. More specifically, an example is the heuristic for computing the occluder, which would fail if the snitch has moved between frames, and it is visible but somewhere else (and this case is not even generalizing to a slightly different task, but a potential situation in the current dataset). While these heuristics are optional and the authors present ablations, the feeling is general for the whole method. <sep> Some parts of the method could use more clarity: <sep> Figure 3 is confusing, as it does not exactly follow Algorithm 1. It would be convenient to use the same names as in the rest of the paper for the layers and the inputs/outputs. <sep> Why the attentional feature-based gating (line 11 algorithm 1) is necessary? Why doesn't Transformer_f directly output the final U_update? <sep> Why two transformers are necessary, actually? The masking could be applied at the beginning of the first one and just use one. Note this is not the same as the ""hopper-transformer"" in the baselines. <sep> t is computed with a softargmax. However, it is used in places that require it being a hard number, for example in line 2 in the algorithm. This implies these places have to detach the gradient from t. Apart from line 2, is there any other place that requires this? What about lines 12 (Masking) and 5 (Extract). My understanding of the paper tells me that at least the Masking() module requires a hard t. In what case is the gradient actually propagated through t? <sep> Teacher forcing: the paper argues that hop 2 should focus on the first occluder. What about hop 3, what is it supposed to predict? <sep> For line 14 in the algorithm, are the attentions of the heads averaged? <sep> About the 5 steps: Looking at the examples (eg Figure 15b) it looks like there are much more than just 5 key steps to follow. However, Hopper never uses H > 5 in the shown examples (while H=5 is actually the minimum number of hops that are allowed to the system). Why is that? It looks like as soon as it has a chance, the system tries to predict the last frame available, even if it is not the most convenient (it can always get later, to the last frame). This could make sense for CATER, but the shown examples are for CATERh . This is important because the main point of Hopper is that it can select where to attend. I would appreciate some intuitive explanation and statistics of the number of steps Hopper takes. <sep> Unclear what the model is really learning. While it is not learning temporal biases, there are potentially a lot of other biases it can be exploiting. It would be very interesting to have analyses to answer this question. Otherwise it is hard to believe the models are really ""solving"" this task. <sep> Additional comments and questions: <sep> When an object is occluded or contained by another one, the goal is to predict the position of the first object, or the position of the occluder? <sep> Is the 1 fps chosen for any specific reason? The solution to the problem is actually different depending on the sampling rate. For example, it would be possible that in between frames there is a key move that we do not see. <sep> ""Humans realize object permanence by identifying key or critical frames where objects become hidden"", and similar sentences throughout the paper. Citation? <sep> I think the third sentence in the introduction refers to the first one. It is confusing because it reads as it is referring to the second one instead. I would move the second one to later when the authors talk about object permanence (line 20). <sep> If the key point of the paper is the multi-hop transformer, an interesting comparison would be the same system without DETR, and instead using only the true class label and bounding box (remove steps A and B). At least to see how much these are a bottleneck, or how important they really are. <sep> Analyses on learned attributes. The classes are combinations of 4 attributes. Why not learning them separately? Is the model creating different represenations for each specific combination? Would it generalize to a new object or to a new combination of attributes not seen during training? <sep> Will the code for the paper and the cater-h dataset be released? <sep> Final recommendation <sep> Overall, I believe this paper should be clearly accepted to *CONF*, as the strengths outweigh the weaknesses. This paper is not the final solution to the object permanence task, and it has a lot of possible improvements, but it is a good step.","This paper was reviewed by four experts in the field. Based on the reviewers' feedback, the decision is to recommend the paper for acceptance to *CONF* 2021. The reviewers did raise some valuable concerns that should be addressed in the final camera-ready version of the paper. The authors are encouraged to make the necessary changes. It is also very important to think about how to extend this framework to the more challenging CLEVERER dataset (http://clevrer.csail.mit.edu/)."
"abstract | rating_summary | strength | weakness | rebuttal_process | decision  ==> summary <sep> This paper introduces transformer network to visual navigation, specifically, object-goal navigation. It also develops several new feature descriptors as the input of the transformer encoder and decoder. To properly train the whole model, a supervised pre-training stage is used to warm-up the transformer model. Great performance has been achieved on AI-THOR benchmark. <sep> pros <sep> Lots of people must have thought to use transformer to replace the RNN/LSTM in lots of visual navigation framework. This paper provides a good example. Most importantly, this paper focus on the representation learning part of the whole pipeline, which isn't that straightforward of how to use a transformer. <sep> The writing is mostly clear with clear motivation and background discussion. <sep> The performance boost, especially SPL, is relatively significant compared to previous SOTA, and the ablation studies have verified most of the design choices. <sep> cons <sep> There are a couple of things which are not clear to me, or confused me when I was reading the paper: <sep> The writing in the approach section isn't very clear. First, it would be much better to define clear notations for all the features/descriptors, and use such notations in the figure. The current writing uses ""instance feature"", ""global feature"", ""positional-global"", ""spatial feature"", ""spatial-enhanced""..., which are a little bit confusing to me. Second, I think most details are properly ignored in Fig.2. It becomes not as informative as the detailed version (Fig.4) in Appendix. Note that these two figures are not consistent that the ""add"" symbol for positional enhancement is missing is Fig.4. I also suggest that the positional embedding blob not crossing the arrow of global feature, they are just added together.Third, Sec.4.2 writes ""we first reduce the channel dimension of a high-level activation map from D to a smaller dimension d"", how the reduction is done exactly? From appendix it seems like a 256-dim vector is transformed into 249-dim. Fourth, h and ""w"" are abused. In figure, they are annotated  on the long side of the tensor, in eq (1) they seem to be the output of positional embedding, and in Sec.4.2 description they are the resolution of 7. Similarly, L is abused as it means input of encoder in Sec.4.1 but output of encoder in Sec.4.3. Let me stop here, but these things make the approach not super clear to me. <sep> In Sec.4.1, I'm not fully convinced of the statement of faster rcnn even thought the experiments empirically verified it. Faster RCNN w/o FPN only output features after ""conv4+ROI-pooling"" (ResNet-101-C4 variant). Why is it blamed for scale-sensitive? Actually, what does scale-sensitive mean here? Why DETR doesn't suffer from it? Honestly I don't think that's the reason why Faster RCNN performs worse. <sep> Also, I'm not fully convinced of the statement of the ""early stopping"" in Sec.4.4. The penalties are the same for different model in RL,  why this transformer based representation learner suffers from ""early stopping""? Is there a plausible explanation? It's fine that you cannot conclude something for sure because transformers are always hard to train, but the statement in paper reads not super convincing to me. <sep> Sec.5.1 SPL formulation seems to be wrong? The success indicator seems missing? The current equation is simply a ratio between any episode length over the optimal length regardless whether it's an success episode or not. <sep> Why not also adding global features into the transformer encoder? For example, reshape and concat with the input. Is the encoder supposed to be local? <sep> misc <sep> The best results of VTNet in Tab.1 used TPN. It might be better to introduce TPN in Appendix for completeness. <sep> Variance is not reported in Tab.1, which is uncommon for RL/control paper. <sep> Because transformer has attention module and the relationship can be easily visualized. I was expecting more interpretation/visualization like Fig.1 right to show the proposed methods actually attend to proper areas. The numbers are hard to tell what do each modules do exactly. <sep> questions <sep> Just to make sure I understand correctly, the instance feature (100x249) and spatial feature (100x7) are fed into a MLP for fusion? Can you describe the archi? <sep> Local spatial feature contains the normalized bounding box, confidence and top-rated semantic label. Is the semantic label the class index (1,2,...,C)? why not use a one-hot embedding or something? <sep> Is AI2-THOR the most popular benchmark for object-goal nav? I have seen lots of prior paper running on habitat. What's the specific reasons of using AI2-Thor over habitat? <sep> Please address my questions. I'm looking forward to discussing with the authors and the peer reviewers.","This paper addresses the problem of visual object navigation by defining a novel visual transformer architecture, where an encoder consisting of a pretrained object detector extracts objects (i.e. their visual features, position, semantic label, confidence) that will serve as keys in an attention-based retrieval mechanism, and a decoder computes global visual features and positional descriptors as a coarse feature map. The visual transformer is first pretrained (using imitation learning) on simple tasks consisting in moving the state-less agent / camera towards the target object. Then an RL agent is defined by adding an LSTM to the VTNet and training it end-to-end on the single-room subset of the AI2-Thor environment where it achieves state-of-the-art performance. <sep> After rebuttal, all four reviewers converged on a score of 6. The reviewers praised the novelty of the method, extensive evaluation with ablation studies, and the SOTA results. Main points of criticism were about clarity of writing and some explanations (which the authors improved), using DETR vs. Faster R-CNN, and the relative simplicity of the task (single room and discrete action space). There were also minor questions, a request for more recent transformer-based VLN bibliography, and a request for a new evaluation on RoboThor. One area of discussion -- where I empathise with the authors -- was regarding the difficulty of pure RL training of transformer-based agents and the necessity to pre-train the representations. <sep> Taking all this into account, I suggest this paper gets accepted."
"abstract | rating_summary | suggestion  ==> This paper describes a new method for normalizing few-shot learning episodes. The authors point out that the statistics of an episode are unreliable when the size of the episode is small or when the data distribution changes from episode to episode. To remedy this, the authors propose a method called 'MetaNorm' which uses a meta-learning approach to infer the means and variances to be used in the batch normalization layers that are employed in the feature extractor component. In particular, they meta-learn the parameters for a set of hypernetworks in an amortized fashion that learn to generate the means and variances of the batch normalization layers conditioned on the contents of the episode. The paper focuses entirely on the few-shot image classification scenario where MetaNorm is evaluated in various settings including standard few-shot classification and domain generalization (including a novel few-shot domain generalization setting). <sep> Pros: <sep> Fundamentally, the concept behind MetaNorm is innovative and promising. The idea of using meta-learned hypernetworks to generate the means and variances of the normalization layers is good. The paper also makes a good case for the effectiveness of a KL term that is added to the classifer loss function that helps the hypernetworks learn a good set of parameters. The range of experiments is sufficient. <sep> Cons: <sep> The paper is poorly executed and missing too much information to ascertain that the method outperforms competitive approaches. <sep> Specific concerns:(1) There is no mention of the MetaNorm training efficiency (which is generally understood to be the primary goal of batch normalization, see [1,2]). How does training efficiency of MetaNorm compare to other methods? And if training efficiency is not a goal, MetaNorm should be compared to methods whose goal is to adapt a classification system to a variety of data distributions (e.g. [3,4,5,6, etc.]). <sep> (2) Reproducibility: The paper is currently missing details that are required to reproduce the results and ensure fair comparison with other methods including: <sep> Which existing codebases were used (if any)? <sep> What hyper-parameters and training settings are used? (e.g. learning rates, number of training iterations, the use of early stopping, the number of test episodes, weight decay, etc.) <sep> In Section 4, ""Impact of Target Set Size"" indicates that the number of samples in the query set is a key parameter for MetaNorm and that 125 for the few-shot scenario and 128 for the domain generalization scenario are the best values to use. Are these the values used in the experiments? If not, what values are used? <sep> (3) There are many omissions in the paper: <sep> The variable 'm' as used in equations 3, 4, 8, 9, 10 is not defined. <sep> The KL expressions in equations 4, 8, and 10 should be written as a proper minimization expression where the parameters that are minimized are explicitly stated. <sep> The total loss function for MetaNorm is never stated. One could assume that it is a sum between the regular loss function for the particular few shot learning problem and the KL term, but it should be explicitly stated. Also, is there a weight on the KL term in the loss function? If so, how is the optimal weight determined? <sep> Various methods in the tables are not defined or referenced including RN, TaskNorm-I, TaskNorm-L, 'class', and 'example'. <sep> How are the confidence intervals on the results in the table calculated? Are they 95% confidence intervals, or something else? <sep> In the tables, how do you decide which entry to 'bold' given the confidence intervals? In table 2, TBN (and sometimes TaskNorm) are often within error bars of MetaNorm but are not bolded. Same for tables 11 and 13, 14. <sep> It would be good to have more information on the hypernetworks that generate the normalization parameters. In Section 2, it says: ""They are three layer networks with one hidden layer of 128 units and the input size depends on the feature dimensions of the convolutional layers."" How big is the other hidden layer and output size of the network? What sort of non-linearities are used, etc.? <sep> In section 2 it says: ""Note that the inference functions fμl() and fσl() are shared by different channels in the same layer and we will learn L pairs of those functions if we have L convolutional layers in the meta-learning model"". What is the justification for this parameterization (say versus having unique functions per channel)? Was an ablation study that compares various configurations done? <sep> (4) There are factual errors in the paper: <sep> In Section 1 and Section 3, the authors state that Prototypical networks uses transductive batch normalization. This is not the case (refer to https://github.com/jakesnell/prototypical-networks for the version of Prototypical networks authored by Jake Snell). Please correct. <sep> In section 1 it states that TaskNorm ""remains under-performing compared to transductive batch normalization"". While the results listed in Table 2 support this statement, the results in Table 3 refute this statement. Please clarify. <sep> (5) There are several missing attributions and some exact text taken from other papers that is not quoted or cited: <sep> In Table 2, the competitive results for MAML and ProtoNets seem to be extracted from [2], yet these are not attributed. Did you reproduce them? Were the MetaNorm numbers generated with a similar code base and hyper-parameters? <sep> In Tables 11, 13, and 14, the competitive results also seem to be extracted from [2] and were not attributed. Were they reproduced, or just stated? Were the MetaNorm numbers generated with a similar code base and hyper-parameters? <sep> The following sentences seem to be directly copied from [2]. If so, there should be quotation marks and attribution: <sep> ""Batch normalization relies on the implicit assumption that the dataset comprises i.i.d. samples from some underlying distribution."" <sep> ""The challenge constructs few-shot learning tasks by drawing from the following distribution. First, one of the datasets is sampled uniformly; second, the ""way"" and ""shot"" are sampled randomly according to a fixed procedure; third, the classes and context / target instances are sampled."" <sep> ""In the meta-test phase, the identity of the original dataset is not revealed and the tasks must be treated independently (i.e. no information can be transferred between them). The meta-training set comprises a disjoint and dissimilar set of classes from those used for meta-test."" <sep> (6) In Figure 2, the title says ""Impact of target set size."". However, for the left and middle plot, the horizontal axis is labeled as |S|, which is the support set size. Please resolve the ambiguity. <sep> Minor Comments: <sep> In section 2, MetaNorm for Few-Shot Classification: The sentence ""The p(m|Q) can be estimated by directly calculating statistics using the query set, which however performs inferior to inference by optimization."" is grammatically incorrect. <sep> In section 2, MetaNorm for Few-Shot Classification: should ""multiple layer perception networks"" be ""multi-layer perceptron networks""? Similarly, in section 2 it says: ""…which are realized as multi-layer perceptions and we call hypernetworks."" 'perceptions' should be 'perceptrons'. <sep> Sometimes the nomenclature support set / query set is used, and other places it refers to the same as context set / target set. Please use consistent nomenclature. <sep> In section 2, it says ""…which are realized as multi-layer perceptions and we call hypernetworks"". And later in section 2, it says: ""They are parameterized by feed-forward multiple layer perception networks, which we call hypernetworks"".  Note that hypernetworks are an established concept, you should cite [7]. <sep> Section 1 should clarify in the batch normalization exposition that the methods described in the paper apply only to normalizing 2D convolutional layers (i.e. does not apply to fully connected layers as they do not have feature maps). <sep> References:[1] Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In International Conference on Machine Learning, pp. 448–456, 2015.[2] John Bronskill, Jonathan Gordon, James Requeima, Sebastian Nowozin, and Richard Turner. Tasknorm: Rethinking batch normalization for meta-learning. In International Conference on Machine Learning. 2020.[3] Rebuffi, Sylvestre-Alvise, Hakan Bilen, and Andrea Vedaldi. ""Learning multiple visual domains with residual adapters."" Advances in Neural Information Processing Systems. 2017.[4] Rebuffi, Sylvestre-Alvise, Hakan Bilen, and Andrea Vedaldi. ""Efficient parametrization of multi-domain deep neural networks."" Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2018.[5] Requeima, James, et al. ""Fast and flexible multi-task classification using conditional neural adaptive processes."" Advances in Neural Information Processing Systems. 2019.[6] Tseng, Hung-Yu, et al. ""Cross-domain few-shot classification via learned feature-wise transformation."" arXiv preprint arXiv:2001.08735 (2020).[7] Ha, David, Andrew Dai, and Quoc V. Le. ""Hypernetworks."" arXiv preprint arXiv:1609.09106 (2016).","This paper proposes an lightweight method for cross-domain few-shot learning, using a meta-learning approach to predict batch normalization statistics. <sep> After the extensive paper revisions and discussion, the reviewers all agreed that this paper is above the bar for acceptance, assuming that the authors will include results for both the standard and expanded target set size in the final version of the paper. The authors are strongly encouraged to include these results in the camera-ready version of the paper."
"abstract | misc | strength | weakness | decision  ==> The paper provides a theoretical examination of the challenge of fitting recurrent neural networks (RNNs) to fit processes with long memory (or long-range dependence). Dubbed the ""curse of memory"", the author(s) restrict to the case of linear activation functions, and show that for processes with increased spatial dependence: (a) the width of the layers must increase exponentially to guarantee accurate approximations under a provided bound, and (b) a gradient-based optimization algorithm will take exponentially more time to converge. Sufficient details for reproducing the experiments are provided. <sep> I like the paper and believe the contributions to be both substantive and of wide interest in the RNN community. The analysis presented here is rigorous and comprehensive, and while the discussion is limited to linear RNNs, it provides a good starting point for further studies regarding long-range dependence with RNNs. The paper is reasonably well-written, and aside from a few minor typos — see the minor comments below — I did not encounter any serious errors. I do have some criticisms, however: <sep> Perhaps my biggest disappointment is that while the authors do a good summary of the relevant RNN literature, they have failed to mention any analogous ideas from the time series literature. For example, any discussion regarding Hurst parameters would be welcome, since this is precisely the type of ""long-range dependence"" discussed here. The lack of references to this literature is disheartening, since these concepts have played a key role in time series analysis for many decades, and it would have been nice to see this acknowledged, or a connection made. <sep> The takeaway from Theorem 4.2 regarding the exponential increase in the required width for fitting a linear RNN is nice, but is undercut by the fact that Theorem 4.2 is an upper bound. I'm satisfied with the result, but a lower bound for approximating a particular functional would be more convincing. <sep> I was disappointed with the ""informal"" presentation here. I can appreciate the attempt to simplify the full statement, but I still had to go to the supplementary material to understand the statement, which I should hope most readers would not have to do. I think a more precise statement than ""trapped in a plateau with timescale"" would be better. Perhaps something involving the hitting time, or even more simply, ||θ′(t)||=O(...) as ω→0+ for sufficiently large t? <sep> It is also worth noting that there is an extraordinary amount of supplementary material here, much of which, unfortunately, does not get the attention it probably deserves in this format. There are aspects of the paper, including the precise statement of Theorem 5.1 and the definition of the Airy target, that require the reader to visit this material. The proofs of Theorems 4.1 and 4.2 seem fine to me. Unfortunately, the proof of Theorem 5.1 is especially lengthy, so I did not get the opportunity to check each step in detail. However, the general argument appears sound. <sep> Overall, I am impressed with this paper and enjoyed reading it. The missing connections to classical ideas in time series analysis are unfortunate, and I would be willing to give the paper an 8 with a wider literature review and improved presentation of Theorem 5.1. An additional lower bound for Theorem 4.2 would bring it to a 9 at least. But, even in its current form, I recommend this paper for acceptance to *CONF*. <sep> Other comments: <sep> I believe Theorem 4.1 also follows from the fact that matrix exponential distributions are dense (see [1]), together with the Riesz-Markov-Kakutani representation theorem used here. <sep> ""We first show that the training dynamics of ExJm exhibits very interesting behaviors depending on the form of target functionals."" — This is a little too vague, maybe consider rewording? <sep> In multiple places: ""x being/is the white noise"" -> ""white noise x"" or ""x is white noise"". <sep> Check capitalization in the statement under Theorem A.1. <sep> ""Airy"" should be capitalized in each appearance. <sep> [1] He, Qi-Ming, and Hanqin Zhang. ""On matrix exponential distributions."" Advances in Applied Probability 39.1 (2007): 271-292.","This paper provides a theoretically rigorous treatment of approximation properties and convergence analysis of LINEAR RNNs. The reviewers were divided in their evaluation. On the positive side, the presented relation between approximation error and required memory size is not obvious and interesting. On the less positive side, two of the reviewers raised the necessity of mathematical machinery that were invoked. Furthermore, its applicability is unclear in ML, since they aren't applicable to the usual nonlinear RNNs. However, given that the theoretical contributions are clear, the final decision was to accept."
"rebuttal_process | strength | weakness | rebuttal_process | suggestion  ==>  ==> The paper proposes to add a KL-divergence regularization to the objective of regularized continual learning in order to encourage the output prediction to be close to a uniform distribution over classes (i.e., increasing the entropy). They argue that this regularization makes the local minima flat and thus less prone to forgetting. They try to build a theoretical connection using results from information projection but there is still a large gap. In experiments, they show on several benchmarks that applying the KL divergence regularization to different regularization-based continual learning brings improvements. <sep> It is always interesting to find a simple method that can consistently bring improvements. However, the simple method in this paper lacks sufficient explanation and theoretical justification. The experiments are also less convincing if without comparison to other types of continual learning methods. <sep> (1) The paper uses too much space (~4 pages) to explain the motivation of preferring flat minima and KL divergence and make them overcomplicated, which are actually quite straightforward to understand. However, it uses less than 1 page for theoretical justification of the main idea (Section 2.3) with few explanations about the equations and a large gap in building the connection between ""flat minima"" and KL divergence regularization. In particular, in Proposition 1: <sep> (i) The cross-entropy on the left or right-hand side of Eq.7 is defined on two joint distributions in the form of P(X,Y)=P(Y|X)P(X), i.e., it shows that two joint distributions P1(X,Y) and P2(X,Y) becomes closer after projecting one of them to the KL ball. However, what we truly care is whether P1(Y|X) and P2(Y|X) are close in expectation of X drawn from P(X), i.e., whether EX ∼P(X)[KL(P1|P2)] or EX∼P(X)[CE(P1|P2)] becomes smaller after the projection, where E is expectation and CE is cross-entropy. To close this gap, the authors need to prove this very specifically. <sep> (ii) The result is mainly due to an artifact of constraining all previous tasks' classifiers' outputs close to the uniform distribution. It is obvious that the distance of distribution outside the ball to distribution inside becomes smaller if we project the outside distribution onto the ball. However, previous tasks' classifiers might already perform very poorly on their data distributions because their predictions are too close to uniform sampling, and the KL regularization accumulates the error and only make the prediction quality worse. In the extreme case, one can think that the classifier for task t-1 produces almost uniform predictions so it performs very poorly. Now making the classifier at task t also produce similar uniform predictions for data drawn from task t-1 does not help. <sep> (iii) Proposition 1 holds only when Lemma 1 holds, and Lemma 1 holds only when P* is the minimum solution for minimizing the KL divergence, i.e., Eq.6. However, the KL divergence is only a regularization term in the objective of Eq.3 (and there are two other terms) so it almost cannot be minimized. So both Lemma 1 and Proposition 1 do not hold rigorously. <sep> (2) It is encouraging to see that the proposed KL divergence regularization can improve regularization-based continual learning. However, as shown in recent works, other types of continual learning (e.g., memory replay based and dynamic architecture based) usually performs better. Since the proposed regularization can easily work within these methods, it is more convincing to compare their performance before and after adding the regularization. <sep> (3) The experiments do not cover several standard continual learning benchmarks such as permuted/rotated MNIST/FMNIST and sequential MNIST/Tiny-ImageNet. <sep> (4) Encouraging flat minima can alleviate forgetting in continual learning has been analyzed and studied in a recent paper: <sep> Mirzadeh et al., ""Understanding the Role of Training Regimes in Continual Learning"", NeurIPS 2020. A discussion and comparison to it are recommended. Its analysis is in the model parameter space instead of the output space and is rigorous. <sep> (5) In Figure 3(c), the EWC+CPR achieved flat minima has much worse(larger) loss. Does this indicate that the stability is achieved with the price of significantly degraded loss? <sep> (6) Minor: the notations need to be more rigorous, for example (i) expectations E_{P_X} and E_X are both used in the paper at several positions, but they should be E_{X~P(X)}; (ii) to make the paper and appendix consistent, Q* in the proof of Lemma 1 of the appendix should be P*; (iii) The superscripts and subscripts can be simplified.","There was some positive consensus towards this paper, which slightly improved after the very strong author rebuttal. Reviewers, in general, appreciate the simplicity of the approach as well as its effectiveness. The most acute criticisms derived from several theoretical and technical points, similarity with [Mizadeh, 2020], and missing baseline comparisons. The author rebuttal responds to each of these points very clearly and convincingly, as well as with new experimental baseline comparisons that clearly demonstrate the effectiveness of the CPR approach. I encourage the authors to include the extensive comparison with [Mizadeh, 2020] provided in the rebuttal, especially given the similarity to the proposed approach. and to also tone down the strong claims of novelty in light of the similarities."
"strength | weakness | misc | strength  ==> ---------- After feedback ---------- <sep> First of all, I greatly appreciate the authors patient response to me during the feedback period. The discussion was really fruitful. Unfortunately, I still have a concern about interpretability of the proposed method, which is a central topic in the paper. <sep> First, we find it a bit strange when the reviewer says ""it is difficult to find importance/meaning of comparing motifs is unclear"", we clearly show our method does find importance in NAS-Bench-101, all 3 tasks of NAS-Bench-201 and DARTS search space (Fig 1 and 7) -- if we can't find importance/distinguish different motifs, none of the results we've shown would've been possible. <sep> Even when a method works empirically, if a rationale behind the procedure is not clarified, a paper would not be scientifically convincing. Thus, I still do not think my claim is strange. <sep> Second, the example the reviewer gives is not a case when averaged gradient fails. On the contrary, it is exactly an example of when averaged gradient works. A motif with high and diverse local gradient magnitudes but average to near-0 is not important for the purpose of interpretability, as it doesn't consistently explain the network performance by itself (just based on such motifs, one cannot conclusively deduce the impact on performance of an arbitrary, unseen architecture in general) ... <sep> In the last response, the authors explained the interpretability issue through combination of motifs, but it did not resolve my concern. To simplify the discussion, consider a bit extreme case in which only one motif is employed in a network simultaneously, and assume WL parameter h = 0. Let g(c) = d \\mu / d \\phi^j |_\\phi^j=c. Then, consider a hypothetical case as follows: <sep> motif a) g(1) = 10, g(2) = 10 ... g(10) = 10, g(11) = -10, .... g(20) = -10 : AG = 0 <sep> motif b) g(1) = 1, g(2) = 1 ... g(10) = 1, g(11) = 1, .... g(20) = 1 : AG = 20 <sep> In this example, b) has a larger AG, but a) can have larger importance in practice, and now, since only one kind of motif is employed simultaneously, the explanation of the authors cannot be applied. For the exploration purpose, I do not find any rationale to consider that b) is more important than a). I know that these are extreme examples and may depend on an application scenario, but my point is that these examples reveal difficulty of interpretation of AG. The authors introduce AG at Section3.2 as an importance measure without carefully discussing how it can be interpreted in the context of the WL based exploration (just referring other papers without discussing details in a sense of the above averaging). The explanation through marginalization also does not get rid of this question. Since the interpretability is a main theme of the paper, providing a better interpretability of AG would be desired. <sep> ---------- Before feedback ---------- <sep> The paper proposes to use Weisfeiler-Lehman (WL) kernels for neural architecture search. WL kernel can incorporate the topological structure of the network, and the authors combines WL kernels with Bayesian optimization (BO) to optimize the validation performance of the network. Further, the authors also claim that WL kernels provide a useful interpretation about good / bad network structures by using the derivative of Gaussian process (GP), which can also be used for 'pruning' architectures. The performance is shown for several benchmark datasets. <sep> Overall, the idea would be reasonable, and the approach would be useful. However, I'd have to say that the technical novelty and depth would be somewhat weak because the standard WL kernel is directly used without any significant modification, and a gradient-based importance evaluation is also a known technique (and its interpretation in this context is a bit difficult). Further, in my understanding, the paper should have provided more general discussions, not only to show data-specific observations. Detailed comments are as follows. <sep> The proposal of the paper is not fully clear for me because the strategies are described for each one of datasets, separately. I couldn't find general procedures for the architecture search, from the main text of the paper. In practice, of-course, tuning on each dataset would be required, but showing specific tunings for well-known benchmark datasets is not attractive. A strategy applicable to wide range of tasks would be required for a methodology paper. <sep> Interpretation of the gradient-based motif identification is difficult for me. Even when a motif has a large positive or negative gradient value, it only implies 'local' importance around the given architecture. To derive general insight, more careful treatment would be required. The authors provide the motif discovery procedure in D.1, but it should be shown in main text because interpretability is one of main theme of this paper. In Section D.1, the authors described an approach taking average of all possible values, but the average is also difficult to interpret importance because it compresses the entire space, and as the author admitted, the computation would be often intractable in practical settings. <sep> Although the authors claim that good/bad motif identification is useful for 'pruning', no detailed general pruning procedure is shown in the main text. Providing a general algorithm would be required. What does 'prune' mean in this context? If a motif is regarded as 'bad' once, it is discarded forever, or can revive somewhere? As I mentioned above, gradient information is only local information. Even when a motif is 'bad', simply discarding it completely would be risky. Even when the 'average' gradient is used, the problem would not be mitigated, because even if the average gradient suggests a motif is useless, it may help to improve accuracy locally. For me, the rationale behind the exploration strategy with the pruning in the paper is quite unclear. <sep> The authors claim that the identified motif is trasferable. However, evidence of this claim is not fully clear. It seems empirical suggestions only from a few (similar) datasets. When is transferring effective, how do you know it holds when, and can it harm in some case? I think that a general discussion is missing in the paper. <sep> Another difficulty of the gradient-based importance evaluation is that the lack of uncertainty evaluation. The gradient (3.2) is the expected value of the predictive distribution of GP. Therefore, the variance is not considered. For example, if GP does not have any observations, the expected gradient would be 0 (when prior is f(x) = 0 with the unit variance for any x, which is a standard setting), but variance of gradient would be large, meaning that a motif is still has a potential to become important. Again, discarding a motif by the expected gradient without considering uncertainty is seemingly risky, though the paper lacks this kind of discussion on uncertainty, though the uncertainty evaluation is a central issue on in the context of BO.","Most reviewers found the method proposed to be technically sound, well-motivated and particularly interesting due to the interpretability of its results. Indeed, the extraction of interpretable motifs from NAS is a valuable contribution. One of the reviewers was particularly concerned by the lack of guarantees of the proposed method and a perceived failure mode of averaged gradients. We thank both the reviewer and the authors for the detailed discussion on these points. Ultimately, the benefits of the method proposed and the magnitude of the contributions in the paper outweigh these concerns."
"abstract | strength | suggestion  ==>  ==> Summary: the paper attacks the problem of model selection for individual treatement effect (ITE) models when the domain of learning and prediction differ. Proposal is to use causal consistency as an additional ""regularizer"" in existing domain adaptation (DA) model selection methods. The ""regularizer"" would be scoring to which extent replacing factual outcomes by their counterfactual predictions would preserve conditional independence relations (induced by the causal graph) in the prediction domain. Experiments on a variety of datasets are conducted to show the added performance induced by using the ""regularizer"". <sep> Good points: <sep> novel and very creative idea excellent writing that introduces concepts as they are needed + illustrative figures (Fig. 1 and 2) <sep> rigorous exposition of assumptions - quite important in this kind of problems interesting, illustrative use-case on covid <sep> Questions: <sep> Is the dataset on covid openly accessible ? <sep> It seems that all base models are deep i.e. involving a non-convex optimization problem that makes the model prone to initial solution. Have you taken the ""choice"" of the random seed into account in the same manner as the hyper-parameters to select the model ? <sep> Points currently limiting the relevance of the paper: <sep> [impact] model selection is here studied in isolation to other related techniques, notably causal feature selection (e.g. https://arxiv.org/abs/1911.07147) - it seems to me that it thus limits notably the generality of the conclusions as I don't see why practitioners would use one technique without the other. It is a big question mark for me and I'd like to read the point of view of authors on that point. <sep> [relevance of baselines] In related works authors mention two methods that they place as ""state of the art"" for ITE model selection: Causal Assurance and Influence Functions. Even if they are not designed specifically for the DA use case why not include them in the experiments ? It is often the case in practice that methods not designed for a specific case are unexpectedly robust to more specific setting. In particular I think it would be most telling to compare an augmented/""regularized"" method (e.g. DEV+ICMS) vs generic SOTA methods (CA or IF). Disclaimer: I don't know enough of these methods to evaluate how easy it would be to perform such a comparison. <sep> [metrics] The standard PEHE metric seems to have been replaced by PEHE-10. I can't seem to find the definition of this metric in the paper ? why not use the standard ? <sep> [reproducibility] I can't seem to find the hyper-parameter grid that was used for the different base models and among which the competing methods would choose the best candidate - even in supplementary ? To allow other researchers to independently reproduce your results this is just mandatory. <sep> [reproducibility2] I can't find details on your strategy for seeding the different models. How many random seeds ? are they considered as hyper-params or part of an inner optimization loop ? <sep> [insights on results] The provided result in Table 1 indicates that adding ICMS to DEV or IWCV uniformly improves model selection. It seems in a sense ""too good to be true"". Can you provide intuition on why it is so ? Could it be possible to check that using ICMS alone is not already a very strong baseline - or is it in combination only that it works ? <sep> Points that could improve the paper (and my score): <sep> provide a self-contained formula of NLL directly when introducing it in Sec. 4 <sep> give full hyper-param grid for all models in supplementary define PEHE-10 <sep> clarify setting for random seeds add an experiment with the Causal Assurance and Influence Functions baselines add an experiment comparing best model selected by proposal (e.g. DEV ICMS) vs best model found by causal feature selection (e.g. Rojas-Carulla 2018 or Magliacane 2018) <sep> Overall, it seems to me the work deserves a more rigorous evaluation part as the underlying idea is very interesting and seems to be a very strong addition to the causal toolbox for ITE modeling.","This paper considers the problem of identification of causal effects under the unsupervised domain adaptation setting. The authors assume the invariance of the causal structure and use it to regularize the predictor of causal effects. The method is interesting and looks effective, although this assumption may not hold always true (e.g., in some domains, some causal influences may disappear, leading to extra conditional independence relations). Hope the authors will update the paper to address the concerns raised by the reviewers, especially to conduct a sensitivity analysis of the framework to misspecification of the causal structure and make the motivation for the used evaluation metrics clear, and also provide a more thorough review of related work."
"weakness  ==> First of all I want to point something out I found quite bothersome: <sep> The abstract states "" A desirable agent should be capable of balancing between different sub-tasks: navigation to find enemies and shooting to kill them.""  and the intro begins with ""...it is an urgent need to use DRL methods to solve more complex decision-making problems.""  I want to state that I strongly believe we should not be framing our research problems with these types of problems, nor trivializing concepts such as killing 'enemies'.  I'll try to be as unbiased in my scientific evaluation of this paper, but I would request that the language be toned down a bit, and ideally other types of tasks considered down the road. <sep> Back to the review: <sep> This paper presents a multi-task agent architecture with a final mixture component.  The authors show that this approach can work on a custom-built FPS game better than competing SoTA methods (both multi-task and mono-task agents.  Overall the method uses a bi-level optimization to find the optimal mixture of sub-policies as well as individually optimize each sub-policy. <sep> Pros: <sep> This seems like a relatively simple architecture and the empirical results are promising, and the analysis of alpha values correlation to sub-tasks is interesting and seems to indicate that the meta-controller does gain some insight into sub-task structure. <sep> Cons: <sep> There are many confusing points about the paper that made it hard to follow and that I would need clarified to argue for acceptance: <sep> Doesn't min(clip(ρ) · A, ρ · A) = clip(ρ) A ? <sep> Doesn't ωω\\gradαLtrain(ω+ϵ,α)−\\gradαLtrain(ω+ϵ,α)=0. Perhaps you meant \\grad w on the second term? <sep> What are L_train and L_val?  I couldn't find a clear definition. <sep> Do the sub-agents receive the values of the respective R_i or is only R=∑iαiRi passed to the agent? <sep> I am not familiar with bi-level optimization, it could be worth talking a bit more about this instead of environmental architecture choices which are relatively irrelevant to the core of the paper. <sep> Could you come up with  a task with more than 2 sub-tasks?  I find that 2 is likely a corner case and going beyond just two policies would make the method more convincing.  Trying out on a smaller more synthetic environment that would more easily allow task factorization would also allow for better empirical evaluation. <sep> Although I am not super familiar with the FPS environments for RL, are there some already existant environments that have been independently benchmarked?  Any FPS env has a mixture of navigation and fighting, so I wonder what the value of proposing a new environment for this would be. <sep> Conclusion: <sep> I am not an expert of the multi-task literature but although the proposed idea seems to have merrits, the paper would need to be more clear on the points above for me to consider it clean enough for publication.  As it stands the approach is too opaque to really understand what is going on.","The reviewers agreed that the paper presents interesting ideas but the presentation of the paper needs be improved. Also, the experiments and the related work section need be improved."
"abstract | weakness  ==> This paper addresses an interesting problem in retrieval system - compatible features learning. Given the old feature extractor and a new dataset, the objective is to learn a new feature extractor, so that the features extracted by two (old and new) feature extractors are comparable to each other. In the proposed setting, the old dataset (including its statistics), old classifier, and the parameters of the old model are not available. <sep> The presentation of this paper is clear, including the problem description and basic idea of the methodology. The construction of the pseudo old classifier is well motivated and reasonable. The performance improvements are significant based on the presented results. <sep> However, I still have some concerns. <sep> First, it is unclear that why random walk provides benefit to obtaining a better feature matrix. <sep> Based on eq 10, each column of the new feature matrix (left side) is the linear combination of the columns of the old feature matrix. Subsequently, the feature matrix is averaged to obtain the pseudo classifier. So intuitively, the pseudo classifier is obtained by weighted averaging of the feature matrix, where the weights are learned by random walk from their similarity scores. <sep> Given these facts, more explanations are needed for why random walk could produce the beneficial weights. <sep> Second, the formulation of the loss function (L) is not given. (eq 3), which is an important detail. <sep> Third, there is no experimental comparison with other works. So it is hard to see the contribution of this paper. <sep> I understand that this is a novel problem, which may not have prior work to compare with. However, to make the results more convincing, the proposed methods can be used to other similar settings for evaluation, e.g. the setting in Shen et al. 2020.","This paper deals with a problem of feature compatible learning, where the features produced by new model should be compatible with old features. As pointed out by the reviewers, there are several weaknesses with this paper: (a) the novelty is not strong enough, (b) the experimental results should be better explained and be more thorough, (c) the formulation is not well motivated."
"abstract | weakness | suggestion | weakness | decision  ==> This paper proposes a view-consistent framework to address the issues of expensive labels. In particular, this work first uses graph neural networks and graph attention networks to construct two different latent features of the same data. Then, it uses the same classification neural networks to produce the node classification outcomes. Finally, it uses the classification outcome to construct a so-called ""view loss"". In addition, it uses an incremental strategy to gradually included pseudo labels until some termination conditions are satisfied. <sep> Overall, the paper is easy to understood. However, I think the paper can be improved in each sections: <sep> [Introduction & Related work] <sep> The authors can better organize their presentation on the development and understanding of Graph Neural Networks. At the current stage, these content does not seem to connect to the current development of GNN. <sep> [Model architecture] <sep> 3.1 <sep> 1, can the authors explain the reason of using three-head representation? Also, why do the authors use the same non-linear graph convolution layers? Is it because the feature is different already? Can the authors specify the detail settings on this graph convolution layers? I did not find it in other places? <sep> 3.2 <sep> 2, I can roughly understand the reason of introducing the contrastive learning and co-training. However, maybe the authors should put of the content in the related work part and emphasise the difference of view-consistent algorithm from these two methods. Plus, I did not find experiments that uses the data augmentation method (instead of the view-consistent method). I can see there is contrast learning comparison, however, the other settings of constrast learning may be different? <sep> 3, the inclusion of pseudo labels are not well explained in this section. I was expecting to see more systematic analysis on this procedure, however, the current version can not fully convince me on this procedure. <sep> 4, small issues: view 1 and viewer 1 are both used in the paper and they should be consistent. Eq. (5) and Eq. (6) can be well formatted to save more space for presentation. <sep> [Experiments] <sep> 5, I am expecting to see the implementation code for at least the neural network specification in the main paper or supplementary material. However, they are missing.","The authors consider view-consistency when learning graph neural networks. However, as mentioned by the reviewers, the novelty of the proposed method is limited and the rationality of the implementation is not convincing. More deep discussions about related papers and analytic experiments are required to support this work. Additionally, I have concerns about the scalability of the method --- whether it can deal with more than two views and how it will perform are not studied in this work. I tend to reject it based on its current status."
"abstract | weakness | suggestion  ==> This paper introduces a new online convex optimization algorithm that operates in via the reduction to online linear optimization in which the regret is bounded by ∑t=1T⟨gt,xt−u⟩ where gt is the gradient of the t^th loss at xt. The algorithm is based on online mirror descent with non-decreasing quadratic regularizers x⊤Htx, using the update xt+1=argminx⟨gt,x⟩+(x−xt)⊤Ht(x−xt) (or, equivalently using the terminology in the paper, argmin⟨gt,xt⟩/(t)+(x−xt)⊤Ht(x−xt) where we replace Ht by Ht/t. The analysis is restricted to diagonal Ht, for which we can break the regret into a sum of d 1-dimensional problems, so it suffices to do the analysis in the scalar case. The idea is to break out the standard analysis of mirror descent regret as the sum over all t of D2(Ht−Ht−1)+gt2Ht−1, where D is the ℓ∞ diameter of the domain, and then choose Ht to minimize each of these terms greedily subject to the non-decreasing condition. A regret bound is provided for this algorithm that achieves worst-case T regret, but in cases in which the gradients are small, the regret is much better. By employing this strategy on a per-coordinate basis one can obtain an adagrad-esque regret bound. <sep> As far as I can tell, the algorithm is equivalent to the update: <sep> xt+1=argminx⟨gt,x⟩+(x−xt)2maxt′≤tt′|gt′| <sep> Where I remove the composite term and consider 1-d problems for simplicity. <sep> The analysis seems correct here, and the idea is a good approach. However, I am a bit concerned about the quality of the theoretical results obtained. In particular, it is extremely unclear to me that the main regret bound in Theorem 4.1 actually offers any advantage whatsoever over AdaGrad. In the paragraphs following the result, the authors offer the example in which gt=O(1/t). The authors then observe that in this setting, AdaGrad will obtain O(log⁡(T)) regret. They then also note that Theorem 4.1 obtains regret ≪T. This is true, but my reading of theorem 4.1 is that it will have O(log⁡(T)) regret, which is worse than AdaGrad by a log⁡(T) factor. <sep> I am not sure what is happening in Figure 1- my understanding is that this is plotting the analytical regret bound which seems actually smaller for AdaGrad using the provided example. I suspect the learning rates for adagrad are not tuned properly, but perhaps I am missing something. <sep> I have been unable to conceive of any sequence of gradients in which theorem 4.1 actually outperforms AdaGrad, and it is very easy to find sequences in which it does much worse (e.g. simply reverse the sequence of gradients in the provided example). It is, however, plausible to me that in reasonable settings in which the gradients decrease over time one might expect the bound to be within a constant (or maybe a log factor) of AdaGrad's bound. <sep> Looking at the analysis, I suspect that this is a fundamental issue with the approach of bounding bregman divergences based on the diameter of the domain. Once we commit to this, it is clear that the final regret bound must be at least on the order of D2HT by telescoping sum, since HT is increasing. Further, the ∑tgt2Ht−1 term can be lower-bounded by ∑gt2HT−1. Clearly, the minimizing value for HT here is then D∑gt2 to yield a bound of O(D \\sqrt{\\sum_t g_t^2}), which is exactly what AdaGrad does. <sep> I am willing to believe that the analysis can be improved here, but I am pretty sure that one cannot due so by bounding the Bregman divergences with the diameter. This leads to a pleasant telescoping sum, but I think AdaGrad may already optimize that style of analysis. <sep> As for the empirical results, these seem a bit more promising so perhaps there is some improved analysis that could be made. There could be a few clarifications here though: is the value for αt set to α/t in these experiments, or is it tuned via some other schedule? In analysis of Adam and AMSGrad, the authors typically use the α/t approach, but in practice I think this is not usually employed. If αt is set via some other schedule, and momentum is used as well, then it becomes unclear if the α tuning and the momentum is not what is providing the gain over AdaGrad rather than other differences. <sep> I am less expert in evaluating the significance of the final numbers in the empirical study. They do seem relevant, but frankly I feel that the theoretical discussion is currently dragging the paper down quite a bit.","The paper presents a new online convex optimization algorithm that uses per-coordinate learning rates. The learning rates are changed over time using information coming from the gradients. A regret upper bound is proved and the algorithm is empirically validated on deep learning experiments. <sep> While the analysis is in principle correct, it does not seem to provide any advantage over the guarantees of similar algorithm, for example the mirror descent version AdaGrad with diagonal matrices. Also, despite the intuition of the authors, the reviewers have found that the approach used in the analsysis is fundamentally bounded to give a worse guarantee than AdaGrad. Overall, the theoretical contribution appears to be not sufficient. <sep> On the empirical side, the experiments failed to convince the majority of the reviewers that the algorithm has a significative gain over similar algorithms. <sep> More generally, this paper suffers from the same problem of many other similar papers: There is a complete disconnect from the theory proven under restrictive assumptions (convexity, bounded domains, no stochasticity) and the experiments (non-convex functions, no projection on bounded domain, stochastic setting). Unfortunately, the deep learning literature is full of such papers, but the community should strive to do better and substantially raise the quality of field. In this view, I strongly suggest to the authors to try to improve the theoretical contribution, for example, trying to prove a convergence guarantee of the gradients to 0, rather than focusing on regret upper bounds. Such analysis would also suggest better ways to design new optimization algorithms better suited to non-convex problems."
"misc | weakness  ==> This paper proposes to learn multiple near-orthogonal paths (OMP) in the CNN which could provide better adversarial training performance by using one random path selected from the OMP block, improving the diversity of the adversarial training examples generated. Results show some improvements over regular adversarial training. Interestingly, the improvements are very significant on the VGG networks, while not quite significant for the ResNet variants tested. <sep> This paper claimed that it creates orthogonal paths, but it's realistically near-orthogonal since they only added a soft constraint on the OMP regularization term, similar algorithms have been proposed in the past: <sep> [Bansal et al. 2018] Can we gain more from orthogonality regularizations in training deep cnns? <sep> There have also been quite a few work on learning real orthogonal paths based on Riemannian manifold optimization. Some of these are of similar speed as conventional SGD and Adam. A review paper can be found at: <sep> [Huang et al. 2020] Normalization Techniques in Training DNNs: Methodology, Analysis and Application. <sep> Some of those papers should be cited. <sep> In terms of performance, I feel this work should be compared against other regularization-based adversarial defense methods. A couple examples of that are: <sep> Qin et al. Adversarial Robustness through Local Linearization. NeuRIPS 2019 <sep> Mao et al. Metric Learning for Adversarial Robustness. NeuRIPS 2019. <sep> Comparisons against those algorithms would further verify the performance of the proposed approach. <sep> Besides, there should be some discussions on potentially why the improvements on VGG networks are very significant and not so much on ResNet. <sep> There is also some recent evidence on the effect of early stopping on adversarial defenses (e.g. Rice et al. Overfitting in adversarially robust deep learning. ICML 2020). It would be nice if the authors could state when did they stop the training of the respective models. <sep> In terms of ablation, it would be nice to see different inference schemes. e.g. whether using a subset of the paths in the OMP block would be beneficial against adversarial examples or not. <sep> I look forward to seeing the authors rebuttal and comments from other reviewers.","I thank the authors and reviewers for the lively discussions. Although reviewers mentioned the work has potentials to improve adversarial robustness, they agreed that the current draft needs a bit more work specially to strengthen its experimental results and comparisons with related works."
"abstract | weakness | rebuttal_process | decision  ==> Summary: <sep> This paper aims to understand the cause of error in few-shot classification. The authors are particularly interested in the upper-bound of the error rate, which they break down into linear separability in the feature space of the meta-train classes and classifier discrepancy on the meta-train classes, among other terms. Empirical results show that the latter is the dominant term. After identifying this, the authors propose a method, Reducing Classifier Discrepancy, to reduce the classifier discrepancy, lowering the upper-bound of the error rate. Empirical results show the benefits of the proposed method on three few-shot datasets. <sep> Pros: <sep> This work investigates the cause of error in few-shot classification and finds an upper-bound for it. <sep> The proposed method, Reducing Classifier Discrepancy, is simple. Empirical results on three few-shot datasets show its effectiveness. <sep> Cons: <sep> Through experimentation, the authors find that classifier discrepancy dominates the few-shot classification error. This could be due to the dataset being used for the experiments. This might not hold true in a cross-domain dataset, such as Meta-Dataset [1]. Can results be provided on such a dataset? <sep> The proposed method, Reducing Classifier Discrepancy, has two training phases. There have been works [2, 3] that show that after the first phase of conventional supervised training, the model does very well on few-shot tasks. How does the performance after the first phase compare to the that of the proposed method? Is the gain in the proposed method coming from the model being trained longer? <sep> Additionally, the classifier discrepancy loss forces better clustering of samples in the meta-train dataset. Can this not be achieved by training for longer or using better hyper-parameters? <sep> Clarifications: <sep> In Equation 9 of the appendix, is the expansion of the discrepancy between \\Lambda(h) and h^{\\prime *} required for the result? <sep> Notes: <sep> It is hard to follow the equations. Additionally, the notation changes going from the main paper to the appendix. I would suggest cleaning that up for better readability. <sep> Equation 12 seems to have a typo - \\Lambda^{-1} instead of \\Lambda. <sep> [1] Mengye Ren et al. Meta-Learning for Semi-Supervised Few-Shot Classification. <sep> [2] Wei-Yu Chen at al. A Closer Look at Few-shot Classification. <sep> [3] Guneet S. Dhillon et al. A Baseline for Few-Shot Image Classification.","This paper proposes a contribution aiming at understanding the cause of errors in few-shot learning. The motivation is interesting but the reviewers pointed out many aspects that require more precisions and polishing in addition to the fact that the upper bound provided it rather loose. The rebuttal provided addresses some concerns, but there are still some remarks that require some clarifications en work. <sep> Hence, I propose rejection."
"decision  ==> Update after author response: I appreciate the authors' efforts to address my concerns. Thanks for the correction on QBC, I appreciate it. I still believe that the paper needs to accompany a more comprehensive evaluation and qualitative insights to highlight the effectiveness of the proposed method. For instance, it is common practice in Active Learning to report mean accuracy over multiple runs of the same experiment as data is sampled based on a particular heuristic which isn't always deterministic. Furthermore, the choice of warmstart samples could also influence the results, which is why it is recommended to conduct multiple runs of the same experiments and report mean performance. In such a scenario, any claims that arise from only one run of the experiments (as in this paper) should be taken with a grain of salt. I also appreciate the pointer to Equation 4 but how does it translate in practice is another important piece that is missing. BERT based models produce highly confident predictions and to visualize the distribution from your empirical investigation would help bridge the divide between the equations and the empirical results (how they actually turn out in practice). I am also not convinced by the authors' response to why the difference in performance of the BERT model trained on randomly sampled data vs the model trained on data sampled via ALBUS stays within the 1-2% range, often increasing with more data. I believe there are critical questions regarding this paper that need to be addressed before the paper is published, hence, my score remains unchanged. <sep> Summary: The paper proposes a new method to actively sample data to train models in a limited labeled data regime. Focusing on passage based question answering, the authors employ an uncertainty based sampling strategy by selecting examples on which a model has little top-1 prediction confidence. In every training iteration, the model is trained with an adaptive regularizer in an attempt to keep the new model closer to the current model. The results presented in the paper show improvement over three active learning baselines as well as training over iid sampled data on the SQuAD dataset. <sep> Pros: <sep> The paper looks at an important problem which is little studied in context of deep learning based NLP systems, particularly with pre-trained transformers. <sep> The two components that together form ALBUS are simple and the authors show that they perform better than various active learning baselines as well as random sampling on the SQuAD dataset. <sep> I like the approach of ensuring that the model weights do not deviate too much as this also addresses the challenges faced in active learning when we end up sampling outliers. <sep> The paper is easy to follow. <sep> Cons: <sep> All evaluations are conducted using a Bert base model. It is unclear whether this method generalizes to other models or not. Furthermore, the benefits of this approach on NewsQA are not easily interpretable, in fact the plots suggest that in many cases, baseline methods outperform this method. What happens when less than 20000 NewsQA examples are sampled for labeling is also not shown so it is hard to meaningfully interpret the results. <sep> Common and effective Active Learning baselines such as Query by Committee have not been compared against. <sep> The uncertainty based sampling method samples examples on which a model is less confident (determined by a threshold) on its top-1 prediction, however, the paper presents no qualitative analysis to show that this approach is indeed meaningful in context of models like BERT which, to the best of my knowledge, happen to have very confident predictions. It would help if the authors show how the distribution of the metric of informativeness over examples, and as more and more data is labeled. <sep> Questions for authors for rebuttal: <sep> It would be great to share actual numbers for NewsQA besides the existing plots. It is clear from the table and the figure that your method consistently outperforms baselines on SQuAD but it is less clear as to what happens in NewsQA as the only figures present are less clear and show that other baselines often outperform your proposed method. <sep> Several experimental details are missing from the paper that make it hard to put the numbers into context. Are the results averaged over multiple runs? If yes, how many? And if not, then I'd recommend averaging results for each approach over at least 3 runs. <sep> Since the authors have not shared their code with the submission, it is important to share how certain hyperparameter values were obtained. What influenced the choice to finetune BERT for only 2 epochs for both SQuAD and NewsQA or what influenced the choice of the number of datapoints for warm starting, etc.? <sep> On SQuAD, it appears that the difference in performance of the BERT model trained on randomly sampled data vs the model trained on data sampled via ALBUS stays within the 1-2% range, sometimes increasing with more data. Intuitively, this gap should be decreasing with more data (which does happen with other baselines that you've compared against). Why do you think this gap stays more or less the same with your method regardless of the amount of labeled data? <sep> Typos and other suggestions: <sep> Page 2, second paragraph last line: ""more faster"" -> faster <sep> Page 2, first line: a more appropriate reference for Active Learning would be Cohn et al., 1996. Cohn, D. A., Ghahramani, Z., & Jordan, M. I. (1996). Active learning with statistical models. Journal of artificial intelligence research, 4, 129-145. <sep> Even though the paper is easy to follow, I believe the writing can be improved, some sections appear to be written in a casual manner, for instance (these are representative, not exhaustive): <sep> (i) line 4 of introduction ""smooth-talking AI systems."" Consider using a better term to describe these systems. <sep> (ii) lines 13 and 14 of abstract ""demonstrate … that 25% less labeled samples suffice to guarantee."" Since the paper does not establish theoretical guarantees for this method, I would recommend not to use words like guarantee in this context. <sep> Missing references: <sep> Siddhant, A., & Lipton, Z. C. (2018). Deep Bayesian Active Learning for Natural Language Processing: Results of a Large-Scale Empirical Study. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. <sep> Reasons for the score: <sep> I think there are some outstanding questions regarding this paper that can be addressed in the author response, but until then, the proposed approach and associated results are less than convincing. I'm more than willing to increase my score following the author response.","All reviewers agree that the current approach is very similar to traditional uncertainty-based active learning, and that the empirical results are inconclusive, so at this point the paper is not ready for publication."
"abstract | rating_summary | suggestion | weakness | decision  ==>  ==> The paper is an empirical study looking at how different dataset properties affect model calibration in the context of vision tasks. All experiments use a specific well-known vision model (ResNet 50). <sep> In particular, the dataset properties that are investigated are: <sep> Balanced/Unbalanced classes. <sep> Label quality. <sep> Dataset size. <sep> Augmentations. <sep> NLP. <sep> I briefly present the main conclusions below. <sep> Balance in classes. Often times some classes have way more datapoints than others. The authors look at four datasets (Cifar 10, Cifar 100, Eurosat, iNaturalist). The last one's classes are unbalanced, whereas the first three require some sampling method to (artificially) make them unbalanced (note in this case by design there is no relationship between balance/unbalance and the class properties). Figure 1 shows the results. For Cifar and Eurosat those classes with more examples are better calibrated. The trend is somewhat similar for iNaturalist. <sep> Then, the authors present a number of approaches people have tried in the past to mitigate the consequences of unbalance in data. They repeat the previous experiment (on Cifar 10, Cifar 100, Eurosat) but, this time, using each of those methods while training the model. Table 1 shows the results. The ratio column offers very mixed results depending on the dataset and method. The authors conclude that overall the imbalance in calibration persists in most cases. <sep> Q. How do these results compare to accuracy? One would also expect to do better on classes with more data. <sep> Label quality. The authors tackle the question of how label noise affects calibration. In order to do that, they artificially inject noise to the ""true"" labels with increasing probability. Figure 2 summarizes the calibration error for a number of datasets and noise level. The pattern is clear: the more noise, the worse the calibration. Importantly, the calibration is measured on a test set that is not perturbed with random noise. Accordingly, results were to be expected: there's a mismatch between training and test distributions, and the further apart they are, the less ""meaningful"" predicted probabilities one should expect. Again, it would be informative to see how the accuracy of the model also degrades under this circumstances. Similarly, Figure 3 shows the effect of non-uniform noise across classes. Those classes ""attacked"" with more noise are worse calibrated. <sep> Dataset size. Another important practical aspect to study is dataset size. The authors subsample uniformly at random a fraction of the data points, and measure ECE. Figure 4 shows how models trained on more data are better calibrated. Again, the accuracy of the model should also be shown for context. <sep> Augmentations. It is common to use data augmentation to train better models; augmentations make the effective datasize larger. Figure 5 shows how removing augmentation axes leads to worse calibration. The same probably applies to accuracy (that's the reason why people use this!). This result is probably intimately related to the previous point (dataset size). <sep> NLP. The conclusions regarding dataset size also hold with a Transformer on an NLP dataset. <sep> Finally, Section 4 provides some theoretical explanation. We can summarize this as: the cross-entropy loss wants to have more and more confidence / probability on the right class for a given example, and when the data is small and the model powerful enough, we can basically memorize it to make cross-entropy happy. This, however, leads to overconfidence and poor calibration. <sep> On one hand, it's recently becoming clear that ECE is not a very robust estimator. Depending on design choices (as number of bins, argmax vs all, adaptive versus fixed bins, etc.) the ranking among models and conclusions can change wildly [1]. On the other, this study fixed a specific model, so one could say that the conclusions are ""shown"" for the (dataset, model) pairs. Still, I believe the conclusions are true in a more general setting, though, and the model is fairly reasonable. However, while the paper is titled ""Dataset Curation Beyond Accuracy"", I do not see how the outcome and conclusions of all these experiments would be different if we were looking at accuracy rather than calibration. The authors should measure, include, and address this, and try to disentangle both aspects, or argue for any correlation / causation relationship among them. <sep> [1] - Measuring Calibration in Deep Learning - https://arxiv.org/abs/1904.01685","The authors empirically analyse the properties of datasets which lead to poor calibration. In particular, they show that high class imbalance, high degree of label noise, and small dataset size are all likely to lead to poor overall calibration or poor per-class calibration. While there are some interesting insights in this work, the reviewers argued that the contribution is not substantial enough for *CONF*. To improve the manuscript the authors should consider accuracy and calibration jointly and extend the results pertaining to label noise which were appreciated by the reviewers. For the former, the same conclusions hold for accuracy, instead of calibration, which raises the question of their relationship -- is there a tradeoff? For the latter, the reviewers pointed to a concrete extension with structured label noise. Finally, the theoretical analysis is a step in the right direction, but the assumption on the width of the network required to fit the training set is too restrictive in practice. Therefore, I will recommend rejection."
"rebuttal_process | weakness | misc | weakness | suggestion | decision  ==> In this paper, the authors study the problem of unsupervised representation learning from data augmentations. Specifically, the authors claim that existing methods are prone to getting stuck at local minima owing to easy-to-learn local representations that optimise the commonly used MI objectives, and then propose a hierarchical method that tackles optimisation at multiple layers of the feature hierarchy. <sep> Strengths: <sep> A highly novel solution to an important problem in representation learning. <sep> Strong results obtained with respect to the state of the art. <sep> Extensive analysis and comparisons. <sep> Weaknesses: <sep> ""We demonstrate that current methods do not effectively maximise the MI objective"" + footnote: ""We show this by finding higher mutual information solutions using DHOG, rather than by any analysis of the solutions themselves."" => This claim calls for an analysis of the solutions and since this is missing in the paper, I would rephrase the claim. <sep> It would have been nicer to have experimental analysis on different features (e.g. colour) being more prone to local optima, though this is intuitive. This could have significantly increased the impact of the paper. <sep> Minor comments: <sep> ""a reasonable mapping need only compute colour information"" => ""a reasonable mapping needs only compute colour information"". <sep> ""Learning a set of representations by encouraging them to have low MI,"" => This should be high MI? <sep> ""CIFAR-10, CIFAR-100-20 (a 20-way"" => The parenthesis is not closed. <sep> ""A network was learned to associate"" => ""A network was trained to associate"". <sep> AFTER AUTHOR RESPONSE <sep> I have read the other reviewers carefully and the feedback provided by the authors. The reviewers had two major concerns: (i) Theoretical or empirical justification/proof for the following claim (and the motivation) of the paper: ""the current methods do not effectively maximize the MI objective because greedy SGD typically results in suboptimal local optima"". (ii) Lack of comparisons with newer methods from e.g. ECCV2020 etc. <sep> For the second, I feel empathy with the authors: In such a rapidly progressing field, it is difficult to integrate comparisons with new methods that are published while writing/submitting a paper. I am sure all of us have experienced similar problems. <sep> For the first issue, I disagree with the authors and agree with the other reviewers: This is an important claim that needs to be justified and I disagree with the authors's comment on the current results of the paper being a sufficient empirical evidence for the claim. An *CONF* paper should have provided the necessary evidences and justifications.","During the discussion phase, although the reviewers acknowledge superior empirical performance of the proposed method, they shared the two major concerns: <sep> Lack of theoretical or empirical justification/proof for the key statement: ""the current methods do not effectively maximize the MI objective because greedy SGD typically results in suboptimal local optima"". <sep> Lack of comparisons with newer methods from e.g. ECCV2020 etc. <sep> In particular, the first point is crucial. As the reviewers pointed out, since it is the main contribution and the key message of this paper, it should be carefully examined theoretically and/or empirically. However, in its current state, there is no theoretical analysis, and empirical evaluation is not convincing. <sep> About the second point, although I think it cannot be a solo reason for rejection, at least it is better to cite and discuss it fo the completeness. <sep> Overall, the contribution of this paper it not significant enough for publication. Hence I will reject the paper."
"rebuttal_process | rating_summary | weakness | decision | suggestion | misc  ==> Pros: <sep> (1) This paper is well-organized and easy to follow, clearly presenting the composition of the proposed model. <sep> (2) Motivation of this paper is strong, and readers may foresee rich applications. <sep> (3) Each model component is with a clear goal, facilitating the interpretation of the proposed approach. <sep> Cons: <sep> (1) This paper is not technical. All techniques are not originally proposed, and the whole model is simply 'putting everything together'. <sep> (2) For all ablation studies, no evidence is shown which component contributes to the performance improvement compared with the best single-component performance. <sep> (3) No performance variance is provided to make the improvement more convincing considering the close performances of some cases. <sep> Detailed Comments: <sep> This paper proposes an approach for unsupervised learning of universal node representations on heterogenous graphs. Specifically, four components are presented with individual loss functions to ensure the representations encoding a certain category of information, and the final model is attained by 'putting everything together'. Overall, this paper is well-organized and easy to follow, clearly presenting a well-motivated model and how each component handles the information extraction. We can also foresee great application value with the improved fitting ability on universal node representations. However, all techniques are not originally proposed, making this paper less technical. Besides, no evidence is shown which component contributes to the performance improvement compared with the best single-component performance in the ablation study. Also, no performance variance is provided to make the improvement more convincing considering the close performances of some cases in the experiments. To make the paper more convincing, I would have the following suggestions: <sep> (1) Ablation study should be enriched. For the current ablation study, only one component contributes to the main performance. Both the specification of where the performance improvement comes from and how components cooperation facilitates the performance are of little evidence. <sep> (2) More details should be provided on why larger training datasets (e.g., from 40% to 80%) can degrade the performance of some baselines. <sep> (3) For experiments, variance or standard deviation should be provided to make the minor improvements in some cases convincing. Also, performance on more diverse datasets should also be presented for generalization purposes. <sep> (4) In drug-repurposing experiments, the conclusion can also be obtained from Table 4 that PanRep-FT tends to have long-tail effect; however RGCN shows more stable performance. To better evaluate the performance, it is better to present the distribution among all drugs and evaluate the performance of the two models from different perspectives including application consideration. <sep> (5) There is no information on how to balance different loss terms. For example, at least five loss terms should be delicately tuned for a balance in PanRep-FT. However, the author does not present how to balance the weights among loss terms. Besides, I would argue the experimental weights among loss terms should be presented in detail to determine how each component contributes to the final performance.","Although the paper is clearly written overall and well motivated, reviewers raised several crucial concerns and, unfortunately, the authors did not respond to reviews. <sep> During the discussion, reviewers agree with that this submission is not ready for publication. In particular, empirical evaluation is not thorough as important baselines are not included and discussion is not convincing. <sep> I will therefore reject the paper. <sep> For future submission, I strongly recommend the authors to do author response. There are many cases where the reviewers change their scores based on the interaction between the authors and the reviewers, which is healthy for the review process."
"rating_summary | suggestion | decision  ==>  ==> Summary: <sep> The goal of the paper is to perform node classification for graphs. The authors propose a strategy to augment message passing graph neural networks with information from non-local nodes in the graph - with a focus on dis-assortative graphs. Dis-assortative graphs are graph datasets - where nodes with identical node labels are distant from each other in terms of edge connectivity. <sep> With node representation learnt from standard graph neural networks, etc., the authors propose to use an attention guided sorting mechanism, to create a proxy graph, where nodes which may have identical node labels be connected to each other (analogous to creating a k-nearest neighbor graph). Message passing is then employed on the proxy graph to learn final representations for the nodes.  Since the authors employ a single vector, namely 'c' (which they call calibration vector), to capture the 'importance of information' shared across different nodes  - there is a speedup in comparison to strategies which employ a pairwise comparison between all nodes in the graph. <sep> Pros: <sep> The idea to create a proxy graph to capture non local information is interesting <sep> The proposed technique can be augmented with almost any existing GNN <sep> My Concerns: <sep> (Dis-assortative or i.i.d.): - The authors in Figure 1 - show that homophily of the created proxy graph is a value larger than that of the original graph. However, from table A.2 in the appendix - it is clear to see that MLP's outperform GNN's with or without the attention sorting in the dis-assortative graphs - and the performance of the MLP's and proposed augmented NLMLP are well within one standard deviation from each other. This questions the need to employ a proxy graph construction on top of MLP's for these datasets as it appears like the data can be treated as i.i.d (and not relational). Moreover, these datasets (used from Pei et al. 2020) are extremely small to draw any significant conclusion. Also almost no gains are seen on the assortative datasets Citeseer, Cora, Pubmed (Please add datasets from OGB) - and their running times (when augmented with a proxy graph - are the gains worthy of increased run times?). <sep> (Baselines): Since the authors propose a strategy to construct a proxy graph (and the number of neighbors of each node in the proxy graph is the same??) - baselines such as creating graphs where nodes with identical labels are connected are also connected to each other / GNN on simple k-Nearest neighbors created using initial features (While a simple k-NN might appear more expensive - but the computation here is a single time effort) appears crucial. Also add a baseline, where adjacency structure of the graphs are iteratively updated during training such as - Learning discrete structures for graph neural networks (Franceschi, et al. ICML 2019) <sep> (Sufficiency, lack of details): The use of a single calibration vector may not be sufficient to sort the nodes - there are no guarantees in the paper to say when a single calibration vector would suffice. Also, the number of classes of nodes in each of the datasets used here are very small - and also does not trivially extend to multi-label classification of nodes. Also how do you also determine the number of neighbors in a proxy graph and do all nodes need to have the same number of neighbors in the proxy graph??? There are missing equations about how the calibration vector 'c' is learnt (what are the objective, etc) and the effect of running time when there are a large number of neighbors considered in the proxy graph -  without any equations its hard to argue against the case that the number of gradients to be computed would explode, when the number of neighbors are increased in the proxy graph (especially when jointly learning the GNN and the proposed augmentation). <sep> Other minor concerns: <sep> If possible, please include the difference between assortative and non-assortative graphs in the introduction - it makes it easier for a reader. <sep> If details are added and the concerns are addressed, I will be happy to improve my score.","This paper is right at the borderline: the reviewers agree it is well written, proposing a simple but interesting idea. However, there was a feeling among the reviewers (especially reviewer 1) that the paper could be strengthened considerably with a better discussion/some theory on the sufficiency of the calibration vectors, as well as experiments on larger datasets. Doing one of these would have substantially strengthened the paper. Due to the remaining shortcomings, the recommendation is not to accept the paper in its present state."
"abstract | weakness  ==>  ==> This paper presents a model that takes in a keyframe from a video and emits the noun and verb best matching what is being done in the frame. <sep> At inference time, the noun and verb have never been seen in combination with one another, but have each been seen paired with other nouns/verbs at training time. <sep> The paper presents a complex, three part model (ArtNet) to tackle this challenge, as well as unimodal linguistic baselines. <sep> Notably, the evaluation does not include vision-only baselines or pretrained model toplines, making it difficult to assess exactly what ArtNet is learning and where its advantage lies. <sep> Questions: <sep> There are multiple references to ""creation"", e.g., ""create novel compositions"" which makes the method sound like it's performing generation. From my understanding, though, ArtNet is purely discriminative, taking in a keyframe and predicting a noun and verb. However, there is one line in the paper that says ""We also learn visual reconstruction via a regression task."", which makes it sound like there's a formulation of ArtNet that maybe takes in a noun and verb and produces a keyframe image (using a GAN, maybe? Or a nearest neighbor lookup?), and so does ""create novel compositions"". If that's the case, it isn't described, and this image reconstruction task is never mentioned again in the paper or described in any equations. <sep> Were pretrained ViLBERT/UNITER run alone as a topline? Establishing how much is lost in performance due to lack of pretraining + how this method addresses that with sparse data would make a much stronger argument, I feel. In particular, we would want to see pretrained ViLBERT/UNITER and then pretrained + ArtNET to give a sense of how performance will change as models have seen huge amounts of aligned data. <sep> [Related] In Table 1 what's the intuition for language pretrained mBERT/UNITER/ViLBERT falling behind from scratch? Why use language pretraining and not vision pretraining (e.g., topline)? <sep> In Eq (1), why use cosine similarity for visual embeddings but then back off to surface forms for words? Was cosine similarity for word representations computed by UNITER tried? What is the intuition for this not working, if it did not? <sep> Not sure in Eq (4) what the sequence input to the LSTM is; does c range over some sequence? Doesn't the sum already collapse that? <sep> ""To ensure the focus is on new compositions, rather than new words, we removed new compositions that contain new words not seen in the train set."" All new words or just nouns/verbs? It's a big advantage/relaxation on the test set to have no OOV tokens. <sep> Why no vision-only baseline? Strip word contexts at training time except noun/verb, then predict only noun/verb at test. A lot of this could be basically object recognition followed by activity recognition or strong priors on p(activity | object) (e.g., always ""open"" or ""close"" for cabinets). <sep> ""outperforms Multimodal BERT/BERT with 1.23%/4.42% improvements, which is significant""; what statistical significance test was used? How many random initializations were tried to establish the average performance numbers for comparison between performance populations? <sep> Suggestions for Improvement: <sep> ""We call attention to a challenging problem, compositional generalization, in the context of machine language acquisition, which has seldom been studied."" This is poorly worded, since compositional generalization is well and commonly studied, to the point that even in this paper there is a section in the related work about it. Major workshops also list compositionality as a topic of interest, so I don't think it's fair at all to say that this has ""seldom been studied"" [ https://sites.google.com/view/repl4nlp2020/home ]. In the context of language acquisition specifically, emergent communication work focuses heavily on composition [ https://sites.google.com/view/emecom2019/home ]. <sep> ViLBERT in intro, UNITER in description of method, ""Multimodal BERT"" (mBERT?) in Table 1. What was used? Needs to be consistent in presentation. <sep> ""We discard the object labels due to strict constraints in the language acquisition scenario."" Because Faster RCNN is trained on ImageNet, which is based on WordNet, the object categories still exist in the form of supervision. The model has a linguistically-motivated notion of what an ""object"" is that can be traced to the WordNet. This should be acknowledged; you can't actually ""get rid"" of linguistic information inherent in a pretrained Faster RCNN. <sep> ""But there are few works addressing this emerging and valuable challenge for language acquisition in a multimodal reasoning view."" This paper does not really tackle language acquisition, though? There's a restriction so that the test set has no OOV words, even. I think the claims and presentation of the paper need to be carefully re-scoped. <sep> There is a lot of focus on ""learned arithmetic operations"" but no analysis as to what exactly this component ends up doing or learning. <sep> Nits: <sep> Typo Introduction ""a language model that generate"" S/V agreement. <sep> Figure 1 doesn't feel like it communicates anything about the method, and does not seem tied to the caption. Are boxes (1, 2, 3) meant to represent the association, reasoning, and inference steps? What's happening in each? <sep> ""The results show that ARTNET achieves significant performance improvements in terms of new composition accuracy, over a large-scale video dataset."" strange wording makes it sound like ARTNet is outperforming a dataset, not a method. <sep> ""We train the model to acquire words by directly predicting them."" this sounds like the model will be predicting words unseen at training time, which is not so. In particular, ""acquire"" here sounds like the model will be exposed to the word at most once (at inference time) and then be able to memorize that exposure in sequence. <sep> Typo? In 3.1 ""by running faster R-CNN too"" what is the ""too"" pointing to? Do you run Faster-RCNN somewhere? <sep> Typo 3.3 ""stringest baseline"" strictest? <sep> Figures 4 and 5 are so close together their captions bleed together and are really difficult to disentangle. <sep> Typo ""than our baselines 86.5"" makes it sound like the baselines achieved 86.5. <sep> Typo 4 ""the goal of learned model"" missing ""the""","The authors propose a new dataset and compositional task based on the EPIC Kitchens dataset. The goal is to test novel compositions and to build a transformer based network specifically for this inference (by analogy). Specifically, the analogy here references the use of nearest neighbors in the dataset. There are a lot of concerns raised by reviewers which require a large number of changes to the presentation of the manuscript and they are not at present convinced by the current setup or experiments. Explicitly motivating which pretraining methods do or do not violate which aspects of composition and what role other factors like synonymy play in generalization is necessary. Several aspects of the claims made in the paper and in the discussion are big claims that require substantial discussion and analysis (e.g. the surprising weakness of pretrained models) which the reviewers do not feel can be so easily explained away (e.g. by domain shift)."
"abstract | weakness | rating_summary | suggestion  ==> This paper proposes WAFFLe for anonymized federated learning.  The idea seems interesting, but I have a few concerns.  In summary, the motivation/privacy claims are not clear, and the performance evaluation doesn't seem fair as the authors only considered single-model FL algorithms. <sep> Motivation (Why can't we just use SecAgg?): Secure aggregation guarantees that each local model parameter is protected.  See ""Practical Secure Aggregation for Privacy-Preserving Machine Learning"" by Bonawitz et al.  Thus, I am not sure about this problem's motivation.  Is there any reason one should employ this instead of secure aggregation?  (Note that the original secure aggregation protocol is computationally expensive, but the recent variations are not: For instance, see SecAgg+ [Bell et al., ""Secure Single-Server Aggregation with (Poly)Logarithmic Overhead""] and TurboAGG [So et al., ""Turbo-Aggregate: Breaking the Quadratic Aggregation Barrier in Secure Federated Learning""].) <sep> Missing privacy guarantees: While secure aggregation comes with a solid privacy guarantee, there is no theoretical guarantee that WAFFLe indeed can protect clients' data.  Especially with the IBP prior that induces sparsity, each client will update only a small subset of the weight factor dictionary.  This pattern may even reveal more information about the set of weight factors used by each client.  (Maybe the updated r also reveals this support information?) <sep> Non-adaptive attack: While the authors have presented some experimental results to claim improved privacy in Section 4.4, they only used the off-the-shelf attack algorithms.  The authors should have designed an adaptive attack algorithm, which could have performed much better.  The need for ""adaptive evaluations"" has been well described in [Tramer et al., ""On Adaptive Attacks to Adversarial Example Defenses""], though in an adversarial example context. <sep> Multi-task/multi-center/personalization FL: The performance improvements in Table 1 and Table 2 seem mostly due to personalization, i.e., each model has its own model.  However, all the baseline algorithms assume a single model.  The authors should have compared the performance of WAFFLe with other algorithms that also maintain individual local models or multiple global models.  For instance, the authors may want to add MOCHA in [Smith et al., ""Federated Multi-Task Learning""] (cited in the current work), and multi-center FL algorithms in [Sattler et al., ""Clustered Federated Learning: Model-Agnostic Distributed Multi-Task Optimization under Privacy Constraints""] and its follow-up works.  The authors claimed that meta-learning-based approaches require sharing a small subset of data, but not all do.  For instance, see [Fallah et al., ""Personalized Federated Learning: A Meta-Learning Approach""].","This paper proposes an anonymization method for federated learning based on the Indian buffet process. The reviewers found the idea interesting, but raised the following main concerns (please see the reviews for more details): <sep> Motivation and terminology needs clarification <sep> Better comparison with secure aggregation methods <sep> Missing privacy guarantees <sep> Overall the reviewers of this paper are borderline. I hope the authors will take the reviewers' feedback into account when revising the paper."
"weakness | decision  ==> SUMMARY <sep> The authors propose to use a VQVAE-2 setup for video prediction. In particular, they propose a hierarchical discrete latent variable model that compresses videos into a latent space. An autoregressive model is then used to model dynamics in this latent space, which has reduced dimensionality, and can be used together with the VQVAE decoder to predict video.  Empirical results show that this model is comparable to SOTA GAN models and a human evaluation suggests that humans have a preference for the <sep> VQVAE generations. <sep> STRENGTHS AND WEAKNESSES <sep> [+] Good empirical results <sep> [-] Reduced novelty <sep> [-] Weak empirical section - only one dataset, limited comparison to baselines, missing details <sep> [-] Some claims are not properly justified <sep> DETAILED COMMENTS <sep> The main positive aspect of the paper are its results. The authors show that, for the Kinetics-600 dataset, their approach performs similarly to SOTA GAN methods based on the FVD metric. Further, they conduct a human evaluation that indicates that their generations might be preferable to those from GANs. <sep> However, this empirical evaluation is quite limited. The authors only show results for the Kinetics-600 dataset. It is true that this is one of the largest scale video datasets and that it is quite challenging. However, the authors could have shown results for other datasets (UCF101, Kitti/Cityscapes/BDD100K, etc.) that offer different trade-offs between complexity and amount of available data (note that for example BDD100K is in the same order of magnitude as Kinetics in terms of data available) and show the performance of their method on those datasets. Furthermore, that would allow a more direct comparison to some previous baselines including non-GAN approaches. The authors could also show different metrics beyond FVD)to assess the performance of their method - video prediction evaluation is an open research question and some metrics are known to have shortcomings, but given that there is no ideal metric then having multiple metrics could help have a better understanding on the model strengths and weaknesses. <sep> Another issue is the missing details in terms of architectural choices, optimization hyperparameters, computational requirements, training time, etc. These are important to be able to replicate the results and to assess the potential impact of the method, as many video generation/prediction models have large computational requirements that have prevented a wider adoption of some of these methods. <sep> One of the main shortcomings of the paper is its novelty. The paper is a straightforward application of the VQVAE2 model to 4D tensors (video) and for video prediction. The original VQVAE paper already shows some results on video, contrary to the claim that ""this is the first application of VQVAE to video data"". Further, using autoregressive models for video is a common approach (Video Pixel Networks, Weissenborn et al. 2020), and using an autoregressive model on the latent space of a discrete latent variable model is common in all VQVAE approaches. There is merit in showing that this setup works for video prediction, but nothing about the setup is novel. <sep> Finally, there are some claims in the paper that are factually or arguably incorrect. These have not directly influenced my score, but I believe they should be addressed should the paper be accepted. First the authors claim ""we know of no attempt to predict video at resolutions beyond 64x64, except for flow models"". Clark et al., Castrejon et al. and even the codebase for Denton and Fergus, Lee et al. all have results/models for video prediction at 128x128.  Second, the authors claim that most video prediction methods have skewed towards using GANs. This is arguably not true, I believe most GAN models perform video generation (Vondrick, Tulyakov) and most current video prediction models are based on VAEs (Denton, Babaeizadeh, Lee, Castrejon, Villegas, etc.). This is due to the fact that usually it is hard to get GANs to cover all modes and usually they perform better for unconstrained generation rather that conditional generation/prediction. In any case, this is debatable and as such it should be clarified or toned down in the paper. Some of the other unjustified claims include ""this model being the first application of VQVAE to video data"" as discussed. <sep> SCORE <sep> Overall I think this is a borderline paper. I think the empirical results are good and might be of interest to the community. However, the paper has important shortcomings - it has reduced novelty, there are important missing details, and the empirical evaluation is weak. Therefore in its current form I vote for rejection. <sep> POST-REBUTTAL UPDATE <sep> The authors' response did not address my concerns. Given that most current evaluations metrics for video generation/prediction have some shortcomings (including human evaluations), it makes more sense to include a wide range of metrics that showcase the strengths and weaknesses of a method rather than to argue against their inclusion in the paper. Additionally, the authors failed to mention very relevant prior work (Latent Video Transformer). <sep> Therefore I do not think this submission in its current form should be accepted and I have reduced my rating to a 4.","While this paper was perceived as being fairly well written, the level of novelty and the evaluation were seen as weak by many reviewers. The aggregate opinions across reviewers is just too low to warrant an acceptance rating by the AC. The AC recommends rejection."
"abstract | rating_summary | strength | weakness | misc | decision  ==> This paper proposed an unsupervised domain adaptation method for 3D lidar-based object detection. The idea is simple and straightforward: using cross-domain detector + offline tracking to provide pseudo-labels, inspired by similar UDA efforts for 2D detection. Experiments are conducted over multiple self-driving perception datasets, and results validated the effectiveness of the proposed method. <sep> Pros: <sep> The idea is simple and straightforward. The approach is technically sound. <sep> The presentation is clear. The introduction is easy to follow and enjoyable to read; Related work is thorough properly reflects the current states. Technical details are clearly described so that reproducing should not be very difficult. <sep> The experiment showcases solid performance improvement over baselines (self-training and statistical normalization) <sep> The paper also conducted very detailed and convincing ablation studies. <sep> Consistent improvements have been seen in several datasets and two different detectors. <sep> Cons: <sep> I have some concerns regarding claimed contributions/novelty. <sep> Offline tracking is not adequately benchmarked to justify the choice of extrapolation. <sep> The online tracker is not on par with the current state of the art. <sep> There is not enough pseudo GT quality analysis against manually labeled ground-truth. <sep> The usage of ""video"" to produce confident pseudo labels for unsupervised domain adaptation has been stressed in the introduction. However, as the related work described, this has been explored before with a similar technical approach for 2D detection (offline tracking to produce labels); see Roy-Chowdhury et al. <sep> It's hard to say if extrapolation is a significant contribution unless adequately benchmarked, showcasing the offline tracker has improved using this trick. Such benchmarking could be done on the KITTI tracking benchmark to compare with/without extrapolation procedure. The current ablation on UDA tells little information as improvement is not significant. <sep> There is no comparison against other trackers. Based on the reported numbers, the online tracker adapted from Diaz-Ruiz et al. 2019 is subpar from the current state-of-the-art Kalman-based online tracker. It's hard to justify why this one is chosen. Why not Weng et al. 2020 or Chiu et al. 2020, as mentioned in the paper? In particular, the tracker in Weng et al. 2020 is open-sourced. <sep> Please provide an mAP evaluation of the pseudo GT quality over some sequences with GT labels. <sep> Although not required, it would be great to see whether the author plan to release the code. <sep> --------------------------------------------------------- Post-rebuttal comments --------------------------------------------------------------------------- <sep> I carefully read the rebuttal and other reviewer comments. The author addressed my concerns on pseudo-label quality assessment and comparison against SOTA trackers. From the experimental perspective, I am very convinced the paper did a great job now. Please incorporate these additional experiments into the paper making it more complete. <sep> That being said, similar to other reviewers, I am not very convinced about the author's reply on novelty/contribution. It's true it has not been applied in 3D, which is new. However, I am not convinced by the claims in rebuttal, such as ""using physics-based dynamics models"" (I think you are referring to kinematics-based instead of physics-based), ""3D extrapolations"" (which could induce potential problems due to the multi-modal future uncertainty), and ""self-training"" (which is not new). Thus, if the paper gets accepted, I strongly encourage the author to rewrite the introduction and properly reflect the core contributions. <sep> Overall I am still on the positive side. But I am fine with both decisions.","This paper proposed an unsupervised domain adaptation method for 3D lidar-based object detection. Four reviewers provided detailed reviews: 3 rated ""Marginally above acceptance threshold"", and 1 rated ""Ok but not good enough - rejection"". The reviewers appreciated simple yet effective idea, the well motivated method, the comprehensiveness of the experiments, and well written paper. However, major concerns are also raised regarding the core technical contributions on the proposed approach. The ACs look at the paper, the review, the rebuttal, and the discussion. Given the concerns on the core technical contributions, the high competitiveness of the *CONF* field, and the lack of enthusiastic endorsements from reviewers, the ACs believe this work is not ready to be accepted to *CONF* yet and hence a rejection decision is recommended."
"abstract | strength | weakness  ==> Summary: <sep> Recently, several researchers have been trying to combine the goodnesses of direct policy search approaches (mostly based on evolutionary computation approaches)  and those of policy gradient approaches in control tasks. This paper proposes a novel combination of an evolutionary direct policy search and an actor-critic approach. The authors combines a cross-entropy method, which directly samples parameters of actor network (policy) from a Gaussian distribution that is trained during the search process, and the twin delayed deep deterministic policy gradient (TD3), which is an off-policy actor-critic approach. <sep> In existing combination of an evolutionary direct policy search and an actor critic approach, the evolutionary search part contributes to fill the replay buffer with diverse experiences, whereas the actor critic approach leads the population of the evolutionary search by replacing subset of the population with the policy of the actor critic agent, which is often expected to be superior to the population of the evolutionary search. Differently from these existing researches, the proposed approach also make an elite population to guide the actor critic agent when it is stacked at an area where the policy gradient almost vanishes. This idea comes from the recently proposed P3S-TD3, another population based (multi-policy) TD3 approaches. <sep> Comparison has been conduced on 5 MuJoCo environments. In Swimmer-v2 environment, the proposed approach reached the performance between the pure cross entropy method (best) and other combinations of evolutionary approaches and policy gradient approaches. In the other four environments, the proposed approaches reached the competitive or superior average performance. <sep> Criticism: <sep> Compared to the results presented in P3S-TD3 paper (*CONF* 2020), which used a slightly different environment and therefore the direct comparisons may not be valid, differences observed in this paper between the proposed approach and the pure TD3 is rather similar to the differences between P3S-TD3 and TD3. Since one of the main contributions of this paper, if I understand correctly, is the introduction of the component borrowed from P3S-TD3, P3S-TD3 should be included in the baseline approaches in the comparison. Is there any advantage of the proposed approach over P3S-TD3 on HalfCheetah, Hopper, Walker2d and Ant? <sep> Swimmer environment is the only tested environment where CEM works significantly better than TD3. On this environment the proposed approach was shown to be as effective as CEM. Does it generalize to other environment where CEM works significantly better than TD3? The current evaluation lacks the evaluation of the generality of this approach. <sep> In the ablation study (Table 1) the effect of Equation (4) was evaluated, and shows a non-negligible impact. Equation (4), which controls the number of interaction steps performed in each MC evaluation in CEM, can be incorporated to other approaches such as CEMRL, and if the same impact appears in the existing approaches, it is ambiguous whether the other components really contribute to improve the performance over existing approaches.","This paper proposes a hybrid algorithm that combines RL and population-based search. The work is interesting and well-written. But, the contribution of the work is very limited, in comparison with the state-of-the-art."
"abstract | rating_summary | weakness | rebuttal_process  ==> This paper analyzes Dropout through the lens of k-way interactions. The central claim of this paper is that Dropout reduces interaction effects. This is shown through both theory and experiment. The theory suggests that a higher dropout rate reduces the effective learning speed of higher-order interactions. Experiments suggest that increasing the dropout rate reduces the functional magnitude of higher-order interactions, even to some extent in real data. <sep> This paper tackles an interesting problem, and I mostly want to agree with its message, but it lacks in several areas. <sep> First, I would argue that the authors are probably correct in their conclusions. It seems reasonable that Dropout has the claimed effect. This would be coherent with what we know about Dropout. It's less clear if the speculation about larger scale architectures and problems is correct, as other more complex effects may make the interactions interpretation of hidden units less useful, but I think the authors make a good case to appeal to our intuition. More generally, there are several passages that are more speculation than result-backed extrapolation, I'd suggest the authors to modify the text to either clearly identify what is speculation and/or to align the claims with the results. <sep> The theory portion of the paper eludes me. As I currently understand it, it is wrong. Although it seems to reach ""experimentally correct"" and intuitively appealing conclusions, it may be right for the wrong reasons. That being said, I may be totally off the mark and I'm happy to be corrected by the authors. Either way, this portion of the paper needs to be much clearer. <sep> The experimental section of the paper is interesting, but it has some serious flaws. Mainly, the toy experiments may be misleading, and the real-world experiments have suspicious results. <sep> In particular, I see two experimental ""musts"" for this paper: <sep> Redo the Figure 3 experiment with no or much less noise <sep> Find reasonable hyperparameters for BikeShare such that learning doesn't diverge. <sep> Please find detailed comments below. <sep> I'm inclined to reject the paper as it is. I think the authors are exploring some very important aspect of deep neural networks that goes beyond Dropout rates, but the exposition could be improved, the experiments could be much more robust, and I'd like to understand the Theorem proofs before accepting this paper. <sep> ""higher Dropout rates should be used when we need stronger regularization against spurious high-order interactions"", how do we know when data requires high-order interactions or not? how do we know if they are spurious or not? This doesn't seem like a solved problem, as such recommending to tune Dropout rates based on this seems a bit impractical. <sep> ""when NNs are trained on data that has no interactions, the optimal Dropout rate is high, but when NNs are trained on datasets which have important 2nd and 3rd order interactions, the optimal Dropout rate is 0"", arguably any natural/sense-like data has 2nd+ order interactions, because data can only be understood through patterns and aggregate computations. Are the authors suggesting that we should just not use Dropout? <sep> ""Hinton et al proposed Dropout to prevent spurious co-adaptation"", as far as I know Hinton claims that Dropout breaks co-adaptations period, spurious or not. Plus, the original paper never actually backs that statement with quantitative evidence. As far as I know, Dropout has e.g. been shown not to recover causal structures particularly well. Eliminating all co-adaptations will eliminate spurious ones as well; I know Dropout doesn't remove all co-adaptation, I'm just skeptical it removes spurious ones more. <sep> Theorem 1 <sep> there seems to be a typo in the main text, should be fu(Xu)? (as in the proof) <sep> in the proof, I don't understand 4b to 4c. E[fu(XuM+)] is an expectation over all masks, M+. This turns into an integral over the output values obtained by changing one of the features, Xv, ""for some v"". This is like picking two masks which happen to have exactly Mv change, but then instead of taking the expectation, kind of like f(Xv∗0,Xu)+f(Xv∗1,Xu), the integral over the entire domain of Xv is taken. I do not understand how they are equivalent. <sep> I don't understand what makes 4c to 4d valid. 1b for fANOVA's decomposition describes a constraint of the argmin such that the decomposition finds orthogonal functions. (a) I fail to pattern-match the integral of 1b with the integral in 4c, (b) I fail to see why the equality of 1b, the integral being 0, applies to 4c. Is the Bernoulli mask g? This should be made very clear. <sep> I'm skepical that the conclusion is even correct, here's a counterexample, let's take f(X∈R) with f(−1)=−1, f(0)=0.5, f(1)=0.5, 0 otherwise. Assume we have p(X) s.t. E[Y]=0, as in the proof, and note that even so, f(0)≠0, i.e. a dropped out input isn't a 0 output. Let's say p=0.5, for X=−1, E[Y|XM]=−1∗0.5+0.5∗0.5=−0.25 , whereas Theorem 1 states that this value should be (1−p)1f(−1)=(1−0.5)∗−1=−0.5. Am I misunderstanding something? Is there an error in my reasoning? <sep> ""The distribution of training data is different for different levels of Input Dropout"", this seems fairly uncontroversial, by (somewhat) arbitrarily setting some input features to 0 (why not 1.42?), Dropout without a doubt changes the data distribution. A more interesting question is whether it changes the information content of the inputs, or if for some data distributions such as natural images the redundancy leaves information unaffected and Dropout simply forces the network to pick up on such redundancies. <sep> Theorem 2 <sep> I'm not sure what is meant by ""the gradient update for an interaction effect u"", so I don't really understand what is claimed here. <sep> As for theorem 1, it's not clear how to go from 5b to 5c. How is 1b related? <sep> Appendix A, I'm skeptical of the authors adding an extra page in the appendix in expectation of being accepted being in accordance with the spirit of the *CONF* submission instructions. This is fair game I guess, but a bit odd. For the record, Appendix A provides evidence for a base assumption of the paper, i.e. that a DNN can be decomposed into a number of additive low-order low-complexity interaction effects. This is done on a fairly toyish task, a small 5-d task with k=3, but shows this is possible and consistent. <sep> Figure 3 <sep> it's hard to see what is train and test in the 4th column, unless zooming in a lot. The authors could improve this by scaling the figures appropriately such that gaps in the dashed lines are visible. It would be useful for the font size to be scaled up as well for similar readability reasons. <sep> it's very suspicious that the Test MSE virtually doesn't change or gets worse depending on the dropout rate. It suggests that the network used is massively underfitting the problem as soon as some regularization is applied. I looked at the code and plotted the generated data, it seems to me that Y is dominated by noise rather than either additive or multiplicative cos(x)/sin(x) effects. This makes the task analogous if not identical to that of Figure 2. Either way, this is not really representative of any real world setup or of an interesting k-way interaction setup. <sep> The ""real-world"" experiments aren't very revealing <sep> 20-NewsGroups, Table 1. Why add extra interactions? Why not repeat the analysis of Figures 2 and 3 for this setup? It's also not clear if the reported accuracy includes the extra class or not. It's also not clear that k=2 is significantly different from k=3, how are ""best"" columns chosen? <sep> Bikeshare, Figure E.4, it would be interesting to see a repeat of Figure 2 here. It does seem like there is a small effect of dropout for k=3 early on, but by then models start diverging, even the train loss is going up. This, to me, suggests that the learning rate is too large or that something else is wrong. It's hard to draw strong conclusions from this. <sep> The Bikeshare results also suggest, as I pointed out earlier, that Dropout may indiscriminately be damaging to both spurious and real co-adaptations/interactions. This is at odds with statements in the paper. <sep> What is the expected effect size of a randomly initialized DNN? This seems like a useful value to track and compare against in these experiments. <sep> [Rebuttal update] <sep> Thank you for your response. <sep> This alleviates some of my concerns about Theorem 1, although I feel like I'd need to see a revised version of the entire proof to make sure I understand it. <sep> On Figure 3, I'm not sure you've understood my concerns; perhaps I did not explain them clearly enough. Regularized models do no better than chance, and less-regularized models do worse than chance on test points. This is presumably because of what I mention in my review, which is that the synthetic data is basically noise. Thus the ""improvement in test accuracy"" isn't really an improvement, but rather that the model is no longer free to extremely overfit. <sep> On the interpretation of Dropout you provide, this differs somewhat than the message of your paper. I agree more with this interpretation, although not fully. Either way, the paper doesn't really contain strong evidence for that interpretation, which I think would be great to have. <sep> I encourage you to rethink the experimental setup somewhat and to have clear experimental support for the proposed intuitions/insights. I think this is a valuable research direction but I think that a more mature paper would have a much higher impact.","This paper analyzes dropout and shows it selectively regularizes against learning higher-order interactions. The paper received mixed reviews, with two in favor of rejection and one in favor of acceptance. Specifically, while all reviewers find the intuitions and ideas in the paper adequate/plausible, two reviewers didn't find sufficient evident that supports the conclusions. The reviewers provided very detail feedback, which the authors responded to, but it is apparent that some of the analysis needs to be reviewed again before the paper can be published."
"abstract | weakness | decision | suggestion | rebuttal_process  ==>  ==> This paper proposes Expectigrad, which is a new optimizer for nonconvex optimization. The main idea is to consider arithmetic mean of squared gradients instead of exponential moving average and to use a normalization factor that takes into account the number of nonzeros observed during the run of the algorithm, for each component. The algorithm is analyzed for solving smooth nonconvex optimization and its practical performance is investigated. <sep> In terms of the strengths of the paper, I found the idea of using normalization nt depending on the sparsity interesting. However, the idea of using st, which is given in eq. (1) seems to be not novel. In particular, this step is already used in Adagrad based methods. For example, AdamNC method in Reddi et al., 2019 already shows that with such an update for second moment estimate, one can obtain convergence. This algorithm is also analyzed by Chen et al., 2018 in the nonconvex setting, which seems to be missed by the authors. <sep> In terms of the analysis, the authors make the simplifying assumptions of β=0 in page 5 and ignoring sparsity in page 16, which basically converts the algorithm to Adagrad. Can the authors clarify this connection and let me know if I am missing something? Therefore, I am not sure if the analysis brings new results that are not known in the literature. Such simplifications take away the novel parts of the algorithm and makes it impossible for the authors to show the effect of their idea in theory. Therefore the potential benefit of the method becomes purely experimental. For example, it is known in the literature that the extension of analyses for including nonzero (especially constant) β is not always easy and one needs to be careful [1]. <sep> Moreover, the author's usage of the term ""regret"" is not correct. The authors use the expected gradient norm as the optimality measure, which is the standard one for nonconvex optimization. Regret is used for online optimization, which is more general than stochastic optimization which is considered in this paper. <sep> Next, the authors analyze the algorithm in Thm 2 with increasing mini-batch sizes. However, Zaheer et al., 2018 shows that with increasing (or depending on the number of iterations T) mini-batch sizes, Adam (which is non-convergent in general) also works. Therefore,  I think this setting is not suitable since increasing mini-batch sizes shadow most of the theoretical challenges. It is better to analyze the standard setting without increasing mini-batch sizes. <sep> In practice, the comparisons with Adagrad and AdamNC are omitted. Moreover, it seems that the improvement of Expectigrad compared to SOTA is not significant. <sep> Therefore, in terms of algorithmic ideas and the novelties in theoretical analysis, this paper does not meet the bar for *CONF*. In particular, the simplifying assumptions that the authors make for the algorithm (β=0 and no sparsity) essentially takes away the interesting parts of the proposed algorithm, and making the theoretical analysis not consistent with the algorithm used in practice. Second, the algorithm needs to use increasing mini-batch sizes (the rate in Thm. 2 has 1/b as an additive term), which is another oversimplified assumption for the analysis, many adaptive algorithms are proven to converge without this assumption. Empirical merit of the method is also marginal and some important comparisons are missing. As a result, I am voting for rejection. <sep> Chen et al., 2018: Xiangyi Chen, Sijia Liu, Ruoyu Sun, and Mingyi Hong. On the convergence of a class of adam-type algorithms for non-convex optimization. *CONF*, 2019. <sep> [1] A New Regret Analysis for Adam-type Algorithms, Alacaoglu, Malitsky, Mertikopoulos, Cevher, ICML 2020. <sep> ======== after discussion phase ======= <sep> The main drawbacks of the paper remain after the discussion. Mainly, the analysis is too simplistic which basically ignores the new aspects of the algorithm and its comparison with well-known methods is unclear. Therefore, I keep my score.","The paper proposes a new adaptive optimization algorithm which is claimed to have better convergence properties and lower susceptibility to gradient variance. Reviewers found the idea of normalizing on the fly to be interesting, but raised some important concerns. Although similar to AdaGrad, Expectigrad has a very important differentiation due to division by nt. Assuming β=0 in my opinion is also ok and many papers assume this for analysis. Even after accounting for these two facts, during discussions the reviewers considered the work to be incremental and a more thorough evaluation is needed to determine the benefits of algorithm. Specifically, please compare to important and relevant baselines (like AdamNC and Yogi), because sometimes it felt like baselines were picked and dropped randomly. The empirical improvement provided by Expectigrad compared to SOTA is not clear (both on synthetic problems from Reddi et al and real problems). Thus, unfortunately, I cannot recommend an acceptance of the paper in the current form. However, I would strongly encourage authors to resubmit after improving according to reviewer suggestions. <sep> Some other minor points that came up during discussion are: <sep> choice of hyperparameters was not clear to reviewers, e.g. different optimizer may behave very differently for same set of hyperparameters, so it would not be fair to compare them as is. <sep> gradients would never be exactly zero in deep networks, so is current definition of nt good enough?"
"abstract | rebuttal_process  ==> This paper proposes an enhanced variant of the SREDA algorithm (Lou et al 2020), called SREDA-Boost, that improves SREDA on two aspects: the initial complexity and the step-size. The algorithm achieves the same complexity as the original SREDA scheme. <sep> The main contribution of this paper is perhaps the following aspects: <sep> -- C1: Improving the initial complexity for finding a starting point y0 of the SREDA. <sep> -- C2: Proposing a larger stepsize for SREDA compared to the original one. <sep> -- C3: Injecting a zero-order approximation step for stochastic gradients. <sep> The authors also claimed that their analysis is new and different from SREDA. However, in my opinion, this seems to be minor since their proof also relies on the bounds of gradient errors (like variance) as well as the delta_t quantity. There are of course some technical details and steps, but those are not the major contribution. <sep> In my opinion, the theoretical contribution of this paper is incremental. Indeed, the idea of using SARAH to improve oracle complexity has been widely studied in the literature, including Spider, SpiderBoost, ProxSARAH, etc. Since model (1) is nonconvex-strongly concave, it can be reformulated into (2) as a stochastic optimization problem. Several methods can be used to solve (2). The idea of using multiple loops is also widely used. <sep> The first contribution (C1) is not really new. The authors simply replace the step of computing y0 by iSARAH to reduce the computational cost. Because the problem is strongly convex, this step can be done by several methods, including accelerated variance-reduced schemes to further improve its complexity. Since this step is not essential, many previous methods just simply skip it. <sep> The second contribution (C2) is also minor since the idea of using a large batch-size to obtain large step-size has been used in the literature such as Spider-Boost (Wang et al 2019) or ProxSarah (Pham et al 2020). Of course, this enhancement helps SREDA have better practical performance. However, given the previous work, this contribution is very incremental. <sep> The use of zero-order oracle (C3) is not new as well, since it is another way of approximating the gradients, and it has been widely used in the literature based on Nesterov's idea. Hence, this step seems to be unnecessary if we assume that the underlying function is L-smooth. In most applications, we can directly compute the stochastic gradient components without using finite difference approximation. Unless the authors can provide compelling examples showing that this is an important step, otherwise, it is not really convinced. <sep> In terms of algorithm, SREDA/SREDA-Boost is also a multiple-loop algorithm, with at least three loops, making it challenging to implement in practice and it requires a lot of proper tuning and choosing parameters. There are some recent algorithms that can solve the same problems but with a single loop. The authors may want to compare with them, though these are very recent work (see, e.g. https://arxiv.org/abs/2008.08170 and the references therein). <sep> In addition to the above major comments, the following are some concrete comments: <sep> --- In Table 1, why NA is put in the ""Initial Complexity"" column of other methods. I believe that some paper may not describe this clearly, but if they use the exact solution (or up to a given accuracy approximation) of the strongly-concave max-problem, then the complexity is the best one since we can use the best algorithm to find it. <sep> -- The relation between the gradient of Φ and the KKT point of (1) should be clarified. <sep> -- I do not see where v_t is used in Algorithm 2. It seems that the output of Algorithm 2 should include v_t to use in Algorithm 1. <sep> -- It is not clear what is the problem solved in the numerical experiments?","The paper introduces a new variant (SREDA-Boost) of a variance-reduced method SEDRA for nonconvex-strongly-concave min-max optimization. Given that SEDRA is already optimal in the worst case, the proposed modification is intended to improve practical performance of the method, by relaxing conditions needed at initialization and allowing larger step sizes. While the reviewers appreciated the main ideas of the paper, they shared concerns about the significance of the paper's technical contributions, which were ultimately not addressed by the authors in the rebuttal phase."
"rebuttal_process | weakness | ac_disagreement | misc | weakness | decision  ==>  ==> This paper presents an explanation of why convolutional neural networks learn oriented bandpass filters - as has been commonly shown for early layers in various ConvNet architectures. The main argument is that oriented bandpass filters are the eigen-functions of localized convolution operators and in order to span the input signal space (regardless of its a structure) the network will always learn these functions as filters. Additionally, because this result is independent of input signal it should happen at all layers. These are demonstrated by examining filter of several trained neural network architectures and fitting them to Gabors - showing the fit is good with low residual error. <sep> All in all I think this is a very interesting paper which addresses a fundamental question and empirical result observed many times in the past. There are, however, many open questions here, many of them I think the paper should have addressed: <sep> rotation of the basis functions - the main argument of the paper is that these filters are learned because the are the eigen-functions of the convolutional operator and any task that requires spanning the input space (==all tasks) will learn these. However, there are many other possible basis function combinations that would still span the input space but will not be eigen-functions i.e diagonlized. For example, any rotation of these learned filters would be such a basis (but the filters would not be oriented band-pass filters in that case). I don't think there is a good explanation here as to why this specific basis is learned out of all possible bases. <sep> loss, learning etc - although the authors mention these results are independent of learning algorithm or loss, I would have wanted to see this demonstrated. Either by choosing a different task, a different optimisation method or some combination of the two. Specifically, demonstrating these results hold on non image data would have been more convincing I think (see below). <sep> discretization - all the analysis in the paper is for the continuous signals, operators and convolutions. Do results change for the discrete case (which is the case here)? If so how? this is not discussed in the paper. <sep> filter support and size - related to the previous point - how do results change with different filter sizes? the filters used in some of these networks are only 3x3 - this may result in a very coarse estimate for the bandpass-oriented filter parameters. I would like to see some discussion of this. <sep> no convolutional model also learn such filters - older, non convolutional models also learn oriented band pass filters (sparse coding, ICA etc.) when trained on natural images. These results, as the authors suggest, are related to the structure of natural images - is this related in any way to the proposed explanation? I think this requires more discussion - especially since the statistics of natural images is what makes convolutions such a natural choice for processing them. <sep> why do later layers get even better?  - it seems that deeper layers, across all architectures, get lower residual errors. Why is this? does it have to do with filter size? any other explanation? <sep> statistics of orientations and scales - is there any different in orientation and scale statistics for different layers and architectures? this should be easy to obtain and visualize and I think would add to the breadth of results. <sep> Bottom line - I think this is an interesting paper which can be even better with more in-depth discussion.","The reviewers raised a number of concerns, but <sep> the authors provided no rebuttal to the reviewers' comments. <sep> One reviewer felt the experimental fitting was not thorough enough. <sep> Suppose one used layers of oriented bandpass filters, separated by <sep> non-linearities, would that perform well on the task convnets are <sep> trained on? <sep> The AC doesn't agree with the arguments of R3. I hope the comments of <sep> the reviewers, particularly the many specific comments of reviewers R1 and <sep> R2, will be helpful to you as you revise the manuscript. <sep> The AC feels a more thorough experimental evaluation, and following-up <sep> on many of the suggestions of the reviewers will lead to a strong <sep> paper. As it stands, however, with 3 recommendations for rejection (1 <sep> weak), and only 1 weak recommendation for acceptance, we need to reject."
"strength | rating_summary | weakness | rating_summary | decision  ==>  ==> Summary: <sep> The paper presents a new saliency map interpretability method for the task of image classification. It considers the saliency map as a random variable and computes the posterior distribution over it. The likelihood measures the predictions of the classifier for an image and its perturbed counterpart. The prior encodes positive correlation among adjacent pixels. Variational approximation is used to approximate the posterior. <sep> ———————————————————————————————————————————————————————————————————————————————— <sep> Strengths: <sep> S1) The paper is very well written and easy to understand. <sep> S2) The paper does a good job of combining the ideas of perturbation based saliency map methods and total variation regularization with variational approximation in proposing their interpretability approach. <sep> S3) The proposed approach is able to generate real-time saliency maps, and, therefore, is computationally cheap. <sep> S4) The paper shows that their proposed interpretability method passes the sanity check from [Adebayo et al. (2018)]. <sep> ———————————————————————————————————————————————————————————————————————————————— <sep> Weaknesses: <sep> W1) My biggest concern with the paper is that the proposed approach entails training another (non-interpretable) network to explain a given pretrained classifier. One of the goals of interpretability is to ensure that the pretrained classifier doesn't encode any undesired biases learnt from the data. How can we ensure that this newly trained network doesn't encode any biases of its own, especially when this network is also trained on the same data? <sep> W2) Another key concern is the evaluation of the proposed approach. Details below: <sep> W2a) I am not sure if I understand the importance of qualitative examples correctly. The paper claims that VarSal is able to produce high quality object borderlines as compared to other methods. Is that a desirable property we should expect from interpretability methods? Ideally, we want the saliency maps to highlight regions which the classifier considered the most important. It is not clear to me how we can evaluate that from qualitative examples. <sep> W2b) Why is the case where pixels with the largest k% saliency values are erased more prone to creating unnecessary artifacts than the case where pixels with the smallest k% saliency values are erased? Especially, when compared to other methods, is there a reason to believe that these artifacts might be more common in some methods than others? <sep> W2c) Why is Real-Time [Dabkowski & Gal (2017)] omitted from the pixel perturbation benchmark (Figure 5a)? Given that it is computationally equally cheaper to the proposed approach, comparison with Real-Time seems to be the most important one. <sep> W2d) The proposed approach has many similarities to [Chen et al. (2018)]. How does it compare to the proposed approach / what are the advantages of the proposed approach over [Chen et al. (2018)]? <sep> W2e) The paper in its current form doesn't provide an explicit take-away message. Given a number of saliency map based interpretability methods available at this point, why should a researcher choose the proposed approach over other works? I think it would be a good idea for the authors to discuss the advantages as well as the disadvantages explicitly, highlighting applications/tasks/conditions where their approach might be better suited than others as well as cases where it might be better to avoid the proposed approach. <sep> W3) A very clear difference between the proposed approach and previous works is that the proposed approach explains the predicted probability distribution while the previous works focus on the ground-truth target. The difference is clearly an advantage of the proposed approach. However, it is important to disassociate the contribution of this difference from the rest of the approach. Is it possible for the authors to create an ablation of their approach keeping this aspect same as the previous works and observe how that version compares (to previous works as well as the proposed approach) qualitatively and quantitatively? <sep> W4) Saliency maps highlight the parts of the input image that the classifier finds most important while making a prediction. In that sense, isn't it more reasonable that for a given classifier and an image, the saliency map is deterministic? What does it mean intuitively for this saliency map to have randomness? It would be great if the authors can discuss why it makes more sense to treat it as a random variable instead of a deterministic quantity. <sep> W5) Can the authors please describe the construction of the graphical model in Figure 1? Shouldn't there be a solid line from 'M' to 's'? <sep> W6) The paper says that [Fong & Vedaldi (2017)], [Fong et al. (2019)] and [Dabkowski & Gal (2017)] -- ""all three methods have a limitation for producing importance ranking among features of a given image since their objective is to produce a binary mask"". What is the limitation here? And, how is the proposed method overcoming that limitation? <sep> W7) The take-away message from the uncertainity over explanation subsection in unclear. What is the significance of the uncertainity saliency maps? And, what should one learn by generating these maps for the corrupted datasets? <sep> W8) How is the hyperparameter alpha in the soft-TV Gaussian prior selected? <sep> —————————————————————————————————————————————————————————————— <sep> —————————————————————————————————————————————————————————————— <sep> Update after rebuttal: I thank the authors for their responses to all my questions. They satisfactorily answer some of my concerns. However, I still have two major concerns: 1) the faithfulness of the proposed approach, and 2) I see the potential contribution of uncertainty saliency maps but without an application/evaluation, their significance is unclear. I disagree that uncertainty/confidence generated from the same mechanism that generated the explanation is more trustworthy than the explanation itself. Hence, I cannot recommend the paper for acceptance.","Overall the reviewers had various positive things to say about the paper, including that it was well written and easy to understand, topical, that the method was sensible, novel and interesting and that the computational efficiency (i.e. real time) was appealing. However, all the reviewers thought it wasn't quite ready for acceptance, mainly citing concerns with the empirical evaluation. It seems they had trouble interpreting the empirical results and placing the work with respect to other relevant methods. <sep> It seems in the author response, the authors did much to add to the experiments, but ultimately the reviewers were not comfortable with acceptance. Taking the reviewers' feedback into account and adding the desired empirical evaluation would make this a much stronger submission to a subsequent conference."
"strength | weakness | suggestion | strength  ==> This paper presents an approach to accelerating NAS with 'petri-dish' networks, which hope to mimic the response of original networks at a fraction of training time cost. The key idea is to evaluate an architectural setting on a miniaturized network as opposed to the original network. With this approach computational effort is saved by eschewing expensive 'ground truth' original network evaluations. <sep> The largest hinderance by far in the paper is the quality of the writing. Much of the paper is not well written, and difficult to understand. In particular, the design section is written almost completely in prose without a well delineated algorithm. The paper does not do a good job of guiding the reader through their proposed approach, thus much hunting and guessing is required to understand exactly what it is that the authors are proposing. E.g., at first glance I believed that once the 'petri-dish' was initialized then ground truth evaluations would not be carried out again as it seems that is what the introduction is implying. However the authors combine the petri-dish with ground truth evaluations (Sec. 3.3) but this is not clearly presented in the introduction. Another difficulty is it's unclear exactly the precise experimental settings used, making it impossible to be sure whether the experiments are 'fair' to competing approaches. E.g., to generate Fig. 1 a 'petri-dish' is initialized and trained, however is it true that 'inference' carried out using this petri dish involves training a petri-dish network to convergence then evaluating its accuracy on the synthetic validation data? Although 'petri-dish inference' is defined in Sec 3.2, I see no usage of this exact verb in Sec 4.1. It's difficult to connect these concepts as a reader without a well delineated algorithm which concretizes with precision what is being proposed. <sep> Besides these issues, the proposed idea is not fully well explored which also leads to doubt on the robustness of the approach. One concern I have is that the robustness of petri dish idea at mimicking the original network is not well established outside of a rather simple experiment in Fig. 1. As this is a key linchpin to the proposed approach, I strongly require an extensive experiment in a complex setting (with multiple properties of a network being searched over). Another concern with Fig. 1 is that the 'ground-truth' evaluations used to seed the two models appear to be artificially restricted to the shaded box. Neural networks operate under the i.i.d. setting thus making this approach biased against the NN Model approach. The training and test dataset on this experiments should be sampled i.i.d. unless the authors can provide a compelling reason why. <sep> With regards to Fig. 3, from what I can understand the authors considered usage of NAO over 3 iterations of 33 motifs each to yield 100 ground truth evaluation. Did the authors consider 5 iterations of NAO with 20 motifs each as in the petri-dish approach. What do the authors mean exactly with, ""For a fair comparison, original NAO is re-run in this limited ground-truth setting and the resulting performance is depicted by the red-curve in Figure 3""? Can the authors exactly delineate their experimental settings in the appendix or in the main text. <sep> I note that evaluation was not performed for CNNs. Also it would be nice to see the performance of this approach on standard NAS benchmarks such as NASBench. <sep> Another shortcoming with the proposed approach is that the authors haven't proposed a 'general purpose' method to create petri dishes, and only width reduction is explored. Although this is a nice start, clearly this will not work if the architecture parameter being optimized over using NAS is 'width' itself. Have the authors considered such a scenario? How do the authors propose to create petri-dishes in the generalized setting? This shortcoming is glaring, and I'm not sure whether the proposed idea is well explored with a general purpose 'petri-dish' creation mechanism. <sep> Pros: <sep> -Possibly neat idea if executed well. <sep> Cons: <sep> -See above.","The reviewers overall appreciated the efforts of the authors in making NAS more computationally efficient. The paper could greatly benefit from further editing/restructuring with the goal of improving clarity, as it's currently hard to navigate and understand in places. Future submissions of this work would benefit from more extensive empirical validation that motif networks mimic the original network. The reviewers also agreed that for the method to be appealing/useful, a general way to generate motif networks is needed. Overall, the outcome was that this is a very interesting idea but needs further development along the directions outlined above."
"abstract | weakness | suggestion | decision  ==>  ==> The paper introduces a novel LM architecture that combines a Transformer with a PGBN topic model, enabling the transformer model to make use of addtional context topic information. The PGBN extracts topic information from the input, which is then used to enrich the information that is available to the transformer. <sep> They propose three different methods of incorporating this context information from the topic model into the transformer: Topic embedding vectors to add to each token, segment embeddings to summarize preceding segments, and topic attention. <sep> The topic model intervention can be applied to available pretrained transformers without the need to retrain the models from scratch. Experiments using pretrained GPT-2 and BERT show that incorporating topic information outperforms the respective baseline transformers both in language modeling perplexity and on the GLUE benchmark. <sep> To my knowledge this is a novel model design. The information extracted by the topic model is provided to the transformer in three different ways, ensuring the utilization of all levels of topic information. It allows for topic information to be drawn from both the previous and the current segments of the input data. <sep> Moreover, the topic model augmentation can be applied to pretrained transformers. This makes it very versatile and greatly improves its usefulness in practice. <sep> However, <sep> The presented work is motivated by the fact that transformers are limited in the length of the input they can condition on, and that topic information from previous segments could alleviate this problem. The model is then evaluated on GLUE, which is not a test of long-term dependencies. <sep> It remains unclear whether providing topic information of preceding segments is enough to allow the model to draw information from these segments that is useful for a task, beyond mimicking their style. <sep> The authors propose to use the learnt topics to perform conditioned topic-specific text generation. Given the multi-layer topic model architecture and the fact that GPT-2 uses BPE tokenization, splitting words into subwords, this raises the question whether most topics are interpretable enough to use them in a meaningful way for conditioning manually.. <sep> Moreover, I do not find the results convincing that this methods works. I do not think that proper emphasis has been put on the baseline as it is diverging from its initial solution. <sep> In general, I find this paper hard to read and keep track of what is being proposed. I find that it convolutes simple concepts and it could be a lot easier to understand the proposed method if written in a different way. <sep> I think the 54 citations in the introduction is unnecessary, please keep your paper more concise on what exactly you are building on top of and what you're proposing. <sep> I don't fully understand the TE embeddings. Is it N-gram distributions that you make a topic model over? <sep> Is WE a ""normal"" word embedding? <sep> Please elaborate on E_A and E_B","This paper proposes enhancing contextualized word embeddings learned by Transformers by modeling long-range dependencies via a deep topic model, using a Poisson Gamma Belief Network (PGBN). The experimental results show incorporating topic information can further improve the performance of Transformers. While this is an interesting idea, reviewers pointed out some weaknesses: <sep> GLUE evaluation is not a test of long-term dependencies, it remains unclear whether providing topic information of preceding segments is enough to allow the model to draw information from these segments that is useful for a task. <sep> The improvement over the baseline does not seem to be significant. <sep> The ablation study could be improved and more experiments could be done to understand the effect of hyperparameters choices from the topic model, such as the number of layers of PGBN as well as the topic number of each layer. <sep> A comparison of the model performance for different lengths of input sequences would be helpful. <sep> There are many recent methods for long.range transformer transformer variants, it would be interesting to compare them against the proposed latent topic-based method. <sep> Unfortunately, no answers are provided by the authors to the questions asked by the reviewers, which makes me recommend rejection."
"abstract | strength | weakness | decision  ==>  ==> Summary: <sep> This work proposes a fully-explored masking strategy, which maximizes the Hamming distance between any of the two sampled masks on a fixed text sequence. The motivation is to reduce the undesirable large variance of MLM objective, based on the hypothesis that randomly sampled masks in MLM would lead to undesirably large gradient variance, which as a result typically hurts the training efficiency with stochastic gradient optimization algorithms. <sep> Strength: <sep> The hypothesis from the variance reduction is interesting. The method is sound. Theoretical discussion proves that the gradients derived from the new masking schema have a smaller variance and can lead to more efficient self-supervised training. Experiments on both continual pre-training and general pre-training from scratch show the effectiveness of the proposed method. Case studies show that the method can help improve training efficiency. <sep> Concerns & Questions: <sep> Regarding the proof, the notations in Section 3 are quite loose, especially for Section 3.1. It might be better to tighten them and put the proofs in the Appendix into the main body of this paper. <sep> The authors claim the variance of the K-masks gradient can be reduced by decreasing the gradient co-variance between different masks, but this is just for a given sentence and the problem still exists considering different sentences. <sep> The proof in A.2 is not persuasive, I cannot agree that equation 14 can be concluded through previous statements. <sep> In terms of the experiments, there are only dev results reported on GLUE (Table 2). It is hard to infer the test gains, given the possibly significant hyperparameter optimization on the dev set. It seems that the authors have the infrastructure for computing single-model test-set results. So why not report the test results? <sep> Why are the backbone models (RoBERTa and BERT, respectively) different in Table 1 and Table 2? I'm concerned whether these improvements will hold after optimizing BERT carefully like RoBERTa, or using more advanced backbone methods like ALBERT. Why not using a larger model (e.g., Roberta-large)? <sep> There is no comparison with other public masking methods in Table 2, such as whole-word-masking, span masking, etc. Did you use dynamic masking as that was previous used in RoBERTa? How about the benefits compared with the proposed one? <sep> Concerning the claim, ""the proposed fully-explored masking strategies lead to pre-trained models with stronger generalization ability."", it is not clear how the proposed method yields stronger generalization ability. <sep> The ablations cannot serve the topic of this paper well. For the first ablation, it just gives out the performance of intermediate models on a single task. It might be better to give out the trend of training loss and validation loss. For the second ablation, why do all the larger splits lead to similar performance? Intuitively, doing more mask-then-predict procedures is better for learning language representations. The explanation at the end of Section 4 is not persuasive. <sep> Minor issue: the citation format is not consistent, please check the usage of \\citep{} and \\citet{}.","This work proposes a fully-explored masking strategy, segmenting the input text, which maximizes the Hamming distance between any two sampled masks on a fixed text sequence. The hope is to reduce the large variance of MLM objective, based on the hypothesis that randomly sampled masks in MLM lead to undesirably large gradient variance, which typically hurts training efficiency with stochastic gradient optimization algorithms. <sep> Pro <sep> A clear and interesting, novel leading theoretical idea. The paper has one good theme that it pursues. <sep> A mostly well-written paper <sep> Contains good theoretical discussion <sep> Experiments support the idea <sep> Con <sep> The experiments could be better, especially they don't actually measure a reduction in gradient variance only accuracy <sep> The proofs are at best loose <sep> Alternative methods of reducing variance like using large mini batches are not considered <sep> The results might go away with use of stronger (larger) contextual LMs <sep> There isn't good comparison to other methods of masking like span masking and salient term masking <sep> Some of the things included seem quite haphazard (the ablations don't seem to the point, it's not really clear what this has to do with continual learning) <sep> Overall, this paper feels to be in a premature state. The idea is interesting, but the idea and the paper needs to be developed more, with stronger results. I think it doesn't deserve to be accepted at this time."
"abstract | strength | weakness | decision | misc  ==> Summary of the contribution <sep> The paper addresses the issue of combining datasets from different sources, and taking into account the fact that they do not share the same precision. <sep> It aims at predicting the performance of an algorithm trained on those data, through a simple formula taking into account the size of the data, and its composition through the mixture weights of the distribution from which it comes. <sep> As the formula also depends on the estimator trained and the quality of data, it is trained to be estimated fully. <sep> Strengths <sep> The paper and the topic are both interesting. The maths look correct, although I have not checked it all. <sep> Weaknesses and concerns <sep> The paper lacks clarity in general. <sep> Context <sep> The context of mixed-source data is not very clear: <sep> from Section 2, it appears to be applicable e.g. for datasets coming from different sensors, but predicting the same output, while the experiment section makes use of very different datasets, not even predicting the same target; <sep> I encouraged the authors to provide specific examples in Introduction or Section 2 for clarity. The experiments seem to be related to multitask learning, as mentioned shortly for one application but looks to be true also for others; <sep> it could also possibly be related to domain adaptation. In both cases, the experiments should include comparison to state-of-the-art approaches in those learning problems. <sep> It is also not clear how the different sources are mixed together: does the proportion q apply to the number of observations in each source? <sep> Moreover, there is mention in the Introduction of the goal of optimal data collection policy. <sep> How can the proposed work be used in practice for that goal, since we do not know in advance what is the quality of each dataset, or how well does an estimator perform? <sep> Main goal of the work <sep> The title and most of the work is a bit misleading, as it mentions predicting the performances of an estimator, while in fact it predicts its excess loss, that is, how close an estimator is from the best possible estimator (oracle) in the model; <sep> while this is a good approach to improve the fitting of an estimator (and its parameters) inside a family, it does not give any clue as its actual performance. <sep> It is also not clear what the learning problem at the beginning of Section 3 does, and how it is used in practice. <sep> Related work <sep> Appart from the learning problems mentioned before, here are possible issues with related work <sep> Active learning does not qualify as related work, as it is usually concerned with the labelization of unlabeled samples in a single source dataset when labelling is expensive. <sep> If there exist work on that domain that is specific to the problem of combining data from different sources, please provide a more specific reference. <sep> There exist however other works for the issue of performance evaluation through a learning problem, e.g. ""Per Instance Algorithm Configuration of CMA-ES with Limited Budget"" by Belkhir et al (2017); <sep> it should be interesting to compare to the proposed work. <sep> Experiments <sep> The experiments show the following issues: <sep> As the proposed formula estimates the excess loss and not the performance itself, there should also be a comparison of the actual performances obtained both in the full training set and on the test set. <sep> Amazon sentiment: <sep> regression is applied on the sentiment prediction, which is a classification task the estimator considered is ridge regression, which does not fit Proposition 3.1 setting and no comments are made as to how the penalty would impact (or not) the theoretical properties of the work <sep> C(q) contains 4 terms, while it said that there are 3 data sources in training <sep> Minor comments: <sep> Proposition 3.2 is not written in the same format as the others, and makes it hard to see the link with n (or log n). Please re-write it or explain where the n component is. <sep> Overall evaluation: <sep> The paper is interesting and tackles an important problem. However, it lacks clarity in many aspects. <sep> =====POST-REBUTTAL COMMENTS======== <sep> I thank the authors for answering my questions. Their answers did clarify some aspects, such as the ratio of mixed sources, the optimal data collection, and the relation of the proposed work to active learning and multitask. However, the answer provided on my comment about the distinction between excess loss and absolute loss is not sufficient. I believe this is an important point, and calls for a major revision, not just a promise to clarify the point throughout the paper. The same comment applies to the experiment when comparing methods on the basis of computing the excess loss and not showing the absolute loss, which is the actual measure of quality of an estimator.","This paper studies the following broad question: How can we predict model performance when the data comes from different sources? The reviewers agreed that the direction studied is very interesting. While the results presented in this work are promising, several reviewers pointed out some weaknesses in the paper, including a confusion between absolute loss and excess loss, and the limited scope of the experiments. Overall, this paper does not appear to be ready for publication in its current form. In my personal opinion, if the concerns raised by the reviewers are appropriately addressed, this work could be publishable in a high quality venue."
"abstract | rebuttal_process  ==> Summary: <sep> The contribution of the paper is threefold: First, it proposes a novel variation of AdderNet (Chen et al. 2020) that ensures the network is always 1-Lipschitz with respect to ℓ∞ norm. The architecture allows one to generate robustness certificates with respect to ℓ∞ norm with only a single forward pass, which is computationally cheap compared to many of the previous methods. Second, it analyzes the expressive power and robust generalization of the architecture. Finally, it proposes two training techniques (bias batchnorm and p-norm schedule) that overcome the optimization problem inherent with training ℓ∞ Net and analyzes its empirical performance with respect to the previous methods. <sep> Strength: <sep> The approach is easy to implement while performing reasonably well. <sep> Similar to (Anil et al. 2019), the approach is very efficient requiring only a single forward pass for certification, and it performs substantially better on MNIST in comparison with (Anil et al. 2019). <sep> The expressiveness of the architecture is analyzed, though the proof is simple, it is nice to settle the issue within the paper. <sep> I like that they are studying  ℓ∞ robustness since it has been shown to be a more important problem compared to ℓ2 robustness (Goodfellow et al. 2018). This is in contrast with the recent popular randomized smoothing approach, which tends to focus more on ℓ2 robustness. <sep> Weakness: <sep> The experiments do not include results for the convolutional variant of the model nor results for CIFAR-10. It is odd that the convolutional variant of the model is not included since the original AdderNet was developed specifically for convolution. Even though (Anil et al. 2019) did not include results for CIFAR-10, all the other related papers (Wong et al. 2018, Sven 2018, Zhang 2019)  included results for CIFAR-10. I don't think performance on these experiments is a deal-breaker, but it is important to include these results so that future researchers can build upon your work and put your work in the context of existing literature. <sep> Recommendation <sep> Overall, I like the paper and recommend acceptance. I would further raise the score if the experimental concern above is addressed. <sep> I find enforcing the 1-Lipschitz property through architecture to be a very promising direction for efficient robustness certification, and the work proposes an interesting and reasonably effective method for enforcing the 1-Lipschitz property with respect to ℓ∞ norm. My main concern is that the experiments are not comprehensive enough. It is okay if it does not significantly outperform the previous methods, but it would be easier for the community to build upon your work if experiments are more comprehensive. <sep> Miscellaneous: <sep> Since the approach is so similar to AdderNet, I recommend giving more credit to it. I wouldn't have realized the similarities between them unless I dug into the citation. <sep> In Table 1 and 2, why are IBP, CROWN-IBP, and GroupSort considered as not scalable in the comparison? IBP & CROWN-IBP only cost ~2 times for certification compared to normal inference. Groupsort network requires only a single forward pass to calculate the certified radius. <sep> The generalization bound looks nice but I find that it slightly distracts from the main point of the paper. I am not too keen on the generalization bound that depends on Rademacher complexity in general since it does not consider the model learnable through gradient descent (not that it is an easy thing to do). This comment is more for future references, and I am okay with the way the paper is laid out right now. <sep> [1] Chen, Hanting, et al. ""AdderNet: Do we really need multiplications in deep learning?."" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2020. <sep> [2] Goodfellow, Ian. ""Defense against the Dark Arts: An overview of adversarial example security research and future research directions."" https://www.iangoodfellow.com/, 2018, https://www.iangoodfellow.com/slides/2018-05-24-DLS.pdf. Accessed 2020. <sep> [3] Anil, Cem, James Lucas, and Roger Grosse. ""Sorting out lipschitz function approximation."" International Conference on Machine Learning. 2019. <sep> [4]Wong, Eric, and Zico Kolter. ""Provable defenses against adversarial examples via the convex outer adversarial polytope."" International Conference on Machine Learning. PMLR, 2018. <sep> [5]Gowal, Sven, et al. ""On the effectiveness of interval bound propagation for training verifiably robust models."" arXiv preprint arXiv:1810.12715 (2018). <sep> [6]Zhang, Huan, et al. ""Towards Stable and Efficient Training of Verifiably Robust Neural Networks."" International Conference on Learning Representations. 2019.","In this paper, the authors propose a theoretically principled neural network that inherently resists ℓ∞ perturbations without the help of adversarial training. Although the authors insist to focus on the novel design with comprehensive theoretical supports, the reviewers still concern the insufficient empirical evaluations despite the novel idea and theoretical analysis."
"rating_summary | decision  ==>  ==> Summary: <sep> This paper introduces a new approach to learn a multi-class image classification model by fixing the weights of the classification layer. The authors propose to draw the class vectors randomly and set them as fixed during training instead of training them. They analyze this approach when a model is trained with a categorical cross-entropy and or softmax-cosine loss. The proposed approach is tested on 4 datasets: STL, CIFAR-10, CIFAR-100, TinyImagenet <sep> Reasons for score: <sep> I do not think the technical contribution is strong enough for *CONF*. The idea is interesting but the empirical validation of the idea should be improved and some claims should be proved. <sep> Pros: <sep> The idea of using fixed-representation is interesting. It can help to reduce the training time. <sep> The authors explain why cosine-similarity maximization models cannot converge to 0. <sep> Cons: <sep> The title looks very interesting: ""Redesigning the Classification Layer by Randomizing the Class Representation Vectors"". But after reading the paper, it is only about mullti-class image classification. There is no study about other types of data or the multi-label setting. The authors should use a title more accurate about the content of their paper. <sep> Overall, the structure of the paper should be improved. It is quite difficult to read because several sections are a mix of model contributions and experimental results. Maybe using subsections can help to separate the model contributions and experimental results. Also, some information is not at the right place and some sections should be reorganized. For example, the datasets and models are presented in section 4.1 but some results are presented in section 2. The authors should also add a related work section to clearly state the motivations and explain the difference with other approaches. <sep> The authors proposed to randomly initialize the weights of the classification layer but they do not clearly explain how the weights are initialized. There are several standard approaches to initialize weights like uniform, normal, Xavier uniform, Xavier normal, Kaiming uniform, Kaiming normal. It can improve the paper if the authors compare these initialization mechanisms. Similarly, the authors should analyze the results for several runs to see how the fixed weights approach is sensitive to the random initialization. <sep> I have a conceptual problem with fixing the bias. The bias is sampled so it means it can have a large or small value. Let's take an example with 2 classes. The class A can have a large bias (e.g. 0.5) but other class B can have a small value (e.g. -0.5). It means that the class B has a negative bias and will usually have lower scores than A just because there is a difference of 1 between these biases. I am not sure that it is a good idea and there is no motivation about that in the paper. The authors should analyze the bias initialization because it is important. <sep> It is important to show the variance when the model is evaluated on several runs (section 4). It can help to understand how the model is sensible to the initialization. <sep> It is well known that the SGD is sensible to its hyper-parameter and in particular the learning rate. The model will not converge if the learning rate is too large or too small. The authors should explain how they choose the hyper-parameters. I also wonder how the results are specific to the optimizer. Are the conclusions of the analysis the same for other popular optimizers like Adam. <sep> ""These observations can provide an explanation as to why non-fixed models with S = 1 fail to converge."" (page 6): For me it explains why the model cannot converge to 0 but it does not explain why the model fails to converge. They are two different problems. <sep> In the abstract and in some other parts of the paper the authors claim they improve the compactness of the model. But they never show it. They did not define how they measure the compactness of a model. They should clearly present the definition of compactness, and what approach they used to compute it. Based on my knowledge, measuring the compactness of a model is not easy. <sep> The authors should results on low resolution dataset (less than 100*100). I wonder if the results can be generalized to larger resolution dataset. For example, does it also work on ImageNet that has more images, larger resolution images and more classes (1000). I also wonder if it works on other type of datasets like fine-grained datasets (e.g. CUB-200, Stanford Cars, FGVC Aircraft). Also, how does it adapt to new domains like medical images and natural scenes. <sep> I am not convinced that ignoring the visual similarities between classes is a good idea. I think it is important to build spaces that encode some semantic structure. For example, I think it is important to encode that two bird species are more semantically similar than a bird and a car. <sep> It is not clear why the authors decided to focus on the cosine-similarity maximization models. They should motivate this decision more because these models are not so popular. <sep> The authors claimed that ""the low range of the logits vector is not the cause preventing from cosine-similarity maximization models from converging"" (page 5) but they did not show results to prove it. The authors should analyze the range of the logits. The current analysis does not allow us to understand if it is because of the range of value, or the normalization of the weights or a bad tuning of some hyper-parameters. <sep> Minor comments: <sep> The authors should give more information on how they generated the figures 3 and 5.","The reviewers are in consensus that this paper is not ready for publication: cited concerns include simple (interesting) ideas but need to be carefully analyzed empirically, contextualized (other similar studies exist), identifying convincing empirical evidences,. etc. <sep> The AC recommends Reject."
"weakness | suggestion  ==> 1. Brief summary <sep> The authors study what they call a negative pretraining effect = models pretrained on task 1 and tuned on task 2 sometimes underperform compared to just training on task 2 from scratch. This is an important factor in many forms of life long learning, multi-task learning and curriculum learning. They investigate 3 potential remedies / setups: a) using different learning rates for task 1 and task 2, b) changing from task 1 to task 2 more smoothly, and c) resetting network biases at different stages of the process. The perform with a single ResNet18 architecture on MNIST, Fashion MNIST, SVHN and CIFAR-10. <sep> 2. Positive things <sep> I think the problem is very important and applicable in many problems in ML, especially in its practical deployment <sep> The problem is important and interesting from the theoretical point of view as well <sep> The interventions / experimental setup the authors use (a) LR changes, b) smooth task transition, and c) bias resetting) are all reasonable and potentially actionable if shown effective <sep> I like that the authors report results of multiple random seeds and show the full distribution of the results. This allows the reader to better judge the validity of the claims made and <sep> I like many aspects of your experimental setup <sep> I like the idea with path discretization and studying the effect of the number of discrete steps <sep> 3. Negative things / points of confusion <sep> The experiments are way to limited in scope to establish generally applicable results. I'm not asking you to go all the way to ImageNet, but several things would greatly improve the potential value of your work: <sep> a. Adding datasets with a different number of classes than 10. All the datasets you used have 10 classes (MNIST, Fashion MNIST, SVHN, CIFAR-10) and it is at least plausible that some of the effects you see could be caused by some sort of class specialization. It would be very helpful to see smaller number of classes (you can restrict some of the datasets you have already) and let's say CIFAR-100 on the other end. I think these would not be crazy hard to add but would greatly increase the value of your paper. <sep> b. You only use a single architecture. It would be good to try others. Especially the learning rate considerations and experiments could be influenced by the fact that you chose a ResNet which are often hard to train from initialization at low learning rate (people sometimes preheat them by high LR initial phase of training). You could try a simple multi-layer CNN, and fully-connected multi-layer net. It would be good to see that your conclusions hold generally, not just for ResNets. <sep> I am not extremely convinced by some features of your experiment A where you use different learning rates for the blurred task 1 and the sharp task 2. Firstly, why is the baseline performance a fixed LR = 10e-4? Did you use a hyperparameter search of sorts of find it? Because if it were optimal, the fact that e.g. panel 1 in Figure 3 the first two results perform worse could be just because their LRs are smaller, and not because of the effect you're trying to observe (the negative pretraining). It is at least suspicious that the blurred -- sharp task starts performing equally to the default sharp task precisely when the LR of the blurred -- sharp task is at least as big as the default task. It is certainly a confounding factor that does not allow you to draw the conclusion you do -- namely that it is the LR change that helps, rather than just increasing the LR. I suggest you show multiple baselines with different LRs so that we can compare to them. Also, please show the errorbars for the baseline performance as well, so that we can judge the overlap of the uncertainty intervals. <sep> In Figure 3 there is essentially no signal there for Fashion MNIST and a week signal for SVHN. How does that fit with the causal explanation you are offering. I'm really not seeing how you can draw a conclusion as strong as ""the precise order of learning rates across tasks can be crucial"". I don't think Figure 3 shows that clearly, or possibly at all. Especially considering the potential confounding that I described in point 2. <sep> For experiment A in Figure 3, wouldn't you expect that the ordering of the LRs would matter. In panel 1 [5,5] and [10,5] look the same, why? and [10,20] and [20,20] also look the same. How could both be true if you're expecting that you should either increase/decrease the LR to get good performance. <sep> This is a point that I am not sure about, so please correct my if I'm wrong. I don't get how precisely you do the bias resetting. Do you set all biases to 0, or reinitialize them again from a distribution. Why would doing this at the begging before the blurred task have any effect. And if it does, doesn't it just mean that your original initialization was not good enough and now you made it better. It might not have much to do with the sequential learning but rather with this. Let me know if I'm wrong here. <sep> In Section 7 you say ""negative pretraining is not an optimization issue"". That is possible, but I don't think you show that. The fact that it's a question of generalization and not the training loss going to zero does not mean it's not a question of optimization -- optimization includes the generalization properties of whatever optimum it found, so it definitely does deal with this. This might be an issue of semantics and if so just let me know, this is not a major point. <sep> Figure 6 results don't look very conclusive to me, which probably means that the bias reset doesn't work too well (method C). <sep> 4. Tiny issues and suggestions (I'm not judging the paper based on those): <sep> You use e.g. 1e-1 instead of 1×10−1 and it doesn't look very good (it's also kind of hard to read). It might be worth typesetting those values in a more aesthetic way. <sep> 5. Potentially relevant papers <sep> When you discuss the effect of learning rate on updates, it sounds similar to this paper: <sep> [1] Stiffness: A New Perspective on Generalization in Neural Networks (https://arxiv.org/abs/1901.09491) where they measured how ""stiff"" a NN is when trained with different LRs by comparing the learning effect of one image on another (also between datasts). <sep> For the bias resets the class structure the DNN learns might be crucial. Here's a good overview of such class-specific effects: <sep> [2] Traces of Class/Cross-Class Structure Pervade Deep Learning Spectra (https://arxiv.org/abs/2008.11865) <sep> In the NTK discussion [3] discusses how the NTK behaves when you expand it around a partially pre-trained point in the weight space which sounds relevant to the way you study how the task 2 training depends on where task 1 got you: <sep> [3] Deep learning versus kernel learning: an empirical study of loss landscape geometry and the time evolution of the Neural Tangent Kernel (https://arxiv.org/abs/2010.15110, NeurIPS 2020). It also discussed the importance of the early phases of training that you discuss. <sep> [4] The Break-Even Point on Optimization Trajectories of Deep Neural Networks (https://arxiv.org/abs/2002.09572, *CONF* 2020) also looks at the crucial effect of the early phases of training. <sep> 5. Conclusion <sep> I think the question asked is interesting and the approach the authors took promising. However, the breath of experiments is not large enough and I think there are significant potential confounding effects in at least the setup A (changing the learning rate) that make it really hard / inconclusive to draw strong causal conclusions about the effect of the intervention on the negative pretraining effect. I encourage the authors to improve the paper and resubmit -- this seems like a really promising piece of research, but as it is it's not strong enough.","Taking all reviews and the work in consideration, unfortunately the work does not present the breadth it needs to sustain the claims it makes. In particular, there work requires to analyse more architectures/variations of datasets with different properties and to provide more careful ablation studies that shows the efficiency of the 3 different proposed methods. Potentially removing one of this methods in order to give more space to analyse the others that seem more promising."
"abstract | strength | weakness | rating_summary | suggestion  ==> In this paper, the authors address a new weakly supervised regression problem. In this problem setting, upper-side data (labeled above the regression line) and unlabeled data are provided. To solve this problem, the authors derive a learning algorithm in an unbiased and consistent manner to ordinary regression that is learned from data labeled correctly in both upper- and lower-side cases. Experiments demonstrate the advantages of the proposed algorithm. <sep> Pros: <sep> To the best of my knowledge, this paper is the first to solve the weakly supervised regression problem presented in the paper. I consider that it is the biggest advantage of this paper. <sep> This paper proposes a consistent learning algorithm to solve the above problem. <sep> Experiments demonstrate the effectiveness of the proposed algorithm. <sep> Cons: <sep> The presentation of this paper needs to be improved. For example, I understand that in the introduction section, the authors try to justify that the weakly supervised regression problem (where upper-side and unlabeled data are available) is reasonable and could be encountered in real-world settings. However, I personally feel that the presentation is not very clear and I am not fully convinced. In addition, for the order of Eq. (4) and Eq. (5), I think it would be better to present Eq. (5) before Eq. (4), as Eq. (4) relies on Eq. (5). <sep> For the proposed consistent algorithm, I would admit that it is novel to some degree, while the technical contribution of this algorithm is limited. It is worth noting that the proposed algorithm is adapted from the risk estimator of PU learning (Du Plessis et al., 2014; 2015). I think the only key contribution lies in Eq. (6), e.g., the authors show that instead of setting the value of y~lo, we can find the gradient only depends on the sign of f(x)−y~lo. <sep> For the experiments, it seems that the authors do not use a ground-truth regression line to separate the given data, and obtain upper-side and unlabeled data. Instead, they corrupt some selected data by setting their value to the minimum regression value. I feel that this practical operation does not accord with the proposed problem setting. Maybe we could use originally labeled data to obtain a well-trained regression line and then obtain the required upper-side and unlabeled data. <sep> In summary, this paper proposed a novel problem setting and a novel learning algorithm, while the problem setting is not well justified and the technical contribution of the algorithm is limited.","The paper addresses regression in a weakly supervised setting where the correct labels are only available for examples whose prediction lie above some threshold. The paper proposes a method using a gradient that is unbiased and consistent. <sep> Pros: <sep> Problem setting is new and this paper is one of the first works exploring it. <sep> The procedure comes with some unbiasedness and consistency guarantees. <sep> Experimental results on a wide variety of datasets and domains. <sep> Cons: <sep> Novelty and technical contribution is limited. <sep> Motivation of the problem setting was found to be unclear. <sep> Some gaps in the experimental section (i.e. needing the use of synthetic data or synthetic modifications of the real data). <sep> Overall, the reviewers felt that as presented, the paper did not convincingly motivate the proposed upper one-sided regression problem as important or relevant in practice, which was a key reason for rejection. The paper may contain some nice ideas and I recommend taking the reviewer feedback to improve the presentation."
"rating_summary | strength | weakness | rebuttal_process | decision  ==> This paper proposes an approach based on Fourier transforms to predict ratings in collaborative filtering problems. The paper's scope (""smooth reconstruction functions"") gets immediately narrowed down to Fourier transforms--it would be nice to provide some motivation for this choice over alternative smooth functions. The paper then clusters the users as a way to reduce the number of parameters in the model, given that the Fourier transform itself does not reduce it. As a further step, the clustering is replaced by a soft-clustering learned by a neural network. In the experiments, the RMSE of the rating prediction problem is worse than some baselines and better than others. <sep> Besides these technical steps, from a more big-picture perspective, I am not sure if the problem of rating prediction as cast in this paper, misses a key point. The key point I am concerned about is that the observed ratings are missing not at random [a]. For this reason, the collaborative-filtering literature abandoned the minimization of RMSE on the OBSERVED ratings ten years ago. Two different avenues have been pursued since then: most of the papers switched to ranking the entire catalog of items, e.g, see [b] to get started. A few papers continued with rating prediction, but stated the problem correctly by taking into account the fact that the ratings are missing not at random, eg., [c,d]. <sep> In this submission, the problem statement at the top of page 4, and Eq. 6, was not clear to me: while s was defined clearly in the Fourier transform earlier in the paper,  I did not find a definition of s_u in Eq 6 in the context of rating prediction, i.e., is this the vector of ratings of user u? Only the observed ratings? How are the unobserved/missing ratings of user u treated in the proposed approach? <sep> Given the RMSE-values in the experiments, my best guess is that the model was trained on the observed ratings only, ignoring the key problem that the ratings are missing not at random. <sep> I feel like a rating-prediction paper that ignores the key problem of collaborative filtering, i.e., the fact that ratings are missing not at random cannot be accepted (and should actually be desk-rejected). <sep> I encourage the authors to modify the approach to account for this key problem of collaborative filtering. Alternatively, this approach may be useful for different applications, like compressive sensing problems where the observations are truly random. <sep> [a] Collaborative Prediction and Ranking with Non-Random Missing Data by B. Marlin and R. Zemel (RecSys 2009 Best Paper) <sep> [b] Training and testing of recommender systems on data missing not at random by H. Steck (KDD 2010) <sep> [c] Probabilistic Matrix Factorization with Non-random Missing Data by J.M. Hernández-Lobato, N. Houlsby, and Z. Ghahramani (ICML 2014) <sep> [d] Modeling User Exposure in Recommendation by D. Liang et al. (WebConf 2016)","This paper mostly received negative scores. A few reviewers pointed out that the idea of modeling user preference in the frequency domain seems novel and interesting. However, there are a few concerns around the clarity of the paper, the motivation of the proposed approach, as well as the experimental results being unconvincing (both in terms of execution as well as exploration of the results). The authors did not provide a response. Therefore, I recommend reject."
"abstract | weakness | suggestion | misc  ==> Summary <sep> The authors analyze a self-supervised learning framework for downstream (supervised) few-shot classification. The self-supervised stage is a simplified version of MoCo (He et al. 2019) and relies on class-invariant augmentation of unlabeled data to produce samples for a contrastive loss. This produces two encoder networks that are used in the subsequent few-shot learning stage via a distance-based classification scheme similar to that used by Snell et al. (2017), [1], [2], and Chen et al. (2019). <sep> The authors show that the method minimizes an upper bound on an oracle supervised distance-based classification loss. They then further analyze the looseness by decomposing the self-supervised loss into contributions from false-negative and true-negative samples. They relate these quantities to key methodological considerations, such as the level of diversity in the meta-training/base data and the number of negative samples to use during contrastive learning. <sep> The authors assess this method on the Omniglot and miniImageNet few-shot datasets, following the setup proposed by Hsu et al. (2019) in which the meta-training (aka base) split is treated as unlabeled. The results are strong, though are curiously relegated entirely to the Appendix. <sep> Strengths <sep> The overall pipeline is to my knowledge novel, even though the authors are careful to state that the method is not a core contribution as it draws heavily from prior methods. Unlike previous works that consider unsupervised/self-supervised pre-training for few-shot learning, this work provides some theoretical justification for its method. <sep> Due to the judicious choice of considering contrastive learning and distance-based classification, the resulting analysis is relatively straightforward. <sep> Weaknesses <sep> This submission is overall poorly written. It was very difficult to parse due to a copious number of grammatical errors. In numerous instances, I can't quite discern what the authors mean. Aside from this, there are many vague statements unsupported by reference or argument. <sep> The organization leaves much to be desired. For example, results of an ablation take center stage in the main text, while key experimental exposition and benchmark results are left entirely to the Appendix. <sep> Comparison to CACTUs (Hsu et al., 2019) is not entirely fair as the method (like most modern contrastive learning methods) requires the specification of instance transformations that are class-invariant for test tasks. This should be noted. (Though comparison to UMTRA (Khodadadeh et al., 2019) is fair.) <sep> Recommendation <sep> I currently recommend rejection (3), as the submission's poor writing severely hampers clarity and thus prevents it from meeting publication standards. If the writing were fixed, I would probably rate it around a 6. <sep> References <sep> [1] Qi et al., Low-Shot Learning with Imprinted Weights, CVPR 2018 <sep> [2] Gidaris et al., Dynamic Few-Shot Visual Learning without Forgetting, CVPR 2018","This paper proposed to theoretically explain why a pre-trained embedding network with self-supervised training (SSL) can provide representation for downstream few-shot learning (FSL) tasks. The review process finds that the paper may over-claim the results and that the results seem unsatisfactory. Both Reviewer 4 and Reviewer 5 expressed concerns regarding the writing, organizing, and grammar errors of this paper. The paper needs a substantial revision to improve clarity and accessibility. As pointed out by Nikunj Saunshi's public comment, this paper may benefit from discussing the differences from the previous works, including [1]. <sep> [1] Arora et al., A Theoretical Analysis of Contrastive Unsupervised Representation Learning, ICML 2019"
"misc | weakness | strength | decision | suggestion | weakness | suggestion | decision  ==> This paper adapts the recently introduced Neural Shuffle-Exchange (NSE) network for 2D inputs. This is done by introducing two changes; flattening 2D input using a z-order curve, and using 4-to-4 switches rather than 2-to-2. Experiments on synthetic data show that the proposed model can outperform baselines. <sep> Strengths <sep> The extension of NSE to 2D domains is an interesting research direction. <sep> The use of the Z-order curve to map from 2D to 1D such that locality is approximately preserved is interesting. <sep> The model performs well for the tasks explored here. <sep> Weaknesses <sep> The overall contributions are limited. While flattening 2D data using a z-order curve is interesting, it is a well known technique for spatial indexing. The use of z-order curves is also not entirely unprecedented in the field. For example, [1, 2] also use z-order curves in the context of 2D data, and [3] in the context of 3D data. <sep> The paper states that naive flattening of 2D data is expensive since it results in long sequences that are very slow to process with a NSE. The paper then proposes a technique which flattens 2D into an equally long sequence. This is very confusing. I'm assuming that reported speed improvements are due to the use of 4-to-4 rather than 2-to-2 switch, is this correct? Wouldn't this technique also be applicable to the original NSE? <sep> The experiments are limited, consisting exclusively of synthetic data and a single baseline for each task. While an experiment involving images (CIFAR-10) is presented in the Appendix, the description is brief, and the model performs no better than a feed-forward network. This is concerning as CV seems like a natural domain for exploration, especially given overlapping motivation  between this work and recent literature exploring transformers in the vision domain [4, 5]. As such, while the results are promising, they are not enough to convince me of that Matrix-SE is likely to be useful for practical  problems of interest. <sep> The presentation of technical and experimental details could be improved. <sep> I'm confused about how weights are shared. In particular, while the figure make it clear that weights are shared among Shuffle layers in the same block (similarly for Inverse Shuffle layers), I'm unsure if they are shared between units, although I suspect they are not. <sep> The original NSE paper pads ""the input sequence to that length by placing the sequence at a random position and adding zeros on both ends."" Is a similar technique used in this work? If not, and weights are not shared between units (Point 4.1), I'm confused about how the model can generalize to larger matrices. <sep> The paper does not contain information about how hyper-parameters were tuned, or information about stability of results. How robust is the model to different hyper-parameter settings? Are reported metrics averaged over multiple runs? <sep> It should be clarified in the paper that Z-Order curves are only approximately locality preserving and contain large discontinuous jumps. <sep> In section 5.2, the paper states that ""Graph Isomorphism Network struggles to solve these tasks."" However, on the hardest task, GIN outperforms Matrix-SE on triangle-finding on the largest graphs. <sep> Recommendation <sep> I recommend rejection. While I believe the extension of NSE to 2D domains is a worthwhile pursuit, I do not believe this paper represents significant progress is this regard. <sep> Minor Issues <sep> Section 5.1: ""All tasks, except matrix squaring, has simple O(n2) algorithms."" => ""All tasks, except matrix squaring, have simple O(n2) algorithms."" <sep> Section 5.2: ""a common practise"" =>  ""a common practice"" <sep> Inconsistent spacing is used around parenthesis. <sep> References <sep> Zhang, Jianjin, et al. ""Z-order recurrent neural networks for video prediction."" 2019 IEEE International Conference on Multimedia and Expo (ICME). IEEE, 2019. <sep> Kumar Jayaraman, Pradeep, et al. ""Quadtree convolutional neural networks."" Proceedings of the European Conference on Computer Vision (ECCV). 2018. <sep> Corcoran, Thomas, et al. ""A spatial mapping algorithm with applications in deep learning-based structure classification."" arXiv preprint arXiv:1802.02532 (2018). <sep> Parmar, Niki, et al. ""Image transformer."" arXiv preprint arXiv:1802.05751 (2018). <sep> Child, Rewon, et al. ""Generating long sequences with sparse transformers."" arXiv preprint arXiv:1904.10509 (2019).","The reviewers are split. Two reviewers consider the technical contribution of the paper to be insufficient, and raise concerns about comparisons with Transformers or using more standard benchmarks for GNN experiments. The other considers the experiments convincing and the method worth publishing. My own view is that this work is not ready for inclusion in the conference. In particular, I think this paper would be much stronger with either: <sep> 1: a more practical task to illustrate where this method might be applied in earnest, <sep> 2: more analysis and baselines on the synthetic data. Synthetic data can be enough for a new method if it illuminates the functioning and the benefits and drawbacks. In this paper, we have synthetic data with little analysis, and imo (concurring with R5) insufficient baselines. For example, while a vanilla Transformer probably could not do the matrix problems (with the matrices encoded naively), one might expect Transformers with sparse attention to do quite well on e.g. transpose and 90 degree rotation, especially given the training curriculum and proper positional embeddings; a convolutional network seems like a strawman. I also agree with R5 that standard benchmarks for GNN exist, and these might be appropriate (or at least there should be some discussion of why they are not). <sep> 3: some theoretical discussion of what the proposed model can do that other methods fundamentally cannot. <sep> I do think this is interesting work, and encourage the authors to revise and resubmit."
"abstract | weakness  ==>  ==> In this paper, an extension to deep kernel learning is proposed. A linear kernel is used as the base kernel, which enables exact optimization of the kernel hyperparameters. There is a universal approximator theorem stating that the deep neural network with linear kernel could approximate any kernel function, which is quite obvious from the perspective of random Fourier features as well. Also, multiple neural networks are used to produce features and the features are concatenated.  I am not sure why the word ensemble is used, but it is really a concatenation of features instead of ensembling predictions. Besides the exact inference, standard variational inference is also proposed for the linear base kernel. Experiments are conducted on synthetic data and UCI datasets, with comparison to DKL with linear kernel and deep ensembles. <sep> In general, I could not find very interesting contributions from the paper. The framework follows closely from DKL with a linear kernel. But I have a question about the proof of the universal approximator theorem, to approximate the different eigenfunctions, I would assume the hidden layer of the neural network to be at least of O(B) where B is the number of eigenfunctions to be approximated. So potentially, the neural network needs to be very wide which might not be practical. Also, I am confused why the ""concatenation"" of features is referred to as ""ensemble"" in the paper. It is super confusing to me unless I am not understanding how the features are used jointly. Also, the authors mention that ""a learner with M output is simply a concatenation of M single-output learners, suggesting that multi-output learner may help to further reduce the number of H of required learners."" With the same number of nodes in the hidden layers, I don't think a multi-output learner is equivalent to M different single-output learners, therefore the argument made here might not be valid. <sep> In terms of experiments, it is weird that for DKL linear kernel is used instead of the original spectral mixture kernel with random Fourier features. It makes the most sense to compare the linear kernel with something like RBF or spectral mixture kernel. Also, UCI regression tasks seem to be rather easy and do not necessarily need a deep neural network to do feature engineering. It would be necessary to conduct experiments on more complicated tasks such as images to validate the effectiveness of the proposed linear kernel approach. <sep> ------------After author's response---------------- <sep> My major concern is about the connection between the universal approximation theorem and the proposed architecture. In the paper, the authors mentioned that "" the following universal approximation theorem for DKL implies that this effect can be compensated by adding parallel learners"". However, the fact that a multi-output learner is not equivalent to M different single-output learners makes it hard to justify the proposed architecture theoretically from the universal approximation theorem. <sep> I think this is something that is crucial to be justified, otherwise, the theory does not really match with the proposed method.","The paper presents a DKL variant with a linear kernel. Representations from several networks is combined through concatenation, making it not quite an ensemble. It's shown that the model is a universal kernel approximator. Experiments are conducted on a large number of UCI datasets. <sep> Following the discussions, the paper still has the following shortcomings: <sep> some lack of clarity in the presentation (for instance, explaining the equivalence between a multi-output learner and M different single-output learners) <sep> lack of experiments on data where deep learning is typically used (images); the UCI datasets have structured data and other ensembles like XGBoost may outperform the baselines presented in this paper <sep> difference in performance between DKL and DEKL, especially since DKL benefits from a larger model space, theoretically. maybe DEKL has better sample complexity, but does this advantage hold in the case of the large datasets that deep learning is used for?"
"abstract | strength | weakness | misc  ==> Overview: <sep> The paper proposes a novel loss function using Sobolev norms to decrease the computational costs when solving PDEs using neural networks. I think the idea of the work is very interesting and relevant and in a useful direction, however I think the paper in its current form is not yet suitable for publication and it should be strengthened by incorporating more theoretical and numerical aspects. This will make the concept a lot more convincing. I present some ideas and comments for improvement below. <sep> Comments and ideas for improvement as well as clarification questions: <sep> You state ""it requires relatively high computational cost compared to traditional mesh-based schemes in general"". Would you have a reference for this? In part I agree with the notion that neural network training could be computationally-heavy, but on the other hand, as you also mention, it should not suffer from the curse of dimensionality compared to mesh-based methods which would seem that it is computationally efficient? <sep> The main claim of this work is to introduce Sobolev training to speed up convergence, or as you mention in the introduction ""overcoming the issue of high computational cost when solving PDEs using neural networks"". Theoretically the results in Section 4 are not showing this. I know that in the original Sobolev training paper there is a result on how Sobolev training has a lower sample complexity than regular training. Extending such a result to this setting would be necessary to make the claims in the introduction rigorous. <sep> The results in Thm 4.1 and 4.2 are only for 1D equations. I understand that higher order could be more complex, and perhaps the 1D equations are sufficient to convey the intuition, however in that case at least a comment is needed on how these results could be extended to higher orders. <sep> In Figure 1, what is the reason for H2 loss not speeding up convergence with the ReLU? Is it the differentiability? <sep> The results in 5.2 are again only for 1D equations. I think that if theoretically you do not prove the results for high-dimensional PDEs, the value of the proposed methodology for high-dimensional PDEs should at least be shown in extensive numerical experiments. I do think the example in 5.4 is in the right direction, but a more rigorous analysis would be needed. <sep> I would like to have more information on how the ""true"" (PDE) values of the gradients of the boundary and interior differential operators are computed, and whether this is always possible. <sep> My last comment is a general one, but given that the research in this area is growing rapidly with various approaches to improve convergence, a stronger literature review would be necessary. I give two examples of papers which could be of interest below. <sep> Some references which may be of interest: <sep> Ito, Kazufumi, Christoph Reisinger, and Yufei Zhang. ""A Neural Network-Based Policy Iteration Algorithm with Global H^ 2 H 2-Superlinear Convergence for Stochastic Games on Domains."" Specifically, Remark 4.2 could be of interest. The authors also discuss how certain norms cannot guarantee convergence of the derivatives of the numerical solutions. <sep> van der Meer, Remco, Cornelis Oosterlee, and Anastasia Borovykh. ""Optimally weighted loss functions for solving PDEs with Neural Networks."". The authors discuss the choice of loss functions to also speed up / improve convergence and the solution accuracy.","The paper solves a PDE using an additional penalty function between the derivatives of the function. On toy examples and two PDEs it is shown that these additional terms help. <sep> Pros: - The motivation is to include derivatives in the computationa <sep> - Implementation and testing on several examples, including high-dimensional ones <sep> - Timing is included in the latest version <sep> Cons: -The loss is Sobolev norm of the residuals of the equation. <sep> - The usage of the norm of the residual is not 100% consistent with the smoothness properties of the corresponding equation. For example, for the Poisson equation, the problem is selected in such a way the solution is analytic. However, for example, if the zero boundary conditions are enforced, and right hand side is all ones, the solution will have singularities. Thus, the main challenge would be the case when solution does have the singularities (and it will have it in many practical cases). The L2-norm then is not the right functional for the solution to exist, not to say about the higher-order derivatives. So, these functionals are not motivated by the theory of the solution of PDEs, but are rather focused on much smoother solution. <sep> - Convergence. There are quite a few papers on the convergence of DNN approximations to solution of PDEs. The presented methods might have converged to a local minimum. An important reference is the paper by Yarotsky D. Error bounds for approximations with deep ReLU networks. Neural Networks. 2017 Oct 1;94:103-14."
"strength | weakness | decision  ==> Summary <sep> The paper proposes to re-interpret federated averaging (FedAvg) as a version of the expectation-maximization (EM) algorithm under a particular probabilistic model. Further, the authors propose to use spike-and-slab sparsity inducing priors over the local model parameters to sparsify the learned models (the corresponding method is called FedSparse), which naturally reduces the communication cost (only non-zero parameters need to be sent over the network). Improvement in communication efficiency is showcased on a few standard federated datasets. <sep> General comments and evaluation <sep> In my opinion, the paper has some interesting ideas; the effort to connect federated learning with techniques from probabilistic inference strongly resonates with me. However, the connection the authors make between federated averaging and EM is not as insightful as I anticipated. The fact that FedAvg can be seen as is a special case of EM with a particular choice of a prior distribution does not add much to our understanding of the behavior of the algorithm (e.g., convergence, issues related to different types of heterogeneity, etc.). While FedSparse is derived using Bayesian considerations, I would argue that the same algorithm can be derived using FedAvg + simple L1 regularization. Again, I do not see the benefit of the Bayesian formalism, especially given that it is essentially thrown away at the end by using the ""hard"" version of EM (where all the Gaussian distributions are collapsed). This unnecessarily complicates things without offering much insight. <sep> Apart from the formalism, the authors should clarify the modeling assumptions of the FL setting they consider. Currently, there are two canonical FL settings, known as ""cross-device"" and ""cross-silo"" (https://arxiv.org/abs/1912.04977), which require FL methods to operate under drastically different assumptions -- the former assumes that the data is distributed among a very large number of devices (tens of millions) with limited memory and compute; the latter assumes a small number of clients with sufficient computational resources. It is unclear which of the settings the authors aim to address. For instance, arg⁡maxw1:S in Eq. 13 seems meaningless in the cross-device setting since in practice S would be on the order of millions and only a subset of clients will participate in each round. I'm curious to hear which setting the authors had in mind. <sep> All in all, I think the paper would significantly benefit from further clarification, a possibly theoretical analysis of the algorithm in heterogeneous settings (e.g., understand convergence rate and bounds on the tradeoffs between sparsity and performance) and/or a more careful and convincing empirical study. <sep> Detailed comments and questions <sep> While reading the paper, a high-level question that wasn't clear to me: why use specifically ""hard"" EM for FedSparse instead of the standard EM? <sep> In the paragraph after Eq. 13, the authors propose a bunch of approximations, starting with collapsing Gaussian distributions, then removing the entropy term from the objective, setting hyperparameters, etc. This is a lot of approximations, I would like to know the purpose and the effect of each on the final result. Currently, many of these choices are obscure and unclear. <sep> The authors propose to use sampling of the binary variables z to compute gradients in Eqs. 18-19. The approach is indeed unbiased, but I'm worried that the increase in variance might be substantial in some practical cases. Can the authors somehow quantify this? (Ideally, provide a convergence rate for the algorithm that shows the effect of additional variance). <sep> What is the effect of structured pruning proposed to reduce server-to-client communication cost? It seems like a heuristic and it is unclear how it affects the algorithm. <sep> Experiments <sep> Could the authors define local vs. global accuracy exactly? <sep> How many clients participated in each round in each benchmark? Was the server communicating with all clients or just a subset? <sep> The authors show some improvement of FedSparse over FedAvg and FedDrop in terms of the total amount of communication traffic in GB. Why does that metric matter? In which use-case? This goes back to my question about modeling assumptions. <sep> How many epochs per round did each client run? I would be curious to see how FedSparse compares to FedAvg if we double the amount of local computation (eg, make clients train for 2 epochs at each round instead of 1). I imagine that would improve convergence and reduce # of rounds, and as a result, reduce the total communication cost. <sep> I think that the benchmark datasets considered in the study may not be the best to showcase the benefit of sparsity. I would be curious to see how FedSparse compares to FedAvg on StackOverflow dataset introduced by Reddi et al. (2020), where the sparsity benefits can be much more visible. Eg, learning extremely sparse models could enable the use of much larger vocabularies, and as a result, produce more accurate models than what is achievable with FedAvg. That would be much more compelling evidence in favor of FedSparse than the results reported in the paper.","The reviewers agree that the EM perspective of Federated Learning is novel and interesting. However, a common criticism is that the connection made is rather shallow and not sufficiently developed. There look to be quite interesting potentials of the proposed framework and the specific FedSparse method, but I agree with the reviewers that both aspects need further development before they are in publishable form."
"weakness | rating_summary  ==> This paper presents a method for feature attribution for fairness of the classifier.  They also demonstrate a feature augmentation technique to mitigate unfairness.  They connect their attribution method to to the augmentation technique and demonstrate that their method can attribute the necessary changes to achieve fairness. They evaluate their approach on a few tabular data sets. <sep> Introduction + Methods <sep> I'm slightly confused about equation (2), is the correct interpretation that y is the label we're explaining? So, say, if we're explaining the -1 class and f(x) is 0.25, then f_y(x) yields 0.75? <sep> I think some clarity on the notation would be useful in general.  Is y the ground truth label from the data? That seems to be described this way in section 1.  However, in equation (2) is seems to refer to the label we're selecting to explain, which feels slightly different.  I think some of this notation could be cleared up a bit. <sep> ""Splicing disjoint sets of features"" is this common terminology? I think what this is trying to say is that this step is where we substitute values from the data into a point to create a perturbation.  Let me know if I am correct. <sep> For the description for equation (5): ""…the expected accuracy for a model which samples a predicted label according to the predicted probability"" what does this mean? The notation in equation (5) looks like it's the expected prediction of the model, where is the relation to accuracy here? I'm not quite seeing this. <sep> I think that equation (5) could be clarified in general.  It would be good to explicitly state this significance of this property.  (It's stated clearly for equation 9, and stating it explicitly here would help readers) <sep> Is g a value function? If so, it'd be useful to state this leading up to the equation. Also, we've introduced the term ""a"" at this point which refers to the protected attribute (going back to the introduction).  What values can a take on? Are we assuming $a \\in { 0, 1}? It would be good to clarify this. Last what is p(a) meant to refer to? Is this the overall proportion of individuals with a certain protected attribute? If so, why do we need this term? <sep> ""The linearity axiom of the Shapley values guarantees that the fairness Shapley values of a linear ensemble of models are the corresponding linear combination of Shapley values of the underlying models"" — it would be good to clarify this sentence as it's slightly confusing right now. <sep> For equation (10) assuming that f Is a linear model, is δθ meant to be a vector of values added to each of it's coefficients? <sep> Is is slightly contradictory that in equation (10) δθ is a perturbation (and can be explicitly added to f) but then becomes a function in equation (11)? I get what is being said but perhaps the notation could be improved here. <sep> Experiments <sep> Figure 1 is hard to read, can you make the labels bigger for each graph. Also, it might be good to place the y axis values on each graph so that its easier to see the scale.  I see what the graph is saying, but this would make it much easier to figure out. <sep> I think figure 1 is a nice visualization overall and gets the point across well. <sep> I'm not fully understanding the point being made by section 3.2.. Is the idea that we know this suppressed model doesn't rely on sex at all, so the fairness Shapley values shouldn't say it does? <sep> General Questions + Comments: <sep> I liked the ideas presented by this paper overall.  The idea of using Shapley values to provide feature attributions for fairness is interesting — particularly the applications for explaining the differences between two models, one of which has been corrected for fairness. <sep> I do feel however that the paper can be improved in a number of places in order to strengthen the work as mentioned in my comments.  It would be very useful if the authors provided answers to some of my questions and took some of the comments into consideration for a revision. Particularly, I'm somewhat confused about the contribution of section 3.2 and would appreciate clarification. <sep> One related question is that the authors demonstrate their method on the suppression techniques from Dimanov et al.  Would their techniques help at all with the related methods (more so phrased as attacks) from https://arxiv.org/abs/1911.02508? <sep> Overall, my sentiments right now are borderline, leading towards reject. There's a number of points which could warrant further clarification right now in the paper.","All the reviewers found interesting the use of Shapley values to provide feature attributions for fairness, however, the reviewers brought up a number of issues, particularly in terms of presentation and clarity. While the authors' responses did clarify some of these concerns, this was not enough for the reviewers to broadly support acceptance."
"abstract | rebuttal_process | decision | strength | suggestion  ==> The authors examine the commonly used paradigm of not discounting in the policy gradient objective. They propose two hypotheses relating to discounting. (1) discounting the critic improves representation learning. (2) undiscounted policy gradient is similar to discounting + an auxiliary loss. These hypotheses are studied through a series of empirical tests in the MuJoCo domain with PPO. <sep> Strengths: <sep> I believe this paper is asking the right type of questions about common setups. There are a lot of choices made in deep RL algorithms which don't align with theory and are otherwise unstudied and empirical studies are an important. <sep> Some of the approaches used to answer these questions are quite unique. <sep> Overall, there a lot of experiments both in the paper and the appendix, which is detailed. This is a paper which will benefit from the additional page of content as a lot of key figures can be shifted to the main body. <sep> Weaknesses: <sep> Given the empirical nature of this study, it is really important to have robust experimentation to really answer the hypotheses the paper raises. I think the paper falls short at this aspect and I wasn't convinced by the arguments made for either hypothesis. Furthermore, the conclusions that could be drawn from the results are generally not that surprising. <sep> I'm not sure PPO is the best algorithm to analyze many of these questions. For example, Engstrom et al., 2019 showed a lot of very minor implementation level details had a large impact on the performance. Consequently, it may be difficult to disentangle the actual causative factors in performance. This is problematic as many of the claims in the paper are supported by empirical tests where the performance is not strikingly different. For example, Figure 1 is meant to justify that for γc=0.99 additional transitions improved performance, but on several environments increasing N to 2 or 4 seems to hurt performance, going against our intuition about variance reduction. Figure 2 shows that for N≠0 there is a large performance drop, but all values of N≠0 achieve a very similar performance rather than trending downwards as N increases. To me this suggests a very brittle algorithm. <sep> For section 3 the bias-variance trade-off is evident from prior work (as referenced by the authors) so the result is of course not novel. I think analyzing it in a deep RL setting is important but because of the problems mentioned prior, I didn't find that these results provide anything solid to add to our understanding. <sep> The results for Figure 3 aren't convincing (1) because they are overfit, by selecting the best possible H for each it seems likely to always arrive at a high performing agent. (2) This more suggests that these environments don't require the full horizon to achieve a high performance. Consider a simple cartpole problem which is optimal using greedy actions but has a horizon of 1 million time steps. Since were in an approximate setting with deep networks, it isn't surprising that the agent can achieve a high performance without considering the full horizon. <sep> The results from the toy MRP experiment and distributional RL do suggest some kind of connection to representation learning, but isn't considering a longer horizon simply a more difficult learning problem? Is the representation necessarily an important aspect here? I didn't find that the authors answered this question. <sep> The conclusion from Section 4 is that γA=1 is an inductive bias that all transitions are equally important seems entirely self-evident from the mathematical definition given it applies equal weight to all transitions. At the same time the main question of hypothesis 2 seems unanswered. Shouldn't AuxPPO ≈ PPO, rather than DisPPO if this was true? <sep> A single environment for Figure 9 is not enough to draw any meaningful conclusions. I did not find the discussion in B.1. convincing that the other environments were not suitable. Simply change t0 for the other environments. From personal experience the horizon of Ant is generally large (near 1000) as the terminal condition is hard to achieve meaning the difference between Ant and the fixed length environments should be small. <sep> Additional Comments: <sep> I do wonder if this paper is better off as two separate documents where each hypothesis is provided much more significant attention/experimentation. For example, hypothesis 1 isn't actor-critic specific and is also applicable to Q-learning based methods. These experiments could be simplified by looking at algorithms with significantly fewer components and more settings. <sep> For the PPO-TD-Ex experiment I think it's also worth considering extrapolation error (Fujimoto et al., 2019) in TD learning. Since St+1i is sampled from a single transition rather than a full trajectory it is not necessarily contained in the batch. As a result, v^ is not trained on St+1i and produces an erroneous TD target. My first impression was that the performance drop for γc=1 was not surprising but the performance gain from N=1 for γc=0.99 was, and I think are are unanswered questions here. Another important reference is Bengio et al., 2020 which showed TD(0) generalizes worse than TD(λ) and there is clearly a related result here. <sep> Given MuJoCo environments are time-limited to 1000 time steps, 1024 heads for PPO-FHTD seems like a mistake/oversight. <sep> Why does PPO-FHTD with H=1024 produce different results for the different parametrizations? <sep> Is Figure 6 surprising since the value function needs to consider a large space of solutions as the horizon increases? <sep> Given distributional RL provides a large performance gain (which to the best of my knowledge, we are still missing a conclusive reason as to why), I'm not sure PPO-C51 > PPO-TD is a significant result. <sep> It would be clearer if DisPPO was described before mentioning Figure 15. <sep> Figure 15 seems like an important conclusion and should be contained in the main body of the paper. However, the y-axis of Figure 15 also conflicts with the description in the main body so I'm not sure what the correct interpretation is. <sep> I wonder if the result from Figure 9 is reproducible if the flipping was done in a different way. In the MuJoCo environments is the agent is rewarded mainly for velocity and the behavior of the agent in these cases would be enlightening. Does the agent run forward and then attempt to terminate? Can it move backwards? <sep> Conclusion: <sep> I think the authors present a lot of interesting ideas and experimental approaches to answer their underlying questions. However, I felt that the experimentation was not sufficiently robust to justify their conclusions and I cannot recommend acceptance. <sep> References <sep> Engstrom, Logan, et al. ""Implementation Matters in Deep RL: A Case Study on PPO and TRPO."" 2019. <sep> Fujimoto, Scott, et al. ""Off-policy deep reinforcement learning without exploration."" 2019. <sep> Bengio, Emmanuel, et al. ""Interference and Generalization in Temporal Difference Learning."" 2020. <sep> ** Edit (Nov 23): I have slightly increased my score due to the improvements made to the paper (mainly reorganization) & some clarifications made by the authors, but I still don't feel like my main concerns were addressed.","This paper studies the effect of the discount mismatch in actor-critics: the discount used for evaluation (often 1), the discount used for the critic and the discount used for the actor. There's notably a representation learning argument supported by a series of experiments. <sep> The initial reviews pointed out that this paper addresses very relevant research questions, sometimes in a quite original way, with a large set of experiments. However, they also raised concerns about the organization/clarity of the paper, and possible weaknesses about the experimental studies. <sep> The authors provided a rebuttal and a revision, that clarified some points and triggered additional discussions. However, if the revision improved the initial submission, the shared assessment is that the clarity and experiments themselves are still somewhat lacking. As such, the AC cannot recommend accepting this paper. <sep> Yet, this work does have interesting ideas, and the problem considered is of interest for the community and under studied. The authors are strongly encouraged to submit a revised version to a future venue."
"abstract | weakness | rebuttal_process | decision | suggestion  ==> Summary <sep> This paper proposed a new text GAN framework by combining non-autoregressive text generator based on transformer, straight-through gradient approximation, and various regularization techniques such as gradient penalty and dropout. The paper demonstrates the superiority of non-autoregressive generator in the context of text GANs through various experiments including unconditional text generation, latent space manipulation and unsupervised decipherment. <sep> Pros <sep> This work narrows the gap between image GAN and text GAN by leveraging recent advances on non-autoregressive text generators and a straight-through gradient approximation. While these components are well studied in previous works, I think this work presents a neat combination of them in order to solve a well-known problem. <sep> The paper provides rich discussions in training text GAN and comprehensive experiments and ablations to demonstrate the usefulness of an implicit text generator in different contexts. <sep> Concerns & Questions to Answer during rebuttal <sep> The original text GAN papers were mainly motivated to address the exposure bias problem in maximum likelihood estimation for autoregressive generators. In other words, when we use an autoregressive generator to sequentially generate tokens one by one, there is a distribution mismatch between training and test phase. In your case, now that you already have a non-autoregressive text generator, is there any theoretical motivation/insights for using the adversarial training framework? We know that MLE is statistically efficient (achieving Cram´er–Rao Lower Bound) and possesses many good properties, maybe training the non-autoregressive text generator with MLE (e.g., FlowSeq [1] or variational inference) is a better choice? <sep> The non-autoregressive (NA) text generator have been well studied recently so the novelty of this work is more on the integration of NA generator with adversarial training. Thus the main challenge here is how to solve the non-differentiability problem. The paper directly leverages a traditional workaround, the straight through estimator, which is a biased gradient approximation. Is the bias going to be an issue and is there any better strategy? I think the paper need to provide more discussions on this aspect. Overall the method section need to be polished with more details, as I feel this part is currently hard to follow. <sep> In figure 1, z1,…,zL are sampled independently, which are sent to a transformer and later produced the sample. Is the independence between z1,…,zL going to be a problem? When we use transformer to do neural machine translation, the attention mechanism will capture the dependence in the input sentence (z1,…,zL in this context) and then produce the output correspondingly. Hence will the independence in z1,…,zL lead to a less expressive sample distribution in your text generator (although this is not an issue in image GAN)? <sep> The paper also propose to use the Max Gradient Penalty from image GAN domain. The Max Gradient Penalty was introduced under the framework of Wasserstein GAN or Lipschitz GAN framework, which aims to constraint the function space to be Lipschitz smooth. However this work uses the vanilla GAN objective (eq 12 and eq 13), which is not the WGAN or LGAN framework. Thus the regularization may not be theoretically correct. Also why not instead use the WGAN objective which is empirically more stable and theoretically sound? <sep> Experiments: In table 2, all the results for NAGAN use the dropout with a positive ratio. How does NAGAN perform without dropout? Also I wonder if the comparison in the table and figures are fair, since most previous methods.baselines such as MLE or SeqGAN only use a vanilla RNN/LSTM, while NAGAN has a more complicated structure with transformers, and additional regularization such as gradient penalty and dropout. Perhaps we should control at least the number of parameters to be in the same level. <sep> [1] FlowSeq: Non-Autoregressive Conditional Sequence Generation with Generative Flow <sep> Update after rebuttal: <sep> After seeing the author response below, no change to my score.","This paper proposes GAN-training of a non-autoregressive generator for text. To circumvent the usual problems with non-differentiability of text GANs, the authors turn to Gumbel-Softmax parameterisation and straight-through estimation. <sep> There are a number of aspects to this submission and they are not always clearly positioned. I will concentrate on the two aspects that seem most crucial: <sep> The authors position their generator as an implicit generator, but it really isn't. If we take the continuous interpretation of the output distributions: the Gumbel-Softmax transformation does correspond to a tractable density, the Concrete density of Maddison et al, with known parameter. If we take the discrete interpretation of the output distribution: Gumbel-argmax is just an alternative to sampling from a Categorical distribution with known parameter. In either case, the generator maps the noise source to a collection of conditionally independent distributions each of which has a known parameter and analytical density/mass function. The authors do, however, train the architecture using a GAN-type objective as if the generator were implicit. <sep> In the discussion phase the authors added that GAN training overcomes the independence assumptions made by the generator. Whereas that makes intuitive sense, it suddenly changes the emphasis of the contributions, from proposing an implicit generator (presumably powerful for it being implicit) to proposing a way to circumvent the strong independence assumptions of the generator with a mechanism other than more traditional approximate marginalisation of VAEs. In their rebuttal, the authors commented on the use of non-autoregressive VAEs in neural machine translation, and though those observations have indeed been made, they might well be specific to MT. The simplest and more satisfactory response would be to ablate the use of the GAN objective (that is, to train a non-autoregressive VAE, also note that, with the same choice of likelihood, posterior collapse is rather unlikely to happen). <sep> Other problems raised by reviewers were addressed in the rebuttal, and I would like to thank the authors for that. For example, ablating the non-autoregressive generator and comparing to REINFORCE. I believe these improved the submission. <sep> Still, I cannot recommend this version for publication. I would suggest that the authors consider careful ablations of the components they see as precisely important for the results (that currently seems to be the GAN-like objective despite the model not, strictly speaking, requiring it)."
"abstract | rating_summary | ac_disagreement | weakness | rebuttal_process | decision  ==> Summary:  This paper proposes a domain-shift problem using fewer training examples. It suggests representation fusion as the concept of unifying and merging information from different layers of abstraction. <sep> Cross-domain Hebbian Ensemble Few-shot learning (CHEF) is introduced for extracting features using representation fusion.  More importantly, CHEF does not need to backpropagate information through the backbone network.   CHEF  is applied to various cross-domain few-shot tasks and cross-domain real-world applications from drug discovery. <sep> Strong Points: 1-  The paper is well organized and easy to understand. <sep> 2-  The model is evaluated in four benchmark datasets CropDisease, EuroSAT,  ISIC2018, and ChestX.  It also conducted experiments on two large scale datasets (prepared from ImageNet dataset), miniImagenet and tieredImagenet.  The proposed model shows consistent and promising performance in all datasets. <sep> 3- It introduced Hebbian learners for feature fusion that does not require backpropagation of error signals through the entire backbone network.  Only the parameters of the Hebbian learners need adjustment. Therefore it is speedy and versatile. <sep> Weaknesses: 1- The crucial contribution of this work is Hebbian Learner. Therefore it should devote more space for Hebbian Learner. The explanation about Hebbian Learner provided in this paper is not sufficient understanding to propose an approach clearly. I recommend the authors that include more description of Hebbian Learner.  I understand the space limit, but the model's crucial and essential contribution should be included in the main paper. <sep> 2- This paper claims that using ""Hebbian Learner makes CHEF extremely fast,"" but I did not find any supporting experiments in the main paper to prove this statement. It should include results on time comparison. <sep> 3- This paper performed the experiments on a few-shot learning setup for that chosen 5, 20, and 50 examples per class to train the model and observed continuous improvement on model performance. To show the limit of model performance, the experiments should also be performed using all available examples in the datasets. <sep> 4-  authors have selected ResNet-12 as the backbone model; any specific reason for this? I wonder to see the model performance for more deep networks like ResNet-101 as the backbone model. <sep> 5- It would be better to show the individual contribution of the Hebbian Learner. Therefore result should also be included in the ablation analysis section without using Hebbian Learner in the same setting. <sep> 6-   On page#4, the authors have mentioned that ""We combine the NK feature vectors into a matrix Z ∈ R^ NK×D and initialize a weight matrix W ∈ R ^K×D.""  But it is not mentioned about the initialization technique. Random initialization, Xavier initialization, etc. ? <sep> 7- Some essential baseline approaches are missing for comparison, such as  ""Few-Shot Adversarial Domain Adaptation"" by Saeid Motiian et al. NIPS 2017. <sep> Overall: The paper needs to include many things for better clarity. I feel it provides some insufficient information or does not clearly explain the proposed model's crucial contribution. It needs to include experimental results for different settings, as I mentioned in the weaknesses section, to prove the model's efficacy.","The paper deals with cross-domain few-shot learning in the case of large source-target domain shifts. <sep> The paper received mostly below-threshold reviews, with one exception (R3) whose review is addressing more general aspects, but still with some concern, especially in relation to the experimental part (to which authors did not answer). R1's review is not of much help. <sep> Clarity of the presentation and missing details seem to be recurrent issues all over the reviewers, together with remarks concerning the experimental validation, which would have required a deep revision and improvement, in particular regarding the use of more backbones, better ablation (Hebbian learner contribution, unclear initialization), processing times/computational complexity, significant comparative analysis re robust baselines. <sep> The rebuttal clarifies some of the raised remarks but there are still issues, especially regarding Hebbian learning rule and ensemble learning strategies, and about results too, so not all reviewers were convinced to raise their ratings. <sep> Overall, given the above issues, I consider the paper not yet ready for publication in *CONF* 2021."
"abstract | strength | suggestion  ==>  ==> This paper proposes a method for calibrating uncertainty estimates for regression models. It builds off a method proposed by Kuleshov et al (2018). The newly proposed method has the following steps: <sep> Train a conditional density model for a regression outcome y given an input x on training data. The authors use conditional normalizing flows for this task. <sep> For each set of points (x_t, y_t) in a validation set, pass an input x_t to the model in step 1 to learn an induced CDF for the output F_t(). Calculate F_t(y_t), i.e. the induced CDF evaluated at the actual output, and use another flow-based density model to learn the distribution of the CDF values. If the model in step 1 is perfectly calibrated, this density should be uniform, but in practice it seldom is. <sep> For future data points, the composition of densities in steps 1) and 2) provide a new, uncertainty-calibrated density. <sep> The overall problem area is interesting, important, and underexplored. There has been a lot of work on calibration estimates for classification tasks, but there are less methods for regression. Using flows here is a cool idea. Any exact density model can be used for the method above, and normalizing flows are a good solution because they are monotonic by construction. <sep> That being said, after reading the paper, I'm not convinced that the proposed method is a significant improvement over the method by Kuleshov et al. that it's building off of. Kuleshov et al. proposes a similar 2-step process, but instead of explicitly learning a distribution over the induced CDF values, it uses an isotonic regression to calibrate the CDF. Looking at the experimental results, it seems that the isotonic recalibration performs on par with the flow recalibration in terms of test set calibration error. <sep> Put another way, why should someone use flow-based recalibration instead of isotonic recalibration? A possible answer mentioned in the paper is that flow-based recalibration can be used to compute distribution statistics, such as the mean, while isotonic recalibration cannot compute these statistics. (Side question: why can't the isotonic recalibration be used to compute the mean? It seems like isotonic recalibration explicitly transforms an inverse-CDF to another inverse-CDF. Can't distribution statistics be imputed from this transformed inverse-CDF?). <sep> Even though flow recalibration can compute distribution statistics, the MSE never improves after flow recalibration; in some instances, it gets worse. What is the benefit of having distribution statistics? On the one hand, the worsened MSE might be expected behavior. Is uncertainty calibration expected to behave like a regularizer? If so, that should be stated and discussed in the paper. If not, then of course we shouldn't expect improvements in MSE after recalibration, because we're changing the model that had the best training-set performance. This could explain the results we see. In any case, there should be some justification in the paper for a) why computing distribution statistics is important, b) whether we should expect recalibration to behave like a regularizer, c) a discussion about the tradeoff between calibration performance and model error, and d) an illustration of scenarios where distribution statistics are crucial [and e), why the Kuleshov et al method can't be used to calculate test error]. <sep> Additionally, the paper proposes a way to visualize recalibration results. To be honest, I found the CDF performance plot confusing and hard to interpret. How should we interpret the x-axis (are predictions standardized, and if not, what units are they in)? I found the standard qq-plot-like calibration graph a lot more interpretable. What does the new visualization answer? I think the new visualization should be better explained (it also didn't help that the legend in Figure 5 blocked the middle of the graph). <sep> Overall, I think the paper proposes an interesting model, but it doesn't adequately justify when/why the model should be used over the existing method. I think there could definitely be scenarios where it is useful -- I just don't think the paper has adequately and convincingly illustrated them. <sep> Pros: <sep> Interesting and underexplored problem area <sep> Thorough experiments <sep> Normalizing flows are an interesting and new model for this problem <sep> Cons: <sep> Doesn't justify meaningful ways the method is different from existing methods <sep> Visualizations are confusing","The paper proposes to recalibrate predictive models by fitting a <sep> normalizing flow on top of the predictive model on a held out validation <sep> set using side information. At a high level this idea has some potential, <sep> especially in the multivariate setting, but there are several directions for <sep> improvement: <sep> Comparison with a broader set of baselines as suggested by the reviewers <sep> Clarity on why recalibrate with a normalizing flow especially in the 1-d case <sep> Why not any other model with explicit density? Are there other important desiderata? <sep> A motivating experiment that makes the potential value clear"
"rebuttal_process | decision  ==> Paper Summary <sep> The paper extends the variational autoencoder framework with a richer prior distribution to model more complex correlations in the latent variable distribution. They start with a Gaussian mixture distribution as the prior for the latent variables, and add an encoder network to allow richer correlation structure in the latent variables.  Training the prior distribution requires an optimization between the prior distribution and the latent encoded distribution of the training data set. The paper starts with an existing method of optimizing the prior by computing an approximation of the Wasserstein distance between prior and encoded training distribution that uses an average over slices through the prior and encoded training distribution. The paper replaces linear projections used in prior work with a non-linear projection. The paper also employs a structural consistency term which has been used in prior work, however, the paper employs this term differently than prior work by applying it between encoder features and latent variables rather than inputs and latent variables. Since the latent variable space is now a complex and possibly a nonconvex submanifold, points in the latent space R^D  lying between points corresponding to training data points may not actually fall in the training distribution. The paper therefore proposes a method of interpolating between points in the manifold by constructing a graph between points sampled from the manifold and then choosing points lying along lines in the graph. The paper tests the method on three datasets, a synthetic 40 dimensional spiral dataset, the venerable MNIST dataset and a scaled down CELEB A dataset. Plots of the latent space trained on the spiral dataset shows that the latent space can in fact have complex internal structure. <sep> Pros and Cons <sep> Improving the ability of generative models to capture high-dimensional empirical distributions accurately is a key problem in machine learning and central to the representation learning theme of the *CONF* community. <sep> The paper clearly states contributions up front, namely: using an encoder to generate richer priors for the latent variable distribution, non-linear projections for sliced Wasserstein approximation, and a graph based interpolation method.  They also alter the structural consistency term so that it applies to more abstract features instead of inputs. <sep> The paper does a thorough and clear job of covering prior work and technical background such as the Wasserstein metric, perhaps even excessively so. <sep> The particular choice of a sinusoidal non-linear projection, a key contribution according to the paper, is not motivated in the text. On first glance, the sinusoidal term seems like an odd choice for a non-linearity. After looking at the results which include a test on spirals, it is clearer why this might have be chosen, but is a sinusoidal term likely to be helpful for non periodic data? It might be possible to shed some light on this by investigating whether the coefficients in the non-linear term, zeta and gamma, are significantly different from zero after training on MNIST or the CELEB A data set used in the paper. <sep> Figure 3 comparing EPSWAE and SWAE doesn't clearly illustrate the benefits of the EP component. In the print version, both grids of images seem blurry and prone to oddities such as overly large and dark eyes. Even when one blows up the image to 3X size using the digital version the advantage does not jump out. While I recognize that evaluating generative models is hard, the observations do not clearly support the author's hypothesis that the EP component provides an advantage. Maybe they could evaluate Freschet Inception Distance over the whole data set? Use blind human reviewers to choose between EPSWAE and SWAE based on realism and report preference scores? Interestingly, there is one duplicate image in the EPSWAE grid: row 1, col 5 and row 5, col 8 look identical. Seems odd to get identical images: is this because of sampling from a discrete graph structure? The highly similar but not identical images row 1, col 1 and row 1, col 8 are more what I would expect. Maybe the advantage could be made clearer by helping us focus on relevant features. For instance, it might be that EPSWAE is a little bit less likely to generate large black eyes? I can't tell from this small sample, but a grid that focuses on this might make the point. Hair versus background also seems to be a challenge. SWAE image row 3, col 4 seems to have two hair regions for the same face, but, EPSWAE images such as row 4, col 7 also look like they have two distinct hair regions. There are a couple of SWAE images that seem particularly ill formed: row 5, col 5  and maybe row 5, col 8, and row 7, col 4. It might be worth focusing on a few of the worst examples from both EPSWAE and SWAE to show differences in tails rather than the mean? I wonder if you could do a leave-one-out kind of analysis where you check the probability of held out training data points under the prior for EPSWAE vs. SWAE to assess if the prior is capturing the empirical distribution better? You would have to invert the prior network with gradient descent to do this ... but it might work. <sep> It might make sense to compress some of the tutorial material up front to make room for more results demonstrating the efficacy of the model. The MNIST results, fig 7 in appendix D, shows some advantage for EPSWAE: For instance, figure 7b and 7c both have more bloated numbers … especially 8, 3 and 0.  Also figure 7b has one degenerate number in position 1,2 … possibly a 2? <sep> The paper argues that the structured latent space improves generative power. Is there any evidence of structure in the latent space after training on CELEB A? If we plot 2 or 3D projections, or plot projections of structure preservering factorizations such as PCA, do we see structure in the encoding training data points or are they distributed independently? One might also try independence tests between variables in the encoded latent space to see if pairs of variables are being encoded with correlations. This could be compared easily between EPSWAE and SWAE. <sep> Figure 4 in section 5.4 on interpolation shows smooth interpolation between two points A and B in latent space presumably drawn from training data. The interpolations are pretty smooth. Nice! However, the authors claim is that the graph embedding gives better interpolations than linear interpolation between points in the latent space. To make this point, we would also need to see interpolations between points using linear interpolations. The plots on spiral might make this point, but it is not clear. <sep> The ability of VAE's to disentangle the dimensions of an empirical distribution into indpendent latent variables is sometimes seen as a feature, not a bug. For instance, if the training data truly lie along a spiral, isn't this really a 1D latent space and not a 3D space? While I can see the appeal of improving generated distribution realism, some discussion by the paper on the merits of improving encoder and decoder vs. complexifying the latent space would help to motivate this approach. <sep> It isn't clear to me that the structural consistency term is a good idea in general. Ideally we want the latent space to capture something fundamental about the underlying structure of the data and not features of the input. Moving the structural consistency from input to more abstract features addresses this concern somewhat, but aren't the latent values themselves the ultimate goal? <sep> Recommendation <sep> I recommend a rejection of the paper. The hypotheses (that richer priors and geodesic interpolation generate better images on realistic images) are not clearly supported by the experimental results provided. <sep> Questions <sep> For the spiral training, what was the dimensionality of the latent space? Was it 3D? <sep> Figure 4 caption contains statement ""through an intermediate sample corresponding to the midpoint in latent space"". Are you actually literally using the midpoint? I thought graph embedding was being used to avoid using midpoints? Maybe the sentence is just ambiguous? <sep> How many samples are used in the Wasserstein approximation? How were the coefficients in the multi-term loss function defined (alpha, beta and kappa)? Oh - I see these are in the appendix... Seems like appendices are becoming pretty integral to papers these days ... Probably worth including these for the main results in section 5 for the two results presented. <sep> Step 3 in the graph embedding didn't make sense to me after reading it a couple of times. It wasn't quite clear how this sample specific weighting works. It would probably be worth expanding this a bit at the expense of background material, as it is one of the contributions of the paper. <sep> Other Feedback <sep> Page 2 ""Adversarial methods are harder to train"" ... also adversarial methods are implicit distributions -- you can sample from them but you cannot easily calculate the likelihood of an image under the adversarial model (although you can use gradient descent to try to find the latent parameters). This makes things like outlier detection difficult. <sep> Page 7, Section 5.2, last sentence refers to Fig 6, but I think this should be Appendix D, Fig 7? <sep> If you flipped the name around from EPSWAE to SWEP-AE, you would have a much more memorable acronym for people to take away from your paper/talk/poster although I recognize this doesn't have the same ""build"" on previous work dynamic.","All three referees have provided detailed comments, both before and after the author response period. While the authors have carefully revised the paper and provided detailed responses, leading to clearly improved clarity and quality, there remain clear concerns on novelty (at least not sufficiently supported with ablation study) and experiments (neither strong enough nor sufficient to support the main hypotheses). The authors are encouraged to further improve their paper for a future submission."
"abstract | rebuttal_process | weakness | suggestion | weakness | decision  ==> Summary <sep> The authors proposed a new family of LSTMs (i.e. MC-LSTMS) which can be shown to have a mass conservation property for the LSTM memory cells. They have shown in various applications that these models lead to on par or better performance compared to state of art approaches. However, after examining the paper, I am not yet fully convinced that 1) a general unified MC-LSTMs can achieve good performance in all the mentioned cases 2) the conserved mass is interpretable (and corresponds to the problem's invariant, conserved quantity). I have listed those concerns in the question sections as authors might have precise answers. <sep> Pros <sep> The authors in this paper have proposed a new family of LSTMs (i.e. MC-LSTMs) which can be shown to have a conservation property for the mass held by the LSTM memory cells. The authors have applied their models in three different prediction tasks involving quantity conservation, namingly summation(and subtraction), traffic prediction and rainfall runoff modelling. The proposed models (with some variants) achieve better or at least on par with current state of art models. <sep> These different applications illustrate the usefulness of the models; the authors also reference appropriately to current state of art approaches, helping reviewers to well situate the paper's contribution. <sep> Questions: <sep> Well I understand there are differences for each application (motivating to choose different neural architectures), it would be very interesting to know the performance of one quite general architecture (compared to say standard LSTM implemented in torch). In the paper, we have seen in the experiments: <sep> time - independent R^t used in arithmetics time - dependent R^t used in hydrology experiments <sep> Hypernetwork based R^t used in pendulum experiments <sep> And also they vary on the auxiliary/mass inputs choices. <sep> What would be the performance if the authors use a quite general architecture (for example the second setting) for all the experiments please? <sep> Just for confirmation, in the experiments, the LSTMs involve forgetting gate right (contrary to what described in eq (1)) <sep> I don't find explanations for the r value for the hydrology experiments. Meanwhile the r value for the traditional r value is different from what is chosen for the MC-LSTM. Why is this please? <sep> Minor issues: <sep> There seems some formatting issues at the end of ""Mechanisms beyond storing are required for real-world applications"" in Introduction. <sep> In related work, Hamiltonian approaches (Greydanus et al. 2019) don't seem to assume access knowledge to time derivatives w.r.t. Inputs. Rather, it favours the conservation parametrised by the Hamiltonian. <sep> Minor suggestions: <sep> In abstract, ""expressed through continuity equations"", these aspects don't seem to be addressed by MC-LSTMs, so I propose to not include this phrase in the abstract. <sep> I suggest adding some citations In the introduction where the authors said ""LSTM to excel at speech, text, and language tasks"" http://nlpprogress.com/. <sep> The authors mention in the abstract and introduction about the interpretability but only show the mass interpretation in the hydrology experiments. For which I would suggest the authors show it in at least one other experiments (e.g. traffic) to 1) be more convincing 2) highlight this property (which may reveal to be very beneficial for production systems) <sep> I would be good to mention forgetting gate around equation (1) as those are used for later experiments in the paper. <sep> Thank you for the authors to having answered my questions and having addressed all my comments. My main concern was about the generalisation of the proposed architecture and the results at the current revision are convincing to me. I believe this direction of the research together with the approach taken make a nice contribution to the conference. In consequence, I have raised my score from 5 to 7.","The paper proposes a variant of recurrent neural networks based on Long Short-Term Memory. Unlike the standard LSTM, the proposed mass-conserving LSTM subtracts the output hidden state of the LSTM from its current cell state, thus preserving the ""mass"" stored in the cell states at each step. A left-stochastic recurrent weight matrix is also used to conserve the ""mass"" across the time steps. Empirical experiments demonstrated the effectiveness of the proposed MC-LSTM on a range of datasets such as addition & arithmetic tasks, traffic forecast, and rainfall modeling models. <sep> Several issues were clarified during the rebuttal period in a way that satisfied the reviewers. However, some concerns still remain unanswered: <sep> This is an empirical paper that proposes a modified LSTM that brings forward a few different ideas: L1 norm, stochastic transition matrices, and subtracting the output hidden states. An ablation study is a MUST in such an applied work. It has been pointed out by other reviewers that there are many prior references on LSTMs variants. It would greatly strengthen the paper by considering more diverse baselines. There is no experiment nor discussion on how much each modification helps wrt the final accuracy. Thus it remains unclear how the results can generalize to other problems. <sep> Although the results seem convincing across various datasets that mass conservation seems to help, the datasets are non-standard benchmarks in the machine learning conferences thus there is a lack of competitive prior baselines. As the proposed LSTM has a different number of parameters compared to the standard LSTM, is it fair to compare the different architectures under the same number of neurons? What happens if we compare the architectures with the same number of parameters? And how well does the model scale as we vary the hidden size? It would be helpful to keep the contributions into perspective by using standard RNN benchmark datasets such as Penn TreeBank or Wiki-8. <sep> Overall, the basic idea seems interesting, but the lack of ablation studies significantly hurt the contribution and the positioning of the paper. Given the current submission, the paper needs further development, and non-trivial modifications, to be broadly appreciated by the machine learning community."
"abstract | weakness | rebuttal_process | weakness  ==>  ==> In this work generative models using a GP as prior and a deep network as likelihood (GP-DGMs) are considered. In the VAE formalism for inference, the novelty of this paper is located in the encoder: It is sparse and the posterior can be computed even when part of the observations are missing. Sparsity is obtained using inducing inputs and the missing observations are handled through the use of deep sets, i.e. the observations aren't given as a vector, but as a permutation-invariant set of (index, value) pairs. <sep> The idea is  interesting and well executed, and the experimental results are certainly good when compared with the provided baselines. However, there are many unclear details in the experiments. I was left with quite a few unanswered questions: <sep> In the cases in which no imputation is needed, how does the proposed SGP-VAE compare with a standard encoder (instead of one based on deep sets). I.e., are we paying a price for the ability to handle missing data, or are we getting even better results than with a traditional encoder? <sep> How does the model compare with prior similar work, such as the GPPVAE? In principle, it should be easy to improve on this baseline. <sep> How good is the model itself? Here we see results based on amortized inference, but what if we weren't using it? Even though this case would be slow and maybe not able to handle the entire dataset, this can be done in a subset. In general, it is unclear how much is being lost by a) amortized inference; b) sparsity; c) use of deep sets (in this latter case, it could that something is won instead). <sep> In the Japanese Weather experiment, when the standard VAE is used (with no mean imputation), how are the unobserved values being dealt with? <sep> In the Japanese Weather experiment, the baselines don't seem very strong. A standard VAE and independent GPs don't seem to be particularly suited to this spatiotemporal weather prediction. <sep> In general, I'm not sure where the advantage seen in these experiments is coming from. For instance, when compared with published results on the same data (e.g., GPAR on EEG), the SGP-VAE doesn't seem to offer a big advantage. In the other experiments, no strong baselines for that specific  data seem to be provided. <sep> I would be curious to know in which of these datasets the model itself is superior (if we were to skip the encoder and use, say, MCMC for inference) and in which ones it isn't. <sep> Thanks for your response, clarifying.","The paper proposes a method for inference in models with GP priors and neural network likelihoods for multi-output modelling, dealing with the problem of scalability and missing data. The paper builds upon previous work on inducing variables for scalability on GP models and inference networks for amortization (reducing the number of parameters to estimate) and dealing with missing data. <sep> There are several concerns about the paper in terms of generality/flexibility of the approach, as the proposed model shares the NN parameters across tasks and the results on the small datasets do not show improvements wrt baseline such as GPAR. The authors' comments provide somewhat satisfactory replies to these issues. Nonetheless, the major drawback of this paper is its novelty as the ideas on the paper have been explored extensively in the GP literature. Although the authors do make a case for scalability when using inference networks, there are other previous works that perhaps the authors are unaware of, for example, https://arxiv.org/abs/1905.10969 and even more sophisticated inference algorithms than can serve as truly state-of-the-art competing approaches (for example based on stochastic gradient Hamiltonian Monte Carlo, https://arxiv.org/abs/1806.05490)."
"abstract | rebuttal_process | suggestion | decision  ==>  ==> Summary <sep> The paper investigates failure cases for transfer learning (fine-tuning and joint training), specifically in the context where training on the source data may highlight features that are irrelevant for the target data. This is done through semi-synthetic data. Based on the insights, the authors present an approach called Meta Representation Learning (MeRLin) inspired by Meta Learning and Learning-to-learn approaches. This approach is evaluated on several real-world transfer-learning tasks from vision and NLP. The authors also derive theoretical results on constructed data distributions for which superiority of Merlin can be shown analytically. <sep> Quality, clarity, originality and significance <sep> The work is well-motivated, and the paper is well-written and clear. <sep> The ""failure modes"" and the behavior on the semi-synthetic data are not entirely surprising in my opinion (and I would have expected more analysis after reading the introduction), but they highlight the targeted problem well. <sep> The suggested algorithm is well-motivated and linked to existing work. However in many practical settings, it may be infeasible to train including the whole source data for reasons of compute and data availability, which is why many transfer-learning approaches do not assume the availability of the source data during transfer. This scenario is encountered in the case of BERT, where the authors then ""only meta-learn the representation"". In that setting, Merlin seems to reduce to reduce to a changed objective inspired by meta-learning and learning-to-learn. This is not a problem in itself, but it makes it less clear, what exactly the authors think that the main contribution of the paper is. <sep> The experimental results look good at first glance, but overall I found it hard to evaluate how convincing they are because of several potential problems: <sep> The experimental results are not compared to any results from the existing literature, all baselines and comparisons are entirely from this paper. This makes it hard to know how much careful tuning went into the proposed algorithm versus the baselines. I think at least some comparison to results from the literature should be possible or it should be carefully explained why such an overfitting to the proposed algorithm clearly did not happen. <sep> There are a lot of seemingly random choices in the selection of the datasets, e.g. why USPS and not MNIST, why CUB/Caltech/Cars and not e.g. CIFAR, Pets, Flowers, or Birds? I'm not saying any of these choices are inherently better, but the authors should explain why these datasets are chosen, to avoid the impression that the datasets may have been chosen because the proposed method works particularly well on these dataset combinations. One way to make the results stronger would therefore be to use combinations of datasets that other, previous work has already used, which would also enable a direct comparison (see above). <sep> Similarly, it is unclear why only a subset of GLUE was used and not the full GLUE benchmark. Again, it would be better in my opinion to use all of GLUE to avoid the impression that the subset was hand-picked. Also, I tried to find results on GLUE for direct comparison, because they should exist, but was not successful immediately, because the presented results are on the dev-set (Table 2) while usually results for comparison come from the official test set as far as I understand? E.g. the BERT paper seems to have tuned on dev and then reported numbers on test, but I did not find dev-set numbers in a quick search. For something fairly standard as BERT-base and GLUE I think it would be good to be able to find numbers in the literature that are directly comparable. <sep> Again in a similar fashion, Sec.6.3. evaluates only two target data sets, why only these? And A.5 looks only at Food->Cub, why only this pair? <sep> Pros and cons <sep> Pros: interesting approach, well-motivated, well-written, interesting theoretical analysis <sep> Cons: experimental results are not entirely convincing, the main message is not 100% clear to me <sep> Minor details and comments <sep> The paper is fairly ""squeezed"": E.g. lack of vertical space after captions of Figures 1 and 2; also the main suggested algorithm is not explained in detail in the main text, but deferred to the Appendix. It might be more readable to try to shorten the content. <sep> ""Pre-training only"" is not a complete transfer-learning algorithm because of potential class number mismatch, yet it is used for comparison (e.g. Fig. 2b). This only works in this very specially constructed scenario, it seems. Maybe this should be mentioned? <sep> ""Target-only"" is discussed in the context of Fig. 2 but it is not contained in the Figure, why? Also, ""fine-tuning"" results are not shown in Fig 2b. <sep> Towards the end of Section 3, when discussing the synthetic data, ""overfitting"" is mentioned twice as a problem. There exist several approaches that are often used to try to overcome overfitting, and the authors even use L2-sp as a baseline in Section 6. It remains a bit unclear how much a careful combination of fine-tuning and such methods could help here. I don't think this is a major shortcoming, though, because the artificial setup is constructed exactly in such a way that the pre-trained classifier will focus on the non-transferable features. <sep> Table 1: For ImageNet->C256 L2-sp seems to be not clearly worse than MeRLin given the standard deviations of three runs. Maybe both should be bolded? <sep> A.7 ""the model is not sensitive to varying ρ and λ."" - it was not clear to me how much this statement can be derived from the figure. <sep> Minor comments / typos: <sep> Caption of Fig.1 - I did not see how this is an illustration of the ""features learned"" <sep> End of Sec.3: ""on a much simplified settings"" <sep> page 5: ""Merlin to by changing"" - ""to"" too much page 5: ""with AB as the source ."" - space before ""."" <sep> page 6: ""with their own best regularization strength"" - I did not understand this phrase here. <sep> before Eq.(4) - what is Ω here? <sep> In Theorems 1 and 2, what exactly is meant by ""universal constant""? <sep> after Eq.(6): ""because its simultaneously fits"" -> it <sep> Food-101->CUB: here, the model is not mentioned. I guess it was also ResNet-18? <sep> page 7 ""join training"" -> joint page 8 ""the our method"" <sep> References: some letters in the reference titles are lowercased when they shouldn't be, e.g. ""cnns"", ""Mask r-cnn"", ""reuse? towards"", ""t-sne"", ""mnist"", ""Xlnet"". Also, several references do not contain a ""publishing venue"", e.g. Kolesnikov 2019, Lin 2002, Liu 2019a, Neyshabur 2020 - in the age of search engines this is not a real problem, but it might be nice to give at least an arXiv number if available or some of these have probably appeared in conferences or journals. <sep> page 12: ""images is of resolution"" <sep> A.2 ""lambda is found with cross validation"" - how exactly? <sep> A.4 for completeness maybe include ImageNet and USPS? <sep> Figure 4 caption: ""textbfSensitivity""","The paper compares transfer learning with fine-tuning and joint training and then proposes a new approach (Merlin). Reviewers have pointed to the fact that Merlin works in a setting that is different from normal transfer learning settings (it assumes some target domain data is available during training). The authors acknowledge this and think it can still be a reasonable setting, but of course it makes comparisons more difficult. Overall, while there are interesting analysis and results, the paper remains borderline and more work should be done to make it a good contribution, including significantly improving the presentation to make clear the distinction in settings. I therefore recommend to reject the paper."
"abstract | strength | weakness | misc | weakness | suggestion | weakness | suggestion | weakness | suggestion | decision | rebuttal_process | decision  ==>  ==> This work proposes an algorithm that aims at finding a flat minimizer. The high-level strategy in the design of the proposed algorithm is increasing the learning rate when the iterate is in the region of a sharp minimizer. The authors claim that by increasing the learning rate, the iterate can get out of the undesired region. To estimate the local landscape (local sharpness), the authors provide a heuristic, which requires running gradient descent and gradient ascent for some number of iterations. The proposed algorithm has a promising result empirically, compared to entropy SGD (Chaudhari et al.) which also aims at finding a solution that generalizes well. <sep> Strength: <sep> (1) Paper is written well. The motivation is clearly explained. <sep> (2) The algorithm is easy to implement. <sep> (3) The algorithm seems to work well in practice. <sep> Weakness: <sep> (1) Compared to the empirical results shown in the paper, the theoretical result is relatively weak. My concern is that Theorem 5 does not differentiate between flat and sharp local minima. Specifically, it could be the case that the conditions of the theorem are satisfied and that the iterate generated by the proposed algorithm escapes flat minima eventually. <sep> The paper will be in a much better shape if the theoretical result shows that the iterate escapes sharp local minima while attracts to flat minima. <sep> (2) (Computational overhead) Since the algorithm has a subroutine to estimate the local landscape, it incurs computational overhead compared to the baselines. I hope the authors can discuss this issue. <sep> Minor: <sep> (1) (Assumption 1) What are the examples that satisfy the assumption? <sep> (2) (Remark 3 and normalization in Definition 1) <sep> I agree with Remark 3, but the argument made in Remark 3 also implies that the normalization will reduce the sharpness value when the gradient is large, which might be an undesired effect. <sep> === <sep> Overall, I think this is a reasonable paper and I am happy to increase the score if the authors respond well. <sep> === after rebuttal === <sep> I thank the authors for the feedback. <sep> Regarding the proposed theorem, I was hoping to see if the iterate can stay in the flat minima instead of leaving the region eventually, as the constant of the local strong convexity is inside the log's, which is not very sensitive to the value (the landscape) and might be tricky to provide useful guidance to differentiate flat and sharp minima. <sep> I decided to maintain my score, but I still recommend a weak accept of this paper.","This paper proposes a method to update the learning rate dynamically by increasing it in areas with higher sharpness and decreasing it otherwise. This would the hopefully leads to escaping sharp valleys and better generalization. Authors further provide some related theoretical results and several experiments to show effectiveness of their models. <sep> All reviewers find the proposed method well-motivated, novel and interesting. The paper is well-written and easy to follow. However, both theoretical results and empirical evaluations could be improved significantly: <sep> 1- The theoretical results as is provides little to no insight about the algorithm and unfortunately, authors do not discuss the insights from the theoretical results adequately in the paper. See for eg. R1's comments about this. <sep> 2- Given that the theoretical results are not strong, the thoroughness in empirical evaluation is important and unfortunately the current empirical results is not convincing. In particular, there are two main areas to improve: <sep> a) Based on the Appendix D, the choice of hyper-parameters seem to be made in an arbitrary way and all models are forced to use the same hyper-parameters. This way, the choice of hyper-parameters could potentially favor one method over the other. A more principled approach is to tune hyper-parameters separately for each method. <sep> b) It looks like the choice of #epochs has been made in an arbitrary way. For all experiments, it would be much more informative to have a figure similar to the left panel of Fig. 4 but with much more #epochs so that reader can clearly see if the benefit of SALR would disappear with longer training or not. <sep> c) Based on the current results, SALR's performance is on par with that of Entropy-SGD on CIFAR-100 and WP and there is a very small gap between them on CIFAR-10 and PTB. I highly recommend adding ImageNet results to make the empirical section stronger. The other option is to compare against other methods in fine-tuning tasks. That is, take a checkpoint of a trained model on ImageNet and compare SALR with other methods on several fine-tuning tasks. <sep> Given the above issues, my final recommendation is to reject the paper. I want to thank authors for engaging with reviewers during the discussion period and adding several empirical results to the revision. I hope authors would address the above issues as well and resubmit their work."
"abstract | strength | misc | weakness | suggestion | weakness  ==> Overall I vote for (weak) rejecting. IMO the main weakness of the paper is the clarity, for instance I had to read the article multiple times before understanding the main contribution. Some crucial terms should may have to be mathematically defined in the paper, like 'greedyness' for hyperparameter optimization, a term a was not aware of. <sep> Advantages of the paper: <sep> the proposed algorithm seems to lead to significance gain in performance in practice authors proposed a new stopping criterion which seems to be more efficient in practice <sep> Concerns: <sep> 1- The notion of greedyness is paramount for the paper, however it is not defined. Moreover the references provided are not very helpful to understand the concept. The word 'greedy' appears once in [1], twice in [4] and is not defined properly. The word 'greedy' does not appear in [2, 3]. Would it be possible for the authors to define the concept properly in the paper, or the provide a selfcontained reference? <sep> 2- I founded the main contribution of the paper hard to find. If I understood well, authors propose a new to estimate the hypergradient by averaging out, and then updating the hyperparameter. This is very subjective, I would recommend to highlight this contribution, and maybe to remove 'we combined the above [...] with momentum decay' in the introduction. <sep> 3- The experiments do not seem clear to me, I do not know if it comes from my lack of experience in the field of from the lack of clarity of the paper, but experiments were hard to follow. In particular the number of steps in the inner problem is provided, but what is the size of the hyperparameter searching space? In particular all the experiments are provided with convergence as a function of the number of steps. Since authors rely on forward differentiation, one step of the proposed algorithm can be more costly than one step of the algorithms in the baseline. Am I missing something? Is forward differentiation paramount for the proposed algorithm? <sep> 4- Authors claim that hyperparameter sharing is equivalent to averaging (page 5). Is it mathematically grounded? Is it trivial? Or could authors provide a reference? <sep> There are a lot of things I did not understand, thus I voted for weak reject. However if authors answer my questions I am of course willing to change my score. <sep> Minor: <sep> in section 1 there should have a space after the point in l3 <sep> [1] Luketina, J., Berglund, M., Greff, K., & Raiko, T. (2016). Scalable gradient-based tuning of continuous regularization hyperparameters. ICML 2016 <sep> [2] Franceschi, L., Donini, M., Frasconi, P., & Pontil, M. (2017). Forward and reverse gradient-based hyperparameter optimization. ICML 2017 <sep> [3] Baydin, A. G., Pearlmutter, B. A., Radul, A. A., & Siskind, J. M. (2017). Automatic differentiation in machine learning: a survey. JMLR <sep> [4] Donini, M., Franceschi, L., Pontil, M., Majumder, O., & Frasconi, P. (2019). Scheduling the Learning Rate via Hypergradients: New Insights and a New Algorithm. arXiv preprint","This paper investigates methods for gradient-based tuning of optimization hyperparameters. This is an interesting area, and the paper isn't bad. The examination of hypervariance seems relatively novel and useful. I also appreciate the point about Bayesopt sometimes working well simply due to small ranges. <sep> However, I agree with the criticisms of the reviewers. Overall this paper isn't quite clear, thorough and impactful enough to make it in this round, but I think with more attention to baselines and scope this paper could be acceptable. <sep> Some minor comments: <sep> The signed-based optimizer, while simple and sensible (which is good), seem kind of ad hoc. <sep> The authors don't seem to have properly scoped the problem and method, since greediness is only a major concern for inner optimization hyperparameters specifically. It's not clear that for regularization parameters that this problem exists or that your method would apply. <sep> A small nit: Is hypervariance the right thing to look at, since the problem can exist even in deterministic settings? Perhaps some sort of sensitivity analysis would be more appropriate. Also you should reference Barack Pearlmutter's thesis which first explores these issues. I would also mention that the hypervariance is generally tiny for smaller-than-optimal learning rates, and massive for larger-than-optimal learning rates, (the chaotic regime)."
"abstract | strength | weakness | suggestion | decision  ==> Summary: <sep> The paper proposes a clever trick to make instance contrastive learning faster by using the intermediate feature layers to perform the contrastive loss rather than just using the final 2048-d mean-pooled features as is typically done in MoCo or SimCLR. That way, the backprop costs are cheaper. The authors also use this intermediate representation based similarities to guide a better set of negatives to the layers on top. The authors demonstrate good results that show better time to accuracy (on the linear classifier) with both MoCo and SimCLR. A useful consquence of this paper is making contrastive unsupervised learning on ImageNet more accessible to people with less computation resources, ex PhD students in academic labs who may not have a DGX-1 or v3-TPU pods. <sep> Pros: <sep> Tackles an important problem - how can we make self-supervised learning even faster with clever engineering <sep> Delivers on the problem by proposing two sensible solutions - saving the cost of backprop by using intermediate layers for contrastive losses, and also better negative mining while doing the fprop in a computationally efficient manner. <sep> Good results in terms of time to accuracy on linear classifier - both with MoCo and SimCLR - two leading instance contrastive learning approaches. <sep> Cons: <sep> No code release as of yet. I believe the utility value of engineering driven papers is high but heavily relies of clean open source code that's usable by the community. <sep> No results with fewer GPUs. If you can get results faster but still use 8 GPUs (though I do notice you have used 12 GB memory Titan V as opposed to 16 Gb memory GPUs used by MoCo, I don't think that's a deal breaker since MoCo ResNet-50s can be trained with the same HBM too). I think what's more important is to show you need fewer GPUs - can someone with a single GPU get results with the same amount of compute-time taken by Facebook to get MoCo's results.. That's what really increases accessibility to folks with less compute IMHO. <sep> Reducing time to accuracy could be done in a host of other ways - eg - make everything run in fp16 or bfloat16 on the right kind of hardware - you would see a speedup of ~1.3x already. Use smaller image sizes while training, eg - instead of 224x224, use 192x192, use different temperatures (tune hyperparams) for training within as few epochs as possible. Use less unlabeled data to get the same accuracy. All these are orthogonal to your proposed method ofc, so not trying to compare them with yours. My point is that - why not push the broader goal to the extreme - can one combine all these engineering tricks in one piece, and design a way faster contrastive learning pipeline that can train with way less resources and still perform as well in much less wall clock time.. <sep> The Related work section can be improved - example, Contrastive Learning prior work should mention the CPCv2 work (Henaff et al 2019); and the idea of using intermediate layers and backprop. the losses through them should mention the Inception architecture (Szegedy et al. 2015) as well as Deep Classification work that did this few years ago for supervised learning. <sep> Rating: Weak Reject - I would be open to reconsidering this if the authors commit to releasing their code, and also performing some benchmarking with respect to number of GPUs, and revising related work. I also encourage the authors to push further on making their pipeline even more efficient.","This paper introduces modifications that allow to make the training of contrastive-learning-based models practical. The goal of the paper is very interesting, and the motivation clear. This paper tackles a very important issue with recent unsupervised feature learning methods. <sep> However, while the goal is great, the present submission does not provide time improvements on par with the ambitions of this work. As noted by R2, many other hacks could be used in conjunction with the current work to scale this goal to the extreme, yielding time improvements which would be of a more impressive magnitude. In its current form, this paper unfortunately doesn't meet the bar of acceptance. <sep> Given the interesting scope of this work, I strongly encourage the authors to take the feedback from reviews and discussions into account and submit to another venue."
"weakness | decision  ==> Summary of paper: <sep> The authors review the information bottleneck (IB) in the context of deep learning. They discuss the obstacles to applying the IB (and a deterministic variant, the DIB) to modern datasets, review approaches to doing so, and introduce their own scalable approach. Their approach introduces practical surrogate objectives for the information regularizer term, and uses dropout as the source of stochasticity. They take advantage of the scalability of their method to train a ResNet with (D)IB on MNIST, CIFAR-10, and ImageNette and study adversarial robustness and evolution in the information plane. <sep> Pros: <sep> The surrogate objectives the authors introduce allow the application of IB without restricting the latent distribution to take on a form with analytic entropy (i.e. gaussian), as is the case the deep variational IB (VIB). <sep> The authors are the first, to my knowledge, to scale the DIB from tabular settings (where it was developed) to modern function approximation settings using a clever zero-entropy noise trick (although this did come at the cost of diverging from the deterministic solutions that would be optimal). <sep> The authors are the first, to my knowledge, to use dropout as the source of stochasticity that IB requires. This has the advantage of allowing the authors to use nearly arbitrary DNN architectures with (D)IB, as opposed to inserting an explicitly stochastic (gaussian) layer. <sep> The paper functions as a good review, independent of the authors' contributions. A common complaint when reading ML papers is that they don't discuss related work enough, so this paper was refreshing. <sep> Cons: <sep> I struggled to disentangle the novel contributions of the authors from the work they were reviewing. The novel contributions to my knowledge are the first 3 pros above. On the other hand, optimizing decoder uncertainty is not, for example (e.g. it is done by VIB). The authors need to do a better job at highlighting their own contributions but also making clear what is not. <sep> I don't think the authors make a very compelling explicit case for what advantages their approach has over VIB (the main alternative). I do believe there are advantages (see pros 1-3 above), but they are scattered throughout the paper and not always made explicit. I think the authors need a dedicated subsection addressing this. This section should also highlight the disadvantages (looser bound maybe?). <sep> Relatedly, why not include direct comparisons to VIB in the experiments? The authors seem to imply that VIB wouldn't scale to the datasets they tackle, since the experiments in the VIB paper involve pretrained embeddings and smaller models. But a) the VIB paper was written 4 years ago and hardware/software has improved since then b) the model sizes are the same order of magnitude. <sep> Other comments: <sep> I thought the heavy use of color distracted more than it helped, though I appreciate the effort. <sep> This paper (https://arxiv.org/abs/1712.09657) also attempted to scale DIB to non-tabular problems (although far from the scalability of DNNs). The authors also added noise, but in this case to the data rather than the latents. Different problem being solved, but possibly interesting connection for the authors. <sep> UPDATE <sep> Following the author's response and updated draft, I've raised my score from a 6 to a 7. <sep> UPDATE 2 <sep> Following discussion among the reviewers and especially a summary of experimental results by Reviewer 3, I'm lowering score back to a 6.",We have a very well informed reviewer who strongly feels that this paper is insufficiently novel and significant further discussion on how the paper might be raised to a publishable level with more empirical results. I will have to side with the more engaged reviewers who feel that the paper should be rejected.
"weakness | rebuttal_process  ==> This paper introduces POP3D, an on-policy policy gradient algorithm that is a variant of TRPO and PPO. While TRPO uses a particular penalty function to keep the policy from being updated too aggressively, POP3D uses an alternative objective function that lower bounds the square of the total variance divergence between two policy distributions. The authors argue that this alternative formulation results in an algorithm that is sample-efficient, like PPO, but that is more effective at keeping policy updates from overshooting. The authors also argue that this new formulation helps users to avoid the arguably challenging process of selecting penalty constants, as required (for instance) by TRPO. <sep> Overall, this was an interesting paper. I believe that the idea of lower bounding the square of the total variance divergence is worth being investigated. The experimental results also seem promising, showing that the method may outperform POP and TRPO on a wide range of games. I do have a few questions about the statistical significance of the results, though: some of the score differences may not be significant, the results might be based on too few trials, and the authors only presented mean performances, but not standard deviation or standard error information. I liked the idea that the proposed lower bound solves possible issues arising from using the KL divergence, which is a measure that is not symmetric. On the other hand, after presenting the surrogate objective function's mathematical definition, the authors argued that this new formulation results in better exploration and that it ""expands the manifold [of] solutions"". I had a hard time following these arguments. <sep> Finally, in my opinion, writing could be improved. A few sentences are hard to read or appear to be presenting incomplete thoughts. For instance: <sep> ""Hessian free strategy: Fisher vector product is utilized to cut down the computing burden."" (page 1) <sep> ""(...) the action is taken approximately strongly corrected with the highest probability value."" (page 4) <sep> ""Therefore, even if theta outputs theta_old the same high probability for the right action, it's still penalized owing to probabilities mismatch for other uncritical actions."" (page 4) <sep> ""From the perspective of the manifold, if the optimal parameters constitute a solution manifold."" (page 4) <sep> I believe that this is an interesting work, though possibly still showing only preliminary results. I also think that the paper might be improved by clarifying a few of the authors' key mathematical arguments. As it is, and based on the thoughts presented above (and on the questions below), I would say that more work would benefit the paper and would argue for a weak reject. <sep> I have a few questions and comments for the authors: <sep> the authors state the ""[POP3D] dives into the mechanism of PPO's improvement over TRPO by the perspective of solution manifold"". Could you please clarify? What is the solution manifold perspective, and how is it used more effectively by POP3D, compared to previous algorithms? At the end of Section 3.3, you present an argument that I found hard to follow: ""From the perspective of the manifold, if the optimal parameters constitute a solution manifold."". Later on, you also talked about ""[expanding] the solution manifold at least one dimension such as curves to surfaces"". I believe that these ideas may be central to the paper. Still, I had difficulty following the arguments in this section. <sep> a quick note on notation: \\hat{A}t (Eq6) looks like a random variable denoting the action taken in time t. Perhaps the LHS of this equation is missing parameters specifying a (s,a) pair? In particular, note that the advantage function in Eq6 involves computing delta^V{t+l}, a TD error for a specific experience; the current notation does not make it clear which experiences are being analyzed. Compare, for example, the definition of advantage used in Eq6 and the one used in Eq3. <sep> still regarding notation, I wonder if you could clarify which of s, a, s_t, a_t, are random variables and which ones are realizations of the corresponding random variables. In Eq6, for example, why do you write A(s_t, a) instead of A(s_t, a_t)? <sep> in Eq10, please formally define D_{TV}. <sep> immediately after Eq11, the authors argue that because the KL divergence is not symmetric, the choice of whether to use D_KL(p1, p2) or D_KL(p2, p1) might affect the algorithm's behavior, and that (for this reason) it might not be the best choice for a divergence measure. Could you please discuss the possible advantages of simply using the symmetrized version of this divergence (https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence#Symmetrised_divergence)? <sep> in Eq12, where is the action ""a"", on the RHS, coming from? The inputs to D_pp are two distributions, but its definition seems to consider the difference between two particular probabilities. Could you please clarify the notation being used here? <sep> regarding your experiments, I wonder if you could discuss whether the score differences observed in Table 1 are statistically significant. <sep> your method does seem to perform better in some particular games, but it does not do so well in other games. A discussion/interpretation of these results would be nice: what are, for instance, the properties of the games that are hard for POP3D? <sep> regarding Table 3, could you please discuss whether you can draw strong conclusions regarding the performance difference between PPO and POP3D when analyzing just 3 trials? Also, do you have standard deviation information associated with the averages that you could present? <sep> one key point that the authors make is that POP3D encourages exploration by ""expanding the optimal solution manifold"". More efficient exploration is indeed advantageous, but I did not follow the argument being made here. Could you please clarify why POP3D results in better exploration?","While the paper contains some interesting ideas, the reviewers felt that overall the paper is not theoretical well supported, and likewise the experiments are not fully convincing. Even after the rebuttal, these concerns still persist."
"abstract | strength | rating_summary | suggestion | rebuttal_process  ==> In general, this paper deals with an interesting and essential problem to generate geometric graphs under several standards. The whole algorithm seems easy to implement or reproduce. It seems with minor modifications to traditional autoregressor based generative graph models, the proposed framework can effectively model isomorphism as well as delivers certain novelty. The idea of the paper is with novelty and some theorems can support the observations. <sep> However, I still have several concerns about the paper, stated as follows: <sep> While the paper emphasized that the proposed GG-GAN is capable of producing geometric graphs with under several vital criteria (e.g. novelty, scalability and modeling complex dependencies), one critical factor is missing: mode collapse/generation diversity. Many of the generative models still suffer from mode collapse/generation diversity problem, resulting in a small portion of generated variants than empirical observations. I would recommend the authors to discuss and give more evidence showing the ability of the proposed method to avoid such pitfall. <sep> The authors claimed that the proposed method can model complex local and global dependencies among nodes and edges. I understand that such a procedure can handle node dependencies given the design of the generator part. However, I suspect the ability of the proposed method to model complex ""edge dependencies"". To me, the sampling procedure of edges is performed under independent Bernoulli distribution for each edge. Therefore, it's inappropriate the claim that GG-GAN incorporates any mechanism to model dependencies between edges. <sep> In corollary 1, the authors proved the existence of some distribution \\mathcal{D}_x. However, it seems that existence of such distribution is naive: we can simply establish distbution with Delta-functions for each node. If I understand right, this corollary is not very informative. <sep> Proposition is under the condition that each node is sampled independently. However, such a sampling mechanism can be easily replaced with a sampling procedure following a point process to avoid the coincide of nodes. I suppose that it would be better to briefly discuss the sampling procedure under this setting. Otherwise, it would be too weak Proposition 1 is. <sep> Section 2.2.3 Avoiding Collision is obscure to me. The necessity of such a mechanism is not well understood for me. I suggest the authors to give more details or clarify with some theoretical analysis to justify their claim in this section. <sep> Though the authors gave some discussion on the hand-crafted features of nodes, I still think the features employed in GG-GAN is ad-hoc. Hand-crafted features can greatly hinder the capacity of deep learning model and thus weaken the contribution of the paper. <sep> I see that to construct the initial input to the generator, an initial fixed point configuration and a unique sampled z for each node are concatenated. I suggest the authors to show what will happen if we sample z separately for each node. This may help to understand the necessity of unique sampling, then further show the mechanism behind it. <sep> I might consider to raise the rating if the authors can address my concerns well.","In this paper, the authors proposed a geometric graph generator that applies a WGAN model for efficient geometric interpretation. All the reviewers agree that the idea is interesting and the method has the potentials for graph generation tasks. Unfortunately, the experimental part is unsatisfying, which makes the paper on the borderline. More analytic experiments should be designed to verify the properties of the proposed GG-GAN, especially its scalability. Although in the rebuttal phase the authors add a simple example to generate large but simple graphs, we would like to see more experiments and comparisons on more real-world large graphs (even if the performance may not be good, the results will be constructive for both readers and authors to understand the work)."
"weakness  ==> This article extends the notion of alignment [Ji and Telgarsky, 2018] to linear neural networks with multiple output nodes, which requires the consecutive layers (i+1, i) to have the same (right, left) singular spaces. The authors identify necessary and sufficient conditions under which, alignment is an invariant of the gradient descent iterates (Definition 3), which in particular means that the gradient descent iterates only update the singular values of the layers, and not their singular vectors. The authors studied alignment for several shallow and deep linear architectures, and specify learning rates for which gradient descent enjoys exponential convergence. <sep> Here are my main comments, mostly about the significance of the results. <sep> Alignment (Definition 2), itself, is a very restrictive assumption: it requires the left and right singular spaces of all consecutive layers to be aligned, i.e. V_{i+1} = U_{i}. In fact, the authors show in Theorem 3 that alignment cannot occur, for a large class of interesting architectures, including convolutional neural networks. Therefore, studying this notion is not well-motivated to begin with. <sep> Alignment being an invariant of training (Definition 3), is a far more stringent assumption, particularly because it requires the singular spaces of all hidden layers to remain fixed during the training. As it is shown in Theorem 1, this property holds if and only if the input and the output have the same right singular space. On the other hand, when this condition can be satisfied, e.g. for instances that are given in section 4.2, it is not clear if the analysis provides any additional insights/improvements over the previous works. <sep> Some additional comments: <sep> Definition 3 requires interpolation under the linear model, i.e., the data is clean (no noise), and the relationship between the output and the input is completely characterized by a linear map, which makes the result less interesting from a practical view. While this setting is well-studied in the literature, this work does not provide comparisons against the previous works. <sep> Matrix Sensing in section 4.2: how can matrix sensing be an instance of a deep linear network with a multi-dimensional output? Both the labels y_i and the network predictions Tr(M_i^T W_d...W_1) are scalers, and hence 1-dimensional. <sep> To sum up, the paper studies an extension of alignment [Ji and Telgarsky, 2018] to linear networks with multi-dimensional output. This notion is too stringent -- as the authors confirm in the paper -- and cannot be satisfied unless in some special cases. On the other hand, when the condition can be satisfied, it is not clear if the results provide any insights/improvements over the previous work. For these reasons, I vote for rejecting this submission. <sep> ======================== <sep> Final Recommendation <sep> I have read the rebuttal and decided to keep my score. I think this study needs to be further motivated. <sep> I also want to clarify a minor issue in authors rebuttal. In contributions, you say: ""We demonstrate that alignment is an invariant for fully connected networks with multidimensional outputs only in special problem classes including autoencoding, matrix factorization and matrix sensing. This is in contrast to networks with 1-dimensional outputs, where there exists an initialization..."". My point is that the matrix sensing problem that you study here has 1-dimensional output.",The consensus view was that the reviewers were not convinced that the analysis done in the paper was sufficient motivated.
"abstract | strength | weakness | suggestion  ==> The paper presents a way to learn disentangled representations with respect to target attributes of interest by learning to mask weights or activations. A particular piece of text is encoded into distinct vectors that capture different factors of variation in the data. The method involves learning masks for each factor of variation while keeping the pre-trained model parameters fixed. The masks for every layer are trained using a combination of a triplet-loss, attribute classification loss, and one that encourages masks for different factors to be different across all layers. The triplet loss forces representations of examples that are similar with respect to a particular attribute to be closer than one that are similar based on another attribute. <sep> Models are evaluated on a sentiment/genre classification on a dataset sampled in such a way that introduces spurious correlations between genre and sentiment but evaluated on data that does not have any such correlation. The approach is also evaluated on disentangling syntax and semantics. <sep> Strengths <sep> Building models that are robust to spurious correlations in data is important for a variety of reasons and learning disentangled representations is a promising way to achieve that. This paper shows good generalization performance on datasets with such characteristics. <sep> The overall approach is simple and only requires training masks over weights/activations at each layer. The masks are trained with a fairly straightforward choice of training objectives. <sep> The paper is well written and the overall approach is easy to understand. <sep> Weaknesses <sep> The triplet loss as formulated in this work seems to make it possible to disentangle only two factors of variation (a) and (b). <sep> There is still a fair amount of attribute leakage and the probe designed to measure this leak is only a single layer MLP, there might be more leakage with stronger probes. <sep> The weight masking strategy significantly increases the number of parameters (although the masks are binary, so it just requires a single bit as opposed to 16/32 bit floating point numbers). In this particular work, the number of parameters triples, and it scales linearly with the number of attributes as well. <sep> It requires running the model forward multiple times to get representations that encode different factors of variation. <sep> Questions & Comments <sep> What would performance look like if masks were trained after fine-tuning on sentiment/genre classification? Rather than training masks directly on top of BERT-base. It would be interesting to see if the model is stable to recover from fine-tuning on data with spurious correlations and still produce disentangled representations. <sep> Is every single weight/activation masked at every transformer layer? The paper seems to lack some specifics about exactly what layers/weights are masked. Along these lines, did you experiment with masking only the last few layers? This could save time & parameters <sep> In Figure 3 is the model training with L_{cls} corresponding to sentiment and then visualized for sentiment and genre? Or is the top trained with the supervised sentiment loss and the bottom for supervised genre loss? <sep> It would be interesting to explore an L1 penalty on the masks for increasing sparsity, possibly in conjunction with magnitude pruning as well. <sep> The WC task doesn't feel very representative of sentence ""semantics""","This paper explores a methodology for learning disentangled representations using a triplet loss to find subnetworks within a transformer. The authors compare against several other methods and find that their method performs well without needing to train from scratch. The reviewers thought this paper was well written and the authors were very responsive during the review period. However, there were some questions about the experimental setup and empirical performance of the paper, leaving the reviewers wondering if the performance was convincing. We agree that there is value in exploring disentangled representations even if they do not necessarily improve performance (as the authors point out), but clearly explaining the reasoning behind all analyses (e.g. specifically choosing domains to introduce a spurious correlation), and justifying differences in performance is particularly important in these cases."
"abstract | misc | weakness | suggestion  ==> Paper summary <sep> The paper introduces a two server protocol to handle privacy concerns and Byzantine threats in a Federated Learning system simultaneously. <sep> In the protocol, each client secretly shares their model update with the two servers by splitting its model update such that neither server can know what the model update is without colluding with the other server. The servers are able to compute pairwise distances of all updates securely. These distances are used by byzantine robust aggregators to find a robust model update. <sep> Strengths <sep> The paper handles two very relevant and important issues with Federated Learning simultaneously - privacy and Byzantine resilience (which includes data poisoning attacks). <sep> The proposed algorithm is shown to have theoretical guarantees. <sep> A wide array of byzantine robust aggregation rules can be incorporated easily into the framework. <sep> There isn't much communication overhead between the workers and the server. <sep> The two server protocol does not seem to be too difficult to implement in practice. The algorithm requires that the two servers should not collude. This requirement does not seem too difficult to be enforced onto big companies that will use FL. <sep> In the absence of Byzantine machines, the non-robust protocol seems to be compatible with differential privacy. <sep> Concerns <sep> Can it be quantified (using some information theoretic bound) that only pairwise distances cannot leak much information? In the worst case, it can leak some information. For example if the pairwise distance is 0 for some pair, then each machine exactly knows the other's model update. However, I believe that using some assumption on distribution of model updates, we can still get some information theoretic guarantee. The three server algorithm in Appendix D seems to address this, but I wonder if we can say something for the two server algorithm too. <sep> Can it be extended to dimension-independent robust mean estimation techniques? The paper only considers distance based aggregators (with the exception of  (Alistarh et al., 2018), which I discuss later). These aggregators all seem to suffer from an error that grows with dimensions as d (see section 2 in (Wang et al., 2020). A recent line of works has given robust mean estimators that have errors that are dimension independent (Dong et al., 2019; Diakonikolas et al., 2017; Diakonikolas et al., 2016; Lai et al., 2016). However these use second order information like the empirical covariance matrix too. Can the algorithm proposed in this paper be extended to these algorithms too? <sep> Alistarh et al. (2018) also give dimension independent guarantees, but they require the workers to sample a new data point at every iteration, which may not hold for many FL systems. <sep> Score justification <sep> The paper tackles a very relevant problem for Federated Learning. Further, the proposed algorithm has nice theoretical guarantees and it looks like the algorithm can be easily implemented in a variety of large FL systems. <sep> References <sep> Wang, L., Pang, Q., Wang, S. and Song, D., 2020. F2ED-Learning: Good Fences Make Good Neighbors. arXiv preprint arXiv:2010.01175. <sep> Dong, Y., Hopkins, S. and Li, J., 2019. Quantum entropy scoring for fast robust mean estimation and improved outlier detection. In Advances in Neural Information Processing Systems (pp. 6067-6077). <sep> Diakonikolas, I., Kamath, G., Kane, D.M., Li, J., Moitra, A. and Stewart, A., 2017. Being robust (in high dimensions) can be practical. arXiv preprint arXiv:1703.00893. <sep> Diakonikolas, I., Kamath, G., Kane, D.M., Li, J., Moitra, A. and Stewart, A., 2016, October. Robust Estimators in High Dimensions without the Computational Intractability. In 2016 IEEE 57th Annual Symposium on Foundations of Computer Science (FOCS) (pp. 655-664). <sep> Lai, K.A., Rao, A.B. and Vempala, S., 2016, October. Agnostic estimation of mean and covariance. In 2016 IEEE 57th Annual Symposium on Foundations of Computer Science (FOCS) (pp. 665-674). IEEE.","This paper presents a secure aggregation method to ensure byzantine robustness. The reviewers thought that the idea was interesting, but had the following concerns. <sep> Relaxing the assumptions used in the theoretical analysis as much as possible <sep> Run more extensive experiments <sep> I encourage the authors to their feedback into account when preparing the revised draft."
"rating_summary | weakness | rebuttal_process | strength | weakness | decision  ==>  ==> Summary and Contributions <sep> The CAD reconstruction problem is defined as the recovery of the sequence of modeling operations used to construct the CAD model, from the raw geometry input (triangle meshes, point clouds, or, B-reps). The paper proposes a novel dataset as well as a generic framework implementing an MDP (Markov Decision Process) formulation for training a neural CAD reconstruction agent. <sep> This the first dataset which includes the sequence of CAD modeling operations as ground truth containing 8.5K+ human-designed real-world CAD models. The CAD reconstruction task is formulated as that of training a generic neural MDP agent and provided in an programming environment to the research community to train such models. Finally, a novel algorithm is provided as a baseline to solve the CAD reconstruction problem. <sep> Detailed Review <sep> The following is the detailed review of the paper, organized into strengths and weaknesses subsections. <sep> Strengths <sep> Relevance and Significance <sep> There is a large number of man-made objects that we interact with that are created using computer-aided design. The modeling steps, which are often lost, are important in understanding, editing, simulation and manufacturing such objects, and need to be reconstructed. This topic is of considerable importance to the CAD community. In addition, approaches, that can reverse engineer the generative process should be of general interest to the wider ML and computer vision community, as well. <sep> Relation to Prior Art <sep> The paper does a good job of presenting the prior art, identifying the challenges and need for the presented work. The dataset including the modeling sequences for real, human-designed CAD models is the first of its kind. Since it's a novel problem (first dataset of the kind), the proposed baseline implementing a learning-based approach to this problem is novel as well (though simplistic, see below). <sep> Reproducibility <sep> Since the entire dataset and the baseline is released in an open-source programming environment, it should be easy to reproduce and verify the results. <sep> Clarity <sep> The paper is written well and is easy to understand. <sep> Weaknesses <sep> Methodology <sep> This is largely a dataset paper. Introduction of a new dataset to the research community needs to demonstrate that the tasks to be solved on the dataset is not trivially addressed by the known state of the art. The paper falls short of demonstrating that. <sep> Agent model: Two models are considered – MLP (trivial embeddings based on vertex features) and seemingly trivial embeddings of MPNs. The paper doesn't present enough details about the choices made for obtaining meaningful embeddings. This seems to be the heart of the approach and the authors don't present a compelling approach or a comparative evaluation of a set of choices based on the SOTA for graph embeddings, to model the agent. <sep> Search: Similarly, the search is trivially implemented using a random rollout. There are much better search strategies available in the SOTA to bring to bear on the problem. <sep> In summary. I don't think that the presented baseline properly brings the SOTA to bear on the problem and thus demonstrates a need for additional research to be spurred on by this dataset. <sep> Novelty <sep> The presented approach is a straight-forward application of known techniques. <sep> Empirical Evaluation <sep> An important question to consider when proposing a presumably difficult new problem when addressed by known art is to investigate what aspect of the new problem really taxes the state of the art and to stage the experiments and analysis carefully for the same. There are many questions to be addressed here, for e.g. (a) How is stationarity (or, invariance over the topologies) achieved across training, validation and  test sets, (b) Does the training overfit (are the architectures used of enough capacity)?, (c) Is there generalization gap? If so, why? Etc. <sep> Assessment <sep> Though the problem seems relevant and of significance to the research community, the dataset of considerable value, the paper doesn't make a strong case whether and how this problem challenges the known state of the art. In its current form, I do not recommend the publication of this paper.","This paper received 4 reviews with mixed initial ratings: 4, 8, 5, 7. The main concerns of R1 and R2, who gave unfavorable scores, included limited methodological novelty beyond the data generation and insufficient empirical evaluation of state-of-the-art methods on the proposed dataset. The authors submitted a new revision with a summary of changes and provided detailed responses to each of the reviews separately: it addressed some of the concerns, but did not change the overall position of the reviewers. <sep> AC agrees with R3 and R4 that the proposed dataset and the environment may have certain practical impact and enable new research in learning CAD reconstruction. However, the contributions are indeed specific to a narrow CAD community, and R1 felt that the paper needs another round of peer reviews before acceptance, as a significant number of new results have been added during the discussion stage. After discussion with PCs, the final recommendation is to reject."
"abstract | weakness | decision  ==> This paper presents a differentially private method for training a generative model. The proposed method takes advantage of the Sinkhorn divergence to achieve robustness against the hyperparameters' choices. The authors also introduce a cost function enabling the generative model to generate images associated with a specific class label. The experimental results show that the proposed method outperforms the existing methods for learning a generative model in a differentially private manner. Furthermore, such a high accuracy can be achieved without the use of publicity available data. <sep> The strong points of this paper are as follows: <sep> The authors bring the Sinkhorn divergence-based learning of a generative model into the differentially private generative model learning problem. <sep> The weak points of this paper are as follows: <sep> The proposed method is not clearly explained and thus is suspicious in the guarantee of differential privacy. <sep> The presented method is a direct application of Wang et al.'s moment account technique with Zhu et al.'s Poisson sampling. The originality is considerably low. <sep> This paper has no theoretical and experimental analysis about robustness against a hyperparameter choice, while the authors claim it as a contribution. <sep> The proposed approach is not well motivated. We can use some differentially private classification algorithm if the objective is high accuracy in downstream classification. <sep> I recommend rejection of this paper because the proposed algorithm has low originality and is not well motivated. Also, the unclarity of the privacy guarantee is problematic. <sep> The authors do not give a clear explanation of the proposed method. In particular, it is unclear if the proposed algorithm guarantees differential privacy. I guess the authors employ either the composition theorem or moment account technique to prove the algorithm's differential privacy; however, there is no privacy proof of the proposed algorithm. I could not confirm that the proposed method ensures differential privacy. <sep> The proposed method is a straightforward application of the techniques from Wang et al. and Zhu et al. Also, defining the cost function as in Eq. 4 is a straightforward way to combine the multidimensional real-valued data and discrete label. I cannot find any original idea, except introducing the Sinkhorn divergence, in the proposed method. <sep> The authors claim that the proposed method is robust against the choice of its hyperparameters. However, there is no evidence to support the claim. A theoretical or experimental analysis of robustness is necessary to claim it. <sep> Why don't we employ the differentially private classification algorithm, such as M. Adabi et al. Deep Learning with Differential Privacy. In CCS'16. When the objective is high accuracy in the downstream classification, we can utilize such an algorithm directly. What is the benefit of employing the generative model-based privacy preservation? The differentially private classification algorithm can achieve high classification accuracy; for example, in Adabi et al.'s paper, the classification accuracy for MNIST with eps=10 is 97%; this value is significantly higher than that of the proposed method. <sep> Minor comments <sep> What is the definition of S^?","The paper proposes a DP method for generative modelling based on optimal transport. The reviewers agree that the novelty is limited in relation to prior work, while the results are not especially compelling either. So, even though this is a valid approach, correctness is not sufficient for acceptance at *CONF*."
"abstract | strength | weakness | decision | strength | weakness | misc  ==> Summary <sep> The paper studies the effect of different learning rates on finding performant pruning masks that can be applied to train a sparsified neural network in a lottery-ticket-style framework (i.e. train the original random initialization with the resulting pruning mask applied). The study hereby focuses on a particular setting: find a pruning mask using iterative global magnitude pruning (IMP) applied to various ResNets trained on Tiny ImageNet. The paper finds that a small learning rate is beneficial in finding a performant pruning mask, although the resulting network's performance may be worse, while a large learning rate is better suited to then optimize the sparsified network in the subsequent lottery-ticket-style training procedure. The authors further hypothesize that the per-layer prune ratios (LPRs) found by small learning-rate IMP masks are the main driving factor in increasing the performance of the sparse network in the subsequent training process. This is corroborated with a range of experiments where the LPRs from small learning rate IMP pruning masks is used to find masks (with pre-specified LPRs) using larger learning rates. <sep> Score <sep> I enjoyed reading the paper and found the resulting conclusions quite interesting. As pointed out by related work before [1, 2, 3] the learning rate can have a huge impact on the performance of lottery tickets and related experimental settings (such as lottery-ticket-style masks with random re-initialization of the weights). This paper provides additional experimental evidence for this phenomenon by separating the effects of training hyperparameter on the mask-finding procedure and the sparse-training procedure. <sep> However, I am not entirely convinced about the framing of the paper itself. I believe the experiments are valid and have merit on their own but in my opinion the authors mix two related but distinct concepts, namely: <sep> Lottery Tickets (LTs): Pruning masks that occur (in the general setting as pointed out by [2, 3]) early in training and not at initialization. Specifically, the LT hypothesis in the more general setting states that if the experiment is repeated exactly the same starting from that early iteration in training but with the applied pruning mask, it does not harm the performance of the resulting network. <sep> Pruning at initialization: Inspired by LT [4, 5, 6] (but also concurred work [7] with LT), various authors have proposed pruning methods that sparsify the network at the time of random initialization and then train the resulting, sparsified network. These methods usually perform better than when the network is pruned uniformly at random. <sep> Consequently, the conclusions that are drawn in this paper are somewhat confusing, it is really hard to discern the generality of the results, and to understand what is actually observed. Are these observations related to LTs or are the authors proposing another method, i.e. small learning rate IMP, to find performant pruning masks at initialization? <sep> I think that must be clarified before this paper can be accepted in order to for the paper to be beneficial to the community as a whole and to ensure that the observations are not just spurious effects of the particular experimental settings. <sep> Ways to Improve My Score <sep> Mainly, I think the paper must be properly contextualized in a clear setting that either studies LTs in general or studies pruning at initialization. Right now it sits in the middle and thus it hards to draw appropriate conclusions as the reader of the paper. Specifically, I can see two avenues for the paper to improve its framing of the results: <sep> Show that the conclusions hold for a more general LT setting. That is, use a version of Algorithm 1 that rewinds to a stable training phase, c.f. [3, 8], where it has been previously shown that valid LTs can be found using standard IMP. As a result that would naturally require the authors to repeat the experiments. <sep> Frame the procedure (small learning rate IMP) as another way to perform pruning at initialization, which performs well. That would require the authors to compare to other pruning at initialization methods [4-7] to understand the performance. Also this would raise the question of why one would use the method in the first place since it is computationally much more expensive than the methods of [4-7]. <sep> I have additional feedback in the ""Weaknesses"" section as discussed below. The minor feedback won't necessarily change my score but I believe can help you strengthen the paper upon publication. <sep> Strengths <sep> I commend the authors for a very clean and well-written paper. It is easy to follow, well-structured, and provides sufficient context. <sep> The experimental study seems to be carried out at a high level. Hyperparameters are clearly summarized and the provided level of detail is sufficient to reproduce the experiments. <sep> Each plot is clearly labeled and contains shaded error region (i.e. experiments were repeated multiple times). Plots are also all well-structured and easy to interpret. <sep> The study on the effect of separate hyperparameters for sparse training and discovering sparse masks could be helpful in guiding future research on pruning. <sep> The part about LPR being the main driven factor for the improved performance is really interesting. <sep> Weaknesses <sep> On top of the concerns I have previously raised, here are some additional points of feedback: <sep> What is the reason to mostly rely on Tiny ImageNet for drawing the conclusions? Most pruning work considers CIFAR and ImageNet. Since the conclusions of the paper are dependent upon previous papers it would be helpful to consider the inclusion of at least ImageNet. <sep> Why do the late rewinding experiments use epoch 2 as rewind epoch? I believe this is misleading. As stated in [3, 6] the ""stable phase"" of training where LTs can be observed in a general setting usually don't occur until later in the training. So these experiments don't really add value since epoch 2 is most likely not in the stable phase and so it might be confusing to the reader rather than helpful. <sep> This is mostly related to my main points raised in the ""Score"" section. Since all of these experiments consider rewinding to an unstable phase of training (i.e. back to the random initialization or a very early in training), I am really uncertain whether these conclusions hold for LTs or pruning at initialization. To me personally, the main conclusion is that if I want to use IMP for finding a pruning mask at initialization, I should use a small learning rate. But then again. Why would I use IMP to find a pruning mask at initialization if there is more efficient methods [4-7] for that? <sep> The work of [9] is drawing somewhat analogous conclusions about the importance of the distribution of LPRs for pruning at initialization. In particular, the authors conclude that mimicking the LPRs found during pruning at initialization with various methods is essentially sufficient to reproduce the accuracy of the resulting sparsely trained network. I couldn't find a version of [9] in a peer-reviewed venue, so it might count as concurrent submission but nonetheless I think it is crucial to compare the results. <sep> Other Minor Feedback <sep> Please clarify in the introduction that all observations are entirely limited to magnitude pruning. I don't think that any conclusions can be drawn for pruning in general and some of the text may hint at a more general phenomenon. <sep> You could add the appendix directly to the main document. It will be easier to read and jump back and forth between the main body and the appendix. <sep> References <sep> Rethinking the Value of Network Pruning <sep> The State of Sparsity in Deep Neural Networks <sep> Linear Mode Connectivity and the Lottery Ticket Hypothesis <sep> Picking Winning Tickets Before Training by Preserving Gradient Flow <sep> Pruning neural networks without any data by iteratively conserving synaptic flow <sep> Progressive Skeletonization: Trimming more fat from a network at initialization <sep> Snip: Single-shot network pruning based on connection sensitivity <sep> Comparing Rewinding and Fine-tuning in Neural Network Pruning <sep> Pruning Neural Networks at Initialization: Why are We Missing the Mark?","This paper explores the role of hyperparameters in the separate phases of a classic pruning pipeline: mask identification and retraining. Key observations include a set of the hyperparameters to search relative to a standard regime as well as the identification that the layerwise pruning rates from mask finding are intertwined with these hyperparameters and are what chiefly affects the eventual performance of the pruned network. <sep> The pros of this paper are that it works against the contemporary wisdom that the default hyperparameters for a model are the best for finding a mask for the model. Instead, there are improvements to be had by identifying a set of hyperparameters that lead to worse overall model accuracy, but better masks. Second, the work shows that the layerwise pruning rates are the key elements of these hyperparameters effect. The rates can in fact be transferred to more poorly performing network configurations and improve performance. <sep> The cons of this paper, as noted by the reviewers, are the somewhat unclear implications of the technique. The added guidance on directions to improve hyperparameters is valuable but does not necessarily provide a cost-effect strategy to find these. At its strongest, this guidance offers practitioners a recommendation to also consider hyperparameters for the initial model. <sep> The stronger, forward-looking implication is, instead, the connection to layerwise pruning rates. Specifically, while layerwise pruning rates have been demonstrated to be important in the literature (e.g., [1]), there has been a limited study into the exact nature of a good set of pruning rates versus a bad set of pruning rates. Where this paper stops short of a clear result, is if were to connect excessive pruning of the earlier layers, or simply the layerwise rates themselves to another property of the network (e.g., gradient flow, or capacity) that indicates the improved eventual performance. <sep> My Recommendation is to Reject. The paper's core experiments are well-executed. However, this final detail, closing the gap between the portability of these layerwise rates and a conceptual understanding, is a key missing component. Once done, that will make for a very strong paper. <sep> [1] AMC: AutoML for Model Compression and Acceleration on Mobile Devices. Yihui He, Ji Lin, Zhijian Liu, Hanrui Wang, Li-Jia Li, Song Han. EECV, 2018"
"misc | weakness  ==> In this paper, the authors propose a novel manifold learning method, via adding a locally isometric smoothness constraint, which preserves topological and geometric properties of data manifold. Empirical results demonstrate the efficacy of their approach. The authors also show that the reliability of tangent space approximated by its local neighborhood is essential to the success of manifold learning approaches. <sep> Overall I found the ideas in the paper somewhat interesting. Many aspects were unclear to me (refer below). <sep> In the abstract, the authors claim that in their first step there is no loss of topological information. How did the authors measure this i.e. the sparse coordinate transformation step does not result in any loss of topological information ? Do the results in the paper demonstrate this ? <sep> I was not clear to me as to how the authors in their approach are preserving topology and in general geometry of the space while reducing the dimension of the space simultaneously. I would like the authors to add a section on this and/or improve the clarity of their presentation. <sep> What is the intuition for the two step process of their approach ? It definitely fits the encoder-decoder framework but is there some other reasoning behind this approach ? <sep> In section 3.3, the authors introduce the orthogonal loss wherein they force the weight matrices to be orthogonal. After training, are the weights orthogonal or how close to orthogonal are the weights ? Can we possibly add some other constraint in place of this ? <sep> Given the complexity of the model and the different loss functions associated with the objective, how much overhead is involved in model training and execution. Do the authors plan to share any time complexity results so that we can compare training/execution times with other state-of-the-art models ? <sep> In the appendix section A.1, what is the intuition behind the exact mathematical form of the expressions used for the Trust and Cont metrics ? I was curious with regards to the exact mathematical expressions used i.e. the different constants and factors involved. <sep> I did not quite understand the notation of Figure 2. What is the meaning of the different colored arrows and boxes indicate and how are they used ? There is not discussion on this as well. Similarly for Figure 4 for which there is no analysis or discussion. I would like the authors to include these without which the notation is cumbersome to follow. It is surprising that there is no additional discussion or analysis of the different figures included in the paper, given the captions are not sufficient by themselves. <sep> How are the authors deciding on the values of the different hyper-parameters i.e. gamma and alpha in the Appendix ? Overall it felt very heuristic and I did not quite understand how the authors decided on the values used. <sep> Some of the loss based definitions in Section 3.3 are unclear to me. In case the authors used some standard/other loss functions from some paper, they could have added those definitions in the main paper or the Appendix etc. I found that the paper was not self contained as such and I had to navigate to the references quite a bit. <sep> I found quite a few typos in the submitted draft. Kindly proofread and correct these. Overall I felt that the paper does not quite achieve lossless compression as well as there is only marginal improvement in terms of results when compared against other approaches. The approach presented in the paper in not clear in many parts.","While reviewers find the ideas in the paper interesting, they also raise several major concerns. <sep> In particular, R1 and R4 find the claims of ""invertible"" and ""lossless"" to be potentially misleading. <sep> The bijective property is achieve on the first stage (L-1 layers) due to a sequence of one-to-one mappings, as is done in previous work (e.g. i-RevNet) so the novelty is limited. As stated by R3, since the paper is a combination of previous methods, the writing should be substantially improved to clarify what the real, new contributions are. The interpretation of the results (e.g. Figure 4) should also be better explained."
"abstract | weakness | decision | suggestion  ==> UPDATE: After reading through all other reviews and responses by the authors, I share the concern that the theoretical justification of the paper is lacking as the connection between the truncation error and the improved algorithm performance is not rigorously proven. Therefore, I have reduced my score. <sep> Summary: <sep> The paper studies the well-known Nesterov's accelerated gradient method and shows the rate of convergence to the solution of an ordinary differential equation recently proposed by [Su et al, 2014]. Motivated by the proof, the authors then derive a new accelerated method with a faster rate of convergence than the original Nesterov's method, which is shown to be more stable than the original Nesterov's method when the step size is large. The method is combined with the proximal operator into a new algorithm referred to as modified FISTA, which is then applied to the matrix completion problem. <sep> Strengths: <sep> The paper proves the convergence rate of Nesterov's method to the ODE proposed by [Su et al, 2014]. This proof then motivates them to derive a new faster accelerated method where the truncation error has a higher order of O(h4) compared to O(h3) in case of Nesterov's method. <sep> It is shown in two simple examples that the new method is more stable as it can work with larger step sizes. <sep> The method is applied to a matrix completion problem, where it is shown to have faster convergence than standard FISTA and Nesterov's gradient method. <sep> Concerns: <sep> In Section 2.2., the purpose of Lemma 1 and Lemma 2 is not clear without looking into the proof of Theorem 2 in the supplementary material. The flow of the paper could be improved if an intuition was given of which role they play in the proof of Theorem 2. <sep> Similarly, to understand the motivation for the derivation of the new accelerated method in Section 3, one is required to look at the proof of the convergence in the supplement. Also here it would help to provide a detailed motivation for the derivations already in Section 3. <sep> In the numerical results in Figure 1, the gap |F(xn)−F(x∗)| (y-axis) does not seem to monotonically decrease but jump up and down erratically. Also there are periodic wave-like patterns visible in the plot. Why do we see those patterns? <sep> The resulting accelerated numerical method is never explicitly written down, only the specific version derived for the matrix completion problems. The paper would be better understandable if the general numerical scheme (accelerated method) was written down in form of an algorithm after Section 3. <sep> In the end of Section 4, a reference to Algorithm 2 is missing. Moreover, Figure 2 and Figure 3 are never referenced in the text. <sep> Page 2, after (1.2) -> ""achive"" -> ""achieve"" <sep> Conclusion: <sep> The proposed method provides a theoretical contribution to the understanding of Nesterov's accelerated gradient method. Moreover, a novel algorithm is proposed which is shown to have a faster convergence to the underlying ODE. In the paper this is shown only for a matrix completion problem but I feel that this new algorithm could be adopted by the community if further experiments prove its worth. On the other hand, the flow and presentation of the paper could be improved. Overall this is a borderline paper but its merits may outweigh its flaws.","The paper studies a high-order discretization of the ODE corresponding to Nesterov's accelerated method, as introduced by Su-Boyd-Candes. The main claim of the paper is that the more complex discretization scheme leads to a method that is more stable and faster. However, the theoretical claims do not seem sufficiently supported, and the experimental results are insufficient to judge the usefulness of the proposed approach. Thus, the reviews could not recommend acceptance, and I concur. The authors are advised to revise the paper to provide more theoretical and experimental evidence for usefulness/competitiveness of the proposed approach, and resubmit to a different venue."
"strength | weakness | decision  ==> This paper addresses disentanglement in the latent space of autoencoders. To this end, it combines ideas from four existing papers, namely the reconstruction loss of the Wasserstein autoencoder, the regularization term decomposition from the total correlation autoencoder, and entropy estimation using minibatch-weighted sampling or the density-ratio trick. This combination certainly makes sense, as it brings together methods that have previously been shown to work well in isolation. <sep> The main part of the paper is devoted to an empirical evaluation of the new autoencoder training procedure. The new method is compared against various baselines in terms of L2 reconstruction error and three disentanglement scores on four toy datasets. In addition, latent space traversals on 3Dchairs and CelebA are shown to qualitatively demonstrate the disentanglement capabilities of the proposed methods. <sep> Unfortunately, the description of the experiments is not very precise. <sep> The role of the hyperparameter gamma remains unclear. In the ablation study, the authors simply set gamma=beta without further explanation, and in the comparison, they just state ""we first tune gamma"" and ""for gamma >1, better disentanglement is obtained"", again without further explanation. <sep> In the comparison experiment, they report results for the values of beta that achieve ""an overall best ranking on the four different metrics"" without explaining what an ""overall best ranking"" is. Choices like this must not be taken lightly, as the analysis in ""Why rankings of biomedical image analysis competitions should be interpreted with care"" (Nature Communications 9: 5217, 2018) impressively demonstrates. <sep> The experiment in figure 2 seems to have three degrees of freedom (the data instance x, the latent index i, and the size of the modification in direction z_i). However, only two degrees of freedom are shown, and it remains unclear from the caption and associated main text, which ones. Moreover, I cannot deduce justification for the statement ""all methods .. learn to disentangle, capturing four different factors"" from the figure -- I do not see any obvious disentanglement. <sep> The bigger problem with the paper, however, is the question: What have we learned from these experiments? The rankings in table 1 are pretty inconsistent between different metrics, and the corresponding figure 3 appears to be cherry picked, as the ScreamdSprites is the dataset where the proposed methods perform best. <sep> I also do not agree with the claim that ""TCWAEs achieve good disentanglement"" on real-world datasets. Figure 4 shows severe entanglement between unrelated factors. For example, the size feature for the chairs also changes the type of chair. All features in the CelebA examples have a tendency also to change the background appearance. The gender feature dramatically influences person identity in the MWS results, whereas it does not change the gender at all in the GAN variant. Substantial variations in person identity are also visible in most other examples. <sep> In summary, while the paper provides numbers, it lacks new insight. In light of mathematical proofs indicating that the true generative factors are generally unidentifiable in non-linear unsupervised settings (cf. the work of Aapo Hyvärinen and others), I am skeptical that heuristic trial-and-error investigations of disentanglement like the present one will yield interesting results. In a sense, this is also acknowledged by the authors, who merely state in the conclusion that ""our methods achieve competitive disentanglement on toy data sets"" -- that's not much, given the effort that went into the experiments.","There were both positive and negative assessments of this paper by the reviewers: It was deemed a well written paper that explores cleanly rederiving the TC-VAE in the Wasserstein Autoencoder Framework and that has experiments comparing to competing approaches. However, there are two strong concerns with this paper: First, novelty appears to be strongly limited as it appears a rederivation using known approaches. Second, two reviewers were not convinced by the experimental results and do not agree with the claim that the proposed approach is better than competing methods in providing disentangled representations. I agree with this concern, in particular as assessing unsupervised disentanglement models is known to be very hard and easily leads to non-informative results (see e.g. the paper cited by the authors from Locatello et al., 2019). Overall, I recommend rejecting this paper."
"abstract | weakness | decision  ==>  ==> The paper proposes a method to improve the downstream performance of a pretrained Transformer on NLP tasks. The core idea is to not only use the output of the last Transformer layer for prediction, but let the model decide how to fuse the information from intermediate layers as well. To dynamically decide which intermediate layers to use depending on the input example, the model uses a mechanism conceptually similar to self-attention, which yields a normalized importance score for each layer. The importance-weighted sum then yields a complementary representation to the last layer. Lastly, another network produces a final, integrated representation from the output at the last layer and the complementary representation, which is then used for prediction. <sep> The model is evaluated on the GLUE benchmark. <sep> Strengths: <sep> The paper is well written and the experimental evaluation seems correct. The paper has a nice ablation study which shows that the learned importance scores, the complementary representations, and the fusion network are needed to reach the model's full performance. <sep> Weaknesses: <sep> The main weakness is that the proposed extension to the baseline is relatively complex and rather heavy-weight in terms of new parameters (introducing ~25% more parameters compared to the baseline according to Table 2), yet only achieves a very marginal relative improvement of 0.2 percent over the baseline. It seems likely that this improvement could be achieved through much simpler means, e.g., additional self-attention layers on top of the last pretrained layer. This is supported by the fact that the paper's analysis of the proposed importance score mechanism doesn't show comprehensible patterns (or the paper doesn't talk about it). <sep> I lean slightly towards rejection, because, although the proposed model is reasonable and could have potential, the experiments do not currently demonstrate that potential, and I hence expect it hard for the community to learn from this paper. <sep> Questions: <sep> You cite several papers which demonstrate that different layers of pretrained Transformers encode different information, which is the motivation for your architecture. However, the cited sources use a feature extracting approach, i.e., they don't fine tune the encoder on their target task. In the finetuning scenario (yours), can't we expect the model to learn to simply forward the relevant information from intermediate layers to the last layer? <sep> In Section 5 you analyze which intermediate layers are used for which task. You state that we can see that the model learns to prefer different layers for each task, but don't go into detail. Can you relate the nature of the task to the layers it produces? For example, why does QNLI rely on layers 2-4 to some extent despite being a rather high-level understanding task? <sep> In Figure 2, right side you observe that different examples tend to different layers. Did you qualitatively inspect the examples and try to find a pattern/link to their preferred intermediate layers? <sep> Suggestions: <sep> RoBERTa already achieves super-human results on the GLUE benchmark, meaning that it is probably already close to some upper limit. This makes it even more difficult for your model to substantially improve upon it. I would consider evaluating it on the SuperGLUE benchmark instead, since there RoBERTa is not so close to human performance yet. <sep> Your model is designed to make use of representations at lower layers if a task requires it. But all the tasks in your experiments are evaluated on high-level natural language understanding tasks, which typically require representations at higher layers. This makes it more likely that your model does not improve much over the baseline, because the last layer will arguably already contain much of the information needed for the tasks. I think you should consider different tasks in your experiments to increase the chance of significant improvements. <sep> In the current state, it is unclear whether your improvements are coming from the specific model you chose or merely from the fact that you added a lot of parameters. As a control, you could include an experiment where you simply add several self-attention layers on top of RoBERTa such that you match the number of parameters of your model.","This paper proposes a new mechanism, called HIRE, to improve the down-stream performance of a pre-trained Transformer on NLP tasks. Different from directly using the last layer of transformer, the proposed model allows the system to dynamically decide which intermediate layers to use based on the input through some sort of gating. The model is evaluated on GLUE, a benchmark for natural language understanding. My major concerns are the following <sep> the gating mechanism on using intermediate sentence representation is not new, as pointed by some reviewers, although its implementation on transformers is still interesting. <sep> the empirical part is not convincing enough: a) GLUE data set is relatively simple, the authors should try something more complex, b）the improvement over baseline is rather modest, which could be achieved with simpler modification. <sep> I'd suggest to reject this paper."
"misc | decision  ==> -Summary <sep> The authors consider self-play in tabular zero-sum episodic Markov game. In this setting, the goal is to learn an \\epsilon approximate of the Nash equilibrium of the Markov game while minimizing the sample complexity, i.e. the number of episode played by the agent.  They present Optimistic Nash <sep> Value Iteration (Nash-VI) that output with high probability a pair of policies that attains an \\epsilon approximate Nash equilibrium in O(H^3SAB/\\epsilon^2) episodes where H is the horizon, S the number of states,  A and B the number of actions for the max-player respectively min-player. This rate matches the lower bound of order \\Omega(H^3S(A+B)/\\epsilon^2) by Jin et al. (2018) up to a factor min(A,B). They extend this result to the multi-player setting with Multi-Nash-VI algorithm of sample complexity O(H^4S^2\\prod_i A_i/\\epsilon^2) where A_i is the size of the action space of player i. The authors also provide VI-Zero an algorithm for reward-free exploration of N-tasks in Markov game with a sample complexity of O(H^4SAB log N/\\epsilon^2) and a lower bound of order \\Omega(H^2SAB/\\epsilon^2). <sep> -Contributions algorithmic: Nash-VI (significance: medium) <sep> theoretical: Nash-VI sample complexity of order O(H^3SAB/\\epsilon^2) (significance: high) <sep> algorithmic: VI-zero (significance: low) <sep> theoretical: VI-zeros sample complexity of order O((H^4SAB log N/\\epsilon^2) (significance: medium) <sep> algorithmic: Multi-Nash-VI (significance: medium) <sep> theoretical:  Multi-Nash-VI sample complexity  of order O((H^4S \\Prod_i A_i/\\epsilon^2)  (significance: medium) <sep> theoretical:  lower bound for the sample complexity of reward-free exploration in Markov game of order \\Omega(H^2SAB/\\epsilon^2) <sep> -Score justification/Main comments <sep> The paper is well written. The authors use the same technical tool as Azar et al. 2017 to get a sharp dependence in the horizon H and the state space size S. Precisely they use Bernstein bonuses in combination with the Law of total variance in the Markov game to obtain the H^3 and only concentrate the empirical transition along the optimal value function to push the S^2 into second-order terms. <sep> I have mixed feelings concerning this paper. On one hand, I think the contributions deserve to be published on the other hand I not convinced by the proof. Indeed the proofs are dangerously close to a sketch of proofs (see specific comments below) when it is not the case (see proof of Theorem 6). Even if I think most of the issues are fixable, considering the number of corrections required, I would need to read the updated version to assert if the results are correct. <sep> The algorithm that attains a dependence A+B instead of AB is almost adversarial, do you think it is possible to obtain the same result with a model-based algorithm that uses the stochastic assumption, or do you see a fundamental reason why it will be impossible? <sep> -Specific comments <sep> P1: What do you mean by information-theoretic lower bound? <sep> P2, Table 1: It could be interesting to compare these algorithms on a toy example. At least implement Algorithm 1 to prove it is feasible. <sep> P3: precise what is \\pi when you introduce D_{\\pi}Q <sep> P5: Which (estimation) error the bonus \\gamma is compensating precisely? <sep> P6, Algorithm 1: The definition of the empirical transitions and the bonuses are not clear when N_h(s_h,a_h,_b_h) = 0. <sep> P7, comparison with model-free approaches: could you also compare them in term of computational complexity. <sep> P12, Non-asymptotic […] assumptions: precise what do you mean by highly sub-optimal because at the end Algorithm 1 is also sub-optimal by a factor min(A,B). <sep> P15, (Approximate CE): I think that \\phi \\circ\\pi is not a good choice of notation since it is not really composition. <sep> P15, Th 15: Could we, as in the two-player, deduce a Nash equilibrium by only computing CCE and get a polynomial-time algorithm? <sep> P19,Lemma 19: I do not understand the statement of the lemma, since \\bar{V}^k_{h+1}-… is a random variable you need to precise in which sense the inequality |V|(s) \\leq … holds. In particular, considering how you use it in the sequel it is not almost surely. Furthermore in the proof, you need to deal with the case when N_h^k = 0 and you cannot apply the empirical Bernstein Bound from Maurer and Pontil like that since you have a random number of samples. An additional union bound over the possible value of N_h^k is required and you need to prove that conditionally to this choice the samples are independent…. <sep> P19, proof of Lemma 19: you need to precise which event you consider in order to be able to invoke Lemma 18. <sep> P20, top of the page: the two equalities are wrong in (10) they are inequality because of the clipping for the first one and because \\hat{p}_h^k (\\bar{V} ….) \\geq 0. What do you mean by ""with high probability"" exactly? And since it is Hoeffding inequality the second term in 1/n is not necessary. <sep> P21, proof of Theorem 3: Again you need to precise with the event you consider to be able to call Lemma 18 or 19 … and guarantee that this event is of probability at least 1-p. Currently, a lot of union bounds are hidden whereas they should be treated properly.\\zeta_h^k and \\xi_h^k are martingales with respect to which filtration? <sep> When you apply Azuma-Hoeffding inequality you implicitly imply that you can upper-bound all the \\zeta_h^k \\xi_h^k by the same constant (moving the bi O outside the sum) but you cannot because they are not necessarily positive. <sep> For the pigeon-hole argument again you should consider the case N_h^k=0 <sep> After (12) it is \\sqrt(H^3SABT). <sep> P22, Lemma 20: same remarks as Lemma 18 and 19. <sep> P23, Lemma 21: Because of the big O after ""these terms […] separately"" it seems that the constants in Lemma 21 are wrong. Furthermore, here you apply Hoeffding inequality plus a union bound over an \\epsilon-net or control in KL to obtain the H^2\\sqrt{S/n} bounds. You need to explain this properly. <sep> P24, proof of Theorem 4: same remark as for the proof of Theorem 3. For (i) it is not only the Law of total variation but also because of the Hoeffding Azuma inequality ( and in Azar et al. it is only proved for stationary transition). <sep> P27, proof of Theorem 6: since you only prove the theorem for H=S=1 you should only state this case and keep the general case as a conjecture. <sep> S=H=1 in Markov game (where there is nothing to learn) is not equivalent to a matrix game. What do you mean exactly by reward-free matrix game? The agent does not observe the rewards, but in this case, there is nothing to learn, no? I do not think it easily generalizes to the Markov games setting. Could you also obtain the factor \\log(N) in the lower bound?","The reviewers, AC, and PCs participated in a very thorough discussion. AC ultimately felt that the work was unfinished, and in particular that details in the proofs still needed work before publication."
"rating_summary | weakness | suggestion  ==> Summary <sep> In this work, the authors propose the M-Layer, a neural network ""layer"" that consists in affinely transforming the input to a matrix, applying its matrix exponential, and applying another affine transformation. The parameters describing the affine transformations are the learnable parameters. <sep> The authors provide a ""universal approximation result"" that shows that when using a sufficiently large latent space, the matrix exponential is able to replicate arbitrary polynomial functions of the input data, as well as periodic functions. <sep> In numerical experiments on synthetic data (determinant of a matrix, swiss roll), time series data, as well as image data (MNIST, CIFAR10, SVHN), a single M-Layer compares favorably to neural ODEs as well as a very simplistic RELU network, while being outperformed by augmented neural ODE's. <sep> The authors furthermore provide robustness certificates for the M-layer, and point out its connections to Lie-theory and dynamical systems interpretations of deep learning. <sep> Decision <sep> On the positive I think that the idea of going beyond component-wise nonlinearities is interesting and the matrix exponential, with its rich mathematical structure, makes for an intriguing candidate. The authors analyse this proposal from many different view including robustness, approximation capability, computational implementation, and empirical results. <sep> On the negative side, I find the comparison to conventional neural networks not entirely fair. In particular, the claims of improved representational power compared to traditional neural network layers seem exagerated and vague. <sep> The connections drawn to Lie theory and dynamical systems seem superficial in that they mostly restate properties of the matrix exponential instead of providing interpretations of the M layer. <sep> Finally, the numerical experiments are comparing a single M-layer to a very shallow dense RELU network. While I appreciate the intention of the authors to keep the experimental setup simple, comparing the performance of individual layers is a very artificial situation that does not seem to give any insight if M-layers are useful as layers for deep learning. <sep> If they are not, however, this casts doubt on the usefulness and relevance of M-layers in general. <sep> While I do think that the idea of an M-layer is interesting and in particular the relative stability bounds are promising, I feel that the message of the paper is diminished by the unclear comparison to existing deep learning methods. <sep> This makes it difficult to assess the relevance of the work which is why, for now, I tend to recommend rejection. <sep> Detailed comments <sep> Conceptual comparison to RELU: <sep> ""While highly successful in practice, this approach also has disadvantages. In a conventional DNN, any two activations only ever get combined through summation. This means that such a network requires an increasing number of parameters to approximate more complex functions even as simple as multiplication. This approach of composing simple functions does not generalize well outside the boundaries of the training data."" <sep> I would argue that this is misleading, since the results in later layers of a deep neural network depend on the activations in a highly nonlinear way. Furthermore, I believe there exist universal approximation results even for shallow neural networks with arbitrary width, just like the universal approximation results provided for M-layers are true only in the limit of infinitely large matrix exponentials.  They also do not provide any evidence for their claim that the lack of generalization is linked to the composing of simple functions. The authors do provide some experiments to this end on swiss roll and meteorological datasets. However these examples seem somewhat taylor-made for the exponential map approximation. Furthermore, the RELU network used is not even able to fit the training data well, which suggests this issue might be more due to a lack of model capacity. <sep> Lie Algebras and dynamical systems <sep> My understanding is that these sections mostly recapitulate how the matrix exponential appears in lie theory and the solution of linear ODE. However, I don't think they provide a compelling interpretation of the M-layer in terms of either of those points of view. In neural ODE for instance the input data to the layer can be thought of as a state of a dynamical system that is determined by the model weights. <sep> In the M-layer, instead, the input becomes part of the system matrix, then the propagator of this system matrix (the exponential map) is computed, and finally this propagator is applied to initial conditions given by another set of weights by the M-layer. <sep> I cannot think of a meaningful interpretation and I don't think the authors have provided one, either. <sep> Numerical experiments <sep> Reiterating what I wrote earlier, I really think that the performance of deep neural networks constructed with M-Layers needs to be compared to that of conventional deep neural networks to establish the relevance of the M-layer as a neural network layer. <sep> In a nutshell: if the differences between M-layers and RELUs were to disappear when working with deeper networks, then why should one still use the M-layer. <sep> ===================================================================================== <sep> After reading the rebuttal and the other reviews I still lean towards rejection, the main reason being that I am not convinced at this point that exponential nonlinearities are a direction worth pursuing. <sep> I furthermore find the passages on dynamical systems and lie groups to be still lacking, for the same reasons as detailed above.","This paper was reviewed by 4 reviewers who scored the paper below acceptance threshold even after the rebuttal. Reviewer 4 is concerned about motivation, Reviewer 2 rightly points out that there exist numerous works that use some form of spectral layers in a deep setting on challenging datasets - something lacking in this work. Reviewer 3 is concerned about limited discussion on lie groups and the overall benefit of expm(.). Reviewer 1 reverberates the same comments regarding insufficient experiments, comparisons and limited motivation. We encourage authors to consider all pointers given by reviewers in any future re-submission."
"abstract | weakness | rebuttal_process | decision  ==> UPDATE: <sep> As the authors were already aware of the zero loss case and analyzed this previously, I am confident that the authors can address this to the point in an updated version. With this I think this is a good paper that should be accepted. <sep> ########################## <sep> Summary: <sep> The papers addresses the setting of overparametrized models that interpolate the training data, and the related double descent observation in a kernel setting. The overparametrized case of interpolating models is not yet that well understood, but of importance as the success of neural networks is closely related to that setting. This paper shows that the minimum norm interpolating solution is optimal (among all interpolating solutions) with respect to a derived bound on the expected leave-one-out stability, and thus also optimal in the same sense with respect to the excess risk. <sep> ########################## <sep> ########################## <sep> Pros: <sep> The paper is well written, to the point, and technically (mostly! see cons) sound. <sep> To the best of my knowledge this particular stability analysis is novel, and thus warrants a publication, in particular as the overparametrized case is not that well understood as of yet. <sep> ########################## <sep> ########################## <sep> Cons: <sep> I actually don't have many cons, I enjoyed the paper. There is however one thing that the authors seemed to have missed: <sep> The term V(hat(f_S),z_i) is zero, as hat(f_S) interpolates the training data and z_i is part of it. That doesn't mean that any of the theory is wrong, but that creates two problems in my opinion: <sep> The story about leave-one-out stability does not make sense anymore. In fact the expected leave-one-out stability is just the expected risk for interpolating solutions. <sep> I would imagine that most of the results can be simplified because of that. I could imagine that all the results hold with essentially all terms regarding hat(f_s) being removed. I think the qualitative conclusions would remain the same though. <sep> My suggestion would be to leave the paper as is (the results as far as I see also hold for not interpolating solutions), and then discuss the interpolating solutions as an extra case. <sep> ########################## <sep> ########################## <sep> Scoring: <sep> For now I will have to vote for a rejection, as I am not sure if the problem that I mention can be addressed in one rebuttal phase. But I am happily convinced otherwise, or convinced that I am wrong in any other way. <sep> ########################## <sep> ########################## <sep> Additional feedback: <sep> When I first read the title I thought that you wanted to show minimal norm solutions are NOT stable, as it has 'minimal' stability. I understand now that minimal refers to the numerical value of the stability definition, but still the wording was somewhat confusing. (Just to consider, no need to change for me if you think it is correct like that) <sep> Equation (2) and also bit later. You use a comma to separate an index ""S,i"", fairly unusual I would say. <sep> Remark 4, rework first sentence. <sep> Equation (7), in the very basic property of RKHS the kappa would depend on x, for you that seems to follow with one of your assumptions + cauchy-schwarz. I would not consider that a basic property. <sep> ##########################","The paper investigates the average stability of kernel minimal norm interpolating predictors. The main result <sep> establishes an upper bound on a particular notion of average stability for which it is well-known that it <sep> can be used to bound the generalization error. This upper bound holds for all interpolating predictors <sep> from the RKHS, but it is minimized by the minimal norm predictor. <sep> While at first glance this result looks highly interesting, a closer look reveals that the significance of the results <sep> crucially depends on the quality of the derived upper bound. Here two reviewers raised concerns, since it is <sep> by no means clear that even the optimized upper bound produces meaningful bounds on the generalization <sep> performance. The authors tried to address these concerns in their response and promised to update their <sep> paper accordingly. As a result, they added a paragraph on page 8. Unfortunately, this paragraph remains extremely <sep> vague, in particular if it comes to the more interesting case of non-linear kernels. Here, the authors briefly refer to <sep> a paper by El Karoui but no details are given. However, looking at El Karoui's paper it is anything but obvious whether <sep> the results of that paper lead to reasonable upper bounds on the average stability for a sufficiently general class <sep> of distributions. <sep> As a result, I view the paper under review to be premature since it remains unclear if the observed optimality of the minimal norm solution is a real feature or just an artifact due to an upper bound that is simply too loose to make any conclusion."
"abstract | strength | weakness  ==> Summary: <sep> The author proposed a set of tools to analyze the properties of the learned neural network-based optimizers. <sep> This toolset consists of (i) update function visualizer (ii) linearization of the update equation around the fixed point. The author takes an RNN-based optimizer as an example and analyzes it in the empirical section. <sep> The author investigates several properties including (1) momentum using linearization (2) gradient clipping using the function visualizer (3) learning rate schedule and (4) learning rate adaptation. It shows that the learned optimizer indeed possesses some useful properties. <sep> Review: <sep> Clarity: In general, some parts of the paper is clearly written and motivated such as the problems of the current learned optimizer.  However, the paper quality can be improved if some sections can be more clearer. For example, in section 3.2, the author only mentioned the high-level idea of the proposed tool (linearization) without a detailed mathematical introduction. It would be much more helpful the author can give a brief introduction to the linearization in the Appendix, rather than describe it in words and postpone it in the momentum analysis section. <sep> Novelty: The tool seems to be novel but the idea of linearization is not new, which has been used in other analyses of the dynamical systems as mentioned by the author. As for the function visualizer, it seems to be a plot of the parameter update, which is standard. <sep> Technical soundness: <sep> I have checked some of the details, it seems to be correct. <sep> Significance: <sep> The proposed tools may be useful for analyzing neural network-based optimizers, and help to diagnose their behaviors. The tools are easy to implement and can be applied to a broad class of optimizers. <sep> Weakness: <sep> Although the paper proposed an interesting way to visualize some of the properties of learned optimizers, I think the paper in the current stage is not enough for a full conference paper. Here are some of my concerns and questions for the author: <sep> The proposed tools are mainly used to visualize some of the properties of the learned optimizer. This may be helpful for visualizing some properties but it provides no analysis on why it has those behaviors and what advantages these properties when optimizing a function f. Or what properties of the function f can induce such behavior of the learned optimizer. <sep> The author only demonstrates the usage of the tools in analyzing a single/isolated property. How these properties are combined is not analyzed. <sep> The author only analyzes one NN-based optimizer. are there any other forms of the leaned optimizers? What advantages of those optimizers compared to the one used in the paper? Can your analysis confirm those advantages and provide possible reasons? This would be stronger evidence to back up the validity of the proposed method compared to the current analysis without comparisons. <sep> As mentioned in the clarity, better use mathematical equations to explain certain terms rather than words. For example, in section 3.1, the slope w.r.t what, the gradient g? <sep> In section 3.2, I am a bit confused about the difference between the fixed point and convergence point? Any examples of scenarios that it is a fixed point but not a convergence point? <sep> In section 4.1 (momentum), for the first figure in the bottom row (Figure 3), is it the eigenvalue point in the regression task? What about the first one used in Figure 7 (Appendix B). Are they the same? <sep> In section 4.3, your analysis shows the optimizer has this autonomous behavior. What advantages does this behavior provide? The author also mentions that the parameters should not be updated during the autonomous behavior. How can we confirm this through the plot (Figure 5)? <sep> I am not fully sure what you mean in the second paragraph (section 4.4). Do you mean that at the convergence points, you manually give some gradient perturbations, and the fixed points are moved away from the convergence point to get the S-curved shape?","A line of work since 2016 has investigated learning NN-based optimisers, which produce optimisation updates by processing loss/gradient info with neural networks. This paper tries to understand the learned dynamics of these NN-based optimisers by linear approximation to the learned non-linear dynamics. Visualisation of these approximations are shown on 3 optimisation problems: linear regression, Rosenbrock function, and a toy neural network classification problem, with the hope of covering different types of objective landscapes. <sep> Reviewers agreed that the paper studies an important research question, which would interest researchers working on meta-learning learning algorithms. However there are several major concerns raised by the reviewers: (1) the example optimisation problems are toyish, and (2) the paper does not explain very well the link between the visualised behaviour and the better optimisation results, i.e. it is unclear to the reviewers why the learned dynamics lead to better optimisation results. <sep> While I am not too concerned about issue (1), I think issue (2) is a significant one, flagging that the clarity of the paper needs to be improved. Ultimately, the paper is motivated by the question ""How is a learned optimizer able to outperform a well tuned baseline?"", so a reader would expect some clear explanation towards answering this question. Also some reviewers are concerned about the fact that only the RNN-based optimiser in Andrychowicz et al. (2016) is analysed; since there exists other forms of learned optimisers, focusing on studying only one type of them might lead to early conclusions that are not so accurate."
"abstract | strength | weakness | decision  ==> ########################################################################## <sep> Summary: <sep> This paper presents a novel variance reduction method which can adapt to any sampling-based GCN methods (inductive GCNs). The paper draws the idea from VRGCN that integrates the historical latent representations of nodes computed with full Laplacian to approximate the that computed with sampled sparse Laplacian. The variance reduction is implemented on both node embedding approximation, as well as layer-wise gradient computation in back-propagation. The resulting algorithms lead to faster convergence rate. <sep> ########################################################################## <sep> Reasons for score: <sep> Overall, I vote for accepting. The proposed variance reduction techniques can successfully accelerate convergence of any sampling method, according to the experiments, while also enjoys theoretical guarantee. Yet the novelty of the proposed SGCN+/SGCN++ algorithms themselves is a little limited to some extent. <sep> ########################################################################## <sep> Pros: <sep> The authors introduced a doubly variance reduction which can effectively reduce the node approximation variance the layer-wise gradient variance of the existing sampling based GCN methods and accelerate convergence. <sep> This paper also provides thorough theoretical analysis and convergence guarantee of the proposed algorithms. <sep> The  authors have conducted comprehensive experiments using a variety of sampling-based GCN methods as building blocks. The quantitative results clearly demonstrate the effectiveness of the proposed algorithms. The authors also provide detailed empirical analysis on the training time / GPU memory usage of the proposed method. <sep> ########################################################################## <sep> Cons: <sep> To better illustrate the idea the variance reduction, the authors could compare the proposed algorithms with a vanilla full-batch GCN (instead of the mini-batch training version Exact used in this paper, and it could be evaluated on smaller datasets like Cora). This baseline may serve as a theoretical upper bound of SGCN+/SGCN++. <sep> The proposed SGCN+ and SGCN++ requires a full-batch forward/backward computation every k step. As the authors suggest, this might hinder the scalability of SGCN++ on extremely large graphs. The authors hence propose a variant of SGCN++ which applies a large-batch approximation. The authors could also provide  an alternative version of SGCN+ without full-batch that only reduces the zeroth-order variance, and evaluate how SGCN+ without full-batch snapshot computation would impact on the zeroth-order and first-order variance. <sep> Since the snapshot gap K serve as a budge hyper parameter balances between training speed and quality of variance reduction. As the model converges w.r.t. increasing number of epochs, I would like to know whether we can dynamically increase K during the training process to obtain some speed boost. <sep> The paper is well-written in general. However, there is still some typos. For example, in Eq. (1) and Eq. (2), in the computation of gradient G_t^(l), the superscript in D_t should be (l+1) instead of (l), since we require the gradient of the loss w.r.t. the upper layer. <sep> ######################################################################### <sep> Questions during rebuttal period: <sep> Please address and clarify the cons above <sep> #########################################################################","The paper provides variance reduction techniques for GCN training. When training a GCN it is common to sample nodes as in SGD, but also subsample the nodes' neighbors, due to computational reasons. The entire mechanism introduces both bias and variance to the gradient estimation. The authors decompose the gradient estimate into its variance and bias error, allowing them to apply more targeted variance (and bias) reduction techniques. <sep> The results and improvement over existing GCN methods seem to be solid. The main weakness of the paper is its novelty. As pointed out in the reviews the techniques seem to be quite close to papers [5],[11] (referring to the authors posted list). <sep> It therefore boils down to the question of whether the authors simply applied existing techniques, achieving a better implementation than previous art, or did they develop a truly new algorithm that will encourage further research and deepen the understanding of GCNs. Given the decisive opinions of reviewers 1 and 4, that remained after taking the response into account, I tend to believe that the improvement provided here is either too incremental or not stated in a crisp enough manner in order to be published in its current form"
"rating_summary | weakness  ==> Summarize what the paper claims to contribute <sep> The papers introduce an amortisation for inference by Langevin dynamics (LD). Rather than making each particle track the posterior for a given data point as in normal LD, this new method couples the posterior samples of multiple data points by a dynamic recognition model; the parameters of the model are updated following the Langevin dynamics for a collection of data points. Authors also extend its use to two related models of data: variational autoencoder (LAE) with normalised and unnormalised prior p(z) (CLAE), producing increasingly better fits to data distributions. <sep> Strong points: <sep> The idea to amortise inference for a collection of posterior distributions is not new, but I have not seen before its extension to posteriors induced by sampling dynamics, so this paper makes an interesting proposal. In particular, introducing dynamics at the recognition parameter level is very interesting. <sep> The authors did not stop at applying to the common Gaussian generative models as in VAE, but extended it to more complex energy-based prior distributions. <sep> The method itself is introduced clearly with well-written descriptions. The toy experimental results are helpful and demonstrate the power of the amortised Langevin. <sep> Weak points: <sep> My main concern is the experiment which is somewhat lacking on both the design and the quality of the results (detailed below) <sep> The example for dequantization inference is confusing, and the problem might be technically ill-defined. <sep> The description of CLAE is ok, but the authors try to link it to adversarial training which is a very different training objective. I also do not find the discussions clear enough to make it the contrast worthwhile <sep> Recommendation: <sep> I am slightly tending to reject as it stands because of the experiments and some unclear discussions, but I am happy to increase the score if the authors provide a better argument for their design and clarify on their discussions. <sep> Comments on experiments. <sep> Evaluations based on reconstruction error is always prone to the trivial solution of learning an identity mapping. Could the authors try to perform denoising instead? <sep> VAEs with continuous observations, especially when trained on MNIST, is known to be hard. How about comparing it to a binary/Bernoulli likelihood evaluated on binarised MNIST? If it outperforms VAE, then this provides much stronger validation. I do not see from the 10-13 that the samples from LAE is much better than traditional VAE. <sep> The comparison between CLAE and other models is unfair: CLAE has an energy-based prior which may be more flexible (is this the case? Can the authors clarify on the choice of prior energy function? Sorry if I have missed the description.) <sep> There are only FID and for CIFAR-10 and CelebA, but not for MNIST or SVHN. Can the authors report this figure using features extracted by a relevant neural network, e.g. one trained on MNIST classification for FID on MNIST? <sep> The learning curve in Figure 5 is the unnormalised likelihood: it's unclear how its stability or convergence implies about learning speed of the normalised likelihood objective. <sep> Also, how does CLAE look on Figure 5? <sep> A critical question is whether the leaned dynamics is indeed more efficient than normal Langevin. The authors claim this but did not show results on the comparison. The effective sample in Table 1 provides some clue, but a convergence plot would be much stronger to support this claim. <sep> Questions: <sep> ? I find that updating all the recognition model parameters for each new data point a bit counter-intuitive. What if the initial samples for a set of x are very far away from the true distribution? The authors compare this method with the ""amortised"" MCMC in which the initial proposal is drawn from a recognition model, could this be combined with the dynamic-parameter recognition model to yield better results. Basically, if all parameters are dynamic, I do not see how the initial proposals can adapt to the generative model and characteristics of the data distribution. <sep> ? Figure 4: are the samples from the conditional or unconditional Langevin? <sep> ? What's the space of fixed random positions u? Also, could authors clarify the dimensionality of variables of z and u in Table 4? <sep> ? I haven't been able to understand this claim: <sep> ""In VAEs, noise is used to sample from the variational distribution in the calculation of potential U, i.e., in forward calculation. However, in LAEs, noise is used for calculating gradient ∇φU, i.e., in backward calculation."" <sep> The reparametrised samples in VAE are indeed used for the backward calculation. The forward pass simply evaluates the objective and retains dependence on recognition parameters φ. <sep> ? Figure 7: The After dequantization figure is better plotted after passing through a sigmoid? the huge difference in support range makes it hard to compare. <sep> ? I am very puzzled with the content in appendix C and would like to authors to help with understanding. In particular, the sentence preceding (25) doesn't seem logical. I see that any \\hat{x} from (25) is mapped to the x, meaning that ""the likelihood is a constant"", which is OK. However, the first term on the RHS of (26) is, in fact, a (log) conditional distribution of \\hat{x} given x, but there is no distribution over the data x itself. Does this mean that the model is just a conditional distribution, and does not learn the data distribution p(x) at all? Also, what is the distribution for this first term? A delta or Gaussian? <sep> ? Could the authors clarify the following sentence in appendix D: <sep> ""In other words, the latent variable is identical to the observation (i.e., p (x | z) = 1_{x=z}) in GANs"" <sep> I do not see the connection of this method to GAN, I'm happy to start again by better understanding this part. <sep> More generally, I do not believe making the comparison is worthwhile if at all correct, because the different training objectives differ a lot. Also, the solution of GAN is obtained at an equilibrium established by the two players, but the authors do not show such results for this new method. Could something similar be established, i.e. the optimal recognition and data model is established at the minimax solution? <sep> Detailed comments and suggestions (these points are here to help, and not necessarily part of your decision assessment) <sep> The discussion on EBM in the appendix seems more relevant to the GAN discussion. If space permits, it should be moved in to the main text. Also, CLAE applies to a model with energy-based prior. How hard is it to extend to a fully latent variable EBM? <sep> Some typos: <sep> Eqns (12) and (13), commas (,) before \\theta should be semicolons (;) <sep> Second lines below (24) ""Altough""-> Although <sep> Line above (28) ""rewrited""-> rewritten <sep> Third line below (28) ""enough continous"" -> continuous enough <sep> Third line from bottom of page 15. ""caan"" -> can <sep> ===== update ===== <sep> I am very grateful for the patient and detailed response. Due to limited time, I wasn't able to quickly follow up on the discussion.  I think the current quality of the paper is improved, so I increase the score slightly. However, I still struggle to follow some of the statements even after reading the response, it could be my comprehension or something to do with style/writing.","The paper had three borderline reviews. While the idea of posterior sampling of a neural network is potentially useful and Langevin dynamics are a way to attempt to address that, the reviewers did not appear convinced by the experiments and what the MCMC sampling was doing wasn't really front and center there."
"rating_summary | misc | decision  ==> This paper provides a theoretical connection between active inference and reinforcement learning and develops a method that can find a prior preference from experts. The new theory is derived from the concept of expected free energy (EFE) based on the free-energy principle.  Simulation experiments were conducted, and the effect of the prior preference learning was demonstrated. <sep> The theoretical contribution of the paper is to find the relationship between EFE and negative value function and proposed a prior preference learning method. The theoretical connection is insightful and interesting. <sep> However, the originality of the proposed method itself is not clear from the theoretical and practical viewpoints. <sep> In the experiment, they compared their method with a baseline method, i.e., global preference. <sep> There is no comparison between the pre-existing baseline method. <sep> Though the EFE-based approach is very interesting, the authors did not succeed in providing evidence of the advantage of the proposed method. <sep> It is questionable if this experiment is suitable for evaluating the main argument of this paper. <sep> Also, from the viewpoint of the information-theoretic approach to RL and the relation to the free energy principle, studies related to ""control as inference"" is worth mentioning. <sep> Levine, Sergey. ""Reinforcement learning and control as probabilistic inference: Tutorial and review."" arXiv preprint arXiv:1805.00909 (2018). <sep> Okada, Masashi, and Tadahiro Taniguchi. ""Variational inference mpc for bayesian model-based reinforcement learning."" Conference on Robot Learning. 2020. <sep> Hafner, Danijar, et al. ""Action and perception as divergence minimization."" arXiv preprint arXiv:2009.01791 (2020). <sep> <Minor comments> <sep> Capitalized Q is used for representing a variational density function. Q is often used in action-value function in the context of RL. If this is not equivalent to Q-function, it cannot be very clear. I think using q is a better choice. <sep> In 5.1.1, ""We did not run setting 2 in this study, because Acrobat is ambiguous in defining the global preference of the environment."" <sep> -> This may be ""setting 4."" <sep> The definition of ""global preference"" is not given. To my understanding, the term is not so well-known in the community of imitation and reinforcement learning. That should be defined. Because of this, what the experiment showed is unclear to potential readers. <sep> In conclusion, they describe, ""We also show that active inference can provide insights to solve the inverse RL problems."" However, they did not provide any explicit discussion over ""inverse RL."" This is actually the second time they mention ""inverse RL."" The first one is just at the end of the introduction. <sep> This should be explicitly mentioned if the authors put this statement in conclusion.",The meta-reviewer agrees with the reviewers that this is a marginal case. Conditioned on the quality of content and comparisons to other works: <sep> Constrained Reinforcement Learning With Learned Constraints (https://openreview.net/forum?id=akgiLNAkC7P) <sep> Parrot: Data-Driven Behavioral Priors for Reinforcement Learning (https://openreview.net/forum?id=Ysuv-WOFeKR) <sep> PERIL: Probabilistic Embeddings for hybrid Meta-Reinforcement and Imitation Learning (https://openreview.net/forum?id=BIIwfP55pp) <sep> We believe that the paper is not ready for publication yet. We would strongly encourage the authors to use the reviewers' feedback to improve the paper and resubmit to one of the upcoming conferences.
"abstract | weakness | decision  ==> Summary <sep> In this paper, the authors study the problem of learning node embeddings for hypergraphs. While most of the existing studies consider reducing hyper-graphs into graphs, this paper studies learning embeddings directly on the hypergraphs using two stages of aggregations / sampling. The efficacy of the proposed method is illustrated in a semi-supervised as well as inductive setting, where the method achieves better performances than the baselines. <sep> Originality. <sep> This paper may be interpreted as extension of GraphSage towards hypergraphs. The main contribution is in the aggregation function, which has two levels: one over intra-edge neighbourhood, and other over inter-edge neighbourhood. This method is thus suitable for both transductive as well as inductive settings. <sep> Significance. <sep> The experimental results show that HyperSage achieves better performance than HyperGCN in the semi-supervised tasks, while also extends to inductive settings and compares favourably against the MLP+HLR baseline.  In this aspect, the results are significant and relevant to the community. <sep> Clarity: <sep> This paper is well written and easy to follow. The relevant baselines and related methods are discussed appropriately. There are quite a few typos and some lack of clear notations, which are listed below: <sep> - Intra edge neighborhood - definition unclear - How is v_i used in the definition of N(v_i, e). <sep> - Typo in eqn 2, should have been e \\in E(v_i) <sep> - Typo in eqn 3, the second equation has ratio of two sets, which ideally should have been ratio of cardinality of two sets. <sep> - Algorithm 1, line 4, initialization of h_i^l from h_i^{l-1} should be outside the for loop, otherwise it may get reset again repeatedly. <sep> Questions to authors: <sep> - HyperSage has less deviation in the results than other methods such as HyperGCN. Any reasons ? <sep> - In the stability analysis paragraph, it is shown that as the train to test ratio improves to 1/3, HyperGCN and HyperSage have nearly same performances. Would be interesting to see how it behaves as we increase the training ratio further. Also, any insights into why this behaviour happens would be useful. <sep> - Also, what is the specific train-test ratio which is used in reporting Table 1? It would be better to include that in the main paper, since there is a discussion on train-test already present. <sep> - What are the other possible baselines for comparison in the inductive setting (Bai.et.al (Pattern Recognition, 2020) ? ) <sep> - From the results presented in Tables 1,2,3, it appears that the best performance is shared between p = 0.001 (equivalent to that  of Geometric mean aggregator) and p = 1(arithmetic mean aggregator). Any explanations of this would be useful. <sep> Pros: <sep> - An effective and simple extension of GraphSage towards HyperGraphs, which is suitable for inductive and transductive settings. <sep> - Experiment results illustrating better performances than the baselines. <sep> Cons: <sep> - The novelty of the proposed method is limited to that of the aggregation functions. <sep> - Some of the results mentioned lack explanation.","The paper proposes a learning framework for Hypergraphs. The proposed method can be viewed as generalisation of GraphSAGE to hyper graphs. Though the paper emphasises that there is significant differences between Hypergraphs and Graphs and hence new methods are required. However, the proposed methods are not significantly different than that used for Graphs. Thus the novelty seems to be limited and hence it is difficult to strongly argue for acceptance."
"abstract | strength | rebuttal_process | weakness  ==> The paper proposes conditional transport as a new divergence to measure the difference between two distributions. The idea is to learn the conditional transport plan of transporting one point in one distribution to the other marginal distribution. This conditional transport plan is modeled using a neural network. The resulting model is then applied to optimal transport formulation. Experiments are shown on image-based generative modeling dataset. <sep> The main idea is to decompose the joint transportation plan π(x,y) using conditionals as p(x)π(y|x) and p(y)π(x|y). The conditional transportation plan π(x|y) and π(y|x) are then modeled using neural contrastive losses, with the idea being similar points in two distributions are mapped closer. This decomposition is then applied in optimal transport formulation, leading to a new objective for optimizing OT. The authors also derive an empirical version of this objective, that makes it amenable for stochastic mini-batch optimization. <sep> While the idea of this decomposition is interesting, I see the following issues with the formulation. <sep> (1) From the forms of conditional transportation plans π(x|y) and π(y|x) in Eq 2 and 3, p(x)π(y|x)≠p(y)π(x|y). It would be good to have models that satisfy this equality. <sep> (2) I am not sure if the contrastive model assumed in Eq 2 and 3 is expressive enough to model the entire space of marginal constraints in OT. That is, I don't think p(x)π(y|x) will cover Π(μ,ν). Lemma 1 just shows that under some conditions, p(x)π(y|x) lies inside Π(μ,ν), but it would be interesting to see if then entire space of Π(μ,ν) is covered by the neural contrastive model. <sep> (3) When critic is used in ground cost function c(x,y), the resulting optimization (Eq 16) has one additional network compared to standard GANs. Also, the adversarial game still exists. So, it looks like this optimization is more harder (or at least equally harder) compared to standard GANs. Is this true? Do you see any optimization benefits compared to standard GANs? <sep> (4) The results of image-based generative modeling is not that impressive. On CIFAR and LSUN datasets, the performance is similar / slightly better than the compared GAN models. Also, many SOTA GAN models are not compared. So, it is very hard to say if the proposed model advances SOTA results. <sep> (5) I would have liked to see more interesting results. The formulation of authors gives an estimate of transportation plan π(x,y) in addition to the generative model itself, which is not possible to estimate in dual-based OT GAN optimization. The transportation plan can be used in interesting applications. One possibility is to estimate likelihoods and possibly use in OOD detection. Take a look at Balaji et al., ""Entropic GANs meet VAEs: A statistical approach to compute sample likelihoods in GANs"" for this. This is just one idea. Other interesting applications could have been demonstrated as well. <sep> Overall, while the idea is interesting, more results could have positioned the paper better. Just selling it as a paper that improves image-based generative modeling is not that great in my opinion. I would encourage authors to think more interesting experiments.","The paper proposes a new measure of difference between two distributions using conditional transport. The paper considers an important problem. However, some major concerns remain after the discussion among the reviewers. In particular, the paper focuses on the evaluation on a toy dataset. It is unclear whether the claim carries over to large real datasets. The presentation of the paper also needs substantial improvement."
"abstract | suggestion | rating_summary | decision  ==>  ==> Summary <sep> This paper addresses a reinforcement learning problem where the reward function is learned through a classifier that decides whether states are successful or not based on previous examples (i.e. RL after inverse RL). <sep> The authors show that this requires uncertainty-aware predictions, which are difficult with neural networks. An algorithm, BayCLR, is proposed that uses MAML to meta-learn the conditional normalized maximum likelihood, i.e. the ""maximum likelihood distribution"". Connections to the proposed algorithm and exploration methods are discussed before using the algorithm to solve various robotics tasks. <sep> Decision <sep> Although I liked this paper overall, I am rating it tentatively as marginal below the acceptance threshold. The paper is very well written and addresses a relatively clear problem (inverse RL with classifier) <sep> with an interesting method (meta-learning CNML). I have some issues with unclear statements in the motivation and method that should be addressed. While the experiments provide some insight, I think the conclusions the authors draw from them are far stronger than the results imply. <sep> Originality <sep> I am not very familiar with CNML, but this paper seems very original. In particular, the application of meta-learning to conditional normalized seems novel, as well as its application to inverse RL. <sep> Quality and Clarity <sep> The paper is well-written, most statements are clear and easy to follow. <sep> Strengths <sep> The approach of meta-learning CNML is interesting, and if anything deserves further analysis outside of inverse RL / RL. <sep> Although I have some issues with the motivation (see below), I think the authors do a good job of explaining their rationale for using <sep> CNML. In particular, quantifying neural network uncertainty through posterior analysis is quite difficult. <sep> The experiments seem comprehensive. However, I am not very familiar with the environment suite used. Due to the lack of work in this area, there are not many possible baselines, and so the VICE <sep> baseline seems like the best choice. The baseline is further kept fair by adding exploration heuristics. I especially like Section 6.2 <sep> that analyzes BayCRL on the zigzag maze task, which I assume is a tabular environment. In addition, there is a simple ablation but I <sep> would prefer more work on this. <sep> Weaknesses <sep> In Section 4, some motivating statements are unclear to me (see <sep> Detailed Comments). <sep> A single data-point being added to the dataset may not change the distribution much, and this crucial point is only addressed (in an ad-hoc way) in the appendix. It would be good to see an ablation study on this, or perhaps a plot of the average difference between different query points. Perhaps Figure 6 may indirectly explain this, but it should be explicitly addressed. <sep> In Section 6, many statements about BayCLR seem stronger than the results imply. Many confidence intervals overlap, and the results themselves are hard to parse with so many lines in each plot. <sep> Perhaps some of these baselines which are not competitive can be excluded, or perhaps different linestyles should be used. <sep> Detailed comments <sep> Early mentions of exploration (i.e. in the abstract) seem out of place. While you elaborate on the connection to exploration methods, <sep> it does not seem like the main point of this paper is to address exploration. It seems to me that you are tackling a novel RL problem where the task is specified through goal states. Specifically, you learn classifier in place of a reward function, and this is exploited to shape an otherwise sparse reward. <sep> Section 4.1, ""To create effective shaping, we need to impose a prior on our classifier so that it provides a more informative reward when evaluated at rarely vis- ited states that lie on the path to successful outcomes."" <sep> Why is a prior strictly necessary for reward shaping? Unless you mean prior in a very general sense, not a Bayesian prior, I don't see why a prior is strictly necessary. <sep> Section 4.1, ""[CNML].. is essentially imposing a uniform prior over the space of possible outcomes"". This is not obvious to me, and perhaps further explanation is needed. <sep> Section 4.2, Theorem 4.1: Perhaps I misunderstand but if G(s)>0, <sep> shouldn't p(e=1|s)=1? Further, why is that when the agent visit a successful state, i.e. N(s) increases, p(e=1|s) <sep> decreases after each visit. <sep> Section 5.1, ""This algorithm, which we call meta-NML, allows us to obtain normalized likelihood estimates without having to retrain maximum likelihood to convergence at every single query point, since the model can now solve maximum likelihood problems of this form very quickly. "" <sep> Is it correct to say that meta-NML does not need to retrain to convergence at every query point? The second part of the sentence elaborates that metatraining allows you to solve the problem very quickly, but it seems that you still need to solve it at every query point. <sep> Section 6.2, Figure 4: I don't think its fair to say that BayCLR <sep> performs substantially better. The confidence intervals overlap in all but Spiral Maze and Sawyer 3d Pick-and-Place. Other subtleties are not addressed, such as why RND/count-bonus actually hurt VICE in sawyer 2d push. Other statements such as ""significantly more efficiently"" need explanation as well. <sep> Section 6.3, Figure 5: for reproducibility, you should include exactly how many gradient steps are used in the model without meta-learning. <sep> Section 6.4, Figure 6: I'm unfamiliar with the environment being used, so some additional details explaining what z means would be helpful. <sep> ""Furthermore, meta-NML is able to reasonably approximate the idealized NML rewards with just one gradient step…"" <sep> How is this shown in Figure 6? I don't see anything showing the idealized NML rewards. <sep> Minor Comments <sep> Section 4, Line 3: missing space: ""ples.For example,"" <sep> Post Rebuttal <sep> After reading the comments by the other reviewers, I have decided to keep my score at a 5. The authors reply, and the updated manuscript, helped my understanding of the paper. I was considering raising my score, however, the reviewers were nearly unanimous in their confusion regarding the framework or application of CNML. For future iterations of the paper, I suggest that the authors describe CNML, event-based control and their connection more explicitly. If the main contribution is using CNML as the classifier in event-based control, then it would also help to conduct experiments on meta-learning CNML in a supervised learning setting to further elucidate its effectiveness in the reinforcement learning application. I think your paper is very interesting, and I hope that the authors are able to use this feedback to improve their paper.","Summary: <sep> This paper introduces a method to try to learn in environments where a person specifies successful outcomes but there is no environmental reward signal. <sep> I'd personally be interested in knowing where people were able to easily provide such successful outcomes instead of, for instance, providing demonstrations or reward feedback. Similarly, I'd be interested in how other methods of providing human prior knowledge compared. <sep> Discussion: <sep> Reviewers agreed the paper was interesting, but none of the 4 thought the paper should be accepted. <sep> Recommendation: <sep> While I do not think this paper should be accepted in its current form, I hope the authors will find the comments and constructive criticism useful."
"abstract | rating_summary | rebuttal_process | suggestion | weakness  ==>  ==> The authors propose a preprocessing method that eases fair learning called FLAP. <sep> They focus on counterfactual fairness introduced by Kusner et al. 2017. Under certain conditions, the preprocessing allow to biased the data such that traditional learning becomes fair. <sep> This is a very interesting idea that is novel to me. <sep> The paper is well written, and the related work is coherent with the work done. However, I think a few new paper are missing, for example Counterfactual Fairness: Unidentification, Bound and Algorithm from Wu et al 2019 that bounded the reachable counterfactual fairness. It would have been interesting to see how the results related to these bounds. <sep> I think that the statement:  ""We prove that CF is equivalent to the conditional independence of the decision and the sensitive attributes given the processed non-sensitive attributes"" is a bit misleading. As far as I understand, this is only true given that either condition 1 is satisfied or conditions 2 and 3 are satisfied. There exists statistical test for conditional independence but checking for condition 1, 2, 3 seems more complicated. <sep> On the real data, I would like to see a discussion on the meaning of CF-metric and FLAP method given that the condition 1, 2,3 are not verified. <sep> I think that theses clarifications are needed for a clear accept. <sep> ----- edit after the authors' answers ----- <sep> I'm not convinced by the authors' answer on the meaning of the CF-metric when condition 2 is not verified, which is the case on real data. ""... but we would expect fairer decisions to have smaller CF-metric in practice"" doesn't seem enough to claim that FLAP is better because its CF-metric is lower. In addition we don't have confidence interval to assess if FLAP is really better on MAE. <sep> Finally the review of reviewer1 made me realize that CF-metric and FLAP are based on the same function. I partly agree with the authors' answer, i.e., we need a counterfactual function to construct a good metric and then it is optimal to use it as well in the algorithm. However it is not fair to use that metric to claim that the algorithm is fairer. <sep> Therefore I lower my rating to 5.","The paper introduces an approach to counterfactual fairness based on data pre-processing, and compare it to other two counterfactual fairness approaches on the Adult and COMPAS datasets. <sep> The reviewers are in agreement that, in its current state, the paper should not be accepted for publication at the venue. Their main concerns are around the metric used to measure fairness, and these were not resolved during the discussion. The reviewers would have also appreciated more experiments on real-world datasets to get a more comprehensive comparison of the methods. Finally, discussion and comparison with other methods to achieve counterfactual fairness from the literature were limited."
"rebuttal_process | misc | decision | strength | rebuttal_process  ==>  ==> Summary <sep> The paper proposes to combine imitation learning (GAIL) with model predictive control to improve on the policy, especially when the system dynamics are noisier at test time. <sep> Here, MPC uses the reward function and value function learned by GAIL and assumes access to a model of the test-dynamics. <sep> The proposed approach, IMPLANT, is compared to GAIL and behavioral cloning on MuJoCo experiments in a standard imitation learning setting and for three slightly modified test scenarios that add either action noise, transition noise, or causal confusing which adds the last actions to the state space during training and replaces this information by noise at test time. <sep> Strong points / weak points <sep> (+) The paper is well-written and very easy to follow <sep> (+) Using MPC on a reward function that was learned by IRL can be reasonable <sep> (-) Lack of contribution / novelty <sep> (-) Using the GAIL reward doesn't seem sound. An actual IRL method should be used <sep> (-) additional assumptions compared to competitors (access to noisy model, computational tractability of online planning) reduce practicability and are not regarded during the comparisons <sep> Recommendation <sep> I recommend rejecting the submission because I can't find a noteworthy contribution. <sep> Supporting Arguments <sep> Lack of contribution: I don't want to be gatekeeping, but I really don't see any noteworthy contribution. The standard approach for apprenticeship learning is to learn a reward function by IRL and then optimize this reward function to learn a policy. There is no requirement to use the same algorithm to optimize the reward that used for inferring it. It is not surprising that reoptimizing the reward performs better in face of dynamic perturbations compared to a direct policy transfer; after all, this is one of the main motivations for learning a reward function. It is also not surprising that a model-based MPC approach based on the policy can perform better than directly using the parametric policy, even when there are no changes in the dynamics. So, I actually don't even see the research question here. There can be a small contribution due to the empirical evaluation, however, the chosen baselines perform direct policy transfer and, thus, do not seem that interesting. <sep> Using GAIL for IRL: The paper repeatably refers to GAIL as an IRL approach. However, GAIL does not infer a reward function in accordance with the problem formulation of IRL. The ""reward function"" used by GAIL is only valid for small policy updates of the current policy (generator). Maximizing this reward function will in general not result in any reasonable policy. For optimal policy and discriminator, the reward function would even be constant. So using this reward function for MPC doesn't seem sound. The evaluation did show that optimizing the reward function via MPC can be beneficial when using a small enough horizon and when sampling from the parametric policy, but this is not that surprising since the MPC control will in that case tend to remain close to the parametric policy. <sep> Additional assumptions: The paper argues that IMPLANT is ""zero-shot, unlike the proposed solutions of Fu et al. (2017) and de Haan et al. (2019) which require further interactions"" with the test environment. However, I would argue that MPC should be treated like a reinforcement learning method here; it also requires interactions with the test environment (i.e., sampling from the dynamics model). I don't see why it would not be fair to use a policy-parametric RL algorithm (e.g. AIRL + reoptimizing) instead of MPC for optimizing the reward. Actually, I think that the test setting would then still favor MPC since it requires costly online optimization which is not always feasible in practice. <sep> Questions <sep> Are there problem settings where IMPLANT is applicable, but IRL+RL (e.g. AIRL + reoptimizing) is not? <sep> The paper mentions that a random rollout policy for MPC performs worse than using the learned policy. Is this also the case for a very large number of rollouts? Is the resulting control also worse in terms of the learned reward function or only w.r.t. the true reward function? <sep> Additional Feedback <sep> My main concern is that the current submission has too little contribution and, thus, I think that more research needs to be done before publishing a paper on this topic. Performing MPC after IRL seems way too little as a paper story. It's hard to suggest a direction for further research because I do not see the research question to start with. Regarding the combination of IRL and MPC, it might be interesting to investigate using MPC within the IRL loop. For example, it can be challenging to learn reward functions for multimodal demonstration based on a parametric (and usually unimodal) policy. <sep> The current paper could be improved by using an IRL reward and by comparing the approach with IRL+reoptimizing. However, at least for me, this would not affect my recommendation because these changes do not address my main concern. <sep> I hope this review is not too discouraging; the paper does have strong points in the presentation.","The reviewers highly appreciated the replies and the additional experiments. We also had a private discussion on the paper. To summarize: the replies alleviated quite a few concerns, however the consensus was that the paper still does not meet the bar for a highly competitive conference like *CONF*. <sep> The idea of combining MPC (on a 'wrong' model) with a learned cost function is very interesting and a promising direction. On the downside the reviewers are still not entirely convinced about the contribution and believe that the paper requires a significant re-write to incorporate the discussed points as well as an additional round of reviews."
"strength | weakness | suggestion  ==>  ==> Summary <sep> The paper addresses Gaussian process regression (GPR) over 'positive' and 'negative' data points, such that the regression model fit the former and avoid the latter. The 'negative' data points in this work are each represented by individually distributed Gaussian random variables, with known parameters (mean vector and variance). <sep> The authors propose a novel GPR loss (6) for optimizing GP model parameters θ, striking a compromise between the regular GP loss and the KL divergence between the predictive GP distribution and the set of 'negative' data points. The approach is novel and an interesting take on the problem, and it has several beneficial properties which are investigated empirically. <sep> Strong points <sep> Compared to previous work on GPR with 'positive' and 'negative' data points, the proposed solution is elegant in its simplicity, and scalable for predictions (since the 'negative' data is only used during kernel parameter optimization). I especially like the scalability aspect, since it would allow the use of a otherwise prohibitively large amount of 'negative' examples. This is useful in navigation, for example in contextes where the boundaries of static obstacles are commonly represented by potential fields. <sep> The experiments are fairly extensive and demonstrate several beneficial properties of the proposed method, such as fast learning convergence and compatibility with sparse GP methods. <sep> Weak points <sep> The problem of GPR with both 'positive' and 'negative' data points (and in the context of robot navigation) has been investigated before [1], where a non-stationary kernel function is proposed. The approach in [1] should be addressed, and compared with, at least qualitatively. <sep> The experiments are not representing any situation where the distinction of 'positive' and 'negative' data points naturally occur. This makes it hard to assess how well the proposed method works for real problems of the kind the paper intends to address. <sep> The technical clarity is varied: <sep> Equation (5) is incorrect? The two multivariate Gaussian distributions have different dimension (n vs m), and they do not share the same support (X and X¯ can differ). Maybe it should be ""p(y|θ,X¯)"". But is this then entirely compatible with the prior sentence: θ is in essence the ""GP regression model learned from the positive data pairs"" and yet it is stated that you maximize KL divergence between the GP regression model and the positive data pairs with respect to θ? If θ is changed, then the first distribution in the KL divergence is no longer the GP model learned from the positive data points. I suggest that this section is made clearer and more precise. Equation (6) on the other hand corresponds to the stated proposal. <sep> It is not obvious to me how you make use of the re-parameterization trick proposed by (Kingma & Welling, 2013). <sep> Why do you not minimize the loss (6) directly, instead of first minimizing NLL and then maximizing KL at each iteration (Algorithm 1)? <sep> What value does λ and σneg have in the experiments? <sep> The 'Random shuffling technique' is not at all clear to me. <sep> How can you get a set of valid data pairs (i.e. xi and yi) by shuffling the labels (""yi"")? <sep> If randomly create pseudo-negative data points, especially using existing data points (that are either designated 'negative' or shuffled in some way to produce new input-output pairs as 'negative'), then isn't it likely given real data sets that these data points will be in conflict (e.g. overlap) with other 'positive' data points? If so, what do you mean with that these are valid and what does this mean for the experiment (more details on the limitations of the experiment and interpretations of the results should be present in the paper)? <sep> Reason for score <sep> Although an interesting take on the problem, the paper in its current form is lacking technical clarity, comparisons to directly related works, and the evaluation is weak in that it does not consider actual instances of the addressed problem in the empirical evaluation. <sep> Minor comments <sep> ""to model the uncertainty by providing the predictive variance"" -> ""to model [target] uncertainty by providing predictive variance"" <sep> ""in both the positive and negative data pair."" -> ""in both the positive and negative data pair set."" <sep> Page 4: ""P(y|θ,X)"" -> ""p(y|θ,X)"" <sep> ""loglikelihood"" -> ""log likelihood"" <sep> [1] S. Choi, E. Kim, K. Lee and S. Oh, ""Leveraged non-stationary Gaussian process regression for autonomous robot navigation,"" 2015 IEEE International Conference on Robotics and Automation (ICRA), Seattle, WA, 2015, pp. 473-478, doi: 10.1109/ICRA.2015.7139222. <sep> Edit: Markdown problem with nested lists <sep> Edit: Upgraded rating from 4 to 5 <sep> Thank you for addressing most of my concerns. <sep> I have increased my rating, but the paper still needs some work in my opinion. More specifically, <sep> a more thorough comparison with [1], e.g. by including the quantitative comparison you were working on. <sep> an improved representative experiment from the problem domain. The newly added one is a bit simplistic and not motivated enough. Although stated to be 2D, it is (as far as I can tell) indistinguishable from a 1D regression task? How do you handle the 2D output in GPR? How are the data points generated that are used as training data, and what could motivate such a generation of data in a plausibly real setting?","This paper is very pleasant to read. The reviewers also like the key idea discussed and find the targeted application interesting and practical. However, after reading the indeed interesting motivation, all four reviewers expected to see more from the evaluation section, including more challenging and realistic set-ups and clearer gains over standard methods. The reviewers also discuss how both the navigation problem as well as the GP constraint problem have been tackled in the past, often in combination (e.g. reference [1] by R1). Therefore, it would be needed to see additional experimental evaluation in line with those previous works."
"abstract | strength | weakness  ==> Update : Since most of my issues have been addressed, I have changed my rating from 4 to 6 <sep> Summary: <sep> This paper studies the robustness of quantized networks against gradient-based adversarial attacks (for L2 and Linf norms), showing how quantized models suffer from gradient vanishing, giving a false sense of security via gradient masking. To circumvent this issue, the authors propose temperature scaling approaches that can overcome this masking, achieving near-perfect perfect success in crafting adversarial inputs for these models. <sep> ########################################################################## <sep> Reasons for score: <sep> The paper's ultimate goal is to get better gradient-based attack performance on quantized (binarized, in this case) networks. However, key steps that should have been tried first for benchmarking such as adaptive PGD attacks have not been performed. Moreover, it is not clear what benefit the proposed method has in this scenario compared to gradient-free attacks like Boundary++.  The paper's contributions, although including some nice analyses on temperature scaling based solutions, are too weak to be accepted in their current form. <sep> ########################################################################## <sep> Pros: <sep> Improvement in attack success rates for full-precision networks, even for FGSM, seems like an exciting result. Further analyses and methods on top of this could be used to further increase the strength of these first-order gradient attacks. <sep> Jacobian and Hessian based detailed analyses of temperature scaling, and what different solutions correspond to in terms of robustness is quite insightful and interesting. <sep> ########################################################################## <sep> Cons: <sep> Gradient masking is a relatively well-known phenomenon in adversarial machine learning. In cases when normal first-order gradient attacks fail, techniques like adaptive PGD attacks, gradient-free attacks, or even black-box transfer attacks are some straightforward methods to overcome gradient masking. Thus, it is not clear why the authors did not try non-gradient attacks before jumping to a complicated algorithm. At the very least, those attacks (like Boundary++) should at least be part of benchmarks for comparison. <sep> For starters, please refer to 'Reliable evaluation of adversarial robustness with an ensemble of diverse parameter-free attacks': they have a publicly available implementation as well <sep> All of this is crucial, especially since the paper claims (Section 4) that ""we would like to point out that the focus of this paper is to improve gradient-based attacks on already trained BNNs"" <sep> In general, investigating if a model exhibits gradient masking is not a contribution: standard checks like comparing transfer rates, multi-step to single-step performance, attack rates for increasing attack budgets, etc are often used to check for gradient masking. <sep> Figure 1: For which norm are these numbers reported? Without knowing the norm, it is hard to say if Figure b is a sign of gradient masking or not. <sep> Section 2.2 ""..these attacks have been further strengthened by a random initial step"". This is partially true: the real benefit comes from having multiple random restarts. Having just one random initialization by itself is not that useful. Please re-run evaluation experiments with random restarts (20 is a good number). <sep> Section 3: What does ""adversarial accuracy"" refer to? Is it accuracy on perturbed inputs f(x') = y, or success rate of the adversary when trying to change predictions aka f(x) ~= f(x')? Please clarify <sep> Section 3.1 ""... clearly indicate gradient masking issues.."" please elaborate: not every reader will be familiar with the set of checks used for gradient masking. <sep> Issues with Cross-Entropy based loss and how they promote certain magnitudes of logit values are not new. The authors might want to have a look at Section 4.1 of Reliable Evaluation of Adversarial Robustness with an Ensemble of Diverse <sep> Parameter-free Attacks to see if there are similarities/differences in the proposed temperature-based variant, and how the proposed method is better than the one in the Difference of Logits-Ratio based loss? This work seems to be a key and relevant part of related work and should be included in comparisons/benchmarking. <sep> ""implementation of our algorithm will be released upon publication"". Please anonymize and attach the code in response. <sep> The benefit of using the proposed FGSM++/PGD++ attacks on full-precision models trained with adversarial robustness seems to be negligible (Table 4), and should not be overstated in results. Also, since these attacks all have random seeds, please perform experiments multiple times for statistical significance and report summary statistics. <sep> ########################################################################## <sep> Minor Edits: <sep> Section 2.2 ""..perturbations to the images..."" the definition here is for adversarial examples in general, and should thus be ""perturbations to data"" <sep> Section 2.2 ""Gradient-based attacks can be... written as Projected Gradient Descent (PGD)"" this is true only for first-order gradient-based attacks, not all gradient-based attacks (examples JSMA). Please correct. <sep> Section 4.1 ""...since most of the modern networks consist of ReLU nonlinearities"" this can (and often is) circumvented using Fake-ReLU. Example implementation here <sep> Section 5 ""...and they hypothesize that linear networks would be robust to adversarial attacks."" this is not their conclusion, and seems to be out of context. <sep> Section 6 should preferably be either towards the end or at the beginning? Not clear why it is in the middle of other sections <sep> Please address and clarify the cons above","The paper studies the robustness of binary neural networks (BNNS), showing how quantized models suffer from gradient vanishing. To solve this issue, the authors propose temperature scaling approaches that can overcome this masking, achieving near-perfect perfect success in crafting adversarial inputs for these models. The problem is interesting and important. However, the major concerns are that the technical novelty is limited raised by two Reviewers, small improvements for linear loss functions. The most related work is not compared in the experiment."
"weakness | suggestion | weakness | suggestion  ==>  ==> The paper is about an interesting setting, where observations correspond to lots of individuals evolving over time. The twist is that individual identifiers are not available.  Thus from one time point to the next, the observer does not know which individual is which. The problem is to learn a time series model from this, and models considered here are in the form of an SDE. <sep> In my first reading of the paper, I thought the topic was very interesting but I struggled to follow some parts of the manuscript, which I thought was overly unclear. Some of the notation is undefined or ill-defined, the general reasoning is hard to follow, the notation does not help identifying the objects that are observed from the objects that are simulated, there is a lack of acknowledgement of limitations of the proposed method, the synthetic examples are not well-motivated, and various other fairly standard concerns. <sep> However, during my second reading I looked at the reference Wang et al, ""Learning Deep Hidden Nonlinear Dynamics from Aggregate Data"", published in UAI in 2018.  From this I got more serious concerns. <sep> The authors' abstract starts with ""Learning nonlinear dynamics from aggregate data is a challenging problem since the full trajectory of each individual is not available, namely, the individual observed at one time point may not be observed at next time point, or the identity of individual is unavailable due to technical limitations, experimental cost and/or privacy issues."" <sep> Wang et al's abstract mentions: ""However, in most of the practical applications, these requirements are unrealistic: the evolving dynamics may be too complex to be modeled directly on observations, and individual-level trajectories may not be available due to technical limitations, experimental costs and/or privacy issues."" <sep> The authors write ""In the work of Hashimoto et al. (2016), a stochastic differential equation(SDE) was adopted to capture the dynamics of particles directly on observations, in particular the drift coefficient is parameterized by a recurrent neural network."" <sep> Wang et al. write: ""Modeling the dynamics on aggregate observations have been investigated recently in (Hashimoto et al., 2016), where a stochastic differential equation (SDE) has been used to capture the transition directly on observations Yt."" <sep> The authors write: ""There are many existing models to learn dynamics of full-trajectory data. Popular ones include Hidden Markov Model (HMM)(Alshamaa et al., 2019; Eddy, 1996), Kalman Filter (KF)(Farahi & Yazdi, 2020; Harvey, 1990; Kalman, 1960) and Particle Filter (PF) (Santos et al., 2019; Djuric et al., 2003). These models and their variants (Deriche et al., 2020; Fang et al., 2019; Hefny et al., 2015; Langford et al., 2009) require full trajectories of each individual, which may not be directly applicable to the aggregate data as we mentioned earlier. "" <sep> Wang et al. write: ""Existing models such as Hidden Markov Model (HMM) (Eddy, 1996), Kalman Filter (KF) (Harvey, 1990) and Particle Filter (PF) (Djuric et al., 2003) are popular methods with hidden variables. However, these models and their variants (Langford et al., 2009; Hefny et al., 2015) require individual-level trajectories, which may not be available, as was mentioned earlier."" <sep> -- <sep> Based on these strong similarities I do not think that the proposed manuscript is suitable for publication.","While the reviewer's noted a number of strengths of your paper, the approach that you took, and agreed that you had tackled an important problem, concerns remained about presentation and clarity. I agree. (Here are just a few miscellaneous comments: the very first paragraph of the Introduction needs to be rewritten for clarity, in my opinion. Later on page 1, you use the term ""the dynamics of density"" but you should not assume that the reader knows what that means. There are typos as well, e.g. ""make all predictions base[d] on Equation (6)"" on page 4. It would be helpful to know something about why you chose the experimental setups in Synthetic-1, 2, and 3. ) <sep> Regarding the similarities between this paper and a previously published article I believe that the authors have addressed these concerns; I hope they are careful to avoid this situation in the future."
"abstract | strength | rebuttal_process | weakness | decision  ==>  ==> Strengths: <sep> The paper is well written and clean. <sep> Weaknesses: <sep> I have several concerns regarding this paper. <sep> •    Novelty. The authors propose to use Ricci flow to compute the distance between nodes so that to sample edges with respect to that distance. Using Ricci flow for distance computation is a well-studied area (as indicated in related work). The only novel part is that each layer gets a new graph; however, this choice is not motivated (why not to train all layers of GNN on different graphs instead?) and has problems (see next). <sep> •    Approach. Computing optimal transport distance is generally an expensive procedure. While authors indicated that it takes seconds to compute it on 36 cores machine, it's not clear how scalable this method is. I would like to see whether it scales on normal machines with a couple of cores. Moreover, how do you compute exactly optimal transport, because the Sinkhorn method gives you a doubly stochastic matrix (how do you go from it to optimal transport?). <sep> •    Algorithm. This is the most obscure part of the paper. First, it's not indicated how many layers do you use in experiments. This is a major part of your algorithm because you claim that if an edge appears in several layers it means that it's not adversarial (or that it does not harm your algorithm). In most of the baselines, there are at most 2-3 layers. There are theoretical limitations why GNN with many layers may not work in practice (see, the literature on ""GNN oversmoothing""). Considering that you didn't provide the code (can you provide an anonymized version of the code?) and that your baselines (GCN, GAT, etc.) have similar (or the same) performance as in the original papers (where the number of layers is 2-3), I deduce that your model Ricci-GNN also has this number of layers. With that said, I doubt that it's possible to make any conclusive results about whether an edge is adversarial or not with 2-3 graphs. Moreover, I would expect to see an experiment on how your approach varies depending on the number of layers. This is a crucial part of your algorithm and not seeing discussion of it in the paper, raises concerns about the validity of experiments. <sep> •    Design choices. Another potential problem of your algorithm is that the sampled graphs can become dense. There are hyperparameters \\sigma and \\beta that control the probabilities and also you limit the sampling only for 2-hop neighborhoods (""To keep graph sparsity, we only sample edges between pairs that are within k hops of each other in G (we always take k = 2 in the experiments)."" This is arbitrary and the effect of it on the performance is not clear. How did you select parameters \\sigma and \\beta? Why k=2? How do you ensure that the sampled graphs are similar to the original one? Does it matter that sampled graphs should have similar statistics to the original graph? I guess, this crucially affects the performance of your algorithm, so I would like to see more experiments on this. <sep> •    Datasets. Since this paper is mostly experimental, I would like to see a comparison of this model on more datasets (5-7 in total). Verifying on realistic but small datasets such as Cora and Citeseer limits our intuition about performance. For example, Cora is a single graph of 2.7K nodes. As indicated in [1], ""Although small datasets are useful as sanity checks for new ideas, they can become a liability in the long run as new GNN models will be designed to overfit the small test sets instead of searching for more generalizable architectures."" There are many sources of real graphs, you can consider OGB [2] or [3]. <sep> •    Weak baselines. Another major concern of the validity of the experiments is the choice of the baselines. Neither of GNN baselines (GCN, GAT, etc.) was designed for the defense of adversarial attacks, so choosing them for comparison is not fair. A comparison with previous works (indicated in ""Adversarial attack on graphs."" in related work section) is necessary. Moreover, an experiment where you randomly sample edges (instead of using Ricci distance) is desirable to compare the performance against random sampling. <sep> •    Ablation. Since you use GCN, why the performance of Ricci-GCN is so different from GCN when there 0 perturbations? For Citeseer the absolute difference is 2% which is quite high for the same models. Also, an experiment with different choices of GNN is desirable. <sep> •    Training. Since experiments play important role in this paper, it's important to give a fair setup for the models in comparison. You write ""For each training procedure, we run 100 epochs and use the model trained at 100-th epoch."". This can disadvantageous for many models. A better way would be to run each model setup until convergence on the training set, selecting the epoch using the validation set. Otherwise, your baselines could suffer from either underfitting or overfitting. <sep> [1] https://arxiv.org/pdf/2003.00982.pdf <sep> [2] https://ogb.stanford.edu/ <sep> [3] https://paperswithcode.com/task/node-classification <sep> ========== <sep> After reading the authors comments. <sep> I applaud the authors for greatly improving their paper via the revision. Now the number of layers is specified and the explanation of having many sampled graphs during training is added, which was missing in the original text and was preventing a full understanding of the reasons why the proposed approach works. Overall, I am leaning toward increasing the score. <sep> I still have several concerns about the practicality of Ricci-GNN. In simple words, the proposed approach uses some metric S (Ricci flow) that dictates how to sample graphs for training. The motivation for using Ricci flow is ""that Ricci flow is a global process that tries to uncover the underlying metric space supported by the graph topology and thus embraces redundancy"". This claim cites previous papers, which in turn do not discuss what exactly is meant by ""a global process that tries to uncover the underlying metric space"". Spectral embeddings also can be considered as a global metric, so some analysis on what properties of Ricci flow makes it more robust to attacks would be appreciated. Also including random sampling in comparison would confirm that the effect is coming not from the fact that you use more graphs during the training, but from how you sample those graphs. In addition, as the paper is empirical and relies on the properties of Ricci flow which was discussed in previous works and was not addressed in the context of adversarial attacks, having more datasets (especially larger ones) in the experiments would improve the paper.","The paper proposes a new defense against adversarial attacks on graphs using a reweighting scheme based on Ricci-flow. Reviewers highlighted that the paper introduces interesting ideas and that the use of Ricci-curvature/flow is a novel and promising contribution. Reviewers also recognized that the paper has significantly improved after rebuttal and clarified some aspects of their initial reviews. <sep> However, there exist still concerns around the current version of the manuscript. In particular, important aspects of the method and algorithm, as well as some design choices are currently unclear. This includes evaluating and discussing robustness, training method, and practicality/improvements in real-world scenarios. I agree with the majority of the reviewers that the current version requires an additional revision to iron out the aforementioned issues. However, I also agree with the reviewers that the overall idea is promising and I'd encourage the authors to revise and resubmit their work with considering the feedback from this round of reviews."
"abstract | rating_summary | weakness | rebuttal_process | decision  ==>  ==> Summary: <sep> This work proposes to attack object detectors by targeting their relevance maps of the different detected objects. The proposed RAD attack shows better black box transferability across different detectors on MSCOCO dataset. The relevance maps are calculated based on SGLRP act as an attention mechanism to the attack to focus on relevant regions in the more meaningful image and hence produce more transferable attacks. <sep> Strengths : <sep> Good attack performance and transferability between detectors, which poses a security threat for SDV applications that use object detectors <sep> Eight different detectors and three segmentation models are used in the RAD attack, which shows good generalization. <sep> Weaknesses: <sep> Missing important references [a,b,c]. All of these works attack object detectors and target transferability. <sep> The paper is poorly written and ambiguous. Variables are introduced without proper definitions. It is not clear how to obtain the gradients in eq(3) with respect to the relevance maps. <sep> No use of the proposed dataset. The authors propose a new dataset of adversarial objects but never mention or showcase the dataset's usefulness. A straightforward way to show the dataset's usefulness is by performing adversarial training and making robust detectors against the proposed attacks. <sep> No enough ablation is performed. The only ablation to the proposed method is in table 7 regarding the way to pick the detection target. The relevance maps based on LRP are expensive and worse than recent saliency maps like CAM and grad-CAM. The attack budget ϵ=16 picked in the experiments is not justified ( it might be big or small for attack success ), and a plot of mAP vs. ϵ for different detectors would give more information about the effect of the attack. <sep> All the attacks in the paper are performed on YOLOv3 and transferred to other models. It would be more informative to show transferability matrices of attacks performed on all models and transferred to all others. <sep> The novelty of the proposed methodology is limited. While the use of relevance maps to improve the transferability of attacks on object detectors is novel, no proper explanation is provided. The attacks are based on PGD, and the relevance map is adapted from SGLRP. The paper offers no theoretical results or exciting insights. <sep> [a] Huang et al. ""Universal Physical Camouflage Attacks on Object Detectors"", ( CVPR 2020) <sep> [b] Wu et al. ""Making an Invisibility Cloak: Real World Adversarial Attacks on Object Detectors"", (ECCV 2020 ) <sep> [c] Xu et al. ""Adversarial T-shirt! Evading Person Detectors in A Physical World"" (ECCV 2020). <sep> Minor issues : <sep> Many grammar mistakes:"" because they possess multiple-output."", ""Among the classification attacks and detection ones, cross-domain attack (Naseer et al. (2019)) is the most effective, but RAD is more aggressive"" ..etc. <sep> No question marks in titles 3.1-3.5. <sep> Table 2-5 could have been visualized better by using a bar chart, for example, to observe the relative performance of attacks and defenses.","This paper proposes a transferable adversarial attack method for object detection by using the relevance map. Four reviewers provided detailed reviews: 2 of them rated ""Ok but not good enough - rejection"", 1 rated ""Marginally below"" and 1 rated ""Marginally above"". While reviewers consider the paper well written and using relevance map novel, a number of concerns are raised, including limited novelty, the lack of theoretical results, no use of the proposed dataset, insufficient ablation, etc. During the rebuttal, the authors made efforts to response to all reviewers' comments. However, the major concerns remain, and the rating were not changed. The ACs concur these major concerns and agree that the paper can not be accepted at its current state."
"abstract | strength | weakness  ==> Update <sep> The errors in the paper were fixed, and the discussion was improved. <sep> It is very rare to see a novel result as fundamental as the one presented in this paper, and I believe this puts it in the top 5% of accepted papers, so I have updated my score accordingly. <sep> I think the discussion and experimentation still has room for improvement, but I am not too bothered, as there do not appear to be any major errors remaining in the paper. <sep> Summary <sep> The paper considers the task of estimating the gradient ddθEpθ(z)[f(z)]. <sep> This is a fundamental task relevant in all fields of machine learning, e.g. policy gradients, variational inference, or any other situation with stochastic computations. <sep> The authors come up with a constructive method for deriving an estimator for this gradient for any distribution p(z) based on Fourier analysis. The derived gradients include some known estimators, e.g. the Gaussian gradient identities, but also include some new estimators based on infinite series of higher order gradients of f(z) for distributions such as Gamma or Laplace. <sep> The analysis also works for Dirac delta distributions or discrete distributions. <sep> They perform toy experiments to evaluate the new gradient estimators, and show that they work and seem to give better accuracy than previous pathwise gradients (but they use higher order gradients). For the infinite series based estimators, they truncated the length of the series at a certain depth (they tested up to 4th order and up to 8th order). <sep> Strengths <sep> Gradient estimation is fundamental, and deriving new methods is a great contribution. <sep> The paper didn't discuss this well, but what is particular about the method derived is the form of the estimator based on separating out the computation of the expected gradients of f(z), and the weighting applied to it, i.e. all of the gradient estimators derived with the method have the form ∑kak(θ)Ep(z)[dkf(z)dzk]. What is neat about this, is that one only needs estimators for the expected gradients dkf(z)dzk, it is not necessary to know what the sampled z values were. If you contrast this to some other gradient estimators, for example, the reparameterization gradient for the standard deviation parameter of a Gaussian distribution has the form ϵ∇zf(z), where it is necessary to weight the separate values of ∇zf(z) with different multipliers, but in the current paper, they derive a method where the multiplier is the same for all samples z. Moreover, from the derivation it is clear that there is only one way to perform this decomposition for each distribution, and the method to perform the decomposition for any p(z) is provided in this paper. This is a general fundamental result. <sep> Weaknesses <sep> The discussion could be improved, e.g. the above points I mentioned were not explained. <sep> I believe Lemma 3 and Corollary 3.1 are incorrect, because they ignore the singularity at the ""kink"" of the relu. This issue does not affect the experiments though, because relus were not used in the experiments. (This Lemma was about making the expressions based on infinite series of higher order derivatives tractable in cases where the higher order derivatives are 0.) I explain this in more detail later. <sep> The discussion around the ""discrete derivative"" for categorical distributions was unconvincing to me. It appears that the derivation does not require selecting a special z∗ choice, and the derivation goes through by just summing all z without computing any difference at all. <sep> From a practical point of view, probably the method will not be immediately used. <sep> Recommendation <sep> I recommend accepting the paper, because the theoretical result is fundamental. They performed experiments showing that the new methods work on toy problems, which I think is sufficient. Any issues I found were minor, and could be improved with a bit of revision. Probably, I will further increase my score if they adequately revise the paper. <sep> Questions <sep> In the experiments, what is the performance for truncation depth 1, 2, 3? <sep> I am interested in at what point the performance starts degrading. Currently the experiments only show 4 and 8, and both give similar performance, so it is not clear whether, for example, 1 may also work or not. <sep> In equation 6, α is a multi index, not an integer. What does <sep> (iω)α mean? <sep> Additional comments, suggestions, clarifications <sep> Kink in the relu: For example, consider f(z)=max(0,z) is a relu. Then consider <sep> Ep(z)[d2fdz2]. Due to the effectively infinite second derivative at 0, I believe this expectation should be considered as p(0)(df(0+ϵ)dz−df(0−ϵ)dz)=p(0), and I believe the values at the singularities will matter for the gradient estimator. As long as I have not misunderstood something, I would suggest to just remove the discussion about relus, and say that if the higher order derivatives disappear, it becomes tractable. <sep> In general, I think the discussion would be greatly improved if you emphasize the structure of the gradient estimator based on separating out the weighting for the expected gradients a, and computing the expected gradient <sep> Ep(z)[dnfdzn]. And contrast this to the other existing gradient estimators, e.g. reparameterization or pathwise derivatives, which often apply a different weighting for derivatives at different z values. It would be good to emphasize that what you have derived is not a general form for gradient estimators, it is a particular (fairly broad) family of gradient estimators among other techniques that are not described by your derivation. I also did not find the discussion about the Dirac delta or the discrete distributions particularly insightful, so I would suggest to spend less time emphasizing these points. <sep> ""Furthermore, we show that the classical deterministic backproapagation rule is a special case of stochastic backpropagation where the distribution is a Dirac delta, bridging the domains of neural networks and probabilistic graphical models."" <sep> I would not emphasize this point, as it is probably obvious to many researchers. The Dirac delta can be acquired by letting σ→0 <sep> for a Gaussian distribution, which directly gives the result. <sep> In the introduction, there are some spaces before the '?' signs, which should be removed. <sep> I disagree with some of the discussion in the introduction. In particular the questions posed are already answered to some extent: <sep> ""How to develop stochastic backpropagation rules similar to those of <sep> (Rezende et al., 2014) for a broader range of distributions?"" <sep> Implicit reparameterization gradients work for a broad range of distributions. Moreover, the reparameterization based on the cdf of a distribution always works as long as the cdf can be inverted (this is just a computational issue, rather than a theoretical one). <sep> ""What is the link between the discrete random variable case and the continuous case? And finally, what is the relation between stochastic backpropagation and classical deterministic backpropagation?"" <sep> I wouldn't say the paper gives a definite answer to these questions. It just provides another interpretation based on Fourier transforms/characteristic functions, which is good, but I don't believe the new interpretation is better than previous ones; it is just different. I would tone down the discussion and just say that you provide interpretations based on Fourier analysis. <sep> sommable -> summable <sep> Perhaps the multi-index notation could be clarified by 1-2 examples, and/or emphasized by putting in a definition block. I am not sure whether it will improve it though. <sep> The weighting function for the derivatives is outside the expectation, <sep> i.e. the weights do not depend on the sampled z position, and the z only comes into play for computing the expectation of some gradients of f(z). I think this structure of the gradient estimator should be emphasized more. <sep> Equation 3 claims that the weights are unique, so there is only one way to construct the estimators that they have constructed. I think this should be emphasized more. Also, instead of ∃! it may be better to just write ""there exists a unique"". <sep> aα is a bit difficult to read. Consider using different notation for either a or α. <sep> "" Putting everything together"": you have an extra space "" "" in the beginning of the sentence. <sep> In the Theorem 1 statement , can you write out that a are the <sep> Taylor weights of ∇θlog⁡φθ(ω)? <sep> ""Plugging this expression to equation equation 5"" -> there's a duplicate <sep> ""equation"" <sep> The ""(AUEB & Lazaro-Gredilla,2015)"" citation author names should be fixed <sep> (the default on google scholar is not good). <sep> The discrete derivative explanation in equations 10 and 13 is not convincing to me. Instead of pulling out the φ(z∗) term in eq <sep> 11, you could have just kept the summation across all z, and it would have lead to the gradient estimator also summing all f(z). It is not clear why the difference between f(z) and f(z∗) is necessary or why it should be interpreted as a discrete derivative as the z vector is not being perturbed. <sep> Laplace is reparameterizable. What is meant by the pathwise derivative in the experiments? The work from Jankowiak and Obermeyer defines a family of gradient estimators not a single one. <sep> For the toy problems. Rather than showing only the learning curves, <sep> it would be good to also test that the expected gradients for the new methods and old methods are the same at some particular parameter values (up to estimation accuracy, but you should be able to get the estimates accurate by repeating the computation many times and averaging). Also, for the experiments with truncation, probably what is more important than the variance is the gradient accuracy. So, it would be good to show both the bias and variance at different truncation depths. (Ideally, I envision a graph showing bias and variance of the gradient at a particular parameter value plotted against different truncation depth values.) <sep> Rezende et al (2014) also mention the reparameterization of the variable under stochastic backpropagation, and this gives a different gradient for the covariance parameters of a Gaussian compared to the stochastic backpropagation rule derived in the present paper. Hence, it does not generalize stochastic backpropagation as envisioned by Rezende. Instead, <sep> it is a separate method for deriving one particular type of stochastic backpropagation rule for any distribution. I would suggest something like Fourier stochastic backpropagation, characteristic stochastic backpropagation, Fourier expectation gradients, etc. <sep> The log characteristic function for Laplace should be iωμ−log⁡(1+b2ω2), you're missing the μ in your equation. <sep> ""Our approach, in contrast generalizes stochastic backpropagation as presented by (Rezende et al.,2014), where the derivative is explicitly transported to the random variable"" <sep> No, it doesn't generalize it as presented by Rezende. In Rezende's work, <sep> reparameterization is a subset of stochastic backpropagation (it is listed under section 3 titled stochastic backpropagation). And reparameterization contains gradient estimators not derived by your method (e.g. the ddσ(⋅) gradient for a Gaussian). <sep> Hence, it is wrong to say that your method generalizes stochastic backpropagation.","The focus of the paper is stochastic backpropagation for both continuous and discrete random variables. By using standard results from Fourier analysis the authors rewrite the corresponding gradients in an infinite weighted sum form ((3) and (9)), extending the results of (Rezende et al. 2014) and (Fellows. et al., 2018). The efficiency of the approach is illustrated in 2 toy examples. <sep> As summarized by the reviewers, the problem tackled is interesting. However, they also pointed out that the novelty of the approach is quite limited and its practical usefulness is not clear (it should by demonstrated against state-of-the-art baselines, on realistic benchmarks)."
"abstract | strength | weakness | suggestion | weakness | suggestion  ==> Summary <sep> The paper presents a hierarchical reinforcement learning approach to solve large-scale vehicle routing problems (VRPs). A ""rewriting agent"" is responsible for dividing the customers into regions while a ""generating agent"" is responsible for computing the vehicle routes in each region, independently. The rewriting agent learns to score pairs of regions to be merged, using as a reward the reduction of VRP cost gained by the merge. The VRP costs are computed by the generating agent, which is based on the attention model of (Kool et al 2019), that is known to perform well for smaller scale VRPs. <sep> Strong points <sep> The main contribution of the paper is the novel rewriting process that allows to decompose the problem into smaller subproblems, that can then be tackled with state-of-the-art methods. <sep> The numerical experiments show that this approach allows to efficiently solve large problems (2000 nodes). <sep> Weak points <sep> Many confusions in the text, in particular regarding the related works (see Feedback to improve the paper) <sep> To validate the hierarchical framework and the rewriting process, it would have been great to apply it to other large scale combinatorial problems. For instance, the multiple vehicle routing problem, where the rewriter would be responsible of assigning the customers to the vehicles. <sep> The authors do not mention whether they will release their code. <sep> Recommendation <sep> I would vote for accept. To me the hierarchical framework and the rewriting process are novel in this context and can potentially be applied in other problems. <sep> Questions to authors <sep> ""large-scale VRP is an unexplored and challenging problem"": VRP is one of the most studied problems in Operations Research. Among other aspects, there are of course works that aim at solving large scale problems. What do you mean by unexplored? <sep> In Section 3.3, after the selection and merging phases, the hyper-regions are again split into 2  ""regular sized"" regions, while maintaining the routes computed in the hyper-region. Is this always guaranteed to be feasible? Say you end up with 3 routes, of 10 customers each, in the hyper-region. How to split them in this case? <sep> ""These heuristics usually have a time complexity of O(n 2 log n 2 )"" can you share a reference for this claim? <sep> Sec 3.3 ""we restrict Gj to the K nearest regions to Gi"". How is K chosen? <sep> Table 1: The results reported for AM-sampling vs AM-train-on-100 are a bit surprising to me. The paper (Kool et al 2019) reported the best performance when the model was trained and tested on the same size. Do you have an idea of why it's not the case here? <sep> Regarding AM-train-on-100, was sampling or greedy rollout used? <sep> Table 2: are these results averages over a number of runs? Do you have an intuition about why the values are so close in all cases? <sep> Figure 3: what are the ""rollout steps"" here exactly? Do you have an explanation for the rapid drop of the ratio right before 100? <sep> Figures 7 and 8: Why are the routes not starting and ending at the depot in red? <sep> Feedback to help improve the paper <sep> In the introduction, ""since pre-defined rules are not suitable for various cases of customer distribution, those methods have poor generalization ability"". It does not make sense to talk about generalization ability for non-learning based methods. <sep> The ""exploration space of large-scale VRP"" / ""exploration complexity"" is confusing. I think you mean solution space instead of exploration space. <sep> There are confusions at several points regarding heuristics vs learning algorithms: ""....can be divided into heuristics and reinforcement learning (RL)"". RL-based approaches are also computing heuristics to solve the VRP, in the sense that they are not computing exact solution of the problem. So I guess here by heuristics, the authors mean non-learning based heuristics. <sep> The Ant Colony baseline of 1999 does not look competitive at all with the other standard Operations Research heuristics for VRP (here LKH3 and OR Tools) so I don't see the point in reporting its results (Table 1). <sep> Sec 4.1 ""…many realistic industrial applications in which a vast number of customers may occur frequently."" Do you have any reference for that? I would guess that when the number of customers is really large, it's more likely to resort to the multiple vehicle problem. <sep> Sec 4.3 ""…this is the rare case in real-world."" Do you have a reference? Intuitively I tend to disagree, customers are likely to be grouped within cities for instance. <sep> For a variety of VRP instances, you could use CVRPlib: http://vrp.atd-lab.inf.puc-rio.br/index.php/en/","The authors propose an RL-based approach, ""Rewriting-by Generating (RBG)"", to solve large-scale capacitated vehicle routing problems (CVRPs): such problems are NP-hard in general and are ubiquitous. The RL agent consists of a ""Generator"" and ""Rewriter"". In generation, the graph is sub-divided into several regions and in each region, an RL algorithm runs to get the best (or near-optimal) route. The rewriter then patches these near-optimal sub-solutions together using ""hierarchical RL"". <sep> The paper is generally well-written. <sep> One main concern is related to generalizability: the authors respond that their approach can work for other NP-hard combinatorial-optimization problems such as knapsack. The authors are encouraged to do a systematic study of several such (related) problems where their approach can work. It was also a concern that the overall approach of partitioning the input instance and rewriting the CVRP solution by merging regions and recomputing routes, is also employed by commercial OR solvers. The authors are encouraged to do a careful comparison (and perhaps melding) with such available solvers, to get a hybrid ""OR + ML"" improvement. It is also suggested that the authors include several different constraints from real-world VRP (e.g., heterogeneous vehicle costs, costs of missed shipment, route limits, upper-bounded number of vehicles etc.)."
"strength | weakness | suggestion  ==>  ==> Review summary <sep> The proposed models are theoretically sound, as it is as expressive as satisfying the universality theorem. Also, the proposed methods are empirically superior to existing methods. However, I think there is much room for improvement in the presentation of the paper. Besides, it is unclear to me what the research question is and how the proposed methods solve the problem. I would recommend reconsidering the organization of the paper. <sep> Summary of the paper <sep> This paper proposed a horocycle neuron that acts on the hyperbolic space. It uses the Hyperbolic Poisson kernel in place of the standard Euclid norm on the Euclidean space. This paper proposed an architecture called horocycle MLR, which used a horocycle neuron as a building block, and Poisson neural MLR. They showed the universality of a model with a single hidden layer of horocycle neurons or fa,p1, which has been used in the existing literature. They applied the horocycle feature to a subtree classification task of Poincare embedding, a horocycle MLR to a clustering task of 2D embedding, and horocycle and Poisson MLRs to classification tasks on image datasets. <sep> Claim <sep> If I understand correctly, this paper claims that the horocycle and Possison neurons are theoretically sound and empirically effective. However, it is not clear to me the research question that this paper addressed and how the theoretical and empirical properties of the proposed methods answer the question. It is true that they discussed the heuristic connection between the universal approximation property and the integral representation of the form (8) of a horocycle neuron. However, I think it is not a research question but supporting evidence that the universal approximation property is likely to hold. <sep> Soundness of the claims <sep> Can theory support the claim? <sep> The authors proved the universal approximation theorem for horocycle and fa,p1. Although it is not a constructive proof due to the Hahn-Banach theorem's nature, as the paper pointed out, it gives an affirmative answer for the theoretical justification and is a good first step to study the expressive power. <sep> If I do not miss any information, the Poisson neuron model (Section 4.2, Paragraph 4) is introduced without its motivation nor justification. In addition, this paper does not provide the theoretical superiority of the model. For example, Theorem 1 and Theorem 2 does not apply to the Poisson neuron model. I want to know if there are theoretical justifications for the Poisson model. <sep> Can empirical evaluation support the claim? <sep> Section 6.1: I confirm that the horocycle model's overall performance is better than Ganea et al. (2018a) and Shimizu et al. (2020). Especially, the proposed method significantly outperforms them when the embedding dimension is two, or the subtree is ""worker.n.01"". <sep> Section 6.2--6.4: I confirm that the proposed method's error rate is smaller than the existing methods. <sep> Section 6.5: I could not understand the motivation for the experiments of the CIFAR-10 and Fashion-MNIST datasets in this section. Figure 8 claimed that Poisson MLR shows good generalization in the Flowers dataset. However, this paper does not provide such a comparison in the CIFAR-10 and Fashion-MNIST datasets. Also, the performance in these datasets is not as good as the SOTA models (I referenced [1] for CIFAR-10 and [2] for Fashion-MNIST]). Therefore, I think these results do not support the empirical superiority of Poisson MLR. <sep> [1] https://paperswithcode.com/sota/image-classification-on-cifar-10 <sep> [2] https://paperswithcode.com/sota/image-classification-on-fashion-mnist <sep> Significance and novelty <sep> Novelty <sep> To the best of our knowledge, this is the first study that proves the universal approximation theorem for a single-hidden layer model on a hyperbolic space. <sep> Relation to previous work <sep> Although this paper mentioned Ganea et al. (2018a) and Shimizu et al. (2020), with which this paper compare the proposed method in the experiment, it did not compare the methodological difference (especially novelty and superiority) of the proposed method from the two. Same is true of the baseline methods in Table 2 and the method by Ontrup & Ritter (2005) and Grattarola et al. (2019) in Table 3. I would like to recommend to make it clear what is the drawback of the existing model. <sep> Correctness <sep> Is the theory correct? <sep> Yes, So far as I check the proof, Theorem 1 and Corollary 1 (universality of horocycle neurons and the function fa,p1) are correct. <sep> Is the experimental evaluation correct? <sep> Yes, I did not find any methodologically incorrect point in the experimental procedures. In Table 1, ideally, we should compare three methods with the same train/test partitions because the class label is highly imbalanced (e.g., 1115/82114 is positive in the case of worker.n.01 ); I am wondering if the performance variance caused by the randomness of data partition could be high. <sep> Reproducibility of the experiments <sep> Yes. It explains experimental settings in detail in the appendix. Also, it has a runnable code with trained parameters. <sep> Clarity <sep> I would say that there is much improvement in the clarity of the paper. <sep> First, I took some time to understand how sections are related and how paragraphs in a section are related. I think adding discourse markers and organizing sentences so that readers can do paragraph reading could make the paper more understandable. Take the introduction section as an example. I feel there is a large gap between the third and fourth paragraphs. In addition, I could not understand that the fourth paragraph intends to explain the horocycle neuron until I reached the end of the paragraph. Also, although the function fa,p1 is introduced in the fifth paragraph, the introduction does not mention it in the remaining part and goes back to the explanation of horocycle neurons. <sep> Another problem is that the tables and figures are not prepared appropriately. For example, Table 3 is inserted within a paragraph. Also, captions and legends of figures are tiny and hard to read. <sep> Additional feedback <sep> Abstract: The acronym MLR is used without what it stands for. So, I recommend writing the meaning of MLR without abbreviation. <sep> Section 1, Paragraph 3: This paper study → studies <sep> Section 1, Second bullet: Although this sentence mentioned the Poisson neuron and the horocycle MLR, they were not mentioned before this sentence. Similarly, the term ""end-based"" is used in the introduction but is explained in Section 4.2 for the first time. I would recommend writing their explanation before they are used. <sep> Section 2 Paragraph 3 (Hyperbolic deep learning): I could not see what this paper intended to mean by the term ""prototype"" at first reading. This wording may need some definition. <sep> Section 4.1, Paragraph 3 (Neuron models): What does the following sentence mean?: We accept the representation properties of eigenfunctions on compact manifolds. <sep> Section 4.2, Paragraph 5 (End-based clusters, end prototypes): It was hard for me to understand the relationship between sentences in the paragraph. For example, it is not clear at first sight how RBF is related to the discussion of clustering algorithms. I would recommend reconsidering the organization of the paragraph. <sep> Section 5 (9): ⟨x,ωi⟩ → ⟨x,ωi⟩H <sep> Section 6.1, Table 1: Could you explain what H2, H3, etc. means? <sep> Section 6.1: The task (Ganea et al., 2018a) is to classify all other nodes as [...]. → It is not clear what ""other"" nodes mean solely from the main text. I understand it after I read the first sentence of Section A.11. <sep> Appendix A.15: This paper says that it adds a loss to distinguish the prototypes of 4 and 9. However, looking at the code, it seems the algorithm randomly selects two classes and adds the loss from prototypes of these classes. I want to confirm if my understanding is correct and recommend explaining the procedure if it is correct.","Reviewers generally appreciate the contributions of the paper, namely the horocycle neuron, Poisson neuron, and the universal approximation properties. However, there are concerns, especially by R4 and R5, that the presentation is confusing, lacks clarity, and should be substantially improved. <sep> Note: Theorem 1.7 in (Helgason, 1970) is proved explicitly for the case n=2, not for general n as claimed in (9). Thus the Laplacian eigenspace motivation needs to be re-written/re-examined."
"abstract | strength | weakness | rebuttal_process  ==> Description: <sep> This paper aims to ""factor out"" existing prior knowledge from embeddings, by adapting a tSNE objective or from other methods by operating on input distances. The prior knowledge is assumed to be pairwise distances. <sep> In the tSNE case, a neighborhood distribution is derived from the prior pairwise distances, and a ""parameterized Jensen-Shannon divergence"" is proposed to measure deviation of the output neighborhood from that. The objective is then a difference of the tSNE objective and that divergence, optimized by gradient descent. <sep> In the general case, it is proposed to edit the pairwise distances of the data by, substracting out the pairwise distance given in the prior information (up to some constants), for all pairs of different points. A metric property of the definition is proven, and a proof that independent prior distances would not on expectation change the neighborhoods. <sep> Experiments compare the result to conditional tSNE and to a modification of supervised LLE, on synthetic gaussian-cluster data and on two real data (flowers and single cell sequencing data). <sep> On synthetic data, results are better than conditional tSNE but difference to the modified supervised LLE seems small. <sep> On flower data, there are no comparisons to others except unmodified tSNE, but the proposed methods may be able to show structure beyond the prior as claimed. <sep> In the single cell sequencing data, the comparisons are to unmofidied UMAP and to ctSNE. The UMAP with modified distances seems to show some batch-d and tissue-type differences. <sep> Evaluation: <sep> The idea of factoring out prior knowledge seems very meaningful, although not completely new. <sep> The approaches here seem reasonable but are somewhat simplistic: <sep> The tSNE approach seems reasonable but not very striking: essentially it is a weighted sum of two cost functions, and most of the detail is just to ensure the new divergence does not overwhelm the original one. <sep> The distance editing seems rather ad-hoc; while the result may have a metric property, it is hard to say in what sense this is the ""right"" way to combine the prior information to the distances. <sep> The experiments are rather limited, which is somewhat disappointing; in the flower case no comparison to ctSNE or SLLE-1 is done, or to SLLE-1 in the single cell sequencing data (too large matrix inversion issues are claimed - could those have been resolved e.g. by computing the result for a subset instead?). The results do seem to show ability to show information beyond the prior knowledge, but for a modern dimensionality reduction paper I would expect a more thorough evaluation. <sep> Comments: <sep> Neighbor embeddings that explore beyond known annotation have been proposed long before conditional tSNE: e.g. the method of [1] includes extraction of gene expression features unrelated to an ontological annotation, to embed structure not explained by that annotation. <sep> [1] Peltonen J, Aidos H, Gehlenborg N, Brazma A, Kaski S. An information retrieval perspective on visualization of gene expression data with ontological annotation. ICASSP 2010. <sep> The equation in definition 1 seems to be wrong. In the second divergence term, ""P' + (1-beta)Q"" is not a proper distribution that sums to 1. <sep> In the single cell sequencing data, you used marker gene expression for ""Z"" data, but what was the X data? Clarify. <sep> Figure 1: marker shapes are far too small to be seen at a normal viewing factor or in a printed paper. <sep> In experiments authors write that JEDI is ""not designed for labels as priors"" but in the introduction you write ""we consider background knowledge in the form of pairwise distances between samples. This formulation allows us to cover a plethora of practical instances including labels"". In what sense is JEDI not designed for labels as priors? <sep> The flower experiment seems somewhat biased because the input distance is a linear sum of distances including color-distance: therefore the distance-editing by subtracting the color-distance is of course well matched to this construction. <sep> In the single-cell sequencing case, to use ctSNE authors ""provide it the cluster labels from an agglomerative clustering"": why was this not done for the flower data too (and for SLLE-1 too)? <sep> In the single-cell sequencing case, Figure 3(a) is hard to compare to (b) and (c) since the coloring means different things. <sep> In Figure 3(c), why are there smooth changes of the batch-id colors? Are batches with close-by bacth numbers somehow expected to be more similar to each other?","A method is proposed for removing prior knowledge, presented as a <sep> distance matrix, from low-dimensional embeddings, to focus them on <sep> what is new. <sep> The task of visualizing novely in data is interesting and good <sep> solutions would potentially be highly useful. <sep> The proposed method essentially substracts a distance matrix from <sep> another. While this is sensible, it is not completely clear in what <sep> sense this is the right solution for what the embeddings will be <sep> used for. <sep> In final discussions among the reviews, the main remaining concerns <sep> were considered severe: comparisons to other methods being limited, <sep> and possible problems in one of the experiments."
"abstract | strength | weakness | strength | rebuttal_process | strength | weakness  ==>  ==> Summary: <sep> This paper proposes a method to train weight-quantized neural networks.  The authors propose to directly calculate the endpoints that minimize the quantization error according to the weight distribution of each layer. Empirical results on image classification tasks and object detection tasks show that the proposed method outperforms other compared weight quantization methods under the same number of bits. <sep> Strengths: <sep> The paper is clearly written and the proposed method is simple. <sep> Experiments are performed on image classification tasks CIFAR-10 and ImageNet, with extensive comparisons with other methods. <sep> Weaknesses: <sep> Since the Floyd algorithm requires alternating minimization between q and e, one concern is that this may be costly. How often is the Floyd algorithm called in Algorithm 1? What is the training time compared with other methods? <sep> Comparison with one important reference [1] is missing. [1] also learns the quantized values, but (i) through the step size instead of the endpoints,  and (ii) uses uniform distributed quantized values instead of non-uniformly distributed ones. However, the reported results in [1] show that their 3-bit, 4-bit and 8-bit quantized models on ImageNet have  smaller accuracy gaps with the full-precision baseline compared to the proposed method. <sep> One other concern is that while the authors claim that this method is proposed for compute-in-memory (CIM) system, and discussed in section 5 that the property of the CIM does not restrict the quantized values to be uniformly strictly, compared to digital systems, there is no empirical comparison of the efficiency (e.g. storage, latency) of the proposed method and other quantization methods in CIM. Do they just perform similarly in terms of computation and memory efficiency? If so, when the uniform quantization in [1] already has good accuracy, using generalized quantization as in the proposed method does not seem quite well-motivated? <sep> [1] Esser, Steven K., et al. ""LEARNED STEP SIZE QUANTIZATION."" International Conference on Learning Representations. 2019.","This work develops a weight-quantization method for deep neural networks that is suitable for a type of analog hardware system known as crossbar-enabled analog computing-in-memory (CACIM). The goal of this work is to train models on GPUs in such a way that they retain their predictive accuracy during inference when deployed on the analogue hardware system. <sep> Pros: <sep> Good adaptation of quantization methods to the CACIM system <sep> Simple method <sep> Validation of the proposed method on multiple datasets and models <sep> Cons: <sep> Lack of novelty: the proposed method is a simple combination of two popular methods, LLoyd's quantization and noise-aware training <sep> All reviewers appreciate the simplicity of the method and the good fit to the hardware. The authors responded to all reviews and two reviewers acknowledged the authors' response. The authors acknowledge some reviewer observations (motivation of quantization as reducing analogue noise, lack of experiments on the actual CACIM system), and the authors added an experimental evaluation on the actual physical CACIM system showing that their method performs well. <sep> Overall the work is well-executed and the proposed method is a good fit to the CACIM system. However, the proposed quantization method is a straightforward adaptation of popular quantization methods."
"misc | strength | weakness | rebuttal_process | weakness | decision  ==>  ==> Summary <sep> This work proposes CVaDE which is an extension of variational based deep clustering model (VaDE) with additional incorporation of prior clustering preferences as supervision. These priors guide the underlying clustering process towards a user-desirable partitioning of input data. The priors are provided in the form of pairwise constraints indicating which pair of samples belongs to same or different class. Clustering process is modelled using variational Bayes in which the clustering constraints are incorporated into prior probabilities with varying degree of uncertainty. The empirical results shows that in comparison to unconstrained clustering the small amount of pairwise constraints significantly improves clustering performance. Further, it demonstrates CVaDE's robustness to noise, generation capability as well as successful incorporation of different desirable preferences to drive clustering performance towards completely different partitioning. <sep> Quality <sep> The paper is well written albeit with numerous typographical error (some of which are listed at the end of this review). Experimental evaluation seems thorough. However, I would like to compare results on complex datasets as well as with large classes (> 10). Complex datasets includes STL-10, YouTube Faces, mini ImageNet etc. Please show efficacy on diverse sets of data covering large variation in number of classes, dimensionality, attributes. <sep> Moreover, clustering being unsupervised (here semi-supervised) one should not (rather cannot) employ different hyper-parameters for different dataset. Under the context of zero ground truth data availability, they should rather be fixed. Table 7 says otherwise. <sep> Originality <sep> As mentioned above, CVaDE is extended from VaDE but with prior input constraints. Thus the conditional ELBO loss objective is thus a simple extension of VaDE objective. Apart from this, the prior distribution used for pairwise constraints is adapted from work of Lu & Leen (2004). In summary, the work carries very little novelty. <sep> Significance <sep> Constrained clustering has been around for some time in various forms. However, the subtle difference CVaDE brings to the table is how to incorporate them into prior probabilities. <sep> Like VaDE, CVaDE is also clustering cum generative model. Once trained, model can be employed for sampling new data. Due to better training procedure using constraints, the generated samples is bound to be perceptually better. However, the samples are not better than the state of the art conditional generative models such as InfoGANs. <sep> Clarity <sep> In eq(2), shouldn't it be μzi instead of μxi. Is function f(zi;θ) not deterministic ? My understanding is given fixed μzi one can sample as many xi. Same goes for σxi. <sep> Figure 5, axis labels are missing. <sep> Under experiments, please make clear what are we solving for - z and c ? Have you tried k-means on extracted z post training ? <sep> What is penalty weight ? I did not find any description. <sep> Why C-IDEC cannot model different noise levels within same data set ? <sep> Where is the derivation for Conditional ELBO formulation ? In appendix I only find solution to C-ELBO not how to derive Eq (5). <sep> What is the impact of imbalanced dataset on CVaDE ? I presume apriori this imbalance is not known to the user. <sep> Eq (19), is E different from E ? <sep> Eq (19), Eq (20) summation w.r.t. is pulled out. Typo in Wij component. <sep> Eq (21), some of the terms are approximated by Monte carlo sampling while others are still taking expectation <sep> In Eq (18), If 3rd term is marginalised w.r.t. q(Z|X) then it is technically wrong to apply monte-carlo sample to central line in Eq (21). Remember 1L approximates q(zi|xi) which is applicable for 1st, 2nd and 4th terms. Not for all. <sep> Eq(12) δcicj is missing","We thank the authors for their detailed responses to reviewers, and for engaging in a constructive discussions. <sep> As explained by the reviewers, the paper is clearly written and the method is novel. However, the novelty is to combine existing ideas and techniques to define an objective function that allows to incorporate cluster assignment constraints, which was considered incremental. Regarding quality, the discussion highlighted some possible improvements that the authors propose to do in a future version of the paper, and we encourage them to follow that direction. Regarding significance, although the experimental results are promising there were some concerns that the improvement over existing techniques is marginal, and that more experiments leading to a clearer message would be useful. <sep> In summary, this is not a bad paper, but it is below the standards of *CONF* in its current form."
"rating_summary | abstract | decision | weakness | suggestion | weakness | suggestion  ==> The authors present a novel change detection test for non i.i.d. data motivated by applications in RL. At first, they provide an offline version of the test, then they extend it to the online setting. <sep> The paper is clearly written and presents both theoretical results and convincing experimental results. My two concerns are about the novelty of what has been proposed w.r.t. standard CDT procedures and on the fact that a consistent part of the material of what has been proposed in the paper is deferred to the appendix. <sep> I would like to have more discussion on the difference between what has been proposed here and the standard multivariate CDTs, for instance: <sep> Kuncheva, Ludmila I. ""Change detection in streaming multivariate data using likelihood detectors."" IEEE transactions on knowledge and data engineering 25.5 (2011): 1175-1180. <sep> Boracchi, Giacomo, et al. ""Quanttree: histograms for change detection in multivariate data streams."" International Conference on Machine Learning. 2018. <sep> A strong motivation of the novelty w.r.t. to the literature might make me increaese the paper score. <sep> In my opinion, the paper is not self-contained. Not even the main theorem proof are included (the sketches are not useful in understanding the proof line) and the experimental setting is described in details only in the additional material. I think you should rearrange some of the material from the appendix to the main paper and viceversa. <sep> I would have appreciated a more detailed description of Algorithm 1. In this version of the paper it is difficult to understand the procedure you proposed, if you do not refer to the appendix for details. <sep> In your setting the change in the episodic reward is only about the expected values. What happens if the new reward distribution changes in terms of covariance \\Sigma? <sep> Minor: <sep> ""in RL ... life-time of the task."" I would have preferred a citation about this statement. Showing evidence using your experiment is a bit premature at this stage of the presentation. <sep> assume strong assumptions -> require strong assumptions <sep> After rebuttals the authors significantly improved the presented work, including and discussing some relevant work which was previously missing.","The paper's initial evaluation was below par, but the author feedback helped clarify several crucial points after which two of the reviewers increased their scores by a point, bringing the current evaluation to borderline. <sep> The paper addresses a relevant and challenging problem in the RL domain. However, in my opinion, from the reviewers' and authors' remarks and from my own reading of the paper, there are concerns that need to be addressed before the paper can be publication worthy. Primary among these is the quantum of novelty -- as many reviews point out, the key idea of viewing an episodic trajectory as a multivariate (vector) sample for running hypothesis tests is not novel in itself, as is the claim that new tests have been devised. Another crucial issue is the (parametric) assumption of normality for the episodic reward sequence which is not adequately justified in the paper -- even a two time-step trajectory with normal rewards per state transition can exhibit a mixture-of-Gaussians type reward distribution for the second state, breaking the assumption. As it transpired from the reviews of Reviewer4, reducing environment shift/degradation to just a mean change problem, without even considering a change in the variances (2nd order statistics), seems to be too stylized to be effective. There are other, nonparametric approaches in statistics based on testing for changes in the distribution function (kernel density estimation approaches, for instance), which could perhaps be applied without normality assumptions and yield favourable results. The experimental results for detection delay often show significant overlaps of the delay distributions for different procedures (e.g., Hotelling vs. Mean vs. UDT etc.), which does not indicate an advantage of the proposed method. <sep> I would urge the author(s) to assimilate the feedback and delve deeper as to why and how parametric procedures based on normality assumptions may or may not succeed, so as to significantly strengthen the theoretical and practical message of this work."
"abstract | weakness | rebuttal_process  ==>  ==> Summary <sep> This paper introduces a new benchmark for imbalanced few-shot learning where the number of samples per class is different. The authors extensively evaluate 10 SOTA few-shot methods on this benchmark and show consistent performance drop in this challenging setting. They also show that simple over-sampling techniques can alleviate the imbalanced issue in few-shot learning. <sep> Pros <sep> -This paper introduces a new benchmark to evaluate the imbalance problem in few-shot learning.-The evaluation is quite extensive and includes 10 SOTA method under different imbalanced settings <sep> Cons <sep> -The authors ignore the long-tail recognition literature [1, 2, 3] which is highly relevant and in my opinion, more important than the proposed imbalanced few-shot learning problem. [1] introduced a long-tail recognition benchmark where the ImageNet classes are divided into many-shot, medium-shot and few-shot classes based on the number of training examples. This setting is more realistic because the statistics of real-world datasets also follow a long-tail. I do agree that the imbalanced problem is very important. But I am not convinced that the proposed imbalanced few-shot learning (or meta-learning) evaluation protocol is appealing to the imbalanced problem community. <sep> [1] Large-Scale Long-Tailed Recognition in an Open World. Liu et al., CVPR'19 <sep> [2] Learning imbalanced datasets with label-distribution-aware margin loss. Cao et al., NeurIPS'19 <sep> [3] Decoupling representation and classifier for long-tailed recognition. Kang et al., *CONF*'20 <sep> -The authors also ignore the generalized few-shot learning literature [4, 5, 5] which is closely related to imbalanced problems and few-shot learning. In particular, [4] introduced a benchmark that evaluates the performance on both base and novel classes where base classes have many samples and novel classes have only few shot examples. <sep> [4] Low-shot visual recognition by shrinking and hallucinating features. Hariharan et al., ICCV'18 <sep> [5] Low-shot learning with imprinted weights. Qi et al., CVPR'18 <sep> [6] Low-shot learning from imaginary data. Wang et al., CVPR'18 <sep> -There is no novelty except the proposed benchmark. The over-sampling techniques are standard and expected to improve the performance. <sep> Justification of the rating <sep> As an evaluation paper, the authors ignore a large group of highly relevant works in long-tail recognition and generalized few-shot learning. I think the proposed benchmark is somewhat incremental to the existing long-tail recognition benchmark and recommand a rejection. <sep> Post-rebuttal <sep> I have read the rebuttal and other reviews. The first version of this submission fails to cite any highly relevant works in long-tailed recognition and generalized few-shot learning. I believe that addressing this issue would require a major revision. The authors argue that their proposed few-shot imbalanced setting is different from the long-tailed recognition problem and generalized few-shot learning setting.  But this is only partially true. It is unclear whether previous approaches for long-tailed and generalized few-shot learning can be already applied to address the few-shot imbalanced problem. Moreover, I am not convinced that the proposed setting is more appealing than those two existing imbalanced settings because the 5-way classification problem seems to be an artificial setting that rarely happens in practice. Finally, I realize that the proposed setting is actually not new. [Lee et al., *CONF* 2020] have explored a very similar setting. Thus, I would keep my original review and recommend rejection.","The paper studies the effectiveness of few-shot learning techniques in settings where the training labels are imbalanced. While addressing an interesting practical problem, reviewers raised concerns about the paper's technical depth, insufficient distinction to existing techniques for coping with label imbalance, and limited qualitative conclusions from the results. The authors incorporated some of these comments in their revision, but a more comprehensive update on the latter two points appears appropriate."
"abstract | rating_summary | strength | misc | weakness  ==> This paper proposes regularizing the conventional MARL learning objective with a mutual information term to encourage more correlated behaviors among different agents. The contribution is clearly stated. However, the similarity to previous works is not sufficiently discussed, and the paper leaves out some important related works. <sep> The paper maximizes the mutual information between agents' policies, given the current states. To allow policies to be conditional dependent, the authors assume there exists a dummy variable. The contributions of the paper about the lower bound of the mutual information (Sec. 4.2) and policy update (Sec. 4.3) are not significant. The lower bound of the mutual information has recently been extensively explored in multi-agent settings, used for encouraging role emergence, minimized communication, and exploration. The policy update and policy improvement guarantee can be easily obtained based on soft reinforcement learning literature. <sep> ** Major concern: About related works. My major concern is not about the two contributions mentioned above. Instead, I think that the first and main contribution of this paper is a subset of previous works' contributions. EDTI [1] discusses how to maximize the mutual information between the \\emph{trajectories} of different agents. Their discussion already covers the correlation of policies of different agents at \\emph{a single timestep}. More importantly, the authors of that paper also point out that only optimizing mutual information between trajectories is not enough because the reward signal has to be considered for better policy learning. They even discuss the second-order influence between these mutual-information-based intrinsic rewards. The contribution of this paper seems to be the first part of [1]. Frans Oliehoek and other researchers also did lots of excellent works on this topic [2,3,4,5]. However, the discussion about these related works is absent from this paper. <sep> Additionally, the difference between the proposed method and Jaques's paper (social influence) is not significant. The only difference is whether the action of other agents, aj, is ajt or ajt+1 when calculating the mutual information (In Jaques's paper, they prove that their formulation is equivalent to a mutual information formulation). I do not think the author's definition is an improvement of that of Jaques. At least, the authors should provide a more serious discussion about this point, perhaps providing a matrix game to show that different timesteps do indeed make a difference. The authors may argue that their experiments show that their method has better performance. The problem is that SSD used by Jaques is a more challenging task than those used in this paper. Moreover, social influence is sensitive to hyperparameter settings and needs fine-tuning to reach its full potential. I will change my mind regarding the experiments if the authors could provide SSD results (adopting their methods to tasks with discrete actions is not very difficult). Besides, Jaques et al. also discuss how to make condition dependency clear between agents and how to carry out influential communications, which are not discussed in this paper. <sep> [1] Wang, T., Wang, J., Wu, Y. and Zhang, C., 2019. Influence-based multi-agent exploration. *CONF* 2020 spotlight <sep> [2] F. A. Oliehoek, S. Witwicki, and L. P. Kaelbling. Influence-based abstraction for multiagent systems. In Proceedings of the Twenty-Sixth AAAI Conference on Artificial Intelligence, pages 1422–1428, July 2012 <sep> Also see a recent longer version: https://arxiv.org/abs/1907.09278 <sep> [3] R. Becker, S. Zilberstein, V. Lesser, and C. V. Goldman. Transition-independent decentralized Markov decision processes. In Proceedings of the International Conference on Autonomous Agents and Multiagent Systems, pages 41–48, 2003. <sep> [4] Miguel Suau de Castro, Elena Congeduti, Rolf A.N. Starre, Aleksander Czechowski, and Frans A. Oliehoek. Influence-Based Abstraction in Deep Reinforcement Learning. In Proceedings of the AAMAS Workshop on Adaptive Learning Agents (ALA), May 2019. <sep> [5] Frans A. Oliehoek and Christopher Amato. A Concise Introduction to Decentralized POMDPs, SpringerBriefs in Intelligent Systems, Springer, May 2016.","Overview: <sep> This paper introduces a maximum mutual information method for helping to coordinate RL agents without communication. <sep> Discussion: <sep> Some reviewers leaned towards accept, but I found the two reviewers recommending rejecting to be more convincing. <sep> Recommendation: <sep> This is an important research topic and I'm glad this paper is focusing on the problem. Hopefully the reviews will help improve a future version of this paper. I agree that this is a new way of using mutual information, but it seems more like a small improvement rather than a very significant step forward. <sep> In addition, I think the setting needs to be better motivated. This is a centralized training with decentralized execution (CTDE) setting, and this paper helps the agents coordinate. In CTDE, the agents work in the environment and then pool their information to train before deploying on the next episode. I don't understand why, e.g., in multiwalker, agents would not be able to communicate while walking, can communicate after they succeed or drop the object (the episode ends), and then cannot communicate once the next episode starts."
"abstract | weakness | rebuttal_process | weakness | suggestion  ==> This paper analyzes the behavior of Bellman update in cooperative multi-agent setting, when the value function has the form of a linear value decomposition of individual Q function per agent. The paper assumes no partial observability (i.e., all agents can see all states) and shows that under deterministic dynamics and factorizable Dataset, after one update, the individual Q function has close form, building connection with COMA. The paper then shows with this function class, the Bellman update might diverge in some MMDP. By extending to a broader function class, the Bellman update is close again. <sep> The theory proposed in the paper looks interesting. I appreciate that the authors make an attempt to create a theory to analyze how the value decomposition fits to the Bellman equation, which is a fundamental question to ask. It can be a good contribution to the research community. However, there are several questions: <sep> Questions: <sep> The assumption of factorizable data and no partial observability seems to be super strong. What is the reason behind the two assumptions? If we remove them, to how much extent you conclusion still holds? <sep> I don't see very strong connections between the theoretical analysis and the empirical studies. Why ""|Q_tot^{FQI-LVD} - Q_tot^{VDN}| = 0.22 strongly illustrates the accuracy of Theorem 1""? Shouldn't we compare the two terms with relative error? Note that FQI-IGM basically removes the constraint of linear value decomposition. It looks like QMix is in FQI-IGM (the Q combination is nonlinear) and it should work according to the theory, but why it doesn't work in Matrix Game? <sep> The StarCraft experiment is kind of confusing. How is it connected to the theories? Basically you just show methods with more rich neural network models work better, which usually holds without the theory. Note that the dataset is constructed with a pre-trained policy (by VDN), I am not sure whether the factorizable assumption holds here? Did you check? It looks like a lot of ablation studies are needed to make the connection clear. <sep> ========= <sep> After rebuttal I will still keep the score. <sep> The Assumption 1 is still quite strong. While it is true that p(a|s)=∏ip(ai|s) holds for any policy with decentralized execution, the assumption of full observability is really strong and it doesn't seem that it can be got rid of easily. With this assumption, p(a|s)=∏ip(ai|s) is actually a trivial assumption, since all information needed to make a decision of action ai for agent i is already contained in the full state s and can be determined independent of each other. In the revised paper, the authors suggest that communication can solve it but this would require thorough communication over the entire MDP, which can be hard to achieve (and if that's easily achievable then there is no need to study Dec-POMDP anymore). Without relaxing this assumption, I have concerns that the theory is not substantial (which are also concerns from other reviewers like R3). One baby step is to at least assume each agent may receive a noisy version of the full state s, and see what's going on. <sep> I thank the authors for additional experiments. Note that in addition to the proposed theory, there are many possible explanations of the empirical results presented by the authors. E.g., as a general rule of thumb in RL training, using offline data is often worse than using online data. Without detailed analysis, it is hard to tell. I would rather use an environment that is more complicated than the matrix game, but much more simpler than SMAC, which is partial observable and has too many moving parts.","This paper begins to formalize a connection between value decomposition and difference rewards. Whilst we are in agreement with the authors that papers do not need to make new algorithmic contribution and purely theoretical papers that deepen our understanding of established methods can be significant contributions, all reviewers had doubts on the maturity of the theoretical contribution of this paper. <sep> Given the concerns raised by the authors for the attention of the area chair, I would like to reassure the authors that the majority of reviewers engaged in discussion after the rebuttal but remained unconvinced of the significance of the theoretical results. As these are representative of the potential audience at *CONF*, it is clear further improvements to the motivation given in the paper and/or weakening of the assumptions within the theory are needed to engage the interest of the wider machine learning community. <sep> The empirical studies in the paper also seem disconnected from the theoretical contribution and more like a continuation of the paper ""Qplex: Duplex dueling multi-agent q-learning."" Given the theoretical connection to difference rewards (e.g. COMA as explicitly noted by the authors in Implication 1) I would expect these methods to be included in the experiments to demonstrate how this theoretical connection affects performance in practical applications."
"abstract | weakness | strength | weakness | strength | rebuttal_process | weakness | decision  ==> The paper introduces a novel unbalanced Gromov-Wasserstein type problem. The Gromov-Wasserstein distance is very useful in practice for comparing probability distributions that do not lie in the same metric spaces. It has recently found several successful applications in ML for computational chemistry, graphs comparisons or NLP. Following previous works on unbalanced optimal transport (i.e. soft constraints over marginals enforcement of the coupling matrix), and the rationale that disposing of unbalanced versions of transport problems can alleviate in some ways presence of outliers or noise in the distributions, the authors propose two 'unbalanced' variants of the Gromov-Wasserstein (GW) problem, that allow comparison of metric spaces with arbitrary positive measures up to isometries (I.e. rigid transformations). <sep> The paper is fairly well written, original, and the related works is particularly complete. The theoretical part of the paper is sound, rigorous and well-motivated, and I learnt many things from it. The idea of using a quadratic \\phi-divergence is neat and particularly clever. Yet, the paper requires, to some extent, very good notions in (unbalanced) optimal transport and probability, and I wonder, if some notations could have been eased a little bit (for instance, it seems that D_\\phi is usually chosen as KL), but I guess the choice was made to be as general as possible.  While I did not check in details all the proofs in the appendix Section, I believe the work is solid. The algorithm parts is a little less satisfactory, as it amounts to optimize a upper bound  of the described problem, following existing work from [Mémoli11]. Though appealing, this upper bound is known to lose some properties of the original GW distance (for instance, two close points in the source can be matched to two distant points in the target if they share a very similar 'distance profile' or distance distributions wrt. the other samples). It seems that this effect is observable in Figure 2, where parts of a moon are 'flipped' in the matching. Combined with entropy and unbalanced formulation, I guess the final result can be very hard to interpret in a practical setting. <sep> Finally, the weakest part of the paper is the experimental Section. Only two toy examples are presented. While the method could have been used in many settings (graph classification, embedding matching in NLP or  even graph matching, for which many algorithms exist, etc.), it is very hard to conclude about the practical interest of the method. <sep> While I guess this is not a problem if one focuses on the original contribution of this novel unbalanced distance, it is more problematic, in the reviewers perception, for a machine learning venue such as *CONF*. <sep> For this reason, I will solely suggest a 'weak accept' decision, while I really believe the theoretical sections have a lot of merits. <sep> Minor comments. <sep> On page 1, it is said that « the paper defines for the first time a class of distances between the [aforementioned] objects ». This claim is a bit strong knowing that GW has already been used in several (cited) applications. I guess authors could be a little bit more precise on the meaning of this sentence; <sep> Page 4, just after definition 1, it seems (at least with my PDF reader), that there is a problem in the symbol used for \\phi on the last line of the paragraph; <sep> The claim that a cost of O(n^3) in time and O(n^2) in memory allows to scale to large problems is a bit strong. In practice, what is the maximum size of the problem that can be addressed in reasonable time with this method ? I expect that handling graphs that have more than 10k nodes for instance in very difficult. <sep> The final rescaling of \\gamma (line 8 of Algorithm 1), is a little bit difficult to understand from the alternating minimization point of view. <sep> In the experimental Section, which algorithm is used to compute GW ? Also, I guess Figure 3 could present the original graphs (without scaling the dots), to gain a better understanding of the original problem <sep> EDIT after rebuttal period many thanks to the authors for taking into consideration my comments. I decided not to change my note because I still believe the paper lacks of a significative experimental Section.","This paper present novel formulations to address the problem of unbalanced Gromov. The Conic formulation is very interesting but stays theoretical until optimization algorithms are available. The Unbalanced Gromov is a nice extension of Gromov and comes with relatively efficient solvers. Some very limited numerical experiment show the proposed UGW used between 2D distributions (two moons) and graphs. <sep> The paper had some mixed reviews with reviewers acknowledging the novelty of the approach (albeit an extension similar to unbalanced OT) and of the theoretical results. The detailed a very well written response to the reviewers comment has been appreciated. But all reviewers also noted a lack of numerical experiments outside of the very simple illustrations in the paper. This paper is a very nice contribution to the theory of optimal transport but fails at illustrating its relevance to the ML community. Despite acknowledging the theoretical contributions of the paper, the AC recommends a reject but strongly encourages the authors to complete the experimental section with some ML applications or at least proof of concepts (graph classification, domain adaptation, ...)."
"abstract | strength | weakness | suggestion  ==> ########################################################################## <sep> Summary: <sep> In this paper the authors attempt to solve the circuit routing problem through a novel approach of combining search-based routing techniques (e.g. A* search) and evolutionary strategies (e.g. OpenAI-ES). The authors define new parameters e.g. cost maps and a ranking parameter which improve the efficiency and effectiveness of their circuit routing solution. As opposed to heuristics based (2-step) routing solutions, the authors develop a solution which optimizes a single global cost function. This makes the solution easily scalable to new constraints and designs while avoiding sub-optimal solutions (common problem for 2-stage routing). The authors show that their approach doesn't require human demonstrations or training data of any kind thereby giving their solution an edge over learning-based techniques requiring training data. Lastly, through experiments the authors establish that their approach is scalable to larger applications. <sep> ########################################################################## <sep> Reasons for score: <sep> The authors present an all-round discussion for the different approaches to solve the circuit routing problem with their limitations. They also establish how the proposed approach excels where the previous approaches get stuck or give sub-optimal solutions. However, the authors do not present any information regarding the time complexity of their proposed approach. Along with that, the paper is also missing comparisons with the exact solutions proposed in past papers targeting this problem. Supplementary data and experiments can help alleviate these concerns. <sep> Updated: The authors added the requested timing complexity data and additional experiments with better baselines to compare the proposed RC algorithm against. They present valid issues with reproducing some of the previous work. While a theoretical proof establishing the worst-case time complexity of RC to be better than random sampling plus Seq A* would be ideal. The empirical data presented does support the claim that RC algorithm is useful for finding more optimal solutions for large maps faster than Seq A* plus random sampling. <sep> ########################################################################## <sep> Pros: <sep> The authors present a detailed description of existing approaches to solve the circuit routing problem along with the key limitations of the approaches discussed. <sep> Specifically, since the proposed approach does not require training data or human demonstration, it has a big advantage on other learning-based methods. Since, training data for routing problems is hard to generate (time-consuming) or find easily (lack of open source benchmarks/data). <sep> Section 4 presents a well written incremental description of the proposed routing approach starting from ranking learning to cost map learning to ranking cost. <sep> The experiments presented in Section 5, clearly demonstrate the superiority of the proposed approach over the baseline approaches considered in the paper. <sep> Lastly, the ablation studies clarify the limitations of scaling the random sample bucket for rank learning. Figure 2 b) does a particularly good job of illustrating the usage of Cost maps. Further, the authors demonstrate the scalability of their approach to larger applications. Which are missing in the cited learning-based previous approaches. <sep> ########################################################################## <sep> Cons: <sep> The presented approaches rely heavily on the ability of OpenAI-ES algorithm to learn the cost maps and ranking order that will help achieve optimal routing. It would be useful if the authors could present an analysis of the limitations of this algorithm. Are there any cases when it fails to converge to the optimal solution? Are there other alternative solvers that could replace this evolutionary strategy in such scenarios? <sep> Section 5, the choice of baseline approaches while covering the fundamental types of routing approaches (sequential, learning (RL) based, cost-map learning) do not show performance compared to the related works cited in the paper. For instance, Liao et al. (2020), He and Bao (2020), or 2-stage routing algorithms. <sep> While the authors correctly list the limitations of heuristic based (2-stage) routing algorithms, they fail to credit their simplicity which can be critical when routing very large-scale designs like VLSI chips. Such a tradeoff of simplicity vs optimality of solution would require a comparison of the time complexity of the different approaches analyzed in this paper and how they scale with the complexity of the problem. <sep> Albeit the heuristic based approaches might converge on sub-optimal solutions. However, if they are much faster than ranking cost, combining multiple trials of heuristics-based approaches with human expert interventions could give ranking cost a competition in finding the optimal routing faster. <sep> Above suggested analysis of time complexity will help understand up to what number of random samples of ranking orders can be tried, while matching the time taken by ranking cost, for Sequential A* and VIN baselines. Thereby allowing for a fairer comparison in the results presented in Table 1. <sep> ########################################################################## <sep> Questions during rebuttal period: <sep> Kindly address the concerns regarding time complexity of the proposed and past routing approaches as described in the Cons section. <sep> ######################################################################### <sep> Some typos: <sep> (1)    Section 2.3, ""Recently, Liao et al. (2020) trained* a deep Q-network"". <sep> (2)    Section 6, ""Our method is a* one-stage"".","The paper studies an interesting problem motivated by VLSI design. The reviewers agree that there are interesting aspects of the RC algorithm. Nevertheless, the paper could be improved by a clearer characterization/apples-to-apples comparison to baselines, particularly regarding computation cost, use of parallelism, as well as a more thorough contrast to state of the art in general. Given the contribution is experimental, and this is a well studied problem, it is important to establish whether the solution is indeed best-in-class; cost due to training should be taken into account, and minimized to the extend possible. Going beyond the baselines considered here, as well as reviewing possible theoretical connections to other problems and guarantees, would also strengthen the paper."
"abstract | weakness  ==> In this paper, the authors present a method to use unstructured external knowledge sources to improve visual question answering and image-caption retrieval. The proposed method can achieve somewhat improvement for visual question answering, but drop the performance for image-caption retrieval with a more complex model. Some concerns are as follows: <sep> The authors claim that the proposed method achieved state-of-the-art performance on both COCO and Flickr30k  image-caption retrieval. However, their retrieval scores are lower about 10 than the state-of-the-art counterparts, such as TERAN. The statement is not correct. <sep> Although the authors stated the proposed method uses raw images as input, the adopted backbones (i.e., image/text encoders) should be frozen to extract the features for the following components in their pipeline, which is similar to the other feature-based methods (e.g., TERAN) that also can be seen as freezing their backbones (e.g., Faster R-CNN) during their training and inference stages. Thus, the inputs between the proposed method and other methods have no essential difference. What is the significance to design such a much more complex model for image-caption retrieval? What are the advantages of the proposed method comparing prior superior methods? I am confused that if it is worthy to adopt such a complex model with worse performance. <sep> It is interesting to see that the proposed method could improve the performance of VQA. However, Table 3 does not give us a throughout comparison. There are many results missed in the table, such as different training types for Flickr30K, some results for Movie+MCAN, etc. From the results, we also could draw that the improvement of the proposed method is very limited for a good VQA method, i.e., Movie+MCAN with Vanilla. The experiments could not significantly demonstrate the significance and advantages of the proposed method.","The paper discusses the problem of how to augment cross-modal retrieval for the task of multi-modal classification -- it uses image caption pairs to improve downstream multimodal learning, and shows improvement in the task of visual question answering. However, the paper has the following weaknesses: (a) lack of novelty, (b) lack of thorough empirical evaluation, (c) the complex model did not give significant gains."
"abstract | rating_summary | weakness | decision  ==> Summary: <sep> Taylor polynomial based loss function metalearning acts as a regularizer that improves the networks adversarial attack robustness, performance, training time, and data utilization. The authors evolve weights, and so add arbitrary other factors to the loss, including adversarial robustness to learn a loss function parameterization which is more robust. They provide analysis of the attractor states under the optimization of a suite of loss functions. <sep> Quality: <sep> Writing Quality: <sep> The authors have paid attention to detail. The writing is succinct and precise throughout, and I was only able to find one typo though the first 8 pages of the manuscript. The progression between concepts is well motivated. <sep> Evaluation Quality: <sep> The evaluation methodology is novel and its results address the dynamics of training rather than static outcomes. The choice of adversarial attack robustness to demonstrate the value of optimizing the loss function for an alternate metric is a sound one. Validation acuuracy is used elsewhere to evaluate the generalization ability of the loss. Transfer of the learned loss across datsets and models is not evaluated. <sep> Result Quality: <sep> The improvements in adversarial attack robustness are large when optimizing TaylorGLO directly for that objective. The differences in attractor dynamics are dramatic (but also are less surprising). <sep> Clarity: <sep> Graphs & Tables: <sep> Figure 1, the attraction dynamics graph, is readable and quite clear. Unfortunately the entropy reduction definition of attraction in equation 17 can't be efficiently described in the caption in the same way that it is described in the text. <sep> Table 1's invariance results are clearly presented. The Welch's t-Test results generating P-value scores and the corresponding bolding is good. I would like to have seen a measure of meta-training stability or consistency in addition to or instead of accuracy, backing the claim about improved stability moving with the evolution population size. <sep> Figure 2's attack strength against testing accuracy plot is clean. Each axis' meaning is clear, as and the interpretation of the result is natural. <sep> The paper's writing clarity is very good. The background is comprehensible. The decompositions in section 3 are well factored. The disagreement with Blanc et al. (2020) in section 4.1 can be fleshed out in more detail, but the writing in the rest of the section is precise and succinct. <sep> Originality: <sep> One challenge with addressing the originality in the paper is understanding what novelty should be attributed here and what should be attributed to the original TaylorGLO paper. <sep> Novel evaluation methodology (attractor dynamics) are underemphasized relative to novel regularization results, and depends on the insight that the upside of the zero training error regime is that much of the continued update is all about the optimizer's bias and not about the training data. <sep> Significance: <sep> One major question in this work concerns the generality of its findings. Are these attractor dynamics specific to TaylorGLO? What are ther implications for other regularizers? <sep> The method's added complexity makes the method unlikely to be used unless it can clearly differentiate itself from other regularizers. For example, label smoothing is likely to create very similar attractor dynamics to the dynamics seen in TaylorGLO. The regularization effect (output entropy penalty) is also very similar. One differentiating feature is the ability to add other objectives (like adversarial robustness) to the learned loss. <sep> Pros: <sep> The novel evaluation methodlogy which relies on the insight that all loss function changes will have a downstream impact on the gradients is a nice addition to the loss function metalearning suite. Attractor dynamics, optimizing for an alternate objective like adversarial robustness and the demonstrated flexibility of TaylorGLO to cover label smoothing, MSE, Cross-Entropy, and more are welcome contributions. <sep> Cons: <sep> The appreciation of the existing loss function metalearning literature is poor in this paper. Loss functions are commonly learned with Neural Networks! These losses are often easier to optimize and are more flexible than standard losses. They can also make non-differentiable feedback differentiable. See this metalearning survey for plenty of references. https://arxiv.org/pdf/2004.05439.pdf <sep> Comparisons between taylor approximation paremeterized loss functions and neural network parameterized loss functions would have been an important comparison to see, but this side of loss function metaleanring isn't referenced. What are the attractor dynamics for those neural network learned losses? Is this different / improved? While many of these papers focus on reinforcement learning or unsupervised learning, they point to very similar improvements coming out of loss function metalearning. Ex, any of the following: <sep> Evolved Policy Gradient https://arxiv.org/abs/1802.04821 <sep> Learning to Learn: Meta-Critic Networks for Sample Efficient Learning https://www.researchgate.net/publication/318029457_Learning_to_Learn_Meta-Critic_Networks_for_Sample_Efficient_Learning <sep> Online Meta-Critic Learning for Off-Policy Actor-Critic Methods https://arxiv.org/abs/2003.05334 <sep> Online-Within-Online Meta-Learning (learned regularization) <sep> http://papers.nips.cc/paper/9468-online-within-online-meta-learning.pdf <sep> Meta-Learning Update Rules for Unsupervised Representation Learning https://arxiv.org/abs/1804.00222 <sep> Learning to Learn by Self-Critique https://papers.nips.cc/paper/9185-learning-to-learn-by-self-critique <sep> This paper doesn't focus on task generality. Many of the other metalearning loss functions papers do. Why? How general are their learned losses? Can they generalize from task to task, or does it have to be retrained every time? Why don't they discuss these issues? <sep> They don't release code for reproducibility. <sep> Notes: <sep> Ideally Figure 1's information would be shown for Bikal, MSE, and Label Smoothing as well (perhaps in the appendix) to assess whether TaylorGLO's training dynamics add anything on top of Label Smoothing (which one would expect to have the same transition to push away from the correct label in later epochs). But there the definition of zero training error itself is modified, and so their metric may not capture very similar optimization dynamics. <sep> The claim that TaylorGLO lowers confidence could be evaluated on calibration, rather than or in addition to entropy. <sep> TaylorGLO may be doing much more than regularization in practice, and the evaluiton criteria don't seem sufficient to know that more isn't happening to the model optimzed for this loss. <sep> It would be good to see the attractor dynamic graphs for label smoothing (presumably it is very similar to TaylorGLO). <sep> It appears that Theorem 4.2 basically describes label smoothing, though they don't say this. <sep> A typo on page 6! ""Thus, values less than zero imply that entropy is increased, values greater than zero that it is decreased, and values equal to zero imply that there is no change.""","The paper presents a method for meta-learning the loss function. The analysis mainly concerns the recently proposed TaylorGLO method on the (slightly less recent) Baikal loss. There was no consensus on this paper, but no reviewer was willing to fight for acceptance either. I found the paper not self-contained, with important non-standard elements undefined, starting with the Baikal loss, notations that are not defined in the main text, and a nomenclature that is also unusual with important terms such as ""attractor"" or ""invariant"" used in meanings that are non-standard in optimization or machine learning. <sep> Regarding content, most of the analyses refer to properties of the Baikal loss (not presented in the main text) that are deemed to be positive, without any theoretical support (Theorems 1 and 2). The inability to overfit is here posed as an obvious quality of a training loss. Then, a way to prevent the failure of the meta-training algorithm is presented in Theorem 3. Finally, an experiment is provided, showing that the proposed meta-training algorithm performs better than ""vanilla"" training with respect to adversarial attacks with FGSM. There is no comparison with other defense mechanisms and no analysis explains the results. Overall, although some interesting aspects may be developedin this paper, they are currently not well served by writing or the experimental evidences, so I recommend rejection."
"strength | weakness | suggestion  ==>  ==> Paper summary <sep> This paper addresses the problem of inferring spreadsheet formula from surrounding cell values and headers (i.e. brief description of each column). Compared to the standard programming-by-example line of work, which concerns synthesizing a program from a set of independent input/output examples, the problem setup proposed in this paper is more realistic and allows a model to synthesize a program by incorporating the contextual information. To this end, the paper proposes a framework that is specially designed to encode the header and the data from multiple rows and columns. To alleviate the difficulty of decoding a program from scratch, it proposes to first produce a program sketch that consists of formula operators and literals, and then predict the relative range which specifies where the formula operators should be applied. The experiments compare the performance of baselines as well as the proposed framework and its variations. The experimental results show that the proposed framework outperforms the rest and justify many design choices (e.g. leveraging a pre-trained BERT model, predicting a program sketch first, encoding rows and columns independently, etc.) I believe this work proposes an interesting and promising research problem as well as a framework that can achieve reasonable performance. Therefore, I vote for acceptance. <sep> Paper strengths motivation & problem setup <sep> The motivation for incorporating contextual information for programming by example is convincing. I believe this will make the neural program synthesis line of work more applicable to real-world problems. <sep> novelty & technical contribution <sep> As far as I am concerned, the problem setup is novel, since it includes header information, does not consider different rows independently, predicts a formula/program in a specified target cell, and allows to reference multiple cells. <sep> Leveraging a pre-trained BERT model seems crucial (~10% performance gap). <sep> Encoding row and column information separately and merging them with convolutions is compelling. <sep> Predicting a program sketch first to alleviate the difficulty of sequentially producing each token of a program is convincing. This paper presents an effective way to implement this idea. <sep> clarity <sep> The writing is very clear and the figures illustrate the ideas well. Also, the organization of the paper is easy to follow. <sep> ablation study <sep> Ablation studies are comprehensive. The proposed framework consists of multiple components. The provided ablation studies help analyze the effectiveness of each of them, including using a row BERT encoder only / using a column BERT encoder only / removing convolution layers that merge BERT's output decoding a program sketch and its range altogether (without the proposed two-stage decoding) <sep> using a BERT model that is not pre-trained on text corpora experimental result <sep> The presentation of the experimental results is very clear. The authors present insightful information, which includes: <sep> top-n accuracy performance vs. different program sketch lengths sketch only accuracy and range only accuracy <sep> The analysis of the experiment results is detailed and insightful. <sep> Paper weaknesses formula accuracy <sep> If I understand correctly, it is possible to synthesize a formula differently from how the ground truth formula is written. If this is the case, I wonder how formula accuracy is computed as one would need to enumerate all the possible ways of writing a particular formula to do so. <sep> pre-trained BERT for headers only <sep> Given the sampled shown in the paper, I am not sure if it is good to employ a pre-trained BERT to encode the content of the spreadsheet except for the header. Therefore, I would like to know if the authors have tried to use a pre-trained BERT for encoding headers only while using a BERT learning from scratch to encode the rest of the spreadsheet. <sep> noisy data/header <sep> I wonder how the proposed framework can deal with headers that do not properly/correctly indicate the meaning of the data. <sep> dataset availability & more samples <sep> It would be great if the dataset was provided so that the readers can better judge the difficulty of the problem and the performance of the models. Even if it is not possible to make the dataset publicly available, it would be better if a set of randomly selected data points (spreadsheet + target formula) were included. Also, providing some statistics of the dataset would be helpful, such as the length distribution of the target formula, the most commonly used operators, etc. <sep> reproducibility <sep> Given the clear description in the main paper and the details provided in the appendix, I believe implementing the proposed framework is possible. Yet, without access to the dataset, it is still impossible to reproduce the results. <sep> related work <sep> While the related work sections in the main paper and the appendix sufficiently cover most of the relevant works, I believe this paper can still be benefit from including the following papers <sep> Improving Neural Program Synthesis with Inferred Execution Traces <sep> Execution-Guided Neural Program Synthesis <sep> Neural Scene De-rendering <sep> Learning to Describe Scenes with Programs <sep> Neural Program Synthesis from Diverse Demonstration Videos  <sep> NL2Bash: A Corpus and Semantic Parser for Natural Language Interface to the Linux Operating System","This paper clearly has great ideas and reviewers appreciated that. However, the lack of experiments that can be validated by the community (only 1 experiment on the proprietary dataset) is an issue. We don't know if the reported accuracy is a respectable one (in the public domain). Having a proprietary dataset is a plus, but no public benchmark raises concerns about reproducibility. <sep> We recommend the authors to add some tasks and benchmarks for the community to check for themselves that the numbers reported are non-trivial."
"abstract | weakness  ==> Modified score: thank you authors for your thorough response. Given the new information and baselines, I think this is a promising paper that passes the acceptance threshold. <sep> Overall Quality: The authors present a method to improve the efficiency of transformer models when computing attention over previous time steps. Although this presents a neat idea that has the potential to improve an increasingly important model architecture, the experiments fall short of matching the claim that this method provides enables more efficient attention computation over memories in practice. Specifically, their baselines do not include relevant transformer modifications aimed at efficiency and they provide no detailed analysis on the memory size in practice. If the authors included more thorough experiments, this would be a strong paper. In their absence, it is marginally below the acceptance threshold. <sep> Clarity: The abstract, introduction, background, and methods section were detailed yet easy to follow. The comparison of time complexity of prior work in the background section was particularly helpful. However, this precision did not carry over into the experimental section, which lacked thorough experimentation (detailed under weaknesses below) and figures 3-5 were out of order relative to the prose (the latter point is minor and does not affect my rating). <sep> Significance: The potential impact is very high, especially as applications for transformers grow. If the authors could address the weaknesses outlined below, this could be an enormously helpful augmentation to the transformer architecture. <sep> Strengths: <sep> The authors focus on an important problem for a very relevant architecture. <sep> The writing is clear and enjoyable. Section 3 in particular is a very friendly introduction to transformer time complexity. <sep> Evaluations performed over a variety of applications, spanning simple/toy to more realistic tasks. <sep> Weaknesses: <sep> Corridor, instruction, portal, copy, pg-19, and colliding objects tasks only show comparisons for standard transformer models, as opposed to (at least one or two) comparable efficiency-optimized models. Giving the authors the benefit of the doubt, the first few experiments may serve more as proofs of concepts, where direct comparison with prior work is not as relevant or useful. But this leaves only one task in the paper with comparison to prior work on improving transformer efficiency: en-wiki-8. On en-wiki-8, the authors compare with just 1 modification and the improvement seems rather small. Small margins of improvement alone are not enough to reject a paper, but, given that this is the only result with a head to head comparison of efficiency optimized transformers, it makes it difficult for the community to discern the contribution of the work. Furthermore, on pg-19, copy task, and object collision, the authors do not provide the memory size/average memory size/effective memory size. This makes it difficult to understand if performance gains correspond with performance improvements, which is the methods stated purpose. <sep> Intuitively, an inductive bias to expire memories would make a learned model more brittle when transferring to new tasks. E.g., in the instruction task a new form of instruction may become relevant in a test task that was never relevant in training tasks Why is this a reasonable trade-off to make? <sep> Question: <sep> What value is shown in table 2? The caption says bit-per-byte, but the numbers are inconsistent with figure 7. <sep> in figure 11, how is memory computed?","The paper studies the problem of identifying what information to forget in attention mechanisms, with the goal of enabling attention mechanisms to deal with longer contexts. This is a simple yet intuitive extension: self-attention is augmented with an expiration value prediction. Experiments were carried out on NLP and RL tasks. <sep> Overall, the paper has novelty in the proposed idea, however, there are concerns about the strength of the experiments; that the experiments fall short."
"misc | strength | weakness | decision  ==>  ==> Summary <sep> The authors introduce a simple extension of PPO that uses a single expert demonstration and a modified sampling algorithm to show better performance than vanilla PPO in a difficult 3D setting. <sep> Strengths & Weaknesses <sep> Strengths <sep> The paper is well-written and the method is simple, yet powerful. Learning something from a single demonstration is no easy feat. <sep> The videos shown in the submission look impressive in how difficult the setup is and how the agent manages to learn complex strategies like. <sep> Overall, the paper is very ""short & sweet"" in that it's not a groundbreaking new technique, but a small change to PPO but it's well explained, and the results that are in the paper are good. <sep> Weaknesses <sep> The main problem I have with this is actually the lack of further experiments. For such an easy extension of PPO, I would've expected you to have no problem running this on an Atari environment and at least one MuJoCo environment too (where it's also easy to gather a human demonstration, like controlling the reacher via inverse-kinematics or the tricky Pusher). Compared to vanilla PPO we should see improvements across the board, no? <sep> Similarly, you just arrived at the hyperparameters p=0.1,ϕ=0.3 without explanation or ablation. Do you maybe want to justify how these parameters came to be and what happens if either parameter is higher or lower? <sep> TL;DR how to make me raise my score: Include at least one Atari and one Mujoco/Robot environment (since Ilya Kostrikov's implementation that you use supports these out of the box) and either add an ablation study on a single env over p/phi or explain the importance of these values. <sep> Impact & Recommendation <sep> This is fundamentally good material. It's not groundbreakingly new but I think it could make for an easy-to-use imitation learning baseline that would help in a lot of scenarios get the method off the ground. However, the current paper doesn't show the rigour and depth of analysis that I would expect from an *CONF* paper. I hope the authors can make up for that in the rebuttal week and then I'm happy to up my score. Currently, my recommendation is borderline. If the present method was really an all-around improvement over PPO, why did the authors not show it on a tried-and-true OpenAI Gym task but only in their own made-up setting? <sep> Minor Nitpicks <sep> I'd report a few more seeds - I think 5 seeds is a good starting point. <sep> Your plotting of runs is uncommon - usually, you either plot the mean and standard deviation or the mean and min/max. <sep> In Fig. 3 and 4, the fonts in the legend need to be bigger <sep> On page 6 you're twice in a row weirdly enthusiastic for Unity-ML / the ""flexibility allowed by this environment"". These are odd things to say in a research paper unless you're an employee of Unity <sep> Algorithm 1 is a bit verbose but great. Makes it very clear. On the flip side of that, Figure 1 is a bit redundant. These two things communicate the same idea and I like Algo 1 better. <sep> There are a few missing commas, like ""In our approach,"", bottom of the first page.","There was quite a bit of internal discussion on this paper. To summarize: <sep> The idea is very neat and interesting and likely to work <sep> The paper is likely to inspire future work <sep> There are still serious doubts about the experimental evaluation that is not entirely up to par with current standards <sep> The reviewers were not convinced 100% by the arguments about the 'custom' environments <sep> The reviewers were not convinced 100% that the baselines were given their best shot <sep> While the paper has potential to provide valuable input for the community, it needs a bit more work before being presentable at a highly competitive venue like *CONF*."
"abstract | strength | weakness | misc  ==> This paper builds on Kipf et al. (2018)'s Neural Relational Inference. In particular, this work introduces a latent variable model which treats the interactions (i.e. relations) between different agents as dynamic and time-varying. As in NRI, the interaction variable between any two agents is conditioned on the history of those agents' states. An agent's future state is conditioned on its history of states as well as its interaction variables with other agents. <sep> The results from DYARI are interesting but I am concerned about the intricacies of setting the ""inference period"" to be aligned with the ""dynamic period."" I would have expected that choosing a smaller inference period (e.g. half the dynamic period) should lead to little to no loss in performance. The authors ascribe the observed loss in performance to ""the extra uncertainty introduced by estimating more latent variables."" I'm not totally convinced. <sep> Physical systems like springs have an inherent temporal invariance which you don't seem to be exploiting. Did you consider (a) processing timesteps in a moving window of size T (i.e. z^{ij}_t is conditioned only on the last T observations) or (b) using a recurrent encoder to process observations sequentially, rather than process all timesteps at once with the PSPNet encoder? That might help with the inference challenge you're facing. <sep> More questions and requests for clarification: <sep> What if the inference period is out of sync with the dynamic period (e.g. 3 steps versus 5 steps)? Does the model still perform reasonably? <sep> Why not model relations as continuous latents (setting aside the fact that NRI used discrete variables)? That way you can model negative interaction (e.g. competition), zero interaction, as well as the magnitude of interaction. <sep> I don't understand Figure 8 at all. Consider the two blue players who stay on the leftmost side of the basketball court. They have a dotted blue line between them in all the ""coordination"" plots and a dotted red line between them in all the ""competition"" plots. Could you please explain what's going on? I would expect only one relation to be on at any time step. <sep> In Table 6, you show average pooling leads to higher accuracy but also a higher MSE. Shouldn't you have a lower MSE with a higher accuracy? <sep> Could you please add error bars for your reported scores over independent runs? <sep> What value of Beta did you use for your loss? What was the effect of varying this hyperparameter? <sep> Minor: <sep> In equation 2, I assume the density p(x^(i){t+1} | x^(i){< t}, z^{ij}_t) was meant to be conditioned on N-1 latents not just a single j? In fact, j is undefined in the equation at that point. <sep> Figure 2 shows the hidden interaction nodes z^{ij}_t are not conditioned on any other nodes. That is not true from equation 2: each z^{ij}_t seems conditioned on agent i and j's past trajectories. <sep> Could you please use x_{\\leq t} instead of x_{< t} to denote states up to and including time t?","This paper presents a method for relational inference in multi-agent/multi-object trajectory prediction tasks. Different from the neural relational inference (NRI) model [1], the presented method is able to model time-varying relations. Experimental results on physics simulations and sports games (basketball) show benefits over variants of the NRI model. <sep> The reviewers agree that the presented method is mostly solid, that the experiments are insightful, and that this is generally a well-written paper. The authors, however, have apparently overlooked recent related work [2] (dNRI) that proposes a very similar model. In the light of dNRI, it is difficult to argue for the novelty of the presented approach, and the paper needs to undergo a revision in order to more clearly differentiate it from the dNRI model, and to resolve the other concerns raised by the reviewers. <sep> [1] Kipf et al., Neural Relational Inference for Interacting Systems (ICML 2018) <sep> [2] Graber et al., Dynamic Neural Relational Inference (CVPR 2020)"
"weakness | rebuttal_process | suggestion  ==> This paper studied the two time scale A3C in discounted MDP based on recent development in the finite sample analysis of A2C. The sample complexity result in this paper matches previous result in two time-scale A2C in terms of the dependence of \\epsilon, and this paper further shows the benefit of ""linear speed up"" brough by the structure of A3C. Given the practical usefulness of A3C, the result established in this paper is meaninful. <sep> Although most of the technical proof seem correct to me, the algorithm studied in this paper has the following issues. <sep> (1) In the policy evaluation part (critic), samples are generated according to the the ""hybrid"" transition kernel \\tilde{P}, which is not reasonable. The purpose of the policy evaluation is to evalue the value function, thus the samples used by the critic need to follow the transition kernel P not \\tilde{P}. Otherwise, the value function learned by the critic would be different from the pure TD(0). I understand that the the author adopt this sampling method because the author want to make a ""two time-scale"" algorithm to study here. However, under such a sampling startegy only the actor can obtain appropriate samples. In fact, it is very difficult to design a two time-scale algorithm for a discount MDP, as the transition kernel required by actor and critic are different. I suggest the author to study ""averaged MDP"" instead. In the averaged MDP, both the actor and critic can share the same transition kernel, which make designing a two time-scale algorithm possible. <sep> (2) Another problem is the projection used in the critic. The projection in linear SA is a practical problem, as we do not have prior knowledge of the radius before we run the algorithm. The projection issue has been criticized by many researchers' in the RL community in the last few years thus should not be ignored here. In fact, the projection issue can be avoid if the author adopts a nested-loop structure, and I think the sample complexity result can even be  improved in the nested-loop setting. Thus I suggest the author to used nested-loop structure to avoide this projection issue. <sep> Overall I think the paper is well written. However, considering the two problems that I mentioned above, I think the current version is not ready to be published.","Although the reviewers found the paper well-written that analyzes a relatively popular algorithm (TD(0) version of A3C), there are concerns regarding the novelty of the convergence results given those for A2C, the comparison of the results with those for A2C, and the sufficiency of the experiments. Although the authors addressed some of these issues/comments during the rebuttals, it seems none of the reviewers is excited about the paper and there still exist concerns regarding the novelty of the results and how they are compared with those in the literature. I would suggest that the authors take the reviewers' comments into account, have a more comprehensive discussion about the relation of their results with those in the literature (two-time scale algorithms), and prepare their work for future conferences."
"misc | rating_summary | decision  ==> The paper proposes a new graph convolutional layer for graphs with relative position encoding. These types of graphs occur in applications such as meshes, point clouds, and super-pixel neighborhoods. The method extremely simple and generic. <sep> Strengths: <sep> The method is simple and can be easily integrated into existing frameworks. <sep> The method is more general than approaches designed directly for mesh processing <sep> Despite the simplicity, the paper performs well on the Faust node correspondence task. The results are close to that of recent approaches (Haan et al. 2020, Gong et al. 2019) which where directly designed for meshes, while this approach is more general. <sep> Weaknesses: <sep> Most of the experiments were devoted to super-pixel image classification, MNIST and CIfFAR which isn't a very compelling use case for this type of method. The paper would be stronger with more experiments on datasets like Faust where the data is not grid structured. It would be interesting to see results on point cloud data, where graphs could be constructed using KNN or other methods. <sep> Limited ablations. The features for message passing are derived from spatial gradients, it would be useful to know which operators are necessary. To my knowledge, without the gradient operators, the network reduces to something more like a generic graph convolutional network. Other parts of the paper say things like "" <sep> The term psuedo differentiable operators isn't defined in the introduction which makes it difficult to understand how this paper relates to other work. It is not clear from to me the novelty compared to (Tencer & Potter, 2020) <sep> Minor Points: <sep> The paper mentions trying voxel_grid and graclus but does not discuss what theses operations are for <sep> The organization of the paper could be improved. Experimental details are scattered throughout the paper. Implementation details should be moved from the method sections to the experiments section. <sep> Conclusion <sep> The proposed method is simple and seems to work well on super-pixel and mesh processing tasks. However, limited experiments make it difficult to assess the generality of the method to other tasks <sep> Post-Rebuttal Update: The response from the authors addressed several of my concerns and several clarity issues where fixed in the update paper. However, I don't think the results on ModelNet10 provide strong support for this method. While I don't think it is reasonable to expect this method to outperform other works which are specifically designed for mesh/point cloud inputs (given that this method is more general), I think there needs to be some application outside of super-pixel classification where the proposed method shows an clear advantage.","All three reviewers expressed consistent concerns on this submission in their reviews. In addition, none of them enthusiastically supported this work during discussion. It is clear this submission does not make the bar of *CONF*. Thus a reject is recommended."
"abstract | strength | weakness | rating_summary | decision  ==> The authors proposed a neural network architecture, Group-Connected Multilayer Perceptron (GMLP), which automatically groups the input features and extracts high level representations according to the groups. This paper focuses on classification problems. <sep> The architecture can be decomposed to three stages. The first stage is to automatically group the input features by multiplying soft-max of a routing matrix. At the second stage, a locally fully connected layer with the corresponding activation functions is used for each group, and a pooling layer merges two groups to a new group. At the final stage, all the groups would be concatenated and input to a fully connect layer to get the final output. <sep> The experiments showed that GMLP outperforms vanilla MLP, SNN, SET, FGR on seven real-world classification datasets in different domains. GMLP ensures higher accuracy with lower complexity compared to vanilla MLPs. <sep> The authors claimed that if we consider the groups as leaves, this method then becomes growing a binary tree from the leaf to the root. The extensive experiment results showed the effect of GMLP hyper-parameter choices, e.g. number of groups (number of nodes), width of each group (size of nodes) and type of pooling layer (way to built parent nodes). But in terms of a tree, it would be interesting to have some experiments to show the effect of the way combining feature groups. <sep> The experiments on the simulated Bayesian network dataset supported the claim that this architecture can utilize the fact that some of the features are not related and do not need to interact with each other. However, the architecture the authors used corresponds to the model that generated data, which is almost impossible in many real life problems. It can be helpful to have some results on simulated data with mismatched architectures from the model to help better understand the performance. <sep> One of the most important ideas in this paper is limiting the group-wise interactions. The size and number of groups the experiments chose would lead to many overlapped groups and many features chosen multiple times. It would be nice to have some analyses on the chosen groups and selected features, e.g. the existence of a set of features that always come into one group, and/or comparison of derived groups by GMLP and randomly chosen groups. <sep> A more detailed explanation of the dataset would be needed. For example, the authors used MIT-BIH dataset to compare the accuracy of GMLP and MLP with different sizes without introducing the dataset. <sep> If the complexity analysis of GMLP, equation (7), is only for inference, please also include the training complexity. Another concern is that the results suggest that the number of feature groups may need to be quite large (compared to the number of original features). In addition to figure 2, please provide the analysis of model size in addition to complexity analysis. <sep> GMLP selects features by using soft-max of a km×d matrix. The authors may want to investigate reparametrization tricks to solve similar problems, including concrete relaxation in the following references: <sep> C. J. Maddison, Andriy Mnih, and Yee Whye Teh. The concrete distribution: A continuous relaxation of discrete random variables. In *CONF*, 2017. <sep> Muhammed Fatih Balin, Abubakar Abid, and James Y. Zou. Concrete autoencoders: Differentiable feature selection and reconstruction. In ICML, 2019.","The paper proposes an MLP based approach for data without known structure (such as tabular data). At first, the data are partitioned into K blocks in a differentiable way, then the standard MLP is applied to each block. The results are then aggregated recursively to produce the final output. <sep> Pros: <sep> Handling less structured data is surely an important problem in machine learning and is much less explored. <sep> The paper is well written, easily understandable even with a fast browsing. <sep> The experimental results show some improvement. <sep> Cons: <sep> The approach is somewhat trivial, and the framework could be improved, see, e.g. Reviewers #3&#4. <sep> By the structure of the approach and the type of target data, a more reasonable comparison is with random forest (echoing Reviewer #1), which the authors added during rebuttal, rather than MLP etc. Maybe should even compare with deep random forest. Although the comparison with MLP etc. is quite favorable, the advantage over random forest is somewhat marginal (except on HAPT, which is a imagery data set and random forest may not be good at; also echoing Reviewers #1&#4's comment on why using imagery data, which do not fit the theme of the paper). Reviewers #3&#4 also had some concerns with the experiments. Reviewer #4 confirmed in the confidential comment that the performance improvement is incremental. <sep> Although the rebuttal seemed to be successful, thus both Reviewers #1 and #4 raised their scores, the average score is still at the borderline. Due to the limited acceptance rate, the area chair has to reject the paper."
"abstract | weakness | suggestion  ==>  ==> This paper proposes a new method applicable to a specific case in unsupervised learning: having access to a small amount of labels during the training phase where label signal is not used. <sep> First, I would like to say this is a well motivated task. Many recent works on self-supervised learning (image classification using instance discrimination signals) eventually need to use all the labels in the linear evaluation phase to obtain the final performance numbers. The authors also point out a similar view in the last paragraph in Section 3. <sep> In the related works section, the work by Khosla et al. 2020 is mentioned. I wonder if it makes sense for the authors to add their numbers in the experiment comparison given that Khosla et al also use the original SimCLR as benchmark? In addition, I think a ""soft-nearest neighbor loss"" paper could be cited and compared (for example: Zhirong Wu, Alexei A Efros, and Stella Yu. Improving generalization via scalable neighbor- hood component analysis. In European Conference on Computer Vision (ECCV) 2018, 2018.) <sep> I understand that most supervised contrastive learning frameworks assume full label during the entire process, which is different from the setting in the paper. But it would be interesting (even critical) to compare the proposed method vs the other supervised contrastive learning methods when SuNCEt is given the full data. <sep> The main contribution of the paper is the proposed SuNCEt loss, which is modified on top of the regular NCE loss in instance discrimination training. However, this ""supervised constrastive learning"" only accelerates the SIMCLR learning process (by ~2x in terms of epochs from Table 1 and Table 2) and it does not significantly improve the accuracy over regular SIMCLR. <sep> Overall, I feel that the proposed loss is not a very novel idea (i.e., supervised contrastive learning) and the experiment results are not significantly better than prior arts.","The paper proposes to speed-up self-supervised learning for semi-supervised learning by combining self-supervised pretraining and supervised fine-tuning into a single objective. The proposed supervised loss builds on Neighbourhood Components Analysis and soft nearest neighbor losses. Most reviewers are concerned about the novelty of the approach and the significance of empirical results. I agree with both concerns. I appreciate the comparison between log⁡∑exp and ∑log⁡exp, but it seems a simpler cross entropy loss also achieves a similar goal (potentially somewhat slower). I believe adding more experiments comparing different supervised loss functions across different architectures can help improve the paper."
"abstract | rating_summary | weakness | decision  ==>  ==> Summary <sep> The paper proposes an approach to employ successor representation combined with marginalized importance sampling. The basic idea exploited in the paper consists of expressing the occupancies in terms of the successor representation and to model it via a linear combination of some features. This allows handling, although approximately, continuous state-action spaces. After having derived the objective function, an experimental evaluation on both Mujoco and Atari domains is presented, including an ablation study. <sep> Major <sep> (About the linearity of the weight) Linear representations expressed in terms of a feature function are common in RL as the reward function can be often seen as a trade-off of different objectives encoded in the features. However, the choice of the linear representation in Equation (7) is based on the assumption that the marginalized weight is linear in the feature function. This assumption seems to me less justified compared to the one for the reward function. Clearly, a suitable feature design could overcome this limitation. Can the authors explain how the features \\phi are selected or learned? <sep> (Experimental evaluation) The results presented in the experimental evaluation are partially unsatisfactory, as also the authors acknowledge. It seems that there is no clear benefit in employing the marginalized importances sampling (both the baselines and the proposed approach) compared to standard deep temporal difference approaches. The authors suggest that this phenomenon can be ascribed to the fact that the quality of the marginalized weights is affected by the successor representation learned. I don't think this is the main weakness of the paper, but a reflection of the usefulness of the method in complex scenarios is necessary. Alternatively, it would be interesting to compare the proposed approach with DualDICE and GradientDICE on simpler tasks (maybe toy ones) in which DualDICE and GradientDICE work well. <sep> Minor <sep> The related work section should be moved later in the paper, maybe after Section 4 <sep> Pag 2, two lines above Equation (2): the transition model is here employed as a distribution over the next state s' and the reward r, but the reward function is considered separately in the definition of MDP presented before <sep> Figures 2, 3, and 4: the plots are not readable when printing the paper in grayscale. I suggest using different linestyles and/or markers <sep> Typos <sep> Pag 2: isn't -> is not <sep> Pag 2: doesn't -> does not <sep> Pag 8: the the -> the <sep> Overall <sep> The paper can be considered incremental compared to DualDICE. I did not find any fault, but I feel that the significance of contribution is currently insufficient for publication at *CONF*. In particular, for a paper that proposes a practical variation of a theoretically sound algorithm, the experimental evaluation is essential. I think that the results are currently unable to clearly show the advantages of the proposed method.","The paper is about an approach that combines successor representation with marginalized importance sampling. <sep> Although the reviewers acknowledge that the paper has some merits (interesting idea, good discussion, extensive experimental analysis) and the authors' responses have solved most of the reviewers' issues, the paper is borderline and the reviewers did not reach a consensus about its acceptance. In particular, the reviewers feel that the contributions of this paper are not significant enough. <sep> I encourage the authors to modify their paper by taking into consideration the suggestions provided by the reviewers and try to submit it to one of the forthcoming machine learning conferences."
"abstract | strength | decision | weakness | decision  ==> This paper proposes a new black-box attack that assumes access to loss-oracle of the target model. The attack exploits temporal and spatial correlation of block-wise pixel perturbations in a Bayesian optimization framework to achieve query efficiency. <sep> Pros: <sep> Overall, I enjoyed reading this paper. It is one of the few papers that employ scalable GP regression in black-box attacks. It also provides a novel formulation of how the action space of perturbation should be searched by optimizing a surrogate reward function over a low-dimensional space comprising image block location and a PCA-based feature. <sep> The proposition combines several aspects of different attacks from the literature in one framework: sign flips, Bayesian optimization, hierarchical blocking, and exploitation of temporal- and spatial-correlation in natural images. <sep> Cons: <sep> I have highlighted my remarks below and hope the authors address them. <sep> Comments/Questions: <sep> Fig 1: I appreciate the authors' attempt to motivate contextual bandits and slow varying property, but making this conclusion based on one image of one dataset (ImageNet) and one architecture (ResNet50) is not sufficient. Is this observation consistent over multiple images of multiple datasets and across multiple architectures? <sep> Table 1: NAttack achieves 100% on VGG16 and DenseNet. It is not highlighted in bold. <sep> Table 3: query limit ""1000"" -> ""10,000""? <sep> Page 2, Paragraph 2: What is the distinction between BayesOpt and Bayes-Attack. Most of the discussion is around BayesOpt. <sep> Page 2, Sec 2, Paragraph 2: Reference of AutoZOOM is missing. <sep> Page 2, Sec 2: The context in which NES and CMA-ES are presented gives the impression that they are adversarial attacks, please rephrase to indicate they are gradient-free optimization techniques. <sep> Eq. 9 : the notation is not clear. Is eijk of the same dimensionality as ∇xtl(xt,y)?. The definition of eijk does not imply that. <sep> It seems that most of the computational gains that CorrAttack has over BayeOpt and BayesAttack is due to the use of scalable GP regression (GPytorch) - not to mention the low-dimensionality of its GP. To highlight the benefits of the other aspects of CorrAttack, I think all (BayesOpt, BayesAttack, and CorrAttack) should employ the same GP tool <sep> Fig 3: Are the rewards across different block sizes comparable? If so, the plot suggests that using smaller blocks yields faster convergence to better rewards which in turn does not justify the hierarchical bayesian optimization search of Sec 4.3. <sep> PCA formulation: please elaborate on how PCA is employed to compute a single feature per block. In particular, is PCA performed for each block (pixel-level PCA) or over all the pixels of the image? <sep> Just as the authors compared the Bayesian aspect of the attack to Bayesian attacks (BayesOpt, Bayes-Attack), it would have been great if the authors could contrast the ""flip"" aspect of the attack with flip-based attacks---e.g., SignHunter & SIMBA ---that the authors mention in Sec 2. <sep> Ablation Studies: I appreciate the study but I think it lacks supporting evidence of ""hierarchical vs non-hierarchical attack"" and ""small (one-pixel) block vs 32-pixel block""","The paper presents a new Bayesian optimization method based on the Gaussian process bandits framework for black-box adversarial attacks. The method achieves good performance in the experiments, which was appreciated by all the reviewers. <sep> At the same time, the presentation of the method is quite confusing, which currently precludes acceptance of the paper. In particular, during the discussion phase the reviewers were not able to decipher the algorithm based on the description presented in the paper. It is not clear how the problem is modeled as a bandit problem, what the loss function ℓ is minimized and why minimizing it makes sense (assuming, e.g., that ℓ it the hinge loss as suggested and the initial prediction is good with a large margin, that is, the loss is zero, equation 6 never changes xt when the procedure is started from x). This connection, since it is the fundamental contribution of the paper, should be much better explained. Once the problem is set up to estimate (maximize?) the reward, it is changed to calculating the difference in the minimization (cf. equation 11), which is again unmotivated. (Other standard aspects of the algorithm should also be explained properly, e.g., the stopping condition of Algorithm 1) <sep> Unfortunately, the paper is written in a mathematically very imprecise manner. As an example, consider equation (6), where Bp and the projection operator are not defined, and while these can be guessed, a projection of the argmin seems to be missing as well in the end (otherwise nothing guarantees that xT, which is the final outcome of the algorithm, remains in the Lp ball). Another example is the Discrete Approximate CorrAttackFlip paragraph which requires that every coordinate of x should be changed by ±ϵ. It is also not clear what ""dividing the image into several blocks"" means in Section 4.1 (e.g., are these overlapping, do they cover the whole image, etc., not to mention that previously x was a general input, not necessarily an image). It is also unlikely that the stopping condition in Algorithm 1 would use the exact same ϵ for the acquisition function as the perturbation radius for adversarial examples, etc. While some of these inaccuracies and unclear definitions are also mentioned in the reviews, unfortunately there are more in the paper. <sep> The authors are encouraged to resubmit the paper to the next venue after significantly improving and cleaning up the presentation."
"abstract | strength | weakness | rebuttal_process | weakness | decision  ==> Summary <sep> The paper studies the importance of similarity between augmentations and corruptions for improving performance on those corruptions. To measure the distance between the augmentation and corruption distributions, the paper proposes a new metric, Minimal Sample Distance (MSD), which is the perceptual similarity between an average corruption and the closest augmentation from a finite set of samples sampled from the augmented data distribution. It is shown that MSD overcomes the drawbacks of distributional distance measures like Maximum Mean Discrepancy (MMD). A new benchmark, called ImageNet-C-bar, made up of corruptions that are perceptually dissimilar to ImageNet-C, is introduced. Using standard evaluation, it is empirically shown that several recent augmentation schemes show degraded performance on the new dataset, suggesting that they generate augmentations only perceptually similar to ImageNet-C and thus are prone to overfitting. <sep> +ves <sep> Although the notion of the relation between augmentations and test-time corruptions has somewhat already been empirically observed and stated in many previous works, the paper tries to correlate this relation statistically. To my knowledge, this is the first such work. <sep> Through comprehensive evaluations, the paper shows that the proposition of computing MSD rather than MMD correlates well with the relation between augmentations and corruptions observed in reality. <sep> A new benchmark, ImageNet-C-bar is proposed, which shows a useful result to the community that recent SOTA augmentation methods have degraded performance on the new dataset because they generate augmentations close to ImageNet-C corruptions. <sep> Concerns <sep> The paper says (pg 2) - ""we empirically show an intuitive yet surprisingly overlooked finding: Augmentation-corruption perceptual similarity is a strong predictor of corruption error"". However, this notion of the relation between augmentations and test-time corruptions does not seem very surprising. It's perhaps well-known that DNNs will generalize well only when test distributions are fairly similar to training distributions. Hence, the importance of this observation does seem to be overemphasized, although this is certainly of use. <sep> It's been recently shown that removing texture biases by introducing stylized transformations also improves robustness to common-corruptions (Geirhos et al, *CONF* 2019). However, stylized transformations don't look close to any Imagenet-C corruptions. If MSD of stylized transforms is large (which intuitively seems so), then it will mean that MSD is not a reliable metric in such a case. Was this studied? Would MSD be a reliable metric even in such cases? It would be good to understand this, to get a more well-rounded picture of the proposed metric. <sep> Paper introduces MSD as a distance metric. However, distance metrics should be symmetric in nature. It is not quite evident from just Eq-1 if MSD is symmetric. <sep> In addition, adding high severity corruption as training augmentation leads to better performance on the same low severity corruption but vice-versa is not true in general. This suggests that measures should perhaps ideally be asymmetric. Clarifying the notion of ""distance metric"" in this work may be important to make the work mathematically correct. <sep> Standard choices for measuring perceptual distances are VGG-16 or 19 networks pre-trained on ImageNet. However, the paper chooses to use WRN-40-10 trained on CIFAR-10. This seems to deviate from standard settings. The paper should explain the rationale behind their choice. It will be great to show an ablation on how this choice of feature extractor (VGG-19, Robust VGG etc) affects the MSD. Ideally, the metric should be robust i.e. shouldn't be sensitive to the choice of feature extractor. <sep> The practical utility of ImageNet-C-bar seems limited and unclear. The only use that I can think of is using it to identify overfitting on ImageNet-C. I would be happy to understand what I am missing here. <sep> POST-REBUTTAL: <sep> I thank the authors for the response and the revisions to the paper. I appreciate the authors' efforts towards the rebuttal. I am however left with some concerns which did not have a convincing resolution: <sep> Regarding the comment on how the proposed analysis would look for stylized transforms, the authors say in the response that ""...we don't expect that perceptual similarity is the only cause of improved corruption error, only that perceptual similarity is particularly salient for predicting generalization to dissimilar corruptions..."". The work seems to be one-sided in this regard. If stylized transforms don't look perceptually similar to ImageNet-C corruptions but provide robustness, this counters the proposed hypothesis. It then becomes important to say where the proposed analysis is meaningful and where it is not. This seems to be lacking at this time in the work. <sep> Regarding the robustness of MSD to the choice of feature extractor (as also asked by R1), the revised paper includes results on VGG as feature extractor (thanks to the authors for this), but uses a model that is finetuned for CIFAR-10. In general, perceptual similarity is studied directly taking VGG pre-trained on ImageNet - without finetuning on the target dataset. This leaves this question open, and makes one wonder if the latter features did not support the analysis. <sep> The utility of Imagenet-C-bar as an additional benchmark to check the goodness of performance on Imagenet-C seems a bit convoluted. Would we need a Imagenet-C-bar-bar to check the goodness for corruptions that may be beyond perceptual similarity (such as stylized transforms)? This is not very convincing. <sep> Overall, I am still on the fence on this work (and retain my original decision at this time). I think the paper does present an interesting insight, but I am not very convinced it has been studied and explored comprehensively enough. I would have ideally preferred to give a borderline (neither positive nor negative) decision, and will not be disappointed if the work is accepted, considering the interesting insights it offers.","The paper investigates the relationship between data augmentations used during training and their effect on the accuracy when evaluated on unseen corruptions at test-time. The paper proposes a metric called minimal sample distance (MSD) to measure the similarity between augmentations during training time and corruptions at test time. <sep> The reviewers agree that the paper aims to solve an important problem and the paper has some interesting findings. However, the current version has a few shortcomings: <sep> Some of the claims about ""overfitting"" are confusing, especially for data augmentations that use ops similar to those in ImageNet-C. This is already known and which is why some papers uses a subset of operations (e.g. AugMix uses a subset of AutoAugment operations). <sep> The main take-home message and novelty is unclear: The initial version titled (""Is Robustness Robust?"") seemed to argue that we may be overfitting to Imagenet-C, but the rebuttal and the updated version revised some of the claims (see response to R3 and R4). In light of the revision, I'm not sure how the main take-home messages differ from existing papers such as Yin et al. 2019 or ""Many faces of robustness"". <sep> One of the main differences is quantification of the distribution similarity, however, as pointed out by R2, this analysis does not explain when stylized corruptions would help, so the current version of the paper feels a bit incomplete to me. <sep> I recommend the authors to revise the draft based on reviewer feedback and resubmit the paper to another venue."
"abstract | rating_summary | decision | rating_summary | weakness | ac_disagreement | rating_summary | weakness | ac_disagreement | strength | weakness | decision  ==>  ==> Paper summary <sep> This paper focuses on domain generalization, targeting the challenging scenario where the training set might not include different sources; even under the presence of different sources, the problem formulation does not takes into account domain labels. The proposed solution is based on meta-learning, following the path drawn by Li et al. AAAI 2018; the Authors propose to adversarially split the training set in meta-train and meta-validation sets, and then update the current model in a direction that fosters good generalization performance on the meta-test. Results on standard benchmarks are encouraging. <sep> Pros <sep> The proposed idea is rather interesting, enabling to apply meta-learning solutions also in absence of domain labels. In particular, I like the idea of finding meta-train and meta-test splits in an adversarial fashion. This is crucial, since randomly splitting the training set in meta-train and meta-validation would not be helpful, since it would lead to episodes where meta-train and meta-test are iid. <sep> The Authors provide a theoretical interpretation of their approach (due to my background, I was not able to properly review it though) <sep> Cons <sep> My main concern is related to the way the maximization problem is tackled, in the objective in Eq. (5). I have reviewed Appendix C, but I cannot understand how convergence would require so few iterations. Even restricting the size of Sv as mentioned in Section 3., the number of possible sample combinations that generate couples (Sv, St) is huge -- if I understood the process correctly, then with |Sv|=K and |S|=N, the number of combinations is (NK) -- and for each of them the gradients that lead to the meta-update are different. Could the authors comment on this? Am I missing something? Related to this point, I am also concerned by the ablation study in Table 5 - where it is shown that, while helpful, the adversarial strategy does not help very significantly in the whole picture. <sep> The overall writing could be improved. Sentences like ""this model can be further transfored to a minimax problem"" in the Abstract are not properly exposed, and there are several examples throughout the manuscript. There is a misconception related to prior work: Carlucci et al. 2019 (as well as Volpi et al. 2018) also tackles the case where the training data only comprises a single source domain. This should be clarified in the Introduction/Related work. <sep> Review summary <sep> I like the idea this paper starts from, and I like the proposed solution. I still do not properly understand how the maximization problem at the core of the method is approached, and I believe that the paper needs some exhaustive proof checking to improve the overall writing. I look forward reading the Author response and iterating the discussion. <sep> ---- Post-rebuttal comments---- <sep> I thank the Authors for their explanations. Yet, I still believe that this work is not ready for publication. Random splitting and adversarial splitting perform very comparably (1% is not a lot), in my opinion casting some doubts on how meaningful the solution found to the proposed optimization problem is. The Authors included a toy example in the Appendix, but this did not mitigate my concerns on the original manuscript's experiments. I still believe that the core idea is very interesting, and hope that it will be further investigated by the Authors for a subsequent submission.","The paper is proposing a domain generalization method based on the intuition that an invariant model would work for any split of train/val. Hence, the method uses adversarial train/val splits during training. The paper is reviewed by three expert reviews and none of them championed the paper to be accepted. I carefully checked the reviews and the authors' response and agree with the reviewers. Specifically: <sep> R#1: Argues that the paper is not ready for publication. Also argues the optimization problem is only a motivation as it is not directly solved. This is an important issue and it needs to be addressed in a conclusive manner. <sep> R#2: Argues empirical studies do not show the value of train/val splitting. I partially disagree with this issue but it is clear that more qualitative and quantitative study is needed to properly justify the proposed method. <sep> R#3: Argues the contribution is not enough for publication. The paper is clearly novel but the contribution and novelty is not presented in a clear manner. Moreover, the empirical study does not complement the novelty. Hence, I disagree with the comment. <sep> Overall, I believe the paper proposes an interesting idea. However, the presentation and empirical studies need to be improved significantly. I recommend authors to address these issues and submit to the next conference."
"abstract | strength | weakness  ==>  ==> Summary: <sep> This paper proposes a framework to discover graph-algorithms that are learned by neural networks to solve combinatorial optimization problems. To this end, the authors propose (a) augmenting the input of DNN-solver using features extracted by existing combinatorial algorithms and (b) explaining the DNN based on an additional ""explainer"" model trained based on maximizing the lower bound of mutual information between subgraph of the input and the labels predicted by the DNN solver. I think this paper pursues an important and promising direction to extract algorithms from DNN-based solvers. However, I think (a) additional baselines should be incorporated for evaluating the DNN-solver, (b) the proposed explainer does not generate practically useful outputs for discovering new algorithms, and (c) the proposed explainer seems a bit flawed. <sep> Strength: <sep> The proposed augmentation scheme gives interesting insights and can be applied to other DNN-based solvers for combinatorial optimization. <sep> This paper tackles an interesting problem of discovering new algorithms from DNN-based solvers for combinatorial optimization. <sep> Weakness: <sep> It is not clear why the authors only consider baselines with polynomial-time running time. To show that the proposed DNN-based solver is practically useful, the authors should compare with state-of-the-art solvers (both DNN-based and non-DNN-based) under limited running time. To compare the algorithms based on the tradeoff between complexity and approximation ratio, the authors should provide a theoretical analysis of the approximation ratio of the proposed algorithm. <sep> It is not clear how to use the proposed algorithm for discovering new graph algorithms. Especially, the algorithms produce results that are not very ""explainable."" For example, how did the authors use the explainer to ""re-discover the node-degree greedy algorithm?"" The proposed framework seems to assume that humans can easily analyze the provided explanations (i.e., graphs with colored nodes). However, it seems hard for me to analyze the pattern in node-degree of nodes just by glancing at several explanations provided by the proposed framework. Such a process becomes especially harder if we aim to discover algorithms based on novel concepts (instead of node-degree). Even more, researchers are usually interested in developing algorithms for large-scale graphs (where exact solutions are intractable), yet the explanation becomes notoriously hard to analyze in the eyes of humans for this case. The authors are encouraged to provide an actual process of extracting analysis on the explanations, e.g., did they (1) look at hundreds of explanations provided by the explainer, (2) intuitively group explanations with the common pattern in node-degree, and (3) infer a greedy pattern in node-degree of the selected nodes? <sep> The proposed explainer seems flawed for explaining the DNN-based solver. Namely, the proposed explainer only accesses the DNN-based solver based on its prediction probability. However, I do not think it makes sense to rely only on the prediction to explain the black-box algorithm. To demonstrate, both the brute force search and integer programming solver will give an identical prediction (i.e., optimal solution) for the combinatorial optimization. Applying the proposed explainer to two algorithms would give an identical explanation, but brute force search and integer programming operate in a very different way. <sep> I appreciate the thoughtful rebuttal provided by the authors. My main concerns are on Q2, i.e., the practical usefulness of the algorithm. I do think that the authors provide a convincing argument on ""we can only understand what we can understand,"" hence we should set up a hypothesis and see if it aligns with the explanation. However, I think the usefulness is not well-supported in the current state of the paper. The authors can come up with (a) a stronger example of such a hypothesis and (b) a better measurement of how the hypothesis aligns with the explanation to strengthen the paper. <sep> Regarding Q3, I still think that it is not correct to provide the same explanation for different algorithms when they produce the same output. Hence, the proposed algorithm should be modified to consider this aspect.","This paper proposes a method for automatically discovering graph algorithms using GNNs. In general, the reviewers find the paper well-written, and the problem and the approach interesting. However, there is a concern on the practical usefulness of proposed method as shown in the following comments: ""My main concerns are on Q2, i.e., the practical usefulness of the algorithm""[R1]; ""It sounds like the proposed model is hard to generalize to different datasets"" [R3]; ""The proposed explainer does not generate practically useful outputs for discovering new algorithms""[R4]."
"abstract | misc | rating_summary | weakness | suggestion  ==> The authors propose to extract two types of explanations: abductive and contrastive explanations to address a gap in the literature of explainable AI. Indeed, that's a great point and often explainable models address the ""why"" and rarely the ""why not"" that can help identify the features guiding the change in the class. <sep> The ideas presented are compelling and it is good to see that we can re-use state of the art AI first order logic (FOL) statements in the field of explainable models. The references are excellent. <sep> However, the paper suffers from several drawbacks: <sep> 1- The setting is limited to ML models that are expressed as a set of FOL sentences <sep> 2- Discretization of numerical features is required. We know static discretization can be problematic (large versus small interval); no discussion is in the paper to how to address this point <sep> 3- More details should be presented about SHAP since this is the main method the authors compare to; <sep> 4- Figure 1 part c) is not explained. There seems to be missing parentheses in the FOL statement; <sep> 5- The experimental section is rather weak.  The example on the real-vs-fake digit is not clear. Pointing out the brighter pixels as those responsible of the classification is not convincing to me. Comparison to Shap (using correlation) is not discussed. <sep> The second experiment. provides some statistics on the time, number of abductive and contrastive explanations. Perhaps it would have been good to provide other examples of the importance of extracting both explanations and how pertinent they are. Overall, there is a need for some baseline to validate the explanations (both types); <sep> 6- The methodology should be made clearer, notations introduced or re-introduced, many readers might not be familar with some notations like entailments; <sep> Overall, I feel there are good ideas in there, the authors should consider enlarging the spectrum of the applicability of their approach, may be rework the definitions and methodology section and design more solid experiments. <sep> Minor comments: <sep> seeFigure","The authors consider local 'why' or 'abductive' explanations for a model and a given class, which identify a minimal subset of features such that they're sufficient to imply that the model predicts the class; and 'why not' or 'contrastive' explanations, which identify a minimal subset s.t. they're sufficient to imply that the model predicts a different class. The two types of explanation are related using earlier work on minimal hitting sets going back to Reiter (1987). <sep> Reviewers were divided in their opinions. R4 was very positive but with little detail and only medium confidence, then did not participate in discussion. R2 was the only reviewer with high confidence, leaning against acceptance. The paper relies on FOL which was hard for reviewers to grapple with, and may make it challenging for readers. The presentation could be improved by clearly linking to existing work and demonstrating why the new approach is important."
"abstract | strength | misc | weakness | suggestion | decision | suggestion | misc  ==> I generally like the neat idea of introducing hierarchical spheres to model the intra- and inter-class relationships among the hierachical labels. It is naturally motivated to combine the hierarchical structure in the label space as a prior knowledge to supervise the training of neural networks. The overall idea is simple and easy to understand. The method models the labels at different levels by adding a free vector that is constrained on a specific hypersphere, then uses a smart way of formulating this procedure with simple matrix multiplication, and finally considers an alternative manifold optimization method to train the neural network in an end-to-end fashion. As far as I'm concerned, the intuition has also been explored in [Deep neural decision forests, ICCV 2015], but differently, the ICCV 2015 paper considered the hierarchical label structure with a decision forest. I believe this direction is of sufficient significance to the ML community. <sep> This paper has several aspects that I found most interesting: <sep> (1) The formulation is interesting and is novel from my perspective. Modeling the fine-grained classes by successively adding a ""perturbation"" vector makes sense to me. Then, the authors are able to formulate this in a matrix multiplication, which is basically a linear matrix factorization that over-parameterizes the classifiers. Although technically the linear matrix multiplication is still equivalent to a linear classifier, the fact that it can still improve the network generalization is interesting and is partially verified by a number of theory works. Besides, as a way to combine prior knowledge to supervise the neural networks, such a simple linear matrix factorization (with some constraints like sphericity, radius decay, etc.) provides a potentially useful way to incorporate some regularization priors. <sep> (2) The use of spherical constraints is interesting and empirically make senses to me. By constraining the learning on the spherical space can ease the training difficulties of the over-parameterized classification layers. This is, in fact, also observed and verified by [Neural Similarity Learning, Neurips 2019]. It will be potentially interesting to connect these two papers and have some discussions. The radius decay for the hierarchical spheres is also novel to me, because labels in finer-grained level should be modeled with less capacity. <sep> Desipte these interesting aspects, I also have a few concerns and suggestions to improve the paper: <sep> (1) The empirical evaluation is relatively weak and the evaluation metric seems not to well reflect the advantages of hierarchically modelling the label space. For example, I think it will be more informative to incorporate the classification accuracy of the super-classes. It will make this paper more interesting to have more experiments that analyzes the difference in feature distributions between normally trained neural networks and the hierarchically trained neural networks. For example, an intuitive visualization of the feature space will be of great interest. An easy way for the visualziaiton is to set the outpute feature dimension as 2 and directly plot them, similar to [A Discriminative Feature Learning Approach for Deep Face Recognition, ECCV 2016] and [Large-Margin Softmax Loss for Convolutional Neural Networks, ICML 2016]. <sep> (2) Some important ablation studies to justify some heuristic designs are very important and necessary. For example, there is a hyperprameter in the radius decay, how it will affect the performance is crucial. Potentially, the authors can also evaluate what if no sphericity constraint is applied, or what if no radius decay is used, etc. Since this paper proposes a number of heuristic designs, it is very important to justify them (either from theoretical perspective, or from empirical evaluations). <sep> (3) Although I believe it is useful to model the hierachical label space in an explicit way, the empirical evaluation does not really convince me on that, especially experiments on CIFAR-100 and Tiny-ImageNet. The method uses additional prior knowledge on the label space, but only yields very limited performance gain. I think using some other SOTA regularization can easily improve more. What is the underlying reason? I think more discussions and insights will be useful. <sep> (4) The usefulness of the hierachical label structure should be evaluated and verified in the first place. A simple way to evaluate it is to use some random assignment or simple K-means assignments for the super-classes. If using the ground truth hierachical strucutre can consistently outperform the random or K-means super-class assignment, then one can believe that incorporating the ground truth hierachical label structure is indeed useful. Until then, it makes little sense to argue it is beneficial to generalization to combine the ground truth hierachical label structure. I highly suggest the authors conduct such an experiment. <sep> Some minor concerns and suggestions: <sep> (5) I cannot find the Spherical CNN from Xie et al. (2017) on page 1. I think the paper is more closely related to [Deep Hyperspherical Learning, Neurips 2017] in terms of the spherical regularization. The authors may discuss the connections and differences to this paper. <sep> (6) Since the authors consider regularizations for the intra-class hierachical label structure, it will be interesting to see whether the regularization on the inter-class regularization will be beneficial or not. For example, the authors can use some diversity regularization on sphere to push away classifiers from different super classes. A potential regularization for this is [ <sep> Learning towards Minimum Hyperspherical Energy, Neurips 2018]. I want to note that it is a suggestion for the paper rather than a weakness. <sep> To summarize, I think the paper proposes a very interesting and potentially widely useful method to incorporate the hierachical label structure to train neural networks. Currently, I feel postive to accept this paper, and I am sitting between 6 and 7 (I give a 6 for now). I will consider to increase my score if the authors well address the concerns.","This paper introduces a method for hierarchical classification with deep networks. The idea is interesting, and as far as I know novel: namely, the authors add a regularizer to the last layer in order to enforce a hierarchical structure onto the classifiers. The idea of placing spheres (with a fixed radius) around each classifier and forcing the child-classifiers to lie on these spheres is quite clever. <sep> The reviewers have pointed out some concerns with this paper. Some had to do with terminology (which the authors should fix but which is no big deal), but the main weakness are the experimental results and the ablation study. The reviewers were not convinced that the optimization in the Euclidean space wouldn't be sufficient. A more thorough ablation study could help here. <sep> This is the kind of paper that I really want to see published eventually, but right now isn't quite ready yet. If you make one more iteration (in particular adding a stronger ablation study) it should be a strong submission to the next conference. Good luck!"
"weakness | rating_summary  ==> Post-rebuttal update: Thanks to the authors for engaging in the discussion and for the responses. The authors have provided a satisfying argument that with an appropriate choice of hyper parameters, the SQLoss does promote cooperative behaviors whenever all utilities are negative -- this addresses my main concern regarding the validity of the SQLoss objective. I remain skeptical of the value of the visual Coin game, because the results do not disentangle the usefulness of the cooperation objective and the clustering from GameDistill. I (and it seems, R1 and R3) had several concerns about the presentation of the material: mine in particular about lack of clarity, statements provided without motivation, and lack of details in Section 3, both for SQLoss and GameDistill. The authors have provided clarifications to some of these concerns in the response, but the new revision of the manuscript does not seem to reflect any of these changes. I would not be strongly opposed to acceptance, conditional on the visual coin game ablations and clarifications being added in the final version. Nevertheless, it is hard for me to recommend acceptance, given the number of unseen changes that still need to be made to the paper. <sep> Summary: This paper studies the problem of learning cooperative behaviors in social dilemma problems for multi-agent deep RL with independent agents. The authors propose a simple loss that emphasizes the ""status quo"",  which increases the emphasis on the one-step reward, to force agents to avoid short-term exploitation. To extend this idea to a temporally-extended sequential setup, the authors introduce a clustering algorithm that reduces the game into a matrix game with ""cooperate"" and ""defect"" options. <sep> Assessment: The paper studies an interesting problem: without explicit communication between two agents, how to impose protocols that ensure convergence to cooperative / optimal behaviors. The paper is also generally well-written and quite easy to understand, which I appreciated. Nonetheless, the algorithmic contributions of the paper appear lacking and incomplete. The paper does not current convince me that the proposed solution SQLoss actually solves the cooperation problem (discussion below). The GameDistill algorithm is an interesting way to reduce general environments to matrix games, but seems highly instrumented for the current environments, and unlikely to work in more general environments (discussion below). Due to my concern and confusion about the effectivity of the algorithms proposed, I currently tend to reject the paper. <sep> Major Concerns: <sep> (SQLoss) From what I ascertained from SQLoss, it changes the policy gradient to re-weigh the reward at the current time-step higher than future rewards. The text does not make it clear to me why this should force / encourage cooperative behaviors. I do understand that SQLoss will ""cause an exploited agent (in DC) to ... quickly move to (DD)"", but what remains confusing to me is why SQLoss will force a system in (DD) to move to (CC). If the system originally starts in a place of mutual defection (or more generally, prefers mutual defection to mutual cooperation), how does the SQLoss incentivize either player to switch to a cooperative behavior? If each agent could reason about the other agent's learning procedure, then this behavior might emerge, but the PG update does not do this: it is conditional only on the other agent's current parameters, and not how it learns. With the PG update, even when the reward at the first time-step is emphasized, neither player is incentivized to increase the probability of cooperative behavior, since cooperative behavior has strictly lower future value. One experiment that might help resolve this confuion is to show that SQLoss also induces cooperation even when the initial agents prefer mutual defection. <sep> (GameDistill) The GameDistill algorithm is an interesting clustering method for turning a sequential game into a static game where strategies correspond to ""options"", but seems rather specific to the current tested environments. While the GameDistill algorithm handles discrete behaviors that ""cooperate"" or ""defect"" well, it doesn't seem to capture games where there are potentially multiple (and unknown number of) ways to cooperate or defect, or where there are graded levels of cooperation. A comment about how well GameDistill handles environment stochasticity / etc would also be helpful. Also, since it requires the practitioner to train several intermediary models (a trajectory encoder, a clustering model, an action predictor), it seems to me that the solution may be sensitive to hyperparameters, and thus not scale well to other environments. This final point is hard to judge in the current setup. <sep> (Experiments / GameDistill) It is unclear in the experimental section, whether the performance of the SQLearner agent is due to the GameDistill algorithm reducing the dimensionality of the problem to only choosing between two options, or if it is because the SQLoss actually promotes cooperativeness. Do the other baselines, e.g. Lola-PG and SL also use GameDistill? <sep> Additional Comments: <sep> The derivation on Page 5 is incorrect: Equation (7) does not result from ∇θE[R^01(τ1)], since R^01 contains ""imagined"" trajectories only for the first time-step, and not for subsequent timesteps (that is, it does not contain any terms involving R^t1(τ1) for t>0). It is not immediately clear to me that there is a simple objective function for which (7) is the gradient. <sep> Why does the learning behavior of the SQLearner spike immediately to the optimal policy (after the initial period of being flat)? <sep> The related work section appears to be comprehensive, but I am not well-versed with the multi-agent literature. <sep> The code for running experiments is provided, which is a positive point for this paper. <sep> Minor Comments: <sep> ""Each agent independently attempts to maximize its expected discounted return"": For this sentence in Section 2.1, what does the agent assume about the other agents? That the other agents are fixed? That the other agents can act adversarially? <sep> The word loss is used throughout for referring to both ""a loss function"", and a ""bad reward"" -- this makes the discussion in places confusing <sep> Use the math operator log instead of log","In general there is agreement under reviewers that the ideas/method presented are somewhat interesting/promising but also that the paper lacks a lot of clarity. Reviewers agree that the paper needs more work (on the method) and more extensive experiments to be convincing, and that in its current form it is not mature enough for publication at *CONF*."
"abstract | weakness | decision  ==> UserBERT: Self-supervised User Representation Learning <sep> ###################################################################### <sep> Summary: <sep> The paper provides an extension of BERT to user data for pre-training user representation in a self-supervised manner. In particular, it analogise the user behaviour sequence to words in a sentence and leverages the Masked Language Model (MLM) approach typically used in NLP to train the user embedding. To facilitate such extension, the paper also proposes a discretisation approach and a unified input structuring to include long-term, short-term and demographic information. <sep> ###################################################################### <sep> Pros: <sep> Even though, the idea of extending the self-supervised pre-training for user representations is not new, it is still an interesting area of research <sep> The discretisation of user behaviour signals over long-term and short-term to form ""behavioural words"" is quite reasonable <sep> ##################################################################### <sep> Concerns: <sep> The key concerns about the contributions of the paper are as follows: <sep> Overall, the novelty of this work is very limited. To elaborate: <sep> The major contribution of this work is two fold: <sep> Discretisation of raw user behaviour sequences (for long-term and short-term) and <sep> Using the discretised aggregated ""behaviour words"" as inputs to the BERT architecture as is. <sep> The other claimed contributions such as having a unified architecture and experiments to validate the approach do not seem substantial. <sep> When it comes to ""discretisation"", though the idea seems appropriate, two crucial questions regarding this step are not validated: <sep> There is no empirical evidence presented in the paper which shows ""discretisation"" improves UserBERT's accuracy. Since it is a major contribution, I request the authors to design and implement an ablation study to address this point. <sep> Seemingly, the authors have come up with a hand crafted/heuristic-driven  approach for discretisation. Why can't it be data-driven too? Meaning, can clustering of actions be done in a data-driven manner? If so, what is the difference in accuracy between the proposed heuristic-driven and the data-driven alternative. <sep> When it comes to the second contribution, it is certainly not novel i.e., the paper does not propose any architectural change to BERT. Though the paper claims that the presented model is a unified model to learn long term, short term and demographics based user profile, the unification is brought upon as a by-product of feeding multimodal inputs to vanilla BERT. <sep> Hence, the overall novelty of the paper is very limited. <sep> The following comments are my major concerns in each section of the paper: <sep> Section 2: <sep> While the authors have reviewed some literature in transfer learning and self supervised learning and have cited some relevant work, they have not cited even one reference in section 2.3 which on ""User Modeling"" i.e., the main theme of the paper. I request the author to make a thorough survey and cite related work in section 2.3 and also highlight how UserBERT is different from them. <sep> Section 3: <sep> Overall, this section (and section 4) lack cohesion and can be written clearer with the figures, tables, algorithms and descriptions. This could help the reader better understand the approach. For instance, the following main points in the approach are not explained well: <sep> The paper states ""the final loss for one input sequence is the weighted sum of the losses of all masked tokens"". There is no detail what the weights are, whether they are assigned based on heuristic or learnt. <sep> The approach considers ""ordinal attributes"" such as expense and age similar to ""categorical attributes"" (e.g., each age has a unique embedding). This seems counterintuitive and there is no empirical evidence to show that this counterintuitive design works well. <sep> It is not clear how to use the hidden representation to predict attributes from the transformed masked tokens. More precisely, it is possible that many attributes belonging to different actions are masked and then converted into one token embedding. So what attributes are to be predicted in the final fully connected layer, in this case? <sep> Minor concern: what does E stand for in equation 1? <sep> Section 4: <sep> Overall, in this section, the experimental design is not comprehensive, and the results are not convincing for the following reasons: <sep> Along with the Wide&Deep, LSTM, Transformers as baseline, it would have been better to also include vanilla BERT to the baselines against which the UserBERT can be compared. In fact, Vanilla BERT would be the closest and most appropriate baseline for comparison. Hence, I request the authors to include it. <sep> All the experiments are conducted on custom datasets. Since user profiling is an extremely useful and ubiquitous activity that benefits multiple domains, I request the authors to experiment UserBERT on well-known open source e-commerce (and other user profiling) datasets (ref: [1] and [2]). In fact, the profiles could be tested on downstream tasks like ""next genre prediction"" with these datasets. This will help the reader to trust the UserBERT model better. <sep> Input representation, being one of the major contributions of the paper, it would give more insights if an ablation study is made on the user behaviour data (long-term features, short-term features, demographic features) to compare and contrast the contribution and lift by each of the behaviour categories <sep> In the attribute prediction task, within the two attributes experimented, the performance of the proposed model is quite unconvincing. Would benefit if more experiments are performed. <sep> For the ""next genre prediction"", though there are more than 10k genres, each users' typically have a very small subset of interest. Therefore, it would be more informative if can compare the model's mAP@10 with the user-level mode's mAP@10. <sep> The discussions of results are very vague and could be a lot deeper and precise. <sep> ##################################################################### <sep> Questions during rebuttal period: <sep> Please address and clarify the concerns above <sep> ##################################################################### <sep> Reference <sep> [1] Sun, Fei, et al. ""BERT4Rec: Sequential recommendation with bidirectional encoder representations from transformer."" Proceedings of the 28th ACM International Conference on Information and Knowledge Management. 2019. <sep> [2] Kang, Wang-Cheng, and Julian McAuley. ""Self-attentive sequential recommendation."" 2018 IEEE International Conference on Data Mining (ICDM). IEEE, 2018.","The paper discusses an extension of BERT for learning user representations based on activity patterns in a self-supervised setting. All reviewers have concerns about the validity of the claims and the significance of the experimental results. Overall, I agree with the reviewers that the paper needs more work to be published at *CONF*. I recommend rejection."
"rating_summary | weakness | suggestion  ==> Update <sep> I thank the authors for their comments and answers. While they agree on some of the concerns I raised, others are still left open. <sep> I believe they could be addressed in new major revision of the paper and I encourage authors to do so. <sep> Summary <sep> This paper qualifies as a discussion around architectural choices when using variational-autoencoders (VAEs) for multi-modal learning. <sep> The main claim is that architectures supporting the mixture-of-experts (MoEs) paradigm favor benchmarks where modalities appear in an 'OR'-relationship, while those implementing products-of-experts (PoEs) favor 'AND'-relationships. <sep> To show this point and in 'defense' of PoE-VAEs, the paper introduces a missing-value imputation experiment over MNIST images to evaluate the second aspect. <sep> As another contribution, the authors introduce the SVAE as a variation of a sota PoE-VAE, the VAEVAE, for bi-modal learning. Specifically, they introduce a variation that employs auxiliary encoder networks for each modality and derive accordingly a new composite ELBO loss to train them. <sep> % <sep> The point raised by author is reasonable, but the execution of the experimental section and little relevance of the introduced SVAE limit the value of the paper in its current state. Detailed comments follow. <sep> Presentation <sep> The paper is generally understandable and well-written. <sep> The introduction might benefit a rewriting as to gently introduce the larger scope of multimodal learning and then focusing on the more recent advancements, such as PoE-VAEs and MoE-VAEs. <sep> For example, the first lines of the first paragraph directly jump to PoEs without providing the reader enough background. <sep> I suggest authors to clarify that the scenario they are tackling is that of generative modeling in presence of missing data on the inputs (as a modality missing is a special case of missing not at random) more than referring to it as semi-supervised learning. <sep> I recognize that the authors follow a recent trend in the VAE-literature, but classically, semi-supervised has been referred to as the case when missing values are on the output, in a clear discriminative setting. <sep> Some typos are present, e.g., ""which corresponds to ""AND"" combination of the modalities and favors the MoE architecture"" (should be ""OR""). Clearly, they can be easily fixed by an additional proofreading pass. <sep> Contributions <sep> On the one hand, the discussion of the pros and cons of PoE-VAEs and MoE-VAEs boils down to one conjecture ('AND' versus 'OR' modality fusion) that is shallowly tested in the experimental section. <sep> Which can be greatly strengthened, see comments below. <sep> On the other hand, the introduction of the SVAE architecture, while potentially appealing, seems a minor contribution after seeing that VAEVAE and MMVAE generally outperform it in the graphs and tables in the experimental section (with the exception of the first experiment, joint modalities and one single percentage of low supervision). <sep> I would advice authors to explicitly say what is the benefit (maybe didactic?) of introducing the SVAE as a new model. <sep> Furthermore, a limitation of a certain regard concerns dealing with bi-modal data only in the text and in the experiments. <sep> This makes unclear what is the price to pay to scale these architectures to truly-multimodal data. <sep> For examples, Eqs. 7-1 suggest that the new composite ELBO can be extended to include uni-modal ELBO terms for each modality. However, one ordering over modalities for conditioning (according to Appendix B) shall be chosen, and it is not clear how this can influence learning and inference (in a similar fashion variable ordering influences autoregressive models). <sep> Architecture-wise, it is not self-evident how many additional encoder components are needed for more than two modalities. If more than one per modality, then the challenges should be discussed in depth. <sep> Experiments <sep> As already stated, experiments are limited to bi-modal data only. <sep> One additional downside of the experiments is that only coherence inter-modalities is measured as a metric. Sample/modality quality is not discussed, not even reported in a qualitative way (for the exception for some samples for the CUB dataset and some MNIST image reconstructions only for SVAE in Fig.3). <sep> I suggest authors to report the FID scores (or any other suitable variant like KID or precision-recall curves) of the joint and single modalities for the generated images to assess their quality. This is a fundamental aspect as modalities can be coherent but very far from the true data distribution or still not exactly close to the reconstruction, which is just a mode of the whole distribution. <sep> Along this direction, one shall evaluate conditional sampling and not only reconstructions to see if the VAE have collapsed to pointwise densities. <sep> Lastly, the CUB experiment provides some empirical evidence that is hard to evaluate or pose in the context of generative modelling. <sep> In order to follow the MMVAE paper, the authors are decoding images not in pixel space, but in the latent space of a  ResNet-101. Then the showed images are the nearest neighbours in the training data. <sep> This is the opposite of the generative modelling paradigm, and misleading: equivalently accurate and good-looking final images could come from a model memorizing the training set. <sep> I advice authors to evaluate the quality of generated samples and reconstructions in the pixel space (with the metrics discussed above), alternatively, to introduce a proper decoder for the ResNet-101 embedding or not to include the above experiment at all.","The reviewers found this to be an interesting and clearly-written paper, but broadly agreed that it is not yet ready for acceptance. In particular, multiple reviewers felt that the experiments don't show clear benefits of the proposed SVAE approach when compared to the VAEVAE and other baselines; nor do they sufficiently back up the central claim regarding relative benefit of PoE vs MoE for either ""AND"" or ""OR"" relations. Hopefully the comments and suggestions from the reviewers, particularly regarding framing and experimental validation, will help in revising the paper."
"abstract | strength | weakness  ==>  ==> Summary: <sep> Efficient training is becoming a crucial research topic as deep learning models get deeper and more complex to improve model accuracy. The authors propose a low-bit floating point quantization method to reduce energy and time consumption during training. To enhance training efficiency, the authors suggest multi-level scaling (MLS) tensor format that enables low-bit training. MLS is designed to compute complex operations with lower-cost operators (e.g., shift, add) or lower-bit operators that can be hardware-friendly. This reviewer considers the proposed hardware-friendly MLS format as the main contribution of the manuscript. <sep> This reviewer raises the following serious concerns. <sep> I have no idea whether this work can help efficient training in practice. Since there is no prior information on how many bits are required for training without serious accuracy degradation, any efficient training method needs to be robust to various learning settings and dataset configurations. This work, however, studies the optimal formats for particular models, such as ResNet models. It would be required to suggest a general data format and demonstrate that such a format can be applied to various DNN models. <sep> The authors argue that the proposed method achieves over 6.8x higher energy efficiency than training with floating-point (FP) arithmetic units. Unlike training with FP, the proposed method requires dynamic quantization for every batch. The authors need to present the computational overhead of the dynamic quantization with a sufficiently thorough analysis. If the cost is negligible, the ground should also be provided. Also, it would be better if the authors suggest the expected time reduction and space complexity compared to full-precision training. Even though Section 5.2 briefly addresses such overhead of dynamic quantization (as to be comparable with that of a batch normalization), this reviewer cannot estimate the overall benefit of the proposed method compared with the previous works. <sep> Overall, this reviewer votes for rejection. An efficient training algorithm needs to be supported by detailed experimental results on various types of DNN models while any computational overhead should be reasonably addressed. <sep> Minor comments: <sep> Section 2 needs to be focused on training while the comparisons with previous works can be elaborated in the experimental results. <sep> Table 1 is not relevant unless the authors want to suggest ResNet-specific accelerator designs. <sep> Similarly, Table 2 and 3, and Figure 3 are not necessary if the authors claim that the flow in Figure 2 can be general.","The authors propose a low-bit floating point quantization method to reduce energy and time consumption for deep learning training. Dynamic quantization and MLS tensor arithmetic are used to enhance the effectiveness of MLS. The motivation is clear and the efficient training is an important problem to address. However, the effectiveness of proposed method is not well justified and experimental results are less convincing. In addition, the clarify of paper still needs to be further improved."
"weakness | rebuttal_process | rating_summary | decision | suggestion  ==> The authors propose a method for crosslingual sentence retrieval that uses images to ground sentences in different languages and to project these sentences into a meaningful semantic space. The data used in the model are image-caption pairs in each language, and never parallel sentences in two or more languages. Authors use existing English-language image captioning datasets (Flickr30k, MSCOCO, Google Conceptual Captions) and translate the English captions into 51 other languages using a machine translation system, therefore bootstrapping image-caption pairs in 52 languages including English. The paper is very well written and easy to read. <sep> Some positive points: (1) the method is simple and elegant, and seems to produce strong results compared to a few other baselines; (2) the collected dataset will be released with the research community; (3) parts of the experimental setup were thoughtfully done, e.g. how to create the data splits across languages/images. <sep> Some negative points: (1) the framing of the paper is not adequate, i.e. authors propose a model for ""unsupervised multilingual translation"" but they propose a model for crosslingual retrieval; (2) there are issues with the evaluation of the models, i.e. machine-translated data is used not just at training time but also as validation/test data; (3) parts of the experimental setup could be improved, i.e. a more thorough comparison to crosslingual retrieval models in the recent literature. <sep> I recommend that the paper do not be accepted for publication in its current form for all the reasons mentioned above. I will provide detailed comments on these points below. <sep> My first comment has to do with the framing of the paper. I am not sure I would call the proposed method one for ""machine translation"". This is not a generative translation model, but rather a retrieval model that retrieves similar sentences in a foreign language. I would certainly add a very big disclaimer in the introduction, if not in the title of the paper, clearly stating this. Machine translation's main issues are due to the fact a model needs to generate text in natural language, which is (1) hard to evaluate due to the difficulty in defining what consists a good translation, (2) the lack/difficulty in having adequate automatic metrics to evaluate MT, (3) the need for reference translations to which to compare hypotheses generated by the model, etc. This paper does not address any of these problems. It is true that the authors show a few examples at the end of Appendix A where they generate translations with GPT-2, but these are utterly secondary experiments, there is no evaluation conducted on the generated translations, etc. <sep> Another central issue in the paper: Models are trained on datasets where the non-English language sides of the data were all machine-translated from English. Validation and test sets do not seem to be human translated either. To summarize: authors train and evaluate their models on data which is machine translated to begin with. Training on MT'ed data is not an issue necessarily and can often be helpful, i.e., back-translation is an example where training on additional MT'ed source sentences paired with gold-standard target sentences can help. However, you need to control for the quality of your validation and test sets. If they are also machine translated, you are probably introducing a lot of unintended biases (see [1] for a discussion). Note that in addition to evaluating in ""translationese"", these MT'ed data were not even validated by a human, meaning we do not even know if translations used as references in the validation/test sets are correct. This makes the whole evaluation questionable. <sep> All issues considered, since the method is a crosslingual retrieval model that uses images, the authors should compare to other baselines proposed specifically for crosslingual sentence retrieval. One such model is X-STILTs [2], which performs well on Tatoeba and BUCC, two datasets proposed specifically for crosslingual sentence retrieval. Since these datasets have no associated images, it is not straightforward to evaluate on them with the proposed model. However, authors could retrieve images for sentences in Tatoeba/BUCC using a crossmodal retrieval model, and train their proposed model on the retrieved image-sentence pairs. Since the proposed model is already robust to noisy image-sentence alignments (Section 3.2, last paragraph), perhaps authors could show how it performs compared to strong crosslingual retrieval baselines. I mentioned [2] but there are many more, check the Google XTREME Benchmark for more examples [6]. <sep> A few other comments: <sep> ""Machine translation aims to learn a mapping between sentences of different languages while also maintaining the underlying intent."" -> This is an odd way to define/introduce machine translation (MT). I'd say ""underlying semantics"", not intent. <sep> ""Experiments and visualizations show that the transitive relations through vision provide excellent self-supervision for learning neural machine translation"" -> ""In our experiments and visualizations we show (...)"" <sep> Correct citation for ""Image pivoting for learning multilingual multimodal representations"" is the peer-reviewed paper ""https://www.aclweb.org/anthology/D17-1303/"". You are also missing the paper ""Sentence-Level Multilingual Multi-modal Embedding for Natural Language Processing"", ""https://www.aclweb.org/anthology/R17-1020/"", which additionally uses learned representations for neural machine translation. Please double-check all your citations that reference pre-prints to make sure there is no peer-reviewed version of a cited pre-print available. <sep> In your related work, perhaps you could at least mention (the massive amount of) previous work on sentence-image ranking/retrieval, even if these works are not multilingual [3,4,5]? <sep> Section 3.2 ""However, these constraints provide a sparse gradient for learning, which makes large-scale optimization difficult."" What do you mean by sparse gradients? <sep> ""cross-modal similarity as well as a cross-image similarity"" -> ""sentence-image similarity as well as a image-image similarity"" <sep> You could improve the mathematical notation greatly. I would mention, for example, the different variables represented by alpha. The variable \\alpha_{ii} is especially badly named, please use more discriminative/clearer variable names. Different indices (i, j, etc.) should index different things. <sep> If Eq.5 is being maximised, you should not call the quantity being maximised a loss. A loss is always minimised by definition, e.g. negative log-likelihood loss. <sep> [1] M. Zhang and A. Toral (2019). The Effect of Translationese in Machine Translation Test Sets. In: WMT 2019. URL: https://www.aclweb.org/anthology/W19-5208/ <sep> [2] J. Phang et al. (2020). English Intermediate-Task Training Improves Zero-Shot Cross-Lingual Transfer Too. In: AACL 2020. <sep> [3] Kiros et al. (2014). Unifying visual-semantic embeddings with multimodal neural language models. In: arxiv. <sep> [4] Frome et al. (2015). DeViSE: A Deep Visual-Semantic Embedding Model. In: NIPS. <sep> [5] Faghri et al. (2018). VSE++: Improving Visual-Semantic Embeddings with Hard Negatives. In: BMVC 2018. <sep> [6] Google XTREME Benchmark: https://sites.research.google/xtreme","Overall, all reviewers generally agree that the idea of using visual similarity to unsupervised alignment of multiple languages is interesting and the proposed method and dataset are well-designed, while three of them raised some concerns related to the retrieval nature of the method. In particular, discussions about its place as a study of machine translation and comparison with other cross-lingual retrieval baselines were the main issues. Although authors made great effort to address reviewers' concerns points and did clarify some of them, unfortunately the reviewers were not fully convinced by the response, and one reviewer decided to downgrade the initial score. After all, three reviewers rate the paper as 'below the acceptance threshold'. Based on their opinions, I decided to recommend rejection. <sep> I think the entire picture of the work and the logic flow could be much clearer by discussing in a top down manner why this idea should be implemented with a retrieval-based approach, rather than superficially adding ""using retrieval"" to some sentences."
"abstract | weakness  ==> This paper presents a GCN-based solution for multi-step spatio-temporal data forecasting. The main contributions are 1) a spatio-temporal joint graph convolution network to simultaneously capture spatial and temporal correlations; 2) a combination of the sequence decoder and short-term decoder to alleviate the error accumulation when modeling the temporal dependencies. I agree with the incremental contribution of this paper. To verify the effectiveness of their approach, the authors conduct experiments on three real-world datasets. <sep> However, I have the following concerns: <sep> Presentation: <sep> The major concern is the writing of this paper. There are many parts (e.g., notations, statements) that are unreadable or hard to understand. For example, <sep> What is ""ST in GCM"" in Figure 2(a)? <sep> In page 4, x \\in \\mathbb{R}^n. What is n here? It seems to be inconsistent with the notation N in Section 3.1. <sep> This paper introduces a learnable spatial mask matrix W_{mask}, but how to use it is not clear. <sep> Does Figure2(c) in page 5 mean Figure 2(b)? <sep> In the last equation of page 5 (for computing Y_{s_out}), what is t? Moreover, since W_F is of shape F by 1 and W_t is t by M, how can we multiply W_F by W_t? <sep> How to obtain Y_{m_out} in equation 7? <sep> Many typos and grammatical errors. I only point out several of them. Spatial-temporal data is -> are, correlations STG2Seq -> correlations. STG2Seq, Laplqaacian->Laplacian, which needed -> which is needed, we conjecture the reason is-> we conjecture that…. <sep> Technical: <sep> Novelty of the proposed model is limited. The proposed spatio-temporal joint graph convolution is very similar to the concept of 3D GCN [1]. However, in this paper, I could not see any discussion about the difference and comparison between them. <sep> In section 3.1, this paper assumes that the graph is undirected. However, the propagation of traffic in spatio-temporal domains are certainly directed (e.g., downstream, upstream). <sep> What is the difference between the temporal attention in Section 3.4 and an FFN (two fully-connected (FC) layers)? In my view, this paper only uses two matrices for the dimension transformation, which is the same as two FC layers. <sep> Experiments: <sep> It is more reasonable to compare the proposed model with CNN-based solutions in the Mobile traffic dataset, as there is no explicit graph structure (only Euclidean structure exists). Baselines like ST-ResNet, ConvLSTM, DeepSTN+ or DeepLGR should be included for comparison. <sep> According to the paper, I cannot see any detail of the multi-run experiments (e.g., in Table 1 and Figure 3). How about the stability (i.e., variance) of the proposed method? <sep> The experiment shows little improvement to the existing approaches in the first two datasets. There should be significant test (e.g., student t-test) conducted to make sure the experimental result is reliable. Considering the limited improvement, it is also questionable whether adding such complexity to the model is worthy. It may not make sense to improve the MAE by 0.02 in the first dataset by increasing the runtime a lot. <sep> No experiments to show the effects of the spatio-temporal kernels. Why should we set it 3x1, 1x3 and so on? <sep> It seems that the MAPE in Table 1 largely exceeds 100%, which is unusual to see. What is the unit of MAPE here? <sep> Minor issues: <sep> All acronyms should be expanded for the first time in text (e.g., HA is not expanded. What is HA? Historical average?). <sep> There should be one space before each citation. For example, DCRNN(Li et al., 2017) -> DCRNN (Li et al., 2017). <sep> In the first paragraph of Section 1, the paper claims that ""Typical applications include …, traffic road condition forecast (…, Liang et al., 2018), … and geo-sensory time series prediction (Li et al., 2017)."" However, Liang et al., 2018 is for geo-sensory time series modeling while Li et al., 2017 is for traffic prediction. Please carefully read these papers before citing them. <sep> It is difficult for those who are not familiar with the Inception Network to understand how your model draws insights from it. In addition, the paper proposing the Inception Network should be cited. <sep> No future work discussed in this paper. <sep> In summary, I recommend not to accept this paper in its present form. <sep> Reference: <sep> [1] Yu et al., 3D Graph Convolutional Networks with Temporal Graphs: A Spatial Information <sep> Free Framework For Traffic Forecasting, Arxiv 2019.","The paper proposes a multi-scale spatial-temporal joint graph convolution for spatiotemporal forecastings. Many reviewers have concerns regarding novelty, baseline comparisons, and writing clarity of the draft."
"abstract | strength | weakness | rebuttal_process | decision  ==> This paper claims to present an algorithm which enables a population of (two) agents to learn to communicate and coordinate to solve a task, and thus positions itself in the field of multi-agent Deep RL. After a long but rather vague and unspecific introduction and related work (see below), it describes the algorithm, then presents experiments where the introduced algorithm is compared with model-free MARL baselines. <sep> While the algorithm presented is interesting and has potentially some novelties compared to the state-of-the-art (e.g. differentiability of message passing in model-based MARL), it has also a number of weaknesses: <sep> Globally, I had a lot of difficulty understanding clearly what are the aims of this paper: <sep> What are the problems it aims to solve? What are the scientific questions adressed? <sep> Neither the abstract nor the text provide sharp explanations of these aims. <sep> The paper uses very loaded but undefined vocabulary like ""imagination"", ""language"" and ""communication"". <sep> While in general I think it can be sometimes useful to use concepts and terms from human cognitive sciences to describe AI systems, in this particular case I found it very far fetched to speak of ""imagination"" and ""language"", <sep> even ""communication"". It seems in practice authors might simply mean something like ""prediction of future states"" <sep> when they use the term ""imagination"". ""Language"" and ""communication"" are also far-fetched because in cognitive science and linguistics it refers to systems that enable different individuals, with different world views, to communicate an intent to each other. Here, the ""agents"" share the same world model, so they are not really different individuals with their own world representations, and their communication is rather like message passing in GNNs, which is pretty far from ""language"" or ""human-like communication"". <sep> It is not even clear whether it is meaningful to call the presented system as ""multi-agent"", since in addition to a centralized shared reward, there is also a shared world model. To me, the system looks rather like an RL <sep> system that controls a multi-component body with local controllers that synchronize through message passing, <sep> quite similary to graph neural network controllers (also including message passing) used for e.g. in Pathak et al. 2019. <sep> A discussion of the similarities and differences with work such as Pathak et al. is needed. <sep> the authors are right to say that there is little research on model-based MARL, and cite one exception: <sep> Krupnik et al. However, it is not justified why this closely related work is not included in the baselines, <sep> or at least compared in discussion more thoroughly. Authors might also want to discuss another model-based MARL <sep> paper: Zhang et al. 2020. <sep> A large part of the related work section is not relevant to this paper, in particular about Deep RL and model-based RL, <sep> which are much broader topics than the one addressed in this paper <sep> The description of the method lacks sufficient technical details for reproducibility, in particular it lacks detailed pseudo-code (some refs are said to be in an appendix, but I did not find an appendix), and no links to code is provided. <sep> Furthermore, there is no sufficient information on how hyperparameters selection for baselines was made. <sep> The two environments in the experiments are not sufficiently well motivated: why did you need to introduce them rather than reuse existing test environments? E.g. which particular problems did you want to address that was not possible with existing environments ? <sep> Since the claimed topic of the paper is about the emergence of a ""communication system"", one would expect a detailed analysis of the emergent communication code (currently only figure 5 gives a quite superficial qualitative analysis). <sep> The quantitative comparison of algorithms is not made using a sufficiently strong statistical method (only 5 seeds, <sep> no tests such as Welch t-tests) <sep> For these reasons, while the particular algorithms studied is in itself interesting, I think the paper would need a major conceptual reframing and a better experimental methodology and justification before publication. <sep> References: <sep> Pathak et al. (2019) Learning to Control Self-Assembling Morphologies: A Study of Generalization via Modularity https://arxiv.org/pdf/1902.05546.pdf <sep> Zhang, K., Kakade, S. M., Başar, T., & Yang, L. F. (2020). Model-based multi-agent rl in zero-sum markov games with near-optimal sample complexity. arXiv preprint arXiv:2007.07461.","The authors present a model-based method for cooperative multi-agent reinforcement learning and propose to use communication of future predictions (as given by a learned world model) as a way to overcome partial observability. <sep> Overall, all reviewers found this work to be of great interest and the combination of planning + communication novel. However, all reviewers pointed that the claims that the papers makes are not fully supported by the experimental framing of the paper pointing to several shortcomings around experimental design in general and better control of appropriate baselines. The authors have since clarified several aspects in their paper and also included a new RL environment. <sep> However, as the paper still stands does not fully provide convincing evidence of their proposal, which is however very intriguing. I would like to echo though reviewers' suggestions that the authors work a bit more on the experimental design and I really hope this work will appear at a later venue."
"abstract | weakness | misc | weakness | decision  ==>  ==> Summary <sep> The paper outlines the construction of filters in a group equivariant convolutional network equivariant to the groups CN (cyclic group of order N (N rotations)) and DN (dihedral group of order N (N rotations and flipping)). The authors achieve this by taking linear combinations of (harmonic) basis filters, as was shown in Weiler et al. (2018). They then proceed to show explicitly how one can build filters between particular representations of each group, namely, the trivial, irreducible, and regular representations. For experiments they demonstrate that the filters perform en par with those of Weiler and Cesa (2019), who had performed a near-exhaustive comparison among representations of SE(2, R). <sep> Pros <sep> Technically I believe the work to be sound. I did not see any mistakes jump out at me. I also think that it is laudable that the authors included a high amount of detail in their exposition, which lays bare the exact mechanisms by which one would effectively go about building an equivariant layer. <sep> Experimentally it appears that the authors chose a sensible baseline and that they compared on dataset, which are relevant to the topic and commonly used in the equivariance literature. The authors also provide timing information on how fast each filter can be generated, which is something I have not seen before in the equivariance literature and which I appreciate seeing. <sep> Cons and constructive feedback <sep> Perhaps my largest criticism is that it is not obvious exactly what the contribution of the paper is meant to be. Perhaps this originates from my not knowing what the authors mean by ""filter transform"" and ""steerable CNN"". My understanding is that the authors believe it is currently unknown how to construct a steerable CNN for the cyclic and dihedral groups. They provide explicit constructions in section 3, but to my knowledge these are already provided in the long appendix of Weiler and Cesa (2019), furthermore they can be found in Cohen and Welling (2016), Cohen and Welling (2017) and Bekkers et al. (2018) and most notably Weiler et al. (2018), who were to first to show how to construct equivariant filter layers some linear combinations of harmonic basis filters. As a result I am unsure of how to gauge the novelty of this contribution. <sep> In the experiments of Table 2 (classification and regression) the authors compare their framework with an R2conv (representative harmonic based convolution) and a regular translationally equivariant network. I would like to know what exactly is a representative harmonic based convolution? Is that meant to refer to the work of Weiler and Cesa (2019)? <sep> In the results tables there are no error bars. If possible I would have liked to have seen them. Since they are not there is it very hard to judge the efficacy of the results, which differ from the baselines by very small amounts. <sep> Given the proximity of the work to Weiler and Cesa (2019), I would like to know exactly what differentiates the two works. <sep> Please include exactly what the functions roll, flipped, and circulant actually do mathematically. I found these difficult to parse. <sep> The mathematical level of the paper is pretty heavy for the uninitiated. It may be advisable for the authors to include a glossary of terms, if not short descriptions, in an appendix. If this is too much, at least please point to other papers, which have easily readable background sections for those not already well-read in group theory. <sep> The authors may wish to have the submission proofread for spelling and grammar. <sep> Post rebuttal review <sep> Having read through the rebuttal, the updated submission, and the reviews of the other reviewers I have upgraded my review from a reject to marginally below acceptance. This is for two main reasons. 1) the authors have vastly improved the presentation of the submission, which now looks a lot easier to read, 2) the authors have clarified for me, at least, what the main contribution of the work is. <sep> That said I am not entirely sure what this contribution adds to the equivariance community, hence why my recommendation still leans towards reject. As far as I am aware, solving the equivariance equations is not the large bottleneck to progress in our community. They are linear equations, and there is work back into the 80's solving them (check out people like Pietro Perona, Patrick Teo). I think more importantly we need to focus on pushing the boundaries in areas such as extension to non-Euclidean manifolds, convolution over non-compact groups, learning symmetries, etc. While this work is clearly mathematically sound and the authors have demonstrated deep knowledge of the area, it feels a little like retracing prior works. That said, if the other reviewers disagree then I don't mind this paper being accepted. Perhaps since I have worked in this area, what appears as obvious to me is not generally acknowledged and this paper may serve as a useful clarification for those wishing to dive into the literature.","This paper introduces an approach based on filter transform for designing networks equivariant to different transformation groups. <sep> Especially, the authors rely on the haramonic analysis view of steerable CNNs given in Weiler & Cesa (2019) to design an equivariant filter bank by computing simple transforms over base filters. <sep> The reviewers finds the paper technically solid but difficult to read and with a limited contribution. <sep> The AC carefully reads the paper and discussions. Although the connection between steerable CNNs and filter transform are interesting, the AC considers that the main contributions of the paper should be consolidated, especially the positioning with respect to Weiler & Cesa (2019). Therefore, the AC recommends rejection."
"abstract | decision  ==>  ==> Summary: <sep> This paper attempts to solve the problem of seeking novel policies in reinforcement learning from a constrained optimization perspective. This new perspective motives two new algorithms to solve the optimization problem, which are based on feasible direction and the interior point methods. The authors provide empirical results on several Mujoco benchmarks. <sep> Details: <sep> The idea of formulating the problem from a constrained optimization perspective is interesting. This new perspective motivates new and better algorithms to solve the optimization problem. <sep> However, I feel like the presentation is poor and the writing should be improved. <sep> What's the exact problem setting? The authors should clearly describe the problem setting before presenting the methods, even one paragraph would be helpful. <sep> A lot of algorithm details are missing: <sep> Q1. D¯Wq(θi,θj) is a metric for any state distribution q. What's the motivation of using q=ρ¯? <sep> Q2. When computing the policy distance, what is ρθi in (4)? Is it the current policy, or a reference policy? <sep> Q3. I assume θi is the current policy. According to (4), the algorithm uses θi to get samples, and compute an importance correction ratio q/ρθi to approximate the distance. How is the q(s)=ρ¯(s) computed? The authors propose to approximate ρθ using monte-carlo methods. Does it mean the algorithm need to approximate ρ¯(s) using the reference policies for each s∼ρθi? Is there a computation issue? <sep> Q4. This goes back to Q1. Why just using the on policy samples to estimate the distance? Is there any potential advantage to use q=ρ¯? <sep> Q5. Learning the stationary distribution is a hard research problem itself. See recent work for example: <sep> Zhang, R., Dai, B., Li, L. and Schuurmans, D., 2019, September. GenDICE: Generalized Offline Estimation of Stationary Values. In International Conference on Learning Representations. <sep> I agree the stationary distribution can be approximated using MC methods, but it might need a lot of samples as the variance is very high. This makes me wonder how is the algorithm implemented in practice, and how does the stationary distribution estimation subroutine affect the algorithm's performance. <sep> Other suggestions: <sep> If I understand correctly, this paper tries to solve the problem of finding a set of novel polices that solve a given task while exhibiting different behaviors. This seems also related to the exploration problem, as some works try to make the current policy different with previous policies to encourage exploration. See for example: <sep> Hazan, E., Kakade, S., Singh, K. and Van Soest, A., 2019, May. Provably efficient maximum entropy exploration. In International Conference on Machine Learning (pp. 2681-2691). <sep> It might be worth to discuss how the novel policy seeking problem is related to the exploration problem.","This paper investigate the interesting problem of policy seeking in reinforcement learning via constrained optimization. Conditioned on reviewers' judgements, this is a good submission but hasn't reached the bar of *CONF*."
"strength | suggestion | strength | weakness  ==> The paper proposes a few-shot meta-learning method for recommender system that uses a new feature's meta-information and observed samples for the features to predict the network weights for predicting the feature value from other features. The paper focuses on the cold-start problem where few samples with a new feature observed is available. The method outperforms a wide range of baselines on MovieLens-1M, a medical synthetic dataset, and a e-learning dataset. <sep> Disclaimer: I am not familiar with recommender systems. <sep> Pros: <sep> Methods straightforward and easy to understand, and does not have much ad-hoc design choices that are hard to validate. <sep> The experiments are relatively thorough. A lot of baselines. <sep> Cons <sep> Novelty is limited -- a mixture of meta-learning and content-based method. There are also existing few-shot continual learning papers available (e.g. https://openaccess.thecvf.com/content_CVPR_2020/papers/Tao_Few-Shot_Class-Incremental_Learning_CVPR_2020_paper.pdf), so the motivation needs to consider differences from them. <sep> Experiments does not do external comparisons with other recommender systems that deal with cold starting other than its own baselines. <sep> Despite the large amount of baselines, ablation still lacks: (1) Train from random, but instead of training for a fixed number of epochs, just train an SVM or linear regression until convergence. This is what people would do as a baseline. (2) not important but it would be nice to ablate the CHN by taking out metadata and Cn from input. <sep> Not clear if the set of datasets is persuasive. One is a synthetic dataset. One recommendation system and one grading dataset. <sep> There is no Appendix despite referencing it. <sep> Minor issues: <sep> Not clear how the ""adapting to new features"" is different from meta-learning's adapting to new tasks, since they also use old tasks to inform new tasks. The abstract seems to suggest the method can take new features as input incrementally, which is a little confusing. <sep> Overclaim at the end of Section 2.2: it is not O(1) if you have to do distributed computing. If you allow distributed computing, NP=P. <sep> It's hard to tell why MAML is similar to CHN in one dataset but underperforms drastically in the e-learning dataset. <sep> Post-rebuttal: <sep> I appreciate the additional ablation study, but unfortunately the results did not strengthen the paper's distinction from related work. The explanation of the motives and related work comparisons only clarified differences between this paper and prior work that are either inherent to the task being addressed, or contribution unsupported by experiments. Unless the AC agrees with the authors that the paper is acceptable even without external comparisons (despite being a merge of two lines of work), I will not be changing my score.","This work presents a straightforward and easy to understand method for using hypernetworks to adapt existing models to be able to increase their output space. The method itself is also interesting and is detailed enough for reproducibility. However, the experiments and results should be improved by expanding the demonstration of CHNs beyond the narrow P-VAE application and comparing against relevant baselines in the recommendation system literature. <sep> Pros <sep> Clear writing. <sep> Detailed hyperparameters to aid reproducibility. <sep> Straightforward model. <sep> Cons <sep> Lack of sufficient comparison to related work, especially to existing recommendation systems that handle the cold-start issue and to Vartak, 2017. <sep> Limited results that only demonstrate application to P-VAE meaning it's still unknown if CHNs work well with other models. The result on the synthetic dataset is also less persuasive. <sep> Lack of sufficient ablations, i.e. training a SVM/linear regression model until convergence."
"abstract | strength | weakness  ==> ==========Summary========== <sep> In this paper, the authors investigate how to improve pooling functions in graph neural networks for the purpose of better addressing graph classification problems. The core idea in this paper is to group node representations. In particular, the authors develop StructAgg, a softmax based implementation, to parameterize the grouping process, and the parameters in StructAgg can be learned from downstream supervision signals. Empirical results focus on three aspects: (1) Improvement in graph classification accuracy brought by StructAgg; (2) Visualization on grouped node representations; and (3) Comparison between two variants of StructAgg. <sep> ==========Reason for the rating========== <sep> For the current draft, I am leaning to reject. Although pooling for graph classification is a meaningful problem, it is difficult to see the unique perspective or value in the proposed technique compared with existing approaches. For now, the technical depth in this paper seems to be limited, and more convincing empirical evidences are expected. <sep> ==========Strong points========== <sep> This paper delves into a meaningful problem. Indeed, pooling functions in graph neural networks are critical for graph classification tasks. While existing solutions are simple and efficient, they may be sub-optimal in some cases. <sep> The authors propose a grouping-based method to reduce variance in graph representations, which potentially could improve generalization performance. <sep> Empirical evidences are provided to confirm the effectiveness and impact from StructAgg. <sep> ==========Weak points========== <sep> The idea of ""grouping node representations using softmax for graph classification"" may have been investigated in existing works, e.g., [1]. For the core idea in this paper, the authors may need to discuss its connection with existing literature, and highlight the unique perspective. <sep> The technical impact of this paper could be limited. <sep> From the current draft, the proposed StructAgg looks like incremental changes to the existing methods. The authors may provide more theoretical or empirical evidences to motivate the problem, highlight the uniqueness in the technique, and justify their design choices. <sep> The theoretical results in Theorem 1 may need more work. Theorem 1 could aim to answer an important question in StructAgg, but the authors may raise the question first and justify why this is an important/non-trivial question. <sep> The empirical evidences could be stronger. <sep> From the current results, StructAgg only performs the best in one of the three datasets. The authors may consider more datasets that can highlight the value of StructAgg. <sep> StructAgg may work with the existing GNNs. The authors may connect StructAgg with the existing GNNs, and demonstrate the incremental gain from StructAgg. <sep> For the evaluation in Table 2, it is unclear why the comparison is limited between GCN and the proposed technique. <sep> For the evaluation in Table 3, the claim on ""node embedding"" may not be sound. Between StructAgg and StructHist, the difference is not just ""node embedding"", and one cannot ignore the impact from ""histogram"". The authors may need to carefully reason the conclusion from the empirical numbers. <sep> ==========Questions during rebuttal period========== <sep> Please address and clarify the weak points above. <sep> In addition, it will be great if the authors could address the following questions. <sep> Q1. As the authors emphasize the term ""node embedding"", does this paper target on transductive or inductive settings? <sep> Q2. For the entropy minimization discussed in Section 3.3, is it going to bring more overfitting risk? <sep> Q3. For the first condition in Definition 1, it is a bit vague. The authors may give a more accurate mathematical description. <sep> Q4. For the first bullet in Remarks under 3.3, what does ""The structural classes identified by our algorithm are local"" exactly mean? <sep> Q5. In the experiment setup, what do l1 and l2 refer to? <sep> ==========Reference========== <sep> [1] Substructure Assembling Network for Graph Classification, AAAI 2018 <sep> ==========Post rebuttal========== <sep> The authors' response does not fully address my concerns. I keep the rating as it is.","The paper addresses an important unsolved problem, i.e. deriving explainable features for use in graph classification. It does it by providing: <sep> i) a simple to implement (local) node aggregation approach; <sep> ii) some theoretical support to the proposed approach; <sep> iii) empirical evidence that the proposed approach could be effective. <sep> Notwithstanding the above merits, the reported work seems to still be in a preliminary phase. In fact: <sep> i) reference to literature is missing some important recent contributions to the addressed problem (e.g. Gated Graph Sequence Neural Networks, GNNExplainer); if possibile, also experimental comparisons vs those approaches is desirable; <sep> ii) experimental results do not provide a solid evidence that the proposed approach can really help to provide a clear explanation of the output, and the overall performance in classification is mostly below SOTA models; adding more datasets could help to give a more solid support to the main statement about explainability/performance; <sep> iii) presentation needs to better highlight the original contribution w.r.t. relevant literature (which is not completely clear in the current version of the paper), to improve the explanation of proofs, to discuss (both from a theoretical and empirical perspective) some important issues, such as computational scalability with the increase of size of local structures, and robustness to noise of the proposed (local) aggregation method. <sep> In summary, although the proposed approach seems to be of some value, more work is needed to better place the proposed approach in the context of current literature and to gain a stronger experimental support to the main claim of the paper w.r.t. explainability."
"rating_summary | weakness | decision  ==> Summary: <sep> The paper provides a set of comparisons among different scene generation methods. It assesses ability of the models to fit the training set (seen conditionings), generalize to unseen conditionings of seen object combinations, and generalize to unseen conditionings composed of unseen object combinations. It finds that these models fit the training distribution with a moderate success, display decent generalization to unseen fine-grained conditionings, and have significant space for improvement when it comes to generating images from unseen coarse conditionings. <sep> ############################################################ <sep> Strengths: <sep> The authors provide a comprehensive set of experiments to compare performance of different scene generation methods using several metrics. This can be helpful for researchers to assess strengths and weaknesses of each model and its components, and helps them to gain insights into which aspect of models need to be improved. <sep> ########################################################### <sep> Weaknesses: <sep> While the comparisons among different scene generation methods are valuable, there are concerns about novelty of the paper especially since similar works have been published considering other computer vision tasks (e.g. [A, B]). The authors assess existing models in different settings and report their findings. <sep> There are some concerns regarding the overall setup for the experiments. Out of the three cases considered, the ability of a model to fit its training set (case 1) is not very interesting practically as we are more interested in the model's generalization. Case 3, generalizing to unseen conditionings composed of unseen object combinations, is also not expected as the models are not particularly trained to be generalizable to unseen coarse conditionings. If one is seeking models with generalization ability to unseen coarse conditionings, he/she needs to incorporate a form of transfer/meta-learning and train the models differently. Generalization to novel categories is also an issue in object-level GANs. <sep> The paper claims that it is very hard to assess which models perform better due to ""models being trained to fit different data splits, using different conditioning modalities and levels of supervision, and reporting inconsistent quantitative metrics (e.g. repeatedly computing previous methods' results using different reference distributions, and/or using different image compression algorithms to store generated images), among other uncontrolled sources of variation"". However, I do not see a clear evidence supporting this. Each of the other papers (LostGAN, OC-GAN, etc.) provides a set of comparisons with other methods. The authors need to note specifically which setups are inconsistent among different papers. They report that LostGAN-v2 outperforms other models in most tasks. This is consistent with results reported in the LostGAN-v2 paper (although they evaluate their method on a smaller number of metrics). <sep> ############################################################## <sep> Reason for rating: <sep> While the paper provides a systematic evaluation of scene generation methods, there are some concerns regarding novelty and the proposed setup. I hope the authors clarify these in the rebuttal. <sep> ############################################################## <sep> Additional comments: <sep> There are some minor grammatical errors in the paper, and it needs further proofreading. <sep> ############################################################## <sep> References: <sep> [A] Are GANs Created Equal? A Large-Scale Study; Lucic et al.; NeurIPS 2018 <sep> [B] A metric learning reality check, Musgrave et al., ECCV 2020 <sep> ################################################################ <sep> After author response: The authors have addressed my comment about inconsistent evaluation setups among different papers. However, I sill think novelty of the paper is limited as it is a conditional counterpart of [A]. As mentioned by other reviewers, findings of the paper are quite incremental and are in line with LostGAN-v2 although the authors use a more consistent evaluation setup. <sep> Overall, I keep my current rating.","The paper received mixed reviews that overall lean negative. <sep> The main concern shared by reviewers is the novelty of the findings. Although the paper presents a systematic study that certainly has value, reviewers do not find sufficient insights from the analysis. The ACs agree with the reviewers that the paper is below the bar for acceptance."
"abstract | rebuttal_process | rating_summary | decision  ==> This paper proposes a new approach to train VAEs with binary latents, using an evolutionary algorithm to optimise a discrete set of variational parameters rather than the usual amortised variational model trained with gradient-based methods. The authors consider the setting of training a VAE with discrete, Bernoulli distributed latents and a continuous, Gaussian distributed output. They use a novel approach to form and train a posterior on the discrete latent space, parameterising the posterior somewhat implicitly via the sets defining a truncated posterior - in which the approximate posterior is proportional to the true posterior, but only within a subset of all points. Defining the subset of points amounts to parameterising the approximate posterior. Optimisation of these parameters amounts to a discrete search problem, which the authors tackle using an evolutionary algorithm. <sep> Overall, I find the paper very well written and straightforward to follow. Indeed, the style of writing tends to pique the interest of the reader without being colloquial. I think the motivation of the paper is well established, given the number of papers that have investigated the training of modern generative models with discrete latents. Importantly, I think the proposed approach is novel and seemingly effective. The use of the truncated posterior along with a relatively simple evolutionary optimisation procedure is quite different from the previously seen methods that generally attempt to maintain a differential proxy to the ELBO. <sep> The proposed method does have limitations, which the authors acknowledge. Namely that the method will not scale to large datasets, since the posterior is not amortised. I think it must also be true that the evolutionary optimisation is not particularly suited to high-dimensional spaces, since it is generally accepted that search suffers in such settings. However, these limitations I do not think are overwhelming. The authors find a niche problem setting in which their method is practical - that of ""zero-shot"" data denoising. In this setting they demonstrate that their method (with a small VAE) outperforms the state-of-the-art methods. The authors also provide a small set of straightforward and convincing toy experiments to show that their approach satisfies basic properties that we require in a learning system, such as appropriate scaling behaviour and recovery of artificial data generating parameters. Although I think the empirical demonstration may be enough, given the method is relatively novel, it is still limited. The main experimental result of the paper is denoising on a single image. Even performing the denoising on a small set of images would help the paper, since there will undoubtedly be variance in the performance across different images. <sep> One other thing I would say about this paper is that identifying the approach as a VAE almost feels inappropriate. It is already true that the usual VAE is not an autoencoder at all, and so the name is a slight misnomer. But with the authors' approach here, the posterior is not even parameterised as a function of the data (although there is of course an implicit dependence established through training). Hence identifying the method as related to an autoencoder seems to be misleading (although understandable given that the method is clearly related to the VAE).","The paper presents an evolutionary optimization framework for training discrete VAEs, which is different to the standard way of training VAEs. One of the main criticism of the paper was the choice of experiments, but the authors addressed this point by adding an inpainting benchmark. <sep> Unfortunately, the reviewers' scores are borderline, and one of the reviewers pointed out the lack of scalability (more precisely, linear scalability with the number of observations) and cannot recommend acceptance based on the limited application impact. Given the large number of *CONF* submissions, this paper unfortunately does not meet the acceptance bar. That said, I encourage the authors to address this point and resubmit the paper to another (or the same) venue."
"abstract | misc | weakness | strength | weakness  ==> I want to congratulate the authors on producing an attractive narrative, with a very nice flow to it and a very clear heuristic content.  I believe if I were seeing these ideas for the first time, I would definitely find this presentation friendlier and clearer than if I learned it from some other presentations. <sep> The topic is important. The analysis of the spectrum of the Hessian of the loss function gives us significant insight, and it is rather amazing and beautiful that the spectrum has definite mathematical properties which also have mathematical explanation. The narrative excels in making us feel this as something clear and natural. <sep> This is my first time ever reviewing for conferences of this nature; I am a bit unclear about the level of contribution we are looking for. My understanding is that researchers in this field are writing numerous very short papers each year and a paper doesn't need to represent more than an incremental bump over previous work. <sep> One reason I bid to review this Mss. is that the topic is not new to me. In the last few years three separate researchers at my institution have written papers on this general topic; and so I feel more conversant with this topic than some other material I might have been assigned. So … <sep> I feel that this work may not know about or may not have fully assimilated the contributions by other authors. I can mention these papers: <sep> arXiv:1901.10159 <sep> An Investigation into Neural Net Optimization via Hessian Eigenvalue Density <sep> Authors: Behrooz Ghorbani, Shankar Krishnan, Ying Xiao <sep> Abstract: To understand the dynamics of optimization in deep neural networks, we develop a tool to study the evolution of the entire Hessian spectrum throughout the optimization process. Using this, we study a number of hypotheses concerning smoothness, curvature, and sharpness in the deep learning literature. We then thoroughly analyze a crucial structural feature of the spectra: in non-batch normalized networks, we observe the rapid appearance of large isolated eigenvalues in the spectrum, along with a surprising concentration of the gradient in the corresponding eigenspaces. In batch normalized networks, these two effects are almost absent. We characterize these effects, and explain how they affect optimization speed through both theory and experiments. <sep> So arXiv:1901.10159 clearly calls out the phenomenon of batch norm changing the hessian. In comparison, the paper under review gives a heuristically very clear explanation of why such an effect should be present, i.e. why batch normalization should be effective at making the objective easier to optimize.  However, the paper under review does not fully discuss the above research contribution of arXiv:1901.10159,  and it would be very easy for a reader of this paper to imagine that the phenomenon being presented here originates with this paper.   I should point out that the authors of arXiv:1901.10159 have made company-wide presentations about that work. Since the company is Google, that means that their work is fairly well known. <sep> arXiv:1811.07062The Full Spectrum of Deepnet Hessians at Scale: Dynamics with SGD Training and Sample Size <sep> Authors: Vardan Papyan <sep> Abstract: We apply state-of-the-art tools in modern high-dimensional numerical linear algebra to approximate efficiently the spectrum of the Hessian of modern deepnets, with tens of millions of parameters, trained on real data. Our results corroborate previous findings, based on small-scale networks, that the Hessian exhibits ""spiked"" behavior, with several outliers isolated from a continuous bulk. We decompose the Hessian into different components and study the dynamics with training and sample size of each term individually <sep> To my knowledge arXiv:1811.07062  is the first paper to show the empirical deep learning community that one can compute the eigenvalue density spectra of modern deepnet classifiers eg those of the kind that are in use on large datasets like imagenet and in real applications. The result of having the technology in arXiv:1811.07062  is that the author is able to show that some of the key features of eigenvalue spectra that were seen in small scale situations are also present at full scale. The examples shown in the paper under review, in contrast, are of quite limited scale, and to my understanding do not represent the current state of the art. Consequently, although tools re available to test out the authors' ideas on realistic problems, we are left wondering what seen in these small examples might generalize. <sep> arXiv:1901.08244 Measurements of Three-Level Hierarchical Structure in the Outliers in the Spectrum of Deepnet Hessians <sep> Authors: Vardan Papyan <sep> Abstract: We consider deep classifying neural networks. We expose a structure in the derivative of the logits with respect to the parameters of the model, which is used to explain the existence of outliers in the spectrum of the Hessian. Previous works decomposed the Hessian into two components, attributing the outliers to one of them, the so-called Covariance of gradients. We show this term is not a Covariance but a second moment matrix, i.e., it is influenced by means of gradients. These means possess an additive two-way structure that is the source of the outliers in the spectrum. This structure can be used to approximate the principal subspace of the Hessian using certain ""averaging"" operations, avoiding the need for high-dimensional eigenanalysis. We corroborate this claim across different datasets, architectures and sample sizes <sep> To my knowledge arXiv:1901.08244 goes over some of the same territory as the present manuscript, but at full scale and in a much more penetrating way. arXiv:1901.08244  shows that not only do the C class means influence the eigenvalues, but actually there are C(C-1) eigenvalues that have structure deriving from means. The examples shown in the paper under review, in contrast, are of quite limited scale, and in contrast don't seem able to show the full structure which we now know to be present in deepnet spectra across a very wide range of networks and datasets at full scale.  However, the paper under review does not discuss the research contribution of arXiv:1901.08244,  and in consequence a reader could get the misleading impression that this paper's heuristic explanations of the mean structure are the only knowledge we have about this phenomenon, when we have actually dramatically more information. <sep> arXiv:2008.11865  Traces of Class/Cross-Class Structure Pervade Deep Learning Spectra <sep> Authors: Vardan Papyan <sep> Abstract: Numerous researchers recently applied empirical spectral analysis to the study of modern deep learning classifiers. We identify and discuss an important formal class/cross-class structure and show how it lies at the origin of the many visually striking features observed in deepnet spectra, some of which were reported in recent articles, others are unveiled here for the first time. These include spectral outliers, ""spikes"", and small but distinct continuous distributions, ""bumps"", often seen beyond the edge of a ""main bulk"". The significance of the cross-class structure is illustrated in three ways: (i) we prove the ratio of outliers to bulk in the spectrum of the Fisher information matrix is predictive of misclassification, in the context of multinomial logistic regression; (ii) we demonstrate how, gradually with depth, a network is able to separate class-distinctive information from class variability, all while orthogonalizing the class-distinctive information; and (iii) we propose a correction to KFAC, a well-known second-order optimization algorithm for training deepnets. <sep> To my knowledge arXiv:2008.11865  again goes over some of the same territory as the present manuscript, but at full scale and in a much more penetrating way. arXiv:2008.11865  in particular discusses the KFAC approximation of Martens and Grosse and in fact claims, with substantial evidence gleaned from many realistic examples, that KFAC is not a good approximation. The paper under review doesn't cite arXiv:2008.11865, but does make heuristic claims about the adequacy of the KFAC approximation.  However, the evidence presented is much weaker and the implications much more sketchy than what is discussed at greater length  in arXiv:2008.11865.","This paper studies different properties of the top eigenspace of the Hessian of a deep neural network and their overlap. It raised quite a lot of discussion, which finally went in not very constructive way. The reviewers generally agree that the paper has potential, but the actual contribution is limited. <sep> Pros: <sep> The idea that top eigenspaces between different models have high overlap is interesting <sep> The explanation that these structures can be explained by Kronecker-product approximation of the Hessian. <sep> Cons: <sep> The connection to PAC-Bayes is unclear and seems artificial. <sep> Many of the related work is missing <sep> The models and datasets are too simple, and general conclusions can not be made on such kind of models. Much more testing is needed to verify the claims, including state-of-the art architectures and datasets."
"abstract | weakness  ==>  ==> The authors propose a 3-phase heuristic algorithm to learn a causal graph from interventional data using continuous optimization. Unfortunately, the paper is hard to follow. Specifically, the exact procedure should be clarified by the authors. If I understand correctly, first they fit to observational data by searching over the space of graphs using a smooth representation for the adjacency matrices. To fit to the interventional data, first, the interventional target is estimated by a heuristic approach and the contribution of these variables to the likelihood is ignored since they are set by the experiment. (there are random graph sampling stages in between that are not clear to me, please elaborate on this). This interventional scoring is done for all interventional data and is turned into a single gradient update. <sep> The paper is hard to parse. My main concern is that, unlike the existing work which the authors compare with in the experiments, the proposed method is not a systematic approach and accordingly it is hard to reason about its use even though it performs well in the experiments. Especially given that some choices made in the algorithm design are not properly justified. Indeed, even with interventions, we do not expect to recover the full structure but only a subset of the edges correct. <sep> Comparisons with the other methods should be expanded into a section where these methods are detailed to showcase the methodological differences. <sep> The following are my detailed feedback. <sep> ""A natural application of Bayesian networks is to describe cause-effect relationships between variables."" <sep> Please distinguish Bayesian networks from the causal networks. Former do not carry causal meaning. <sep> A good reference to cite in addition to Peters et al. for SCMs is Pearl's 2009 Causality book. <sep> ""Although there is no theoretical guarantee that the true causal graph can be identified in that setting, evidence so far points to that still being the case."" <sep> Please modify this statement as it sounds too vague. <sep> The list of contributions require knowledge of the latter sections. Please make it self contained if possible. <sep> ""can't""->""can not"" <sep> SCM definition is not only structural equations but also talks about interventional distributions. Please see Pearl 2009. <sep> The last line in page 2 overlaps with the page number. <sep> Some recent related work is missing: <sep> Mooij et al. ""Joint Causal Inference from Multiple Contexts"" JMLR'20. <sep> Kocaoglu et al. ""Characterization and Learning of Causal Graphs with Latent Variables from Soft Interventions"" NeurIPS'19. <sep> Brouillard et al. ""Differentiable Causal Discovery from Interventional Data"" arXiv'20. <sep> Mooij et al. is cited but please add it in Section 3 among constraint-based inverventional learning frameworks as well. <sep> Brouillard et al. is too recent, hence its omittance is understandable. However, it attacks the same problem considered here. I believe including it as independent discovery would help connect literature together nicely. I am not going to take this work into consideration in my evaluation since it is uploaded on arXiv only very recently. <sep> ""the methods only uses""->""the methods only use"" <sep> citing Murphy ""This is different from our setting where the intervention is unknown to start with and is assumed to arise from other agents and the environment."" <sep> Murphy can handle unknown interventions as well. Moreover Mooij et al. handles unknown interventions too. <sep> ""The set of functional parameters θi parametrizes the conditional probability distribution of Xi given its parent set Xpa(i,C), with C ∼ Ber(σ(γ)) a hypothesized configuration of the SCM's DAG."" <sep> Can you clarify this sentence? <sep> ""During Phase 1, the functional parameters θ are trained to maximize the likelihood of randomly drawn observational data under graphs randomly drawn from our current beliefs about the edge structure."" <sep> Why do you draw synthetic data? Likelihood is typically maximized using real data at hand. It's hard to follow the exact procedure here. <sep> Intervention targets are predicted using a heuristic. Why not use the existing methods? I believe the computational aspect is seen as a problem but JCI by Mooij et al. should be fast enough. <sep> Can you convert Section 3 into a pseudo-code for the algorithm description? I believe many details are skipped and some key points of the approach is not clear by the brief text in each subsection. <sep> ""should be taken as givens""->""should be taken as given"" <sep> In the experiments, please compare with Mooij et al. Their method should be as fast as FCI and it would be interesting to see how the results compare. <sep> ==== After the Response by the Authors ==== <sep> Thank you for the detailed reply. For the clarifications the authors made to the algorithm description, I will increase my score. <sep> The authors state <sep> ""If the scientific community had waited for deep learning to prove that it could discover the true conditional distribution of outputs given inputs, we would not have had the progress we achieved in the last two decades in AI. We believe that it is important to take into consideration all sources of evidence about the usefulness of a method, and experimental evidence is at the heart of the success of the scientific method and should not be discarded because of an established cultural habit of relying on proofs of identifiability."" <sep> Note that the objections I (R1), and I believe also R3 and R4, have are not about theoretical vs. experimental research and that the paper lacks proofs or identifiability results. It is perfectly fine to not have a theoretical understanding of a proposed algorithm. But the authors should be able to justify the choices they made in the algorithm design, and especially in light of the prior work. The main justification given by the authors both in the paper and in their rebuttal is that the algorithm performs well. I believe the paper needs an iteration to address these issues. <sep> The following is my detailed feedback in addition to my original review in light of the authors' response. I hope this will help the authors in improving their paper. <sep> On fully learning the causal graph: <sep> I suggest the authors examine and try to identify, in small graphs, what aspect of their method allows it to perform better than the existing methods such as JCI or allows it to go beyond the existing equivalence classes. Without such justification, I do not think the paper in its current form will influence future research. <sep> Remark on interventions having variety: <sep> This is not sufficient for exact recovery. Imagine intervening on the same node with different mechanisms over and over. This does not allow recovery outside of the local structure around the intervened node for most causal graphs. This also relates to the remark above. Full identifiability is always related to having variety in the intervention targets and not just in interventional mechanisms. This is why some of the datasets where the exact graph is recovered by the algorithm need a detailed investigation. <sep> About synthetic experiments: <sep> One explanation for full structure recovery in the synthetic experiments could be the following: The authors randomly pick one target variable to intervene on. My guess is that this randomness in the experiment design is sufficient to have diverse enough target sets for the equivalence class to shrink to a single graph. Can you verify/check this? <sep> How many interventions do you use in the synthetic experiments? How many samples are collected per intervention? Unless I am missing something, these are not provided until page 19 but then it is not clear if these numbers are kept identical throughout the experiments. x-axis is set to be # of episodes or # of steps in most experiments whereas # of samples would be more informative. <sep> About JCI comparison: <sep> I did not completely understand why the authors could not run JCI in synthetic data. They say it is due to its complexity. But JCI's complexity comes from the graph degree and not from the number of samples for a small enough state space. It would be very interesting to compare what JCI learns relative to the proposed method in these synthetic experiments. This should test my hypothesis above that the random intervention target is providing enough diversity to reduce the equivalence class to one graph, which should be detected by JCI. <sep> Inferring a Markov equivalence class from the adjacency matrix by early stopping is definitely an interesting idea and I would encourage the authors to further pursue and formalize this direction. <sep> Sample complexity: <sep> The authors mention that their method is ""sample-hungry"". Given that the method presents significant divergence from the standard literature on causal inference that relies on conditional independence tests, which are known to require many samples, it is especially important to clearly present the number of samples used by the method. The main paper does not present the number of samples used in the synthetic experiments. These should be made explicit. <sep> Finally, the title and abstract still state ""dependency structure discovery"" and learning ""Bayesian networks"" whereas the authors attempt to learn causal graphs from interventions. I suggest an update to the narrative to clarify the objective of the paper.","In this paper, the authors study how to incorporate experimental data with interventions into existing pipelines for DAG learning. Mixing observational and experimental data is a well-studied problem, and it is well-known how to incorporate interventions into e.g. the likelihood function, along with theoretical guarantees and identifiability. Ultimately there was a general consensus amongst the reviewers that without additional theoretical results to advance the state of the art, the contribution of this work is limited."
"abstract | strength | weakness | misc | decision  ==> This work introduces a method to accelerate the training speed of deep learning models. The key idea is to start training with shared weights and unroll the shared weights in the middle of the training. The authors report that this strategy accelerates the convergence speed. The paper introduces heuristics on when to stop weight sharing and how many layers to share weights. It further provides an analysis via the view of deep linear models on why weight sharing helps improve the convergence speed. In the evaluation, the paper evaluates their approach against the training of BERT, and shows that their method can obtain comparable and sometimes even better accuracy on downstream tasks while with 50% faster training speed. <sep> Strengths: <sep> The paper aims to address an important problem in large model training: slow training speed. <sep> The paper proposes an easy-to-implement approach to accelerate the convergence speed of BERT-like models. <sep> Weakness: <sep> The technical contribution seems rather incremental. The main difference between this work and the prior work [1], which also train Transformers via shared weights,  seems to be switching from sharing weights to unsharring weights in the middle of the training. <sep> Important references are missing, making it not clear the advantage of this work as compared with existing approaches that accelerate the training of Transformer networks. <sep> The comparison with existing work is inadequate. Important ablation studies are needed. <sep> Comments: <sep> Prior work [1] uses weight sharing to train a smaller Transformer model to obtain similar accuracy.  However, weight sharing does not improve training speed per batch, because the training still needs to perform the same amount of forward and backward computation for each batch. Training may actually be slowed down, since the model may need to train with more batches to reach the same accuracy. This paper aims to speed up the training process by switching from shared to unshared weights in the middle of the training, and it observes faster convergence -- achieving similar downstream accuracy with less number of pretraining samples. This is an interesting empirical observation and can potentially become useful in practice. However, there are some major concerns about the paper. <sep> It is still unclear whether this faster convergence comes from switching from sharing to unsharring weights or is an effect of the model or hyperparameter changes. First, the stop condition (e.g., the switching threshold) cannot be known as a prior. Therefore, it is controlled with an additional hyperparameter. From the text, it is unclear how this hyperparameter has been chosen or will be chosen when training a new model. It would be better to test the sensitivity of the hyperparameter on another model such as GPT-2 to verify the effectiveness of the proposed method. <sep> Second, the paper adopts Pre-LN in its evaluation (as briefly mentioned in Section 5.1). However, from the text description, it seems it employs the original BERT as the baseline (""We first show experiment results English Wikipedia and BookCorpus for pretraining as in the original BERT paper""). As Pre-LN has been studied in several prior work [2,3] and has been demonstrated to also have the effect of accelerating the convergence speed, it is unclear whether the observed speedup in this paper is an effect of PreLN or weight sharing/unsharing. An ablation study with BERT-PreLN is needed if not already included. <sep> Third, the analysis on deep linear models appears to be over-simplified, where important characteristics of the DNNs such as non-linear activations and residual branches are not represented, making it difficult to connect it with the actual observations in practice. <sep> Finally, importance reference [4] is missing. [4] starts with a shallow BERT and progressively stacks Transformer blocks to accelerate training speed, which is in some sense similar to the proposed technique, which starts with shallow BERT with shared weights and switches to full-length BERT in the middle of the training. The paper might need to highlight the difference between this work and [4]. <sep> Several places in the paper are vague or inconsistent: <sep> The paper claims that it uses the same BERT implementation and training procedure as the one used by Devlin et. al.. However, the accuracy reported in Table 2 seems to be consistently lower than what was reported in the original BERT paper. For example, QQP in the original paper reaches 72.1, whereas this paper reports 71.4, QNLI was 92.7 in the BERT paper and 91.7 in this paper. If we use the original BERT reported results, the proposed technique seems to incur low accuracy on most tested datasets. Some clarification on the accuracy results is needed. <sep> The paper claims that ""it sounds natural for us to expect that ALBERT's performance will be improved if we stop its weight sharing at some point of training. The optimal models are supposed to not be far from weight sharing."" However, this explanation actually creates some confusion. First, the paper does not provide an analysis of how the weight distribution between the weights trained with and without weight sharing, so it is unclear what ""not be far"" means. Second, what does it mean by ""optimal model""?  Does it refer to models trained not through weight sharing? If so, prior work [5] identified that model weights and gradients at different layers can exhibit very different characteristics, which seems to contradict the argument that ""The optimal models are supposed to not be far from weight sharing"". <sep> I find it challenging to claim the proposed technique generic for models with repeatedly layers, whereas only BERT is evaluated in the experiments. <sep> The paper says ""This means that the weight sharing stage should not be too long"", but it is unclear how long is considered as not too long. <sep> [1] Lan et. al. ""ALBERT: A Lite BERT for Self-supervised Learning of Language Representations"", https://arxiv.org/abs/1909.11942 <sep> [2] Xiong et. al. ""On Layer Normalization in the Transformer Architecture"", https://arxiv.org/abs/2002.04745 <sep> [3] Shoeybi et. al. ""Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism"", https://arxiv.org/abs/1909.08053 <sep> [4] Gong et. al. ""Efficient Training of BERT by Progressively Stacking"", http://proceedings.mlr.press/v97/gong19a/gong19a.pdf <sep> [5] You et. al. ""Large Batch Optimization for Deep Learning: Training BERT in 76 minutes"", https://arxiv.org/abs/1904.00962","The paper proposed a new way for training models that stack the same basic block for multiple times -- share the weights first and then untie the weights. Ablation study shows that the proposed algorithm has marginal improvement over the baseline. The authors also provide some theoretical justifications to how the proposed idea works. <sep> The proposed idea is straightforward and intuitive. Weight sharing has been used in previous works, and what's new in this paper is to unshare the weights in the middle (with a heuristic rule). The hope is that by doing so, one can achieve a better tradeoff between speedup and accuracy. However the experimental supports are somehow weak and incomplete. For example, in order to show the real speedup, one should provide the full training curve (until convergence) under different settings, instead of just showing one data point (at 500K). It is very common that one can get some speedup at 500K, but the speedup totally disappears after another 500K steps. <sep> Furthermore, the theoretical analysis is conducted in a simplified setting, and it is not very clear whether it can be used to explain what really happened during BERT training. <sep> The reviewers conducted some lengthy discussions after the author rebuttal was available. As a final consensus, we think that there are still concerns on the paper, which makes us hesitate to give an ACCEPT recommendation."
"abstract | weakness | rebuttal_process | decision | suggestion  ==> ############# Summary of contributions ############## <sep> This experimental paper proposes a method for simulating a dataset to evaluate algorithms for enforcing fairness criteria on deep learning models. The proposed simulated dataset pertubs MNIST in various ways to create data distributions that could impact the difficulty of satisfying various fairness criteria. The paper then uses this simulated dataset to empirically evaluate three existing approaches for enforcing fairness on deep learning models using adversarial techniques. For each approach, they evaluate group-based fairness criteria such as demographic parity, equal opportunity, and equal odds under different dataset simulations. <sep> The most potentially impactful contribution of this paper is the simulated dataset and the comparison of different approaches under different dataset simulations. <sep> ############# Strengths ############## <sep> Their proposed simulated modifications of MNIST are well documented. <sep> The methodology of stress-testing different algorithms by tuning different aspects of the simulated data distribution seems intuitive and useful. <sep> The experiments are well structured, and they provide a mostly clear explanation for tuning each of the dataset simulation parameters. <sep> While they only evaluate a few group-based fairness criteria in their experiments, they provide an implementation to evaluate many more metrics listed in Table 2 in the appendix. <sep> The experiments provided seem thorough, with many different dataset simulation parameters. <sep> ############# Weaknesses ############## <sep> The dataset features seem a bit contrived, and it's not entirely clear how the different simulated features correspond to real data scenarios. For example, as one modification of MNIST, they add 6-20 drawn horizontal lines on some digits, and 2-6 drawn lines on the rest of the digits. They modify how many of the 6-20 line digits are odd, and how many are even, such that the number of lines on the digit can correlate with whether the digit is even or odd. It's not clear to me if there are real image datasets where something like this would occur. Of course, I recognize that it's impossible to capture every possible real data distribution in a simulation like this, but it would be helpful to have some explanation of why they use horizontal lines specifically. A similar explanation would be helpful for the use of the green column. <sep> They only use a binary sensitive attribute, encoded in the simulation as various shades of red/blue. It would be nice to see an example with more than two categories for the sensitive attribute, as the difficulty of satisfying many of these group-based fairness measures can increase with more categories. <sep> The related work has limited discussion of other papers that evaluate multiple fairness criteria and algorithms. The closest citation is Friedler et al. 2019, but they do not discuss or compare to the findings of Friedler et al 2019, which also evaluates four different algorithms for enforcing group-based fairness. Specifically, this paper should discuss how their conclusions and evaluations relate to the conclusions and evaluations of Friedler et al. 2019. (As a concrete example, the Stability analysis in Section 7.2 of Friedler et al. 2019 is very similar to this paper's ""Impact of seed"" analysis in the Experimental Results section.) <sep> The paper would benefit from deeper discussion of how or why the three evaluated approaches differ in performance. Otherwise, the experimental results are not particularly novel (for example, the fact that a correlation between the sensitive attribute and the label can lead to unfair predictions is well known). <sep> ############# Recommendation ############## <sep> Overall, my recommendation is 5: Marginally below acceptance threshold. While this paper has a nicely organized experimental section and a clear methodology for simulating a dataset, I don't think there's enough discussion of the dataset features and enough discussion or novelty in the experimental results themselves. I do think this type of survey work can be impactful, and would encourage the authors to add more discussion of the methodology, results, and related work, as suggested above. <sep> ############# Questions and clarifications ############## <sep> The dataset feature g_2 is introduced in Section 4.1, but not mentioned in experiments. Is g_2 used in any experiments? <sep> What were the sizes of the train, validation, and test sets? <sep> How many seeds were used for each experiment? For example, in the ""Impact of seed"" experiment, how many seeds were used to compute the standard deviation? <sep> In Section 5, the authors say, ""For each metric, the best value is reported, meaning the performance of the best-performing model on the fairness metric (chosen across hyper-parameters on the validation set) is reported on the test set...This would allow us to report the best performance on each metric, without choosing how to compromise between accuracy and fairness metric."" I don't understand what this means. Does this mean that they chose a different model for each metric reported? Did they choose one model for the equal opportunity metric, and a different model for the accuracy metric? <sep> On page 7, the authors say ""LAFTR models drop performance in the biased setup of g_1 = 2."" Can the authors clarify what they mean by ""drop performance""? Which metric is hurt? And why is g_1=2 considered to be a ""biased"" setup? It seems the g_1=2 is simply a setting when it is easy to identify odd from even numbers -- I don't understand why the authors frame this as ""biased."" <sep> ############# Additional feedback ############## <sep> The related work section is currently not organized well. I would suggest breaking the large block of text on page 2 into multiple paragraphs addressing different aspects of related work. For example, have one paragraph discussing different methods for enforcing fairness on deep learning models, and have a different paragraph discussing other survey papers for evaluating various fairness criteria and algorithms. <sep> The plots in the Experimental Results section are really tiny and hard to read (Figures 2-4). <sep> [page 6, ""The goal is to evaluate when such cases arise, do models perform fairly in test setups, where everything is completely balanced?""] It's great to outline the goal so clearly, but this isn't actually a grammatically correct sentence. I would suggest modifying this (and similar other sentences in the paper) to: ""The goal is to evaluate the following: when such cases arise, do models perform fairly in test setups, where everything is completely balanced?""","The paper studies benchmarking of bias mitigation methods. The authors propose a synthetic dataset of images (alike colored-MNIST) that enables a controlled setup over different types of correlations between a binary sensitive attribute, dataset features, and a binary outcome label. The authors have evaluated 2K models that are the variants of three recently proposed debiasing methods using fair representation learning across various settings. <sep> While the reviewers acknowledged the importance of benchmarking fair learning methods in a systematic controlled setting, they have raised several concerns: <sep> (1) the proposed benchmark is too abstract/unrealistic (R4, R2, R3); it is not clear whether the findings from this benchmark can be generalized to real-world data with real sensitive features, (2) the proposed benchmark is limited to pseudo sensitive attributes (R1) that are binary (R1, R2), (3) the paper lacks in-depth analysis on why certain methods work under certain conditions (R3). Among these, (2) did not have a major impact on the decision, but would be helpful to address in a subsequent revision, (3) was partially addressed in the rebuttal. However, (1) makes it very difficult to assess the benefits of the proposed benchmark, and was viewed by AC as a critical issue. <sep> The authors provided a detailed rebuttal addressing multiple of the reviewers' concerns. AC can confirm that all four reviewers have read the author responses and have contributed to the discussion. A general consensus among reviewers and AC suggests, in its current state the manuscript is not ready for a publication. See R1 post-rebuttal encouragement and suggestions how to strengthen the work. We hope the detailed reviews are useful for improving the paper."
"rating_summary | misc | weakness | decision  ==> Summary <sep> The paper is about ANN being best-known models of developed primate visual systems. However this fact does not yet mean that the way those systems are trained is also similar. This distinction and a step towards answering this question is the main motivation of this work. The authors demonstrate a set of ideas that while drastically reducing the number of updates maintain high Brain Predictability according to the BrainScore. The significance of this result in my opinion largely depends on how well we can map those observations and methods to biological meaning and knowledge on how primate brains are trained (see the discussion point below). <sep> Critique, Questions, Discussion <sep> (1) How good the ""match"" between the brain and DCNN is in the first place? For example, if we measure the match in terms of correlation (between responses, or predictions, any metric would work in the context of this question), then 80% of corr=1.0 would be very impressive and significant, while 80% of corr=0.2 (being corr=0.16) could well fall under the noise and while being significant numerically, does not give us the opportunity to say that we have captured 80% of the match between the artificial system and the ventral stream (because what we have actually captured is 80% of corr=0.2, which might as well be almost nothing). <sep> (2) ""squirrels to jump from tree to tree within months of birth"", ""macaques to exhibit adult-like visual representations after months"" -- hoe many synaptic updates happen during those months? Do we know? Maybe it is also in trillions? In which case this portion of the argument would fall apart. Emphasis on ""supervised"" would probably still survive. <sep> (3) ""a child would need to ask one question every second of her life to receive a comparable volume of labeled data"" -- are they not? I would say children get even more data if by ""question"" we will mean not only verbal questions and answers, but also answers that are tactile (""how this will feel on touch?""), auditory (""what does this object sound like""), visual prediction (""will this thing now move to the right or to the left?""), etc. Seen like this I would say that children receive tons of supervised data and ""one per second"" is an underestimation. <sep> (4) How does the ""match"" vary depending on random initialization? Is it consistently 54% or is there a substantial +/-? <sep> (5) How do we know the ""true zero"" in terms of the ""match""? What would be a model (function? maybe a constant function?) that clearly has zero ""match""? If we now take this function and run it through your pipeline to get the match%, would the result be indeed 0% or something else? Maybe 54% is the ""true zero"" and not 0%. <sep> (6) Why sampling from CORnet-S-based clusters of parameters is a good way of modeling ""at-birth"" situation? Compared to 54% achieved with this methods, what would be the match% if the network would initialized with vanilla Kaiming Normal? Uniform? <sep> Recommendation and justification <sep> My main concern is with the interpretation of the meaning of this work. BrainScore's metric is a very approximate proxy that weakly reflects the match between models of vision. In this work, however, this metric is taken as a ""gold standard"" and it is assumed that achieving, for example 50% of BrainScore of 0.42 is something biologically meaningful. An ablation experiment that would demonstrate that achieving these 50% (or other numbers presented in the paper) is a non-trivial event which can only happen if the model is indeed becoming more ""brain-like"" would go a long way in making the case of this work strong. I suspect, however, that such an ablation study will show that there are ways to achieve high% of BrainScore using models that are completely dissimilar to the brain. I currently evaluate this submission as borderline, and am looking forward to authors' views on the concerns I have outlined above: do these indeed matter and affect the claims of this work (and how should we see them if that's the case), or are these concern largely irrelevant (and why we can ignore them if that's the case?). <sep> Additional remarks <sep> Arguably missing references on modeling of the ventral stream with ANNs: https://www.nature.com/articles/s42003-018-0110-y, https://www.jneurosci.org/content/35/27/10005 <sep> UPDATE - Nov 30 <sep> After looking at the revised version of the manuscript I am still concerned that the claims made in the abstract (and implied in the main text of the paper) about the match of ANNs to the brain are misleading the reader into assigning greater biological significance to the reported result than it actually holds. While the authors made slight modifications in the text and added a few sentences commenting on the issue, these changes did not constitute a change would make the reader ""extremely aware that when you say ""80% match"" you don't mean ""80% match to the brain"", but ""80% match to the score"". I find that a softer claim that would explicitly acknowledge that 5% of ""synaptic"" updates explain 80% of the predictivity score and not 80% of the match to the brain would make this work more scientifically precise and thus more valuable. I am keeping my original assessment of this paper as being borderline.","This paper received 2 borderline accepts, 1 accept, and 1 reject. <sep> This paper was discussed on the forum and no consensus was reached. The two reviewers who rated the paper as borderline accept emphasized that the biological claims are overblown, that the intellectual contributions (the initialization scheme and partial training) are incremental from a statistical learning perspective, and that the potential applications for the future (like alternate learning rules) are too speculative. I agree with both of these reviewers (and the negative reviewer) that the biological rationale is problematic and the approach is not credible as a model of biology. It is not evaluated as a computer vision model either. And I completely agree with the point raised by several reviewers that there is simply no data about how many synaptic updates to target. Hence, statements regarding % of total synaptic updates and % of brain matches seem empty without a precise target. For all these reasons, I recommend this paper be rejected."
"abstract | strength | weakness | rating_summary | weakness | suggestion  ==> Summary: <sep> The authors apply reinforcement learning to automated theorem proving to eliminate the need for human-written proofs as training data. During training, the prover is improved incrementally by using the current version to prove theorems and add proved theorems to the training data. The authors propose to use TF-IDF for premise selection. They show that it enables the prover to outperform a supervised learning baseline without using human proofs. <sep> Strengths: <sep> The paper addresses an interesting and important problem—learning theorem provers without explicit supervision from human proofs. <sep> The evaluation is sound. The authors are able to show some benefits from their IF-IDF premise selection. The proposed method outperforms the baseline on the HOList benchmark. <sep> The paper is well-written overall, though some sentences are difficult to parse, e.g.,  ""comparison OF the effect OF availability OF human proofs"" in the intro. <sep> Weaknesses: <sep> The method is not new. There exists a body of work leveraging reinforcement learning to learn theorem provers [A, B, C, etc.]. Many of them also do not rely on human proofs, e.g., [B]. Although the authors may be the first to apply reinforcement learning to prove theorems in interactive theorem proving environments, this is not a sufficiently novel contribution. <sep> [A] Kaliszyk, Cezary, et al. ""Reinforcement learning of theorem proving."" Advances in Neural Information Processing Systems. 2018. <sep> [B] Piotrowski, Bartosz, and Josef Urban. ""ATPboost: Learning premise selection in binary setting with ATP feedback."" International Joint Conference on Automated Reasoning. Springer, Cham, 2018. <sep> [C] Zombori, Zsolt, Josef Urban, and Chad E. Brown. ""Prolog technology reinforcement learning prover."" arXiv preprint arXiv:2004.06997 (2020). <sep> Besides, using TF-IDF for premise selection is not new. In the related work section, the authors say, ""Gauthier et al. (2017) uses a tf-idf based premise selection model, but does not learn a model."" I do not understand what the authors mean by ""does not learn a model."" It would be great if the authors can further clarify the difference with Gauthier et al.","Overview This paper applies RL to automated theorem proving to eliminate the need for human-written proofs as training data. The method uses TF-IDF for premise selections. The experiments compared with supervised baseline demonstrate some good performance. <sep> Pro The paper provides a side-by-side comparison of the effect of the availability of human proofs on the final theorem proving. <sep> The experiments compared with supervised baseline show that the proposed method has good performance even without human knowledge. The prosed TF-IDF selection algorithm addresses a challenging issue in exploration of RL. <sep> Con The reviewers primarily concern about the novelty of the methods. It appears the method is not new since there exist a body of work leveraging RL to learn theorem provers. The tasks are also not novel. After rebuttal, the reviewers are not convinced that the novelty is significant enough for *CONF*. The reviewers are also concerned that the proposed method might not be easily generalized to other tasks. <sep> Recommendation Although the proposed method and experiment demonstrate some merits, there is a lack of novelty in terms of approaches. Since existing results already consider similar methods and similar tasks, it would make the paper stronger if thorough experimental comparisons are performed."
"misc | strength | misc | rebuttal_process | suggestion | misc  ==> ########################################################################## <sep> Summary: <sep> This paper studies the convergence of adaptive gradient methods under an interpolation assumption, showing for example these methods can converge at an O(1/t), instead of O(1/\\sqrt{t}) rate when perfect interpolation is satisfied. Convergence behaviors with line search and Polyak step sizes are also analyzed. <sep> ########################################################################## <sep> Reasons for score: <sep> This paper provides good insights regarding how an interpolation assumption may help accelerate adaptive gradient methods. I do not feel the technical results very solid, as some difficult-to-check properties are just put as assumptions (see cons). <sep> ##########################################################################Pros: <sep> Pros: <sep> The results provides insights regarding why adaptive gradient methods may converge faster when the interpolation assumption is satisfied.   <sep> Line search and Polyak step size methods help address the need of problem and algorithm parameters in standard theories. Moreover, there are few papers discussing line search and Polyak step size methods in the finite-sum setup. <sep> The Polyak step size is well-motivated in the interpolation setting. <sep> ########################################################################## <sep> Cons: <sep> The abstract claims that ""AdaGrad can achieve an O(1) regret in the online convex optimization framework."" I do not see this result in the main text. <sep> The paper reads waiving regarding difficult-to-check assumptions. In particular, this paper assumes the sequence of iterates is bounded in a set of radius D and the eigenvalue of the preconditioning matrices are bounded. The main argument supporting these assumptions are simply they are ""common"" in existing literature. I feel the technical challenges in the analyses are alleviated a lot because of these ""common"" assumptions.   Notice that without the conditions, the convergence guarantees may not be meaningful because the D parameter in all theorems and a_{min} and a_{max} in Theorem 3 and Theorem 4 can scale with the iteration counter.   <sep> The proposed line search methods seem to be computationally very expensive. The proposed line search methods require computing the largest step size satisfying the desired inequality, instead of the largest among a sequence of exponentially decaying step sizes as in standard Armijo line search methods. Is it possible to analyze the performance of latter—more computationally favorable—scheme?  <sep> It is claimed that the step size chosen by the proposed conservative Lipschitz line search method is bounded in [2 (1 - c) / L_{max}, \\eta_{k - 1}]. Can it happen that \\eta_{k - 1} \\leq 2 (1 - c) / L_{max}? If yes, then is the step size selection rule well-defined? There is also a similar claim for the stochastic Armijo line search method.   <sep> I don't get the sentence that ""a similar distinction between the convergence of constant step-size Adam (or AMSGrad) vs. AdaGrad has also been recently discussed in the non-convex setting (Défossez et al., 2020)"" in Section 4. What is the distinction?   <sep> Minor comment: <sep> Typo in the first paragraph: online batch reduction -> online-to-batch reduction m_1 is not specified in (1). <sep> The abbreviation SPS is not defined when it appears for the first time in the main text.   <sep> ########################################################################## <sep> After reading author rebuttal: <sep> I think the value of this work is to demonstrate the possible benefits of an interpolation assumption. Hence, though there are some theoretical issue and theory-practice gap, I keep the original score. <sep> I do not feel it very reasonable to emphasize the regret result in the abstract and then put the corresponding section in the appendix. It is more reasonable to move Appendix C.2 to the main text though I guess it would be difficult in practice. <sep> I understand adding a projection step typically does not change the proof much. However, this is not the setup analyzed in this paper. I feel the associated newly-added clarification in Section 2 reads somewhat waiving. I suggest the authors add some clarifications in the revision similar to the following in the rebuttal. <sep> ""Without an explicit projection step, we believe it is possible to adopt the recent SGD analysis ""On the almost sure convergence of stochastic gradient descent in non-convex problems, NeurIPS, 2020."" to prove that the iterates do remain bounded with high-probability. We leave this for future work."" <sep> The other clarifications are OK to me. <sep> I suggest the authors add some clarification regarding 4 in the main text. <sep> #########################################################################","Dear authors, <sep> the paper contains many interesting and novel ideas. Indeed, tuning step-size is very time and energy-consuming, and deriving and analyzing new adaptive algorithms has not only theoretical benefits but, more importantly, is a key when training more complicated ML models. <sep> The paper contains many weaknesses as noted by reviewers. I know that you have addressed many of them one of the reviewers is still concerned about the other issues involving Theorem 1 and the assumption of the bounded preconditioner. <sep> He thinks the preconditioner bound is troublesome. In the overparameterized regime, he would expect the gradients to become near zero as the algorithm converges, which would actually cause the preconditioner to NOT be bounded below. It seems that the analysis might actually improve if the authors abandoned AMSGrad/Adam and instead just considered SGD for which the preconditioner assumption is not an assumption but just a property of the algorithm. <sep> Thank you"
"abstract | rating_summary | decision  ==> Summary: <sep> This paper focuses on model-based black box optimization problems in the offline setting. These are settings where access to ground truth is expensive, and instead the optimizer has access only to a trained model of the ground truth based on limited data. While optimizing on this surrogate space, a good optimizer often needs to account for model uncertainty and accuracy degradation. The main aim of the paper is to provide a test bed for algorithms that try to solve this challenge. <sep> Positives: <sep> This is a well-motivated line of work because there is a large interest in these problems across fields. There is indeed a need for better benchmarks and better libraries to make it easy to compare methods. I think the paper is executed cleanly and the it's well-written. I also think the work, when completed, has potential to be very useful. <sep> Areas for improvement: <sep> In my view, the paper has shortcomings in it's design, development and scope. A paper like this is helpful when it can: <sep> (i) Establish good practices across the board by streamlining workflow, and ensure the interface is used the same way when comparing methods. <sep> (ii) Contribute code that make it easy and fast to use and develop on. <sep> (iii)  Make it easy to collect and report relevant performance statistics the same way across algorithms, helping the field by making it easy to benchmark. <sep> (iv) Includes challenges that are diverse but relevant to the use case of the algorithm. <sep> (v) Synthesize a suite of methods from the literature that are distinct from each other and of interest to the community as strong benchmarks. <sep> I don't think the paper is addressing these aspects sufficiently. Some detailed comments below. <sep> (1) The categorical choice of benchmark tasks is not clearly justified (e.g. Why should anyone care if a protein design algorithm is poor at designing a robot controller?) At the end of the day, these problems share little structure, and ""no free lunch"" arguments (Wolpert 95) would suggest that there is no one good algorithm for every challenge. The real world settings for these problems don't map well to each other. This paper as designed, could result in follow up work with meaningless comparisons between algorithms that have no business being compared (unless there is a meaningful connection between the challenges). If the authors feel like they can justify this particular set of challenges, I'm open to be convinced. E.g. if the argument is that there will be a ""master algorithm"" that is just good at designing everything, and this benchmarking set is designed to enable that, I could drop this point, but then other issues will be relevant. <sep> (2) The particular choices within each class of tasks is insufficiently justified. Why is GFP a good design challenge given that the ground truth is also necessarily a trained oracle with likely poor performance outside the training data? Just because some previous papers chose this as the design task doesn't make it a good benchmark. No statistics is provided for how good the GP model for GFP is.  As for other proteins, GB1 for instance is far more completely surveyed than GFP.  There are also better published models of GFP available (e.g. TAPE, UniRep). All of these would of course struggle with out-of-domain samples. Why not use a physical simulator like Rosetta that wouldn't have this issue? I believe when designing a benchmarking suite, these decisions should be considered more carefully, as it would quickly become the test bed for follow up work and a flawed design choice here amplifies in future work. <sep> (3) A body of literature for algorithms that can readily perform MBO has been neglected.  It is trivial to run a regular optimization algorithm with the model (instead of ground truth) and compare the proposed solutions to ground truth.  Quality-Diversity/EDA algorithms (e.g. genetic algorithms,  simulated annealing,  CMA-ES, or even pure CEM (rather than DbAS)...) consistently perform ""well"" in these high-dimensional optimization settings.  The success of the gradient-based method gives more reason to believe representatives of each of these classical approaches should be included and suggests that the claim that climbing proxy model will necessarily result in bad ""ground truth"" outcomes is a weak one. <sep> (4) For a benchmarking library like this, there needs to be mature code available for review (not submitted). I've checked the provided website multiple times, and while it is under active development, the code is not accessible. From what I gather the current code interface simply gives access to some data points and a ground truth. This is too little API. A good benchmarking tool would let the user abstract away the modeling part easily, and be able to readily port and run their algorithm against benchmarks, producing the results in the same way. It should also take care of running sanity checks/tests for the user and generating the same plots as those in the paper. <sep> (5) The fact that gradient-based methods outperform other methods presented here is only surprising in the sense that they were not included in the original papers (i.e. why weren't gradient-based methods benchmarked there? not this paper's fault of course).  The authors express  a general conviction where gradient based methods have done very poorly in other attempts for MBO, but provide no references, it would be great to cite relevant references for this claim. <sep> (6) As is, the paper/library only compares CbAS/DbAS with MINs and hill-climbing methods. I think it is not sufficient breadth of methods to make a ""benchmarking"" suite. As far as I can tell, CbAS/DbAS and MINs are not the best published algorithms in any of the domains suggested, so the authors should justify why they are the algorithms to benchmark against? <sep> For instance, Angermueller et al 2020 *CONF*, have an offline RL algorithm that can in principle solve all of these problems. In fact DynaPPO, PPO, and Ensemble-based Bayesian optimization all outperform CbAS in that study. Some sequence design challenges used there seem to be better benchmarks than GFP.  There is substantive work in molecular design on MBO,  but none of the SOTA algorithms are included (e.g the now-classic Gomez-Bombarelli 2016, or  perhaps adaptation of Zhou et al 2019 Scientific Reports). A good rule of thumb in my view is to include the SOTA or well-established algorithm for each task category. <sep> (6) I suggest the authors think carefully about what the evaluation criteria are. While optimization itself is a good metric, other factors, such as providing a way of evaluating diversity of solutions, or sensitivity to dataset size (e.g. by subsampling), are good to consider. <sep> ========== <sep> I would like to encourage the authors to continue the pursuit of this work because it is relevant and well-motivated, and has great potential, in my view it is simply not ready. I think this work needs to be reviewed again when it is more mature,  with wider range of algorithms, better justified challenges, a larger set of metrics that can be easily collected, and available code such that the reviewer can vet the benchmarks and code properly. Right now, it's a comparative review of a small set of methods, not a good benchmarking suite. <sep> If done well, it can be a very useful suite that can help researchers develop better algorithms. The danger of accepting it prematurely is that it will be a basis for future work that ""game-ify"" studies of algorithms against irrelevant/misleading set of benchmarks. That is only damaging to the development of good algorithms and could misguide research. As it stands,  I find the latter risk higher than it's contribution, and hence I believe it should be rejected and reviewed once more of these structural issues are addressed (and code is available to review). <sep> ~~~~~~~~~~~~~~ <sep> Post review and discussion remarks: <sep> I think the authors have improved the paper significantly during the review period. However, three of my main concerns about the paper remain to a degree that I'm not confident about the paper's value (or risk of misleading followups). (1) That the set of challenges is somewhat arbitrary, some tasks are using ""real"" ground truths while others are simply running on known trained oracles.  (2) That implementation of strong offline RL benchmark algorithms are missing (because they don't exactly apply across domains) even though they can always be applied in this setting even if ""exact"" conditions are not applied in every case (just like Gradient Ascent or BO were) (3) That the API needs to offer more for this to be a good benchmarking suite. <sep> I've been most concerned about 2 and 3, and after reading the code, I find that it is still too ""bare bones"" to be a good package. I looked back at OpenAI gym, and there are several abstractions that they make, including actions, observations, environments, spaces that help the implementer unify how they deal with the complexity underneath. <sep> So far as I can tell, most of what design-bench does is load a csv matrix into an task.x, task.score(x) , and also lets the user access some approximate oracle task.y. This are critical to the process but their abstraction as related to the paper are not clear to me at this time. How is task.y computed, how is the ground truth actually representative of reality. How does optimization depend on the choice of oracle for task.y? <sep> Having read the code, useful elements are in there to make for a good package, I feel like it needs improvements and another review for scientific soundness. <sep> I've updated my score to address the improvements made. The paper scores somewhere between a 4 and a 5 for me.","This paper proposes a benchmark suite of offline model-based optimization problems. This benchmark includes diverse and realistic tasks derived from real-world problems in biology, material science, and robotics contains a wide variety of domains, and it covers both continuous and discrete, low and high dimensional design spaces. The authors provide a comprehensive evaluation of existing methods under identical assumptions and get several interesting takeaways from the results. They found there exists surprising efficacy of simple baselines such as naive gradient ascent, which suggests the need for careful tuning and standardization of methods in this area, and provides a test bed for algorithms that try to solve this challenge. However, most reviewers agreed that a more in-depth analysis and insightful explorations for the RL experiment results will help readers understand why their method has superiority even without trajectory data, and that the paper needs another revision before being accepted. Therefore, I recommend rejection although all reviewers agreed that the tasks is very interesting and a good start."
"abstract | rating_summary | weakness | suggestion | rebuttal_process | suggestion  ==> Summary: <sep> In this work, the authors consider the problem of continual learning with distribution shifts.  The work extends the recent work invariant risk minimization (IRM) from Arjovsky et al. to a continual learning setup. IRM was designed as an offline learning framework. In this work, the authors consider the setting where the different domains arrive sequentially. The authors propose a Bayesian extension of the IRM framework that allows sequential updates as the environments arrive. They provide a justification for how the KL divergence term helps in shrinking the support continually to arrive at an approximately invariant support. Several experiments were carried out on colored MNIST and its variants to show how the proposed scheme is better than existing continual learning methods and existing IRM frameworks. <sep> Pros: <sep> I like the problem the authors consider. It is indeed important to understand how to extend IRM type frameworks in settings such as continual learning. I also liked the ADMM based approach taken by the authors as this approach gives another way of learning IRM based predictors in addition to existing approaches. <sep> Cons: <sep> There are several problems with the paper. I discuss these problems below in different subsections. If these major issues are addressed in the rebuttal, I would be open to changing my score. <sep> a) Information Projection (Theorem 3) <sep> In the work, the authors show in Theorem 3 as to how when the model is trained sequentially the support of the distribution shrinks. <sep> Let us consider optimization in 2 time steps t=1, q_{t} = min_{q_{t}\\in P} KL (q_t||q_{t-1}) --> support of q_1 = P \\cap support of q_0 <sep> t=2, q_{t} = min_{q_{t}\\in P} KL (q_t||q_{t-1}) --> support of q_2 = P \\cap support of q_1 = P \\cap P \\cap support of q_0 = P \\cap support of q_0 <sep> It is not clear from just the KL term why support would strictly shrink. The reason why the authors approach works is because the first term based on the IRM loss allows to update and shrink the support and the KL divergence term ensures that there is no need to expand the existing support. Define the first term in the loss associated with IRMv1 as Q_{IRMV1}(q_t) <sep> t=1, q_{t} = min_{q_{t}\\in P} Q_{IRMv1}(q_t) + KL (q_t||q_{t-1}) --> support of q_1 \\subset P \\cap support of q_0 <sep> t=2, q_{t} = min_{q_{t}\\in P} Q_{IRMv1}(q_t)  + KL (q_t||q_{t-1}) --> support of q_2 \\subset P \\cap support of q_1 <sep> It is the combination of the IRM term with KL term that ensures shrinkage and KL term on its own only ensures support does not expand. A proof or an illustration of what I state would have been nice as the current intuition from the authors is incomplete and to some extent a bit misleading. <sep> b) Why penalize the IRM constraints as well?The authors arrived at a variational formulation in 9a and 9b with new objective functions defined in 10 a and 10b. It is unclear why it was chosen to impose KL penalty in the constraint (10b) as well. If not a theoretical justification, an experiment in the supplement illustrating why these choices were made would be nice. At least some good intuition needs to be provided. <sep> c) About the ADMM approach: <sep> I am happy to see the authors tried the ADMM approach. However, one thing that is unclear is it does not necessarily serve the purpose of solving the problem authors want. From the experiments it seems a continual extension of IRMv1 itself suffices. Is there any other reason why ADMM approach was used. Also, if ADMM approach was proposed as another way to solve standard                                                                                                               IRM in the offline setting, then some experiments explaining any advantage would be useful. At least some offline comparisons would make a better case for using ADMM. <sep> d) Why not empirically compare with Javed et al.? <sep> As the authors correctly identify there is a recent work from Javed et al., which solves the same problem but makes more assumptions about the data.  I understand that the work uses one hot encoding of color etc. Despite that you can allow your model to also have access to the same data format and then see if your model has any more advantage to offer over and above what was done in Javed et al. <sep> e) Comparisons with offline IRM based methods (IRMv1 Arjovsky et al., IRMG Ahuja et al.) do not seem to have been carried out in a fair manner. <sep> The authors have not provided a very clear description of how IRM based methods were trained. It seems to me that the authors are running offline IRM based methods in the following manner.  They provide the data from the first environment and then keep the offline IRM based models fixed and not update them when the data from the next environment arrives. If they indeed update the offline IRM based methods, then there is no reason why the methods perform so poorly.  What was the reason for not updating these models? The reason I am suspecting is that everytime retraining the entire model on the entire data is computationally expensive. If that is the concern, then authors should have shown a comparison of run-times. The authors should have run experiments allowing all methods the same run-times and then shown that IRM based methods are still not able to perform better. <sep> For instance if the time to train continual IRM based model proposed by authors for the two environments is a total of 20 seconds. <sep> Suppose the total time to train a standard IRMv1 is say a total of 15 seconds for the first environment. There is still a five second window for the IRMv1 model to be updated.  If run-time comparisons are non-trivial, then at least there should be a budget on the total number of gradient computations that is fixed across methods. <sep> f) Compare with Creager et al. <sep> The reason IRM based offline methods did not perform well is because they were given data from one environment only.  Recently, in Creager et al., it was shown how one can learn to split the data to create new environments and then train an IRM on those environments.  The authors should compare with the Creager et al. as follows. When the data from first environment comes in you can use the approach introduced in the paper http://www.gatsby.ucl.ac.uk/~balaji/udl2020/accepted-papers/UDL2020-paper-045.pdf to learn environments (code available at https://github.com/ecreager/eiil). Therefore, by doing this you can split the data in the first environment into smaller environments and use IRM. This would lead to a lot of performance improvement for the offline IRM based methods. <sep> g) Current experiments do not reflect the true potential of the method proposed by the authors: <sep> The current experiments do not do a good job of reflecting a continual learning setup. Simple modifications on existing IRM based methods can outperform the method proposed by the authors. However, I believe the method proposed by the authors is nice and much more experimentation is needed. I make some suggestions on how to improve the experiments. <sep> It seems that in the current experiments after 2 environments there is no more gain from using any method, in fact more environments seem to hurt. I would encourage the authors to do an experiment where adding more environments actually removes spurious correlations more and more and helps.  To this end, there are three suggestions I would like to make: <sep> i) First let us understand is why is two environment sufficing in colored MNIST dataset.  The reason is that the dimension of the spurious factor, i.e., color is small (only two colors). You can increase the number of colors, then more environments will be needed to decorrelate. <sep> ii) If  adding colors and doing comparisons using raw images is hard, then you can try using the one hot encoded colored MNIST from Javed et al. and add more colors to it. <sep> iii) Another suggestion would be to try the regression experiments from Arjovsky. In Arjovsky, the authors use two environments only. However, the two environments differ a lot 0.2 and 2.0 variance. If you introduce a sequence of environments from 0.2 to 2.0, say 0.2, 0.4, 0.6, ...2.0., then it is likely that more environments will have some benefit. <sep> Quality: The solution proposed seems promising. There are problems with theoretical justification. Experiments need a lot of work as I stated above. <sep> Significance: The problem considered in the paper is interesting. The experiments carried out do not do a justice to the problem being considered. <sep> Clarity: The writing of the paper can be improved. The experiments need to be described much more clearly and use the space in the supplement to give all the details. Also, the steps of ADMM shoould have been better explained. <sep> References <sep> Javed et al. ""Learning causal models online"" <sep> Creager et al. ""Environment Inference for Invariant Learning""","The authors address the problem of learning environment-invariant representations in the case where environments are observed sequentially. <sep> This is done by using a variational Bayesian and bilevel framework. <sep> The paper is borderline, with two reviewers (R2 and R3) favoring slightly acceptance and two reviewrs (R4 and R1) favoring rejection. <sep> R4 points out that the current experiments do not do a good job of reflecting a continual learning setup and that simple modifications on existing IRM based methods could outperform the method proposed by the authors. The authors are encouraged to take into account the reviewer's <sep> suggestions to improve the paper. <sep> R1 argued initially that the proposed solution is not learning at all since it has errors very close to random guessing. While the authors have improved their method in the revision, the results are still close to random guessing, which questions the practical usefulness of the proposed approach. Also, in the revision, the authors managed to obtain better results when their method is combined with Environment Inference for Invariant Learning (EIIL), but these results are secondary and not the main part of the paper. <sep> The authors should improve the work taking into account the reviewrs' comments."
"abstract | suggestion | weakness  ==> This paper assumes no access to the reward values and attempts to learn a policy by starting just with one demonstration to define the reward. For obtaining the reward, the authors rely on the ideas from Value Iteration Networks (VIN) method and they add the modules that help to deal with cases with complex transition dynamics. The resulting method is tested on atari domain and on continuous control tasks. <sep> Strong points: <sep> The ideas from Value Iteration Networks are applied to the environments with complex dynamics. <sep> The problem setting is interesting. The restrictive assumption about the amount of demonstrations makes the method practical. <sep> The results are promising and seem to significantly outperform the competing baselines from inverse reinforcement learning. <sep> Weak points: <sep> The paper relies a lot on VIN method ideas without introducing it with sufficient detail. This makes it difficult to follow as many  concepts (and notation) in Section 3 are not properly introduced. Besides, the contributions of this paper are not very clearly stated. <sep> The analysis part in 4.1 is not very informative and does not provide experimental evidence on why the proposed method works well and what issues with previous methods it addresses. For example, it is still not clear to me why the small model performs better than the big model in Table 2. Another example is the lack of ablations to understand the contribution of each of the components of the method. Additionally, the reader would benefit from understanding how the gap between the proposed method and the baselines changes with the growing amount of demonstrations: In particular, does the proposed model address the issues of low data regime or does it help in general? <sep> The atari environment actually includes the scores in the input images (clearly in Breakout, Bean Rider, Space Invader, Seaquest, Kung Fu, and maybe in Qbert, BattleZone). Is it fair if no reward is assumed to be available? <sep> The paper is quite hard to understand, it immediately jumps into details of the method in Section 3 without providing sufficient overview and intuition into what the method tries to achieve and what the issues with previous works are. <sep> I am leaning towards the rejection of this paper. While the experimental results are very encouraging as they are provided for the environments with complex dynamics and with very little expert data, my main concerns are that 1) the method description is currently very hard to understand, the contributions are not stated clearly, and 2) the experimental results do not include any informative analysis to understand the underlying reasons for why the method performs well. <sep> Questions: <sep> In the conclusion, the authors say ""policies that generalise well to new tasks"", what is meant by this and what are the supporting experiments? <sep> Why does the small method model work better than the big model? <sep> What is the contribution of different components of the method? How sensitive the method is to the choice of hyperparameters, such as K? <sep> In Figure 4 it seems that many methods didn't converge to the final score yet. Why was such a number of training steps chosen? What happens if the experiments run for longer? <sep> What is the computational complexity of the proposed algorithm? <sep> Why is it possible to take average or max reward values of the reward map? How is the reward map idea justified when it is provided not for the map of states but where each map corresponds to one state? <sep> Additional comments <sep> The clarity of writing could be further improved. For example, see the Conclusion section (but it applies in other parts of the paper as well), avoid -> avoids, outperform -> outperforms, limiting and limited are repeated etc. Figure 3 has its labels shifted and Figure 1 seems to have the rows swapped. In Figure 2: is q_\\phi meant? <sep> Maybe the analysis part in 4.1 is more suitable for the related work. <sep> === Post rebuttal === <sep> I would like to thank the authors for the very detailed response and the improvements in the manuscript. I found the ablation experiments and experiment with ""No scores"" particularly useful. However, I am still a bit confused about better performance of the smaller model. Maybe, more understanding could be gained if there was an additional ""tiny"" model that would show that when going beyond a certain size, the performance degrade. Additionally, if the overfitting is an important issue, some regularisation methods could be explored. <sep> Finally, in the light of many changes in the paper and the original request of all the reviewers to have the writing in the manuscript improved, I think the paper would benefit from another round of reviews. However, I find this method promising and if the paper is not accepted this time, I encourage the authors to re-submit a revised version.","The authors introduce vPERL, a model that generates an intrinsic reward for imitation learning. vPERL is trained on demonstrations to minimise a variational objective that matches a posterior formed by ""action backtracking"" and a forward model, with the intrinsic reward coming from the reward map. The authors might be interested in related work on few shot imitation learning: e.g., ""One shot imitation learning"", Duan et al, 2017, ""Watch, try learn: meta-learning from demonstrations and rewards"", Zhou et al 2019. As all reviewers pointed out, and I can confirm, the paper is quite tricky to understand in its present form, and would very much benefit the writing being re-visited to more clearly express the ideas within (in particular, section 3, which is the core of the contributions)."
"abstract | rebuttal_process | suggestion | rebuttal_process | decision  ==>  ==> Given two input graphs G1,G2 the maximum common subgraph detection problem asks to find an induced subgraph of both G1 and G2, with as many vertices as possible. In the recent years, there have been papers that introduce different heuristics for guiding the search of this subgraph within branch & bound algorithms. The main contribution of this paper is a combination of graph neural network embeddings and RL to guide the search more efficiently.  The function used to guide the deep Q-network is given in Equation (3). The paper performs a set of experiments on synthetic and real world pairs of graphs, where it is shown that it performs well in practice. The supplementary material provides more details on the experiments. <sep> While the exploration strategy is more sophisticated than previous works, it comes at a greater computational cost than prior work. The experimental section should explain the trade-offs. Some of the plots from the supplementary material should be included in the main text, but the overhead that one needs to pay due to running GNNs and RL should be clearly stated. <sep> In figure 2, shouldn't the output graphs, be induced, MCS subgraphs of the input graphs? While the figure serves the purpose of illustrating the exploration idea, it is a bit confusing. The caption and/or the text should better clarify the description of figure 2. <sep> Are there some interpretable heuristics that you can derive by studying the policy of your algorithm? <sep> It would be interesting to observe the effect of planted isomorphic subgraphs within larger graphs, with different connectivity patterns. For instance suppose we plant a large isomorphic subgraph on S={1..k} for convenience in two graphs G1,G2, but the connection between S, V/S is totally different between G1,G2. How would this affect GNN embedding for instance? <sep> Given that some times inputs are noisy, are you aware of works where the goal is to find approximate MCS subgraphs, i.e., a small number of edges differing between the two subgraphs? This would cause issues to the branch-and-bound policy, but nonetheless it is interesting to think whether your method can be adapted to this case within a different search framework? What about when graphs have (non-negative) weights on their edges, or they are directed? Have you performed experiments on the latter two settings? <sep> The authors motivate their problem with applications in drug discovery, chemoinformatics etc. Such an application is missing. Furthermore, for many real-world applications the input graphs have specific structure that enable the discovery of large common induced subgraphs even in polynomial time provably.  See for example ""A polynomial-time maximum common subgraph algorithm for outerplanar graphs and its application to chemoinformatics"" by Leander Schietgat, Jan Ramon & Maurice Bruynooghe. While this fact does not render the contributions of the author(s) useless, it does reduce their value in terms of application domains, that are not appropriately discussed in the paper. <sep> Overall I found the paper to be interesting, but there are some non-trivial issues that need to be better addressed.","The paper present a new learning-based approach` to solve the Maximum Common Subgraph problem. All the reviewers find the idea of using GCN and RL to guide the branch and bound interesting although, even after reading the rebuttal, there are some important concerns about the paper. <sep> The main issue raised by many reviewers are on scalability of the methods and motivation of the problem. It would be nice to add a scalability experiments on large networks(>1M nodes) to show that the method could potentially scale. In fact, the original motivation based on drug discovery, chemoinformatics etc. application is a bit weak because in those area domain specific heuristic should work better. <sep> Overall, the paper is interesting but it does not meet the high publication bar of *CONF*."
"abstract | rating_summary | weakness | strength | suggestion  ==> This paper studies the relationship between adversarial and knowledge transferability. It develops two metrics to measure adversarial transferability and empirically shows that adversarial transferability correlates with knowledge transferability. <sep> Pros: <sep> The paper studies an interesting problem. Intuitively, both adversarial and knowledge transferability show how much the decision boundaries of the source and target classifiers are aligned. This paper is interesting in that it tries to quantify their correlation. <sep> Cons: <sep> The paper provides an interesting observation but fails to further investigate how this observation can be used to gain better understanding of either fields of adversarial examples or knowledge transfer or how the insights can lead to better techniques. In the following, I provide a few comments and questions. <sep> On metrics tau1 and tau2: <sep> Why do we need \\tau1 and \\tau2 as proxies for adversarial transferability? It seems from experimental results that the simple metric of adversarial loss can indeed be a simpler and more robust indicator of the knowledge transferability. <sep> \\tau1 measures the correlation of the perturbations for the two models. The attack method used finds the perturbation that causes the largest change in output, which is the formulation for the misclassification attack. Different models may classify the perturbed example into different classes and so the output changes would be different. The paper says \\tau1 is not enough and we need to look into the changes made to the output as well and whether an affine layer can map one into another. I feel if paper considered targeted attack, the attack could generate highly correlated output changes and so \\tau2 wouldn't be necessary. I wondered if authors tried targeted attack formulation and how the new tau1 would correlate with adversarial transferability. <sep> I'm a bit confused about Theorem 1 and how it relates to definition of \\tau2. As far as I understand, \\tau2 captures how much outputs of two models can be aligned using an affine transformation. Theorem 1 then says large \\tau2 means an affine function g can be found that aligns the two models. Isn't it just the way that \\tau2 is defined? <sep> Analysis and results: <sep> Some parts of the paper seem a bit disconnected or not elaborated well enough. For example, what do theorems 2 and 3 tell us? Similarly for proposition 2. What are the takeaways? <sep> Do experiments confirm the theoretical bounds and derivations? It might be good to define a simple synthetic task (maybe on a logistic regression or 1-layer neural net) to evaluate the claims and show the relationship between adversarial and knowledge transferability. <sep> The results need more investigation. For example, in figure 2 (right), \\tau1 values are almost identical, which as paper mentions, shows \\tau1 is not sufficient for determining transferability. However, although \\tau2 values are correlated with transferability, they are almost zero. What does this imply? Do these results statistically support the claims? Similarly, in table 1, the relationship between \\tau1 and \\tau2 and adversarial-knowledge transfer does not seem to be statistically significant. <sep> Some statements in observations seem obvious. For example, section 5.1 reads ""we show that the closer the source data distribution is to the target data distribution, the more adversarially transferable the source model to the reference model, thus we observe that the source model is more knowledge transferable to the target dataset."" This is a known concept that has been used in both designing better attacks and also better transfer learning methods. <sep> Paper edit suggestions: <sep> Writing and paper structure can be polished. I suggest moving background info and definitions into a section background and then laying out the structure for the rest of the paper. <sep> The discussion of related work and the cited papers need a revision. Some papers are repeatedly cited while others seem to be discussed out of place. For example, page 8 refers to (Zamir et al., 2018) a total of nine times.","This paper studies the relationship between adversarial transferability and knowledge transferability. It develops two metrics to measure adversarial transferability and a theoretical framework to justify the positive correlation between adversarial transferability and knowledge transferability. Synthetic experiments show that adversarial transferability measured by the proposed metrics indicates knowledge transferability. <sep> While the paper studies an interesting and fundamental problem, with a sound theoretical analysis and a clear presentation, reviewers still have several reservations to directly accept it. <sep> Lack of interpretation. How this observation can be used to gain better understanding of either fields of adversarial examples or knowledge transfer? <sep> Lack of inspiration. How the insights can lead to better transfer techniques, apply to practical applications, and foster future research? <sep> Lack of justification. Why such definitions of metrics are the intrinsic ways of measuring adversarial transferability? How well do they correlate with the practical experience with advanced attack, defense, and transfer methods? <sep> AC believes the endeavor made by this paper towards a fundamental problem is highly necessary to our field. But given the above reservations, AC would encourage the authors to further strengthen their work to make it more inspiring and useful."
"strength | decision  ==>  ==> This paper proposes an empirical study of the nature of `""local"" and ""global"" features and how well they can be approximated via fully connected networks under a highly controlled experimental setup. <sep> Pros': <sep> The experiments are highly controlled and the research seems fairly reproducible <sep> The paper is self-contained <sep> The paper -- though containing a mathematical motivation linked to approximation theory -- is written in a very accessible way for non-expert researchers in the field (myself) <sep> The overall question and motivation of the problem the authors are studying is of great importance in the fields of approximation theory, local vs global (Theory of Deep Learning, Applied Vision Science & Representation Learning) and learning capacity via SGD as well. [I am willing to raise my score if authors convince me what I have missed, or where I was confused -- see below]. <sep> Cons': <sep> There seems to be a misnomer on the definition of what local and global means. I am confused, and these two concepts that should be quite intuitive, are actually defined to mean different things (section 2.1 -- see critical observation below). <sep> Authors present good empirical analysis but there is no potential explanation of their conclusion that suggests that global features can be approximated better with shallow networks, and local features with deeper networks. Arguments potentially linked to receptive field size and hierarchy like in vision science (Neyshabur, 2020; Deza, Liao, Banburki & Poggio 2020) would be interesting, but naturally authors cannot make such claims given that all networks trained/tested were fully connected with a R.F. size of 1. <sep> Critical Observation [Section 2.1] <sep> K-local label vs K-global label: Why does each k-local label depend on a multiplication of all k entries, while k-global depends on a sum of all k-entries? There seems to be a misnomer on what local and global means here. It is as if in both cases the label is determined by ""global"" properties (as all the inputs are used to compute the label), but the only difference is the operation type: multiplication vs sum -- but I could have missed something. This is quite confusing even though the authors state in the paper that locality need not mean spatial locality. <sep> The problem with the confusing definition of locality and global properties is that it permeates into the introduction and conclusion of the paper and other relevant work. For example the claim in the introduction: ""deeper is better for local labels, while shallow is better for global labels"" under this new definition is confusing as it suggests something that is not what it seems. Unless local here literally means spatially local or implies that a function ignores/zeroes out other inputs (as it would be for functions with restricted -- hence local -- domains to compute the output), then the claim of the paper is misleading/confusing with the relevant literature. Authors should do a better job in arguing why they choose multiplication as a proxy for a local operation and summation as a proxy for a global one. If anything it would make more sense that a k-local label is essentially a subset of the k-global label (example a partial sum, vs a different operation altogether). <sep> Again, at a higher level: I would have expected a sort of definition where there is only a single ""global"" label (vs k-global label) computed from a feature vector, and `multiple' k-local labels, depending on what entries in the feature vector are sampled, such that as k reaches the total input size k-local is equal to ""global"" -- but this does not seem to be the case. <sep> Sections 2.2 and onwards seem more clear. <sep> Other observations: I really like the dissociation for k-local (continuing with the definition proposed by authors) and k-global labels expressed in Figure 2. This is a neat result, but again I think the argument made should be pushed towards the fundamentally different nature of the operation (multiplication vs addition -- and not local vs global) <sep> The rest of the paper Figure 3 seems quite interesting to find the point in depth such that there is equalized performance for the 2-local and 2-global cases. On the other hand, I am not sure what Figure 4 is supposed to bring to the table (it seems like comparisons to performance with NTK) -- but, so what? Why does this matter? <sep> I think the paper overall proposes a good first step via empirical analysis of the local vs global problem (under the authors definitions), but I would have wished that the paper ended on a higher note with regards to, why is k-locality approximated better with deeper networks, and k-global labels with shallow ones. Not that a proof would be necessary, but at least an intuition given the mathematical structure of the network (stacked dot products + non-linearity), their approximation power, and the nature of the label (multiplication vs addition).","Reviewers found the construction is very clever and the empirical results are interesting. However, a more thorough theoretical explanation is needed for acceptance."
"weakness | rebuttal_process | weakness | misc  ==> Interesting Paper; Some Confusions <sep> This paper considers the problem of predicting the infection status of untested individuals. The authors propose a recurrent neural network-based model to impute the infection statuses over time, incorporating information on both the individuals (features of the individual) as well as a contact network that defines which individuals are related to each other. <sep> The problem the authors describe is interesting and important. It has seen substantial attention in the ML-Healthcare community in recent years. Nevertheless there are some confusing aspects to the paper that make it hard to evaluate the efficacy of the proposed method. <sep> Comments as they appear in order in the paper (comments with higher importance denoted with **): <sep> The authors' use of the variable e for exposure state is a bit confusing and inconsistent. In S3P1 they define e as a sum of contacts' exposures and so it should be an integer. Later in S4P1, they describe a failure mode in which e_a = .9 and e_b = .1, which would suggest the authors are viewing e as on a spectrum from 0 (little exposure) to 1 (a lot of exposure). It's also not clear, throughout, how exposure (and the features x) relates to infection; are there example joint distributions or conditional distributions of y given e that could describe that relationship? Is it being treated merely as another feature (normalized/standardized for the purposes of the NN model) or does it have some special status in how it relates to x^bar and y? <sep> **In S3P2, the authors define biased testing as entailing p(Xt | Ot = 1) \\neq P(Xt | Ot = 0) \\neq p(Xt). This is rather non-intuitive and it would make more sense for the variables to be flipped: there is bias in testing if the probability of being observed (i.e. tested) is different depending on the patient features. Can the authors clarify why they've chosen to define bias like this? <sep> **In S3P2 the authors state that they assume i's outcome is independent of their contacts (entirely? just the contacts' outcomes?) given xi. Can the authors clarify why this might be a reasonable assumption? The authors, for instance, don't specify any notion of time in making this assumption and so really it seems to be saying y_i^t \\indep y_j^t | x_i^1, x_i^2, ..., x_i^t. This is potentially problematic though since it would make sense for y_i^t to depend on y_j^{t-1} (my contact's true infection status yesterday affects my infection status today) and it's obvious that y_j^t and y_j^{t-1} should be dependent. The authors should consider providing a figure (e.g. non-causal DAG) that illustrates these assumptions. As is, I'm very concerned that either the assumption is not sufficient to eliminate network dependency biases, or that the assumption isn't valid in the data the authors are envisioning their method being applied to, or both. <sep> S3P2: J_i^t \\in D_0 \\cup D_1 should be \\subset (or \\subseteq) rather than \\in. <sep> S3P3: The authors define the inverse probability of being tested as p(Ot = o)/p(Ot = o | Xt). This is also a bit weird of a way to define it: the probability of being tested is the authors' numerator P(Ot = o) and the conditional probability of being tested (or propensity for being tested) is the authors' denominator p(Ot = o | Xt) so it doesn't make sense that the inverse should be this fraction rather than just 1/(the numerator). <sep> **S3P3: Using the authors' definition of the inverse probability of being tested, it's not clear how they obtain the risk in Eq. 1. Starting with the risk in the paragraph above, Rf = E[l(f(Xt), Y^{t+1})], conditioning on O = 1 corresponds to dividing by p(O = 1). To balance this, since the LHS of Eq. 1 is the same risk Rf, it makes sense that the authors multiply the loss by p(Ot = o), the numerator of w. It is not clear how dividing by the denominator of w (thus just multiplying the whole loss by w) results in the same risk. It's unclear to me whether this is a confusion on probability algebra or these some other assumption being made here (overlap should not be sufficient). <sep> S3P4: ""However, the following reweighted empirical loss is an unbiased estimator of (...)"" -- how do we know it's unbiased? Is there a proof? <sep> S4P1: What is the reason for adding 'hats' (^) to the y's in the sentence ""This becomes obvious if we breakdown \\hat{e}_i^t draws from Q as follows (...)"". Is the idea that we're iteratively plugging in the predictions for the contacts' infection status? This seems to be the first time in the paper this sort of idea is hinted at and so it's not clear what the estimate is meant to be. <sep> **S4.1P1 ""We stress that we do not require \\hat{y} to be an accuracte estimate of the true labels, but only require that there is significant separation between the imputed values (...)"" Is this formalized anywhere? Can the authors please provide equations and math to provide intuition for why this might be the case if not a formal proof? Empirical evidence isn't exactly enough since it's feasible for there to be other explanations for ""good"" performance. <sep> **S4.1P2 The same comment as above applies to the hand wavy description of the cluster assumption at the bottom of the next paragraph. ""The clusers assumtion states that (..) exposure states tend to form near discrete clusters"" -- what's meant by 'tend to'? More intution would be nice here. <sep> S5P1 ""Let A_i^t be the set of ancestors"" -- in what sense? Is there a (potentially causal) graph underlying your model? If not, what is an ancestor? <sep> S5P1 What is \\mathcal{U}^t? This doesn't appear to be defined <sep> **General: What assumptions are being made about the missingness mechanism for the true infection states. The authors should be very clear by stating something along the lines of ""we're assuming missingness at random"". That seems to be what the authors are getting at with their conditional independence assumption but i) it's not obvious that assumption is sufficient (see above) and ii) the authors haven't done a great job of linking that assumption to their argument for the efficacy (identification) of their model. <sep> S5P2/3: The authors describe the method by which they determine convergence of model fit -- I'm curious, given that they cite Chernozukov 2017, what properties their model has wrt sample efficiency and convergence to the ""truth"" (i.e. the parameters of their model converge to the true parameters of the function that maps X to Y)? In general, I'm not aware of theory that gives convergence rates for neural networks in the way Chernozukov discusses and so I'd be concerned the authors might be picking an arbitrary convergence cutoff rather than something principled. <sep> S6.1P1: ""Our goal here is to highlight how MIINT can be used to inform testing and isolation policies that lead to reduction in infection rates"" -- the authors should be very careful with using this sort of language. It implies a sort of ground truth causal efficacy to the work when this is not causal work. I agree that semi-supervised approaches are very similar to causal methods but the authors do not appear to spend sufficient time considering the assumptions necessary to ensure valid causal inferences, rather than simply fitting a black-box prediction model. <sep> Simulation study: what is the mechanism for deciding whether somewhat gets tested? It isn't obvious in the description how this selection works. Additionally, from the description, it's not exactly clear what the probability of actually being infected is, given the features (from MNIST) and the contacts (indirectly from MNIST?) <sep> Sensitivity to the potency property: ""This confirms out conjecture (...)"" The authors should be careful with such strong language based on empirical, simulated evidence. <sep> Minor: <sep> Throughout the paper there are several instances of incorrect grammar or other writing issues that make it hard to determine the meaning of the sentence in question. For instance in the first paragraph of S2, the authors say ""In the machine learning literature, previous work has relied on proxies for exposure, e.g., the prevalence of a disease in a community (citation), and implicitly assume that conditioning on individual characteristics."" The authors should carefully proofread to remove these artifacts as there are some instances where they pose a more substantive impact on the readability of the paper.","This paper studies a timely problem and consider an interesting approach, but overall there were many concerns about technical details and the validity of the framework. The positive reviewer also mentioned concerns about the experiments, which others also found to be an insular comparison with weak baselines. Following the response period, in discussion there are additional concerns arising related to the lack of details, for instance related to possible unidentifiability of the model. As one reviewer discusses, the authors are attempting to use RNNs to impute missing infection status labels when the missingness mechanism is assumed to be (i) not at random, (ii) playing out over time (as it is unclear whether Y^t is assumed (conditionally) independent of Y^t' with t' << t), and (iii) subject to interference (whether someone is tested is the 'treatment' here since it's a missingess problem and one person's propensity to be tested could causally affect another person's downstream infection status since apparently no Markov independence is assumed. There is also consensus that the writing quality can be greatly improved. Overall this work contains some ideas with potential in a thorough revision"
"abstract | strength | weakness | decision | misc  ==> Summary <sep> This work proposes a technique to compute sparse patterns in the input space that are highly predictive of a class, even when added to samples from a different class (non-target samples). <sep> Patterns for class c are produced by optimizing a point-wise weighted average of two samples (a fixed sample called the canvas and one that changes after each optimization step referred to as xs∈Xs) s.t. the result is highly predictive (yields a low error) for c. Learned weights for the average (the mask) are thresholded and used as a binary mask for the canvas (which is either a sample that has been labeled/predicted as class c or a white image). <sep> Analysis of the resulting patterns focuses on explainability on various fronts: first, the consistency of the pattern is evaluated (via a custom metric called 'predictive power') which shows that resulting patterns do cause a CNN to predict the class that has been optimized for. <sep> A few remarks on the patterns are made and also some claims regarding the link to the model's learned features (like position, structure, and number of shapes). <sep> Attention maps confirm that the saliency of the patterns prevails over the original content of the canvas image. Finally, models trained adversarially and with backdoors are analyzed by computing the sparse-mask patterns. <sep> Strong and weak points of the paper: <sep> Strengths: <sep> The paper is easy to read and the main ideas can be followed. A wide array of experiments are conducted with a small and (at least a subset of) a large dataset. <sep> The proposed method is straightforward and described in enough detail to be easily reproducible. <sep> Comprehensive and relevant related work has been cited and compared. <sep> Section 4.2: an interesting baseline showing that the resulting patterns are more effective at causing the model to predict a target class than highly responsive areas of a ""natural"" sample. <sep> Section 4.3 and 4.4: the inclusion of an analysis of training regimes that are known to affect the feature space learned by a neural network (adversarially trained, trained with robust features, and trained with backdoors) provides relevant use-cases for a method that is proposed as an interpretability tool. <sep> Section 4.4: interesting results that seem to agree with Ilyas et al. 2019 in that more robust models require a perturbation that looks more like the adversarial target. <sep> Weaknesses: <sep> The main concern is that the goal of the paper is not aligned with the findings: I consider that the patterns shown are a valuable tool to analyze the behavior of a neural network, but from a completely different perspective. These patterns (and the algorithm to obtain them) suggest that there are global-targeted adversarial perturbations (a close analogous to the work of Moosavi et al.: global perturbation because it is one perturbation applied to a set of samples; targeted because each perturbation has been designed to cause attacked samples to predict one specific class). In turn, the patterns' appearance is as difficult to interpret as those of classical adversarial perturbations or global perturbations. Claims regarding shape, texture, position, etc, are not well supported by measurable evidence (beyond a few observational remarks supported by a handful of examples), and are more likely the result of the method by which these perturbations are obtained (e.g, the regularization w.r.t. the mask, clipping, binarization, point-wise multiplication). Instead of trying to interpret the appearance of such perturbations, a focus on the implications of global-targeted adversarial perturbations can provide valuable insights into the manifold's structure learned by modern neural networks as well as their shortcomings. <sep> Most experiments are missing key details regarding their setup. In particular, the canvas image is often not shown (especially relevant for Figures 1, 2, and 7). From figures where it is shown (like Figure 4), it seems like regardless of the canvas, a binary pattern with a global-targeted perturbation is learned. <sep> An analysis of the impact of the canvas is missing w.r.t. predictive power which could support/invalidate the idea of the learned pattern to be a global-targeted adversarial perturbation. <sep> Section 4.1, Patterns Revealed by Positive Canvas: several claims regarding the properties of the patterns (e.g., structural consistency, number, and position of shapes) in the mask are only supported by a qualitative evaluation of a few samples. No metrics are proposed. <sep> Section 4.1, Patterns Revealed by Positive Canvas: although a metric (predictive power) is used to measure the consistency of the pattern w.r.t. a target class, results are based on a few classes. Why not report it for all 10 classes on CIFAR10? Also, results on 5 classes of Imagenet is not representative of the variety found in that dataset. A simple average of the predictive power over each class would provide stronger support for the results found in the subset of selected classes. <sep> Section 4.1, Patterns Revealed by Different Canvases: the absence of texture in the patterns is more likely caused by the sparsity term on equation 2 (together with the threshold and the point-wise multiplication with the canvas) rather than by an intrinsic property learned by the model. <sep> Recommendation: <sep> In its current form and with the proposed focus, I consider that this paper cannot be accepted. The focus of the analysis can yield stronger, more conclusive results from the standpoint of global adversarial perturbations. <sep> Supporting arguments for recommendation: <sep> Without the right context (i.e., not that of explainability but adversarial attacks) the analysis of what the patterns represent is poorly supported by experiments. There is a lot of work done in the direction of adversarial attacks that findings in this work could re-purpose. Meanwhile, the impact of proposing a ""global-targeted adversarial attack"" based on one sample seems promising, but it is out of the scope of this work's analysis. Refer to the list of weaknesses for more details. <sep> Questions to clarify: <sep> How are the class-patterns related to universal perturbations? <sep> How is this method fundamentally different (i.e., more advantageous) than a universal adversarial perturbation (Moosavi-Dezfooli 2017) generated for each class? <sep> If the learned pattern says something about the (clean) feature space of the classifier, what do these patterns reveal about classes with a large intra-class variation? For example, the class ""clock"" contains digital and analog clock faces (on the extreme, digital clocks show only numbers, and analog ones only the hands of the clock). Apart from their semantic relationship, there is no visual correspondence between the two sub-categories. Would this scenario be a limitation of the proposed method or what does it reveal about the model/the data? <sep> Section 3, Equation 2: what happens to the mask (therefore to the conclusions from the patterns in the mask) with other regularization metrics like Total Variation (TV)? TV is used in [2] to find masks with connected areas that are representative for each sample in a counterfactual manner (i.e., blurring the area of the mask, causes the model to predict something else). In fact, not using TV makes the mask converge to an adversarial perturbation. Comparing those findings to those on this work (in particular Equation 2), makes me think that the resulting mask is more of a targeted adversarial attack rather than a representative pattern of the class. Moreover similar work published in *CONF* 2016 [3] shows how almost any source image can be 'anchored' to the prediction of a guide image (equivalent to the canvas image in this work). This paper is essentially computing a sparse version of that attack via a masked input and the output layer (in [3] they don't apply masks and the deep representation was not constrained to the output but an arbitrary layer). It would be interesting to show the reconstruction of samples that have been ""attacked"" with the patterns in this work (following [3]). <sep> Section 3, sampling canvas and results in Figure 4: how is the predicted power affected by the choice of canvas (positive, negative, or white)? Evaluating on a few classes can be misleading and although the metric of predictive power (PW) seems adequate, I suggest presenting a global metric by taking the mean PW (mPW) to better support the generality of the corresponding results. <sep> Experiments with Imagenet: can you specify which classes the chosen IDs map to? Are they all related somehow? What were the criteria to select those 5? Why only 5? <sep> Figure 2: it would be important to see what the canvas image looks like in order to establish how much of the pattern is just coming from that canvas. <sep> Section 4.1, Patterns revealed by Positive Canvas: how are the number of shapes measured? Where does one shape start and one ends? It seems this observation is rather empirical and only a few samples are shown as evidence (one may ask how often does this happen and whether it happens because the dataset also has multiple shapes -which requires no mask, just simply looking at the samples). The same issues arise with claims about the position where the patterns occur (how is this measured?). <sep> Section 4.1, Patterns Revealed by Different Canvases: how is the consistency of patterns evaluated? Is there further evidence beyond the 4 samples shown in Figure 4 for CIFAR10? Those seem to share structural similarity but, is this always the case? How do you establish structural similarity on the samples from Imagenet? <sep> Section 4.2: taking one test image per class (source) whose highest activation area (with GradCAM) is pasted into ...how many target images? In other words, are the results in figure 5 (right) based on how many samples overall? This experiment needs to be described in more detail to be reproducible. <sep> Claims for backdoor images about the trigger being recoverable is rather weak for two reasons: (1) there is only one class being evaluated and more importantly (2) there is only one (simple) pattern that has been analyzed. From this experiment, there are some indications that backdoor patterns may be reliably retrieved but nothing conclusive (the full trigger pattern has not been reconstructed and other, more complex patterns have not been evaluated). <sep> Additional feedback (not necessarily part of the decision assessment): <sep> The notion of 'backdoor' is not explained in the abstract and it is not clear from the context what it could be. Including a brief description about it on the abstract could improve readability. <sep> In the second paragraph of Section 1: references for ""backdoor"" attacks and ""adversarial"" attacks are swapped. <sep> Section 1, 4th paragraph: the notion of ""canvas image"" is used without it being defined. <sep> Section 2, ""General Understandings of DNNs"": there is work that has also focused on modeling properties of the input space over the dataset (and not only sample-wise) [1]. <sep> Section 3 paragraph ""Canvas Sampling"": typo for ""reveal"". <sep> References: <sep> [1] Palacio, Sebastian, et al. ""What do deep networks like to see?."" Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2018. <sep> [2] Fong, Ruth C., and Andrea Vedaldi. ""Interpretable explanations of black boxes by meaningful perturbation."" Proceedings of the IEEE International Conference on Computer Vision. 2017. <sep> [3] Sabour, Sara, et al. ""Adversarial manipulation of deep representations."" *CONF* (2016).","The authors propose an algorithm that learns sparse patterns of images that are highly predictive of a target class, even if added to a non-target class. The reviewers agree that the algorithm is novel, is tested on a wide array of experiments, and the paper well written. <sep> Unfortunately, it seems that some of the main claims, such as DNNs trained on clean data ""learn abstract shapes along with some texture"", resort to qualitative evaluation of the few examples shown in the paper. Furthermore, two reviewers were concerned with how one particular design choice in the algorithm might bias the authors' claims. In particular, pointed out that the patterns learned are highly to the initial canvas used, which is not necessarily strongly motivated. <sep> As these two issues are integral parts of the paper, I hesitate to recommend Acceptance at this point. That said, the approach looks very promising and I hope the authors continue to pursue this idea."
"rebuttal_process  ==>  ==> The paper aims at providing a mathematical structure for explaining the mechanism behind the attention block characteristic to transformers. The focus of the paper is on the scaled dot-product attention, reviewed in Eq. (1). In my understanding, the whole mechanism can be seen as an instance of the set kernel. The inputs are bags of items, where an item is denoted with sj. The items are embedded into some feature space via matrix multiplication WVsj. The set kernel representation of a bag is obtained by weighted averaging of item embeddings, where the item-specific weight is the output of an exponential family model. The latter model is obtained by combining embeddings of items and corresponding context vectors, denoted with ti (see Eq. 1 for more details). <sep> The paper in itself does not explain this mechanism as a whole but focuses on the un-normalized exponential family model that provides importance weights in the set kernel embedding. In particular, the main focus of the work is to find a bilinear form and, thus, a mathematical structure that could give rise to non-symmetric similarity function defining the un-normalized importance weights in the set kernel described above. More formally, the work seeks a kernel function k(t,s)=exp⁡((WQt)⊤(WKs)d) , <sep> where d is the rank of W-matrices. <sep> This is achieved by introducing the so called reproducing kernel Banach space (Sections 3 and 4). Following this, the paper introduces a form of regularized risk minimization problem in reproducing kernel Banach spaces. I fail to see a direct link to transformers and backpropagation used in training of such models. The conclusion is that transformers learn a kernel or similarity function that can be assigned to a reproducing kernel Banach space. I disagree with this because the whole attention mechanism is a set kernel, with the supplied bilinear form amounting to importance weights only. The section 5 concludes with a representer theorem and regularized risk minimization problem in reproducing kernel Banach spaces. Again, this is a completely disconnected part of the paper from the introduction and provided motivation. <sep> clarity <sep> I find the paper clear in most parts and have not had problems following the main arguments. Related work on learning with similarity measures, indefinite symmetric kernels, and general similarity functions seems to be well covered. <sep> quality <sep> I think this paper should really be focusing on learning in reproducing kernel Banach spaces with non-symmetric similarity measures. It is difficult to tell how much novelty it brings compared to relevant related work and how useful it would be in practice. If the authors decide to take this direction, then there should be a detailed experiments section where trade-offs between effectiveness and computational complexity are carefully studied. <sep> It is unclear to me why this work would be associated with attention models and transformers. The story just does not hold and it does not explain the scaled dot-product attention. At the moment, it just seems as an unfinished work that is unnecessarily associated with attention and transformers.","Reviewers have different views on the paper and after going through the reviews, replies and the papers, we believe that <sep> there is room for improvement here. <sep> While the part related to indefinite symmetric kernels, and general similarity functions seems to be well covered, as <sep> well as the part on Transformers, the relation with learning in RKBS and Transformer is far from being clear and Reviewer 4 makes a strong point on this. For instance, <sep> what is the goal of the section 5 and Definition 1 . Indeed it is not clear here if the point of the authors is to learn the kernel parameters in equation 9 or to learn to predict the output of a transformer. If it is the latter, the connection with the first part is unclear. <sep> In Equation 11, I can understand that x and y are the sequences t and s but what is z_ij and how it is obtained? So again, the learning problem drops in without justification and it is not explained how it can be solved. The theoretical results involving the representer theorem is nice though. <sep> The experiment does not seem very related to the learning problem in Equation 11 introduced by the authors.it seems to me that they are just trying different kernels on top of the dot product."
"abstract | weakness  ==> Summary <sep> This paper proposed to explore a given disentangled latent code and explore it through random interpolations. The idea is to ensure these interpolated codes rely inside data convex hull by enforcing latent code reconstruction by first decoding and then reencoding this explored codes. The authors proposed a way to manipulate the final latent space by using Unit Direction Vector (UDV) to generate unseen attribute values. The results are illustrated on 2 datasets (Fonts and RaFD) both qualitatively and quantitatively. <sep> Reason for score <sep> Overall I vote for reject, this paper has limited contribution and lack of consistency (disconnected ideas: DEAE and UDV), clarity (respective contributions of GSL-AE and DEAE) and justification (spurious and not convincing experimentations). <sep> Pros <sep> The general topic of having more controlable synthesis by having more disentangled representation if of clear interest for the field. <sep> The idea of reconstructing random interpolations of valid latent codes by decoding/reencoding is interesting. I don't think this is novel, but not widspread enough in the field so far, so the interest remains. <sep> Cons <sep> While the topic is of interest for the field, the interest of the proposed approach is clearly limited since it relies on a preprocessing step (GSL-AE) of which it is already the role. A clearer distinction of the contributions of each step would help to illustrate the additional interest of DEAE. Unfortunatly the paper relies too much on GSL-AE which limits the potential scope of their main contribution which could be wider. <sep> The title is misleading since it presents the approach to proposed a ""disentangled exploration"" while it is essentially provided by GSL-AE used as a preprocessing step. Thus, it would also be good to mention that the approach is supervised (like GSL-AE). <sep> The paper lacks of clarity: UDV or ""dataset bias elimination"" seems to be  spurious addition and are not really well connected in the paper. It's hard to understand how it contributes to illustrate the interest of DEAE. <sep> Results are not convincing. <sep> MSE loss between latent spaces from different methods cannot be compared at all. We can imagine spurious differences only due to scale differences for instance. <sep> Fig 2. is mentioned p.5 to illustrate the qualitative superiority of DEAE over GSL-AE: not really obvious of this state. <sep> Section 3.3 + fig 7.: interpolation is not smooth which contradicts the quality of the representation and the interest of UDV. <sep> Section 3.4: the idea of evaluating the performance of generative methods through their capacity of improving a classification task by augmenting the initial dataset, is very indirect and not really convincing. <sep> Gradcam results (Fig. 8) does not illustrate any improvement in favor of background information for the ""unbiased model"" <sep> ""Dataset bias elimination"" <sep> This section seems spurious. Nothing in the proposed approach is really specific to such an application appart the global context to which it is attached. I don't see what this section brings to the paper. <sep> The way the unbias model is obtained is not realistic since you need to know what information has to be dropped, whereas in real applications you generally don't know the source of bias. <sep> While having thinner way of exploring the latent space is interesting, the proposed UDV approach sounds spurious and ad hoc and does not seem to be connected to DEAE. <sep> Questions <sep> What is the context of application of your approaches since it relies on a supervised approach to create the initial disentangled embedding ? Coutrolable synthesis is generally understood in an unsupervised setting. <sep> Is there a link between UDV exploration and the way DEAE is learnt ? <sep> Why using ""random interpolation"" for inference and illustrating performance (in Fig. 5) ? It would be clearer with standard interpolation between left and right bounds. I also would expect that interpolations would reach (or at least get closer to) the targeted right bound. <sep> How do you ensure that MSE (from section 3.2) can be compared between totally different latent spaces ? <sep> Section 3.4: Why D_S is used rather than D_L for further analyses ? (to get augmented datasets) <sep> Minor comments <sep> Intro: Flows could be mentionned on top of VAE and GANs <sep> Intro on GAN & VAE limits are too much like a caricature regarding recent results. VAE can be used to generate HD images now, and GAN are much easier to train these days with GP strategy or different learning rates between generator and discriminator for instance. Approaches like InfoGAN could be mentioned to illustrate standard strategies to control GANs. <sep> Fig 2. Not clear which kind of interpolation has been used for GSL-AE and DEAE since neither font, font size, font color, letter nor background color are interpolated in the figures. This figure would be more powerful to stick to one (or a few) letter(s) and change only the considered attribute (background or font color). Here the comparison and the illustration of the effect is not easy. <sep> Section 2. ""whic h maps""  ""which maps"" <sep> notation "" has not been introduced (like in f_theta^ and g_phi^*) <sep> L_reg has not been introduced or referred in the paper. <sep> Fig 7. Choosing a fg color different from the targeted bg color would be better","Overall, the paper makes some interesting and intuitive observations regarding the autoencoders with a cycle consistency, and aims at achieving controllable synthesis via a disentangled representation. However, the overall consensus was that the manuscript needs further iterations: <sep> In particular: <sep> The ideas should be made more precise using mathematical arguments, as it stands some ideas are (e.g. DEAE and UDV) disconnected. <sep> The scope needs to be clarified, e.g. respective contributions of GSL-AE and DEAE, use of label information <sep> More numerical/quantitative evaluations, the current experimentation is not convincing enough, needed for better justification (spurious and not convincing experimentations) <sep> The English of the manuscript could be improved as it occasionally hampers the flow."
"abstract | rating_summary | rebuttal_process | weakness | decision  ==> Summary <sep> The paper considers the task of coarsely-disentangling a data-generating process into independent and shared modules. In more detail, it is assumed that each observation in a given dataset was generated by exactly one out of a set of independent generative processes referred to as independent causal mechanisms (ICM) in combination with a global mechanism that is shared across modules. The paper proposed the use of a mixture prior to model separate ICMs within a single architecture/generative model, chosen to be a GAN (Wasserstein-GAN with gradient penalty). Learning is performed by combining the GAN loss with self-supervision by using a separate encoder and adding loss terms resemblant of cycle-consistency-losses. The paper claims to prove a notion of identifiability of the coarse-grained modules in the sense of separation into disjoint manifolds and conducts some experiments on (variations of) MNIST and Fashion-MNIST, comparing their method against vanilla and disentangled VAE variants on some downstream tasks. <sep> Pros the paper tackles a very interesting and highly-relevant topic (disentanglement with non-factorising latent space and connections to causality) <sep> the coarse-grained view and the separation into independent and shared mechanisms seems like a useful abstraction and (to the best of my knowledge) has not been investigated before in this form the authors support their proposed method with some theoretical analysis the latent traversal in Figure 2 and some of the quantitative results look promising <sep> Cons even though I am very familiar with the topic of the submission, I found the paper very hard to follow many of the statements relating to causality and ICMs are inaccurate, misleading, or wrong (see detailed comments below) <sep> the notion of identifiability used in the paper is very unconventional and not consistent with prior work the assumed generative model is never fully specified (in particular, the mapping from latents to observations and the role of the mechanisms is not described in the text or given as formula) and the mixture prior is inconsistent with the graphical model given in Fig.1 <sep> some of the cited works are misrepresented or wrongly protrayed (see detailed comments) <sep> Evaluation <sep> I think the paper tackles a very important problem and presents some interesting ideas. However, the paper contains too many errors, unclear parts, and inconsistencies in the current form which makes it very difficult to gain a clear picture of the proposed method and to assess its correctness. Significant improvements in terms of clarity and exposition are needed before publication and I therefore recommend rejection at the current stage. <sep> Detailed Comments and Questions <sep> The ICM principle states that ""the data generating process is composed of independent and autonomous modules that do not inform of influence each other"" (Peters et al., 2017). I believe you should cite this. Note that this is different from the notion of isolated, as different ICMs may feed into each other. In fact this is how ICMs are conventionally understood in causality: each conditional distribution of a variable given its causal parents P(Xi|PAXi) is interpreted as an ICM, and together they form the data generating process for the joint P(X1,...,Xn)=∏i=1nP(Xi|PAXi). ICMs are thus traditionally understood as independent modules that can be (re-)combined. <sep> I do not understand why the term ""ICM-conditioned mechanism"" is used: what is an ""independent causal mechanism-conditioned mechanism""? <sep> Related Work: you mention three aspects but only two are given. <sep> Functional Causal Models: there are several key differences between FCMs and Bayesian networks (BN), so I think the presentation here is misguided. Firstly, BNs have no causal connotation to them but are only a way to represent a particular factorisation of a probability distribution. Causal graphical models or causal BNs on the other hand are endowed with a notion of intervention and are thus closer to FCMs. Perhaps this is what you intended to say? Moreover, it is unusual to refer to P(X|PAX) as a posterior---this is precisely what an ICM refers to and other common names include (causal) conditional or Markov kernel. <sep> the entire discussion of causality and FCMs misses the notion of intervention and counterfactual which is crucial for defining causal concepts <sep> The description of FCMs is incorrect: The deterministic relationships (structural equations) in SCMs are a set of assignments of the form Xi:=fi(PAXi,Ui) where Ui are the unobserved variables and fi are indeed deterministic. <sep> I have never heard or read about this and would like to ask about the source of this characterisation: ""If each function in FCM represents an autonomous mechanism, such FCM is called a structural model. Moreover, if each mechanism only determines the value of one and only one variable, such a structural model is called a structural causal model (SCM)."" ? This seems strange to me. <sep> (Cai et al., 2019; Monti et al., 2020) are cited incorrectly: actually these aim to discover the graph which is different from learning an SCM as the former supports interventional reasoning, while the latter supports counterfactuals. <sep> Figure 1: the rectangles M_i are not described in the caption. <sep> ICA: the description of recent advances in ICA is incorrect: it refers to ""requiring a relatively large number of independent components"" when this should read ""... large number of environments"" or more generally values of the auxiliary variable which renders sources conditionally independent the specification of the mixture prior at the end of page 3 seems incorrect as it does not integrate to 1. Perhaps you meant to include mixture weights? However, this is also inconsistent with Fig.1: (c) would suggest a prior which factorises as p(z)=p(zC)p(zS)∏i=0Np(zMi|zC), while (d) suggests p(z)=p(zS)∏i=0Np(zMi) both of which are different from the text. As I infer it from reading between the lines, zC is a categorical variable that switches between different mechanisms, and zM, zS are both inputs to the generative process. Why the need for the many zM's? Would a simple mixture distribution on zM with weights and parameters depending on zC not do the job? If not, can you explain why? <sep> How is the isolation constraint implemented, i.e., how does it translate mathematically? Can you please specify the full generative process p(z,x)? <sep> the last paragraph in 3.2 is very hard to follow <sep> 3.3 suddenly mentions the encoder, but no encoder has been introduced up to this point can you justify the loss function in more detail? <sep> Wasserstain --> Wasserstein <sep> The stated version of identifiability does not seem to make sense to me. It is almost always possible to learn the ground truth model, the question of identifiability is whether we are guaranteed to learn the right model. The cited work of Khemakem et al (2020) provides a nice and intuitive definition of identifiability in terms of equivalence classes in parameter space that seems more principled than the notion used here. <sep> Thm.3 This is the first time M_k is mentioned <sep> I am not sure what the point of the paragraph at the end of page 5 is? This does not really provide any intuition on the Theorem or its proof. Moreover, you assume h:Z→Z and then have h(z)∉Z which seems to contradict each other. Please clarify. <sep> the paper refers to ""co-variant shift"" throughout; to the best of my knowledge ""covariate shift"" is the accepted terminology. is the different notation intentional, and if so can you clarify the difference? <sep> some experiments are not described in sufficient detail (e.g., what is being predicted in 5.2? why are there no error bars in Tables 1 or 3) <sep> the paper would benefit considerably from some professional proofreading and grammar checking (though this is not a key factor in my evaluation) <sep> POST-REBUTTAL UPDATE: <sep> I thank the authors for the detailed response. Based on the proposed changes I will slightly increase my score, but I still believe the paper needs additional work before meriting publication.","This paper proposes to learn representations in an unsupervised manner using a generative model in which observations are generated by combining independent causal mechanisms (ICMs), in combination with a global mechanism. The authors introduce an unconventional mixture prior for the shared and independent components of the representation and train an encoder, discriminator and generator using a Wasserstein GAN with additional terms that enforce consistency in the data and latent space. Experiments consider variations of MNIST and Fashion-MNIST and perform comparisons against a standard VAE, a β-VAE, and the Ada-GVAE. <sep> Reviewers are broadly in agreement that this submission is not ready for publication in its current form. R4 in particular has left very detailed comments regarding clarity. The authors were able to in part address these comments, and R4 raised their score in response. That said, from a read of the manuscript in its latest form, the metareviewer (who is very familiar with literature on disentangled representations) is inclined to agree with the reviewers that this is work that has value, but is very difficult to follow in its current form. The metareviewer would like to suggest that the authors regroup, think carefully about how to improve clarity (in addition to addressing concrete points raised in reviews) and resubmit to a different venue."
"abstract | strength | weakness  ==> In this paper, a multi-resolution convolutional architecture is proposed to learn from concentric feature maps. Different from single sphere representation, both graph convolutions and radial convolutions are employed to extract the intra-sphere and inter-sphere information. Benefit form the radial discretization, the proposed CSGNN achieves state-of-the-art results in testing on arbitrarily rotated data under ModelNet40 dataset. However, there are several drawbacks in the  draft, such as ambiguous figure and  insufficient ablation study. <sep> Summary <sep> In CSGNN,  a  two-phase convolutional scheme for learning over a concentric spheres representation, by alternating between inter-sphere and intra-sphere convolutional blocks. Specifically, the graph convolutional network is applied to incorporate intra-sphere information, and 1D convolution to incorporate radial information. Different from previous works,  a multiple sphere representations are integrated in CSGNN, which is robust to rotation operations. In general, the proposed method is innovative on the basis of spherical discretization, but still has some limitations. <sep> Strengths <sep> 1. A multi-sphere icosahedral discretization for representation of 3D data is proposed, which can enhance  representation ability and keep more details over single-sphere representations. <sep> 2. Both graph convolutional networks and 1D radial convolutions are employed to capture the intra-sphere and inter-sphere information of 3D objects. <sep> 3. The proposed CSGNN achieves state-of-the-art results in testing on arbitrarily rotated data. <sep> Weaknesses <sep> 1. The complexity analysis is insufficient. In the draft, the author only provide the rough overall complexity. A better way is to show the comparison between the proposed method and some other methods, including the number of model parameter and network forwarding time. <sep> 2. In the converting of point cloud to concentric spherical signal,  the Gaussian radial basis function is adopted to   summarize the contribution of points. Is there any other function that can accomplish this job? The reviewer would like to the discussion about this. <sep> 3. The Figure 2 is a little ambiguous, where some symbols are not explained clearly. And the reviewer is curious about that  whether there is information redundancy and interference in the multi-sphere icosahedral discretization process. <sep> 4. There are some typos in the draft. The first is the wrong use of ""intra-sphere"" and ""inter-sphere"". The second is the use of two consecutive ""stacking"" in the Spherical Discretization subsection. Please check the full text carefully. <sep> 5. The center choice of the concentric spheres should be discussed both theoretically and experimentally. In the opinion, the center of spheres play a important role in the representation capturing of 3D point clouds in a sphere convolution manner.","The paper proposes to effectively learn representation of 3D data (point clouds/meshes) using a spherical GNN architecture over concentric spherical maps. A method for converting point clouds to concentric spherical images is also proposed. Evaluation is done via 3D classification tasks on rotated data. <sep> Strengths: <sep> Interesting novel method for learning 3D representations <sep> Technically sound <sep> Performs similarly to spherical CNNs and other STOA on the Modelnet40 dataset <sep> Weaknesses: <sep> Presentation of the work needs to be further improved such that it is easier for others to reproduce <sep> More in-depth experiments are needed to justify how much Spherical GNN improves over other STOA, particular given how classification accuracy is very similar to STOA."
"abstract | strength | weakness | decision | misc  ==> This work studies the finite-time convergence for neural networks. In particular, it tries to recast the problem of training neural networks as a control problem. Supervised learning is then reformulated as a non-linear control problem with a Lyapunov based loss. The weight update is then transformed to be the control inputs. Finally, convergence results are obtained with standard theory from non-linear systems. <sep> Overall, connecting neural networks with classical control theory is an interesting direction. However, results presented in this paper seems limited, and it is not clear what contributions the current work really bring to the community. <sep> (1) there does not seem to be enough innovation in this paper. To me, the result simply follows from the classical control theory. The authors simply try to mimic the theory by having a candidate Lyapunov loss and continuous weight update equations. It is not clear why these are used for neural networks at the first place; rather, it seems that these are only applied for the sake of proving some technical results. For example, does the candidate Lyapunov loss actually generalize better (theoretically or empirically)? what's the property of it? Howe does it compare to traditional loss function? It is not convincing for me why someone should use it for training neural networks. It seems to be an artifact used solely for the theorem. <sep> (2) relatedly, the experiments focus on plotting the training loss of Lyapunov loss and l1/l2 loss. From my perspective, this is not informative. The loss function is likely not on the same scale; it is probably better to plot a ""normalized"" version so that the comparison indeed makes sense. Again, such a comparison does not reveal any interesting property about the new loss, e.g.., generalization/testing error? <sep> (3) Please clarity to what extent, the results hold with respect to different activation function. In Section 2.1, it is explicitly mentioned that sigmoid activation is used. In Section 2.2, the authors use the same notation \\sigma. Does the result hold for other common activation functions? If not, any comments on the difficulties?","This paper aims to study the convergence of deep neural networks training via a control theoretic analysis. This is a very interesting approach to establish theoretical understanding of deep learning. However, there are several concerns raised by the reviewers: <sep> The contribution of this paper is limited. The results simply follow from standard optimal control. It is not clear what new insight the paper provides. <sep> There are already quite a few works on control theoretic analysis of deep learning. This paper did not do a good job on presenting its novelty and difference with existing works. <sep> The experimental part is weak. It only involves small data set and very simple networks. <sep> Based on these, I am not able to recommend acceptance for the current manuscript. But the authors are encouraged to continue this research."
"abstract | strength | weakness | rebuttal_process | decision | suggestion  ==> This paper compares different routing strategies in Mixture-of-Experts for multilingual NMT, and proposes to route by tasks instead of token, which can achieve better or comparable translation accuracy measured by BLEU and also enable separation of network structures at decoding time with affordable serving costs. The paper claims that with task-level routing, the server only needs to pre-load K experts (assuming top-K routing for MoE layers) during inference, instead of loading all experts as in token/sentence level routing. <sep> Overall, I appreciate the analyses and comprehensive experiment studies in this paper, and also the large-scale in-house datasets used in experiments. The experiment findings about the different routing strategies in the encoder and decoder in en-xx and xx-en settings are also interesting. The connections between the gating distribution and the similar of languages in Figure 3 also make sense. <sep> However, I doubt the novelty and machine learning contribution of this paper: 1) The different routing strategies are natural and seems to have already been proposed by previous works. e.g., for task-level routing, [1] used similar kind of mixture of experts in the language level. 2) This paper simply studies different routing strategies, which is more like empirical analyses. Although the results are somewhat interesting, they are not surprising and most findings are in expectation. <sep> [1] Universal Neural Machine Translation for Extremely Low Resource Languages. https://arxiv.org/pdf/1802.05368.pdf <sep> Meanwhile, I have some questions on the experiment settings: when comparing with the single multilingual base model in Table 1 and Figure 2, the parameters of the MoE model are larger than the single multilingual model (e.g., 533M vs 142M in Table 1). Therefore, it is obvious that MoE model achieves better accuracy than smaller single multilingual model. The MoE models should compare with a single multilingual model with the same amount of parameters. <sep> Another important thing I need to point out is that this paper seems to violate the anonymous policy of *CONF* 2021. In Section 4.3.1, the paper says ""We use an in-house training corpus (Arivazhagan et al., 2019)"".  However, Arivazhagan et al., 2019 shows the authors from Google, which reveals the organization of authors in this paper.  Meta reviewer can further double confirm if this violates the policy. <sep> Besides, this paper is not carefully written and there are many typos which affect the reading. e.g., 1) Two ""wo_e"" in the line below the equation in Section 2; 2) ""to route the token to a select few experts"", there is an additional ""a""; 3) ""a learning rate of a learning rate of 3.0"" in Section 4.1.","This paper proposes routing strategies for multilingual NMT. The motivation is to train a single mixture model that can serve the training and prediction of multiple models. Several strategies are proposed: token-level, sentence-level and task-level. This is a simple and straightforward approach (which is fine). The main concerns from the reviewers regard novelty and missing comparisons. In their updated draft, the authors added comparisons to bilingual models and they added a discussion wrt related work. However, the author's response did not address enough some of other reviewers' concerns regarding comparison with other approaches, and the lack of novelty persists (mixture models for multi-task learning have been previously proposed in the literature), which makes me lean towards rejection. I suggest the authors address these aspects in future iterations of their work."
"abstract | strength | weakness | rebuttal_process | weakness | rebuttal_process | strength | weakness | decision | rebuttal_process | weakness | decision  ==> Summary: The paper discusses three mutual information (MI) objectives for representation learning in RL, referred to as forward, state, and inverse. The forward MI objective models latent dependencies given the action. The state MI objective models latent dependencies alone. And the inverse MI objective models dependencies between actions and future states (empowerment). The paper shows that of these three common objectives, only the forward objective is sufficient for learning the optimal policy / value function. This is demonstrated using simple examples and experiments on a simple game environment. <sep> Strong Points: This paper attempts to provide an overarching view of the many previous works that have used MI objectives for representation learning in RL. Multiple approaches are compared under one generic formulation. This can be helpful in connecting disparate research areas, e.g., connecting contrastive representation learning with empowerment and other task-agnostic policy learning objectives. Papers that present more general interpretations, such as this one, help to build consensus in the research community and standardize techniques, which can be more useful that proposing modifications of existing techniques. <sep> Along similar lines, to the best of my knowledge, this paper presents a somewhat novel idea: analyzing MI objectives in terms of whether they are sufficient for (optimally) performing downstream tasks. Although representation learning and policy/value learning are not always performed separately, many works do not theoretically interrogate whether their representation learning objective is sufficient to perform the task. Having a clearer theoretical grounding can be helpful for deciding which representation learning objectives are worth pursuing. <sep> Overall, the paper is well written. Key mathematical concepts are defined clearly in sections 3 and 4. The descriptions and definitions are clear and concise. <sep> Although the experiments are rather limited, they do help to isolate key aspects of the analysis. In particular, I found the regression of components of the environment (fruit error and gripper error) to be compelling and helpful. <sep> Weak Points: I found the technical formulation somewhat unclear. The paper presents representations as stochastic mappings from states (s) to latent variables (z). Sufficiency is defined as having the same optimal policies / value functions when the representations are the same. However, it is unclear, in practice, how such representations are intended to be used for value/policy learning. From the proof sketch in proposition 1, it seems as though the value is estimated by integrating over Z, however, this may not be feasible in practice, and we may need to use samples. Regarding this point, it is also unclear what is performed in the actual experiments of the paper. <sep> It's unclear whether analyzing the sufficiency of representations for downstream tasks is impactful for future work. Sufficiency (see definitions in section 3) describes whether the learned representation removes any task-relevant information. However, there are infinitely many representations that are sufficient. While sufficiency is certainly important for learning representations, it is only half of the consideration. In practice, one would want a minimal sufficient representation that is amenable for learning. Likewise, there are cases where representation learning and task-based learning occur together, in which case, task-based learning may be able to overcome the insufficiency of representation learning. <sep> The paper is almost entirely lacking in experiment details. The descriptions for the examples in figures 2 and 3 could be improved in clarity, along with details on the estimation of each of the MI terms. Likewise, beyond basic descriptions of the tasks in the experiments section, very few details are present. While I understand that the focus of this paper is more theoretical in nature, such details are essential for reproducible results and ensuring technical rigor. <sep> This paper may not be entirely representative of previous work. Mutual information objectives of differing types are applied in various contexts, in many cases trained during data collection. This paper explores a limited setting, in which these objectives are trained on data collected from a uniform policy and (I assume) the policy / value function is estimated only from the representation. While the authors claim that this enables a fair comparison between the objectives, it also somewhat limits the scope/impact of the paper, as it is less realistic. Further, experiments are performed on fairly limited, fully-observed environments. While I understand that these simple environments are meant to capture the essential differences in these objectives, focusing only on these relatively ""toy"" environments could limit the impact of this paper. For instance, in the two experiments in the experiments section, representation learning provides fairly marginal improvements in performance over just directly training end-to-end. Even just sticking with these environments, the paper could be improved with further analyses on the types of representations that are learned in each case. <sep> Accept / Reject: Given the relative lack of details and the limited experimental investigation, I would lean slightly toward rejection. While I agree with the direction of this paper, I feel that these aspects would need to be improved for the paper to have substantial impact on the rest of the research community. With a more in-depth discussion of the experiment details and perhaps further analysis of what is and is not captured by each objective, this paper could reach the bar for acceptance. Experiments and analysis with previously published approaches would help to further improve the paper. <sep> Questions: <sep> Could you please provide more details on the training scheme in the experiments section. How is the representation used in practice for downstream tasks? <sep> Where do other unsupervised representation learning schemes (e.g. VAEs and normalizing flows) fit within this framework? <sep> For J_state, it is stated that previous values of Z can be ignored due to the Markov state. However, if the mapping from S to Z discards dynamics information, Z will not be Markovian. Is this formulation assumption still valid? <sep> Additional Feedback: <sep> Related Work:Equations 2 and 4 are referenced far before they are defined. I generally avoid referencing equations this far in advance.I would consider citing Mohamed & Rezende, 2015. <sep> Representation Learning in RL:Missing the discount factor in the preliminaries section. <sep> Sufficiency Analysis:Figure 2: J_inv —> J_state <sep> Experiments:Figure 6: should be J_state in the table.","Overview: <sep> The paper tries to answer which mutual information (MI) objective is sufficient for representation learning (repL) in reinforcement learning (RL). Three common objectives are considered: forward, state, and inverse. The paper shows that only the forward objective is sufficient for learning, i.e., sufficient for learning of optimal policy/value function. The authors also demonstrate this phenomena using empirical experiments. <sep> Quality, Clarity, Originality and Significance: <sep> All the reviewers believe this paper is novel in terms of methodology, i.e., evaluate the sufficiency of the repL in terms of down stream tasks. However, there is a lack of clarity in the experiment sections. The authors have provided more details in the rebuttal phase. The reviewers also have concerns that this paper may be too far from typical experimental settings to have a real impact on the field. An unofficial review pointed out there is a mistake in the proof of the paper. The authors later also confirmed the flaw and claimed it is fixed. <sep> Recommendation: <sep> The paper is indeed interesting and novel. However, the impact to the practice community might not be significant. That being said, the paper should warrant publication eventually. However, the authors changed large amount of text about the proofs before and after rebuttal, which also introduced some additional typos, confusions, and also technique sloppiness or flaws. The reviewers are concerned about this. Overall I believe that the paper is not in a state to be published yet."
"abstract | weakness | misc | weakness | misc | weakness | decision  ==> Summary: <sep> The paper proposes a new multigraph architecture called Multi-Order-Graph to explain the representation generation process in neural sequence encoders (Self-Attention or SAN based models). The main contribution of this MoG is the introduction of n-order dependency which can model not only relationships between words but also high order relationships such as syntax and semantics between subgraphs. Taking inspiration from MoG explanation, a self-attention powered Graph Transformer is proposed which beats the Transformer baselines on NMT tasks (English-German and German-English). <sep> Pros: <sep> The proposed idea of representing the encoding process as generation of a Multi-Order-Graph is novel and is able to provide good insights for SAN-based models. <sep> Proposed Graph Transformer uses self-attention and also attends over different order of subgraphs (low-order, middle-order and high-order). Since these subgraphs represent high order relationships (syntax, semantics etc.), the model is able to pay attention to salient subgraphs. <sep> This paper could fuel further research in the explainability of neural sequence encoders and SAN based models. <sep> Cons: <sep> The key concern about the paper is the lack of rigorous experimentation to study the usefulness of the proposed method. Only two datasets are used for NMT task. There has been a lot of improvement on base Transformer models (cited in the paper), still there is no comparison with them. <sep> This paper claims that with the proposed MoG explanation it is possible to model high order relationships such as Syntax and Semantics. However, there is no example provided for this. Even some cherry picked examples would have helped in visualising the behaviour. <sep> Overall, I would like to see the paper at the conference. The idea of modelling the relationships as a Multi-Order-Graph is certainly novel and can fuel further research. However, due to the current state of experimentation I am voting for rejecting the paper.  I am willing to increase the score if the concerns are addressed by authors in the rebuttal period. <sep> Questions and Suggestions: <sep> I would like to see comparisons with models like Transformer(big), Shaw et al., He et al.. I would also encourage usage of some more datasets for the experimentation, for example En-Fr, En-Ro. Datasets can be found in the papers linked above. <sep> It would be interesting to see how different components of the Graph Transformer affect perplexity. <sep> What is Transformer(small)? I am sorry but I am not aware of this variation. It would be good if you could provide a reference for this particular variation of the model. <sep> In Fig 4a, it is evident that self-gate works better at encoding longer sentences. It would be good to see some discussion on this in the paper. <sep> Which dataset is used for Fig 4? <sep> In fig 4b, we can notice a sudden decrease in subgraph weight at layer 4 for 12 layered model. In figure 5, similar trend can be seen for different layered model except for 9 and 10 layered model. What is the significance of this behaviour? <sep> I would encourage the authors to provide the implementation for the model proposed in this paper. <sep> Minor Comments/Typos: <sep> Section 2.4: source node of edge Snj -> Snj, source node of edge Ej <sep> Section 2.4: target node of edge Tnj -> Tnj, target node of edge Ej <sep> Section 3.3: the order of subgraph is in the range of 2^(n-1) to 2^(n) -> the order of subgraph is in range 2^(i-1) to 2^(i) <sep> Similar mistake is in Middle-Order and Low-Order section. <sep> Section 7: MoG connects only only words but also subgraphs -> MoG connects not only words but also subgraphs. <sep> A.5.3: Rlationship information generated -> Relationship information generated <sep> Post Rebuttal Comments: Authors have addressed most of my concerns and as a result I have increased the rating from 5 to 6. Thanks!","The paper proposes to explain the representation for layer-aware neural sequence encoders with multi-order-graph (MoG). Based on the MoG explanation, it further proposes Graph-Transformer as a graph-based self-attention network empowered Transformer. As commented by the authors, a main purpose of Graph-Transformer is to show an example application of the MoG explanation. <sep> During the discussion period, after reading the paper and checking the code, the AC had raised a serious concern: There is a big gap between the MoG motivation and the actual implementation. The AC had urged the referees to take a careful look at the implementation details, in particular, Lines 524-561 in the attached code: ""supplement/fairseq-0.6.2_halfdim_gate⁩ ▸ ⁨fairseq⁩ ▸ ⁨models⁩ ▸transformer.py"". The AC had made the following comments to the referees: ""Whether the performance gain of Graph-Transformer over Transformer is due to the MoG explanation is highly unclear. There is no direct evidence, such as appropriate visualization, to support that. In a high-level description, instead of using a usual skip connection that would combine beforex and x, the actual implementation is to 1) define increamental_x = x - beforex, 2) let increamental_x attend on beforex to produce x1, let beforex attend on increamental_x to produce x2, and let increamental_x attend on increamental_x to produce x3, 3) combine beforex, x1, x2, x3 in a certain way to produce the layer output."" <sep> Reviewer 2 responded to the AC's concern: ""After examining the transformer.py and Section 2 & 3, we cannot understand why the output of self-attentions could be regarded as MoG subgraphs? The authors did not explain the connection. In their code, the graph transformer seems to just utilize 3 multi-head attentions (line 539-541) in their encoder. Using MoG to interpret the outputs of three attentions (line 539-541) is not very convincing. The link is weak. We agree with your comments."" <sep> To summarize, the link between the actual implementation in the code and all the MoG explanations is quite weak, and the technical novelty of the actual implementation is not strong enough for an *CONF* publication. Therefore, the AC recommends Reject."
"abstract | strength | weakness | rating_summary | weakness | suggestion  ==> I have read the authors' responses to all reviews and ultimately elected to leave my score as it is (weak accept). I think the empirical results are strong, and while I am not as troubled by the motivation and framing of the work as reviewers 3 and 4, I think their more conceptual and methodological critiques have merit, dampening my enthusiasm for the submission. <sep> This submission proposes a model-driven data augmentation strategy that aims to improve the calibration and reduce the over-confidence of a variety of Bayesian neural network architectures when dealing with out-of-distribution (OOD) samples. It involves adding a generator network that aims to generate plausible OOD samples during training, along with an objective term that tries to force the predictor network to make high entropy (low confidence) predictions for these samples. The paper does a fairly thorough empirical comparison with ten datasets (eight regression, two image classification) and half a dozen baselines, most of which can be combined with PAD. The results indicate that PAD usually improves both calibration and accuracy by at least a small amount. <sep> This is a solid paper: the proposed method seems sensible (if pretty complex) and appears to be modestly effective in the included experimental results. The introduction summarizes the paper's contributions as: <sep> It proposes a model-driven data augmentation technique aimed at improving calibration and reducing over-confidence for OOD samples. <sep> It adapts and extends the technique to regression problems, which the paper argues is unprecedented. <sep> It demonstrates empirically that the proposed approach improves the OOD accuracy and calibration of four different strong Bayesian neural net models. <sep> I lack the broad familiarity with the data augmentation literature required to verify claim (2.). I suspect that if this simple claim is true, then it may be trivially so: it's hard to believe that no one has applied data augmentation to regression tasks, so perhaps folks haven't bothered to publish it. The authors can always modify or remove this claim, if needed. The other two contributions seem supported, although the empirical improvements are for the most part small (and probably not statistically significant?). I lean weakly toward acceptance: I would not oppose its inclusion in the *CONF* 2021 proceedings, but I wouldn't enthusiastically endorse it. I'll explain below. <sep> The paper's motivation as laid out in Sections 1 and 2 is sound: calibration and proper quantification of uncertainty are increasingly important in a wide range of applications where machine learning has real world consequences for safety, fairness, etc. What is more, existing techniques based on neural networks (increasingly widespread) do seem to suffer significant flaws, especially exhibiting overconfidence when they should not. The paper offers a diagnosis in the form of a conjecture (Section 2.2): ""failure to revert to the prior p(θ) for regions of the input space with insufficient evidence to warrant low entropy predictions."" Figure 1 effectively visualizes this phenomenon in a toy setting, but no further proof is offered. Further, the assumption that prior reversion is the correct thing to do isn't examined (though that's a basic tenet Bayesian modeling, so we'll set that aside). <sep> The proposed technique seems sensible, if complicated: add a generator network to produce OOD pseudo-samples during training and penalize the prediction network for making high confidence (low entropy) predictions on these pseudo-samples. We can consider this a form of model-driven (vs. heuristic) data augmentation. The generator loss, given in Equation (5), looks correct to my non-expert eye, and I suspect it's immediately comprehensible to readers familiar with GANs, VAEs, and Bayesian neural nets. The intuition for the OOD samples resonates with me: they should be close enough to real data to be plausible but far enough away that the predictor would be unjustified in assigning a high confidence or departing from the prior. <sep> The regularization term in Equation (7) is a bit more arcane at first glance, but it's intuitive: the conditional prediction distribution should be close to the prior for pseudo-data points far from the real training distribution. The derivations of the K-L term for regression and categorical classification are given in the appendix, but these aren't critical details for judging the significance of the paper (they're quite straightforward). <sep> The design of the experiments is sound: they simulate OOD settings by clustering each dataset and using distinct clusters for training and test splits and measure both accuracy and calibration. I don't have an opinion about the choice of the Kuleshov metric for calibration. The chosen baselines look strong, but I am not up-to-date on the relevant literature so I would not be able to identify a non-obvious missing baseline. <sep> The experimental results are promising. A PAD variant is usually (but not always) the best for each task and metric (exceptions include GP for Naval/accuracy and R1BNN for Power/calibration). Perhaps more important PAD does generally seem to improve both accuracy and calibration across both variants (DE, MC, SWAG, etc.) and datasets. So in other words, if a modeler chooses to use one of the compatible Bayesian neural networks, in most cases they should also use PAD. <sep> The work and manuscript have a few weaknesses that prevent me from more strongly recommending acceptance. For one, some of the exposition around training is unclear, in particular, how the objectivees in Equations (5) and (8) are combined during training. <sep> I praised the results above, but I think the manuscript's interpretation of its results (Section 4.3) is still more generous than mine. PAD does consistently improve accuracy and calibration, but the margin is sometimes small, raising the question aboout whether the added complexity is worthwhile in all cases. The paper argues that PAD consistently improves the calibration curves in Figure 3, at least for poorly calibrated models, but that does not seem obvious to me. This might be because the curves are somewhat cluttered, but I see a number of exceptions where the PAD look potentially worse: Energy, Naval (SWAG), and Yacht. <sep> I think perhaps the real problem is not the results themselves, which overall are strong, but rather the manuscript's rather cursory discussion of the results and its failure to offer any insights or guidance about, e.g., when PAD should be expected to help (based on task or dataset) or which baselines it works best with. <sep> One last note: I don't want to over-index on a toy figure included for illustration purposes, but I don't find the results in Figure 1 convincing! Perhaps I am misunderstanding what behavior we desire (if so, please correct me). I agree that the PAD distribution does a better job of capturing the uncertainty in the central low-data area, but at the left- and righthand ends, the baselines actually look preferable in that the uncertainty is often appropriately wider and contains or at least follows the true function. It looks like PAD might be over-regularizing things in these cases. <sep> Here are some actionable suggestions for improvements: <sep> Clarify how the objectives are combined and how training proceeds. Consider adding, e.g., an ""Algorithm"" summary figure. <sep> Expand the results discussion beyond simply restating the results (which are displayed in the Tables). For the raw accuracy and calibration numbers, perhaps you could compute some summary statistics for the baseline vs. PAD differences, so readers could get a quick sense of whether PAD usually beats the baseline. Maybe also some counts for how often a PAD variant has the best performance. <sep> Also in the discussion, try to distill out some illustrative patterns that could be turned into insights or practical guidance. <sep> For the calilbration curve plots (Figure 3), consider reducing the clutter by removing redundant curves. For most of the tasks, most baselines (and corresponding PAD variants) are quite similar so perhaps you could show a representative subset for each task (and then put the complete figures in the appendix). <sep> Here's a laundry list of questions: <sep> How does training proceed? Is it a typical alternating adversarial optimization, i.e., optimize generator, then discriminator, repeat? <sep> What additional computational complexity does PAD introduce during training? <sep> Under which conditions PAD should be expected to help most: type or distribution of data, task structure, baseline model, etc.?","This paper studies the problem of uncertainty estimation under distribution shift. The proposed approach (PAD) addresses this under-estimation issue, by augmenting the training data with inputs that the network has unjustified low uncertainty estimates, and asking the model to correct this under-estimation at those augmented datapoints. Results show promising improvement over a set of common benchmark tasks in uncertainty estimation, with comparisons to a number of existing approaches. <sep> All the reviewer agreed that the experiments are well conducted and the empirical results are very promising. However, they also had a shared concern on the justification of the approach. Reviewers are less willing to accept a paper merely for commending its empirical performance. <sep> I share the above concern as the reviewers, and I personally found the presentation of the approach a bit rush and disconnected from the motivation. For example, the current presentation feels like the method is motivated by BNNs but it is not clear to me how the proposed objective connects to the motivation. Also no derivation of the objective is included in either main text or appendix. <sep> In revision, I would suggest a focus on improving the clarity and theoretical justification of the proposed objective function."
"abstract | strength | weakness | rebuttal_process | weakness | rebuttal_process | decision  ==>  ==> The authors propose a novel robust training approach for deep metric learning (DML), accounting for the dependencies of metric losses within mini-batches. The proposed approach is evaluated on several popular metric learning datasets, demonstrating that the method works as intended, and achieves a certain level of robustness, unlike the baseline non-robust model which achieves a very poor performance when exposed to an adversarial attack. <sep> Comments: <sep> In the opening paragraphs of the paper, the authors state 'Our key insight is that during an inference-time attack, the positive point in the triplet will be modified by the attacker, and thus, during training, we must perturb positive points in the triplets, instead of anchors or negative points.', yet towards the end (and despite having motivated it in between), they show that the results on this are inconclusive and that it is not clear that perturbing only the positive points in the triplets is the right / best thing to do. Unless I am misunderstanding something, this feels out of sync - either it is a key insight, or inconclusive? It can't be both. <sep> Opening paragraphs / under Q1 and Q2 in Section 4 (Experiments) - this feels both redundant (both were mentioned before under Contributions) and also out of place, as it mentions results before even introducing the experimental setup. <sep> The authors evaluate their proposed robustness approach under a projected gradient descent (PGD) attack. While this is certainly sufficient to establish that the method works and that it provides some level of robustness to adversarial attacks, it feels really limiting to only assess a single attack type, of various options that are available - as it would have been potentially valuable to establish the degree of provided robustness under these different cases. As is, it is unclear whether the proposed approach will be universally helpful, or merely helpful with a particular attack type. While there is no reason to believe in the latter, the former hasn't been substantiated nor argumented. <sep> Apart from the general metrics shown in the tables, there isn't much additional analysis that would aim to reveal whether there were any patterns in these datasets on where the method worked vs didn't. For example, was the performance uniform across (pairs of) classes? If not, why? What about contrasting class pairs that are more/less similar? Or are there issues with rare classes? <sep> Tables 1, 2 and 3 should include confidence intervals. <sep> The authors use the phrase significant to qualify differences in several parts of the paper, yet there is no mention of which statistical test has been used to claim statistical significance of the differences? The authors should conduct proper statistical testing and highlight the exact test in the text. <sep> There is no discussion of the limitations of the current approach / areas for potential improvement and future work in the main paper - instead, some open questions are mentioned in the Appendix. It would be good to include key discussion points in the main body of the paper. <sep> Nit: Page 3, when giving lb(A,z) = c_k(A,z) - what if there is more than a single index returned by the argmin - what if there is a tie? The authors should specify if they are doing random tie breaking or taking the majority label (if a multi-way tie) <sep> Nit: Figure 4, please update with the finalized results.","This paper proposed a novel Adversarial Deep Metric Learning approaches. The reviews pointed out the paper proposes an interesting idea and it is among the rare works that address directly robust metric learning which an important topic for efficient metric learning. <sep> Some concerns were raised about the analysis and the lack of comparisons notably with other types of adversarial attacks. <sep> The authors provide a rebuttal where they addressed some concerns raised by reviewers with some precisions on the work, its positioning with respect to other related papers and additional comparisons notably with other types of attacks. <sep> A minor remark: there is a typo in Eq(13), where the z in the loss function is actually not defined and should be included in the max function. <sep> That being said, the contribution is still limited in considering only the infinite norm, analysis and comparisons to prior work remain weak. The paper does not meet the requirements for acceptance to *CONF* in its current form. <sep> I have then to propose rejection."
"abstract | rating_summary | weakness  ==> Summary: This paper is an empirical investigation into the role of architecture and objective choices in Neural Process (NP) models when the amount of conditioning data is limited. Specifically, they investigate the question of well-calibrated uncertainty. <sep> Clarity: The overall quality of the writing is clear, but missing details/explanations leave some claims unjustified and therefore lines of argument difficult to follow. <sep> Originality: Limited -- the paper is mostly an empirical investigation, with the modifications to existing NPs being (a) max-pooling and (b) SIVI on the posterior z's. <sep> Significance: Seems limited (see more on ""Cons"" section and ""Questions/comments""), though I am wondering whether the authors could have done more to emphasize the main contributions of this work. The paper's significance for me has been hampered by lack of details and clear takeaway/implications of the results. <sep> Pros: There's been a lot of work on NPs and their variants, but less so on empirically investigating how and when they work well. There is also a range of systematic empirical evaluations both in the main text and supplement, which I appreciated. <sep> Cons: There are some assumptions/statements that would be beneficial to elaborate upon in the paper. First, calibrated uncertainty is defined to be ""high sample diversity for small conditioning sets; and sharp-looking, realistic images for any size of conditioning set."" This statement, introduced early on in the paper without much justification, is a point that the authors repeatedly return to, and I found myself wondering why (especially since NPs are built for prediction/regression, so a lot of the prior work on calibration for classification models should hold here such as (Murphy 1973), (Gneiting et. al 2007), (Kuleshov et al., 2018)). What is the benefit of reasoning about calibration in the generative modeling case (e.g. with Inception Scores)? <sep> Additionally, why should a more flexible approximate posterior be more beneficial for better calibrated uncertainty? (Is this a log-likelihood argument, since the log-likelihood decomposes to calibration + ""sharpness""?) More broadly, my biggest problem was that the paper makes claims about improving calibration (e.g. samples are obtained from ""well calibrated posteriors"" in Figure 6) without formally defining how to evaluate ""good calibration,"" what it means to have ""calibrated uncertainty,"" etc. I would appreciate if the authors could clear up any misunderstandings I may have had about the work. <sep> Questions/comments: <sep> (Heusel et. al 2017) show that the inception score (IS) is not a valid metric for CelebA -- would the authors report FID scores instead? <sep> Regarding the comment in Section 4 about posterior contraction: aren't NPs exchangeable by design via the iid latent variable z (Korshunova et. al 2019, among many others)? I thought that invoking (conditional) de Finetti via the iid latent variable is what allows NPs to model (exchangeable) stochastic processes. <sep> It would be helpful to formally define ""posterior contraction"" for the reader -- is it referring to a reduction in posterior uncertainty? <sep> Intuitively, why would sample diversity decrease as the size of the conditioning set grows? For example, if I have a dataset of 10 examples of only cats and dogs and increase its size to 1000 (which say, also includes examples of sheep), shouldn't that also increase my sample diversity as well? <sep> Shouldn't the arrow going from z -> y_i in Figure 2 be reversed? <sep> UPDATE: I have read the authors' rebuttal and revised draft and have raised my score to a 5.","This paper analyzes some design choices for neural processes, paying particular attention to their small-data performance, uncertainty, and posterior contraction. This is certainly a worthwhile project, and R3 found the analysis interesting, giving the paper a score of 8. However, R1, R2, and R4 found the experimental validation to be incomplete and insufficient to support the paper's broader recommendations. As the paper is investigating the various combinations of implementations, I tend to agree with R1, R2, and R4 that this paper---while having some interesting ideas---needs a bit more precision and breadth to its experiments."
"abstract | weakness | misc | weakness | suggestion | rebuttal_process | suggestion | weakness | misc | suggestion | misc | weakness  ==>  ==> Summary: The paper proposes to solve the conditional inference problem by performing a relaxed version of variational inference in the prior space of the flow-based model. The model p(x) is pretrained, and one is interested sampling from p(x|observation). The observation could be some subset of x (inpainting), gray scale representation of an image (colorization), lower-res representation (super resolution), or noisy version of the data (compressed sensing). The paper proposes to perform inference in the prior space, by composing the post-hoc trained latent flow with the trained invertible decoder, as the conditional distribution in that space is believed to have a better geometry. <sep> Contributions and novelties: (A) propose to perform inference in the latent space of a latent variable model to side step the bad geometry, (B) propose to replace hard constraint with stochastic relaxation (placing an additional likelihood term to model the dummy variable). (C) Applications seem interesting for testing the quality of an unconditional flow-based model. <sep> Flaws: missing several important references in the discussion to prior works. These include [1], which describes a more general framework to post-hoc perform sampling from a conditional distribution of a learned latent variable model by fitting a distribution in the latent space; [2,3,4] which propose to mitigate the bad geometry of the learned data energy (in this case, the density model itself) by transforming it into a space where it's more Gaussianized. Similar idea has also been incorporated in [5] to enable the training of a residual EBM (no need to cite). The key novelties (A and B) are incremental in nature given the above related works that are not cited. <sep> [1] Latent Constraints: Learning to Generate Conditionally from Unconditional Generative Models, 2017 <sep> [2] Transport map accelerated markov chain Monte Carlo, 2014 <sep> [3] NeuTra-lizing Bad Geometry in Hamiltonian Monte Carlo Using Neural Transport, 2019 <sep> [4] Learning Energy-based Model with Flow-based Backbone by Neural Transport MCMC, 2020 <sep> [5] VAEBM: A Symbiosis between Variational Autoencoders and Energy-based Models, 2020 <sep> Additional details: <sep> For experiments, please report the bits-per-dim (BPD) of the pre-trained flow model, as well as unconditional samples for reference and better comparison. <sep> ""Ambient"" langevin dynamics is a very weak baseline, since one can easily improve the mixing via deriving a kernel in the latent space (as per [3]). It wouldl be a more fair comparison since the proposed method also composes with the learned decoder flow. Same goes to the naive implementation of the PL-MCMC which only uses a random walk kernel, which is a very weak baseline.Are the rows of Fig 4 independent? Why do they have the same stroke style if they are all conditioned only on the label only? Is it an indicator of the mode seeking behavior of the reverse KL?In the qualitative results (tab 2, fig 3 and fig 5), the generated samples do not really satisfy the hard constraints that they are conditioned on (e.g. the subsets of x for inpainting), possibly due to the relaxation via the dummy variable. This is not desirable. Is it possible to anneal the σ while training the latent flow so that it will concentrate on the (potentially degenerate) solution that satisfies these constraints? <sep> The authors claim the hardness result is surprising. It has been long shown that sampling from a general Bayesian Belief Network is NP-hard [6]. Can't the same conclusion be derived from that, with the main difference being the explicit parameterization via a coupling flow? <sep> [6] The Computational Complexity of Probabilistic Inference Using Bayesian Belief Networks, Cooper, 1990 <sep> Additional questions about the hardness result (I didn't read the proof): <sep> Generality of the hardness result: the statement is about the hardness of sampling from the conditional distribution. What is the conditional distribution in this context, is it referring to p(x2|x1) (i.e. observation is a subset of x)? Please be precise. If this is the case it is consistent with the presentation of the previous section (VI). However the proposed method seems to require a more general treatment to take account of the other tasks, e.g. inverse problems. <sep> The discussion of hardness seems to be used to motivate the relaxation of the hard constraints (of the givens). It doesn't seem that relevant when the observation is not a deterministic function of x (e.g. inpainting, coloration, etc). For compressed sensing for example, the likelihood p(observation|x) naturally exists and is non-degenerate (by assumption). The presentation seems a bit confusing. <sep> --- POST REBUTTAL --- <sep> Modified my score after the rebuttal, since (1) I believe the re-purposing achieved by this work can potentially broaden the applicability of flow-based generative model (2) the authors have toned down the abstract and clarified the contributions in the intro, which now better reflects the value of the work. <sep> I am still leaning towards rejection at the end given the limited originality of the proposed method and the lack of a more comprehensive discussion of different possible approaches, but as means to the same end. For example, the relaxed inference problem can be solved with an MCMC method. These should all be discussed and compared if the contribution is about repurposing a joint likelihood model using flows. <sep> PS. the last line (the references) of the last page might have been a mistake.","This paper proposes a method for conditional inference with arbitrary conditioning by creating composed flows. The paper provides a hardness result for arbitrary conditional queries. Motivated by the fact that conditional inference is hard the paper therefore suggests a novel relaxation where the conditioning is relaxed. <sep> There were various concerns from the reviewers regarding notation, comparison algorithms, and how the hardness result motivates the smoothing operation introduced. After careful study of the paper and all the comments I find that I am most concerned about the hardness result and how it motivates the smoothing operation that is done. Novel computational complexity results as such are not really in the scope of *CONF*. There's nothing wrong with having such a result in a paper, of course, but a paper like this should be evaluated on the basis of the algorithm proposed. <sep> Like R4, I do not follow how this hardness result is meant to motivate the smoothing that's applied. The paper is unambiguous that the goal is to do conditional inference. A hardness result is presented for conditional inference, and so a relaxed surrogate is presented. This has a minor problem that it's not clear the relaxed problem avoids the complexity boundary of the original one. There's a larger problem, though. The hardness result has not been sidestepped! The goal is still to solve conditional inference. The algorithm that's presented is still an approximate algorithm for conditional inference. R4 suggests that other approximation algorithms should be compared to. The authors responded to this point, but I am not able to understand the response. For the same reason, I think it is valid to ask for comparison to other approximate inference algorithms (e.g. without smoothing) <sep> None of the above is to say that the smoothing approach is bad. It may very well be. However, I think that either the existing argument should be clarified or a different argument should be given. <sep> Finally here are two minor points (These weren't raised by reviewers and aren't significant for acceptance of the paper. I'm just bringing them up in case they are useful.) <sep> Is Eq. 3 (proof in Appendix B.1) not just an example of the invariance of the KL-divergence under diffeomorphisms? <sep> Proof in appendix B.2 appears to just a special case of the standard chain rule of KL-divergence (e.g. as covered in Cover and Thomas)"
"abstract | weakness | rating_summary | decision  ==>  ==> This paper proposes Trust Entropy Actor Critic (TEAC), a novel algorithm for reinforcement learning (RL) combining the idea of TRPO/PPO and max-entropy RL, together with the corresponding critic, actor and dual updates. The high level idea is that trust region methods ensure stability by constraining the KL divergence from the previous policy, while entropy regularization encourages exploration, and hence combining the two may achieve the best of both worlds and obtain a good trade-off between stability and exploration. To achieve this goal, the authors propose to augment the original trust-region subproblem in TRPO with an additional constraint on the lower bound of the policy entropy (together with two other trivial constraints corresponding to the validity of the policy in the MDP framework). Then by forming the Lagrangian function and setting the gradient to zero, the authors obtain both the critic (value) and actor (policy) updates with different choices of dual variables (corresponding to the two trivial constraints), together with the dual updates. Numerical experiments compared to some popular baseline RL algorithms are also reported to demonstrate the improvement of TEAC compared to the existing works. <sep> in general, the high level idea is interesting and reasonable, and some empirical improvement is also shown. However, there are several undefined terms and technical issues/mistakes that largely downgrade the quality and contribution of this paper. <sep> Qπ in (1) and (2) is not defined, and the meaning of Eρπ(s),π(a|s) is unclear. Also, according to standard literature like [1, Chapter 4], the expected reward J(π) should indeed be something like 11−γEs∼ρπ,a∼π(⋅|s)r(s,a), so the term inside the expectation should probably be the reward r(s,a) instead of the Q function Qπ(s,a). <sep> The formulation in (2) is confusing. Firstly, it is unclear what the notation Eρπ(s)π(a|s)=1 means, and what the difference is between Eρπ(s)π(a|s) and Eρπ(s),π(a|s). Similarly, it is unclear what V is (as a function of π or a free variable), and what the notation Eρπ(s)π(a|s)p(s′|s,a) means. In general, all the terms appearing in the optimization problem should either be some constants or a function of π, but this is not made clear by the authors. Also, it's unclear why the authors switch the orders of π and πold in the KL divergence compared to the original TRPO formulation, and some explanations should be provided. <sep> The formula (4) seems to be weird. Firstly, according to the right-hand side, it seems that the left-hand side should indeed be the (s,a)-entry of the gradient instead of the entire gradient. Secondly, it is unclear why ρ(s) disappears in the second equality. <sep> The authors argue that the dual variables ν and λ can be taken arbitrarily as the policy is parametrized as a Gaussian via neural networks so that it is always a valid randomized policy (satisfying the third constraint in (2)), while the ""value function"" always satisfies the Bellman equation corresponding to the fourth constraint in (2). However, with the neural network parametrization θ, the optimization problem should also be solved w.r.t. θ instead of π, in which case (4) no longer makes sense (since it's differentiating w.r.t. π instead of θ). Also, as mentioned above, the value function V is undefined and hence the second claim does not make much sense. In addition, if the fourth constraint in (2) corresponds to the Bellman equation, then it is weird why no reward r or discount factor γ is involved there. <sep> In Lemma 1, the ""trust entropy Q-value"" is undefined. <sep> In (10), why can exp⁡(−(α+β+λ)/(α+β)) serve as a normalization term? In particular, why does it hold that ∫aπold(a|s)α/(α+β)exp⁡(Qπ(s,a)/(α+β))da always equal to exp⁡(−(α+β+λ)/(α+β)) for any state s? <sep> Also, the experimental part has some limitations. <sep> The numerical improvement is not very convincing, as it seems that TEAC only improves over SAC in two out of six tasks and only has significant improvement on one of these two tasks. Some more thorough comparisons are needed to validate the empirical advantage of the proposed method. <sep> The actually implemented algorithm, Algorithm 1 contains several tricks unexplained in the main text. In particular, why do we need two target critic networks? And in the update (38), should ϕ¯ be ϕ¯i? <sep> Finally, please find some miscellaneous suggestions/comments on additional references, technical issues fixing and typo fixing below. <sep> The work [2] seems to be closely related to the high level idea of this paper, and should better be compared with. <sep> In the abstract, ""transforms"" should be ""transform"". In the first paragraph of the introduction, ""learning process"" should be ""learning processes"". <sep> In the last paragraph of the introduction, ""guaranteed to converge"" is not very accurate. In fact, only the critic/policy evaluation is guaranteed to converge, and the authors show that policy improvement does hold (but may or may not lead to eventual convergence of the whole TEAC algorithm). <sep> In the first paragraph of Section 2, the definition of ρπ is not provided, although from the literature it probably means the discounted state-visitation distribution/measure. The authors should provide a clear definition for self-contained-ness, and the term ""state of the trajectory distribution"" does not seem to make much sense and should better be replaced with more standard terminologies like ""discounted state-visitation distribution/measure"". Also, ""qantified"" should be ""quantified"". <sep> In TEAC, the parameters τ and η are always fixed. However, as the algorithm proceeds, it may not make much sense to keep a constant exploration power as required by the constant η>0. Will a decreasing η be a better choice? Some discussions on this should better be provided. <sep> In (3), ρ(s) should be ρπ(s), and the dual variables α and β should be required to be non-negative. Accordingly, the dual updates should probably better be projected onto the non-negative orthant. <sep> In the sentence before (4), ""derivation"" should be ""derivative"". <sep> In (5), Q should be Qπ. <sep> In (20), the entropy terms lack right parentheses. <sep> Hence in general, I think the paper is still not ready for publication and needs substantial fixing and improvement. <sep> [1] Agarwal, Alekh, Nan Jiang, and Sham M. Kakade. Reinforcement learning: Theory and algorithms. Technical Report, Department of Computer Science, University of Washington, 2019. <sep> [2] Pajarinen, Joni, Hong Linh Thai, Riad Akrour, Jan Peters, and Gerhard Neumann. ""Compatible natural gradient policy search."" Machine Learning 108, no. 8-9 (2019): 1443-1466.","The paper proposes a reinforcement learning algorithm that combines trust region policy optimization and entropy maximization. The starting point is the Lagrangian of a constrained optimization problem that upper bounds the change in the policy and lower bounds the entropy of the policy. The paper proves that the algorithm converges, and evaluates it experimentally in MuJoCo domains. <sep> The main issues raised by the reviewers were related to the proofs (see especially R1) and experimental evaluation (R4). The authors did a great job improving the paper during the discussion phase, but some of the issues remain unresolved, and thus reviewers find the paper not to be ready for publication. Thus, I'm recommending rejection."
"abstract | weakness  ==>  ==> Summary: This paper investigates the relationship between deep ResNets and (implicitly) iterative computations. The authors introduce two main hypotheses that are at the core of the investigation: 1) whether the iterative inductive bias improves ResNet performance; and 2) whether recurrent ResNets are more parameter-efficient. The paper also proposes three metrics for studying the convergence and divergence behaviors of these networks in order to investigate this matter. <sep> Post-rebuttal thoughts: <sep> I would like to thank the authors for their detailed response and the revisions made to the paper. I'm updating my score to 5 as part of my concerns are satisfactorily addressed, and I wished I could have more opportunities to discuss with the authors on their response. In general, my opinion is that the authors have introduced too many ""artificial"" components to the study (e.g., soft gradient coupling, the convergence/divergence indices) that make me slightly dubious of how generalizable this characterization is. For example, as the authors indicated, spectral normalization creates a different phenomenon (at a cost of worse performance), but with no change to the structure itself (so unlike the soft gradient coupling), a different phenomenon could be challenging the conclusion of the paper. <sep> My suggestion would be that the authors delve deeper into the observations here and better integrate the revisions with their original approach (e.g., the high-dimensional discussion; the spectral normalization discussion, etc.) <sep> I feel that in the rebuttal phase the authors made certain important new edits to the paper (e.g., <sep> My general opinion is that this paper investigates an interesting direction on the learning behavior of ResNets, but is still not quite ready for publication in a venue like *CONF*. There is an obvious gap in related work (see my detailed comment below) on implicit deep networks; moreover, the definition of the various indices (e.g., convergence index) is also rather confusing to me. The empirical results are not strong enough evidence, in my view, to make most of the claims conclusively. I also have some doubts on the motivation for the methodology that the authors are using. <sep> Pros: <sep> Interesting direction; as the author shows, the ResNet architecture itself is expressive enough for implementing iterative computations/algorithms. So it is worthwhile to study its behavior along this trail. <sep> The paper is overall written in a clear manner and the author explained their methodology well. <sep> Cons: <sep> Many arguments are too hand-wavy and I don't particularly find the metrics the authors define to analyze convergence/divergence particularly convincing. (See my comment below) <sep> The experimental setup is mostly on small scales. <sep> Even with the small scale setups, the experimental results don't seem conclusive enough (at least to me) to draw the conclusion that the authors were trying to claim. The verification of hypothesis 2 is especially hasty. <sep> The motivation behind the soft gradient coupling is not clear to me. <sep> There is a clear missing gap in the related work that I think the authors should pay attention to. <sep> I will expand on some of the Cons above, and provide the following detailed comments/questions: <sep> Again, I think it is interesting to investigate the relationship between ResNets and iterative computations. But besides the canonical, plain unrolling of the layers that the authors have looked at, implicit models (i.e., models that study the continuous dynamics of a layer f) like Neural ODEs [1] and Deep Equilibrium Models [2] (there's a ResNet version of it) are both also looking at compact recurrent networks. In particular, the deep equilibrium models especially targets the convergence (i.e., the ""fixed point"" of the layer), and seems to demonstrate state-of-the-art level performances. In contrast to what the authors provided in the last paragraph of Section 1, I would therefore argue (based on Neural ODEs and deep equilibrium nets) that recurrence does offer some notable advantages like constant memory cost and analytical gradients. The other related thread of work is simply the classical recurrent backprop (RBP) theories, which study the convergence of recurrent networks and how one can leverage such property for the backward pass of these networks. I found the current version of the paper did not discuss either aspect of this, which I believe is important literature that actually is on the opposite side (partially) of what the authors are trying to claim. <sep> There are actually many ways that I can think of to make recurrent residual blocks converge when you infinitely repeat it. For example, with spectral normalization [3], we can simply make the Jacobian of the block have an operator norm <1. Then Banach fixed point theorem will guarantee convergence. Other methods are also possible (e.g., via a provably convergent optimization perspective). These are not discussed in the paper (nor are they the main focus, I guess), but this doesn't mean that ResNets do not converge in general. The authors argue that ""some balance between feedforward and iterative computations might have been learned by the ResNets"", but there is actually a lot of noise in the analysis... for example, the networks could be overfitting, etc. The point is, as long as you regularize the model in that direction, the ResNets could still converge. <sep> One main problem that I found about this paper is its definition of the convergence/divergence indices. The ""convergence"" concept in this paper is constrained to look at the accuracy convergence, by which the authors look at the inverse of the AUC of the classification rate curve. But given the nature of softmax and classification task itself, I don't think a convergence in accuracy is a good ""index"" for measuring convergence of an architecture, which Section 3.1 looks at (for z^i(t)). For example, softmax is constant up to a shift of constant. And for classification of, let's say 10 objects (x1,…,x10), getting x1,…,x5 correct is still different from getting x6,…,x10 correct, even though they both have ""50% accuracy"". The paper investigates CIFAR-10, where one can achieve >94% accuracy, but in cases like ImageNet where 70% accuracy is normal, these two 50% are certainly non-convergent to me. Also, I'm assuming the entire Figure 1 is on the simple 2-dimensional linear task? Does the phenomenon in Figure 1d repeat in high dimensionality? If so, what does it look like? (My experience with this suggests that if you keep stacking the same block, the activations will eventually oscillate, if not converge, but it could differ by initialization.) <sep> Some arguments are also a bit handwavy to me and I'd appreciate if the authors can expand on them. For example, in Section 3.2, the paper claims ""in contrast, the skip connections encourage a ResNet to use the same representational format across blocks... [and] are therefore better aligned with the final decoder"". As another example, the paper claims ResNets learn a balance between ""feedforward and iterative computations"". These are all intuitively reasonable arguments indeed, but considering that this is an empirical study paper, I think actually verifying these would make the paper stronger. <sep> About the soft gradient coupling, doesn't this simply mix the gradients and inject more stochasticity to them? In general, would you expect (when 0<λ<1) that just like in typical SGD, this stochasticity will be averaged out by the optimization procedure of deep networks? Since the Δ~t no longer fully reflect the mini-batch gradient descent direction, have you checked how the block parameters within the same stage gradually deviate from one another as you optimize the network (e.g., how does the standard deviation of Wl(s) over all layer l's in the same s change over training iterations? Do they deviate or stay around? If these weights are eventually still different, why can one still consider them to be ""similar"" (other than the RI metric, which I find to be a debatable metric given the #3 above...)? <sep> For the EPC, have you computed the EPC of an ordinary ResNet and a purely recurrent ResNet? How do their EPC look like when compared to the soft gradient coupled ResNets (e.g., λ=0.5)? <sep> In Section 5.3, the paper claims that ""if this is the case, we would expect soft gradient coupling to find such a solution."" Why? And isn't a soft gradient coupled ResNet still a non-recurrent ResNet (in the sense that you can't simply unroll a single layer to get the output; you still need to store all parameters of the network, rather than only a single layer of it)? <sep> I look forward to the authors' response on my questions/comments above. I'm happy to consider adjusting my score accordingly. <sep> [1] https://arxiv.org/abs/1806.07366 <sep> [2] https://arxiv.org/abs/1909.01377 <sep> [3] https://arxiv.org/abs/1802.05957","This work provides evidence against the hypothesis that ResNets implement iterative inference, or that iterative convergent computation is a good inductive bias to have in these models. The reviewers indicate that they think this hypothesis is interesting and relevant to the *CONF* community, but they do not find the current work sufficiently convincing. Both theoretically and experimentally the paper does not fully demonstrate the claim that iterative inference is not useful in ResNets, and the reviewers are unanimous in their recommendation to reject the paper until the evidence for this claim is strengthened."
"misc | strength | rating_summary | weakness | decision | misc  ==> I will initially provide a summary of the paper and list overall strengths and weakness of the paper. Then, I present my additional comments which are related to specific expressions in the main text, proof steps in the appendix etc. I would appreciate it very much if authors could address my questions/concerns under ""Additional Comments"" as well, since they affect my assessment and understanding of the paper; consequently my score for the paper. <sep> Summary: <sep> •    The paper focuses on convergence of two newly-proposed versions of AdaGrad, namely AdaGrad-window and AdaGrad-truncation, for finite sum setting where each component is smooth and possibly nonconvex. <sep> •    The authors prove convergence rate with respect to number of epochs T, where in each epoch one full pass over the data is performed with respect to well-known ""random shuffling"" sampling strategy. <sep> •    Specifically, AdaGrad-window is shown to achieve O~(T−1/2) rate of convergence, whereas AdaGrad-truncation attains (T−1/2) convergence, under component-wise smoothness and bounded gradients assumptions. Additionally, authors introduce a new condition/assumption called consistency ratio which is an essential element of their analysis. <sep> •    The paper explains the proposed modification to AdaGrad and provide their intuition for such adjustments. Then, the main results are presented followed by a proof sketch, which demonstrates the main steps of the theoretical approach. <sep> •    In order to evaluate the practical performance of the modified adaptive methods in a comparative fashion, two set of experiments were provided: training logistic regression model on MNIST dataset and Resnet-18 model on CIFAR-10 dataset. In these experiments; SGD, SGD with random shuffling, AdaGrad and AdaGrad-window were compared. Additionally, authors plot the behavior of their proposed condition ""consistency ratio"" over epochs. <sep> Strengths: <sep> •    I think epoch-wise analysis, especially for finite sum settings, could help provide insights into behaviors of optimization algorithms. For instance, it may enable to further investigate effect of batch size or different sampling strategies with respect to progress of the algorithms after every full pass of data. This may also help with comparative analysis of deterministic and stochastic methods. <sep> •    I have checked the proof of Theorem 1 in details and had a less detailed look at Theorems 2 and 3. I appreciate some of the technically rigorous sections of the analysis as the authors bring together analytical tools from different resources and re-prove certain results with respect to their adjustments. <sep> •    Performance comparison in the paper is rather simple but the authors try to provide a perspective of their consistency condition through numerical evidence. It gives some rough idea about how to interpret this condition. <sep> •    Main text is written in a clear; authors highlight their modification to AdaGrad and also highlight what their new ""consistency condition"" is. Proposed contributions of the paper are stated clearly although I do not totally agree with certain claims. One of the main theorems has a proof sketch which gives an overall idea about authors' approach to proving the results. <sep> Weaknesses: <sep> •    Although numerically the paper provides an insight into the consistency condition, it is not verifiable ahead of time. One needs to run a simulation to get some idea about this condition, although it still wouldn't verify the correctness. Since authors did not provide any theoretical motivation for their condition, I am not fully convinced out this assumption. For instance, authors could argue about a specific problem setting in which this condition holds. <sep> •    Theorem 3 (Adagrad-truncation) sets the stepsize depends on knowledge of r. I couldn't figure out how it is possible to compute the value r ahead of time. Therefore, I do not think this selection is practically applicable. Although I appreciate the theoretical rigor that goes into proving Theorem 3, I believe the concerns about computing r weakens the importance of this result. If I am missing out some important point, I would like to kindly ask the authors to clarify it for me. <sep> •    The related work which is listed in Table 1, within the group ""Adaptive Gradient Methods"" prove \\emph{iteration-wise} convergence rates for variants of Adam and AdaGrad, which I would call the usual practice. This paper argues about \\emph{epoch-wise} convergence. The authors claim improvement over those prior papers although the convergence rate quantifications are not based on the same grounds. All of those methods consider the more general expectation minimization setting. I would suggest the authors to make this distinction clear and highlight iteration complexities of such methods while comparing previous results with theirs. In my opinion, total complexity comparison is more important that rate comparison for the setting that this paper considers. <sep> •    As a follow up to the previous comment, the related work could have highlighted related results in finite sum setting. Total complexity comparisons with respect to finite sum setting is also important. There exists results for finite-sum nonconvex optimization with variance reduction, e.g., Stochastic Variance Reduction for Nonconvex Optimization, 2016, Reddi et. al. I believe it is important to comparatively evaluate the results of this paper with that of such prior work. <sep> •    Numerically, authors only compare against AdaGrad and SGD. I would say this paper is a rather theory paper, but it claims rate improvements, for which I previously stated my doubts. Therefore, I would expect comparisons against other methods as well, which is of interest to *CONF* community in my opinion. <sep> •    This is a minor comment that should be easy to address. For *CONF*, supplementary material is not mandatory to check, however, this is a rather theoretical paper and the correctness/clarity of proofs is important. I would say authors could have explained some of the steps of their proof in a more open way. There are some crucial expressions which were obtained without enough explanations. Please refer to my additional comments in the following part. <sep> Additional Comments: <sep> •    I haven't seen the definition that xt,m+1=xt+1,1 in the main text. It appears in the supplements. Could you please highlight this in the main text as it is important for indexing in the analysis? <sep> •    Second bullet point of your contributions claim that ""[consistency] condition is easy to verify"". I do not agree with this as I cannot see how someone could guarantee/compute the value r ahead of time or even after observing any sequence of gradients. Could you please clearly define what verification means in this context? <sep> •    In Assumption A3, I understand that Gtei=gt,i and Gte=∑i=1mgt,i. I believe the existing notation makes it complicated for the reader to understand the implications of this condition. <sep> •    In the paragraph right above Section 4.2, authors state that presence of second moments, Vt,i enables adaptive methods to have improved rates of SGD through Lemma 3. Could the authors please explain this in details? <sep> •    In Corollary 1, authors state that ""the computational complexity is nearly O(m5/2nd2ϵ−2)~"". A similar statement exists in Corollary 2. Could you please explain what ""nearly"" means in this context? <sep> •    In Lemma 8 in the supplements, aaT and bbT in the main expression of the lemma are rank-1 matrices. This lemma has been used in the proof of Lemma 4. As far as I understood, Lemma 8 is used in such a way that aaT or bbT correspond to something like gt,j2–gt−1,j2. I am not sure if this construction fits into Lemma 8 because, for instance, the expression gt,j2–gt−1,j2 is difference of two rank-1 matrices, which could have rank \\leq 2. Hence, there may not exist some vector a such that aaT=gt,j2–gt−1,j2, hence Lemma 8 may not be applied. If I am mistaken in my judgment I am 100% open for a discussion with the authors. <sep> •    In the supplements, in section ""A.1.7 PROOF OF MAIN THEOREM 1"", in the expression following the first line, I didn't understand how you obtained the last upper bound to |∇f(xt,i)|. Could you please explain how this is obtained? <sep> Score: <sep> I would like to vote for rejecting the paper. I praise the analytically rigorous proofs for the main theorems and the use of a range of tools for proving the key lemmas. Epoch-wise analysis for stochastic methods could provide insight into behavior of algorithms, especially with respect to real-life experimental setting. However, I have some concerns: <sep> I am not convinced about the importance of consistency ratio and that it is a verifiable condition. <sep> Related work in Table 1 has iteration-wise convergence in the general expectation-minimization setting whereas this paper considers finite sum structure with epoch-wise convergence rates. The comparison with related work is not sufficient/convincing in this perspective. <sep> (Minor) I would suggest the authors to have a more comprehensive experimental study with comparisons against multiple adaptive/stochastic optimizers. More experimental insight might be better for demonstrating consistency ratio. <sep> Overall, due to the reasons and concerns stated in my review, I vote for rejecting this paper. I am open for further discussions with the authors regarding my comments and their future clarifications. <sep> ======================================= Post-Discussions ======================================= <sep> I would like to thank the authors for their clarifications. After exchanging several responses with the authors and regarding other reviews, I decide to keep my score. <sep> Although the authors come up with a more meaningful assumption, i.e., SGC, compared to their initial condition, I am not fully convinced about the contributions with respect to prior work: SGC assumption is a major factor in the improved rates and it is a very restrictive assumption to make in practice. <sep> Although this paper proposes theoretical contributions regarding adaptive gradient methods, the experiments could have been a bit more detailed. I am not sure whether the experimental setup fully displays improvements of the proposed variants of AdaGrad.","Dear authors, <sep> Improving the theoretical understanding of powerful algorithms is an important contribution to our field. Nevertheless, most of the reviewers are inclined to reject the paper. I somehow have to agree with them as e.g., adding more restrictive assumptions can allow deriving better bounds, but the question then is how useful this result will be to the *CONF* community. I would encourage you to chose maybe another venue. <sep> Thanks"
"abstract | weakness | suggestion | weakness | suggestion | weakness | strength | weakness | suggestion  ==>  ==> This paper proposes a method to learn robust policies in multiagent environments: in particular, the policies should continue to work even if other agents in the environment deviate slightly in their behavior. Unlike previous work which allows for perturbations in other agents' policies, this work allows for perturbations in the reward functions that those agents optimize, allowing for more relevant robustness. <sep> Overall I liked the work -- robustness to what the agents optimize seems quite important and (to my limited knowledge) novel. The experiments seem reasonable and establish that the method (ERMAS) works well. I note a few qualms below. <sep> One worry about the theory is that it is not well-defined because it allows for arbitrary exogenous perturbations, as in this line: <sep> The robustness objective in Equation 4 considers general agent perturbations, e.g., agents may exhibit an exogenous aversion to complex policy choices. <sep> With this, it is no longer necessarily the case that a Nash equilibrium exists: Nash equilibria are only guaranteed to exist when the utility functions are of the game outcomes; they need not exist when the utility functions can also apply to the chosen policies. <sep> For example, consider rock-paper-scissors between Alice and Bob. Wins get +1 utility, losses get -1, and ties get 0. Now suppose we add the perturbation ""-100 if you play a stochastic policy"" (normally not allowed, but allowed in this paper's formalism as an ""exogenous preference""). It is easy to see that there is no Nash equilibrium where either player plays a stochastic policy, as they could then switch to a deterministic policy. But we know there is no Nash equilibrium for rock-paper-scissors under deterministic policies. <sep> Since Nash equilibriums are no longer guaranteed to exist, the objective in (4) may be undefined in some settings. <sep> This could be fixed by restricting perturbations to only affect utilities of states, though this would significantly decrease the expressive power of perturbations. <sep> I was confused by the fact that ERMAS outperformed MARL even when there is no change in agent policies. The authors note this fact as well: <sep> In fact, ERMAS outperforms the baseline AI Economist for the original setting of η = 0.23. This suggests the robustness and performance do not necessarily pose a zero-sum game: ERMAS can find equilibria with high performance and strong generalization. <sep> I am not sure I share the authors' enthusiasm. It is straightforward to show that the optimal policy in the MARL setting does at least as well as the optimal policy in the robust setting. The fact that ERMAS actually outperforms MARL is surprising and suggests that ERMAS optimizes better than MARL for some reason. This could happen in one of two ways: <sep> Something about ERMAS leads to better learning, in a way that can't apply to MARL (e.g. perhaps by having adversaries learning progresses faster since any flaws are found more quickly) <sep> Something about ERMAS leads to better learning, in a way that could apply to MARL (e.g. better hyperparameter tuning). <sep> If it's the second case (especially hyperparameter tuning), then it calls into question whether any of the improvements in the experiments come from the design of ERMAS, rather than coming from something unrelated like better hyperparameter tuning. <sep> Quality: Decent. I liked the algorithm derivation (though I did not carefully check the math), and the evaluation does show the benefits of the approach, though it would have been nice to test the approach on more environments (there is just one toy environment and one more complex environment). In particular, as far as I can tell the algorithm can be applied to any multiagent RL (MARL) problem; there are several benchmark suites on which this could be tested. <sep> Clarity: I found the paper to be quite clear. <sep> Originality: I believe this is a novel formulation, though I am not very familiar with the robust RL literature. <sep> Significance: Clearly relevant and important.","The paper presents a multi-agent RL algorithm where the rewards of the other agents are only known up to some accuracy. The setting is somewhat restrictive, in the sense that the transition is assumed to be known. It would perhaps have been more interesting for the paper to also consider unknown transitions, so as to bring it closer to work in single-agent reinforcement learning. It also seems to not be making a very good job of linking the related work to the contribution of this paper (even after looking at the appendix). <sep> Authors briefly say in the introduction <sep> ""Alternative frameworks improve robustness, e.g., to changes in environment dynamics, observation or action spaces (Pinto et al., 2017; Li et al., 2019; Tessler et al., 2019), but do not address reality gaps due to reward function mismatches, as they use inappropriate metrics on the space of adversarial perturbations"" <sep> Authors should try and better explain the differences with those papers. Do they consider changes in dynamics rather than the reward? It appears that the former is more general than the latter. Couldn't the authors compare with them with an appropriate experiment? <sep> It is also hard to see how this exactly connects with a reality gap. What is the 'training' environment? What is the 'testing' environment? This is simply a robust optimisation algorithm applied to multi-agent games with partially unknown reward functions. <sep> In addition the experiments themselves are not explained clearly. <sep> On the plus side, I think the algorithmic details and experimental are interesting. If there was a better explanation and discussion/comparison with related work, then it would have been a good paper. Authors are encouraged to make a stronger effort to compare with other methods both in terms of the algorithm and experimentally."
"abstract | strength | rebuttal_process | weakness | misc  ==>  ==> This paper suggests an interesting approach to knowledge distillation, which uses architectural properties rather than the loss function to encourage knowledge transfer between a teacher and a student. A student and teacher net are jointly trained on a prediction task, with forward connections from layers in the student net to layers in the teacher net. At test time, the teacher net can be stripped away. The method results in improvements in student performance compared to training the student without the teacher. <sep> The main positives I see in this paper are: <sep> modest improvements over baselines in the one-stage KD setting provides a different perspective on KD compared to recent works that are based on loss functions <sep> Although the results look promising, I think the paper is not yet ready for publication. The main issues I see are: <sep> no comparisons against network pruning and compression methods, which are arguably more relevant than KD baselines little insight or analysis of why the method works concerns with the experiments, detailed below <sep> This paper positions itself as a KD method, and argues that a novelty is in doing KD via architectural tricks rather than via a loss function. This may indeed be a new perspective for the KD literature, but it isn't without precedent. Methods that use architectural scaffolding at training time, which is removed at test time, do exist, simply under other names such as weight pruning or model compression (many of these papers are cited in the intro of the current submission). The current paper needs to make it clearer how the proposed method relates to those methods, and how it goes beyond them. Ideally this should include quantitative comparisons, or an explanation for why the other methods are not applicable. <sep> My second major concern is that this paper provides very little in the way of an explanation for why the method works. There is an intriguing statement that the method ""enhances the learning performance of the student model due to the backward gradient flows from the teacher,""  but nothing to back this statement up. Some analysis of how these gradients achieve desirable effects would greatly strengthen the paper. <sep> My third concern is with the experiments. First, the numbers in Table 2 are somewhat lower than those reported in prior papers (see Table 1 of Tian et al. 2020). Appendix A2 states that the code from Tian et al. 2020 was used to run the comparisons. Why then the discrepancy in performance? Second, many of the prior KD methods perform best when their objective is combined with the original Hinton KD objective (see Table 7 of Tian et al. 2020), but this comparison is not provided in the current paper. These two concerns mean that I'm not sure the proposed method is really outperforming competitive baselines from prior work. Lastly, I did not find enough details about ECD* to be able to really evaluate if those results are strong or interesting. <sep> Stylistically, I think the paper would be improved by adopting a more even-handed tone. Statements like ""compared to existing KD methods, ECD is more friendly to end users thanks to its simplicity"" or that the method is ""simple and neat"" come across more as advertisement rather than as scientific analysis. The intro argues that the one-stage nature of the proposed approach makes it more applicable than two-stage approaches, but I would say one and two-stage methods are simply targeting qualitatively different applications. Two stage approaches are useful when you are given a big model, which maybe you do not have the resources or data to train, and want to compress it or adapt it, e.g. for mobile deployment. One stage approaches are useful when you are able to train the big model yourself. There are interesting tradeoffs between these two paradigms and one is not better than the other. The current paper should acknowledge this. In general, the advantages and disadvantages of the method should be discussed equally. I also think the paper overemphasizes how simple the method is. I don't personally feel this method is any simpler than methods that use loss functions, and in fact I find the proposed method more conceptually complex since I don't know why it works. <sep> Despite these criticisms, something interesting does seem to be going on with this method, and I encourage the authors to pursue it further. <sep> Minor comments: <sep> Abstract: ""temporally"" —> ""temporarily""? <sep> Table 1: what is the ""Baseline""? The student? <sep> Table 2: citations should be added for each method","Knowledge distillation (KD) has been widely used in practice for deployment. In this paper, a variant of KD is proposed: given a student network, an auxiliary teacher architecture is temporarily generated via dynamic additive convolutions; dense feature connections are introduced to co-train the teacher and student models. The proposed method is novel and interesting. Empirical results showed that the proposed method can perform better than several KD variants. However, it is unclear why the proposed method works, although the authors tried to address this issue in their rebuttal. Besides this, a bigger concern on this work is that it missed a comparison with a recent approach in [1] which looks much simpler and performs significantly better on similar experiments. In [1], their ResNet50 (0.5x) is smaller than the student model in this paper (which used more filters on the top) but showed much stronger performance on both relative and absolute improvements over the same baseline (training from scratch) for the ImageNet classification task. On the technical side, the method in [1] simply uses the original ResNet50 as the teacher model, and the student model ResNet50 (0.5x) progressively mimics the intermediate outputs of the teacher model from layer to layer. [1] also contains a theoretic analysis (mean-field analysis based) to support their method. Comparing with the method in [1], the proposed method here is more complicated, less motivated, and less efficient. <sep> [1] D. Zhou, M. Ye, C. Chen, T. Meng, M. Tan, X. Song, Q. Le, Q. Liu and D. Schuurmans. Go Wide, Then Narrow: Efficient Training of Deep Thin Networks. ICML 2020."
"abstract | strength | weakness | suggestion  ==> The paper deals with the problem of community detection on graphs, examining the impact of graph measures. To do so, the paper proposes an experimental framework where clustering is achieved using the kernel k-means algorithm, and the performance of graph measures is examined on various instances of artificially generated graphs using the LFR benchmark. The overall approach is empirical, supported mainly by the experimental results. The main observations concern the consistent behavior of particular graph measures across multiple settings of the dataset. <sep> Strong points: <sep> -- The paper addresses an important problem in network analysis, which also concerns practitioners in a wide range of disciplines. <sep> -- Various graph measures are considered in the evaluation. This is very interesting, since, in my view, some of them are not very well-known among the graph clustering / community detection communities. <sep> -- The paper is well-structured and well-written. Most of the concepts, including the experimental framework, are clearly presented. <sep> Weak points: <sep> --- My main concern about the paper has to do with the consistency of the proposed evaluation framework under different evaluation criteria and graph data beyond the LFR benchmark. Firstly, as the paper also mentions, focusing only on LFR graphs can definitely reveal important properties of algorithms, but limits the generalization of the observations in the case where other generators might be used (e.g., SBM) or even real-world graphs. Besides, the argument made in the paper that the LFR benchmark generates graphs similar to real-world ones is not very accurate. LFR focuses on the clustering structure as well as on the degree distribution but might miss other key properties including graph diameter or the number of triangles (or the clustering coefficient). Is there any evidence that could support the observations of the paper in the case of real graphs? <sep> --- A closely related point has to do with the observation that real-world graphs do not have a clear clustering structure, i.e., well-defined cuts. Focusing only LFR graphs might not be enough to capture such instances. <sep> --- The modularity criterion is used to evaluate the quality of communities, which overall is a widely used criterion. Nevertheless, modularity has been shown to be prawn to the particular structure of the communities (e.g., resolution limit). How is this taken into account in the evaluation of the communities? <sep> --- Another point is that the paper is purely empirical. Definitely, this is a good starting point, especially when the focus is on experimental settings that have not been used before. Nevertheless, despite the interesting observations, I would expect to have a (theoretical) justification or some reasoning of why SCCT performs well on LFR graphs, or for instance why PPR which is a widely used measure, shows poor behavior. <sep> --- In the paper, all graph measures are considered as kernels. How valid is this argument? Why not just using the pure k-means for graph measures which are not kernels?","This paper studies various graph measures in depth. The paper was reviewed by three expert reviewers who complemented the ease of understanding because of clear writing. But they also expressed concerns for limited novelty, theoretical justification, and unrealistic setting. The authors are encouraged to continue research, taking into consideration the detailed comments provided by the reviewers."
"weakness | decision  ==> Update after revision <sep> I thank the authors for their work on this paper. The second reading was more pleasant. I agree with the authors that performing a user-study is an important effort, that should be encouraged. I however still believe that, if not benefitial to the user, the complexity of the method can be a drawback. I also wished that more comparisons, but especially other data modalities were investigated. I have updated my rating to reflect the improvement in the text. <sep> Short summary <sep> The authors propose a technique based on an invertible network to provide counterfactuals relative to one class of interest. The counterfactuals can be interpolated across an isosurface, displaying parameters which do not affect the model's decision. The authors propose an attribution map based on those counterfactuals and evaluate counterfactuals in a qualitative manner, based on their own observations on 3 datasets, as well as based on a human-grounded evaluation on a synthetic dataset. <sep> Strengths <sep> The use of an invertible dataset is rather novel in the field of explainability, and the relationship between the obtained counterfactuals and gradient-based interpolation methods is interesting. The human-grounded evaluation is definitely a large undertaking that is not often performed to assess the usefulness of interpretability techniques. <sep> Weaknesses <sep> I have identified several weaknesses of the work that justify my recommendation: <sep> the (lack of) clarity of the text. <sep> the assessment of the technique, as the results of the human-grounded evaluation are mixed, with users not being significantly more accurate in finding confounding factors compared to a baseline technique. <sep> the limitations of the technique, not discussed in depth. For instance, I can see difficulties in evaluating the effect of classes that are not present as ""training classes"" in the dataset, which requires a large labeling effort. In addition, how the technique would transpose to non-image datasets, or whether there are limitations in the invertible architectures to consider should be mentioned. <sep> Novelty <sep> The ""Related works"" section is rather limited, which makes it difficult to evaluate. In general, the use of invertible networks as interpretable networks is novel. <sep> Clarity <sep> Clarity was a major weakness of this work for me: <sep> the datasets are illustrated in figures but not mentioned until much later the maths are described in sections that seem unrelated to each other, without depicting the relationships between the different steps multiple concepts are unclear (see detailed comments) <sep> the motivations are not clearly explained <sep> Rigor <sep> I found the qualitative evaluation on the 3 datasets unconvincing, as it is unclear whether the same conclusions could not have been reached using other techniques. <sep> While I was most interested by the discussion around the generation of counterfactuals based on the invertible network compared to based on the integration of gradients, I wished there was a definition of an ""ideal"" counterfactual, qualitative or (preferably) quantitative. The single example provided in the main text is appealing but this requires more evidence to me. <sep> Finally, the ""saliency"" maps defined in this work do not seem to be used later on in the work. I doubt that looking at them would improve human evaluation of a model's behavior. <sep> Detailed comments <sep> Counterfactuals: their quality seems subject to appreciation and confirmation bias, especially on potentially cherry picked examples. To assess their quality, I would suggest to use the BAM dataset (Yang and Kim, 2019, https://github.com/google-research-datasets/bam) which was generated to benchmark attribution methods. I would overall suggest the use of this dataset for assessing the faithfulness (sensitivity, specificity) of the proposed approach. <sep> The choice of the mice dataset should be justified as this doesn't seem like an obvious choice to assess the quality of attribution techniques. It is quite difficult to estimate any effect, and feels like qualitative evaluation is biased by the authors' remarks given the lack of knowledge of the problem. <sep> There should be more details about the Two4Two dataset and its motivations, as well as how it relates to other datasets (e.g. Goyal et al., 2019) <sep> How does the proposed approach relate to ""completeness"" (Sundararajan et al., 2017)? <sep> What is the mathematical justification to resize the saliency map of an intermediate layer to the input resolution? Is there a citation for this process showing that this is a reasonable assumption? <sep> I am confused by the section on saliency maps: what does h represent? The activations at an intermediate layer? The motivation is unclear: what are the authors trying to highlight in these ""saliency maps""? Are these computed attributions or are these L1 distance between activations (in %) between x and x_tilde? Or is it a cosine distance (as suggested by the next sentence mentioning the angle?) <sep> The tasks used for illustration are not described in the text. Examples of y and epsilon should be provided. <sep> Is the technique limited to the model's predicted classes? <sep> How is ""ideal"" counterfactual described and mathematically verified? <sep> The relationship between counterfactuals and e.g. integrated gradients is unclear: the first clearly needs a model that can generate data, while the latter integrates the gradients between a baseline (defined by the user) and the input. More details and explanations are required to make this relationship clearer. <sep> What are the participants in the human-based study viewing? Are they comparing the counterfactuals to e.g. SmoothGrad maps, or the saliency as defined per the proposed approach? <sep> It is unclear what the participants answered: Figure 5a mentions that the main score is ""strongly disagree"" for ""arms"" (both baseline and interpolation) while the text refers to ""strongly agree"". Example questions would help. <sep> The results of the human-grounded study are not very conclusive. Note: please correct for multiple comparisons due to multiple statistical testing of the same effect. <sep> Kim et al., 2018 already displayed that human users were performing poorly at identifying a network's decision behavior based on saliency maps. A better comparison could have relied on TCAV instead, especially as the concepts can easily be mapped to the features given the synthetic dataset. This could have made a stronger case for the use of invertible networks, especially as Goyash et al (2019) mention the use of counterfactuals based on concepts. <sep> How about non-image datasets? <sep> Minor <sep> Intro: I would suggest using ""transparency"" rather than ""interpretability"" when referring to logistic regression (e.g. Lipton, 2016). The interpretability of linear model weights is indeed debatable, as weights will depend on the regularization and signal-to-noise ratio in the data (Haufe et al., 2014). <sep> No clear flow between the different works in the intro. No clear motivation behind counterfactuals. <sep> proofreading: paper is quite hard to follow and minor changes to grammar (e.g. ""Their similarity is easy to seen"") makes it more difficult to assess. The quality of the writing deteriorates in sections 3, 4 and 5. <sep> It is unclear what scale delta epsilon represents, and whether we can expect the norm of the different techniques to be comparable.","All the reviewers agree that the paper presents an interesting idea, and the main concern raised by the reviewers was the clarity of the paper. I believe that the authors have improved the presentation of the paper after rebuttal, however, I still believe that the paper woudl require another round of reviews before being ready for publication, in order to properly assess its contributions."
"abstract | rating_summary | suggestion | decision  ==> This paper proposes a conditioning method to train the GANs. The conditioning trick is based on the DFN (departure from normality) metric computed in the spectrogram domain of Schur decomposition to ensure the correlation between real and generated samples. Experiments were performed on a few public audio datasets, and the results suggest that imposing the proposed constraint on the generators not only delays the collapse in training but also yields the better reconstruction performance. <sep> Pros: <sep> An important issue of stable training of the GANs is addressed. <sep> The proposed conditioning trick is novel and interesting in some sense. <sep> Cons: <sep> The major concern about this paper is the lack of thorough experimentation to prove the usefulness of the proposed method. The experiments and evaluation don't seem to sufficiently support the main arguments. In particular, the authors put an emphasis on the importance of generating the high-fidelity audio spectrograms for higher classification accuracies. This was never supported by any experiments. The reconstruction of speech samples are not sufficient to back up the arguments. Furthermore, another advantage of the proposed method the authors state was its ability to generate the samples with high diversity. This too was not demonstrated with experiments. <sep> The authors state that the proposed method is generalizable but it should be confirmed through more experiments on the other domains. Otherwise, it is recommended that it must be explicitly stated that the proposed method applies to audio or speech, including the title. <sep> The paper is not easy to follow. Some terms and notations are not adequately explained. So are the descriptions of the figures. For example, in Figure 2, it is difficult to follow the tradeoff between the quality and the diversity according to different alpha, epsilon values. <sep> Speech samples in the supplementary material don't seem to match the corresponding DWT spectrograms and waveforms. There are some clipping noises and sudden silences in audio while the spectrograms and waveforms look almost identical to those of the original audio. <sep> To summarize, the paper presents a novel and interesting idea to tackle the important problem in the field, but fail to provide the experimental evidences to support the idea. Therefore, I recommend the paper is not ready to be published in its current form.","The paper proposes a trick for stabilizing GAN training and reports experiment results on spectrogram synthesis. All the reviewers rate the paper below the bar, citing various concerns, including a lack of clarity and unconvincing results. Several reviewers suggest conducting evaluations in the image domain as most of the GAN training techniques are proposed in the image domain. After consolidating the reviews and rebuttal, the area chair finds the reviewer's argument convincing and would not recommend acceptance of the paper."
