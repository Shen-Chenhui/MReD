2018-12	This paper proposes "spectral normalization" -- constraining the spectral norm of the weights of each layer -- as a way to stabilize GAN training by in effect bounding the Lipschitz constant of the discriminator function.	abstract
2018-12	The paper derives efficient approximations for the spectral norm, as well as an analysis of its gradient.	abstract
2018-12	Experimental results on CIFAR-10 and STL-10 show improved Inception scores and FID scores using this method compared to other baselines and other weight normalization methods.	abstract
2018-12	Overall, this is a well-written paper that tackles an important open problem in training GANs using a well-motivated and relatively simple approach.	strength
2018-12	The experimental results seem solid and seem to support the authors' claims.	strength
2018-12	I agree with the anonymous reviewer that connections (and differences) to related work should be made clearer.	weakness
2018-12	Like the anonymous commenter, I also initially thought that the proposed "spectral normalization " is basically the same as "spectral norm regularization", but given the authors' feedback on this I think the differences should be made more explicit in the paper.	rebuttal_process
2018-12	Overall this seems to represent a strong step forward in improving the training of GANs, and I strongly recommend this paper for publication.	decision
2018-12	Small Nits: Section 4: "In order to evaluate the efficacy of our experiment": I think you mean "approach".	suggestion
2018-12	There are a few colloquial English usages which made me smile, e.g. * Sec 4.1.1.	misc
2018-12	"As we prophesied ...", and in the paragraph below	suggestion
2018-12	* "... is a tad slower ...".	weakness
2018-12	This paper borrows the classic idea of spectral regularization, recently applied to deep learning by Yoshida and Miyato (2017) and use it to normalize GAN objectives.	abstract
2018-12	The ensuing GAN, coined SN-GAN, essentially ensures the Lipschitz property of the discriminator.	abstract
2018-12	This Lipschitz property has already been proposed by recent methods and has showed some success.	abstract
2018-12	However,  the authors here argue that spectral normalization is more powerful; it allows for models of higher rank (more non-zero singular values) which implies a more powerful discriminator and eventually more accurate generator.	abstract
2018-12	This is demonstrated in comparison to weight normalization in Figure 4.	abstract
2018-12	The experimental results are very good and give strong support for the proposed normalization.	strength
2018-12	While the main idea is not new to machine learning (or deep learning), to the best of my knowledge it has not been applied on GANs.	strength
2018-12	The paper is overall well written (though check Comment 3 below), it covers the related work well and it includes an insightful discussion about the importance of high rank models.	strength
2018-12	I am recommending acceptance, though I anticipate to see a more rounded evaluation of the exact mechanism under which SN improves over the state of the art.	decision
2018-12	More details in the comments below.	misc
2018-12	Comments: 1. One concern about this paper is that it doesn't fully answer the reasons why this normalization works better.	weakness
2018-12	I found the discussion about rank to be very intuitive, however this intuition is not fully tested.	weakness
2018-12	Figure 4 reports layer spectra for SN and WN.	weakness
2018-12	The authors claim that other methods, like (Arjovsky et al. 2017) also suffer from the same rank deficiency.	weakness
2018-12	I would like to see the same spectra included.	suggestion
2018-12	2. Continuing on the previous point: maybe there is another mechanism at play beyond just rank that give SN its apparent edge?	suggestion
2018-12	One way to test the rank hypothesis and better explain this method is to run a couple of truncated-SN experiments.	suggestion
2018-12	What happens if you run your SN but truncate its spectrum after every iteration in order to make it comparable to the rank of WN?	suggestion
2018-12	Do you get comparable inception scores?	suggestion
2018-12	Or does SN still win?	suggestion
2018-12	3. Section 4 needs some careful editing for language and grammar.	abstract
2018-12	The paper is motivated by the fact that in GAN training, it is beneficial to constrain the Lipschitz continuity of the discriminator.	abstract
2018-12	The authors observe that the product of spectral norm of gradients per each layer serves as a good approximation of the overall Lipschitz continuity of the entire discriminating network, and propose gradient based methods to optimize a "spectrally normalized" objective.	abstract
2018-12	I think the methodology presented in this paper is neat and the experimental results are encouraging.	strength
2018-12	However, I do have some comments on the presentation of the paper: 1. Using power method to approximate matrix largest singular value is a very old idea, and I think the authors should cite some more classical references in addition to (Yoshida and Miyato).	weakness
2018-12	For example, Matrix Analysis, book by Bhatia	weakness
2018-12	Matrix computation, book by Golub and Van Loan.	misc
2018-12	Some recent work in theory of (noisy) power method might also be helpful and should be cited, for example,	weakness
2018-12	https://arxiv.org/abs/1311.2495 2. I think the matrix spectral norm is not really differentiable; hence the gradients the authors calculate in the paper should really be subgradients.	weakness
2018-12	Please clarify this. 3. It should be noted that even with the product of gradient norm, the resulting normalizer is still only an upper bound on the actual Lipschitz constant of the discriminator.	weakness
2018-12	Can the authors give some empirical evidence showing that this approximation is much better than previous approximations, such as L2 norms of gradient rows which appear to be much easier to optimize?	suggestion

2018-85	This paper proposes an improvement in the speed of training/inference with structured prediction energy networks (SPENs) by replacing the inner optimization loop with a network trained to predict its outputs.	abstract
2018-85	SPENs are an energy-based structured prediction method, where the final prediction is obtained by optimizing min_y E_theta(f_phi(x), y), i.e., finding the label set y with the least energy, as computed by the energy function E(), using a set of computed features f_phi(x) which comes from a neural network.	abstract
2018-85	The key innovation in SPENs was representing the energy function E() as an arbitrary neural network which takes the features f(x) and candidate labels y and outputs a value for the energy.	abstract
2018-85	At inference time y can be optimized by gradient descent steps.	abstract
2018-85	SPENs are trained using maximum-margin loss functions, so the final optimization problem is max -loss(y, y') where y' = argmin_y E(f(x), y).	abstract
2018-85	The key idea of this paper is to replace the minimization of the energy function min_y E(f(x), y) with a neural network which is trained to predict the resulting output of this minimization.	abstract
2018-85	The resulting formulation is a min-max problem at training time with a striking similarity to the GAN min-max problem, where the y-predicting network learns to predict labels with low energy (according to the E-computing network) and high loss while the energy network learns to assign a high energy to predicted labels which have a higher loss than true labels (i.e. the y-predicting network acts as a generator and the E-predicting network acts as a discriminator).	abstract
2018-85	The paper explores multiple loss functions and techniques to train these models.	abstract
2018-85	They seem rather finnicky, and the experimental results aren't particularly strong when it comes to improving the quality over SPENs but they have essentially the same test-time complexity as simple feedforward models while having accuracy comparable to full inference-requiring energy-based models.	weakness
2018-85	The improved understanding of SPENs and potential for further work justify accepting this paper.	decision
2018-85	The paper proposes training ``inference networks,'' which are neural network structured predictors.	abstract
2018-85	The setup is analogous to generative adversarial networks, where the role of the discriminator is played by a structured prediction energy network (SPEN) and the generator is played by an inference network.	abstract
2018-85	The idea is interesting. It could be viewed as a type of adversarial training for large-margin structured predictors, where counterexamples, i.e., structures with high loss and low energy, cannot be found by direct optimization.	strength
2018-85	However, it remains unclear why SPENs are the right choice for an energy function.	weakness
2018-85	Experiments suggest that it can result in better structured predictors than training models directly via backpropagation gradient descent.	weakness
2018-85	However, the experimental results are not clearly presented.	weakness
2018-85	The clarity is poor enough that the paper might not be ready for publication.	decision
2018-85	Comments and questions: 1) It is unclear whether this paper is motivated by training SPENs or by training structured predictors.	weakness
2018-85	The setup focuses on using SPENs as an inference network, but this seems inessential.	weakness
2018-85	Experiments with simpler energy functions seem to be absent, though the experiments are unclear (see below).	weakness
2018-85	2) The confusion over the motivation is confounded by the fact that the experiments are very unclear.	weakness
2018-85	Sometimes predictions are described as the output of SPENs (Tables 2, 3, 4, and 7), sometimes as inference networks (Table 5), and sometimes as a CRF (Tables 4 and 6).	weakness
2018-85	In 7.2.2 it says that a BiLSTM is used for the inference network in Twitter POS tagging, but Tables 4 and 6 indicate both CRFs and BiLSTMS?	weakness
2018-85	It is also unclear when a model, e.g., BiLSTM or CRF is the energy function (discriminator) or inference network (generator).	weakness
2018-85	3) The third and fourth columns of Table 5 are identical.	weakness
2018-85	The presentation should be made consistent, either with dev/test or -retuning/+retuning as the top level headers.	suggestion
2018-85	4) It is also unclear how to compare Tables 4 and 5.	weakness
2018-85	The second to bottom row of Table 5 seems to correspond with the first row of Table 5, but other methods like slack rescaling have higher performance.	weakness
2018-85	What is the takeaway from these two tables supposed to be?	weakness
2018-85	5) Part of the motivation for the work is said to be the increasing interest in inference networks: "In these and related settings, gradient descent has started to be replaced by inference networks.	weakness
2018-85	Our results below provide more evidence for making this transition." However, no other work on inference networks is directly cited.	weakness
2018-85	= Quality = Overall, the authors do a good job of placing their work in the context of related research, and employ a variety of non-trivial technical details to get their methods to work well.	strength
2018-85	= Clarity = Overall, the exposition regarding the method is good.	strength
2018-85	I found the setup for the sequence tagging experiments confusing, tough.	weakness
2018-85	See more comments below. = Originality / Significance =	misc
2018-85	The paper presents a clever idea that could help make SPENs more practical.	strength
2018-85	The paper's results also suggest that we should be thinking more broadly about how to using complicated structured distributions as teachers for model compression.	strength
2018-85	= Major Comment = I'm concerned by the quality of your results and the overall setup of your experiments.	weakness
2018-85	In particular, the principal contribution of the sequence tagging experiments seems top be different than what is advertised earlier on in the paper.	weakness
2018-85	Most of your empirical success is obtained by taking a pretrained CRF energy function and using this as a teacher model to train a feed-forward inference network.	weakness
2018-85	You have have very few experiments using a SPEN energy function parametrization that doesn't correspond to a CRF, even though you could have used an arbitrary convnet, RNN, etc.	weakness
2018-85	The one exception is when you use the tag language model.	weakness
2018-85	This is a good idea, but it is pretrained, not trained using the saddle-point objective you introduce.	weakness
2018-85	In fact, you don't have any results demonstrating that the saddle-point approach is better than simpler alternatives.	weakness
2018-85	It seems that you could have written a very different paper about model compression with CRFs that would have been very interesting and you could've have used many of the same experiments.	weakness
2018-85	It's unclear why SPENs are so important.	weakness
2018-85	The idea of amortizing inference is perhaps more general.	weakness
2018-85	My recommendation is that you either rebrand the paper to be more about general methods for amortizing structured prediction inference using model compression or do more fine-grained experiments with SPENs that demonstrate empirical gains that leverage their flexible deep-network-based energy functions.	suggestion
2018-85	= Minor Comments = * You should mention 'Energy Based GANs"	suggestion
2018-85	* I don't understand "This approach performs backpropagation through each step of gradient descent, permitting more stable training but also evidently more overfitting." Why would it overfit more?	weakness
2018-85	Simply because training was more stable?	weakness
2018-85	Couldn't you prevent overfitting by regularizing more?	weakness
2018-85	* You spend too much space talking about specific hyperparameter ranges, etc.	weakness
2018-85	This should be moved to the appendix.	weakness
2018-85	You should also add a short summary of the TLM architecture to the main paper body.	suggestion
2018-85	* Regarding your footnote discussing using a positive vs.	suggestion
2018-85	negative sign on the entropy regularization term, I recommend checking out "Regularizing neural networks by penalizing confident output distributions."	suggestion
2018-85	* You should add citations for the statement "In these and related settings, gradient descent has started to be replaced by inference networks."	suggestion
2018-85	* I didn't find Table 1 particularly illuminating.	weakness
2018-85	All of the approaches seem to perform about the same.	weakness
2018-85	What conclusions should I make from it?	weakness
2018-85	* Why not use KL divergence as your \\Delta function?	weakness
2018-85	* Why are the results in Table 5 on the dev data?	weakness
2018-85	* I was confused by Table 4.	misc
2018-85	First of all, it took me a very long time to figure out that the middle block of results corresponds to taking a pretrained CRF energy and amortizing inference by training an inference network.	weakness
2018-85	This idea of training with a standard loss (conditional log lik.) and then amortizing inference post-hoc was not explicitly introduced as an alternative to the saddle point objective you put forth earlier in the paper.	weakness
2018-85	Second, I was very surprised that the inference network outperformed Viterbi (89.7 vs.	weakness
2018-85	89.1 for the same CRF energy).	weakness
2018-85	Why is this? * I'm confused by the difference between Table 6 and Table 4?	weakness
2018-85	Why not just include the TLM results in Table 4?	weakness

2018-88	The authors adapts stochastic natural gradient methods for variational inference with structured inference networks.	abstract
2018-88	The variational approximation proposed is similar to SVAE by Jonhson et al. (2016), but rather than directly using the global variable theta in the local approximation for x the authors propose to optimize a separate variational parameter.	abstract
2018-88	The authors then extends and adapts the natural gradient method by Khan & Lin (2017) to optimize all the variational parameters.	abstract
2018-88	In the experiments the authors generally show improved convergence over SVAE.	abstract
2018-88	The idea seems promising but it is still a bit unclear to me why removing dependence between global and local parameters that you know is there would lead to a better variational approximation.	weakness
2018-88	The main motivation seems to be that it is easier to optimize.	weakness
2018-88	- In the last two sentences of the updates for \\theta_PGM you mention that you need to do SVI/VMP to compute the function \\eta_x\\theta.	weakness
2018-88	Might this also suffer from non-convergence issues like you argue SVAE does?	weakness
2018-88	Or do you simply mean that computation of this is exact using regular message passing/Kalman filter/forward-backward?	weakness
2018-88	- It was not clear to me why we should use a Gaussian approximation for the \\theta_NN parameters?	weakness
2018-88	The prior might be Gaussian but the posterior is not?	weakness
2018-88	Is this more of a simplifying assumption?	weakness
2018-88	- There has recently been interest in using inference networks as part of more flexible variational approximations for structured models.	weakness
2018-88	Some examples of related work missing in this area is "Variational Sequential Monte Carlo" by Naesseth et al. (2017) / "Filtering Variational Objectives" by Maddison et al. (2017) / "Auto-encoding sequential Monte Carlo" Le et al. (2017).	weakness
2018-88	-  Section 2.1, paragraph nr 5, "algorihtm" -> "algorithm" The paper seems to be significant since it integrates PGM inference with deep models.	strength
2018-88	Specifically, the idea is to use the structure of the PGM to perform efficient inference.	strength
2018-88	A variational message passing approach is developed which performs natural-gradient updates for the PGM part and stochastic gradient updates for the deep model part.	strength
2018-88	Performance comparison is performed with an existing approach that does not utilize the PGM structure for inference.	strength
2018-88	The paper does a good job of explaining the challenges of inference, and provides a systematic approach to integrating PGMs with deep model updates.	strength
2018-88	As compared to the existing approach where the PGM parameters must converge before updating the DNN parameters, the proposed architecture does not require this, due to the re-parameterization which is an important contribution.	strength
2018-88	The motivation of the paper, and the description of its contribution as compared to existing methods can be improved.	weakness
2018-88	One of the main aspects it seems is generality, but the encodings are specific to 2 types PGMs. Can this be generalized to arbitrary PGM structures?	weakness
2018-88	How about cases when computing Z is intractable?	weakness
2018-88	Could the proposed approach be adapted to such cases.	weakness
2018-88	I was not very sure as to why the proposed method is more general than existing approaches.	weakness
2018-88	Regarding the experiments, as mentioned in the paper the evaluation is performed on two fairly small scale datasets.	weakness
2018-88	the approach shows that the proposed methods converge faster than existing methods.	weakness
2018-88	However, I think there is value in the approach, and the connection between variational methods with DNNs is interesting.	strength
2018-88	This paper presents a variational inference algorithm for models that contain deep neural network components and probabilistic graphical model (PGM)	abstract
2018-88	components. The algorithm implements natural-gradient message-passing where the messages automatically reduce to stochastic gradients for the non-conjugate neural network components.	abstract
2018-88	The authors demonstrate the algorithm on a Gaussian mixture model and linear dynamical system where they show that the proposed algorithm outperforms previous algorithms.	abstract
2018-88	Overall, I think that the paper proposes some interesting ideas, however, in its current form I do not think that the novelty of the contributions are clearly presented and that they are not thoroughly evaluated in the experiments.	weakness
2018-88	The authors propose a new variational inference algorithm that handles models with deep neural networks and PGM components.	abstract
2018-88	However, it appears that the authors rely heavily on the work of (Khan & Lin, 2017) that actually provides the algorithm.	weakness
2018-88	As far as I can tell this paper fits inference networks into the algorithm proposed in (Khan & Lin, 2017) which boils down to	weakness
2018-88	i) using an inference network to generate potentials for a conditionally-conjugate distribution and ii) introducing new PGM parameters to decouple the inference network from the model parameters.	weakness
2018-88	These ideas are a clever solution to work inference networks into the message-passing algorithm of (Khan & Lin, 2017),	weakness
2018-88	but I think the authors may be overselling these ideas as a brand new algorithm.	weakness
2018-88	I think if the authors sold the paper as an alternative to (Johnson, et al., 2016)	weakness
2018-88	that doesn't suffer from the implicit gradient problem the paper would fit into the existing literature better.	weakness
2018-88	Another concern that I have is that there are a lot of conditiona-conjugacy assumptions baked into the algorithm that the authors only mention at the end of the presentation of their algorithm.	weakness
2018-88	Additionally, the authors briefly state that they can handle non-conjugate distributions in the model by just using conjugate distributions in the variational approximation.	weakness
2018-88	Though one could do this, the authors do not adequately show that one should, or that one can do this without suffering a lot of error in the posterior approximation.	weakness
2018-88	I think that without an experiment the small section on non-conjugacy should be removed.	weakness
2018-88	Finally, I found the experimental evaluation to not thoroughly demonstrate the advantages and disadvantages of the proposed algorithm.	weakness
2018-88	The algorithm was applied to the two models originally considered in (Johnson, et al., 2016) and the proposed algorithm was shown to attain lower mean-square errors for the two models.	weakness
2018-88	The experiments do not however demonstrate why the algorithm is performing better.	weakness
2018-88	For instance, is the (Johnson, et al., 2016) algorithm suffering from the implicit gradient?	weakness
2018-88	It also would have been great to have considered a model that the (Johnson, et.	suggestion
2018-88	al., 2016) algorithm would not work well on or could not be applied to show the added applicability of the proposed algorithm.	weakness
2018-88	I also have some minor comments on the paper: - There are a lot of typos.	weakness
2018-88	- The first two sentences of the abstract do not really contribute anything to the paper.	weakness
2018-88	What is a powerful model?	weakness
2018-88	What is a powerful algorithm?	weakness
2018-88	- DNN was used in Section 2 without being defined.	weakness
2018-88	- Using p() as an approximate distribution in Section 3 is confusing notation because p() was used for the distributions in the model.	weakness
2018-88	- How is the covariance matrix parameterized that the inference network produces?	weakness
2018-88	- The phrases "first term of the inference network" are not clear.	weakness
2018-88	Just use The DNN term and the PGM term of the inference networks, and better still throw in a reference to Eq. (4).	suggestion
2018-88	- The term "deterministic parameters" was used and never introduced.	weakness
2018-88	- At the bottom of page 5 the extension to the non-conjugate case should be presented somewhere (probably the appendix) since the fact that you can do this is a part of your algorithm that's important.	weakness

2018-89	In this work, the authors suggest the use of control variate schemes for estimating gradient values, within a reinforcement learning  framework.	abstract
2018-89	The authors also introduce a specific control variate technique based on the so-called Stein's identity.	abstract
2018-89	The paper is interesting and well-written.	strength
2018-89	I have some question and some consideration that can be useful for improving the appealing of the paper.	strength
2018-89	- I believe that different Monte Carlo (or Quasi-Monte Carlo) strategies can be applied in order to estimate the integral (expected value) in Eq.	suggestion
2018-89	(1), as also suggested in this work.	suggestion
2018-89	Are there other alternatives in the literature?	suggestion
2018-89	Please, please discuss and cite some papers if required.	misc
2018-89	- I suggest to divide Section 3.1 in two subsections.	suggestion
2018-89	The first one introducing Stein's identity and the related comments that you need, and a second one, starting after Theorem 3.1, with title "Stein Control Variate".	suggestion
2018-89	-  Please also discuss the relationships, connections, and possible applications of your technique to other algorithms used in Bayesian optimization, active learning and/or sequential learning, for instance as	suggestion
2018-89	M. U. Gutmann and J. Corander, "Bayesian optimization for likelihood-free inference of simulator-based statistical mod- els," Journal of Machine Learning Research, vol.	misc
2018-89	16, pp. 4256– 4302, 2015. G. da Silva Ferreira and D.	misc
2018-89	Gamerman, "Optimal design in geostatistics under preferential sampling," Bayesian Analysis, vol.	misc
2018-89	10, no. 3, pp. 711–735, 2015. L. Martino, J. Vicent, G.	misc
2018-89	Camps-Valls, "Automatic Emulator and Optimized Look-up Table Generation for Radiative Transfer Models", IEEE International Geoscience and Remote Sensing Symposium (IGARSS), 2017.	misc
2018-89	-  Please also discuss the dependence of your algorithm with respect to the starting baseline function \\phi_0.	suggestion
2018-89	The paper proposes action-dependent baselines for reducing variance in policy gradient, through the derivation based on Stein's identity and control functionals.	abstract
2018-89	The method relates closely to prior work on action-dependent baselines, but explores in particular on-policy fitting and a few other design choices that empirically improve the performance.	abstract
2018-89	A criticism of the paper is that it does not require Stein's identity/control functionals literature to derive Eq. 8, since it can be derived similarly to linear control variate and it has also previously been discussed in IPG [Gu et.	weakness
2018-89	al., 2017] as reparameterizable control variate.	weakness
2018-89	The derivation through Stein's identity does not seem to provide additional insights/algorithm designs beyond direct derivation through reparameterization trick.	weakness
2018-89	The empirical results appear promising, and in particular in comparison with Q-Prop, which fits Q-function using off-policy TD learning.	strength
2018-89	However, the discussion on the causes of the difference should be elaborated much more, as it appears there are substantial differences besides on-policy/off-policy fitting of the Q, such as: -FitLinear fits linear Q (through parameterization based on linearization of Q) using on-policy learning, rather than fitting nonlinear Q and then at application time linearize around the mean action.	weakness
2018-89	A closer comparison would be to use same locally linear Q function for off-policy learning in Q-Prop.	suggestion
2018-89	-The use of on-policy fitted value baseline within Q-function parameterization during on-policy fitting is nice.	strength
2018-89	Similar comparison should be done with off-policy fitting in Q-Prop.	suggestion
2018-89	I wonder if on-policy fitting of Q can be elaborated more.	suggestion
2018-89	Specifically, on-policy fitting of V seems to require a few design details to have best performance [GAE, Schulman et.	rebuttal_process
2018-89	al., 2016]: fitting on previous batch instead of current batch to avoid overfitting  (this is expected for your method as well, since by fitting to current batch the control variate then depends nontrivially on samples that are being applied), and possible use of trust-region regularization to prevent V from changing too much across iterations.	suggestion
2018-89	The paper presents promising results with direct on-policy fitting of action-dependent baseline, which is promising since it does not require long training iterations as in off-policy fitting in Q-Prop.	strength
2018-89	As discussed above, it is encouraged to elaborate other potential causes that led to performance differences.	suggestion
2018-89	The experimental results are presented well for a range of Mujoco tasks.	suggestion
2018-89	Pros: -Simple, effective method that appears readily available to be incorporated to any on-policy PG methods without significantly increase in computational time	strength
2018-89	-Good empirical evaluation Cons: -The name Stein control variate seems misleading since the algorithm/method does not rely on derivation through Stein's identity etc.	weakness
2018-89	and does not inherit novel insights due to this derivation.	weakness
2018-89	This paper proposed a class of control variate methods based on Stein's identity.	abstract
2018-89	Stein's identity has been widely used in classical statistics and recently in statistical machine learning literature.	abstract
2018-89	Nevertheless, applying Stein's identity to estimating policy gradient is a novel approach in reinforcement learning community.	strength
2018-89	To me, this approach is the right way of constructing control variates for estimating policy gradient.	strength
2018-89	The authors also did a good job in connecting with existing works and gave concrete examples for Gaussian policies.	strength
2018-89	The experimental results also look promising.	strength
2018-89	It would be nice to include some theoretical analyses like under what conditions, the proposed method can achieve smaller sample complexity than existing works.	suggestion
2018-89	Overall this is a strong paper and I recommend to accept.	decision

2018-111	This paper studies the problem of learning one-hidden layer neural networks and is a theory paper.	abstract
2018-111	A well-known problem is that without good initialization, it is not easy to learn the hidden parameters via gradient descent.	abstract
2018-111	This paper establishes an interesting connection between least squares population loss and Hermite polynomials.	abstract
2018-111	Following from this connection authors propose a new loss function.	abstract
2018-111	Interestingly, they are able to show that the loss function globally converges to the hidden weight matrix.	abstract
2018-111	Simulations confirm the findings. Overall, pretty interesting result and solid contribution.	strength
2018-111	The paper also raises good questions for future works.	strength
2018-111	For instance, is designing alternative loss function useful in practice?	strength
2018-111	In summary, I recommend acceptance.	decision
2018-111	The paper seems rushed to me so authors should polish up the paper and fix typos.	weakness
2018-111	Two questions: 1) Authors do not require a^* to recover B^*.	weakness
2018-111	Is that because B^* is assumed to have unit length rows?	weakness
2018-111	If so they should clarify this otherwise it confuses the reader a bit.	weakness
2018-111	2) What can be said about rate of convergence in terms of network parameters?	weakness
2018-111	Currently a generic bound is employed which is not very insightful in my opinion.	weakness
2018-111	[ =========================== REVISION ===============================================================] I am satisfied with the answers to my questions.	weakness
2018-111	The paper still needs some work on clarity, and authors defer the changes to the next version (but as I understood, they did no changes for this paper as of now), which is a bit frustrating.	rebuttal_process
2018-111	However I am fine accepting it.	decision
2018-111	[ ============================== END OF REVISION =====================================================]	misc
2018-111	This paper concerns with addressing the issue of SGD not converging to the optimal parameters on one hidden layer network for a particular type of data and label (gaussian features, label generated using a particular function that should be learnable with neural net).	abstract
2018-111	Authors demonstrate empirically that this particular learning problem is hard for SGD with l2 loss (due to apparently bad local optima) and suggest two ways of addressing it, on top of the known way of dealing with this problem (which is overparameterization).	abstract
2018-111	First is to use a new activation function, the second is by designing a new objective function that has only global optima and which can be efficiently learnt with SGD	abstract
2018-111	Overall the paper is well written.	strength
2018-111	The authors first introduce their suggested loss function and then go into details about what inspired its creation.	strength
2018-111	I do find interesting the formulation of population risk in terms of tensor decomposition, this is insightful	strength
2018-111	My issues with the paper are as follows: - The loss function designed seems overly complicated.	weakness
2018-111	On top of that authors notice that to learn with this loss efficiently, much larger batches had to be used.	weakness
2018-111	I wonder how applicable this in practice - I frankly didn't see insights here that I can apply to other problems that don't fit into this particular narrowly defined framework	weakness
2018-111	- I do find it somewhat strange that no insight to the actual problem is provided (e.g. it is known empirically but there is no explanation of what actually happens and there is a idea that it is due to local optima), but authors are concerned with developing new loss function that has provable properties about global optima.	weakness
2018-111	Since it is all empirical, the first fix (activation function) seems sufficient to me and new loss is very far-fetched.	weakness
2018-111	- It seems that changing activation function from relu to their proposed one fixes the problem without their new loss, so i wonder whether it is a problem with relu itself and may be other activations funcs, like sigmoids will not suffer from the same problem	weakness
2018-111	- No comparison with overparameterization in experiments results is given, which makes me wonder why their method is better.	weakness
2018-111	Minor: fix margins in formula 2.7.	suggestion
2018-111	This paper proposes a tensor factorization-type method for learning one hidden-layer neural network.	abstract
2018-111	The most interesting part is the Hermite polynomial expansion of the activation function.	strength
2018-111	Such a decomposition allows them to convert the population risk function as a fourth-order orthogonal tensor factorization problem.	strength
2018-111	They further redesign a new formulation for the tensor decomposition problem, and show that the new formulation enjoys the nice strict saddle properties as shown in Ge et al. 2015.	abstract
2018-111	At last, they also establish the sample complexity for recovery.	abstract
2018-111	The organization and presentation of the paper need some improvement.	weakness
2018-111	For example, the authors defer many technical details.	weakness
2018-111	To make the paper accessible to the readers, they could provide more intuitions in the first 9 pages.	suggestion
2018-111	There are also some typos: For example, the dimension of a is inconsistent.	weakness
2018-111	In the abstract, a is an m-dimensional vector, and on Page 2, a is a d-dimensional vector.	weakness
2018-111	On Page 8, P(B) should be a degree-4 polynomial of B.	weakness
2018-111	The paper does not contains any experimental results on real data.	weakness

2018-117	In this paper, the authors propose a novel method for generating adversarial examples when the model is a black-box and we only have access to its decisions (and a positive example).	abstract
2018-117	It iteratively takes steps along the decision boundary while trying to minimize the distance to the original positive example.	abstract
2018-117	Pros: - Novel method that works under much stricter and more realistic assumptions.	strength
2018-117	- Fairly thorough evaluation. - The paper is clearly written.	strength
2018-117	Cons: - Need a fair number of calls to generate a small perturbation.	weakness
2018-117	Would like to see more analysis of this.	weakness
2018-117	- Attack works for making something outside the boundary (not X), but is less clear how to generate image to meet a specific classification (X).	weakness
2018-117	3.2 attempts this slightly by using an image in the class, but is less clear for something like FaceID.	weakness
2018-117	- Unclear how often the images generated look reasonable.	weakness
2018-117	Do different random initializations given different quality examples?	weakness
2018-117	The authors identify a new security threat for deep learning: Decision-based adversarial attacks.	abstract
2018-117	This new class of attacks on deep learning systems requires from an attacker only the knowledge of class labels (previous attacks required more information, e.g., access to a gradient oracle).	abstract
2018-117	Unsurprisingly, since the attacker has so few information, such kind of attacks involves quite a lot trial and error.	weakness
2018-117	The authors propose one specific attack instance out of this class of attacks.	abstract
2018-117	It works as follows. First, an initial point outside of the benign region is guessed.	abstract
2018-117	Then multiple steps towards the decision boundary is taken, finally reaching the boundary (I am not sure about the precise implementation, but it seems not crucial; the author may please check whether their description of the algorithm is really reproducable).	abstract
2018-117	Then, in a nutshell, a random walk on a sphere centered around the original, benign point is performed, where after each step, the radius of the sphere is slightly reduced (drawing the point closer to the original point), if and only if the resulting point still is outside of the benign region.	abstract
2018-117	The algorithm is evaluated on the following datasets: MNIST, CIFAR, VGG19, ResNet50, and InceptionV3.	abstract
2018-117	The paper is rather well written and structured.	strength
2018-117	The text was easy to follow.	strength
2018-117	I suggest that a self-contained description of the problem setting (assumptions on attacker and defender; aim?) shall be added to the camera-ready version (being not familiar with the area, I had to read a couple of papers to get a feeling for the setting, before reviewing this paper).	suggestion
2018-117	As in many DL papers these days, there really isn't any math in it worth a mention; so no reason here to say anything about mathematical soundness.	weakness
2018-117	The authors employ a reasonable evaluation criterion in their experiments: the median squared Euclidean distance between the original and adversarially modified data point.	weakness
2018-117	The results show consistent improvement for most data sets.	weakness
2018-117	In summary, this is an innovative paper, proposing a new class of attacks that totally makes sense in my opinion.	strength
2018-117	Apart from some minor weaknesses in the presentation that can be easily fixed for the camera ready, this is a nice, fresh paper, that might spur more attacks (and of course new defenses) from the new class of decision-based attacks.	strength
2018-117	It is worth to note that the authors show that distillation is not a useful defense against such attacks, so we may expect follow-up proposing useful defenses against the new attack (which BTW is shown to be about a factor of 10 in terms of iterations more costly than the SOTA).	strength
2018-117	This is a nice paper proposing a simple but effective heuristic for generating adversarial examples from class labels with no gradient information or class probabilities.	abstract
2018-117	Highly relevant prior work was overlooked and there is no theoretical analysis, but I think this paper still makes a valuable contribution worth sharing with a broader audience.	strength
2018-117	What this paper does well: - Suggests a type of attack that hasn't been applied to image classifiers	strength
2018-117	- Proposes a simple heuristic method for performing this attack	strength
2018-117	- Evaluates the attack on both benchmark neural networks and a commercial system	strength
2018-117	Problems and limitations: 1. No theoretical analysis.	weakness
2018-117	Under what conditions does the boundary attack succeed or fail?	weakness
2018-117	What geometry of the classification boundaries is necessary?	weakness
2018-117	How likely are those conditions to hold?	weakness
2018-117	Can we measure how well they hold on particular networks?	weakness
2018-117	Since there is no theoretically analysis, the evidence for effectiveness is entirely empirical.	weakness
2018-117	That weakens the paper and suggests an important area of future work, but I think the empirical evidence is sufficient to show that there's something interesting going.	ac_disagreement
2018-117	Not a fatal flaw. 2. Poor framing.	weakness
2018-117	The paper frames the problem in terms of "machine learning models" in general (beginning with the first line of the abstract), but it only investigates image classification.	weakness
2018-117	There's no particular reason to believe that all machine learning algorithms will behave like convolutional neural network image classifiers.	weakness
2018-117	Thus, there's an implicit claim of generality that is not supported.	weakness
2018-117	This is a presentation issue that is easily fixed.	strength
2018-117	I suggest changing the title to reflect this, or at least revising the abstract and introduction to make the scope clearer.	suggestion
2018-117	A minor presentation quibble/suggestion: "adversarial" is used in this paper to refer to any class that differs from the true class of the instance to be disguised.	weakness
2018-117	But an image of a dalmation that's labeled as a dalmation isn't adversarial -- it's just a different image that's labeled correctly.	weakness
2018-117	The adversarial process is about constructing something that will be mislabeled, exploiting some kind of weakness that doesn't show up on a natural distribution of inputs.	weakness
2018-117	I suggest rewording some of the mentions of adversarial.	suggestion
2018-117	3. Ignorance of prior work.	weakness
2018-117	Finding deceptive inputs using only the classifier output has been done by Lowd and Meek (KDD 2005) for linear classifiers and Nelson et al. (AISTATS 2010, JMLR 2012) for convex-inducing classifiers.	misc
2018-117	Both works include theoretical bounds on the number of queries required for near-optimal adversarial examples.	abstract
2018-117	Biggio et al. (ECML 2013) further propose training a surrogate classifier on similar training data, using the predictions of the target classifier to relabel the training data.	abstract
2018-117	In this way, decision information from the target model is used to help train a more similar surrogate, and then attacks can be transferred from the surrogate to the target.	abstract
2018-117	Thus, "decision-based attacks" are not new, although the algorithm and experiments in this paper are.	abstract
2018-117	Overall, I think this paper makes a worthwhile contribution, but needs to revise the claims to match what's done in the paper and what's been done before.	weakness

2018-143	The work claims a measure of robustness of networks that is attack-agnostic.	abstract
2018-143	Robustness measure is turned into the problem of finding a local Lipschitz constant which is given by the maximum of the norm of the gradient of the associated function.	abstract
2018-143	That quantity is then estimated by sampling from the domain of maximization and observing the maximum value of the norm out of those samples.	abstract
2018-143	Such a maximum process is then described by the reverse Weibull distribution which is used in the estimation.	abstract
2018-143	The paper closely follows Hein and Andriushchenko (2017).	misc
2018-143	There is a slight modification that enlarges the class of functions for which the theory is applicable (Lemma 3.3).	weakness
2018-143	As far as I know, the contribution of the work starts in Section 4 where the authors show how to practically estimate the maximum process through back-prop where mini-batching helps increase the number of samples.	weakness
2018-143	This is a rather simple idea that is shown to be effective in Figure 3.	strength
2018-143	The following section (the part starting from 5.3) presents the key to the success of the proposed measure.	strength
2018-143	This is an important problem and the paper attempts to tackle it in a computationally efficient way.	strength
2018-143	The fact that the norms of attacks are slightly above the proposed score is promising, however, there is always the risk of finding a lower bound that is too small (zeros and large gaps in Figure 3).	strength
2018-143	It would be nice to be able to show that one can find corresponding attacks that are not too far away from the proposed score.	suggestion
2018-143	Finally, a minor point: Definition 3.1 has a confusing notation, f is a K-valued vector throughout the paper but it also denotes the number that represents the prediction in Definition 3.1.	weakness
2018-143	I believe this is just a typo.	weakness
2018-143	Edit: Thanks for the fixes and clarification of essential parts in the paper.	misc
2018-143	Summary ======== The authors present CLEVER, an algorithm which consists in evaluating the (local) Lipschitz constant of a trained network around a data point.	abstract
2018-143	This is used to compute a lower-bound on the minimal perturbation of the data point needed to fool the network.	abstract
2018-143	The method proposed in the paper already exists for classical function, they only transpose it to neural networks.	abstract
2018-143	Moreover, the lower bound comes from basic results in the analysis of Lipschitz continuous functions.	abstract
2018-143	Clarity ===== The paper is clear and well-written.	strength
2018-143	Originality ========= This idea is not new: if we search for "Lipschitz constant estimation" in google scholar, we get for example	weakness
2018-143	Wood, G. R., and B. P. Zhang.	misc
2018-143	"Estimation of the Lipschitz constant of a function." (1996)	misc
2018-143	which presents a similar algorithm (i.e., estimation of the maximum slope with reverse Weibull).	weakness
2018-143	Technical quality ============== The main theoretical result in the paper is the analysis of the lower-bound on \\delta, the smallest perturbation to apply on a data point to fool the network.	weakness
2018-143	This result is obtained almost directly by writing the bound on Lipschitz-continuous function	weakness
2018-143	| f(y)-f(x) | < L || y-x ||	weakness
2018-143	where x = x_0 and y = x_0 + \\delta.	weakness
2018-143	Comments: - Lemma 3.1: why citing Paulavicius and Zilinskas for the definition of Lipschitz continuity?	weakness
2018-143	Moreover, a Lipschitz-continuous function does not need to be differentiable at all (e.g. |x| is Lipschitz with constant 1 but sharp at x=0).	weakness
2018-143	Indeed, this constant can be easier obtained if the gradient exists, but this is not a requirement.	weakness
2018-143	- (Flaw?) Theorem 3.2 : This theorem works for fixed target-class since g = f_c - f_j for fixed g.	weakness
2018-143	However, once g = min_j f_c - f_j, this theorem is not clear with the constant Lq. Indeed, the function g should be g(x) = min_{k \\neq c} f_c(x) - f_k(x).	weakness
2018-143	Thus its Lipschitz constant is different, potentially equal to	weakness
2018-143	L_q = max_{k} \\| L_q^k \\|,	weakness
2018-143	where L_q^k is the Lipschitz constant of f_c-f_k.	weakness
2018-143	If the theorem remains unchanged after this modification, you should clarify the proof.	weakness
2018-143	Otherwise, the theorem will work with the maximum over all Lipschitz constants but the theoretical result will be weakened.	weakness
2018-143	- Theorem 4.1: I do not see the purpose of this result in this paper.	weakness
2018-143	This should be better motivated.	weakness
2018-143	Numerical experiments ==================== Globally, the numerical experiments are in favor of the presented method.	weakness
2018-143	The authors should also add information about the time it takes to compute the bound, the evolution of the bound in function of the number of samples and the distribution of the relative gap between the lower-bound and the best adversarial example.	suggestion
2018-143	Moreover, the numerical experiments look to be realized in the context of targeted attack.	weakness
2018-143	To show the real effectiveness of the approach, the authors should also show the effectiveness of the lower-bound in the context of non-targeted attack.	suggestion
2018-143	####################################################### Post-rebuttal review --------------------------- Given the details the authors provided to my review, I decided to adjust my score.	misc
2018-143	The method is simple and shows to be extremely effective/accurate in practice.	strength
2018-143	Detailed answers: 1) Indeed, I was not aware that the paper only focuses on one dimensional functions.	weakness
2018-143	However, they still work with less assumption, i.e., with no differential functions.	weakness
2018-143	I was pointing out the similarities between their approach and your: the two algorithms (CLEVER and Slope) are basically the same, and using a limit you can go from "slope" to "gradient norm".	weakness
2018-143	In any case, I have read the revision and the additional numerical experiment to compare Clever with their method is a good point.	misc
2018-143	2) " Overall, our analysis is simple and more intuitive, and we further facilitate numerical calculation of the bound by applying the extreme value theory in this work.	strength
2018-143	This is right. I am just surprised is has not been done before, since it requires only few lines of derivation.	strength
2018-143	I searched a bit but it is not possible to find any kind of similar results.	weakness
2018-143	Moreover, this leads to good performances, so there is no needs to have something more complex.	strength
2018-143	3) "The usual Lipschitz continuity is defined in terms of L2 norm and the extension to an arbitrary Lp norm is not straightforward"	weakness
2018-143	Indeed, people usually use the Lipschitz continuity using the L2norm, but the original definition is wider.	weakness
2018-143	Quickly, if you have a differential, scalar function from a space E -> R, then the gradient is a function from space E to E*, the dual of the space E.	weakness
2018-143	Let || . || the norm of space E.	weakness
2018-143	Then, || . ||* is the dual norm of ||.||, and also the norm of E*.	weakness
2018-143	In that case, Lipschitz continuity writes f(x)-f(y) <= L || x-y ||, with L >= max_{x in E*} || f'(x) ||*	weakness
2018-143	In the case where || .	weakness
2018-143	|| is an \\ell-p norm, then || .	weakness
2018-143	||* is an \\ell-q norm; with 1/p+1/q = 1.	weakness
2018-143	If you are interested, there is a clear and concise explanation in the introduction of this paper: Accelerating the cubic regularization of Newton's method on convex problems, by Yurii Nesterov.	strength
2018-143	I have no additional remarks for 4) -> 9), since everything is fixed in the new version of the paper.	strength
2018-143	In this work, the objective is to analyze the robustness of a neural network to any sort of attack.	abstract
2018-143	This is measured by naturally linking the robustness of the network to the local Lipschitz properties of the network function.	abstract
2018-143	This approach is quite standard in learning theory, I am not aware of how original this point of view is within the deep learning community.	weakness
2018-143	This is estimated by obtaining values of the norm of the gradient (also naturally linked to the Lipschitz properties of the function) by backpropagation.	weakness
2018-143	This is again a natural idea.	strength

2018-156	I read the rebuttal and thank the authors for the thoughtful responses and revisions.	misc
2018-156	The updated Figure 2 and Section 4.4.	rebuttal_process
2018-156	addresses my primary concerns. Upwardly revising my review. ====================	rebuttal_process
2018-156	The authors describe a method for detecting adversarial examples by measuring the likelihood in terms of a generative model of an image.	abstract
2018-156	Furthermore, the authors prescribe a method for cleaning or 'santizing' an adversarial image through employing a generative model.	abstract
2018-156	The authors demonstrate some success in restoring images that have been adversarially perturbed with this technique.	abstract
2018-156	The idea of using a generative model (PixelCNN) to assess whether a given image has been adversarially perturbed is a very interesting and understandable finding that may contribute quite nicely to the adversarial literature.	strength
2018-156	One limitation of this method, however, is our ability to build successful generative models for high resolution images.	weakness
2018-156	However, I would be curious to know if the authors tried their method on high resolution images, regardless?	suggestion
2018-156	Major comments: 1) Cross validation. Figure 2a is quite interesting and compelling.	strength
2018-156	It is not clear from the figure if the 'clean' (nor the other data for that matter) is from the *training* or  *testing* data for the PixelCNN model.	weakness
2018-156	I would *hope* that this is from the *testing* data indicating that these are the likelihood on unseen images?	weakness
2018-156	That said, it would be interesting to see the *training* data on this plot as well to see if there are any systematic shifts that might make the distribution of adversarial examples less discernible.	suggestion
2018-156	2) Adversary to PixelCNN. It is not clear why a PixelCNN may not be adversarially attacked, nor if such a model would be able to guard against an adversarial attack.	weakness
2018-156	I am not sure how well viable of strategy this may be but it is worth understanding or addressing to determine how viable this method for guarding actually is.	weakness
2018-156	3) Restorative effects of PixelDefend.	weakness
2018-156	I would like to see individual examples of (a) adversarial perturbation for a given image and (b) PixelDefend perturbation for that adversarial image.	suggestion
2018-156	In particular, I would like to see how close (a) is the negative of (b).	suggestion
2018-156	This would give me more confidence that this techniques is successfully guarding against the original attack.	suggestion
2018-156	I am willing to adjust my rating upward if the authors are able to address some of the points above in a substantive manner.	misc
2018-156	The paper describes the creative application of a density estimation model to clean up adversarial examples before applying and image model (for classification, in this setup).	abstract
2018-156	The basic idea is that the image is first moved back to the probable region of images before applying the classifier.	abstract
2018-156	For images, the successful PiexlCNN model is used as a density estimator and is applied to clean up the image before the classification is attempted.	abstract
2018-156	The proposed method is very intuitive, but might be expensive if a naive implementation of PixelCNN is used for the cleaning.	weakness
2018-156	The approach is novel. It is useful that the density estimator model does not have to rely on the labels.	strength
2018-156	Also, it might even be trained on a different dataset potentially.	strength
2018-156	The con is that the proposed methodology still does not solve the problem of adversarial examples completely.	weakness
2018-156	Minor nitpick: In section 2.1, it is suggested that DeepFool was the first optimization based attack to minimize the perturbation wrt the original image.	weakness
2018-156	In fact the much earler (2013) "Intriguing Propoerties ...	weakness
2018-156	" paper relied on the same formulation (minimizing perturbation under several constraints: changed detection and pixel intensities are being in the given range).	weakness
2018-156	The authors propose to use a generative model of images to detect and defend against adverarial examples.	weakness
2018-156	White-box attacks against standard models for image recognition (Resnet and VGG) are considered, and a generative model (a PixelCNN) is trained on the same data as the classifiers.	abstract
2018-156	The authors first show that adversarial examples created by the white-box attacks correspond to low likelihood region (according to the pixelCNN), which first gives a classification rule for detecting adversarial examples.	abstract
2018-156	Then, to turn the genrative model into a defensive algorithm, the authors propose to preprocess test images by approximately maximizing the likelihood under similar constraints as the attacker of images, to "project" adversarial examples back to high-density regions (as estimated by the generative model).	abstract
2018-156	As a heuristic method, the authors propose to greedily maximize the likelihood of the incoming images pixel-by-pixel, which is possible because of the specific form of the PixelCNN likelihood in the context of l-infty attacks.	abstract
2018-156	An "adaptive" version of the algorithm, in which the preprocessing is used only when the likelihood of an example is below a certain threshold, is also proposed.	abstract
2018-156	Experiments are carried out on Fashion MNIST and CIFAR-10.	abstract
2018-156	At a high level, the message is that projecting the image into a high density region is sufficient to correct for a significant portions of the mistakes made on adversarial examples.	abstract
2018-156	The main result is that this approach based on generative models seems to work even on against the strongest attacks.	abstract
2018-156	Overall, the idea proposed in the paper, using a generative model to detect and filter out spurious patterns that can appear in adversarial examples, is rather intuitive.	strength
2018-156	The experimental result that adversarial examples can somehow be corrected by a generative model is also interesting.	strength
2018-156	The design choice of PixelCNN, which allows for a greedy optimization seems reasonable in that setting.	strength
2018-156	Whereas the paper is an interesting step forward, the paper still doesn't provide definitive arguments in favor of using such approaches in practice.	weakness
2018-156	There is a significant loss in accuracy on clean examples (2% on CIFAR-10 for a resnet), and more generally against weaker opponents such as the fast gradient sign.	weakness
2018-156	Thus, in reality, the experiments show that the pipeline generative model + classifier is robust against the strongest white box methods for this classifier, but on the other hand these methods do not transfer well to new models.	weakness
2018-156	This somewhat weakens the result, since robustness against these methods that do not transfer well is achieved by changing the model.	weakness

2018-161	tl;dr: - The paper has a really cool theoretical contribution.	strength
2018-161	- The experiments do not directly test whether the theoretical insight holds in practice, but instead a derivate method is tested on various benchmarks.	weakness
2018-161	I must say that this paper has cleared up quite a few things for me.	rebuttal_process
2018-161	I have always been a skeptic wrt LSTM, since I myself did not fully understand when to prefer them over vanilla RNNs for reasons other than "they empirically work much better in many domains." and "they are less prone to vanishing gradients".	weakness
2018-161	Section 1 is a bliss: it provides a very useful candidate explanation under which conditions vanilla RNNs fail (or at least, do not efficiently generalise) in contrast to gated cells.	strength
2018-161	I am sincerely happy about the write up and will point many people to it.	misc
2018-161	The major problem with the paper, in my eyes, is the lack of experiments specific to test the hypothesis.	weakness
2018-161	Obviously, quite a bit of effort has gone into the experimental section.	misc
2018-161	The focus however is comparison to the state of the art in terms of raw performance.	weakness
2018-161	That leaves me asking: are gated RNNs superior to vanilla RNNs if the data is warped?	weakness
2018-161	Well, I don't know now.	weakness
2018-161	I only can say that there is reason to believe so.	misc
2018-161	I *really* do encourage the authors to go back to the experiments and see if they can come up with an experiment to test the main hypothesis of the paper.	suggestion
2018-161	E.g. one could make synthetic warpings, apply it to any data set and test if things work out as expected.	suggestion
2018-161	Such a result would in my opinion be of much more use than the tiny increment in performance that is the main output of the paper as of now, and which will be stomped by some other trick in the months to come.	suggestion
2018-161	It would be a shame if such a nice theoretical insight got under the carpet because of that.	misc
2018-161	E.g. today we hold [Pascanu 2013] dear not because of the proposed method, but because of the theoretical analysis.	weakness
2018-161	Some minor points. - The authors could make use of less footnotes, and try to incorporate them into the text or appendix.	weakness
2018-161	- A table of results would be nice.	weakness
2018-161	- Some choices of the experimental section seem arbitrary, e.g. the use of optimiser and to not use clipping of gradients.	weakness
2018-161	In general, the evaluation of the hyper parameters is not rigorous.	weakness
2018-161	- "abruplty" -> "abruptly" on page 5, 2nd paragraph	weakness
2018-161	### References [Pascanu 2013] Pascanu, Razvan, Tomas Mikolov, and Yoshua Bengio.	misc
2018-161	"On the difficulty of training recurrent neural networks." International Conference on Machine Learning.	misc
2018-161	2013. Summary: This paper shows that incorporating invariance to time transformations in recurrent networks naturally results in a gating mechanism used by LSTMs and their variants.	abstract
2018-161	This is then used to develop a simple bias initialization scheme for the gates when the range of temporal dependencies relevant for a problem can be estimated or are known.	abstract
2018-161	Experiments demonstrate that the proposed initialization speeds up learning on synthetic tasks, although benefits for next-step prediction tasks are limited.	abstract
2018-161	Quality and significance: The core insight of the paper is the link between recurrent network design and its effect on how the network reacts to time transformations.	strength
2018-161	This insight is simple, elegant and valuable in my opinion.	strength
2018-161	It is becoming increasingly apparent recently that the benefits of the gating and cell mechanisms introduced by the LSTM, now also used in feedforward networks, go beyond avoiding vanishing gradients.	strength
2018-161	The particular structural elements also induce certain inductive biases which make learning or generalization easier in many cases.	strength
2018-161	Understanding the link between model architecture and behavior is very useful for the field in general, and this paper contributes to this knowledge.	strength
2018-161	In light of this, I think it is reasonable to ignore the fact that the proposed initialization does not provide benefits on Penn Treebank and text8.	strength
2018-161	The real value of the paper is in providing an alternative way of thinking about LSTMs that is theoretically sound and intuitive.	strength
2018-161	Clarity: The paper is well-written in general and easy to understand.	strength
2018-161	A minor complaint is that there are an unnecessarily large number of paragraph breaks, especially on pages 3 and 4, which make reading slightly jarring.	weakness
2018-161	The paper provides an interesting theoretical explanation on why gated RNN architectures such as LSTM and GRU work well in practice.	abstract
2018-161	The paper shows how "gate values appear as time contraction or time dilation coefficients".	abstract
2018-161	The authors also point out the connection between the gate biases and the range of time dependencies captured in the network.	abstract
2018-161	From that, they develop a simple yet effective initialization method which performs well on different datasets.	abstract
2018-161	Pros: - The idea is interesting, it well explain the success of gated RNNs.	strength
2018-161	- Writing: The paper is well written and easy to read.	strength
2018-161	Cons: Experiments: only small datasets were used in the experiments, it would be more convincing if the author could use larger datasets.	weakness
2018-161	One suggestion to make the experiment more complete is to gradually increase the initial value of the biases to see how it affect the performance.	suggestion
2018-161	To use 'chrono initialization', one need to estimate the range of time dependency which could be difficult in practice.	suggestion

2018-191	This paper presents two complementary models for unsupervised domain adaptation (classification task): 1) the Virtual Adversarial Domain Adaptation (VADA) and 2) the Decision-boundary Iterative Refinement Training with a Teacher (DIRT-T).	abstract
2018-191	The authors make use of the so-called cluster assumption, i.e., decision boundaries should not cross high-density data regions.	abstract
2018-191	VADA extends the standard Domain-Adversarial training by introducing an additional objective L_t that measures the target-side cluster assumption violation, namely, the conditional entropy w.r.t. the target distribution.	abstract
2018-191	Since the empirical estimate of the conditional entropy breaks down for non-locally-Lipschitz classifiers, the authors also propose to incorporate virtual adversarial training in order to make the classifier well-behaved.	abstract
2018-191	The paper also argues that the performance on the target domain can be further improved by a post-hoc minimization of L_t using natural gradient descent (DIRT-T) which ensures that the decision boundary changes incrementally and slowly.	abstract
2018-191	Pros: + The paper is written clearly and easy to read	strength
2018-191	+ The idea to keep the decision boundary in the low-density region of the target domain makes sense	strength
2018-191	+ The both proposed methods seem to be quite easy to implement and incorporate into existing DATNN-based frameworks	strength
2018-191	+ The combination of VADA and DIRT-T performs better than existing DA algorithms on a range of visual DA benchmarks	strength
2018-191	Cons: - Table 1 can be a bit misleading as the performance improvements may be partially attributed to the fact that different methods employ different base NN architectures and different optimizers	weakness
2018-191	- The paper deals exclusively with visual domains; applying the proposed methods to other modalities would make this submission stronger	weakness
2018-191	Overall, I think it is a good paper and deserves to be accepted to the conference.	decision
2018-191	I'm especially appealed by the fact that the ideas presented in this work, despite being simple, demonstrate excellent performance.	strength
2018-191	Post-rebuttal revision: After reading the authors' response to my review, I decided to leave the score as is.	rebuttal_process
2018-191	The paper was a good contribution to domain adaptation.	strength
2018-191	It provided a new way of looking at the problem by using the cluster assumption.	strength
2018-191	The experimental evaluation was very thorough and shows that VADA and DIRT-T performs really well.	strength
2018-191	I found the math to be a bit problematic.	weakness
2018-191	For example, L_d in (4) involves a max operator.	weakness
2018-191	Although I understand what the authors mean, I don't think this is the correct way to write this.	weakness
2018-191	(5) should discuss the min-max objective.	weakness
2018-191	This will probably involve an explanation of the gradient reversal etc.	suggestion
2018-191	Speaking of GRL, it's mentioned on p.6 that they replaced GRL with the traditional GAN objective.	weakness
2018-191	This is actually pretty important to discuss in detail: did that change the symmetric nature of domain-adversarial training to the asymmetric nature of traditional GAN training?	suggestion
2018-191	Why was that important to the authors?	suggestion
2018-191	The literature review could also include Shrivastava et al. and Bousmalis et al. from CVPR 2017.	suggestion
2018-191	The latter also had MNIST/MNIST-M experiments.	ac_disagreement
2018-191	As there are many kinds of domain adaptation problems, the need to mix several learning strategies to improve the existing approaches is obvious.	strength
2018-191	However, this task is not necessarily easy to succeed.	weakness
2018-191	The authors proposed a sound approach to learn a proper representation (in an adversarial way) and comply the cluster assumption.	abstract
2018-191	The experiments show that this Virtual Adversarial Domain Adaptation network (VADA) achieves great results when compared to existing learning algorithms.	abstract
2018-191	Moreover, we also see the learned model is consistently improved using the proposed "Decision-boundary Iterative Refinement Training with a Teacher" (DIRT-T) approach.	abstract
2018-191	The proposed methodology relies on multiple choices that could sometimes be better studied and/or explained.	abstract
2018-191	Namely, I would like to empirically see which role of the locally-Lipschitz regularization term (Equation 7).	suggestion
2018-191	Also, I wonder why this term is tuned by an hyperparameter (lamda_s) for the source, while a single hyperparamer (lambda_t) is used for the sum of the two target quantity.	abstract
2018-191	On the theoretical side, the discussion could be improved.	weakness
2018-191	Namely, Section 3 about "limitation of domain adversarial training" correctly explained that "domain adversarial training may not be sufficient for domain adaptation if the feature extraction function has high-capacity".	weakness
2018-191	It would be interesting to explain whether this observation is consistent with Theorem 1 of the paper (due to Ben-David et al., 2010), on which several domain adversarial approaches are based.	suggestion
2018-191	The need to consider supplementary assumptions (such as ) to achieve good adaptation can also be studied through the lens of more recent Ben-David's work, e.g. Ben-David and Urner (2014).	suggestion
2018-191	In the latter, the notion of "Probabilistic Lipschitzness", which is a relaxation of the "cluster assumption" seems very related to the actual work.	suggestion
2018-191	Reference: Ben-David and Urner. Domain adaptation-can quantity compensate for quality?, Ann. Math.	misc
2018-191	Artif. Intell., 2014 Pros: - Propose a sound approach to mix two complementary strategies for domain adaptation.	strength
2018-191	- Great empirical results. Cons: - Some choices leading to the optimization problem are not sufficiently explained.	weakness
2018-191	- The theoretical discussion could be improved.	weakness
2018-191	Typos: - Equation 14: In the first term (target loss), theta should have an index t (I think).	weakness
2018-191	- Bottom of page 6: "...	weakness
2018-191	and that as our validation set" (missing word).	weakness

2018-205	This paper proposes a small modification to the monotonic attention in [1] by adding a soft attention to the segment predicted by the monotonic attention.	abstract
2018-205	The paper is very well written and easy to follow.	strength
2018-205	The experiments are also convincing.	strength
2018-205	Here are a few suggestions and questions to make the paper stronger.	misc
2018-205	The first set of questions is about the monotonic attention.	rebuttal_process
2018-205	Training the monotonic attention with expected context vectors is intuitive, but can this be justified further?	weakness
2018-205	For example, how far does using the expected context vector deviate from marginalizing the monotonic attention?	weakness
2018-205	The greedy step, described in the first paragraph of page 4, also has an effect on the produced attention.	weakness
2018-205	How does the greedy step affect training and decoding?	suggestion
2018-205	It is also unclear how tricks in the paragraph above section 2.4 affect training and decoding.	weakness
2018-205	These questions should really be answered in [1].	suggestion
2018-205	Since the authors are extending their work and since these issues might cause training difficulties, it might be useful to look into these design choices.	suggestion
2018-205	The second question is about the window size w.	weakness
2018-205	Instead of imposing a fixed window size, which might not make sense for tasks with varying length segments such as the two in the paper, why not attend to the entire segment, i.e., from the current boundary to the previous boundary?	suggestion
2018-205	It is pretty clear that the model is discovering the boundaries in the utterance shown in Figure 2.	suggestion
2018-205	(The spectrogram can be made more visible by removing the delta and delta-delta in the last subplot.) How does the MoCha attention look like for words whose orthography is very nonphonemic, for example, AAA and WWW?	suggestion
2018-205	For the experiments, it is intriguing to see that w=2 works best for speech recognition.	strength
2018-205	If that's the case, would it be easier to double the hidden layer size and use the vanilla monotonic attention?	suggestion
2018-205	The latter should be a special case of the former, and in general you can always increase the size of the hidden layer to incorporate the windowed information.	suggestion
2018-205	Would the special cases lead to worse performance and if so why is there a difference?	rebuttal_process
2018-205	[1] C Raffel, M Luong, P Liu, R Weiss, D Eck, Online and linear-time attention by enforcing monotonic alignments, 2017 The paper proposes an extension to a previous monotonic attention model (Raffel et al 2017) to attend to a fixed-sized window up to the alignment position.	abstract
2018-205	Both the soft attention approximation used for training the monotonic attention model, and the online decoding algorithm is extended to the chunkwise model.	abstract
2018-205	In terms of the model this is a relatively small extention of Raffel et al 2017.	abstract
2018-205	Results show that for online speech recognition the model matches the performance of an offline soft attention baseline, doing significantly better than the monotonic attention model.	abstract
2018-205	Is the offline attention baseline unidirectional or bidirectional?	abstract
2018-205	In case it is unidirectional it cannot really be claimed that the proposed model's performance is competitive with an offline model.	abstract
2018-205	My concern with the statement that all hyper-parameters are kept the same as the monotonic model is that the improvement might partly be due to the increase in total number of parameters in the model.	weakness
2018-205	Especially given that w=2 works best for speech recognition, it not clear that the model extension is actually helping.	weakness
2018-205	My other concern is that in speech recognition the time-scale of the encoding is somewhat arbitrary, so possibly a similar effect could be obtained by doubling the time frame through the convolutional layer.	weakness
2018-205	While the empirical result is strong it is not clear that the proposed model is the best way to obtain the improvement.	weakness
2018-205	For document summarization the paper presents a strong result for an online model, but the fact that it is still less accurate than the soft attention baseline makes it hard to see the real significance of this.	weakness
2018-205	If the contribution is in terms of speed (as shown with the synthetic benchmark in appendix B) more emphesis should be placed on this in the paper.	weakness
2018-205	Sentence summarization tasks do exhibit mostly monotonic alignment, and most previous models with monotonic structure were evaluated on that, so why not test that here?	weakness
2018-205	I like the fact that the model is truely online, but that contribution was made by Raffel et al 2017, and this paper at best proposes a slightly better way to train and apply that model.	weakness
2018-205	--- The additional experiments in the new version gives stronger support in favour of the proposed model architecture (vs the effect of hyperparameter choices).	weakness
2018-205	While I'm still on the fence on whether this paper is strong enough to be accepted for *CONF*, this version is certainly improves the quality of the paper.	decision
2018-205	This paper extends a previously proposed monotonic alignment based attention mechanism by considering local soft alignment across features in a chunk (certain window).	abstract
2018-205	Pros. - the paper is clearly written.	strength
2018-205	- the proposed method is applied to several sequence-to-sequence benchmarks, and the paper show the effectiveness of the proposed method (comparable to full attention and better than previous hard monotonic assignments).	strength
2018-205	Cons. - in terms of the originality, the methodology of this method is rather incremental from the prior study (Raffel et al), but it shows significant gains from it.	weakness
2018-205	- in terms of considering a monotonic alignment, Hori et al, "Advances in Joint CTC-Attention based End-to-End Speech Recognition with a Deep CNN Encoder and RNN-LM," in Interspeech'17, also tries to solve this issue by combining CTC and attention-based methods.	weakness
2018-205	The paper should also discuss this method in Section 4.	suggestion
2018-205	Comments: - Eq. (16): j in the denominator should be tj.	weakness

2018-220	This is a well-written paper that proposes regularization and optimization strategies for word-based language modeling tasks.	abstract
2018-220	The authors propose the use of DropConnect  on the hidden-hidden connections as a regularization method, in order to take advantage of high-speed LSTM implementations via the cuDNN LSTM libraries from NVIDIA.	abstract
2018-220	The  focus of this work is on the prevention of overfitting on the recurrent connections of the LSTM.	abstract
2018-220	The authors explore a variant of Average-SGD (NT-ASGD) as an optimization strategy which eliminates the need for tuning the average trigger and uses a constant learning rate.	abstract
2018-220	Averaging is triggered when the validation loss worsens or stagnates for a few cycles, leading to two new hyper parameters: logging interval and non-monotone interval.	abstract
2018-220	Other forms of well-know regularization methods were applied to the non-recurrent connections, input, output and embedding matrices.	abstract
2018-220	As the authors point out, all the methods used in this paper have been proposed before and theoretical convergence explained.	abstract
2018-220	The novelty of this work lies in its successful application to the language modeling task achieving state-of-the-art results.	strength
2018-220	On the PTB task, the proposed AWD-LSTM achieves a perplexity of 57.3 vs 58.3 (Melis et al 2017) and almost the same perplexity as Melis et el.	strength
2018-220	on the Wiki-Text2 task (65.8 vs 65.9).	strength
2018-220	The addition of a cache model provides significant gains on both tasks.	strength
2018-220	It would be useful, if authors had explored the behavior of the  AWD-LSTM algorithm with respect to various hyper parameters  and provided a few insights towards their choices for other large vocabulary language modeling tasks (1 million vocabulary sizes).	suggestion
2018-220	Similarly, the choice of the average trigger and number of cycles seem arbitrary -  it would have been good to see a graph over a range of values, showing their impact on the model's performance.	weakness
2018-220	A 3-layer LSTM has been used for the experiments  - how was this choice made?	weakness
2018-220	What is the impact of this algorithm if the net was a 2-layer net as is typical in most large-scale LMs?	weakness
2018-220	Table 3 is interesting to see how the cache model helps with rare words  and as such has applications in key word spotting tasks.	weakness
2018-220	Were the hyper parameters of the cache tuned to perform better on rare words?	weakness
2018-220	More details on the design of the cache model would have been useful.	weakness
2018-220	You state that the gains obtained using the cache model were far less than what was obtained in Graves et al 2016 - what do you attribute this to?	weakness
2018-220	Ablation analysis in Table 4 is very useful - in particular it shows how lack of regularization of the recurrent connections can lead to maximum degradation in performance.	strength
2018-220	Most of the results in this paper have been based on one choice of various model parameters.	strength
2018-220	Given the emperical nature of this work, it would have made the paper even clearer if an analysis of their choices were presented.	suggestion
2018-220	Overall, it would be beneficial to the MLP community to see this paper accepted in the conference.	decision
2018-220	Clearly presented paper, including a number of reasonable techniques to improve LSTM-LMs. The proposed techniques are heuristic, but are reasonable and appear to yield improvements in perplexity.	strength
2018-220	Some specific comments follow. re. "ASGD" for Averaged SGD: ASGD usually stands for Asynchronous SGD, have the authors considered an alternative acronym?	weakness
2018-220	AvSGD? re. Optimization criterion on page 2, note that SGD is usually taken to minimizing expected loss, not just empirical loss (Bottou thesis 1991).	weakness
2018-220	Is there any theoretical analysis of convergence for Averaged SGD?	weakness
2018-220	re. paragraph starting with "To prevent such inefficient data usage, we randomly select the sequence length for the forward and backward pass in two steps": the explanation is a bit unclear.	weakness
2018-220	What is the "base sequence length" exactly?	weakness
2018-220	Also, re. the motivation above this paragraph, I'm not sure what "elements" really refers to, though I can guess.	weakness
2018-220	What is the number of training tokens of the datasets used, PTB and WT2?	weakness
2018-220	Can the authors provide more explanation for what "neural cache models" are, and how they relate to "pointer models"?	suggestion
2018-220	Why do the sections "Pointer models", "Ablation analysis", and "AWD-QRNN" come after the Experiments section?	weakness
2018-220	The paper sets a new state of the art on word level language modelling on the Penn Treebank and Wikitext-2 datasets using various optimization and regularization techniques.	abstract
2018-220	These already very good results are further improved, by a large margin, using a Neural Cache.	abstract
2018-220	The paper is well written, easy to follow and the results speak for themselves.	strength
2018-220	One possible criticism is that the experimental methodology does not allow for reliable conclusions to be drawn about contributions of all different techniques, because they seem to have been evaluated at a single hyperparameter setting (that was hand tuned for the full model?).	weakness
2018-220	A variant on the Averaged SGD method is proposed.	abstract
2018-220	This so called NT-ASGD optimizer switches to averaging mode based on recent validation losses.	abstract
2018-220	I would have liked to see a more thorough assessment of NT-ASGD, especially against well tuned SGD.	suggestion
2018-220	I particularly liked Figure 3 which shows how the Neural Cache makes the model much better at handling rare words and UNK (!) at the expense of very common words.	strength
2018-220	Speaking of the Neural Cache, a natural baseline would have been dynamic evaluation.	weakness
2018-220	All in all, the paper is a solid contribution which deserves to be accepted.	decision
2018-220	It could become even better, were the experiments to tease the various factors apart.	suggestion

2018-228	In this paper, the authors describe a new formulation for exploring the environment in an unsupervised way to aid a specific task later.	abstract
2018-228	Using two "minds", Alice and Bob, where the former proposes increasingly difficult tasks and the latter tries to accomplish them as fast as possible, the learning agent Bob can later perform a given task faster having effectively learned the environment dynamics from playing the game with Alice.	abstract
2018-228	The idea of unsupervised exploration has been visited before.	weakness
2018-228	However, the paper presents a novel way to frame the problem, and shows promising results on several tasks.	strength
2018-228	The ideas are well-presented and further expounded in a systematic way.	strength
2018-228	Furthermore, the crux of the proposal and simple and elegant yet leading to some very interesting results.	strength
2018-228	My only complaint is that some of the finer implementation details seems to have been omitted.	weakness
2018-228	For example, the parameter update equation is section 4 is somewhat opaque and requires more discussion than the motivation presented in the preceding paragraph.	weakness
2018-228	Typos and grammatical errors: let assume (section 2.2), it is possible show (section 5).	weakness
2018-228	Overall, I think the paper presents a novel and unique idea that would be interesting to the wider research community.	strength
2018-228	This paper proposes an interesting model of self-play where one agent learns to propose tasks that are easy for her but difficult for an opponent.	abstract
2018-228	This creates a moving target of self-play objectives and learning curriculum.	abstract
2018-228	The idea is certainly elegant and clearly described.	strength
2018-228	I don't really feel qualified to comment on the novelty, since this paper is somewhat out of my area of expertise, but I did notice that the authors' own description of Baranes and Oudeyer (2013) seems very close to the proposal in this paper.	weakness
2018-228	Given the existence of similar forms of self-play the key issue with paper I see is that there is no strong self-play baseline in the experimental evaluation.	weakness
2018-228	It is hard to tell whether this neat idea is really an improvement.	weakness
2018-228	Is progress guaranteed? Is it not possible for Alice to imemdiately find an easy task for her where Bob times out, gets no reward signal, and therefore is unable to learn anything?	weakness
2018-228	Then repeating that task will loop forever without progress.	weakness
2018-228	This suggests that the adversarial setting is quite brittle.	weakness
2018-228	I also find that the paper is a little light on the technical side.	weakness
2018-228	The paper presents a method for learning a curriculum for reinforcement learning tasks.The approach revolves around splitting the personality of the agent into two parts.	abstract
2018-228	The first personality learns to generate goals for other personality for which the second agent is just barely capable--much in the same way a teacher always pushes just past the frontier of a student's ability.	abstract
2018-228	The second personality attempts to achieve the objectives set by the first as well as achieve the original RL task.	abstract
2018-228	The novelty of the proposed method is introduction of a teacher that learns to generate a curriculum for the agent.The formulation is simple and elegant as the teacher is incentivised to widen the gap between bob but pays a price for the time it takes which balances the adversarial behavior.	weakness
2018-228	Prior and concurrent work on learning curriculum and intrinsic motivation in RL rely on GANs (e.g., automatic goal generation by Held et al.), adversarial agents (e.g., RARL by Pinto et al.), or algorithmic/heuristic methods (e.g., reverse curriculum by Florensa et al. and HER Andrychowicz et al.).	weakness
2018-228	In the context of this work, the contribution is the insight that an agent can be learned to explore the immediate reachable space but that is just within the capabilities of the agent.	weakness
2018-228	HER and goal generation share the core insight on training to reach goals.	strength
2018-228	However, HER does generate goals beyond the reachable it instead relies on training on existing reached states or explicitly consider the capabilities of the agent on reaching a goal.	weakness
2018-228	Goal generation while learning to sample from the achievable frontier does not ensure the goal is reachable and may not be as stable to train.	weakness
2018-228	As noted by the authors the above mentioned prior work is closely related to the proposed approach.	weakness
2018-228	However, the paper only briefly mentions this corpus of work.	weakness
2018-228	A more thorough comparison with these techniques should be provided even if somewhat concurrent with the proposed method.	suggestion
2018-228	The authors should consider additional experiments on the same domains of this prior work to contrast performance.	suggestion
2018-228	Questions: Do the plots track the combined iterations that both Alice and Bob are in control of the environment or just for Bob?	suggestion

2019-121	Authors propose a decoder arquitecture model named Subscale Pixel Network.	abstract
2019-121	It is meant to generate overall images as image slice sequences with memory and computation economy by using a Multidimensional Upscaling method.	abstract
2019-121	The paper is fairly well written and structured, and it seems technically sound.	strength
2019-121	Experiments are convincing. Some minor issues: Figure 2 is not referenced anywhere in the main text.	weakness
2019-121	Figure 5 is referenced in the main text after figure 6.	weakness
2019-121	Even if intuitively understandable, all parameters in equations should be explicitly described (e.g., h,w,H,W in eq.1) Summary: This paper addresses an important problem in density estimation which is to scale the generation to high fidelity images.	weakness
2019-121	Till now, there have been no good density modeling results on large images when taken into account large datasets like Imagenet (there have been encouraging results like with Glow, but on 5-bit color intensities and simpler datasets like CelebA).	weakness
2019-121	This paper is the first to successfully show convincing Imagenet samples with 128x128 resolution for a likelihood density model, which is hard even for a GAN (only one GAN paper (SAGAN) prior to this conference has managed to show unconditional 128x128 Imagenet samples).	strength
2019-121	The ideas in this paper to pick an ordering scheme at subsampled slices uniformly interleaved in the image and condition slice generation in an autoregressive way is very likely to be adopted/adapted to more high fidelity density modeling like videos.	abstract
2019-121	Another important idea in this paper is to do depth upscaling, focusing on salient color intensity bits first (first 3 bits per color channel) before generating the remaining bits.	abstract
2019-121	The color intensity dependency structure is also neat: The non-salient bits per channel are conditioned on all previously generated color bits (for all spatial locations).	abstract
2019-121	Overall, I think this paper is a huge advance in density modeling, deserves an oral presentation and deserves as much credit as BigGAN, probably more, given that it is doing unconditional generation.	decision
2019-121	Details: Major: -1. Can you point out the total number of parameters in the models?	suggestion
2019-121	Also would be good to know what hardware accelerators were used.	suggestion
2019-121	The batch sizes mentioned in the Appendix (2048 for 256x256 Imagenet) are too big and needs TPUs?	weakness
2019-121	If TPU pods, which version (how many cores)?	suggestion
2019-121	If not, I am curious to know how many GPUs were used.	suggestion
2019-121	0. I would really like to know the sampling times.	suggestion
2019-121	The model still generates the image pixel by pixel.	abstract
2019-121	Would be good to have a number for future papers to reference this.	suggestion
2019-121	1. Any reason why 256x256 Imagenet samples are not included in the paper?	suggestion
2019-121	Given that you did show 256x256 CelebA samples, sampling time can't be an issue for you to not show Imagenet 256x256.	ac_disagreement
2019-121	So, it would be nice to include them.	suggestion
2019-121	I don't think any paper so far has shown good 256x256 unconditional samples.	weakness
2019-121	So showing this will make the paper even stronger.	suggestion
2019-121	2. Until now I have seen no good 64x64 Imagenet samples from a density model.	weakness
2019-121	PixelRNN samples are funky (colorful but no global structure).	weakness
2019-121	So I am curious if this model can get that.	weakness
2019-121	It may be the case that it doesn't, given that subscale ordering didn't really help on 32x32.	weakness
2019-121	It would be nice to see both 5-bit and 8-bit, and for 8-bit, both the versions: with and without depth upscaling.	suggestion
2019-121	3. I didn't quite understand the architecture in slice encoding (Sec 3.2).	weakness
2019-121	Especially the part about using a residual block convnet to encode the previous slices with padding, and to preserve relative meta-position of the slices.	weakness
2019-121	The part I get is that you concatenate the 32x32 slices along the channel dimension, with padded slices.	weakness
2019-121	I also get that padding is necessary to have the same channel dimension for any intermediate slice.	weakness
2019-121	Not sure if I see the whole point of preserving ordering.	weakness
2019-121	Isn't it just normal padding -> space to depth in a structured block-wise fashion?	weakness
2019-121	4. Can you clarify how you condition the self-attention + Gated PixelCNN block on the previous slice embedding you get out of the above convnet?	weakness
2019-121	There are two embeddings passed in if I understand correctly: (1) All previous slices, (2) Tiled meta-position of current slice.	weakness
2019-121	It is not clear to me how the conditioning is done for the transformer pixelcnn on this auxiliary embedding.	weakness
2019-121	The way you condition matters a lot for good performance, so it would be helpful for people to replicate your results if you provide all details.	weakness
2019-121	5. I also don't understand the depth upscaling architecture completely.	weakness
2019-121	Could you provide a diagram clarifying how the conditioning is done there given that you have access to all pixels' salient bits now and not just meta-positions prior to this slice?	suggestion
2019-121	6. It is really cool that you don't lose out in bits/dim after depth upscaling that much.	suggestion
2019-121	If you take Grayscale PixelCNN (pointed out in the anonymous comment), the bits/dim isn't as good as PixelCNN though samples are more structured.	weakness
2019-121	There is 0.04 b.p.d  difference in 256x256, but no difference in 128x128.	weakness
2019-121	Would be nice to explain this when you add the citation.	weakness
2019-121	7. The architecture in the Appendix can be improved.	weakness
2019-121	It is hard to understand the notations.	weakness
2019-121	What are residual channels, attention channels, attention ffn layer, "parameter attention", conv channels?	weakness
2019-121	Minor: Typo: unpredented --> unprecedented General: The paper tackles a problem of learning long-range dependencies in images in order to obtain high fidelity images.	abstract
2019-121	The authors propose to use a specific architecture that utilizes three main components: (i) a decoder for sliced small images, (ii) a size-upscaling decoder for large image generation, (iii) a depth-upscaling decoder for generating high-res image.	abstract
2019-121	The main idea of the approach is slicing a high-res original image and a new factorization of the joint distribution over pixels.	abstract
2019-121	In this model various well-known blocks are used like 1D Transformer and Gated PixelCNN.	abstract
2019-121	The obtained results are impressive, the generated images are large and contain realistic details.	strength
2019-121	In my opinion the paper would be interesting for the *CONF* audience.	strength
2019-121	Pros: + The paper is very technical but well-written.	strength
2019-121	+ The obtained results constitute new state-of-the-art on HQ image datasets.	strength
2019-121	+ Modeling long-range dependencies among pixels is definitely one of the most important topics in image modeling.	strength
2019-121	The proposed approach is a very interesting step towards this direction.	strength
2019-121	Cons: - The authors claim that the proposed approach is more memory efficient than other methods.	weakness
2019-121	However, I wonder how many parameters the proposed approach requires comparing to others.	weakness
2019-121	It would be highly beneficial to have an additional column in Table 1 that would contain number of parameters for each model.	suggestion
2019-121	- All samples are take either at an extremely high temperature (i.e., 0.99) or at the temperature equal 1.	weakness
2019-121	How do the samples look for smaller temperatures?	weakness
2019-121	Sampling at very high temperature is a nice trick for generating nicely looking images, however, it could hide typical problems of generative models (e.g., see Rezende & Viola, "Taming VAEs", 2018).	weakness
2019-121	--REVISION-- I would like to thank the authors for their response.	misc
2019-121	I highly appreciate their clear explanation of both issues raised by me.	rebuttal_process
2019-121	I am especially thankful for the second point (about the temperature) because indeed I interpreted it as in the GLOW paper.	rebuttal_process
2019-121	Since both my concerns have been answered, I decided to raise the final score (+2).	rebuttal_process

2019-126	This paper investigates the average contribution of a sequence input to the contents of memory and derives a simple scheme to maximize the information content in memory, which is essentially to write at uniformly spaced intervals.	abstract
2019-126	Furthermore they present an attention-based version, where the network caches all hidden states in an interval and selects the hidden state to store via attention.	abstract
2019-126	The paper is very well written and has a nice balance of relevant theoretic motivation and experiments.	strength
2019-126	Furthermore the question that the authors are tackling --- how should we compress information into external memories --- feels important and under-explored.	strength
2019-126	The fact that the resulting scheme is simple is nice, because it's easy for people to try, and it now has some motivation beyond a heuristic decision.	strength
2019-126	I think this paper will have impact in opening up more comprehensive research into the reduction of redundancy in the external memories of neural networks, and also could be instantly impactful for people using DNCs and NTMs	strength
2019-126	--- especially since we see the incorporation of UW / CUW can help bridge the gap (or even surpass) LSTMs for the modeling of natural data.	strength
2019-126	As such I think it is a clear accept.	decision
2019-126	--- Comments to the authors: The results in Figure 2 (c) I think are misleading.	weakness
2019-126	The NTM with an RNN controller can solve this task, the limit of 10,000 steps implies that the model may converge to some 50% value with 14 slots but I am absolutely certain that the NTM + RNN controller would converge in 10,000 steps with a careful tuning of gradient clipping and learning rate.	weakness
2019-126	I think this is basically a false result.	weakness
2019-126	Furthermore I would like to really know what the best final performance of the models are on this task once converged, it's not clear if 10,000 steps was enough.	weakness
2019-126	For equation (9), was it necessary to construct the attention weights in this way?	weakness
2019-126	How much better was it to a direct softmax query: softmax(h_{t-1}^T d_j)?	weakness
2019-126	If you are backpropagating through the attention then the network can shape the hidden states to facilitate the relevant attention, as well as contain the information.	weakness
2019-126	In the second paragraph of S2.2.2 you have "a_{t, j} is the attention score" but you should have "\\alpha_{t, j} is the attention score".	weakness
2019-126	Table 3: just include the Transformer results in the table!?	weakness
2019-126	The reasoning to exclude it is not really coherent.	weakness
2019-126	It would have been nice (and would raise my score) to see the UW scheme operating with a large(ish) number of memory slots.	suggestion
2019-126	This paper deals with Memory Augmnted Neural Networks (MANN) and introduces an algorithm which allows full writes to the dense memory to be only exectued every L timesteps.	abstract
2019-126	The controller produces a hidden output at most timestps, whih is appended to a cache.	abstract
2019-126	Every L steps, soft attention is used to combine this cache of N hidden states to a single one, and then this is used as the input hidden state for the controller, with the outputs performing a write in the full memory M, along with clearing the cache.	abstract
2019-126	The authors first derive "Uniform Writing" (UW) which updates the memory at regular intervals instead of every timestep.	abstract
2019-126	The derivation is based on the "contribution" which is norm of the gradient of some input timestep to some hidden state (potentially at a different timestep).	abstract
2019-126	I am not clear on whether this terminology for the quantity is novel, if this is the case maybe the authors should state this more clearly.	weakness
2019-126	UW says that if all timesteps are equally important, and only D writes can be made in a sequence of length T, then writes should be done every T/(D+1) steps.	weakness
2019-126	I have not checked the proof in detail but this seems reasonable that it would maximise the contribution quantity introduced.	weakness
2019-126	I am less clear on whether this is obviously the right thing to do - sometimes this value is referred to in relation to information, but that term does not strictly seem to be being used in the information theory sense (no mention of bits or nats anywhere).	weakness
2019-126	Regardless, as the authors point out, in real problems there are obviously timesteps which have less or no useful information, and clearly UW is mostly defined in order to build towards CUW.	weakness
2019-126	CUW expands on UW by adding the cache of different hidden states, and using soft attention over them.	weakness
2019-126	This feels like a reasonable step, although I would presume there are times when the L hidden states were collected over timesteps with no information, and so the resulting write is not that useful, and times when all of hte L timesteps contain different useful information.	weakness
2019-126	In these circumstances it seems like the problem of getting the *useful* information into the memory is still present, as the single write done with the averaged hidden state will need to contain lots of information, which may be more ideal written with several timesteps.	weakness
2019-126	The experiments are well described and overall the paper seems reproducable.	strength
2019-126	The standard toy datasets of copy / reverse / sinusoid are used.	strength
2019-126	The results are interesting - regular DNC with memory size 50 performs surprisingly badly on clean Sinusoid, my guess would be that with hyperparameter tuning this could be improved upon.	strength
2019-126	I'm not sure that using exactly the same hyperparameters for a wide variety of models is appropriate	weakness
2019-126	- even with optimizers like Adam and RMSProp, I would want to see at least some sweeping for the best hyperparams, and then graphs like figure 3 should show error bars averaged across multiple runs with the best per-model hyperparameters.	suggestion
2019-126	However, The DNC with CUW seems to perform well across all synthetic tasks.	strength
2019-126	There is no mention of Adaptive Computation Time/ACT (Graves, https://arxiv.org/abs/1603.08983) throughout the paper, which is surprising considering Alex Graves' models form two of the baselines used throughout the paper.	weakness
2019-126	ACT aims to execute an RNN a variable number of times, usually to do >1 timestep of processing for a single timestep of input.	abstract
2019-126	In the context of this paper, I believe it could be adapted to do either zero or one steps of computation per timestep, and that would yield a very comparable network where the LSTM controller always executes, and writes to the memory only happen sometimes.	suggestion
2019-126	Given that it allows a learned process to decide whether to write, as opposed to having a fixed L which separates full writes, this should have the potential to outperform CUW, as it could learn that at certain times, writes must happen at every step.	weakness
2019-126	In my view ACT is attempting to solve essentially the same problem as this paper, so it should either be included as a baseline, or the manuscript should be updated to explain why this is not an appropriate comparison.	weakness
2019-126	I think this is an interesting paper, trying to make progress on an important problem.	strength
2019-126	The results look good, but I can only give a borderline score due to missing ACT numbers, and a few other unclear points.	decision
2019-126	The addition of ACT experiments, and error bars on certain results, would change my mind here.	suggestion
2019-126	Notes: "No solution has been proposed to help MANNs handle ultra long sequence" - (Rae et al 2016) is an attempt to do this, by improving the complexity of reads / writes.	weakness
2019-126	This allows bigger memory and longer sequences to be processed.	weakness
2019-126	"Current MANNS only support dense writing" - presumably this means dense as in 'every timestep', but this terminology is overloaded - you could consider NTM / DNC as doing dense writing, and then work of Rae et al 2016 doing sparse writing.	weakness
2019-126	In my experience training these kind of RNNs can have reasonably high variance across seeds - figures 2 & 3 should have error bars, and especially Table 4 as that contains the most important results.	weakness
2019-126	Getting 99 percent accuracy when previous SOTA is only 0.1% lower is only really meaningful if the standard deviation across seeds is very small.	weakness
2019-126	Appendix A: the 'by induction' result - I believe there is an error, it should be: h_t = \\sigma_{i=1}^t U_{t-i}W x_i + C	weakness
2019-126	As W is applied to inputs, before the repeated applications of U?	weakness
2019-126	I believe the rest of the derivation still holds the same, after the correction.	rebuttal_process
2019-126	This paper looks at ways to improve memory-writing in memory augmented neural networks.	abstract
2019-126	Authors proposed two methods to compare against "regular writing" method as well as compare against each other, namely "uniform writing" and "cached uniform writing".	abstract
2019-126	Latter one attempts to utilize a small size memory efficiently by introducing memory overwriting in other words "forgetting".	abstract
2019-126	Authors started with a very interesting section (namely section 2.1.1) and presented a theoretical formulation of "remembering" capability of RNNs, which is fundamental to this work and I really liked it that they did not jump to the proposed methods right away and instead focused on something very fundamental.	strength
2019-126	Authors presented details of the proposed methods very well, and evaluated them on simple tasks such as "double task", "synthetic reasoning", etc.	strength
2019-126	as well as on more challenging/real tasks such as "document classification" or "image recognition task from MNIST".	strength
2019-126	I really liked the fact that the paper looked at different tasks instead of going with one.	strength
2019-126	Results are convincing overall, especially for CUW.	strength
2019-126	One thing that will improve the paper is the analysis part.	strength
2019-126	Due to having 5+ tasks in the results section, I got the feeling that it is hard to follow the analysis presented by authors within each task as well as across tasks.	weakness
2019-126	Also, in some tasks analysis is quite limited.	weakness
2019-126	It would be great for authors to zoom into the memory write operations in each task (e.g., taking a diff between RW and URW for example and see how memory changes and more importantly how "remember" capability changes) and provide more stats on these, and do this across tasks in one section rather than in different sections allocated for each task.	suggestion
2019-126	Also, analysis in more realistic tasks (e.g., document classification) can be extended as well, rather than only comparing against state-of-the-art methods in terms of final metric.	suggestion
2019-126	While reviewing the paper, I couldn't help asking why larger memories were not tried.	weakness
2019-126	I can see the motivation of trying to use smaller augmented memory, however experimentation around slightly larger augmented memories will be useful for the audience to draw some conclusions.	suggestion
2019-126	Especially I'm curious about the effect of memory size on accuracy in tasks like image recognition or document classification.	suggestion

2019-146	The paper proposes a spatiotemporal modeling of videos based on two currently available spatiotemporal modeling paradigms: RNNs and 3D convolutions.	abstract
2019-146	The main idea of this paper is to get the best world of both in a unified way.	abstract
2019-146	The method first encodes a sequence of frames using 3D-conv to capture short-term motion patterns, passes it to a specific type of LSTM (E3D-LSTM) which accepts spatiotemporal feature maps as input.	abstract
2019-146	E3D-LSTM captures long-term dependencies using an attention mechanism.	abstract
2019-146	Finally, there are 3D-conv based decoders which receive the output of E3D-LSTM and generate future frames.	abstract
2019-146	The message of the paper, I believe, is that 3D-conv and RNNs can be integrated to perform short and long predictions.	abstract
2019-146	They show in the experiments how the model can remember far past for reasoning and prediction.	abstract
2019-146	The nice point of the method is that it is heavily investigated through experiments.	abstract
2019-146	It's evaluated on two datasets, with ablation studies on both.	abstract
2019-146	Moreover, the paper is well-written and clear.	strength
2019-146	technically, the paper seems correct.	strength
2019-146	However, my only big concern is about the limited novelty of the method.	weakness
2019-146	E3D-LSTM is the core of the novelty, which is basically an LSTM with extra gate, and attention mechanism.	weakness
2019-146	other comments: - As the method by essence is a spatiotemporal learning model, why the method is not evaluated on full-length videos of the something-something dataset for classical action classification task, in order to compare it with the full architecture of I3D, or S3D?	weakness
2019-146	- While the paper discusses self-supervised learning, I would suggest showing its benefit on online action recognition task.	suggestion
2019-146	One without frame-prediction loss and one with.	suggestion
2019-146	- the something-something dataset has 174 classes, how was the process of selecting 41 classes out of it?	weakness
2019-146	# 1. Summary This paper presents a model for future video prediction, which integrates 3D convolutions into RNNs. The internal operations of the RNN are modified by adding historical records controlled via a gate-controlled self-attention module.	abstract
2019-146	The authors show that the model is effective also for other tasks such as early activity recognition.	abstract
2019-146	Strengths: * Nice extensive experimentation on video prediction and early activity recognition tasks and comparison with recent papers	strength
2019-146	* Each choice in the model definition are motivated, although some clarity is still missing (see below)	strength
2019-146	Weaknesses: * Novelty: the proposed model is a small extension of a previous work (Wang et al., 2017)	weakness
2019-146	# 2. Clarity and Motivation In general, the paper is clear and general motivation makes sense, however some points need to be improved with further discussion and motivation: A) Page 2 "Unlike the conventional memory transition function, it learns the size of temporal interactions.	weakness
2019-146	For longer sequences, this allows attending to distant states containing salient information": This is not obvious.	weakness
2019-146	Can the authors add more details and motivate these two sentences?	weakness
2019-146	How is long-term relations are learned given Eq. 1?	weakness
2019-146	B) Page 5 "These two terms are respectively designed for short-term and long-term video modeling": How do you make sure that Recall(.) does not focus on the short-term modeling instead?	weakness
2019-146	Not clear why this should model long-term relations.	weakness
2019-146	C) Page 5 and Eq 1: motivation why layer norm is required when defining C_t^k is not clear	weakness
2019-146	D) What if the Recall is instead modeled as attention?	weakness
2019-146	The idea is to consider only C_{1:t-1}^k (not consider R_t) and have an attentional model that learn what to recall based only on C.	weakness
2019-146	Also, why does Recall need to depend on R_t?	weakness
2019-146	E) Page 5 "to minimize the l1 + l2 loss over every pixel in the frame": this sentence is not clear.	weakness
2019-146	How does it relate to Eq. 2?	weakness
2019-146	# 3. Novelty Novelty is the major concern of this paper.	weakness
2019-146	Although the introduced new concepts and ideas are interesting, the work seems to be an extension of ST-LSTM and PredRNN where Eq 1 is slightly modified by introducing Recall.	weakness
2019-146	In addition the existing relation between the proposed model and ST-LSTM is not clearly state.	weakness
2019-146	Page 2, first paragraph: here the authors should state that model is and extension of ST-LSTM and highlight what are the difference and advantage of the new model.	weakness
2019-146	# 4. Significance of the work	weakness
2019-146	This paper deals with an interesting and challenging topic (video prediction) as well as it shows some results on the early activity recognition task.	abstract
2019-146	These are definitively nice problem which are far to be solved.	strength
2019-146	From the application perspective this work is significant, however from the methodological perspective it lacks a bit of significance because of the novelty issues highlighted above.	weakness
2019-146	# 5. Experimentation The experiments are robust with nice comparisons with recent methods and ablation study motivating the different components of the model (Table 1 and 2).	strength
2019-146	Some suggested improvements: A) Page 7 "Seq 1 and Seq 2 are completely irrelevant, and ahead of them, another sub-sequence called prior context is given as the input, which is exactly the same as Seq 2": The COPY task is a bit unclear and need to be better explained.	weakness
2019-146	Why are Seq. 1 and 2 irrelevant?	weakness
2019-146	I would suggest to rephrase this part.	suggestion
2019-146	B) Sec. 4.2, "Dataset and setup": which architecture has been used here?	weakness
2019-146	C) Sec. 4.3, "Hyper-parameters and Baselines": the something-something dataset is more realising that the other two "toy" dataset.	weakness
2019-146	Why did the authors choose to train a 2 layers 3D-CNN encoders, instead of using existing pretrained 3D CNNs?	weakness
2019-146	I would suspect that the results can improve quite a bit.	weakness
2019-146	# 6. Others * The term "self-supervised auxiliary learning" is introduced in the abstract, but at this point it's meaning is not clear.	weakness
2019-146	I'd suggest to either remove it or explain its meaning.	suggestion
2019-146	* Figure 1(a): inconsistent notation with 2b.	weakness
2019-146	Also add citation (Wang et al., 2017) since it ie the same model of that paper	suggestion
2019-146	------- # Post-discussion I increased my rating: even if novelty is not high, the results support the incremental ideas proposed by the authors.	rebuttal_process
2019-146	AFTER REBUTTAL: This is an overall good work, and I do think proves its point.	strength
2019-146	The results on the TaxiBJ dataset (not TatxtBJ, please correct the name in the paper) are compelling, and the concerns regarding some of the text explainations have been corrected.	strength
2019-146	----- The proposed model uses a 3D-CNN with a new kind of 3D-conv.	weakness
2019-146	recurrent layer named E3D-LSTM, an extension of 3D-RCNN layers where the recall mechanism is extended by using an attentional mechanism, allowing it to update the recurrent state not only based on the previous state, but on a mixture of previous states from all previous time steps.	weakness
2019-146	Pros: The new approach displays outstanding results for future video prediction.	strength
2019-146	Firstly, it obtains better results in short term predictions thanks to the 3D-Convolutional topology.	strength
2019-146	Secondly, the recall mechanism is shown to be more stable over time: The prediction accuracy is sustained over longer preiods of time (longer prediction sequences) with a much smaller degradation.	strength
2019-146	Regarding early action recognition, the use of future video prediction as a jointly learned auxiliary task is shown to significantly increase the prediction accuracy.	strength
2019-146	The ablation study is compelling.	strength
2019-146	Cons: The model does not compare against other methods regarding early action recognition.	weakness
2019-146	Since this is a novel field of study in computer vision, and not too much work exists on the subject, it is understandable.	strength
2019-146	Also, it is not the main focus of the work.	weakness
2019-146	In the introduction, the authors state that they account for uncertainty by better modelling the temporal sequence.	rebuttal_process
2019-146	Please, remove or rephrase this part.	suggestion
2019-146	Uncertainty in video prediction is not due to the lack of modelling ability, but due to the inherent uncertainty of the task.	rebuttal_process
2019-146	In real world scenarios (eg.	suggestion
2019-146	the KTH dataset used here) there is a continuous space of possible futures.	rebuttal_process
2019-146	In the case of variational models, this is captured as a distribution from which to sample.	rebuttal_process
2019-146	Adversarial models collapse this space into a single future in order to create more realistic-looking predictions.	rebuttal_process
2019-146	I don't believe your approach should necessarily model that space (after all, the novelty is on better modelling the sequence itself, not the possible futures, and the model can be easily extended to do so, either through GANs or VAEs), but it is important to not mislead the reader.	ac_disagreement
2019-146	It would have been interesting to analyse the work on more complex settings, such as UCF101.	suggestion
2019-146	While KTH is already a real-world dataset, its variability is very limited: A small set of backgrounds and actions, performed by a small group of individuals.	weakness

2019-248	The authors extend an existing approach to adaptive softmax classifiers used for the output component of neural language models into the input component, once again allowing tying between the embedding and softmax.	abstract
2019-248	This fills a significant gap in the language modeling architecture space, and the perplexity results bear out the advantages of combining adaptively-sized representations with weight tying.	abstract
2019-248	While the advance is in some sense fairly incremental, the centrality of unsupervised language modeling to modern deep NLP (ELMo, BERT, etc.) implies that perplexity improvements as large as this one may have meaningful downstream effects on performance on other tasks.	weakness
2019-248	Some things I noticed: - One comparison that I believe is missing (I could be misreading the tables) is comparing directly to Merity et al.'s approach (adaptive softmax but fixed embedding/softmax dimension among the bands).	weakness
2019-248	Presumably you're faster, but is there a perplexity trade-off?	weakness
2019-248	- The discussion/explanation of the differing performance of tying or not tying each part of the embedding weights for the different datasets is confusing; I think it could benefit from tightening up the wording but mostly I just had to read it a couple times.	weakness
2019-248	Perhaps all that's complicated is the distinction between embedding and projection weights; it would definitely be helpful to be as explicit about that as possible upfront.	weakness
2019-248	- The loss by frequency-bin plots are really fantastic.	strength
2019-248	You could also try a scatterplot of log freq vs.	suggestion
2019-248	average loss by individual word/BPE token.	suggestion
2019-248	- Do you have thoughts as to why full-softmax BPE is worse than adaptive softmax word level?	suggestion
2019-248	That goes against the current (industry) conventional wisdom in machine translation and large-scale language modeling that BPE is solidly better than word-level approaches because it tackles the softmax bottleneck while also sharing morphological information between words.	weakness
2019-248	This paper introduced a new architecture for input embeddings of neural language models: adaptive input representation (ADP).	abstract
2019-248	ADP allowed a model builder to define a set of bands of input words with different frequency where frequent words have larger embedding size than the others.	abstract
2019-248	The embeddings of each band are then projected into the same size.	abstract
2019-248	This resulted in lowering the number of parameters.	abstract
2019-248	Extensive experiments with the Transformer LM on WikiText-103 and Billion Word corpus showed that ADP achieved competitive perplexities.	abstract
2019-248	While tying weight with the output did not benefit the perplexity, it lowered the runtime significantly on Billion Word corpus.	abstract
2019-248	Further analyses showed that ADP gained performance across all word frequency ranges.	abstract
2019-248	Overall, the paper was well-written and the experiments supported the claim.	strength
2019-248	The paper was very clear on its contribution.	strength
2019-248	The variable-size input of this paper was novel as far as I know.	strength
2019-248	However, the method, particularly on the weight sharing, lacked a bit of important background on adaptive softmax.	weakness
2019-248	The weight sharing was also needed further investigation and experimental data on sharing different parts.	weakness
2019-248	The experiments compared several models with different input levels (characters, BPE, and words).	weakness
2019-248	The perplexities of the proposed approach were competitive with the character model with an advantage on the training time.	strength
2019-248	However, the runtimes were a bit strange.	weakness
2019-248	For example, ADP and ADP-T runtimes were very close on WikiText-103 dataset but very different on Billion Word corpus (Table 3 and 4).	weakness
2019-248	The runtime of ADP seemed to lose in term of scaling as well to BPE.	weakness
2019-248	Perhaps, the training time was an artifact of multi-GPU training.	weakness
2019-248	Questions: 1. I am curious about what would you get if you use ADP on BPE vocab set?	weakness
2019-248	2. How much of the perplexity reduction of 8.7 actually come from ADP instead of the transformer and optimization?	weakness
2019-248	This article presents experiments on medium- and large-scale language modeling when the ideas of adaptive softmax (Grave et al., 2017) are extended to input representations.	abstract
2019-248	The article is well written and I find the contribution simple, but interesting.	strength
2019-248	It is a reasonable and well supported increment from adaptive softmax of Grave et al. (2017).	strength
2019-248	My question is a bit philosophical: The only thing which I was concerned about when reading the paper is projection of the embeddings back to the d-dimensional space.	weakness
2019-248	I understand that for two matrices A and B we have rank(AB) <= min(rank(A), rank(B)), and we are not making the small-sized embeddings richer when backprojecting to R^d, but have you thought about how it would be possible to avoid this step and keep the original variable-size embeddings?	weakness
2019-248	References Joulin, A., Cissé, M., Grangier, D.	misc
2019-248	and Jégou, H., 2017, July.	misc
2019-248	Efficient softmax approximation for GPUs. In International Conference on Machine Learning (pp.	misc
2019-248	1302-1310).	misc

2019-252	This is a well-motived paper that considers bridging the gap in discrete-time continuous-state/action optimal control by approximating the system dynamics with a convex model class.	abstract
2019-252	The convex model class has more representational power than linear model classes while likely being more tractable and stable than non-convex model classes.	abstract
2019-252	They show empirical results in Mujoco continuous-control environments and in an HVAC example.	abstract
2019-252	I think this setup is a promising direction but I have significant concerns with some of the details and claims in this work: 1. Proposition 2 is wrong and the proposed input-convex recurrent neural network architecture not input-convex.	weakness
2019-252	To fix this, the D1 parameters should also be non-negative.	weakness
2019-252	To show why the proposition is wrong, consider the convexity of y2	suggestion
2019-252	with respect to x1, using g to denote the activation function: z1 = g(U x1 + ...)	suggestion
2019-252	y2 = g(D1 z1 + ...)	suggestion
2019-252	Thus making y2 = g(D1 g(U x1 + ...) + ...)	suggestion
2019-252	y2 is *not* necessarily convex with respect to x1 because D1 takes an unrestricted weighted sum of the convex functions g(U x1 + ...)	weakness
2019-252	With the ICRNN architecture as described in the paper not being input-convex, I do not know how to interpret the empirical findings in Section 4.2 that use this architecture.	weakness
2019-252	2. I think a stronger and more formal argument should be used to show that Equation (5) is a convex optimization problem as claimed.	weakness
2019-252	It has arbitrary convex functions on the equality constraints that are composed with each other and then used in the objective.	weakness
2019-252	Even with parts of the objective being convex and non-decreasing as the text mentions, it's not clear that this is sufficient when combined with the composed functions in the constraints.	weakness
2019-252	3. I have similar concerns with the convexity of Equation (6).	weakness
2019-252	Consider the convexity of x3 with respect to u1, where g is now an input-convex neural network (that is not recurrent): x3 = g(g(x1, u1), u2)	weakness
2019-252	This composes two convex functions that do *not* have non-decreasing properties and therefore introduces an equality constraint that is not necessarily even convex, almost certainly making the domain of this problem non-convex.	suggestion
2019-252	I think a similar argument can be used to show why Equation (5) is not convex.	suggestion
2019-252	In addition to these significant concerns, I have a few other minor comments.	misc
2019-252	1. Figure 1 hides too much information.	weakness
2019-252	It would be useful to know,	suggestion
2019-252	for example, that the ICNN portion at the bottom right is solving a control optimization problem with an ICNN as part of the constraints.	suggestion
2019-252	2. The theoretical results in Section 3 seem slightly out-of-place within the broader context of this paper but are perhaps of standalone interest.	weakness
2019-252	Due to my concerns above I did not go into the details in this portion.	weakness
2019-252	3. I think more information should be added to the last paragraph of	weakness
2019-252	Section 1 as it's claimed that the representational power of	weakness
2019-252	ICNNs and "a nice mathematical property" help improve the computational time of the method, but it's not clear why this is and this connection is not made anywhere else in the paper.	weakness
2019-252	4. What method are you using to solve the control problems in	weakness
2019-252	Eq (5) and (6)? 5. The empirical setup and tasks seems identical to [Nagabandi et al.].	weakness
2019-252	Figure 3 directly compares to the K=100 case of their method.	weakness
2019-252	Why does Fig 6 of [Nagabandi et al.] have significantly higher rewards for their method, even in the K=5 case?	weakness
2019-252	6. In Figure 5, f_NN seems surprisingly bad in the red region of the data on the left side.	weakness
2019-252	Is this because the model is not using many parameters?	weakness
2019-252	What are the sizes of the networks used?	weakness
2019-252	The paper proposes neural networks which are convex on inputs data to control problems.	abstract
2019-252	These types of networks, constructed based on either MLP or RNN, are shown to have similar representation power as their non-convex versions, thus are potentially able to better capture the dynamics behind complex systems compared with linear models.	abstract
2019-252	On the other hand, convexity on inputs brings much convenience to the later optimization part, because there are no worries on global/local minimum or escaping saddle points.	strength
2019-252	In other words, convex but nonlinear provides not only enough search space, but also fast and tractable optimization.	strength
2019-252	The compromise here is the size of memory, since 1) more weights and biases are needed to connect inputs and hidden layers in such nets and 2) we need to store also the negative parts on a portion of weights.	strength
2019-252	Even though the idea of convex networks were not new, this work is novel in extending input convex RNN and applying it into dynamic control problems.	strength
2019-252	As the main theoretical contribution, Theorem 2 shows that to have same representation power, input convex nets use polynomial number of activation functions, compared with exponential from using a set of affine functions.	strength
2019-252	Experiments also show such effectiveness.	strength
2019-252	The paper is clearly and nicely written.	strength
2019-252	These are reasons I suggest accept.	decision
2019-252	Questions and suggestions: 1) For Lemma 1 and Theorem 1, I wonder whether similar results can be established for non-convex functions.	suggestion
2019-252	Intuitively, it seems that as long as assuming Lipschiz continuous, we can always approximate a function by a maximum of many affine functions, no matter it is convex or not.	suggestion
2019-252	Is this right or something is missing?	weakness
2019-252	2) In the main paper, all experiments were aimed to address ICNN and ICRNN have good accuracy, but not they are easier to optimize due to convexity.	weakness
2019-252	In the abstract, it is mentioned "...	weakness
2019-252	using 5X less time", but I can only see this through appendix.	weakness
2019-252	A suggestion is at least describing some results on the comparison with training time in the main paper.	suggestion
2019-252	3) In Appendix A, it seems the NN is not trained very well as shown in the left figure.	weakness
2019-252	Is this because the number of parameters of NN is restricted to be the same as in ICNN?	weakness
2019-252	Do training on both spend the same resource, ie, number of epoch?	weakness
2019-252	Such descriptions are necessary here.	weakness
2019-252	4) In Table 2 in appendix, why the running time of ICNN increases by a magnitude for large H in Ant case?	weakness
2019-252	Typos: Page 1 "simple control algorithms HAS ..."	misc
2019-252	Page 7 paragraph "Baselines": "Such (a) method".	misc
2019-252	In the last line of Table 2, 979.73 should be bold instead of 5577.	weakness
2019-252	There is a ?? in appendix D.4.	weakness
2019-252	This paper proposes to use input convex neural networks (ICNN) to capture a complex relationship between control inputs and system dynamics, and then use trained ICNN to form a model predictive control (MPC) problem for control tasks.	abstract
2019-252	The paper is well-written and bridges the gap between neural networks and MPC.	strength
2019-252	The main contribution of this paper is to use ICNN for learning system dynamics.	abstract
2019-252	ICNN is a neural network that only contains non-negative weights.	abstract
2019-252	Thanks to this constraint, ICNN is convex with respect to an input, therefore MPC problem with an ICNN model and additional convex constraints on control inputs is a convex optimization problem.	abstract
2019-252	While it is not easy to solve such a convex problem, it has a global optimum, and a gradient descent algorithm will eventually reach such a point.	abstract
2019-252	It should also be noted that a convex problem has a robustness with respect to an initial starting point and an ICNN model itself as well.	abstract
2019-252	The latter is pretty important, since training ICNN (or NN) is a non-convex optimization, so the parameters in trained ICNN (or NN) model can vary depending on the initial random weights and learning rates, etc.	abstract
2019-252	Since a convex MPC has some robustness (or margin) over an error or deviation in system dynamics, while non-convex MPC does not, using ICNN can also stabilize the control inputs in MPC.	abstract
2019-252	Overall, I believe that using ICNN to from convex MPC is a sample-efficient, non-intrusive way of constructing a controller with unknown dynamics.	weakness
2019-252	Below are some minor suggestions to improve this paper.	misc
2019-252	-- Page 18, there is Fig.??.	weakness
2019-252	Please fix this. -- In experiments, could you compare the result with a conventional end-to-end RL approach?	suggestion
2019-252	I know this is not a main point of this paper, but it can be more compelling.	suggestion

2019-271	The authors introduce two techniques: One is (old school) forward search planning https://en.wikipedia.org/wiki/State_space_planning#Forward_Search for input/output-provided sequential neural program synthesis on imperative Domain Specific Languages with an available partial program interpreter (aka transition function)(from which intermediate internal states can be extracted, e.g. assembly, Python).	abstract
2019-271	Previous work did: which_instruction, next_neural_state = neural_network(encoding(input_output_pairs), neural_state)	abstract
2019-271	This technique: which_instruction = neural_network(encoding(current_execution_state_output_pairs)) next_execution_state = vectorized_transition_function(current_execution_state, which_instruction)	abstract
2019-271	The second one is ensembles of program synthesizers (only ensembled at test-time).	abstract
2019-271	Guiding program synthesis by intermediate execution states is novel, gets good results and can be applied to popular human programming languages like Python.	strength
2019-271	Pros + Using intermediate execution states	strength
2019-271	Cons - State space planning could be done in a learnt tree search fashion, like e.g. Monte Carlo Tree Search	weakness
2019-271	- Ensembling synthesizers at test time only	weakness
2019-271	- why not have stochastic program synthesizers, see them as a generative model, and evaluate top-k generalization?	weakness
2019-271	Page 7 Table 3 line 3: "exeuction" -> "execution This paper proposes guiding program synthesis with information from partial/incomplete program execution.	abstract
2019-271	The idea is that by executing partial programs, synthesizers can obtain the information of the state the (partial) program ended in and can, therefore, condition the next step on that (intermediate) state.	abstract
2019-271	The paper also mentions ensembling synthesizers to achieve a higher score, and by doing that it outperforms the current state-of-the-art on the Karel dataset program synthesis task.	abstract
2019-271	In general, I like the idea of guiding synthesis with intermediate executions, and the evaluation in the paper shows this does make sense, and it outperforms the SOTA.	strength
2019-271	The idea is original and the evaluation shows it is significant (enough).	strength
2019-271	However, I have two major concerns with the paper, its presented contribution, and the clarity.	weakness
2019-271	First, I cannot accept ensembling as a contribution to this paper.	weakness
2019-271	There is nothing novel about the ensemble proposed, and ensembling, as a standard method that pushes models that extra few percentage points, is present in a lot of other research.	weakness
2019-271	I have nothing against achieving SOTA results with it, while at the same time showing that the best performing model outperforms previous SOTA, which this paper orderly does.	strength
2019-271	However, I cannot accept non-novel ensembling as a contribution of the paper.	weakness
2019-271	Second, the clarity of the paper should be substantially improved: - my main issue is that it is not clear how the Exec algorithm (see next point too) is trained.	weakness
2019-271	From what I understand Exec is trained on supervised data via MLE.	weakness
2019-271	What is the supervised data here?	weakness
2019-271	Given the generality claims and the formulation in Algorithm 1/2, and possible ways one could use the execution information, as well as the fact that the model should be end-to-end trainable via MLE, it seems to me that the model is trained on prefixes (defined by Algorithm 1/2) of programs.	weakness
2019-271	Whether this is correct or not, please provide full details on how one can train Exec without using RL.	weakness
2019-271	- By looking at Table 3, it seems that the generalization boost coming from Exec (I'm ignoring ensembling) is higher enough, and that's great.	weakness
2019-271	However, it's obvious that the exact match gain by Exec is minute, implying that the proposed algorithm albeit great on the generalization metric, does not improve the exact match at all.	weakness
2019-271	Do you have any idea why is that?	weakness
2019-271	Is that because Exec is trained via MLE and the Exec algorithm doesn't add anything new to the training procedure?	weakness
2019-271	- how do algorithm 1 and 2 exactly relate?	weakness
2019-271	I guess there is a meaning of ellipses in Lines 1 and 13, however, that is not mentioned anywhere.	weakness
2019-271	Is the mixture of algorithm 1 and 2 (and a non-presented algorithm for while loops) the Exec algorithm?	weakness
2019-271	How exactly are these algorithms joined, i.e what is the final algorithm?	weakness
2019-271	- while on one side, I find some formalizations (problem definitions, definition 1, semantic rules in table 2) nicely done, I do not see their necessity nor big gains from them.	weakness
2019-271	In my opinion, the understanding of the rest of the paper does not depend on them, and they are well-described in the text.	weakness
2019-271	- the paper says that the algorithm "helps boost the performance of different existing training algorithms", however, it does so only on the Bunel et al model (and the MLE baseline in it), and albeit there's mention of the generality, it has not been shown on anything other than those two models and the Karel dataset.	weakness
2019-271	- do lines 6-7 in Algorithm 2 recurse?	weakness
2019-271	Does the model support arbitrarily nested loops/if statements?	weakness
2019-271	- The claim that the shortest principle is most effective is supported by 2 data points, without any information on the variance of the prediction/dependence on the seed.	weakness
2019-271	Did you observe this for #models > 10 too?	weakness
2019-271	Up to what number? - In table 3, is Exec on MLE?	weakness
2019-271	Could you please, for completeness, present the results of Exec + RL + ensemble in the table too?	suggestion
2019-271	- summarization, point 3 - what are the different modules mentioned here?	suggestion
2019-271	Exec/RL/ensemble? Minor issues, remarks, typos: - table 1 position is very unfortunate	weakness
2019-271	- figure 1 is not self-explanatory - it takes quite a lot of space to explain the network architecture, yet it fails to deliver meaning to parts of it (e.g. what is h_t^x, why is it max-pooled, what is g_t, etc)	weakness
2019-271	- abstract & introduction - "Reducing error rate around 60%" absolute percentage points seem like a better evaluation measure (that the paper does use).	weakness
2019-271	Why is the error rate reduction necessary here?	weakness
2019-271	- figure 2 - why is the marker in one of the corners, and not in the cell itself?	weakness
2019-271	- Algorithm 1, step 4, is this here just as initialization, so S is non-empty to start with?	weakness
2019-271	- Table 2 rule names are unclear (e.g. S-Seq-Bot ?)	weakness
2019-271	- Table 3 mentions what Exec indicates twice This paper presents two new ideas on leveraging program semantics to improve the current neural program synthesis approaches.	weakness
2019-271	The first idea uses execution based semantic information of a partial program to guide the future decoding of the remaining program.	weakness
2019-271	The second idea proposes using an ensembling approach to train multiple synthesizers and then select a program based on a majority vote or shortest length criterion.	weakness
2019-271	The ideas are evaluated in the context of the Karel synthesis domain, and the evaluation shows a significant improvement of over 13% (from 77% to 90%).	weakness
2019-271	The idea of using program execution information to guide the program decoding process is quite natural and useful.	strength
2019-271	There has been some recent work on using dynamic program execution in improving neural program repair approaches, but using such information for synthesis is highly non-trivial because of unknown programs and when the DSL has complex control-flow constructs such as if conditionals and while loops.	strength
2019-271	This paper presents an elegant approach to handle conditionals and loops by building up custom decoding algorithms for first partially synthesizing the conditionals and then synthesizing appropriate statement bodies.	abstract
2019-271	The idea of using ensembles looks relatively straightforward, but it hasn't been used much in synthesis approaches.	weakness
2019-271	The evaluation shows some interesting characteristics of using different selection criterion such as shortest program or majority choice can have some impact on the final synthesized program.	abstract
2019-271	The evaluation results are quite impressive on the challenging Karel domain.	strength
2019-271	It's great to see that execution and ensembling ideas lead to practical gains.	strength
2019-271	There were a few points that weren't clear in the paper: 1. Are the synthesis models still trained on original input-output examples like Bunel et al. 2018?	weakness
2019-271	Or are the models now trained on new dataset comprising of (partial-inputs-->final-output) pairs obtained from the partial execution algorithm?	weakness
2019-271	2. In algorithm 2, the algorithm generates bodies for if and else branches until generating the else and fi tokens respectively.	weakness
2019-271	It seems the two bodies are being generated independently of each other using the standard synthesizer \\Tau. Is there some additional context information provided to the two synthesis calls in lines 8 and 9 so that they know to produce else and fi tokens?	weakness
2019-271	3. Is there any change to the beam search?	weakness
2019-271	One can imagine a more sophisticated beam search with semantic information can help as well (e.g. all partial programs that lead to the same intermediate state can be grouped into 1).	weakness

2019-289	The paper proposes a multi-domain music translation method.	abstract
2019-289	The model presents a Wavenet auto-encoder setting with a single (domain independent) encoder and multiple (domain specific) decoders.	abstract
2019-289	From the model perspective, the paper builds up on several exciting ideas such as Wavenet and autoencoder based translation models that can perform the domain conversion without relying on parallel datasets.	abstract
2019-289	The two main modifications are the use of data augmentation, the use of multiple decoders (rather a single decoder conditioned on the output domain identity) and the use of a domain confusion loss to prevent the latent space to encode domain specific information.	abstract
2019-289	This last idea has been also used on prior work.	abstract
2019-289	Up to my knowledge, this is the first autoencoder-based music translation method.	strength
2019-289	While this problem is very similar to that of speaker conversion, modeling musical audio signal (with many instruments) is clearly more challenging.	strength
2019-289	Summarizing, I think that the contributions in terms of methods are limited, but the results are very interesting.	strength
2019-289	The paper gives an affirmative answer to the question of whether existing models could be adapted to handle the case of music translation, which is of value.	strength
2019-289	The paper would be stronger in my view, if stronger baselines would be included.	suggestion
2019-289	This would show that the technical contributions are better than alternative methods.	suggestion
2019-289	Please read bellow some further comments and questions.	misc
2019-289	The authors perform two ablation studies: eliminating data augmentation and the domain confusion network.	abstract
2019-289	In both cases, the model without this add on fails to train.	weakness
2019-289	However, it seems to me that different studies are important.	strength
2019-289	The paper seems to be missing baselines.	weakness
2019-289	The authors could compare their work with that of VQ-VAE.	suggestion
2019-289	The authors claim that they could not make VQ-VAE work on this problem.	weakness
2019-289	The cited work by Dieleman et al provides some improvements to adapt VQ-VAE to be better suited to the music domain.	weakness
2019-289	Did you evaluate also autoregressive discrete autoencoders?	suggestion
2019-289	The proposed method uses an individual decoder per domain.	weakness
2019-289	This is unlike other conversion methods (such as the speech conversion studied in VQ-VAE).	weakness
2019-289	This modification is very costly and provides a very large capacity.	suggestion
2019-289	Have you tried having a single decoder which is also conditioned on a one-hot vector indicating the domain?	suggestion
2019-289	Is it reasonable to expect some transfer between domains or are they too different?	weakness
2019-289	Maybe this is the motivation behind using many decoders.	weakness
2019-289	It would be good to clarify.	suggestion
2019-289	I understand that the emphasis of this work is on music translation, however, the model doesn't have anything specific to music.	weakness
2019-289	In that regard, maybe a way to compare to VQ-VAE is to run the proposed method to the voice conversion of the VQ-VAE.	suggestion
2019-289	Have you tried producing samples using the decoder in an unconditional setting?	suggestion
2019-289	The authors claim that the learned representation is disentangled.	abstract
2019-289	Why is this the case?	suggestion
2019-289	Normally a representation is said to be disentangled if different properties are represented in different (disjoint) coordinates.	suggestion
2019-289	I might not be understanding what is meant here.	weakness
2019-289	The loss used by the authors, encourages the latent representation to not have domain specific information.	weakness
2019-289	The authors should cite the work [A], which has very similar motivation.	weakness
2019-289	It would be interesting to report the classification accuracy of the classifier to see how much of the domain information is left in the latent codes.	suggestion
2019-289	Is it reduced to chance?	suggestion
2019-289	In Section 3.1 the authors describe some modifications to nv-wavenet.	suggestion
2019-289	I imagine that this is because it leads to better performance or faster training.	suggestion
2019-289	It would be good to give some more information.	weakness
2019-289	Did you perform ablation studies for these?	suggestion
2019-289	In the human lineup experiment (Figure 2 b,c and d).	suggestion
2019-289	While the listeners fail to select the correct source, many of the domains are never chosen.	suggestion
2019-289	This could suggest that some translations are consistently poorer than others or the translations themselves are poor.	suggestion
2019-289	This cannot be deduced from this experiment.	suggestion
2019-289	Have you evaluated this? Maybe it would be better to present pairs of audios with reconstruction and a translation.	suggestion
2019-289	While I consider the results quite good, I tend to agree with the posted public comment.	misc
2019-289	It is very hard to claim that the model is effectively transferring styles.	weakness
2019-289	A perceptual test should include the question: is this piece on this given style?	suggestion
2019-289	As the authors mentioned, it is clearly very difficult to evaluate generative models.	misc
2019-289	But maybe the claims could be toned down.	weakness
2019-289	[A] Louizos, Christos, et al. "The variational fair autoencoder." arXiv preprint arXiv:1511.00830 (2015).	misc
2019-289	This paper talks about music translation using a WaveNet-based autoencoder architecture.	abstract
2019-289	The models are trained on diverse training sets and evaluated under multiple settings.	abstract
2019-289	What reported in this paper seems to be interesting and the performance sounds good.	strength
2019-289	However, I have following comments/concerns.	misc
2019-289	1. The paper is not clearly written.	weakness
2019-289	Its exposition needs significant improvement.	weakness
2019-289	There are numerous inconsistent definitions and vague descriptions that make the reading sort of difficult.	weakness
2019-289	a)  It would be very helpful if the authors can put up a figure for the description of  the WaveNet  autoencoder instead of just using words in Section 3.1	weakness
2019-289	b) The paper itself should be self-contained instead of referring readers to other references for the details of model architectures.	weakness
2019-289	c) The math symbols are poorly defined.	weakness
2019-289	What is the definition of C in Section 3.3?	weakness
2019-289	It is defined or referred to as "domain classification network" and also "domain confusion network" but nowhere to find in Fig. 1.	weakness
2019-289	d) "C is minimizes" -> "minimizes"	weakness
2019-289	e)  In Section 4,  it says that "Each batch is first used to train the adversarial discriminator".	weakness
2019-289	Which adversarial discriminator? Where to find in Fig. 1 as it is the only description of the network architecture?	weakness
2019-289	2.  The authors mentioned a couple of observations that left unanswered.	weakness
2019-289	a)   I am surprised to see that without data augmentation, the training does not even converge.	weakness
2019-289	b)  The conversion from unseen domains is more successful than the learned domains.	weakness
2019-289	c)  The decoder starts to be creative when the size of the latent space is reduced.	weakness
2019-289	I sense that these observations seem to point to some (serious) generalization issues of the proposed model.	weakness
2019-289	I would like to hear explanations from the authors.	suggestion
2019-289	After reading the rebuttal: The authors have addressed my major concerns with regard to this paper.	rebuttal_process
2019-289	I have lifted my score.	rebuttal_process
2019-289	Thanks for the nice response.	rebuttal_process
2019-289	A method is presented to modify a music recording so that it sounds like it was performed by a different (set of) instrument(s).	abstract
2019-289	This task is referred to as "music translation".	abstract
2019-289	To this end, an autoencoder model is constructed, where the decoder is autoregressive (WaveNet-style) and domain-specific, and the encoder is shared across all domains and trained with an adversarial "domain confusion loss".	abstract
2019-289	The latter helps the encoder to produce a domain-agnostic intermediate representation of the audio.	abstract
2019-289	Based on the provided samples, the translation is often imperfect: the original timbre often "leaks" into the output.	abstract
2019-289	This is most clearly audible when translating piano to strings: the percussive onsets of the piano (due to the hammers hitting the strings) are also present in the translated audio, even though instruments like the violin and the cello are not supposed to produce percussive onsets.	abstract
2019-289	This gives the result an unusual sound, which can be interesting from an artistic point of view, but it is undesirable in the context of the original goal of the paper.	weakness
2019-289	Nevertheless, the results are quite impressive and for some combinations of instruments/styles it works surprisingly well.	strength
2019-289	The question of whether the approach is equivalent to pitch estimation followed by rendering with a different instrument is also addressed in the paper, which I appreciate.	strength
2019-289	The paper is well written and the related work section is comprehensive.	strength
2019-289	The experimental evaluation is thorough and extensive as well (although a few potentially interesting experiments seemingly didn't make the cut, see other comments).	strength
2019-289	I also like that the authors went through the trouble of doing some experiments on a publicly available dataset, to facilitate reproduction and future comparison experiments.	strength
2019-289	Other comments: * "autoregressive" should be one word everywhere	weakness
2019-289	* In section 2 it is stated that attempts to use a unified decoder with style/instrument conditioning all failed.	weakness
2019-289	I'm curious about what was tried specifically, it would be nice to discuss this.	weakness
2019-289	* The same goes for experiments based on VQ-VAE, the paper simply states that they were not able to get this working, but not what experiments were run to come to this conclusion.	weakness
2019-289	* The authors went through the trouble to modify the nv-wavenet inference kernels to support their modified architecture, which I appreciate -- will the modified kernels be made available as well?	weakness
2019-289	* The audio augmentation by pitch shifting is a surprising ingredient (but according to the authors it is also crucial).	weakness
2019-289	Some more insight as to why this is so important (rather than simply stating that it is important) would be a welcome addition.	suggestion
2019-289	* Section 3.2: "out off tune" should read "out of tune".	weakness
2019-289	* The formulation on p.7, 2nd paragraph is a bit confusing: "AMT freelancers tended to choose the same domain as the source, regardless of the real source and the presentation order." Does that mean they got it right every time?	weakness
2019-289	I suspect that is not what it means, but that is how I read it initially.	weakness
2019-289	* I don't quite understand the point of the semantic blending experiments.	weakness
2019-289	As a baseline, the same kind of blending in the raw audio space should be done, I suspect it would probably be hard to hear the difference.	weakness
2019-289	This is how cross-fading is already done in practice, and it isn't clear to me why this method would yield better results in that respect.	weakness
2019-289	The paper is strong enough without them so these could probably be left out.	weakness

2019-291	The paper describes a general neural  network architecture for predicting satisfiability.	abstract
2019-291	Specifically, the contributions include an encoding for SAT problems, and predicting SAT using a message passing method, where the embeddings for literals and clauses are iteratively changed until convergence.	abstract
2019-291	The paper seems significant considering that it brings together SAT solving and neural network architectures.	strength
2019-291	The paper is very clearly written and quite precise about its contributions.	strength
2019-291	The analysis especially figures 3,4, and 7 seems to give a nice intuitive ideas as to what the neural network is trying to do.	strength
2019-291	However, one weakness is that the problems are run on a specific type of SAT problem the authors have created.	weakness
2019-291	Of course, the authors make it clear that the objective is not really to create a.	weakness
2019-291	State-of-the-art solver but rather to understand what a neural network trying to do SAT solving is capable of doing.	weakness
2019-291	On this front, I think the paper succeeds in doing this.	strength
2019-291	One thing that was a little confusing is that why should all the literals turn to SAT (turn red) to prove SAT (as it is shown in figure 3).	weakness
2019-291	Is it that the neural network does not realize that it has found a SAT solution with a smaller subset of SAT literals.	weakness
2019-291	In other words, is it not capable of taking advantage of the problem structure.	weakness
2019-291	In general though, this seemed to be an interesting paper though its practical implications are quite hard to know either in the SAT community or in the neural network community.	weakness
2019-291	This paper presents the NeuroSAT architecture, which uses a deep, message passing neural net for predicting the satisfiability of CNF instances.	abstract
2019-291	The architecture is also able to predict a satisfiable assignment in the SAT case, and the literals involved in some minimal conflicting set of clauses (i.e. core) in the UNSAT case.	abstract
2019-291	The NeuroSAT architecture is based on a vector space embedding of literals and clauses, which exploits (with message passing) some important symmetries of SAT instances (permutation invariance and negation invariance).	abstract
2019-291	This architecture is tested on various classes of random SAT instances, involving both unstructured (RS) problems, and structured ones (e.g. graph colorings, vertex covers, dominating sets, etc.).	abstract
2019-291	Overall the paper is well-motivated, and the experimental results are quite convincing.	strength
2019-291	Arguably, the salient characteristic of NeuroSAT is to iteratively refine the confidence of literals voting for the SAT - or UNSAT - output, using a voting scheme on the last iteration of the literal matrix.	strength
2019-291	This is very interesting, and NeuroSAT might be used to help existing solvers in choosing variable orderings for tackling hard instances, or hard queries (e.g. find a core).	strength
2019-291	On the other hand, the technical description of the architecture (sec.	weakness
2019-291	3) is perhaps a little vague for having a clear intuition of how the classification task - for SAT instances - is handled in the NeuroSAT architecture.	weakness
2019-291	Namely, a brief description of the initial matrices (which encode the literal en clause embeddings) would be nice.	suggestion
2019-291	Some comments on the role played by the multilayer perceptron units and the normalization units would also be welcome.	suggestion
2019-291	The two update rules in Page 3 could be explained in more detail.	suggestion
2019-291	For the sake of clarity, I would suggest to provide a figure for depicting a transition (from iteration t-1 to iteration t) in the architecture.	suggestion
2019-291	As a minor comment, it would be nice (in Section 2) to define the main parameters n, m, and d used in the rest of the paper.	suggestion
2019-291	Concerning the experimental part of the paper, Sections 4 & 5 are well-explained but, in Section 6,  the solution decoding method, inspired from PCA is a bit confusing.	weakness
2019-291	Specifically, we don't know how a satisfying assignment is extracted from the last layer, and this should be explained in detail.	weakness
2019-291	According to Figure 4 and the comments above, it seems that a clustering method (with two centroids) is advocated, but this is not clear.	weakness
2019-291	In Table 1, the correlation between the accuracy on SAT instances, and the percent of SAT instances solved is not clear.	weakness
2019-291	Is the ratio of 70% measured on the CNF instances which have been predicted to be satisfiable?	weakness
2019-291	Or, is this ratio measured on the whole set of test instances?	weakness
2019-291	Finally, for the results established in Table 1, how many training instances and test instances have been used?	weakness
2019-291	In Section 7, some important aspects related to experiments, are missing.	weakness
2019-291	In Sec 7.1, for SR(200) tasks, was NeuroSAT tested on the same conditions as those for SR(40) tasks?	weakness
2019-291	Notably, what is the input dimension d of the embedding space here?	weakness
2019-291	(I guess that d=128 is too small for such large instances).	weakness
2019-291	Also, how many training and test instances have been used to plot the curves in Figure 5?	weakness
2019-291	For the 4,888 satisfiable instances generated in Sec. 7.2, which solver have been used to determine the satisfiability of those instances (I guess it is Minisat, but this should be mentioned somewhere).	weakness
2019-291	In Section 8, I found interesting the the ability of NeuroSAT in predicting the literals that participate in an UNSAT core.	weakness
2019-291	Indeed the problem of finding an UNSAT core in CNF instances is computationally harder than determining the satisfiability of this instance.	weakness
2019-291	So, NeuroSAT might be used here to help a solver in finding a core.	weakness
2019-291	But the notion of "confidence" should be explained in more detail in this section, and more generally, in the whole paper.	weakness
2019-291	Namely, it seems that in the last layer of each iteration, literals are voting for SAT (red colors) with some confidence (say δ)  and voting for UNSAT (blue colors) with some confidence (say δ′).	weakness
2019-291	Are δ and δ′ correlated in the neural architecture?	weakness
2019-291	And, how confidences for UNSAT votes are updated?	weakness
2019-291	Finally, I found that the different benchmarks where relevant, but I would also suggest (for future work, or in the appendix) to additionally perform experiments on the well-known random 3-SAT instances (k is fixed to 3).	suggestion
2019-291	Here, it is well-known that a phase transition (on the instances, not the solver/learner) occurs at 4.26 for the clause/variable ratio.	suggestion
2019-291	A plot displaying the performance of NeuroSAT (accuracy in predicting the label of the instance) versus the clause/variable ratio would be very helpful in assessing the robustness of NeuroSAT on the so-called "hard" instances (which are close to 4.26).	suggestion
2019-291	By extension, there have been a lot of recent work in generating "pseudo-industrial" random SAT instances, which incorporate some structure (e.g. communities) in order to mimic real-world structured SAT instances.	suggestion
2019-291	To this point, it would be interesting to analyze the performance of NeuroSAT on such pseudo-industrial instances.	suggestion
2019-291	This paper trains a neural network to solve the satisfiability problems.	abstract
2019-291	Based on the message passing neural network, it presents NeuroSAT and trains it as a classifier to predict satisfiability under a single bit of supervision.	abstract
2019-291	After training, NeuroSAT can solve problems that are larger and more difficult than it ever saw during training.	abstract
2019-291	Furthermore, the authors present a way to decode the solutions from the network's activations.	abstract
2019-291	Besides, for unsatisfiable problems, the paper also presents NeuroUNSAT, which learns to detect the contradictions in the form of UNSAT cores.	abstract
2019-291	Relevance: this paper is likely to be of interest to a large proportion of the community for several reasons.	strength
2019-291	Firstly, satisfiability problems arise from a variety of domains.	strength
2019-291	This paper starts with a new angle to solve the SAT problem.	abstract
2019-291	Secondly, it uses neural networks in the SAT problem and establishes that neural networks can learn to perform a discrete search.	abstract
2019-291	Thirdly, the system used in this paper may also help improve existing SAT solvers.	abstract
2019-291	Significance: I think the results are significant.	strength
2019-291	For the decoding satisfying assignments section, the two-dimensional PCA embeddings are very clear.	strength
2019-291	And the NeuroSAT's success rate for more significant problems and different problems has shown it's generalization ability.	strength
2019-291	Finally, the sequences of literal votes in NeuroUNSAT have proved its ability to detect unsatisfied cores.	strength
2019-291	Novelty: NeuroSAT's approach is novel.	strength
2019-291	Based on message passing neural networks, it trains a neural network to learn to solve the SAT problem.	strength
2019-291	Soundness: This paper is technically sound.	strength
2019-291	Evaluation: The experimental section is comprehensive.	strength
2019-291	There are a variety of graphs showing the performance and ability of your architecture.	strength
2019-291	However, the theoretical analysis isn't very sufficient.	weakness
2019-291	For instance, why does the change of the dataset from the original SR(n) to SRC(n,u) lead to the change of the behavior of the network from searching for a satisfying assignment indefinitely to detecting the unsatisfiable cores?	weakness
2019-291	Clarity: As a whole, the paper is clear.	strength
2019-291	The definition of the problem, the model structure, the data generation, the training procedure, and the evaluation are all well organized.	strength
2019-291	However, there is still a few points requiring more explanation.	weakness
2019-291	For instance, in figure 3, I am not sure whether darker value means larger value or smaller value because the authors only mentioned that white represents zero, blue negative and red positive.	weakness
2019-291	Also, in figure 7, I am not sure whether those black grids represent higher positive values or lower negative values.	weakness
2019-291	A few questions: What's the initialization of the two vectors the authors use for tiling operation?	weakness
2019-291	Does the initialization differ for different types of SAT problems?	weakness
2019-291	How do the authors decide the number of iterations necessary for solving a particular SAT problem?	weakness

2019-299	In this work the authors prove several claims regarding the inductive bias of gradient descent and gradient flow trained on deep linear networks with linearly separable data.	abstract
2019-299	They show that asymptotically gradient descent minimizes the risk, each weight matrix converges to its rank one approximation and the top singular vectors of two adjacent weight matrices align.	abstract
2019-299	Furthermore, for the logistic and exponential loss the induced linear predictor converges to the max margin solution.	abstract
2019-299	This work is very interesting and novel.	strength
2019-299	It provides a comprehensive and exact characterization of the dynamics of gradient descent for linear networks.	strength
2019-299	Such strong guarantees are essential for understanding neural networks and extremely rare in the realm of non-convex optimization results.	strength
2019-299	The work is a major contribution over the paper of Gunasekar et al. (2018) which assume that the risk is minimized.	strength
2019-299	The proof techniques are interesting and I believe that they will be useful in analyzing neural networks in other settings.	strength
2019-299	Regarding Lemma 3, the proof is not clear.	strength
2019-299	Lemma 8 does not exist in the paper of Soudry et al. (2017).	weakness
2019-299	It is also claimed that with probability 1 there are at most d support vectors.	weakness
2019-299	How does this relate with assumption 3, which implies that there are at least d support vectors?	weakness
2019-299	-------Revision--------- Thank you for the response.	misc
2019-299	I have not changed the original review.	misc
2019-299	Summary: This paper studies the properties of applying gradient flow and gradient descent to deep linear networks on linearly separable data.	abstract
2019-299	For strictly decreasing loss like the logistic loss, this paper shows 1) the loss goes to 0, 2) for every layer the normalized weight matrix converges to a rank-1 matrix 3) these rank-1 matrices are aligned.	abstract
2019-299	For the logistic loss, this paper further shows the linear function is the maximum margin solution.	abstract
2019-299	Comments: This paper discovers some interesting properties of deep linear networks, namely asymptotic rank-1, and the adjacent matrix alignment effect.	strength
2019-299	These discoveries are very interesting and may be useful to guide future findings for deep non-linear networks.	strength
2019-299	The analysis relies on many previous results in Du et al. 2018, Arora et al. 2018 and Soudry et al. 2017  authors did a good job in combing them and developed some techniques to give very interesting results.	strength
2019-299	There are two weaknesses. First, there is no convergence rate.	weakness
2019-299	Second, the step size assumption (Assumption 5) is unnatural.	weakness
2019-299	If the step size is set proportional to 1/t or 1/t^2  does this setup satisfies this assumption?	weakness
2019-299	Overall I think there are some interesting findings for deep linear networks and some new analysis presented, so I think this paper is above the bar.	decision
2019-299	However, I don't think this is a strong theory people due to the two weakness I mentioned.	weakness
2019-299	This paper analyzes the asymptotic convergence of GD for training deep linear network for classification using smooth monotone loss functions (e.g., the logistic loss).	abstract
2019-299	It is not a breakthrough, but indeed provides some useful insights.	strength
2019-299	Some assumptions are very restricted: (1) Linear Activation; (2) Separable data.	weakness
2019-299	However, to the best of our knowledge, these are some necessary simplifications, given current technical limit and significant lack of theoretical understanding of neural networks.	weakness
2019-299	The contribution of this paper contains multiple manifolds: For Deep Linear Network, GD tends to reduce the complexity: (1) Converge to Maximum Margin Solution;	abstract
2019-299	(2) Tends to yield extremely simple models, even for every single weight matrix.	weakness
2019-299	(3) Well aligned means handle the redundancy.	strength
2019-299	(4) Experimental results justify the implication of the proposed theory.	strength
2019-299	The authors use gradient flow analysis to provide intuition, but also present a discrete time analysis.	strength
2019-299	The only other drawbacks I could find are (1) The paper only analyze the asymptotic convergence; (2) The step size for discrete time analysis is a bit artificial.	weakness
2019-299	Given the difficulty of the problem, both are acceptable to me.	misc

2019-355	Main contribution: devising and evaluating an algorithm to find inputs that trigger arbitrary "egregious" outputs ("I will kill you") in vanilla sequence-to-sequence models, as a white-box attack on NLG models.	abstract
2019-355	Clarity: The paper is overall clear.	strength
2019-355	I found some of the appendices (esp.	strength
2019-355	B and C) to be important for understanding the paper and believe these should be in the main paper.	strength
2019-355	Moving parts of Appendix A in the main text would also add to the clarity.	strength
2019-355	Originality: The work looks original. It is an extension of previous attacks on seq2seq models, such as the targeted-keyword-attack from (Cheng et al., 2018) in which the model is made to produce a keyword chosen by the attacker.	strength
2019-355	Significance of contribution: The lack of control over the outputs of seq2seq is a major roadblock towards their broader adoption.	weakness
2019-355	The authors propose two algorithms for trying to find inputs creating given outputs, a simple one relying on continuous optimization this is shown not to work (breaking when projecting back into words), and another based relying on discrete optimization.	abstract
2019-355	The authors found that the task is hard when using greedy decoding, but often doable using sampled decoding (note that in this case, the model will generate a different output every time).	abstract
2019-355	My take-aways are that the task is hard and the results highlight that vanilla seq2seq models are pretty hard to manipulate; however it is interesting to see that with sampling, models may sometimes be tricked into producing really bad outputs.	weakness
2019-355	This white-box attack applicable to any chatbot.	weakness
2019-355	As the authors noted, an egregious output for one application ("go to hell" for customer service) may not be egregious for another one ("go to hell" in MT).	weakness
2019-355	Overall, the authors ask an interesting question: how easy is it to craft an input for a seq2seq model that will make it produce a "very bad" output.	weakness
2019-355	The work is novel, several algorithms are introduced to try to solve the problem and a comprehensive analysis of the results is presented.	strength
2019-355	The attack is still of limited practicality, but this paper feels like a nice step towards more natural adversarial attacks in NLG.	strength
2019-355	One last thing: the title seems a bit misleading, the work is not about "detecting" egregious outputs.	weakness
2019-355	This paper explores the task of finding discrete adversarial examples for (current) dialog models in a post hoc manner (i.e., once models are trained).	abstract
2019-355	In particular, the authors propose an optimization procedure for crafting inputs (utterances) that trigger trained dialog models to respond in an egregious manner.	abstract
2019-355	This line of research is interesting as it relates to real-world problems that our models face before they can be safely deployed.	strength
2019-355	The paper is easy to read, nicely written, and the proposed optimization method seems reasonable.	strength
2019-355	The study also seems clear and the results are fairly robust across three datasets.	strength
2019-355	It was also interesting to study datasets which, a priori, seem like they would not contain much egregious content (e.g., Ubuntu "help desk" conversations).	strength
2019-355	My main question is that after reading the paper, I'm not sure that one has an answer to the question that the authors set out to answer.	strength
2019-355	In particular, are our current seq2seq models for dialogs prone to generating egregious responses?	strength
2019-355	On one hand, it seems like models can assign higher-than-average probability to egregious responses.	weakness
2019-355	On the other, it is unclear what this means.	weakness
2019-355	For example, it seems like the possibility that such a model outputs such an answer in a conversation might still be very small.	weakness
2019-355	Quantifying this would be worthwhile.	suggestion
2019-355	Further, one would imagine that a complete dialog system pipeline would contain a collection of different models including a seq2seq model but also others.	suggestion
2019-355	In that context, is it clear that it's the role of the seq2seq model to limit egregious responses?	weakness
2019-355	A related aspect is that it would have been interesting to explore a bit more the reasons that cause the generation of such egregious responses.	suggestion
2019-355	It is unclear how representative is the example that is detailed ("I will kill you" in Section 5.3).	weakness
2019-355	Are other examples using words in other contexts?	weakness
2019-355	Also, it seems reasonable that if one wants to avoid such answers, countermeasures (e.g., in designing the loss or in adding common sense knowledge) have to be considered.	weakness
2019-355	Other comments: - I am not sure of the value of Section 3.	weakness
2019-355	In particular, it seems like the presentation of the paper would be as effective if this section was summarized in a short paragraph (and perhaps detailed in an appendix).	weakness
2019-355	- Section 3.1, "continuous relaxation of the input embedding", what does that mean since the embedding already lives in continuous space?	weakness
2019-355	- I understand that your study only considers (when optimizing for egregious responses)) dialogs that are 1-turn long.	weakness
2019-355	I wonder if you could increase hit rates by crafting multiple inputs at once.	suggestion
2019-355	- In Section 4.3, you fix G (size of the word search space) to 100.	suggestion
2019-355	Have you tried different values?	suggestion
2019-355	Do you know if larger Gs could have an impact of reported hit metrics?	suggestion
2019-355	- In Table 3, results from the first column (normal, o-greedy) seem interesting.	weakness
2019-355	Wouldn't one expect that the model can actually generate (almost) all normal responses?	weakness
2019-355	Your results indicate that for Ubuntu models can only generate between 65% and 82% of actual (test) responses.	weakness
2019-355	Do you know what in the Ubuntu corpus leads to such a result?	weakness
2019-355	- In Section 5.3, you seem to say that the lack of diversity of greedy-decoded sentences is related to the low performance of the "o-greedy" metric.	weakness
2019-355	Could this result simply be explained because the model is unlikely to generate sentences that it has never seen before?	weakness
2019-355	You could try changing the temperature of the decoding distribution, that should improve diversity and you could then check whether or not that also increases the hit rate of the o-greedy metric.	suggestion
2019-355	- Perhaps tailoring the mal lists to each specific dataset would make sense (I understand that there is already some differences in between the mal lists of the different datasets but perhaps building the lists with a particular dataset in mind would yield "better" results).	suggestion
2019-355	# Positive aspects of this submission	strength
2019-355	- This submission explores a very interesting problem that is often overlooked in sequence-to-sequence models research.	strength
2019-355	- The methodology in Sections 4 and 5 is very thorough and useful.	strength
2019-355	- Good comparison of last-h with attention representations, which gives good insight about the robustness of each architecture against adversarial attacks.	strength
2019-355	# Criticism - In Section 3, even if the "l1 + projection" experiments seem to show that generating egregious outputs with greedy decoding is very unlikely, it doesn't definitely prove so.	weakness
2019-355	It could be that your discrete optimization algorithm is suboptimal, especially given that other works on adversarial attacks for seq2seq models use different methods such as gradient regularization (Cheng et al. 2018).	weakness
2019-355	Similarly, the brute-force results on a simplified task in Appendix B are useful, but it's hard to tell whether the conclusions of this experiment can be extrapolated to the original dialog task.	weakness
2019-355	Given that you also study "o-greedy-hit" in more detail with a different algorithm in Sections 4 and 5, I would consider removing Section 3 or moving it to the Appendix for consistency.	suggestion

2019-380	Summary: The authors propose to apply the Deep Variational Information Bottleneck (VIB) method of [1] on discriminator networks in various adversarial-learning-based scenarios.	abstract
2019-380	They propose a way to adaptively update the value for the bêta hyper-parameter to respect the constraint on I(X,Z).	abstract
2019-380	Their technique is shown to stabilize/allow training when P_g and P_data do not overlap, similarly to WGAN and gradient-penalty based approaches, by essentially pushing their representation distributions (p_z) to overlap with the mutual information bottleneck.	abstract
2019-380	It can also be considered as an adaptive version of instance noise, which serves the same goal.	abstract
2019-380	The method is evaluated on different adversarial learning setup (imitation learning, inverse reinforcement learning and GANs), where it compares positively to most related methods.	abstract
2019-380	Best results for 'classical' adversarial learning for image generation are however obtained when combining the proposed VIB with gradient penalty (which outperforms by itself the VGAN in this case).	abstract
2019-380	Pros : - This paper brings a good amount of evidence of the benefits to use the VIB formulation to adversarial learning by first showing the effect of such approach on a toy example, and then applying it to more complex scenarios, where it also boosts performance.	strength
2019-380	The numerous experiments and analyses have great value and are a necessity as this paper mostly applies the VIB to new learning challenges.	strength
2019-380	- The proposition of a principled way of adaptively varying the value of Bêta to actually respect more closely the constraint I(X,Z) < I_c, which to my knowledge [1] does not perform, is definitely appealing and seems to work better than fixed Bêtas and does also bring the KL divergence to the desired I_c.	strength
2019-380	- The technique is fairly simple to implement and can be combined with other stabilization techniques such as gradient penalties on the discriminator.	strength
2019-380	Cons: - In my view, the novelty of the approach is somewhat limited, as it seems like a straightforward application of the VIB from [1] for discriminators in adversarial learning, with the difference of using an adaptive Bêta.	weakness
2019-380	- I think the Bêta-VAE [2] paper is definitely related to this paper and to the paper on which it is based [1] and should thus be cited as the authors use a similar regularization technique, albeit from a different perspective, that restricts I(X,Z) in an auto-encoding task.	weakness
2019-380	- I think the content of batches used to regularize E(z|x) w.r.t. to the KL divergence should be clarified, as the description of p^tilde "being a mixture of the target distribution and the generator" (Section 4) can let the implementation details be ambiguous.	weakness
2019-380	I think batches containing samples from both distributions can cause problems as the expectation of the KL divergence on a batch can be low even if the samples from both distributions are projected into different parts of the manifold.	weakness
2019-380	This makes me think batches are separated?	weakness
2019-380	Either way, this should be more clearly stated in the text.	weakness
2019-380	- The last results for  the 'traditional' GAN+VIB show that in this case, gradient penalty (GP) alone outperforms the proposed VGAN, and that both can be combined for best results.	weakness
2019-380	I thus wonder if the results in all other experiments could show similar trends if GP had been tested in these cases as well.	weakness
2019-380	In the imitation learning task, authors compare with instance noise, but not with GP, which for me are both related to VIB in what they try to accomplish.	weakness
2019-380	Was GP tested in Imitation Learning/Inverse RL ?	weakness
2019-380	Was it better? Could it still be combined with VIB for better results?	weakness
2019-380	- In the saliency map of Figure 5, I'm unclear as to what the colors represent (especially on the GAIL side).	weakness
2019-380	I doubt that this is simply due to the colormap used, but this colormap should be presented.	weakness
2019-380	Overall, I think this is an interesting and relevant paper that I am very likely to suggest to peers working on adversarial learning, and should therefore be presented.	decision
2019-380	I think the limited novelty is counterbalanced by the quality of empirical analysis.	weakness
2019-380	Some clarity issues and missing citations should be easy to correct.	weakness
2019-380	I appreciate the comparison and combination with a competitive method (Gradient Penalty) in Section 5.3, but I wish similar results were present in the other experiments, in order to inform readers if, in these cases as well, combining VIB with GP leads to the best performance.	suggestion
2019-380	[1] Deep Variational Information Bottleneck, (Alemi et al. 2017) [2] beta-VAE: Learning Basic Visual Concepts with a Constrained Variational Framework (Higgins et al. 2017) The paper "Variational Discriminator Bottleneck: Improving Imitation Learning, Inverse RL, and GANs by Constraining Information Flow" tackles the problem of discriminator over-fitting in adversarial learning.	abstract
2019-380	Balancing the generator and the discriminator is difficult in generative adversarial techniques, as a too good discriminator prevents the generator to converge toward effective distributions.	abstract
2019-380	The idea is to introduce an information constraint on a intermediate layer, called information bottleneck, which limits the content of this layer to the most discriminative features of the input.	abstract
2019-380	Based on this limited representation of the input, the disciminator is constrained to longer tailed-distributions, maintaining some uncertainty on simulated data distributions.	abstract
2019-380	Results show that the proposal outperforms previous researches on discriminator over-fitting, such as noise adding in the discriminator inputs.	abstract
2019-380	While the use of information bottleneck is not novel, its application in adversarial learning looks inovative and the results are impressive in a broad range of applications.	strength
2019-380	The paper is well-written and easy to follow, though I find that it would be nice to give more insights on the intuition about information bottleneck in the preliminary section to make the paper self-contained (I had to read the previous work from Alemi et al (2016) to realize what information bottleneck can bring).	suggestion
2019-380	My only question is about the setting of the constaint Ic: wouldn't it be possible to consider an adaptative version which could consider the amount of zeros gradients returned to the generator ?	weakness
2019-380	This paper proposed a constraint on the discriminator of GAN model to maintain informative gradients.	abstract
2019-380	It is completed by control the mutual information between the observations and the discriminator's internal representation to be no bigger than a predefined value.	abstract
2019-380	The idea is interesting and the discussions of applications in different areas are useful.	strength
2019-380	However, I still have some concerns about the work: 1. in the experiments about image generation, it seems that the proposed method does not enhance the performance obviously when compared to GP and WGAN-GP, Why the combination of VGAN and GP can enhance the performance greatly(How do they complementary to each other), what about the performance when combine VGAN with WGAN-GP?	weakness
2019-380	2. How do you combine VGAN and GP, is there any parameter to balance their effect?	weakness
2019-380	3. The author stated on page 2 that "the  proposed information bottleneck encourages the discriminator to ignore irrelevant cues, which then allows the generator to focus on improving the most discerning differences between real and fake samples", a proof on theory or experiments should be used to illustrate this state.	weakness
2019-380	4. Is it possible to apply GP and WGAN-GP to the Motion imitation or adversarial inverse reinforcement learning problems?	weakness
2019-380	If so, will it perform better than VGAN?	weakness
2019-380	5. How about VGAN compares with Spectral norm GAN?	weakness

2019-423	This paper proposes graph-convolutional GANs for irregular 3D point clouds that learn domain (the graph structure) and features at the same time.	abstract
2019-423	In addition, a method for upsampling at the GAN generator is introduced.	abstract
2019-423	The paper is very well written, addresses a relevant problem (classification of 3D point clouds with arbitrary, a priori unknown graph structure) in an original way, and supports the presented ideas with convincing experiments.	strength
2019-423	It aggregates the latest developments in the field, the Wasserstein GAN, edge-conditional convolutions into a concise framework and designs a novel GAN generator.	strength
2019-423	I have only some minor concerns: 1) My only serious concern is the degree of novelty with respect to (Achlioptas et al., 2017).	weakness
2019-423	The discriminator is the same and although the generator is a fully connected network in that paper, it would be good to highlight conceptual improvements as well as quantitative advantages of the paper at hand more thoroughly.	weakness
2019-423	Similarly, expanding a bit more on the differences and improvements over (Grover et al., 2018) would improve the paper.	suggestion
2019-423	2) P3, second to last line of 2.1: reference needs to be fixed "…Grover et al. (Grover et al., 2018)"	weakness
2019-423	3) It would be helpful to highlight the usefulness of artificially generating irregular 3D point clouds from an application perspective, too.	suggestion
2019-423	While GANs have various applications if applied to images it is not obvious how artificially created irregular 3D point clouds can be useful.	weakness
2019-423	Although the theoretical insights presented in the paper are exciting, a more high-level motivation would further improve its quality.	weakness
2019-423	4) A discussion of shortcomings of the presented method seems missing.	weakness
2019-423	While it is understandable that emphasis is put on novelty and its advantages, it would be interesting to see where the authors see room for improvement.	suggestion
2019-423	The authors present a method for generating points clouds with the help of graph convolution and a novel upsampling scheme.	abstract
2019-423	The proposed method exploits the pairwise distances between node features to build a NN-graph.	abstract
2019-423	The upsampling scheme generates new points via a slimmed down graph convolution, which are then concatenated to the initial node features.	abstract
2019-423	The proposed method is evaluated on four categories of the ShapeNet dataset.	abstract
2019-423	Resulting point clouds are evaluated via a qualitative and quantitative comparison to r-GAN.	abstract
2019-423	As far as I know, the paper introduces an overall novel and interesting idea to generate point clouds with localized operations.	strength
2019-423	The following questions could be addressed by the authors in a revised manuscript: * The upsampling operation is not well motivated, e.g., neighboring node features are weighted independently, but root node features are not.	weakness
2019-423	What is the intuition besides reducing the number of parameters?	weakness
2019-423	Are there significant differences when not using diagonal weight matrices?	weakness
2019-423	* As computation of pairwise node feature distances and graph generation based on nearest neighbors are expensive tasks, more details on the practical running time and theoretical complexity should be provided.	weakness
2019-423	Can the complexity be reduced by rebuilding graphs only after upsampling layers?	weakness
2019-423	How would this impact the performance of the proposed model?	weakness
2019-423	* Although the evaluation on four categories is reported, Table 2 only gives results for two categories.	weakness
2019-423	* How is the method related to GANs which generates graphs, such as GraphGAN or NetGAN?	weakness
2019-423	The paper proposes a version of GANs specifically designed for generating point clouds.	abstract
2019-423	The core contribution of the work is the upsampling operation: in short, it takes as an input N points, and produces N more points (one per input) by applying a graph convolution-like operation.	abstract
2019-423	Pros: + The problem of making scalable generative models for point clouds is clearly important, and using local operations in that context makes a lot of sense.	strength
2019-423	Cons: - The paper is not particularly well-written, is often hard to follow, and contains a couple of confusing statements (see a non-exhaustive list of remarks below).	weakness
2019-423	- The experimental evaluation seems insufficient: clearly it is possible to come up with more baselines.	weakness
2019-423	Even a comparison to other types of generative models would be useful (e.g. variants of VAEs, other types of GANs).	suggestion
2019-423	There also alternative local graph-convolution-like operations (e.g. tangent convolutions) that are designed for point clouds.	suggestion
2019-423	In addition, it is quite strange that results are reported not for all the classes in the dataset.	weakness
2019-423	Various remarks: p.1, "whereby it learns to exploit a self-similarity prior to sample the data distribution": this is a confusing statement.	weakness
2019-423	p.2, "(GANs) have been shown on images to provide better approximations of the data distribution than other generative models": This statement is earthier too strong (all other models) or does not say much (some other models)	weakness
2019-423	p.2, "However, this means that they are unable to learn localized features or exploit weight sharing.	weakness
2019-423	": I see the point about no weight sharing in the generator, but feature learning p.3, "the key difference with the work in this paper is that PointNet and PointNet++ are not generative models, but are used in supervised problems such as classification or segmentation.	weakness
2019-423	": Yet, the kind of operation that is used in the pointnet++ is quite similar to what you propose?	weakness
2019-423	p.4: "because the high dimensionality of the feature vectors makes the gridding approach unfeasible.": but you are actually dealing with the point clouds where each point is 3D?	weakness

2019-461	In this paper, the authors claim that they are able to update the generator better to avoid generator mode collapse and also increase the stability of GANs training by indirectly increasing the entropy of the generator until it matches the entropy of the original data distribution using functional gradient methods.	abstract
2019-461	The paper is interesting and well written.	strength
2019-461	However, there is a lot of work coming out in the field of GANs currently, so I am not able to comment on the novelty of this regularization approach, and I am interested to know how this method performs when compared to other techniques to avoid mode collapse such as feature matching and mini-batch discrimination, etc.	weakness
2019-461	The authors make use of the theory of functional gradient, based on optimal transport, to develop a method that can promote the entropy of the generator distribution without directly estimating the entropy itself.	abstract
2019-461	Theoretical results are provided as well as necessary experiments to support their technique's outperformance in some data sets.	abstract
2019-461	I found that this is an interesting paper, both original ideal and numerical results.	strength
2019-461	GANs (generative adversarial network) represent a recently introduced min-max generative modelling scheme with several successful applications.	abstract
2019-461	Unfortunately, GANs often show unstable behaviour during the training phase.	abstract
2019-461	The authors of the submission propose a functional-gradient type entropy-promoting approach to tackle this problem, as estimating entropy is computationally difficult.	abstract
2019-461	While the idea of the submission might be useful in some applications, the work is rather vaguely written, it is in draft phase: 1. Abbreviations, notations are not defined: GAN, WGAN-GP, DNN, FID (the complete name only shows up in Section 4), softplus, sigmoid, D_{\\theta_{old}}, ...	weakness
2019-461	2. While the primary motivation of the work is claimed to be 'mode collapse', it does not turn out from the submission what mode collapse is.	weakness
2019-461	3. Estimating entropies is a standard practice in statistics and machine learning, with an arsenal of estimators; the motivation of the submission is questionable.	weakness
2019-461	4. Differentiation w.r.t. functions (or more generally elements in normed spaces) is a well-defined concept in mathematics, including the notions of Gateaux, Frechet and Hadamard differentiability.	weakness
2019-461	It is not clear why the authors neglect these classical concepts, and are talking about 'random functional perturbations', ...	weakness
2019-461	It is also unclear where the optimized transformation (T) lives; the authors are trying to differentiate over some function space which is undefined.	weakness
2019-461	While the idea of the work might be useful in practice, the current submission requires significant revision and work before publication.	decision
2019-461	--- After paper revisions: Thank you for the updates.	misc
2019-461	The submission definitely improved. I have changed my score to '6: Marginally above acceptance threshold'; the suggested regularization can be a useful heuristic for the GAN community.	rebuttal_process
2019-461	Summary: This paper proposes distributional concavity regularization for GANs which encourages producing generator distributions with higher entropy.	abstract
2019-461	The paper motivates the proposed method as follows: -       Using the concept of functional gradient, the paper interprets the update in the generator parameters as an update in the generator distribution	abstract
2019-461	-       Given this functional gradient perspective, the paper proposes updating the generator distribution toward a target distribution which has *higher entropy and satisfies monoticity*	abstract
2019-461	-       Then, the paper proves that this condition can be satisfied by ensuring that generator's objective (L) is concave	abstract
2019-461	-       Since it's difficult to ensure concavity when parametrizing generators as deep neural networks, the paper proposes adding a simple penalty term that encourages the concavity of generator objective	abstract
2019-461	Experiments confirm the validity the proposed approach.	strength
2019-461	Interestingly, the paper shows that performance of multiple GAN variants can be improved with their proposed method on several image datasets	strength
2019-461	Strengths: -    The proposed method is very interesting and is based on sound theory	strength
2019-461	-    Connection to optimal transport theory is also interesting	strength
2019-461	-    In practice, the method is very simple to implement and seems to produce good results	strength
2019-461	Weaknesses: -       Readability of the paper can be generally improved.	weakness
2019-461	I had to go over the paper many times to get the idea.	misc
2019-461	-       Figures should be provided with more detailed captions, which explain main result and providing context (e.g. explaining baselines).	weakness
2019-461	Questions/Comments: -       Equation (7) has typos (uses theta_old instead of theta in some places)	weakness
2019-461	-       Section 4.1 (effect of monoticity) is a bit confusing.	weakness
2019-461	My understanding is that parameter update rule of equation (3) and (6) are equivalent, but you seem to use (6) there.	weakness
2019-461	Can you clarify what you do there and in general this experiment a bit more?	weakness
2019-461	-       Comparing with entropy maximization method of EGAN (Dai et al, 2017) is a good idea, but I'm wondering if you can compare it on low dimensional settings (e.g. as in Fig 2).	weakness
2019-461	It is also not clear why increasing entropy with EGAN-VI is worse than baselines in Table 1.	weakness
2019-461	Overall recommendation: The paper is based on sound theory and provides very interesting perspective.	strength
2019-461	The method seems to work in practice on a variety of experimental setting.	strength
2019-461	Therefore, I recommend accepting it.	decision

2019-473	Dual Block-Coordinate Frank-Wolfe (Dual-BCFW) has been widely used in the literature of non-smooth and strongly-convex stochastic optimization problems, such as (structural) Support Vector Machine.	abstract
2019-473	To my knowledge, the submission is the first sound attempt to adapt this type of Dual-based algorithm for optimization of Deep Neural Network, which employs a proximal-point method that linearizes not the whole loss function but only the DNN (up to the logits) to form a convex subproblem and then deal with the loss part in the dual.	strength
2019-473	The attempt is not perfect (actually with a couple of issues detailed below), but the proposed approach is inspiring and I personally would love it published to encourage more development along this thread.	strength
2019-473	The following points out a couple of items that could probably help further improve the paper.	misc
2019-473	*FW vs BCFW* The algorithm employed in the paper is actually not Frank-Wolfe (FW) but Block-Coordinate Frank-Wolfe (BCFW), as it minimizes w.r.t. a block of dual variables belonging to the min-batch of samples.	misc
2019-473	*Batch Size* Though the algorithm can be easily extended to the min-batch case, the author should discuss more how the batch size is interpreted in this case (i.e. minimizing w.r.t.	weakness
2019-473	a larger block of dual variables belonging to the batch of samples) and the algorithmic block (Algorithm	weakness
2019-473	1) should be presented in a way reflecting the batch size since this is the way people use an algorithm in practice (to improve the utilization rate of a GPU).	weakness
2019-473	*Convex-Conjugate Loss* The Dual FW algorithm does not need to be used along with the hinge loss (SVM loss).	weakness
2019-473	All convex loss function can derive a dual formulation based on its convex-conjugate.	weakness
2019-473	See [1,2] for examples. It would be more insightful to compare SGD vs dual-BCFW when both of them are optimizing the same loss functions (either hinge loss or cross-entropy loss) in the experimental comparison.	suggestion
2019-473	[1] Shalev-Shwartz, Shai, and Tong Zhang.	misc
2019-473	"Stochastic dual coordinate ascent methods for regularized loss minimization." JMLR (2013)	misc
2019-473	[2] Tomioka, Ryota, Taiji Suzuki, and Masashi Sugiyama.	misc
2019-473	"Super-linear convergence of dual augmented Lagrangian algorithm for sparsity regularized estimation." JMLR (2011).	misc
2019-473	*BCFW vs BCD* Actually, (Lacoste-Julien, S.	misc
2019-473	et al., 2013) proposes Dual-BCFW to optimize structural SVM because the problem contains exponentially many number of dual variables.	abstract
2019-473	For typical multiclass hinge loss problem the Dual Block-Coordinate Descent that minimizes w.r.t. all dual variables of a sample in a closed-form update converges faster without extra computational cost.	abstract
2019-473	See the details in, for example, [3, appendix for the multiclass hinge loss case].	misc
2019-473	[3] Fan, Rong-En, et al. "LIBLINEAR: A library for large linear classification." JMLR (2008).	misc
2019-473	*Hyper-Parameter* The proposed dual-BCFW still contains a hyperparameter (eta) due to the need to introduce a convex subproblem, which makes its number of hyperparameters still the same to SGD.	rebuttal_process
2019-473	This paper proposes a Frank-Wolfe based method, called DFW, for training Deep Network.	abstract
2019-473	The DFW method linearizes the loss function into a smooth one, and also adopts Nesterov Momentum to accelerate the training.	abstract
2019-473	Both techniques have been widely used in the literature for similar settings.	abstract
2019-473	This paper mainly focuses on the algorithm part, but only empirically demonstrate the convergence results.	weakness
2019-473	After reading the authors' feedback and the paper again, I think overall this is a good paper and should be of broader interest to the broader audience in machine learning community.	rebuttal_process
2019-473	In Section 6.1, the authors mention the good generalization is due to large number of steps at a high learning rate.	rebuttal_process
2019-473	Can we possibly get any theoretical justification on this?	rebuttal_process
2019-473	This paper uses multi class hinge loss as an example for illustration.	abstract
2019-473	Can this approach be applied for structure prediction, for example, various ranking loss?	rebuttal_process
2019-473	This paper introduced a proximal approach to optimize neural networks by linearizing the network output instead of the loss function.	abstract
2019-473	They demonstrate their algorithm on multi-class hinge loss, where they can show that optimal step size can be computed in close form without significant additional cost.	abstract
2019-473	Their experimental results showed competitive performance to SGD/Adam on the same network architectures.	abstract
2019-473	1. Figure 1 is crucial to the algorithm design as it aims to prove that Loss-Preserving Linearization (LPL) preserves information on loss function.	abstract
2019-473	While the authors provided numerical plots to compare it with the SGD linearization, I personally prefer to see some analytically comparsion between SGD linearization and LPL even on the simplest case.	rebuttal_process
2019-473	An appendix with more numerical comparisons on other loss functions might also be insightful.	rebuttal_process
2019-473	2. It seems LPL is mainly compared to SGD for convergence (e.g. Fig 2).	weakness
2019-473	In Table 2 I saw some optimizers end up with much lower test accuracy.	weakness
2019-473	Can the authors show the convergence plots of these methods (similar to Figure 2)?	weakness

2019-490	This paper proposes a novel approach with the hypothesis that the reliable features can guide the less reliable ones.	abstract
2019-490	This approach is applied to the object detection task and show consistent performance improvements.	abstract
2019-490	pros) (+) This paper is well-written and easy to follow.	strength
2019-490	(+) The base idea that divides the learned features into two sets; the reliable feature set and the less reliable one is very interesting and looks novel.	strength
2019-490	Plus, the hypothesis, which is that reliable features can guide the features in the less reliable set is also interesting.	strength
2019-490	(+) The performance improvements are quite large.	strength
2019-490	(+) Extensive ablative studies are provided to support the proposed method well.	strength
2019-490	cons) (-) The method of obtaining the representative in buffer B is not clearly presented.	weakness
2019-490	(-) The overall training and inference procedure are not clearly presented.	weakness
2019-490	(-) Some notations and descriptions are vague and confusing.	weakness
2019-490	(-) More than two datasets are necessary to show the effectiveness of the methods comments)	weakness
2019-490	- What is the higher level feature map P_m?	weakness
2019-490	and How did you choose the higher level feature map at the m-th level in option (b) and (c) in Section 3.3.	weakness
2019-490	- What is the meaning of the "past" features in Section 3.2?	weakness
2019-490	- It is better to show the exact architecture of the make-up module and the critic module.	weakness
2019-490	- Can this method apply to the other backbones such as VGG or ResNets without FPN?	weakness
2019-490	- The sentences at the bottom of p.4 starting with "Note that only~" looks ambiguous.	weakness
2019-490	- f_critic^j may be the j-th element of F_critic, please denote what f_critic^j stands for.	weakness
2019-490	Even if the paper needs to be revised for better readability, I think this paper is above the standard of *CONF* because the idea is interesting and novel.	decision
2019-490	Furthermore, the experimental studies are properly designed and well support the main idea.	strength
2019-490	I am leaning toward acceptance, but I would like to see the other reviewers' comments.	decision
2019-490	This paper aims to facilitate feature learning in NN models by exploiting more from reliable examples.	abstract
2019-490	This is very similar to self-paced learning where the model  learns from the easier samples at first and proceeds to learn from difficult and challenging samples.	abstract
2019-490	The authors should discuss their difference with self-paced learning.	suggestion
2019-490	The method is positioned as a general one for feature learning.	abstract
2019-490	I do not know the reason why the authors only apply for object detection on a very specific dataset.	weakness
2019-490	It is expected to see whether the proposed method is also effective for image classification.	weakness
2019-490	More datasets for evaluation are needed, even only for the object detection application.	weakness
2019-490	OVERVIEW: The authors tackle the problem of detecting small/low resolution objects in an image.	abstract
2019-490	Their key idea is that detecting bigger objects is an easier task and can be used to guide the detection of smaller objects.	abstract
2019-490	This is done using the "Feature Intertwiner"  which consists of two branches, one for the larger objects (more reliable set that is also easier to detect) and one for the smaller objects (less reliable set).	abstract
2019-490	The second branch contains a make-up layer learned during training (which acts as the guidance from the more reliable set) that helps compensate details needed for detection.	abstract
2019-490	The authors define a class buffer that contains representative elements of object features from the reliable set for every category & scale and an intertwiner loss that computes the L2 loss between the features from the less reliable set & the class buffer.	abstract
2019-490	They also use an Optimal Transport procedure with a Sinkhorn divergence loss between object features from both sets.	abstract
2019-490	The overall loss of the system is now a sum of the detection loss, the intertwiner loss and the optimal transport loss.	abstract
2019-490	They evaluate their model on the COCO Object detection challenge showing state-of-the-art performance.	abstract
2019-490	They also provide thorough ablation analysis of various design choices.	abstract
2019-490	The qualitative result in Fig.1 showing well clustered features for both high & low resolution objects via t-SNE is a nice touch.	strength
2019-490	COMMENTS: Clarity - The paper is well written and easy to follow.	strength
2019-490	Originality & Significance - The paper tackles an important problem and provides a novel solution.	strength
2019-490	Quality - The paper is complete in that it tackles an important problem, provides a novel solution and demonstrates via thorough experiments the improvement achieved using their approach.	strength
2019-490	QUESTIONS: 1. The Class Buffer seems very restricted in having a single element per object category per scale to represent all features.	weakness
2019-490	The advantage of forcing such a representation is tight clustering in the feature space.	weakness
2019-490	But, wouldn't a dictionary approach with multiple elements give more flexibility to the model and learn a richer feature representation at the cost of not-so-good clustering ?	weakness
2019-490	2. Any comment on why you drop performance for couch ?	weakness
2019-490	(and baseball bat + bedroll)	weakness
2019-490	3. In Table 4 of Appendix where you compare with more object detection results, I find it interesting that Mask RCNN, updated results has a might higher AP_S (43.5) compared to you (27.2) and everyone else.	weakness
2019-490	I was expecting you to be the best under that metric due to the explicit design for small objects.	weakness
2019-490	They (MaskRCNN, updated results) are also significantly better than the rest under AP_M but worse under AP_L.	weakness
2019-490	Can you explain this behavior ?	weakness
2019-490	Is the ResNeXt backbone that much better for small objects ?	weakness

2019-499	The manuscript entitled "Kernel Change-Point Detection with Auxiliary Deep Generative Models" describes a novel approach to optimising the choice of kernel towards increased testing power in this challenging machine learning problem.	abstract
2019-499	The proposed method is shown to offer improvements over alternatives on a set of real data problems and the minimax objective identified is well motivated, however, I am not entirely convinced that (a) the performance improvements arise for the hypothesised reasons, and (b) that the test setting is of wide applicability.	weakness
2019-499	A fundamental distinction between parametric and non-parametric tests for CPD in timeseries data is that the adoption of parametric assumptions allows for an easier introduction of strict but meaningful relationships in the temporal structure---e.g. a first order autoregressive model introduces a simple Markov structure---whereas non-parametric kernel tests typically imagine samples to be iid (before and after the change-point).	weakness
2019-499	For this reason, the non-parametric tests may lack robustness to certain realistic types of temporal distributional changes: e.g. in the parameter of an autoregressive timeseries.	weakness
2019-499	On the other hand, it may be prohibitively difficult to design parametric models to well characterise high dimensional data, whereas non-parametric models can typically do well in high dimension when the available data volumes are large.	weakness
2019-499	In the present application it seems that the setting imagined is for low dimensional data of limited size in which there is likely to be non-iid temporal structure (i.e., outside the easy relative advantage of non-parametric methods).	weakness
2019-499	For this reason it seems to me the key advantage offered by the proposed approach with its use of a distributional autoregressive process for the surrogate model may well be to introduce robustness against Type 1 errors due to otherwise unrepresented temporal structure in the base distribution (P).	strength
2019-499	In summarising the performance results by AUC it is unclear whether it is indeed the desired improvement in test power that offers the advantages or whether it is in fact a decrease in Type 1 errors.	weakness
2019-499	Another side of my concern here is that I disagree with the statement: "As no prior knowledge of Q ...	weakness
2019-499	intuitiviely, we have to make G as close to P as possible" interpretted as a way to maximise test power; as a way to minimise Type 1 errors, yes.	weakness
2019-499	Across change-point detection methods it is also important to distinguish key aspects of the problem formulation.	weakness
2019-499	One particular specification here is that we have already some labelled instances of data known to come from the P distribution, and perhaps also some fewer instances of data labelled from Q.	weakness
2019-499	This is distinct from fully automated change point detection methods for time series such as automatic scene selection in video data.	weakness
2019-499	Another dissimilarity to that archetypal scenario is that here we suppose the P and Q distributions may have subtle differences that we're interested in; and it would also seem that we assume there is only one change-point to detect.	weakness
2019-499	Or at least the algorithm does not seem to be designed to be applied in a recursive sense as it would be for scene selection.	weakness
2019-499	Finally there is no discussion here of computational complexity and cost?	weakness
2019-499	+ Using a generative model as the surrogate distribution for kernel two-sample test is novel	strength
2019-499	+ An important and new application of deep generative models	strength
2019-499	+ Strong experiments on synthetic and real-world time series data sets	strength
2019-499	+ Very clear writing and explanation of the idea	strength
2019-499	- reply sample segments from both directions (past and future) while in the practical setting, CPD is usually sequential and in one directional	strength
2019-499	- lack theoretical understanding of the limit of the neural-generator in the kernel two-sample test A new approach to choose a kernel to maximize the test power, for the kernel change-point detection.	strength
2019-499	This provides an extension to the two-sample version of the problem (Gretton et al. 2012b, Sutherland et al. 2017).	strength
2019-499	The difficulty is caused by that there is very limited samples from the abnormal distribution.	weakness
2019-499	The idea is based on choosing a surrogate distributions using generative model.	weakness
2019-499	The idea makes sense although there seems to be not much detail in how to choose the surrogate distribution.	strength
2019-499	There is a mechanism to study the threshold.	strength
2019-499	Real-data and simulation demonstrates the good performance.	strength
2019-499	I think the idea is really interesting and I am impressed by the completeness of the work.	strength

2020-371	Interpreting the policies of RL agents is an important consideration if we would like to actually deploy them and understand their behaviours.	weakness
2020-371	Prior works have applied saliency methods to DRL agents, but the authors note that there are two properties - specificity and relevance - that these methods do not take into account, and therefore result in misleading saliency maps.	abstract
2020-371	The authors define (and provide examples) of these properties, propose simple ways to calculate these (and like prior methods, relying on perturbations and therefore applicable to almost black box agents), and combine them neatly using the harmonic mean to provide a new way to calculate saliency maps for agents with discrete action spaces.	abstract
2020-371	While the improvements on Atari are hard to quantify, the results on chess and go are more interpretable and hence more convincing.	strength
2020-371	The study using saliency maps to aid human chess players is rather neat, and again adds evidence towards the usefulness of this technique.	strength
2020-371	Finally, the chess saliency dataset is an exciting contribution that can actually be used to quantify saliency methods for DRL agents.	strength
2020-371	The proposed method is relatively simple, but is well-motivated and demonstratively (quantitatively, which is great work for general interpretability methods) better than prior methods, and in addition the authors introduce a new dataset + quantitative measure for saliency methods, so I would give this paper an accept.	decision
2020-371	Although the authors motivate their choice of perturbation-based saliency methods as opposed to gradient-based methods, they should expand their review of the latter.	weakness
2020-371	As the technique the authors introduced is very general, it would be useful to know how it compares to the current state of research in terms of identifying properties that attribution methods should meet	suggestion
2020-371	- a good example of this is integrated gradients (Sundararajan et al., 2017), which similarly identify "sensitivity" and "implementation invariance" as "axioms" that their method satisfies.	suggestion
2020-371	[score raised from weak reject to weak accept after rebuttal - on a more fine-grained scale, I would rate this paper now an accept (7), but not a strong accept (8), however since this year's scale is quite coarse, I'll stick with a score of 6]	decision
2020-371	Summary The paper proposes a new perturbation-based measure for computing input-saliency maps for deep RL agents.	abstract
2020-371	As reported in a large body of literature before, such saliency maps are supposed to help "explain" why a deep RL system picked a certain action.	abstract
2020-371	The measure proposed in the paper aims at combining two aspects: specificity and relevance, which should ensure that the saliency map highlights inputs that are relevant for a particular action to be explained (specificity), and this particular action only (relevance).	abstract
2020-371	The paper shows illustrative examples of the proposed approach and two previously proposed alternatives on Chess, Go and three Atari games.	abstract
2020-371	Additionally the paper evaluates the method and the two previous alternatives on two interesting chess-tasks: chess-puzzles where human players were shown to be able to solve puzzles faster and with higher accuracy when given the proposed saliency map in addition, and evaluation against a curated dataset of human-expert saliency maps for 100 chess puzzles.	abstract
2020-371	Contributions i) Novel, perturbation-based saliency measure composed of two parts.	strength
2020-371	Main idea of specificity is (similar to Iyer et al. 2018) to use State-Action value function (Q-function) with a specific action, instead of State-Value function only.	strength
2020-371	Main idea of relevance is to "normalize" by taking into account change in Q-value for all other actions as well.	strength
2020-371	Both parts are combined in a heuristic fashion.	strength
2020-371	ii) More objective/reproducible assessment of how saliency maps produced by different methods overlap with human judgement of saliency of pieces in chess.	strength
2020-371	To this end: two experiments with human chess players/experts (puzzle solving, and expert-designed saliency maps).	strength
2020-371	Quality, Clarity, Novelty, Impact The paper is well written and most sections are easily understandable, though for some parts it might help if the reader is quite familiar with Chess/Go.	strength
2020-371	The proposed saliency measure seems to address some shortcomings of previously proposed measures	strength
2020-371	- my main criticism is that the construction of the measure seems rather ad-hoc and heuristic.	weakness
2020-371	It would have been great to formally define specificity and relevance (e.g. in an information-theoretic framework as Sparse Relevant Information) and then derive a suitable measure that is shown to satisfy/approximate the formal definitions.	suggestion
2020-371	At least, there is one ablation study that justifies parts of the heuristic construction to some degree.	strength
2020-371	The proposed measure seems to do reasonably well on board-game domains, in particular chess.	strength
2020-371	However it might also be the case that the measure does particularly well for generating saliency maps for Stockfish (which is the agent that happens to be evaluated in the chess domain), which might be quite different from previously reported methods that have been designed for deep neural network RL agents.	weakness
2020-371	The illustrative examples on Atari and Go do not allow for a statistically significant judgement of the quality of the proposed method.	weakness
2020-371	On a conceptual level, a bigger issue (of many saliency methods in general, but the criticism also applies to the paper) is that the "explanations" drawn from saliency maps are rarely evaluated rigorously.	weakness
2020-371	The paper makes a nice attempt by trying to establish some "ground-truth" saliency in chess using humans to increase the degree of objectivity, which I greatly appreciate.	strength
2020-371	However, it remains unclear whether explanations that happen to coincide with human notions of saliency really are of higher quality for assessing how an artificial system makes its decisions.	weakness
2020-371	The main goal of explainability/interpretability methods must be to come up with testable hypotheses that tell us something about how the artificial system makes its decisions in novel/unseen situations.	weakness
2020-371	The goal is not to explain a decision mechanistically after the fact (which is trivial, given a deterministic, differentiable feed-forward system), but to come up with non-trivial explanations that extrapolate/generalize.	weakness
2020-371	Specificity and Relevance are probably important ingredients of such explanations, but I think it's important to formalize them properly first.	suggestion
2020-371	Currently I am in favor of suggesting a major revision of the work, but I am happy to reconsider my decision based on the rebuttal and other reviewers' assessment.	suggestion
2020-371	Having said that, I do want to reiterate that I think it is great that the authors included some ablation studies and measures of "usefulness" of the saliency method.	strength
2020-371	Improvements / Major comments i) Formally define specificity and relevance (e.g. as sparsity and compression?).	suggestion
2020-371	Ideally derive a saliency measure based on these formal definitions.	suggestion
2020-371	ii) Show how saliency maps (of the same situation) change when producing explanations for different actions.	suggestion
2020-371	I assume that currently the illustrative examples only show the action with the highest Q-value, what changes when using e.g. a less likely action?	suggestion
2020-371	iii) Show that the saliency map stays roughly constant for seemingly irrelevant perturbations.	suggestion
2020-371	In particular, using the chess-dataset with expert annotations, apply various kinds of perturbations to non-salient pieces (e.g. removing them, swapping them for other pieces and potentially moving them in irrelevant ways) and see whether the AUC stays roughly constant.	suggestion
2020-371	iv) Apply non-relevant perturbations to the salient pieces.	suggestion
2020-371	I.e. take the same puzzle/scene and move it across the board, does the saliency-map move in (roughly) the same way.	suggestion
2020-371	v) The saliency method might be particularly suited for Stockfish (whose action-value estimates might be strongly influenced by human saliency and chess theory).	suggestion
2020-371	See whether the method still produces good results for other chess agents (ideally trained without human heuristics or data).	suggestion
2020-371	If this is hard to do for chess, think about a different application where this is easier.	suggestion
2020-371	vi) Add a section that discusses current shortcomings and caveats with the method, and saliency maps for explainability in general.	suggestion
2020-371	vii) (Experimental details) For each domain, please explain the perturbations that you used (removing pieces in Chess/Go, blurring pixels in Go, anything else e.g. replacing pieces?).	suggestion
2020-371	In all experiments, was it always the action with the highest Q-value that was being explained?	suggestion
2020-371	Minor comments a) Table 1: (add error-bars) What is the variance across players?	suggestion
2020-371	Are the results for the proposed method statistically significant?	suggestion
2020-371	b) Chess saliency dataset. Are the expert saliency ratings binary?	suggestion
2020-371	Why not have multiple degrees of saliency?	suggestion
2020-371	c) Would the Greydanus et al. 2018 approach deliver similar results when using a threshold to cut off low-saliency inputs?	weakness
2020-371	d) Why is the saliency map in 3.4 binary (pieces are either salient or not)?	weakness
2020-371	How was the binarization threshold chosen?	weakness
2020-371	What would the non-binary saliency maps look like?	weakness
2020-371	e) Please provide all experimental details (in the appendix) that are necessary to reproduce the experiments.	suggestion
2020-371	Referring to a code-repository is not a replacement for describing the methods in detail.	rebuttal_process
2020-371	This paper proposes an algorithm for explaining the move of the agents trained by reinforcement learning (RL) by generating a saliency map.	abstract
2020-371	The authors proposed two desired properties for the saliency map, specificity and relevance.	abstract
2020-371	The authors then pointed out that prior studies failed to capture one of the two properties.	abstract
2020-371	To combine the two components into a single saliency map, the authors proposed using the harmonic mean.	abstract
2020-371	The experimental results demonstrated that the proposed saliency map successfully focused only on important parts while the other method tend to highlight some irrelevant parts also.	abstract
2020-371	The authors also did a great job for evaluating the goodness of the saliency maps, by preparing a human annotated chess puzzle dataset.	strength
2020-371	I think the paper is well-written, and the basic idea look reasonable and promising.	strength
2020-371	The experimental evaluations are well designed and the results look convincing.	strength
2020-371	Saliency map for RL is not yet mature, and I expect to see further improvements (especially, more theoretically principled ones) follow this study.	strength
2020-371	### Updated after author response ###	rebuttal_process
2020-371	The response from the authors seem to be reasonable to me.	rebuttal_process
2020-371	I therefore keep my score.	rebuttal_process

2020-570	Comments : This paper provides an approach for reducing bias in long horizon off-policy evaluation (OPE) problems, extending recent work from Liu et al., 2018 that estimates the ratio of the stationary state distributions in off-policy evaluation for reducing variance.	abstract
2020-570	The paper provides a doubly robust method for reducing bias, since it requires separately estimating a value function.	abstract
2020-570	The key idea of the paper is to provide low variance, low bias OPE since their approach relies on accurately estimating the state distribution ratio and also the estimation of the value function.	abstract
2020-570	The paper provides a follow up on Liu et al., 2018 and other recent works in off-policy learning that tries to estimate the state distribution ratio directly, but can introduce high inaccuracy if the obtained ratio estimates are inaccurate.	weakness
2020-570	In line of that, this approach seems useful as it tries to reduce the error from inaccurate ratio estimates by incorporating prior works with an additional value function estimation.	weakness
2020-570	Furthermore, the paper introduces a new perspective to doubly robust estimation that tries to reduce bias, instead of variance in previously known OPE literature (Jiang et al., 2016; Thomas et al., 2016).	abstract
2020-570	It is interesting to see how doubly robust can be related to primal-dual relations and the connections between these approaches, which is a novel contribution to the best of my knowledge.	strength
2020-570	- The fundamental connection comes from observing OPE withdoubly robust estimator that estimates \\hat{V} and equation 6 that incorporates the ratio of the state density.	strength
2020-570	This is clearly written in equations 7 and 8, that are two ways of evaluating the value function with off-policy samples known in the literature.	strength
2020-570	- It uses the property of the Bellman recursive expression for the estimated value function of doubly robust OPE estimator and uses it in the OPE with state density ratio, leading to equation 9, and obtaining the bridge estimator that now relies on both estimates of value function \\hat{V} and the state density ratio.	strength
2020-570	Although initially this does not seem useful, but the authors clarify this and how a bias reducting method can be achieved as in equation 11.	rebuttal_process
2020-570	- It is a nice and elegant trick, exploiting the connections between OPE estimators leading to a bias reduction method that seems quite interesting.	strength
2020-570	- The most interesting part of the paper comes from section 4 that exploits the connection between doubly robust approaches with Lagrangian duality, and that their approach is equivalent to a primal dual formulation of policy evaluation.	strength
2020-570	This stands out in itself as a novel contribution of the paper.	strength
2020-570	Equations 14 and 15 best writes down the connections, as to how policy evaluation can be formualted as a Lagrangian function for optimization.	strength
2020-570	- Although the authors point out how equation 15 is equivalent to equation 11 - this does not seem straight forward at first unless carefully looked at.	strength
2020-570	I would encourate the authors to perhaps add more clarity that exploits this connection, to make the paper more self-contained.	suggestion
2020-570	- In terms of experiments, the paper compares the doubly robust approach with previous works that estimates the density ratio, along with other baseline comparisons such as weighted DR.	strength
2020-570	In both the two evaluated problems, their approach seems useful in terms of obtaining better accuracy (lower MSE).	strength
2020-570	- However, I am not sure to what extent this approach can be scaled to more complicated tasks for OPE?	weakness
2020-570	Are there any example domains where the proposed method fails, or is difficult to scale up to more complicated tasks?	weakness
2020-570	- The current set of experiment results seems adequate, given the theoretical contribution and that most OPE papers evaluate on such domains.	strength
2020-570	But overall, it might be useful to analyse the significance of this approach when scaling to more complicated domains.	suggestion
2020-570	Overall, I think the paper has a neat and elegant theoretical contribution, exploiting the connection of OPE with primal-dual frameworks that seems quite novel to me.	strength
2020-570	Experimental set of results are properly presented too showing significance of the approach compared to previously known baselines.	strength
2020-570	I think such papers exploting connections with other literature is useful for the community in general, and this paper has significantly novel theoretical contribution.	strength
2020-570	Hence, I would recommend for acceptance of this paper.	decision
2020-570	*Synopsis*: This paper provides a new doubly robust estimator for off-policy policy evaluation, based on the new infinite horizon technique (i.e. using an estimate of the state density ratio as opposed to long products of action importance weights).	abstract
2020-570	They show the doubly robust estimator's bias is dependent on a product of the error of the value function estimate and stationary distribution ratio estimate, which provides improvements over the initial infinite horizon estimator.	abstract
2020-570	They also provide some nice discussion of the relation of their method and Lagrangian duality, which was quite interesting and insightful.	abstract
2020-570	Finally, the paper shows the usual empirical comparisons.	abstract
2020-570	Main Contributions: - Doubly robust estimator for infinite horizon off-policy policy evaluation	strength
2020-570	*Review*: Overall, I quite like this paper and think the quality is at a high level.	strength
2020-570	The proposed doubly robust estimator is well supported theoretically and empirically and improves on the prior art.	strength
2020-570	I am recommending a weak accept for this paper as it will be a nice contribution for the community, but I have some clarifications/updates I would like to see to improve the readability of the paper (specifically C4).	decision
2020-570	I also have a few questions, and clarifications that I would like the authors to address during the rebuttal period.	misc
2020-570	*Clarifications*: C1: In the section "Off-policy state visitation importance sampling", doesn't equation 4 involve an expectation over the target policy?	weakness
2020-570	Or am I missing something here?	weakness
2020-570	C2: The bias of the estimator decreases as our estimate of the density ratio and value function improves.	weakness
2020-570	It might be useful to more clearly compare this bias to the original estimator proposed by Liu. C3: Proof of theorem 3.1: I would like you to clearly finish the proof.	suggestion
2020-570	I think clarity and completeness in appendices is importance over conciseness as there is usually no limit on pages.	suggestion
2020-570	C4: The proof of theorem 3.2 is not obviously linked to theorem A.3.	weakness
2020-570	The proof for theorem 3.2 should be clearly stated and included in the appendix, without an unstated implicit link to A.3.	weakness
2020-570	This will make your theoretical analysis more clearly understandable, and expendable for future work.	weakness
2020-570	I will happily increase my score if this is clarified in a future version.	suggestion
2020-570	I also will decrease my score if this is not clarified.	misc
2020-570	*Competitors* - You may want to include a model based approach, just for completeness (as in the infinite horizon paper from Liu).	weakness
2020-570	*Questions* Q1: I'm curious about the difference between using an ANN for estimating the density ratio (as opposed to a kernel method).	weakness
2020-570	Have you run experiments with the kernel method proposed in Liu's paper?	suggestion
2020-570	While I don't think it is necessarily needed for the paper to be complete, I think the difference would be interesting to see what factors contribute to the algorithm's better performance.	suggestion
2020-570	Minor typos not taken into account for the review: - Section 3.1 "it is useful to exam the": exam -> examine	weakness
2020-570	- Section 4: "hence yielding an true expected reward": an -> a	misc
2020-570	- Section 4: "it is natural to intuitize": intuitize -> intuit?	misc
2020-570	- The sentence right before section 4 could use a rewrite.	weakness
2020-570	- For readability it would be useful to include theorem statements ahead of proofs in the appendix.	suggestion
2020-570	- Theorem A.3: do you mean with \\hat{R} defined in 3.2 (not the variance as this is not defined in 3.2).	weakness
2020-570	Edit: - Due to the author rebuttal and updates to the paper I've increased my score.	rebuttal_process
2020-570	This paper proposes a new algorithm for the off-policy evaluation problem in reinforcement learning.	abstract
2020-570	It combines the value function learning method and the stationary distribution ratio estimators.	abstract
2020-570	The proposed method achieves double robustness, which means the proposed estimator is consistent as long as the value function or ratio estimator is consistent.	abstract
2020-570	Empirical results on some control domains are presented to verify the effectiveness of the algorithm.	abstract
2020-570	I think this paper has some nice contribution to the area, by introducing a doubly robust estimator based on the density ratio, and also a new idea to achieve double robustness.	strength
2020-570	I will vote for accept, but I think there is room for improvement of this paper.	decision
2020-570	Detailed comments: - The proposed estimator is not using control variate but using dual structure between value function and stationary distribution ratio, which is a novel idea comparing with similar doubly robust estimators.	strength
2020-570	- Theorem 3.2, or at least the way it is presented, is less intuitive and makes me confused.	weakness
2020-570	If the variance of the DR estimator is always larger than the variance of value function, why I should use this estimator instead of value function.	weakness
2020-570	If the argument is the MSE of value function is potentially larger due to the bias.	weakness
2020-570	Then an effective analysis would be about MSE instead of variance.	weakness
2020-570	- If I have an oracle of density ratio, is the doubly robust estimator still unbiased, which is generally true for DR using control variate?	weakness
2020-570	This would be an important point to compare this work with control variate methods.	weakness
2020-570	- Very recent work https://arxiv.org/abs/1908.08526 also proposes a doubly robust estimator in similar settings.	weakness
2020-570	It's worth to mention it in the related work.	weakness

2020-581	This paper's contribution is a sample complexity lower bound for linear value-based learning and policy-based learning methods.	abstract
2020-581	The bound being exponential in the planning horizon is bad news, and has some implications with respect to further analysing sample complexity in RL.	weakness
2020-581	The gist of this paper is that one can craft a hard MDP which requires visiting every state at least once, and that since this MDP's state space is exponential in the MDP's horizon, then there exists a set of MDPs which require an exponential (in the horizon) number of trajectories to be solved.	abstract
2020-581	As a consequence, further analysis of sample complexity in RL may need some much stronger assumptions.	abstract
2020-581	The writing of the paper is good, I was able to understand everything (I think).	strength
2020-581	As far as I can tell, this is novel work.	strength
2020-581	Unfortunately I am currently unable to see why this contribution is valuable.	weakness
2020-581	I have set my score to weak reject but I am very open to having my mind changed, as I feel I may have missed some critical element.	rebuttal_process
2020-581	I have two criticisms: A- I don't understand why this bound is significantly different than previous bounds.	weakness
2020-581	B- I don't understand why this is bad news for representation learning, nor how this failure mode of linear features translates to the "deep" case.	weakness
2020-581	In the same spirit, I find rather odd the way the paper is introduced.	weakness
2020-581	Discussions of representations usually involve some discussion of generalization, but that's not what this paper is about.	weakness
2020-581	Deep neural networks/representation learning are only useful if there is an opportunity for generalization.	weakness
2020-581	With respect to A, I am either grossly misunderstanding past bounds and/or your bounds, or something is wrong with the way complexities are compared: - In Wen & Van Roy, the "polynomial" sample complexity is in the number of states, it is related to |S|x|A|xH^2 (Theorem 3 of Wen & Van Roy)	weakness
2020-581	- In this paper, Theorem 4.1 states that the sample complexity is exponential because it is of the form 2^H.	weakness
2020-581	One *critical* assumption for this bound is precisely that |S| >= 2^H.	weakness
2020-581	Thus the bound that you propose is still polynomial in |S|.	weakness
2020-581	I am thus puzzled, how is this bound significantly different?	weakness
2020-581	With respect to B, I don't see how this bound has much to do with good representations, or even representations at all.	weakness
2020-581	In Lemma A.1, you essentially craft a set of features that, being mutually orthogonal, are in some sense "mutually linearly separable", making learning the mapping from those features to a value function "trivial" once data is obtained.	weakness
2020-581	This is barely different from saying that you assume there is a magical learner that learns in O(1) given the data, because in either case, you need to visit _every_ of the 2^H state in order to solve the MDP, because by construction of your problem, there is _no hope_ of generalization*.	weakness
2020-581	Since learning features or creating "good" features has everything to do with generalization (otherwise we'd just to tabular), I don't see how this bound is relevant to representations.	weakness
2020-581	(We already have Wolpert's no free lunch theorem to tell us that there are always some problems that ML just can't be general enough to solve efficiently.	weakness
2020-581	What is more interesting is understanding how we can efficiently learn where there _is_ structure to a problem.)	weakness
2020-581	* There is no hope of generalization, unless something about the observation space (which is left undiscussed in the paper) contains *information* about the agent being to the unique path to the reward.	weakness
2020-581	In such a case, I can see a probabilistic argument being made where in the worst case the agent needs to visit all 2^H states, but in the average case, the agent may learn to ignore paths where it can generalize that there is no reward.	weakness
2020-581	This is not entirely unreasonable, think of e.g. AlphaGo, where very few states end in victory, where there is an exponential number of states in the horizon, yet learning is totally reasonable because of structure in observation.	weakness
2020-581	This is where I don't agree with a statement like: "Since the class of linear functions is a strict subset of many more complicated function classes, including neural networks in particular, our negative results imply lower bounds for these more complex function classes as well."	ac_disagreement
2020-581	The author question an important aspect which is very often taken for granted in the RL community, that a good representation could lead to data-efficient RL.	ac_disagreement
2020-581	They show negative results, providing pessimistic lower bounds for both value-based and policy-based learning.	abstract
2020-581	I believe the paper is an important contribution, in particular, it has the following advantages: - Well written, clear, and nicely structured.	strength
2020-581	It is self contained, with main results and convincing sketch proofs provided in the main body of the document, and extended technical proofs made available in the supplementary material.	strength
2020-581	- Authors provide the relevant related work and elegantly show how their work connects to the existing literature.	strength
2020-581	- The notation is consistent throughout the document, with necessary assumptions clearly stated.	strength
2020-581	- The discussion highlights important findings, in particular the difference between value-based and policy-based learning.	strength
2020-581	Additionally, it offers some hope for sample-efficient RL, by discussing the exponential separation between policy-based RL and imitation learning, reminding the community that sample-efficient RL can still be achieved by IL even if it can't be achieved through good-but-not-perfect representation.	strength
2020-581	Minor comment: - Phrasing of Assumption 4.3 seems to be off.	weakness
2020-581	This paper presents theoretical lower bounds on sample complexities to learn good policies in reinforcement learning.	abstract
2020-581	The derived theorems show that there exists MDPs which require an exponential number of samples to learn a near-optimal policy even if a good-but-not-perfect representation is given to the agent for both value-based and policy-based learning.	abstract
2020-581	These results constitute the first lower bounds for RL with linear function approximation.	abstract
2020-581	Representation learning is an important area of research and this paper advances our theoretical understanding in a notable way, helping to elucidate the limits of representation learning alone.	strength
2020-581	The lower bounds derived in the paper would be of particular interest to the community as they can apply to a wide range of function approximators, including neural networks.	strength
2020-581	Although this is not my area, the contributions are well-explained in the context of previous work and the theory was fairly easy to follow.	strength
2020-581	The discussion also contained interesting points and summarized possible implications of the theoretical results.	strength
2020-581	Overall, I think this paper presents a solid contribution and recommend acceptance.	decision
2020-581	Although the paper was clear in general, I would like to have certain points clarified: 1) Just to check, is it still possible that there exists certain representations that are not perfect but do lead to sample efficient learning?	suggestion
2020-581	If I'm interpreting the results correctly, the theorems only posit the existence of a representation that is good in the sense of approximation error, but bad in terms of sample complexity, which does not necessarily preclude the possibility of other efficient representations.	weakness
2020-581	2) More generally, why is it that there are few results for lower bounds when it seems like an obvious direction?	weakness
2020-581	Are there technical barriers to proving such results?	weakness
2020-581	3) For value-based learning, the good representation has approximation error \\Omega(\\sqrt(H / d)).	weakness
2020-581	Could the authors explain why this assumption on the error is reasonable?	weakness
2020-581	3) In assumption 4.4, the features are assumed to have an l2-norm of 1.	weakness
2020-581	This seems like a fairly restrictive assumption.	weakness
2020-581	How important is this assumption and can it be relaxed?	weakness
2020-581	4) Also, in theorem 4.2, it is assumed that the dimension of the features, d = H.	weakness
2020-581	This would seem to allow the possibility of policy-based learning being sample efficient when the number of features is much smaller.	weakness
2020-581	The paper is well-polished with few noticeable typos.	strength
2020-581	Minor comments: - p.5 sec3.3: "knows the whole transition" -> "knows the whole transition function"	weakness
2020-581	- p.8 sec5.1: "our lower bound on policy-based learning thus demonstrates" It may be worth reminding the reader that the bound applies to perfect representations -> "our lower bound on policy-based learning---which applies to perfect representations---thus demonstrates	weakness

2020-592	- This paper modifies and extends the recent "free" training strategies in adversarial training for representation learning for natural language.	abstract
2020-592	The proposed "Free" Large-Batch Adversarial Training is well motived, in comparison with plain PGD-based adversarial training and the existing methods like FreeAT and YOPO, which virtually enlarges the batch size and minimize maximum risk at every ascent step.	strength
2020-592	The contributions are solid. - The proposed methods are empirically shown to be effective, in addition to being aligned with some recent theoretic analysis.	strength
2020-592	The models achieve SOTA on GLUE (by time the paper was submitted; it is not the best model now but that does not affect the contributions), ARC, and the commonsenseQA dataset.	strength
2020-592	- The paper conducted good analysis demonstrating the effectiveness of the proposed components, including detailed ablation analysis.	strength
2020-592	- The paper is well written.	strength
2020-592	It is well structured and easy to follow.	strength
2020-592	A minor suggestion (just a personal view) is that the author(s) may consider using "natural  language" instead of just "language" in the title and may consider using more specific words like "representation" instead of "understanding".	suggestion
2020-592	But this is minor. I recommend an accept.	decision
2020-592	In this paper, the authors present a new adversarial training algorithm and apply it to the fintuning stage large scale language models BERT and RoBERTa. They find that with FreeLB applied to finetuning, both BERT and RoBERTa see small boosts in performance on GLUE, ARC, and CommonsenseQA.	abstract
2020-592	The gains they see on GLUE are quite small (0.3 on the GLUE test score for RoBERTa) but the gains are more substantial on ARC and CommonsenseQA.	abstract
2020-592	The paper also presents some ablation studies on the use of the same dropout mask across each ascent step of FreeLB, empirically seeing gains by using the same mask.	abstract
2020-592	They also present some analysis on robustness in the embedding space, showing that FreeLB leads to greater robustness than other adversarial training methods	abstract
2020-592	This paper is clearly presented and the algorithm shows gains over other methods.	strength
2020-592	I would recommend that the authors try testing their method on SuperGLUE because it's possible they're hitting ceiling issues with GLUE, suppressing any gains the algorithm may yield.	suggestion
2020-592	Questions, -  In tables 4 and 5, why are only results on RTE, CoLA, and MRPC presented?	weakness
2020-592	If this is because there was not noticeable difference on the other GLUE datasets, please mention it in the text.	weakness
2020-592	- I realize that this method is meant to increase robustness in the embedding space, but did you do any error analysis on the models?	weakness
2020-592	Did they make different types of errors than models fine-tuned the vanilla way?	weakness
2020-592	Couple typos, - Section 2.2, line 1: many -> much	weakness
2020-592	- Section 4.2, GLUE paragraph: 88 -> 88.8	weakness

2020-611	This paper analyzes the effect of regularization on spectral embeddings in a deterministic block model and explicitly characterizes the spectra of the Laplacian of the regularized graph in terms of the regularization parameter and block sizes.	abstract
2020-611	To my knowledge, this has not been done before.	strength
2020-611	Prior work either derives sufficient conditions for the recovery of all blocks in the asymptotic limit of an infinite number of nodes in the case of (Joseph & Yu, 2016), or lower bounds the number of small eigenvalues of the Laplacian of the unregularized graph on random graphs in expectation (therefore arguing in favor of regularization) in the case of (Zhang & Rohe, 2018).	strength
2020-611	This paper, on the other hand, gives a precise characterization of the eigenvalues and eigenvectors (albeit in the case of a deterministic graph); the results are elegant and the analysis uses simple elementary techniques, which is very satisfying and seems to be easy to build on.	strength
2020-611	The authors mention that they would like to extend this analysis to stochastic block models, which would indeed be interesting.	rebuttal_process
2020-611	The paper is also well written and the results are clearly presented.	strength
2020-611	Overall, this is a nice contribution to spectral graph theory and so I recommend acceptance.	decision
2020-611	The paper explains through a block model the impact of the complete graph regularization, intended as adding to all the entries of the adjacency matrix a constant.	abstract
2020-611	The paper is a nice balance between theory and practical effect, since it shows that at the end spectral embedding has an impact on larger connected block units of the graph, discarding isolated nodes.	strength
2020-611	It also introduces the problem in a gentle way, so that the range of possible readers is wide.	strength
2020-611	In general I'm happy with the paper, no major lacks on my side	misc
2020-611	Suggestions: it is not clear how to get to Eq.7), the authors should explain the last passage before that equation a little more?	suggestion
2020-611	How the values of the noise alpha have been selected?	weakness
2020-611	How the approach scales with the number of blocks, in term of complexity?	suggestion

2020-633	Summary of the paper The authors propose a predictive model based on the energy based model that uses a Transformer architecture for the energy function.	abstract
2020-633	It accepts as input an atom and its neighboring atoms and computes an energy for their configuration.	abstract
2020-633	The input features include representations of physical properties (atom identity, atom location within a side chain and amino-acid type) and spatial coordinates (x, y, z).	abstract
2020-633	A set of 64 atoms closest to the beta carbon of the target residue are selected and each is projected to a 256-dimensional vector.	abstract
2020-633	The predictive model computes an energy for the configuration of these 64 atoms surrounding a residue under investigation.	abstract
2020-633	The model is reported to achieve a slightly worse but comparable performance to the Rosetta energy function, the state-of-the-art method widely used in protein structure prediction and design.	abstract
2020-633	The authors investigate model's outputs and hidden representations and conclude that it captures physicochemical properties relevant to the protein energy in general.	abstract
2020-633	Strengths + A very interesting contribution to structural biology with a non-trivial application of deep learning.	strength
2020-633	+ The study is accompanied with structural biology interpretation of the designed energy predictor.	strength
2020-633	Usually similar studies do not attempt and are limited with prediction rates and mathematical analysis.	strength
2020-633	+ Appropriate background is provided for non-experts in protein structural biology.	strength
2020-633	The paper is clear and well written.	strength
2020-633	+ An adequate literature review is provided relative to the problem.	strength
2020-633	Bird-eye view is given for future direction with justified optimism.	strength
2020-633	Weaknesses: - The study claims to provide an energy predictor in general while training is performed on rotamer recovery of a single amino acid.	weakness
2020-633	These claims should be downplayed and what was shown at most is that this predictor can be applied to predict energy for a restricted problem of the rotamer recovery task.	weakness
2020-633	- A reader gets a wrong impression at the beginning of the manuscript that the study tries to solve general and classical rotamer prediction for an entire protein.	weakness
2020-633	It becomes clear only at the very end that the study does not try to resolve all rotamer conformations in a protein, it can only predict one rotamer at the time given the atoms surrounding the target residue are correct.	weakness
2020-633	This should be explained in the beginning that the study does not attempt combinatorial side chain optimization for a fixed backbone;	suggestion
2020-633	- In-depth description of the neural network is required at least in supplemental materials, so that the study could be replicated.	weakness
2020-633	Ideally, a working application and training protocol code would be available.	weakness
2020-633	- No techniques against overtraining are discussed.	weakness
2020-633	How was the model validated?	weakness
2020-633	Small issues: * function f_theta(A) is used before it is introduced.	weakness
2020-633	* noun is missing: using our trained with deep learning.	weakness
2020-633	* articles missing: that vary from amino acid to amino acid.	weakness
2020-633	* KL abbreviation not explained	weakness
2020-633	* MCMC abbreviation not explained	weakness
2020-633	* wrong artile:  that processes the set of atom representations.	weakness
2020-633	* misprint: resolution fine r than 1.8	weakness
2020-633	* wrong word, has to be "lower": sequence identity greater ˚	weakness
2020-633	* everyday word: break out	misc
2020-633	* abbreviation not explained: t-SNE	weakness
2020-633	* misprint case: Similarly, In contrast to our work The paper proposes an Energy-Based-Model (EBM) for scoring the possible configurations of amino acid side chain conformations in protein structures with known amino acid backbone structure.	weakness
2020-633	The energy of the side-chain conformation (the chi-angle) for a given amino acid in the structure is calculated as a function of a local neighbourhood of atoms (A), where each atom is embedded into a 256d vector using its cartesian coordinates, atom identity, atom side-chain position and amino acid identity.	weakness
2020-633	The model is trained using approximate likelihood where the model samples are generated using precalculated table (from literature) of possible Chi angles conformations conditioned on the back-bone amino acid identity and back-bone angles.	weakness
2020-633	The results seem comprehensive comparing the transformer based energy function parameterization with two sensible baselines as well as the Rosetta energy function  which is the de facto standard tool for these types of calculations.	weakness
2020-633	Using rotamer recovery accuracy as the benchmark measure the empirical results are close to performance as the Rosetta energy model however always slightly worse.	weakness
2020-633	Further visualizations of the energy levels for different Chi angles seems to support that the learned energy function captures well known characteristics of the rotamer configuration energy landscape.	weakness
2020-633	Score: Overall I think that the paper is solid and tackles an interesting problem of learning energy functions for physical systems from data.	strength
2020-633	The experimental results are comprehensive and mostly supports the claims made in the paper.	strength
2020-633	My main (slightly minor) concern about the paper is the obsession with discarding years of learned knowledge about handcrafted energy functions with fully learned functions.	weakness
2020-633	It seems to me that combining domain knowledge with a learned model should close the last small gap (and presumably surpass) the performance of e.g Rosetta.	weakness
2020-633	Combining I think the paper should be accepted at the *CONF*.	decision
2020-633	Comment/Questions: Motivation Q1.1) Paper motivation: The paper several times seems to suggest that energy functions derived from physical knowledge are problematic (e.g. abstract).	weakness
2020-633	For many physical systems I don't think this is true and would like the author's comments on why learned energy functions are preferable - arguably they can only capture properties present in the data and not any prior knowledge?	weakness
2020-633	Q1.2) Given that the paper is motivated by fully learning an energy function from data without injecting any prior knowledge i would like the authors comment on the construction of the q(x|c) distribution.	weakness
2020-633	This is based on essentially a contingency table between Phi/Psi/Chi angles and to me seems like injecting prior knowledge about physically possible rotamer configurations into the learned procedure?	weakness
2020-633	Method / Experimental Results: Q2.1) With respect to the primary results in table 1) and table 2).	weakness
2020-633	The authors claim comparable results to the Rosetta Energy function (page 5.	weakness
2020-633	Sec 4.3, page 9, sec 6).	weakness
2020-633	However the experimental results are always slightly worse than the Rosetta energy function and I (strongly) suggest that the authors rephrase those statements to reflect that.	weakness
2020-633	Q2.2 With Respect to Table 3) Firstly, Why are the only results for 16 amino acids in the table ?	weakness
2020-633	Secondly, just naively counting the Atom Transformer are better than Rosetta in 11 of 16 amino acids - this seems slightly at odds with the main result where the Atom Transformer is performing slightly worse than Rosetta?	weakness
2020-633	Clarity Q3): The notation in section 3 and 4 is slightly confusing.	weakness
2020-633	Especially I think q(x|c) is slightly misleading since it, (to my understanding) is a conditional distribution over Chi, conditioned on Psi/Phi/AminoAcid and not rotamer (x) and surrounding molecular context (c).	weakness
2020-633	The paper proposes an energy based model (learned using the transformer architecture) to learn protein conformation from data describing protein crystal structure (rather than one based on knowledge of physical and biochemical principles).	abstract
2020-633	To test the efficacy of their method, the authors perform the task of identifying rotamers (native side-chain configurations) from crystal structures, and compare their method to the Rosetta energy function which is the current state-of-the-art.	abstract
2020-633	+ Presents an important and emerging application of neural networks	strength
2020-633	+ Relatively clearly written, e.g. giving good background on protein conformation and related things to a machine learning audience	strength
2020-633	-The baselines that are compared to are set2set and Rosetta.	strength
2020-633	Are there newer baselines the authors can compare to?	strength
2020-633	For example, the related works section discusses some recent works.	strength
2020-633	-I am having a hard time interpreting the numbers in the experimental results (Table 1 and Table 2).	weakness
2020-633	I understand that they are rotamer recovery rates and that the max would be 100%.	suggestion
2020-633	But I can't tell what amount of difference is significant.	weakness
2020-633	-Figure 1 should have a caption clarifying notation etc.	weakness

2020-642	In this paper the authors proposed a framework for off-policy value estimation under the scenario of infinite horizon RL tasks.	abstract
2020-642	The new proposed method utilize the variational representation of f-divergence, which quantifies the difference between Tτ and τp, where τ is the parametric density ratio between the unknown behavior policy data and the target policy.	abstract
2020-642	If only if τ is the true density ratio, the loss Df(Tτ||τp)=0.	abstract
2020-642	Compared with prior work (Nachum et.	abstract
2020-642	al 2019), the new proposed framework can generalize the undiscounted case γ=1, and the derivation for the new algorithm is quite simple and easy to follow.	strength
2020-642	The experimental results show the advantage of the proposed methods over baseline methods such as model-based, DualDice etc, for both discrete and continuous cases.	abstract
2020-642	Moreover, I have two specific questions: - The choice of f-divergence.	weakness
2020-642	Although the author mentioned the difficulty of using the dual representation of KL divergence, it would be nice to have an ablation study that shows the effectiveness of various f-divergence (Personally I think Jensen-Shannon Divergence may be also a good choice).	suggestion
2020-642	-The authors should also have a discussion that similar idea can be generalized to more general  distribution metrics such as Integral Probability Metrics, specifically wasserstein-1 distance (similar to wasserstein-gan) or maximum mean discrepancy (Maybe it is unnecessary to conduct experiments, some discussion should be enough to clarify the relationship.	suggestion
2020-642	I think there is a concurrent submission using MMD metrics).	suggestion
2020-642	Overall I think this is a good paper and I recommend for acceptance.	decision
2020-642	Reference Papers: - Nowozin, Sebastian, Botond Cseke, and Ryota Tomioka.	misc
2020-642	"f-gan: Training generative neural samplers using variational divergence minimization." Advances in neural information processing systems.	misc
2020-642	2016. - Arjovsky, Martin, Soumith Chintala, and Léon Bottou.	misc
2020-642	"Wasserstein gan." arXiv preprint arXiv:1701.07875 (2017).	misc
2020-642	- Nachum, Ofir, et al. "DualDICE: Behavior-Agnostic Estimation of Discounted Stationary Distribution Corrections." arXiv preprint arXiv:1906.04733 (2019).	misc
2020-642	- Anonymous, "Black-box Off-policy Estimation for Infinite-Horizon Reinforcement Learning", submitted to *CONF* 2020.	misc
2020-642	This paper proposes a new estimator to infer the stationary distribution of a Markov chain, with data from another Markov chain.	abstract
2020-642	The method estimates the ratio between stationary distribution of target MC and the empirical data distribution.	abstract
2020-642	It is based on the observation that the ratio is a fixed point solution to certain operators.	abstract
2020-642	The proposed method could work in the behavior-agnostic and undiscounted case, which is unsolved by the previous method.	abstract
2020-642	This paper tackles an interesting problem with an increasing number of studies in the reinforcement learning community and gives a practical algorithm with strong empirical justification, as well as theoretical justification.	abstract
2020-642	I think this paper should be accepted.	decision
2020-642	Detailed comments: 1) This paper provides experiment results in multiple domains, including two continuous control domains which are more complex than experiments in previous OPE methods.	strength
2020-642	The paper also provides many details about the learning dynamics and ablation studies, which is very useful for the reader to understand the result of the paper.	strength
2020-642	2) The theoretical result is as same strong as previous work DICE and DualDICE, under similar assumptions.	strength
2020-642	3) I appreciate this paper formalizes the two difficulties of degeneration and intractability, and then explain how those are addressed in a principled way.	strength
2020-642	Degeneration is important and is at least ignored in two similar works on this topic.	strength
2020-642	Main contributions: This paper generalizes the recent state-of-the-art behavior agnostic off-policy evaluation DualDice into a more general optimization framework: GenDice.	abstract
2020-642	Similar to DualDice, GenDice considers distribution correction over state, action pairs rather than state in Liu et al. (2018), which can handle behavior-agnostic settings.	abstract
2020-642	The optimization framework (in equation (9)) is novel and neat, and the practical algorithm seems more powerful than the previous DualDice.	strength
2020-642	As a side product, it can also use to solve offline page rank problem.	strength
2020-642	Clarity: This paper is well established and written.	strength
2020-642	Connection of theory and experiment: I have a major concern for the theory 1 about the choice of regularizer λ.	weakness
2020-642	For infinite samples case, the derivation of theory 1 is reasonable since both term is nonnegative.	weakness
2020-642	However, in practice we will have empirical gap for the divergence term, thus picking a suitable λ seems crucial for the experiment.	weakness
2020-642	I think a discussion on λ  for average case in experiment part should be added.	suggestion
2020-642	And compared to Liu et al. (2018) which normalized the weight of τ in average case, which one is better in practice?	weakness
2020-642	Overall I think this paper is good enough to be accepted by *CONF*.	decision
2020-642	The optimization framework can also inspire future algorithm using different divergence.	strength

2020-653	The paper develops a cyclical stepsize schedule for choosing stepsize for Langevin dynamics.	abstract
2020-653	The authors prove the non-asymptotic convergence theory of the proposed algorithm.	abstract
2020-653	Many experimental results, including ImageNet, are given to demonstrate the effectiveness of the proposed method.	abstract
2020-653	Here I suggest that authors also need to point out that the continuous-time MCMC is the Wasserstein gradient flow of KL divergence.	suggestion
2020-653	The bound derived in this paper focus on the step size choice of gradient flows.	abstract
2020-653	This could be a good direction for combining gradient flows studies in optimal transport and MCMC convergence bound for the choice of step size.	abstract
2020-653	Overall, I think that the paper is well written with clear derivations.	strength
2020-653	I strongly suggest the publication of this paper.	decision
2020-653	This article presents cyclical stochastic gradient MCMC for Bayesian deep learning for inference in posterior distributions of network weights of Bayesian NNs. The posteriors of Bayesian NN weights are highly multi-modal and present difficulty for standard stochastic gradient MCMC methods.	abstract
2020-653	The proposed cyclical version periodically warm start the SG-MCMC process such that it can explore the multimodal space more efficiently.	abstract
2020-653	The proposed method as well as the empirical results intuitively make sense.	strength
2020-653	The standard SG-MCMC basically has one longer stepsize schedule and is exploring the weight space more patiently, but only converges to one local mode.	strength
2020-653	The cyclical SG-MCMC uses multiple shorter stepsize schedules, so each one is similar to a (stochastic) greedy search.	strength
2020-653	Consequently, the cSG-MCMC can collect more diverse samples across the weight space, while the samples of SG-MCMC are more concentrated, but likely with better quality (as shown in Figure 3).	strength
2020-653	Personally I would like to see how Bayesian deep learning can be applied to real large-scale applications.	suggestion
2020-653	Probabilistic inference is expensive; Bayesian model averaging is even more expensive.	weakness
2020-653	That's probably why recent literature focuses on variational inference or expectation propagation-based approaches.	weakness
2020-653	The paper propose a new MCMC scheme which is demonstrated to perform well for estimating Bayesian neural networks.	abstract
2020-653	The key idea is to not keep lowering the step sizes, but -- at pre-specified times -- go back to large step sizes.	abstract
2020-653	The paper is timely, the proposed algorithm is novel, and the theoretical analysis also seem quite novel.	strength
2020-653	My key concern is that with MCMC sampling it is often quite difficult to tune parameters, and by introducing more parameters to tune when step sizes should increase, I fear that we end up in a "tuning nightmare".	weakness
2020-653	How sensitive is the algorithm to choice of parameters?	weakness
2020-653	I would expect that the proposed algorithm is quite similar to just running several MCMCs in parallel.	weakness
2020-653	The authors does a comparison to this and show that their approach is significantly faster due to "warm restarts".	weakness
2020-653	Here I wonder how sensitive this conclusion is to choice of parameters (see nightmare above) ?	weakness
2020-653	I would guess that opposite conclusions could be reached by tuning the algorithms differently -- is that a reasonable suspicion ?	weakness
2020-653	It is argued that the cyclic nature of the algorithms gives a form of "warm start" that is beneficial for MCMC.	weakness
2020-653	My intuition dictate that this is only true of the modes of the posterior are reasonable close to each other; otherwise I do not see how this warm starting is helpful.	weakness
2020-653	I would appreciate learning more about why this intuition is apparently incorrect.	weakness
2020-653	Minor comments: * on page 4 it is stated that the proposed algorithm "automatically" provide the warm restarts -- but is it really automatic?	weakness
2020-653	Isn't this a priori determined by choice of parameters for the algorithm?	weakness
2020-653	* It would be good to use \\citet instead of \\cite at places, e.g. "discussed in (Smith & Topin, 2017)" should be "discussed by Smith & Topin (2017)".	suggestion
2020-653	This would improve readability (which is generally very good).	suggestion
2020-653	* For the empirical studies I think it would be good to report state-of-the-art results as well.	suggestion
2020-653	I expect that the Bayesian nets still are subpar to non-Bayesian methods, and I think the paper should report this.	suggestion

2020-654	This work is an attempt to bridge the gap between discriminative models, which currently obtain the state of the art on most classification problems, and generative models, which (through a model of the marginal p(x)) have the potential to shine on many tasks beyond generalization to a hold-out set with minimal shift in distributions: out of distribution detection, better generalization out of distribution, unsupervised learning etc.	abstract
2020-654	While much of the current work is related to normalizing flows / invertible neural networks, the authors here propose a quite simple but appealing method: A standard neural classifier is taken and the softmax is layer chopped off and replaced by an energy based model, which models the joint probability p(x,y) instead of the posterior p(y|x).	abstract
2020-654	The advantage is an additional degree of freedom in the scale of the logit vector, which is would have been otherwise normalized by the softmax layer and now can now model the data distribution.	abstract
2020-654	The downside is the loss in ease of training.	weakness
2020-654	Whereas (discriminative) deep networks can be easily trained by gradient descent on a cross-entropy objective, the partition function in the energy model makes this un tractable.	weakness
2020-654	This is addressed through sampling, similar to (Welling & Teh, 2011).	weakness
2020-654	One of the biggest achievements reported by the authors is that the performance on discriminative tasks is not hurt (much) by adding the generative model.	weakness
2020-654	There is only a 3 point gap between Wide-ResNet and the proposed model (92.9% vs.	weakness
2020-654	95.8%) … but on what dataset?	weakness
2020-654	3 datasets are mentioned in the experimental section, but table 1 does not mention on which datasets the accuracy is reported.	weakness
2020-654	My guess is that this is a mean or mixture, since GEM performances of 96.7% and 72.2% are reported for SVHN and CIFAR10, respectively, but this should be made clearer.	weakness
2020-654	On out of distribution detection, could the authors comment on the histograms in table 2, in particular the difference between the new measure (AM JEM) compared to JEM log p(x) on CelebA?	weakness
2020-654	The proposed measure does not seem to fare well here.	weakness
2020-654	Although the method does not outperform the gold standard of adversarial training, I found the models robustness to adversarial examples quite appealing, given that it was not trained for this objective (which also means that it does not require an adaptation to a norm).	strength
2020-654	I was very impressed by Figure 6 showing distal adversarial initialized from random images, showing pretty clear images of the modelled class.	strength
2020-654	The modelled variations require more investigation to verify whether we have a collapse for each class, but the results look very promising.	strength
2020-654	The paper is well written and easy to understand.	strength
2020-654	A couple of details on the training procedure are missing in the experimental part.	weakness
2020-654	It is stated that, both, p(y|x) and the generative part p(x), are optimized, but how are these exactly integrated?	weakness
2020-654	Given the difficult in training this model reported in the paper, this seems to be particularly important.	strength
2020-654	I also appreciated the description of the limitations of the algorithm, and the details in the appendix (*CONF* should go back to unlimited paper lengths, btw.).	strength
2020-654	More information on complexity (training times etc.) should also be helpful.	strength
2020-654	The paper uses energy-based model interpretation for the logits of standard discriminative neural network models to define a generative model inside a classifier that proves useful in many downstream tasks such as uncertainty quantification, out-of-distribution detection, etc.	abstract
2020-654	Although there has been previous work attempting to bridge discriminative classifiers with generative modeling, this work proves to be competitive with both specialized models on discriminative/generative tasks as well as in many downstream tasks such as out-of-distribution detection, calibration, and adversarial robustness.	abstract
2020-654	The paper provides a clear exposition of the method, succeeds to discuss related work it bases on, conducts a thorough experimental study providing convincing explanations for results and does not hide the limitations of the work (high computational requirements, optimization difficulties connected with training energy-based model and the method used, limited approximation of the true energy).	strength
2020-654	Overall, the paper provides a substantial contribution and paves the way for further work improving this joint discriminative - generative setting.	strength
2020-654	However, there are points I would like the paper to address for better exposition.	weakness
2020-654	1. It would benefit the paper showing that samples with higher unnormalized likelihood are visually more compelling than those with lower likelihood.	suggestion
2020-654	2. On CIFAR100 the accuracy drop from the reference value is larger than for datasets with 10 classes, could it be due the logits dimension is higher and challenges optimization?	weakness
2020-654	3. It would also be helpful to clarify whether application of the proposed method is primarily restricted by the computational complexity or is there any property inherent to energy-based models that makes treating high-dimensional data challenging?	suggestion
2020-654	Minor remark - Although the paper doesn't state on which dataset results shown in Table 1 were obtained, I suspect its CIFAR10, please specify this.	weakness
2020-654	This paper introduces the idea of energy based model to the traditional classifier, and proposes a new framework to improve the performances of the model in multiple aspects.	abstract
2020-654	The idea of reinterpreting the traditional classifier is very interesting, and the experiments show some good results of the proposed method.	strength
2020-654	Here are my main concerns of the current paper: 1. The training procedure seems to be very sensitive, and the SGLD may take a long time at each iteration to converge.	weakness
2020-654	This may be a big limitation of the proposed method.	weakness
2020-654	2. According to equation (8), the proposed method is having a trade-off between classification and generation, and this seems to be the key to improve the performance of the model in generation by sacrificing some classification accuracy.	weakness
2020-654	I think author should emphasize this instead of energy based model.	weakness
2020-654	3. The presentation is not very clear in section 5.	weakness
2020-654	What is the task of calibration, and what is the definition of ECE?	weakness
2020-654	4. The robustness guarantee seems too good to be true.	weakness
2020-654	Although the authors claim that they allow the attacker to have access to the gradient  of SGLD, the SGLD will add noise during the forward process, this will obfuscate the gradient.	weakness
2020-654	In this sense, I don't think the proposed method will have the strong robustness as they claimed.	weakness
2020-654	---------------- Post-Rebuttal Comments: Thanks for addressing my concerns.	misc
2020-654	Although I think the proposed method is not comprehensive to check obfuscated gradients, I do think the current version is a good fit for *CONF*, and I decide to increase my score.	decision

2020-656	# Summary The authors investigate an attack-defense problem in which an attacker attempts to pass authentication by generating a faked input, while an authenticator attempts to detect the fraud.	abstract
2020-656	They formulate this problem as a zero-sum game and reveal the closed form of the optimal strategies.	abstract
2020-656	Furthermore, they reveal a more insightful closed form of the optimal strategies in the Gaussian case.	abstract
2020-656	This result clarifies the relationship between the success rate of the attacker and the numbers of the source, registration, and leaked observations.	abstract
2020-656	The analysis for the Gaussian case also gives an interesting insight that the optimal attacker's strategy is to generate fake inputs so that its sufficient statistics are matched to that of the leaked observations.	abstract
2020-656	Based on this insight, the authors propose a new learning algorithm for the authenticator and demonstrate by some empirical evaluations that the proposed algorithm is robust against the faked input.	abstract
2020-656	# Detailed comments This is an interesting and well-written paper.	strength
2020-656	I recommend acceptance of this paper.	decision
2020-656	The authors investigate an attack of generating a faked input for passing the authenticator under which the attacker can only observe partial information about the source input.	abstract
2020-656	This is an interesting point of view and allows us to analyze a more practical situation.	strength
2020-656	Furthermore, based on the theoretical analyses, they reveal an interesting insight of the optimal attacker's strategy that the optimal strategy generates a faked input so that its sufficient statistics are matched to that of the leaked observations.	strength
2020-656	This insight introduces the new robust learning algorithm which outperforms the existing robust learning algorithm demonstrated as in the empirical evaluations.	strength
2020-656	Some minor refinements would improve the paper: - \\bar{x} and \\bar{a} in Theorem 4.2 should be clearly defined.	weakness
2020-656	- It seems to me that the authors use the term "ML attacker" to denote some different attacking algorithms.	weakness
2020-656	This paper proposes a new threat model for generative impersonation attacks: The attacker has access to several leaked images of a person; the authenticator knows several registration images per person and decides a person's identify by comparing some newly-sampled images from that person with corresponding registration images.	abstract
2020-656	The authors formulate this threat model as a minimax game and analyzed its Nash equilibrium.	abstract
2020-656	In the simplified case that observations are multivariate Gaussian, the authors are able to characterize the optimal strategies of the attacker and authenticator explicitly, which gives a nice intuition on how the theoretical optimum changes with respect to data dimension, number of leaked images, etc.	abstract
2020-656	Additionally, the authors implemented this attack (named Gan-in-the-middle attack) with an objective similar to GANs, empirically verified the theoretical results, and demonstrated the success of their approach on VoxCeleb2 and	abstract
2020-656	As far as I know, this formulation of generative impersonation attacks is novel.	strength
2020-656	The threat model nicely captures the most important aspects of impersonation attacks and is relatively realistic.	strength
2020-656	The theoretical analysis is insightful.	strength
2020-656	I especially like that the authors can prove no defense is possible when n <= m, which nicely matches the intuition.	strength
2020-656	The results on Gaussian case not only provide intuition, but also provide motivation for the design of attacker and defender architectures in GIM attacks.	strength
2020-656	The experiments are well-designed. The model architectures are well-motivated from Theorem 4.2.	strength
2020-656	It is great to see that results of toy experiments match the theoretical analysis in Figure 1(a).	strength
2020-656	The GIM attack on the Voxceleb2 images generates very realistic and reasonable portraits in Figure 2(a).	strength
2020-656	The data augmentation experiment can be naturally fit into the framework of impersonation attacks and the application of their techniques in this direction is very exciting.	strength
2020-656	I only have two minor suggestions: 1. In Theorem 4.1, the symbol g_{X | Y} was introduced previously, whereas g_{X | A} was never introduced.	weakness
2020-656	I have to go to the appendix to understand the definition of g_{X | A}.	weakness
2020-656	2. There is a minor issue in the proof of Lemma D.2 in page 16.	weakness
2020-656	The authors seem to miss a 1/2 factor in the second to last row in equation (D.3).	weakness
2020-656	This paper addresses the issue of malicious use of generative models to fool authentication/anomaly detection systems that rely on sensor data.	abstract
2020-656	The authors formulate the scenario as a maxmin game between an authenticator and an attacker, with limitations on the number of samples available to the authenticator to fix a decision rule, the number of samples required at test time for the authenticator to take a decision and the number of leaked samples the attacker has access to.	abstract
2020-656	The authors prove that the game admits a Nash equilibrium and derive a closed form solution for the case of multivariate Gaussian data.	abstract
2020-656	Finally, the authors propose an algorithm called "GAN In the Middle" and perform experiments to show consistency with the theoretical results, better authentication performance than state of the art methods and usability for data augmentation.	abstract
2020-656	This pager should be accepted.	decision
2020-656	Overall, it addresses crucial problems with the recent advances of generative models and provides significant theoretical results.	strength
2020-656	The experiments would benefit from some clarification.	weakness
2020-656	For the experiments, the following should be addressed: * Confidence intervals for all results (specifically in Figure 1a and in Table 1)	suggestion
2020-656	* Figure 1a: it would be interesting to see a similar analysis also for other values of m and k.	suggestion
2020-656	* Table 1: This result would also be more supporting if experiments were performed for varying values of m, n and k.	suggestion
2020-656	The description of the RS attack could be made more precise: does it mean that the attacker samples images at random?	weakness
2020-656	Intuitively, it feels confusing that the GIM authenticator would perform worse on this setting.	weakness
2020-656	* The experiments on handwritten data would be more similar to what I imagine being a real-world authentication scenario  if performed on a task where a class is a single person writing multiple characters, as opposed to characters being classes.	weakness
2020-656	Minor comments: * Page 6, second to last row: there is a"the" repeated twice.	weakness
2020-656	Optimal Strategies Against Generative Attacks" describes just what the title implies - various dimensions of the problem of defending against a generative adversary, with theoretical discussion under limited settings, as well as practical experiments extending on the intuitions gained using the theoretical exploration under limited conditions.	weakness
2020-656	Particularly, one of the key stated goals of the paper is to "construct a theorectical framework for studying the security risk arising from generative models, and explore its practical implications".	weakness
2020-656	Given this goal, the paper performs admirably.	strength
2020-656	The appendix is extensive, and gives a lot more insight into the core paper itself.	strength
2020-656	I am unclear on how the data augmentation experiment fits into the overall picture - perhaps a more detailed explanation of how and why this would be used to form an "attack" would help.	weakness
2020-656	The other experiments are sensible, and demonstrate reasonable and expected results.	strength
2020-656	This is a solid paper, and most of my critiques are "out of scope" and revolve around experiments that would be nice to see.	strength
2020-656	Though GAN-for-text is not simple, showing this type of setup for text would be interesting for a number of reasons, same for audio.	strength
2020-656	Continual passes through the text, with a focus on clarity could also be helpful - the topic is dense, and the text does a good job describing what is happening, but it is always possible to further distill these complex topics, and relegate some useful-but-not-critical pieces to the appendix.	strength
2020-656	These are minor quibbles, and overall this paper was an interesting and useful read, on a relatively underexplored topic.	strength
2020-656	It shows theorectical results, and practical experimental demonstrations of the theories proposed.	strength
2020-656	Given the stated goals of the paper, it performs admirably.	strength

2020-657	Summary: This paper proposes GraphZoom, a framework for augmenting unsupervised graph embedding methods by (a) fusing feature information into the graph topology, (b) learning embeddings on a coarsened graph, and (c) refining the coarsened embeddings to obtain embeddings for the original graph nodes.	abstract
2020-657	In particular, a nearest neighbor graph over node features is computed and this adjacency matrix is linearly combined with the original adjacency matrix to obtain a graph with feature information "fused in".	abstract
2020-657	The graph is then coarsened using a spectral approach, embeddings are learned on the coarsened graph (via any strategy), and the embeddings are then refined back to the original nodes (again using a spectral approach).	abstract
2020-657	The authors take care to heed the advice of Maehara et al. and remove high-frequency information from the features.	abstract
2020-657	Assessment: Overall, this is a borderline contribution with some interesting motivation, original ideas, and sound derivations.	strength
2020-657	However, the primary limitation of this work is the empirical comparison.	weakness
2020-657	First, the empirical comparison includes DeepWalk and GraphSAGE as the two base models, and while these are reasonable models, they are known to no longer be state of the art in this area (e.g., see https://arxiv.org/pdf/1809.10341.pdf).	weakness
2020-657	It would be more appropriate to include a more recent and better performing method (e.g., DGI; linked previously), as the reported numbers are very far from state-of-the-art.	suggestion
2020-657	In addition---and perhaps a more concerning issue---is that seems that a randomly initialized GCN can obtain similar or superior performance compared to the numbers reported in this work (again, see the DGI paper linked above).	weakness
2020-657	While it is possible that GraphZoom+DGI or GraphZoom+[some other more recent method] could achieve stronger results, the fact that the current results seem to be below performance of a randomly initialized GCN is a major issue.	weakness
2020-657	Stronger empirical results with better baselines and base models would drastically improve the paper.	suggestion
2020-657	As another point regarding the empirical results, the datasets used are known to be problematic (e.g., see https://arxiv.org/abs/1811.05868).	weakness
2020-657	If these datasets are used, then multiple random splits should be employed and more robust summary statistics should be reported.	weakness
2020-657	Regarding the fusion step, there were also two points that should be addressed in the paper: 1) It seems that this fusion setup is assuming that the network exhibits homophily (i.e., it assumes that nearby nodes have similar features).	weakness
2020-657	This is common in many networks (e.g., the benchmarks that are analyzed) but not always the case.	weakness
2020-657	Some commentary on when (if ever) this fusion process might *not* be appropriate would improve the paper.	suggestion
2020-657	2) The authors state the they use the coarsening process to compute the nearest neighbor graph in order to avoid the quadratic time complexity.	weakness
2020-657	However, there are numerous well-established approaches to deal with this issues (e.g., locality sensitive hashing).	suggestion
2020-657	Why was one of these standard approaches not employed?	weakness
2020-657	Reasons to accept: - Original and well-motivated idea	strength
2020-657	- Clearly written paper Reasons to reject: - Problematic empirical evaluation (e.g., lacking recent baselines)	weakness
2020-657	- Several performance numbers appear to be below random GCN baseline performance	weakness
2020-657	- General applicability of the approach (e.g., to non-homophilous networks) is not clear Summary: The authors propose a way to fuse information on nodes of a graph with the topology of the graph in the large scale setting.	weakness
2020-657	The proposed approach is done in four phases where	weakness
2020-657	(i) the covariates in the nodes of the graph is first mapped in the graph space for fusion and fused using linear combination of the topological graph and feature graph,	weakness
2020-657	(ii) the resulting "adjacency" matrix will almost surely not be sparse even if the original graph space, so they use eigenvalues of the graph laplacian to coarsen the graph	weakness
2020-657	-- remove edges; (iii) they then propose to embed the coarsened graph using "any" unsupervised learning technique;	weakness
2020-657	(iv) then the embedded representation is refined using iterative procedures.	weakness
2020-657	Cheap procedures are introduced to do Phases (i) and (iv).	weakness
2020-657	Experimentally the authors see improvements in the performance using their approach compared to the baselines considered.	weakness
2020-657	Novelty: 1. The approach suggested in this paper is already there in MILE Fig 1., the authors mention that MILE requires training GCN but I am not sure why this is critical.	weakness
2020-657	The authors mention that "MILE cannot support inductive embedding models due to the transductive property of GCN", can you clarify what this means?	weakness
2020-657	I guess one can easily replace GCNs?	weakness
2020-657	2. Covariate adjusted clustering is known to work only when when the features are independent like Stochastic Block Model, see  Covariate-assisted spectral clustering by Binkiewicz et al, 2014.	weakness
2020-657	Is there a reason why the features that we see on nodes are not correlated?	weakness
2020-657	Results: It is hard to see where the performance improvement actually comes from, if at all.	weakness
2020-657	It is interesting to see that the proposed approach saves time and is more accurate in the variety of settings considered, but it is not clear why we see the improvement.	weakness
2020-657	After rebuttal: I have raised my score to 6 after going through the authors' response for my questions, and other reviewers' concerns.	rebuttal_process
2020-657	While the approach performs well in many datasets (thanks to the authors for providing more experimental evidence!), I'm still not convinced with the authors' response on their fusion step -- it seems to me that node attributes are "side" information, that can "boost" the signal on the original neighborhood graph.	rebuttal_process
2020-657	Recall that spectral approaches do have a fundamental barrier -- they fail on "thin" graphs (see https://arxiv.org/abs/1608.04845 ).	rebuttal_process
2020-657	Hence,  node covariance/fusion matrix being dense will be a blessing for spectral approaches since they make spectral methods work.	rebuttal_process
2020-657	However, is this what we want in *all* the cases?	rebuttal_process
2020-657	I'm not sure. This means that the choice of \\beta in their fusion step is *very* important, and I don't see any plots on the sensitivity of their procedure with respect to \\beta.	rebuttal_process
2020-657	I kindly request the authors to include a plot or results showing the sensitivity of the final results with respect to the choice of \\beta.	suggestion
2020-657	Thanks! The paper provides a multi-level graph-coarsening approach that can improve the predictive and computational performances of numerous existing unsupervised graph embedding models.	strength
2020-657	The proposed approach is a pipeline consisting of 4 steps, viz: 1) Graph Fusion - that fuses attribute similarity graph with network topology, 2> Graph Coarsening - that reduces the graph size iteratively, 3> Graph embedding - using existing models and 4> Embedding refinement.	strength
2020-657	While such a pipeline for scaling using a graph coarsening and refinement based approach is not new, the authors have carefully designed the pipeline to be effective and be scalable such as without any costly learning components (as in mile).	strength
2020-657	The effectiveness of the proposed approach is evaluated with the node classification task on 6 datasets.	strength
2020-657	Strengths: - The paper addresses a very important problem.	strength
2020-657	The paper proposes a well-designed pipeline to scale existing embedding models.	strength
2020-657	- Experimental results support that the proposed approach is effective, especially in terms of reducing computation complexity.	strength
2020-657	Weaknesses: - While the experimental results are convincing on the computation front, I have few concerns on the performance front.	weakness
2020-657	a) 'MILE with the fused graph' baseline is missing.	weakness
2020-657	It can been seen from Figure 3 that the incorporation of the attribute graph provides a significant performance benefit.	weakness
2020-657	Thus it is necessary to have this baseline to understand the improvement gap w.r.t to MILE.	weakness
2020-657	I believe this is a fair comparison to make as the graph fusion component is a commonly used technique in the last decade.	weakness
2020-657	b) Improvements are inconclusive without additional results on other standard non-attributed graph datasets.	weakness
2020-657	In Figure 3, ignoring the model with the fused graph, MILE seems to be comparable to GraphZoom overall.	weakness
2020-657	As with the existing results, it's not conclusive whether GraphZoom is better than MILE.	weakness
2020-657	Also, add variance and report t-test results.	weakness
2020-657	c) That said, it can be seen from Figure 2, that GraphZoom significantly outperforms both DW and MILE(DW) on a large non-attributed dataset.	weakness
2020-657	However, it is not clear where the significant increase in performance benefits stems from.	weakness
2020-657	More analysis is required here.	weakness
2020-657	- Results on other unsupervised embedding task missings.	weakness
2020-657	It is important to evaluate the embeddings additionally for the link prediction task at the least.	suggestion
2020-657	Additional comment: - It would be helpful to incorporate one if not some of the attributed graph embedding model as a base model and baseline, such as Deep Graph Infomax (DGI).	suggestion
2020-657	- It should be easy to use a mini-batch version of GCN with MILE and use it for inductive learning.	suggestion
2020-657	- It would interesting to see what the performance will be without the refinement step.	suggestion
2020-657	If my concerns regarding the experiments are positively addressed, I'm willing to improve the score.	misc
2020-657	----------------- After the rebuttal, I have updated my score from 3 to 8 as the authors have satisfactorily responded to the concerns raised.	rebuttal_process

2020-673	This paper presents a method to make Transformer models more efficient in time and memory.	abstract
2020-673	The proposed approach consists mainly of three main operations: - Using reversible layers (inspired from RevNets) in order to prevent the need of storing the activations of all layers to be reused for back propagation;	abstract
2020-673	- Using locality sensitive hashing to approximate the costly softmax(QK^T) computation in the full dot-product attention;	abstract
2020-673	- Chunking the feed-forward layers computations to reduce their cost.	abstract
2020-673	This approach is first applied to a toy dataset to analyze its complexity, then tested on enwik8 language modelling task and imagenet-64 image generation task for ablation study and performance assessment.	abstract
2020-673	The problem approached by the paper is interesting and the proposed approach is novel to the best of my knowledge.	strength
2020-673	The paper is well structured and clearly written a part from some small typos (see minor comments below).	strength
2020-673	While the analysis of complexity is sound and convincing, and the fact of being able to train larger Reformers is very interesting, I have some questions and concerns about the approach and experiments.	weakness
2020-673	- Effect of reversible layers: It is clear for the experiment of Imagenet64 that the effect is negligible, but the experiment on enwik8 in the paper seems unfinished.	weakness
2020-673	Did the authors manage to finish the training, and does it confirm the observation?	weakness
2020-673	- Sharing QK: I am a bit confused about the effect and usefulness of this operation.	weakness
2020-673	Can the authors comment on why it is needed for LSH attention?	suggestion
2020-673	It seems to me that the same operations can be achieved with different Q and K.	weakness
2020-673	Indeed, doing so, the authors slightly reduce the capacity of the model.	weakness
2020-673	The observed non-significantly decreased performance can be an effect of using only 3-layers.	weakness
2020-673	This may explain why the results reported for larger models in figure 5 show higher bpc than similar size state of the art models.	weakness
2020-673	- Time per iterations: Can the authors report the time per iteration for the larger hash rounds (8 and 16) that are closer to full attention?	weakness
2020-673	For the highest reported number (4), from a quick and not precise look at figure 4, it seems that the performance achieved by the proposed method after 140k iterations is achieved by the full attention after ~40k iterations.	weakness
2020-673	The gain in time per iteration for this particular number of hash rounds can be lost by the loss in performance.	weakness
2020-673	- Can the authors detail how they chose the hyperparameters of their approach?	weakness
2020-673	e.g. the size of hash buckets, the distribution used to generate the random matrix R ..	weakness
2020-673	- The reported results can be made stronger by reporting average/error bars across several trial to show consistency.	weakness
2020-673	Minor: typos: Dimension of matrix R [d_k, d_b/2] -> [d_k, b/2]	weakness
2020-673	Last paragraph of page 6: state of these art -> state of the art	weakness
2020-673	——————————————— After rebuttal: I have read the authors answer, and found they addressed my concerns.	rebuttal_process
2020-673	I'm therefore increasing my score.	rebuttal_process
2020-673	This manuscript presents a number of algorithmic techniques to reduce the computational and space complexity of Transformer, a powerful and very popular deep learning model for natural language processing (NLP).	abstract
2020-673	Although Transformer has revolutionized the field of NLP, many small groups cannot make a full use of it due to lack of necessary computational resources.	weakness
2020-673	As such, it is very important to improve the space and computational complexity of this popular deep model.	suggestion
2020-673	The techniques presented in this manuscript seem to be very reasonable and the experimental results also indicate that they are effective.	strength
2020-673	My major concern is that the authors shall present more detailed experimental results.	weakness
2020-673	In addition to bits per dim, it will also better if the authors can evaluate the performance in terms of other metrics.	weakness
2020-673	This paper presents an attempt to reduce the memory complexity of Transformers.	abstract
2020-673	The authors call their model the Reformer.	abstract
2020-673	It presents a LSH based self-attention mechanism, along with reversible adaptation of Transformers.	abstract
2020-673	The Locality sensitive hashing scheme reduces complexity from L^2 to L which is pretty neat.	abstract
2020-673	Tackling the quadratic complexity of self-attention is indeed an important and nice direction.	strength
2020-673	I think the LSH based attention quite novel and is a natural solution to reducing the complexity of the self-attention module.	strength
2020-673	However, I think the technical description could be improved as the current form is quite confusing and difficult to parse.	weakness
2020-673	The experiments are a little on the weaker side.	weakness
2020-673	Authors presented results on imagenet, enwiki and a synthetic task.	weakness
2020-673	I am mainly concerned if the Reformer works on tasks such as machine translation or other NLP tasks.	weakness
2020-673	The paper does not present much evidence that the effectiveness of LSH is broad and versatile.	weakness
2020-673	My current vote is a weak accept, based on some preliminary understanding and the general novelty of the idea.	decision
2020-673	I do have some questions/issues/comments: 1) Given that there is some form of QK sorting, how is it possible to mask the future?	weakness
2020-673	Is this because tokens are sorted within buckets?	weakness
2020-673	2) Can the authors clarify what "Causal masking on the Transformer is typically implemented to allow a position i to attend to itself." mean?	weakness
2020-673	3) I'm a little confused about how the sorting is being done.	weakness
2020-673	Can this be done in an end-to-end differentiable manner?	weakness
2020-673	4) Can the authors present some results on other tasks?	weakness
2020-673	While neat, I think other tasks (e.g., MT or QA) can be investigated to further ascertain that the LSH attention works well.	weakness
2020-673	Current experimental results are not too convincing.	weakness

2020-687	Post Rebuttal Summary --------------------------------- I have nudged my score up to an "Accept", based on my comments to the rebuttal below.	rebuttal_process
2020-687	I hope the authors continue to improve the readability of Sec. 2.1	suggestion
2020-687	Review Summary -------------- Overall I think this is almost above the bar to be accepted, and I could be persuaded with a strong rebuttal.	decision
2020-687	The strengths here are the extensive experiments and the easy-to-implement method.	strength
2020-687	The primary weakness of this paper is that it is a "straightfoward" way to extend the BBP-MAP method to CNNs and RNNs, so the methodological novelty is weak relative to the BBP-MAP past work (Yurochkin et al. ICML 2019).	weakness
2020-687	Other technical weaknesses limit the ability to use this method on clients with diverse class distributions, which will be common in real deployments.	weakness
2020-687	Paper Summary ------------- This paper addresses the problem of federated learning, where J separate "clients" with disjoint datasets each train a neural network model for a supervised problem, and then try to aggregate all J individual client models into one "global model" in a coherent way.	abstract
2020-687	The natural problem is that due to hidden units being permutable within one network, naively taking parameter averages across two client models will lead to bad accuracy without first coming up with a consistent ordering of the units in each layer.	abstract
2020-687	Previous work (Yurochkin et al. ICML 2019) has developed a Bayesian nonparametric model based on the Beta-Bernoulli Process (BBP) for the case of federated learning of multi-layer perceptrons.	abstract
2020-687	However, the extension to convolutional layers or recurrent layers has yet to be solved, which is the focus of this paper.	abstract
2020-687	This paper's algorithm (Federated Matched Averaging (FedMA), see Alg 1), proceeds by iteratively stepping thru the CNN or RNN layer by layer greedily from input to output.	abstract
2020-687	At each layer, we first solve a BBP-MAP optimization (bipartite matching using a BBP maximum a-posteriori objective as cost function, a subprocedure taken direclty from Yurochkin et al.).	abstract
2020-687	This obtains a consistent low-cost permutation for each client model.	abstract
2020-687	Then, the global model weights for that layer is the average of the aligned client weights.	abstract
2020-687	After the current layer update, each client keeps training, keeping all layers up to the current frozen but revising later layers.	abstract
2020-687	This layer-by-layer training can be applied to both CNNs and RNNs.	abstract
2020-687	The proposed approach is compared to FedAvg and FedProx on MNIST and CIFAR image classification tasks with CNNs, and Shakespeare text classification tasks with RNNs.	abstract
2020-687	Later experiments explore the effect of communication efficiency (MB transfered between client and master), effect of local training epochs, handling biased class distributions, and interpretabilty.	abstract
2020-687	Novelty & Significance ----------------------- Solving federated learning problems is of increasing practical importance, and certainly trying to do so for CNNs and RNNs (more than just large MLPs) is important.	strength
2020-687	So I like where the paper is going.	misc
2020-687	Although the method is "new", it is more or less a straightforward extension of work by Yurochkin et al. (ICML 2019) to CNNs and RNNs. If you read the last few sentences of Yurochkin et al., you'll see "Finally, it is of interest to extend our model-ing framework to other architectures such as Convolutional Neural Networks (CNNs) and Recurrent Neural Networks (RNNs).	weakness
2020-687	The permutation invariance necessitating matching inference also arises in CNNs since any permutation of the filters results in the same output, however additional bookkeeping is needed due to the pooling operations." I view this paper as a well-executed implementation of this "bookkeeping".	weakness
2020-687	Certainly not trivial, but to some readers perhaps not clearly "above the bar" for a top conference like *CONF*.	decision
2020-687	Technical Concerns ------------------ ## Concern 1: Client models will not always be alignable after permutation	weakness
2020-687	My first concern is that there will not always be a one-to-one permutation of the neurons learned by two client models with different class distributions.	weakness
2020-687	Given fixed capacity at each layer, some clients may learn a filter for "horse hooves" (esp.	weakness
2020-687	if horse images are common to that client), while other clients may learn a filter for "snake skin" (if snakes are more common to that client).	weakness
2020-687	I wonder if we can quantify how well the aligned filters match in practice, and if there is any benefit to revising the alignment to allow some client-specific customizations (e.g. by having the global model can learn more units than the client model).	suggestion
2020-687	## Concern 2: Use of the BBP-MAP subprocedure poorly motivated	weakness
2020-687	The paper prioritizes a clean and easy-to-implement algorithm to resolve practical alignment issues between client CNN and RNN models.	abstract
2020-687	However, I was a bit underwhelmed that the BBP-MAP solution used by Yurochkin et al. was treated as a black-box subprocedure without much justification.	weakness
2020-687	I could see 2 preferable alternatives to the current use of BBP-MAP.	suggestion
2020-687	Either a simpler approach using Eq. 2 with a squared error cost and the Munkres algorithm to solve bipartitite matching to obtain the permutation (which seems more in spirit of the rest of the paper).	suggestion
2020-687	Or, a more sophisticated probabilistic approach (taking a Bayesian hierarchical model from Yurochkin et al. seriously and forming the estimated global weights from a weighted sums that includes both the clients (weighted by dataset size) and the assumed prior).	suggestion
2020-687	As it is, I feel the BBP-MAP subprocedure in the current Algorithm 1 is poorly motivated for the task at hand.	weakness
2020-687	Experimental Evaluation ----------------------- Overall the experiments were extensive and demonstrated several apparent advantages (reduced need to transfer large memory during communication, etc.).	strength
2020-687	Minor Presentation Concerns --------------------- Before Eq. 2, you should introduce the "\\theta" notation	suggestion
2020-687	I'm a bit confused about how "FedMA" differs from "FedMA with communication", even after reading Sec. 2.3.	weakness
2020-687	How exactly are communicate costs kept down?	weakness
2020-687	What are you sending from master to client at beginning of every "round" if not the full global model (all weights of the CNN)?	weakness
2020-687	Edit: Thanks for the thorough and responsive rebuttal!	rebuttal_process
2020-687	I'm particularly happy to see the additional background on BBP-MAP and the baselines you've added for handling data bias.	strength
2020-687	You've comprehensively addressed my questions and I think this paper should be accepted.	decision
2020-687	Original review: The authors extend the recently proposed Probabilistic Federated Neural Matching (PFNM) algorithm of Yurochkin et al. ( 2019) to more kinds of neural networks, show that it isn't as effective for larger models as it is for LeNet-sized ones, and propose enhancements that lead to a state-of-the-art approach they call FedMA.	abstract
2020-687	I'm convinced that this represents a meaningful advance in federated learning, although the paper could use some tightening up, and the experiments are somewhat limited.	strength
2020-687	Some feedback: - I'd like to see a little bit more description of BBP-MAP, as even though it's not one of the components of the algorithm you directly modify it's still the underlying mathematical primitive.	weakness
2020-687	How far is it from having the same effect that the "best possible" permutation would?	weakness
2020-687	How is it able to allow the number of neurons in the federated model to grow relative to the size of the client models?	weakness
2020-687	- Can you include the "entire data" baseline in more of the figures/plots (especially Figure 2)?	weakness
2020-687	- The models and datasets covered in the experiments are adequate to demonstrate that the presented technique is worth exploring, but probably not for someone considering applying it in the context of a deployed federated learning application.	weakness
2020-687	Since federated learning is a problem domain motivated more by applied concerns (privacy, edge vs.	weakness
2020-687	cloud compute, on-device ML) than other areas of machine learning theory, it would be particularly valuable to see experiments at larger scale (in particular, on larger or more realistic datasets).	suggestion
2020-687	- The section that demonstrates how your model addresses skewed data domains is fascinating!	suggestion
2020-687	That's one area in which your experiments are directly relevant to federated learning in practice, and it's a rapidly growing area of research in itself (e.g. in its relationship to causal learning that Leon Bottou has recently been exploring).	suggestion
2020-687	Exploring this further could make for a whole separate paper.	suggestion
2020-687	In the mean time, though, is there some kind of equivalent of the "entire data" baseline that would represent e.g. the best known technique for taking into account skewed domains outside the federated context?	weakness
2020-687	This paper offers a beautiful and simple method for federated learning.	abstract
2020-687	Strong empirical results. Important area.	strength

2021-7	This paper presented a novel kernel decomposition for nonsymmetric determinantal point processes, which enables linear time of inference and learning w.r.t. the cardinality of the ground set M.	abstract
2021-7	This is a significant improvement over previous arts and makes NDPP practical in relatively large datasets.	strength
2021-7	The theoretical of the paper is solid and supportive to the main claim of the paper.	strength
2021-7	This paper is well written and easy to follow.	strength
2021-7	Even for readers without much theoretical background, this paper is still moderately friendly since the logic and insight are clear.	strength
2021-7	I vote this paper a strong acceptance, except for some minor concerns as follows: The authors employed a way to simplify the kernel decomposition below Theorem 1.	decision
2021-7	However, it is unclear what the impact of this simplification is to the exactness of learning or inference.	weakness
2021-7	It would me a plus if the authors can give some theoretical analysis on the gap between such simplification and P0+.	weakness
2021-7	I notice the authors provide both learning and inference procedures for NDPP.	weakness
2021-7	It seems these two procedures can formulate a way to learn latent NDPP in an Expectation-Maximization fashion.	weakness
2021-7	I would suggest the authors give some discussion about the feasibility of such integration, and this can be an interesting direction.	suggestion
2021-7	In general, I enjoy reading this paper and think this paper is insightful.	strength
2021-7	This paper propose a decomposition for non-symmetric determinantal point process (NDPP) kernels (M*M) which reduces the requirements of storage and running to linear in cardinality (M).	abstract
2021-7	Additionally, they derive a NDPP maximum a posteriori inference algorithm that applies to both their proposed kernel and the previous work (NDPP).	abstract
2021-7	In their experiments, they show both learning kernels and the MAP inference for subset selection on real-life datasets.	abstract
2021-7	Pros: ○ This paper is well-written and easy to follow.	strength
2021-7	○ The author provide sufficient calculation process and relevant proofs of scalable NDPP.	strength
2021-7	○ For the existing problems of traditional DPP method is time consuming, need large memory and could not apply to large set, the scalable NDPP really solves them (e.g., this method could run on Instacart and Million Song datasets).	weakness
2021-7	I think it is practical.	strength
2021-7	Major Concern: ○ I have some doubts about the authenticity of the experimental results in Table 2, for the following reason: in the previous work, i.e., [1] Learning Nonsymmetric Determinantal Point Processes.	weakness
2021-7	Gartrell et al. Neurips2019. The results under average MPR have signifcant difference with same datasets and same hyperparameter settings.	weakness
2021-7	However, in NDPP paper,  for Amazon: Apparel, the MPR of sym dpp is 77.42±1.12, MPR of nonsym DPP is 80.32±0.75, for uk retail,  the MPR of sym dpp is 76.79±0.6, MPR of nonsym DPP is 79.45±0.57.	weakness
2021-7	In this paper,   for Amazon: Apparel, the MPR of sym dpp is 62.63±1.81, MPR of nonsym DPP is 72.2±3.07, for uk retail,  the MPR of sym dpp is 69.95±1.32, MPR of nonsym DPP is 74.17±1.37.	weakness
2021-7	There is a huge gap btw these two versions.	weakness
2021-7	I would like to know what causes this gap.	weakness
2021-7	Nonsymmetric determinantal point processes (NDPPs) received some attention recently because they allow modeling of both negative and positive correlations between items.	abstract
2021-7	This paper developed scalable learning and MAP inference algorithms with space and time complexity linear in ground set size, which is a huge improvement compared to previous approaches.	strength
2021-7	Experimental results show that the algorithms scale significantly better, and can roughly match the predictive performance of prior work.	strength
2021-7	This is a well written paper and I recommend its acceptance.	decision
2021-7	Scalable learning and MAP inference algorithms are important for the application of the NDPPs model, which seems promising compared with its symmetric counterpart in experiments.	strength
2021-7	I have some (minor) comments listed below.	misc
2021-7	In Lemma 1, the result is only proved for skew-symmetric matrices with even rank.	rebuttal_process
2021-7	Does it hold for odd rank matrices?	suggestion
2021-7	This is important to support the claim that the new decomposition covers the P0+ space.	weakness
2021-7	Equation (3) uses notation λi, which is already used in Lemma 1.	weakness
2021-7	This could cause confusion. In the paragraph after Theorem 1, it is proposed to set B = V and relax C.	weakness
2021-7	Is this used in Section 4?	weakness
2021-7	If not, I would suggest moving it to the experiments section, and adding some comparison in Table 2 to show the impact of this simplification.	suggestion
2021-7	I cannot quite understand the last sentence before Lemma 2.	weakness
2021-7	What can be computed in O(K2) time?	weakness
2021-7	The footnote in Table 1 might cause confusion because it can be mis-interpreted as a square.	weakness
2021-7	In G.1, the first sentence after equation (13), do you mean when M is odd or when ℓ is odd?	weakness
2021-7	In equation (24), X should be BTXB	weakness
2021-7	The inverse of C appears in the gradient of Z.	weakness
2021-7	Is C guaranteed to be invertible in the learning algorithm?	weakness
2021-7	And how are V, B, C initialized in the algorithm?	weakness
2021-7	In equation (31), please double check if we need the reciprocal in the denominator.	weakness

2021-11	Summary: This paper proposes a Rao-Blackwellized version of the straight-through gumbel-softmax gradient (STGS) estimator.	abstract
2021-11	The Gumbel-Rao estimator remains single-evaluation (but multiple sample), does not have higher variance than the original straight-through estimator.	abstract
2021-11	The estimator exhibits lower variance at lower temperatures in the experiments.	abstract
2021-11	Contributions: Proposes a single-evaluation estimator that cannot have higher variance than the STGS gradient estimator.	strength
2021-11	Demonstrates effectiveness of proposed estimator in terms of the variance of the gradient estimator and the ELBO on a toy task, a simple parsing task (ListOps), and a mixture model for MNIST.	strength
2021-11	Strengths: The method is simple and the computational overhead is very small compared to the original STGS estimator.	strength
2021-11	The empirical results support lower variance claims and effectiveness at lower temperature.	strength
2021-11	Weaknesses: I am not convinced that the relative gains from training at lower temperatures are significant.	weakness
2021-11	The overall gains over ST-GS seem to be modest on MNIST as well as the L <= 50 setting in ListOps. In the ListOps experiments, lower temperatures barely achieved better accuracy.	weakness
2021-11	Decision: Marginally below acceptance threshold	decision
2021-11	Improving gradient estimators for discrete latent variable models is an important problem.	strength
2021-11	The method is straightforward and the claims of performing better at lower temperatures are supported by empirical evidence.	strength
2021-11	However, the overall performance on the ListOps dataset is lower than related work [1], and there does not appear to be a large gain from low temperatures.	weakness
2021-11	Questions: The main argument of this paper hinges on the claim that lower temperatures result in lower bias of the gradient estimator.	weakness
2021-11	This claim seems reasonable, and is supported by figure 2b.	strength
2021-11	Is there a proof or citation for it, and do we know more?	strength
2021-11	It would be nice to know how variance and bias are traded off, as that would tell us how much (or how little) we could gain from training at lower temperatures.	suggestion
2021-11	Is there an explanation for the difference in performance between the 99% accuracy obtained in Havrylov et.	suggestion
2021-11	al. 2019 [1] and the performance obtained at low temperatures in this paper?	weakness
2021-11	How does this method perform versus the estimator proposed in Pervez et.	suggestion
2021-11	al. [2], which is also single-evaluation?	weakness
2021-11	Suggestions: The GR estimator is not guaranteed to have lower variance than ST-GS, just not higher.	suggestion
2021-11	Is there an application where lower temperatures are necessary for training?	suggestion
2021-11	That would strengthen the argument.	suggestion
2021-11	[1] Serhii Havrylov, German Kruszewski, and Armand Joulin.	misc
2021-11	Cooperative Learning of Disjoint Syntax and Semantics.	misc
2021-11	In Proceedings of NAACL 2019.	misc
2021-11	[2] Pervez, A., Cohen, T., & Gavves, E.	misc
2021-11	2020. Low Bias Low Variance Gradient Estimates for Hierarchical Boolean Stochastic Networks.	misc
2021-11	ICML 2020. Edited score after author comments.	misc
2021-11	This paper introduces the Rao-blackwellization technique to reduce the variance of the straight-through gumbel-softmax gradient (STGS) estimator wrt the parameters of discrete distributions.	abstract
2021-11	The proposed method introduces almost trivial computational costs (relative to function evaluations) and is empirically and theoretically shown to systematically improve STGS.	abstract
2021-11	I don't have a lot of nitpicking to make for this paper, as it is quite well executed.	strength
2021-11	The proposed method is very clean and the improvement over the STGS baseline is very consistent, and makes it even competitive with concrete-relaxation in the discrete latent variable model experiment.	strength
2021-11	Details: Why not show the curve of ELBO during training, but the arrival-time-at-certain-thresholds in Fig 2-c?	weakness
2021-11	Last paragraph of sec 5.4: The larger batch size here also reduces the variance of minibatch SGD, not just the variance of ∇GR in (13).	weakness
2021-11	In fact each instance is a different approximate posterior, which has different base variance.	weakness
2021-11	This makes the discussion in 3.3 a bit misleading.	weakness
2021-11	Suggestions: For figure 1, perhaps visualize the variance of both separately.	suggestion
2021-11	An improvement by 2 is not that meaningful if the variances of both are >> 2.	suggestion
2021-11	-- After rebuttal Thank you for revising the paper.	misc
2021-11	I've read the revised section, and stand by my original evaluation.	misc
2021-11	Summary: The paper presents a new way algorithm to compute the straight-through variant of the Gumbel Softmax gradient estimator.	abstract
2021-11	The method does not change the estimator's bias, but provably reduces its variance (with a small overhead, using Rao-blackwellization).	abstract
2021-11	The new estimator shows good performance on different tasks, and appears to lead to more efficient optimization for lower temperatures (lower bias).	abstract
2021-11	Clarity: The paper is well written.	strength
2021-11	Originality: The use of Rao-blackwellization in the proposed way is, up to the best of my knowledge, novel.	strength
2021-11	Pros of the paper and significance: Relaxation-based gradient estimators are widely used, and the proposed method may their variance quite significantly.	strength
2021-11	The proposed algorithm has a clear justification from a theoretical perspective, and admits a simple implementation.	strength
2021-11	The proposed algorithm does not require additional model evaluations, and thus may lead to large reductions in variance without incurring a high computational cost.	strength
2021-11	The proposed method leads to more efficient optimization at lower temperatures (lower temperature translates to lower bias, but often higher variances).	strength
2021-11	Cons: I'd say one thing that could be included are additional baselines in the experimental section.	weakness
2021-11	There are other estimators that may be use.	weakness
2021-11	For instance, you could compare against VIMCO.	suggestion
2021-11	While this is a different type of estimator (non single evaluation, not based on relaxations), it could be interesting to see how the results compare using this estimator too.	suggestion
2021-11	Recommendation: Accept (reasons in the "pros" list above).	decision

2021-19	This paper presents an approach for end-to-end speech synthesis, where every step is learned jointly with the others.	abstract
2021-19	Specifically,  the proposed model takes a character sequence as input and outputs an audio signal directly.	abstract
2021-19	The model is trained using an combination of losses, including an adversarial loss.	abstract
2021-19	In the experimental section, the proposed approach is compared against strong baselines, and ablation studies are presented.	abstract
2021-19	Pros: The proposed approach is novel and a significant step toward competitive end-to-end models.	strength
2021-19	The related work is thorough as far as I can tell.	strength
2021-19	The experiments are insightful, showing the impact of each part of the system.	strength
2021-19	Cons: The performance are promising, but still below the baselines.	weakness
2021-19	The end-to-end claim is a bit misleading as the character-based model is not performing well, and the phoneme-based model is not really end-to-end, as the g2p part is not trained jointly.	weakness
2021-19	The paper is sometime not easy to follow.	weakness
2021-19	Detailed comments: The reason behind using an adversarial loss is not really explained in the paper.	weakness
2021-19	A few lines before section 2.1 would help clarify that.	suggestion
2021-19	The order of the sub-sections (2.1-2.7) in Section 2 is not intuitive and seems a bit random, making the section slightly hard to follow.	weakness
2021-19	Section 2.7: "inconsistent spelling rules of the English language" -> It's not about inconsistent rules: every language has inconsistent rules, several dialects and variations in the pronunciation of words.	weakness
2021-19	Please rephrase. It's not clear which dataset was used in the experiment.	weakness
2021-19	If it is a private dataset, please state it clearly.	weakness
2021-19	Overall, despite the paper's two main weaknesses (not fully end-to-end and lower performance),  I think it is a significant step towards fully end-to-end model and should be accepted.	decision
2021-19	Summary The authors propose EATS, a method for TTS from unaligned audio and text data, directly to the waveform.	abstract
2021-19	Previous work either use aligned phonetic features, or output spectrograms that are later converted to a waveform by a deep vocoder model.	abstract
2021-19	In order to achieve this, the authors had to use several tricks, some already existing, for instance taken from the GAN TTS architecture, and some novel.	suggestion
2021-19	The three key novelties are: differentiable monotonic attention with gaussian kernel and length prediction.	suggestion
2021-19	dynamic time warping for the spectrogram loss.	misc
2021-19	using both spectrogram and waveform domain discriminators	abstract
2021-19	The authors provide a comprehensive ablation study with MOS score, although their model is under the state of the art by a significant margin.	abstract
2021-19	Review This paper builds on GAN TTS, and tries to make it trainable end-to-end without aligned features.	abstract
2021-19	The two main contributions, namely dynamic time warping and monotonic attention with gaussian kernel are both elegant, and can likely be used for many other applications related to time series with heterogeneous time scales.	strength
2021-19	In particular, the time warping loss allows to accomodate both for the natural irregularities in spoken speech, as well as providing sufficient signal for the monotonic attention to work.	strength
2021-19	The rest of the architecture is very similar to GAN TTS except for the spectrogram domain discriminator that was added.	strength
2021-19	While the model is under the state of the art for TTS, the samples are already quite convincing.	strength
2021-19	The authors conduct a thorough ablation study, both with MOS and audio samples.	strength
2021-19	Overall I think this is a really good paper, that is likely to prove quite useful for the development of end to end speech synthesis solution.	strength
2021-19	As I already mentioned, I also believe that the approach of using dynamic time warping and monotonic attention can be used for other kind of time series.	strength
2021-19	Remarks and questions to the authors	rebuttal_process
2021-19	Table 1, MOS for Tacotron 2 would be very informative.	suggestion
2021-19	All the baselines are trained on aligned data while Tacotron is a legitimate contender for EATS as it can be trained on the same data.	strength
2021-19	The point of the authors is that their methods is simpler because the training is in one stage.	rebuttal_process
2021-19	However, given the large number of losses and components in their model, with their respective hyper-parameters to tune, I'm not entirely sold on the simplicity argument.	weakness
2021-19	The tacotron 2 paper reports a MOS of 4.5 but on a private dataset.	weakness
2021-19	Section 3, [1] used the same simple L1 + log spectrogram loss as used here.	weakness
2021-19	I was surprised by the bad performance of the transformer attention, in particular in the audio samples, the output for this model is garbage towards the end of the signal.	weakness
2021-19	Any clue on why this would happen?	weakness
2021-19	It would be interesting to have a benchmark, in particular, can the model generate speech in real time on GPU and on CPU?	suggestion
2021-19	[1] SING: Symbol-to-Instrument Neural Generator, Defossez et al. Neurips 2018.	misc
2021-19	This paper proposes a fully end-to-end TTS system with adversarial training.	abstract
2021-19	The proposed method has three main advantages: 1) a simple, end-to-end pipeline with only two submodules, with a fully differentiable and efficient architecture;	abstract
2021-19	2) a flexible dynamic time warping to compute the loss between the predicted (generated) and true spectrograms;	abstract
2021-19	3) a good MOS performance compared with the other state-of-the-art systems with more supervision.	abstract
2021-19	The evaluation is done very thoroughly with a variety of ablation studies.	abstract
2021-19	Overall, I vote for accepting the paper.	decision
2021-19	First of all, the paper is very well organized and easy to follow.	strength
2021-19	Related works are well summarized, while focusing on the main differences on the proposed method.	strength
2021-19	Method is explained in great detail with easy-to-follow descriptions, proper equations, and very appropriate figures.	strength
2021-19	Solid evaluation is performed, and experimental results and speech samples are convincing.	strength
2021-19	Pros: The structure of the paper allows an easy read.	strength
2021-19	The main contributions are clearly stated and supported by the experiments.	strength
2021-19	Major works on the similar topic are widely covered and referenced.	strength
2021-19	Evaluation is thorough enough to support the arguments with in-depth ablation studies.	strength
2021-19	Appendices provide useful, supplementary information.	strength
2021-19	Cons: No comparison over the computational cost nor model size is presented.	weakness
2021-19	It is of particular interest because the proposed model is non-autoregressive, and thus may be capable of a causal, real-time inference.	weakness
2021-19	No use of widely accepted benchmark datasets.	weakness
2021-19	More direct comparison would be of interest.	weakness
2021-19	Minor comments/questions: If I understood correctly, the training sample has no phoneme-level alignment.	weakness
2021-19	Instead, only a sentence-speech pair is provided.	weakness
2021-19	If so, how do you select the corresponding text snippet from a sentence when you randomly sample 2 seconds of audio from the training examples whose length varies from 1 to 20 seconds?	weakness
2021-19	Section 2.5 on DTW is rather lengthy.	weakness
2021-19	DTW is a quite well-known algorithm for alignment between the two sequences, the detailed explanation on the algorithm may be omitted without the loss of readability, in my opinion.	weakness
2021-19	In Section 2.1, T is used to denote the total number of output times steps of the aligner, while T in Section 2.4 denotes the number of mel-frequency frames.	weakness
2021-19	Are these T's identical? The proposed aligner module doesn't seem to be very useful compared with the attention-based aligner as seen in the ablation study (Table 1): very small improvement from 3.551 to 3.559 MOS.	weakness
2021-19	Can you provide more explanations?	weakness
2021-19	This paper proposes a novel TTS system that 1) relies on almost no intermediate representation; and 2) is entirely feedforward instead of autoregressive.	abstract
2021-19	There are two major strengths in the paper.	misc
2021-19	First, several modules in the proposed system are novel and smart, including the aligner and the dynamic programming loss, and it is, to my knowledge, the first feedforward text-to-speech system that does not rely on the intermediate representation.	strength
2021-19	Second, the experiment result is convincing.	strength
2021-19	There are, however, some room for improvement and questions for the author to clarify.	weakness
2021-19	First, the elegance in the architecture is overshadowed by the complicated training algorithm.	weakness
2021-19	The training loss is like the superposition of common loss terms in the speech synthesis community, making the method look a bit heuristic.	weakness
2021-19	The complicated training algorithm also makes the proposed method harder to reproduce.	weakness
2021-19	It would be helpful if the authors can provide brief guidelines for readers trying to reimplement EATS, such as how to tune the hyperparameters.	suggestion
2021-19	Second, notice that EATS performs slightly worse than GAN-TTS, which does not quite show the benefit of end-to-end training (unlike ClariNet).	weakness
2021-19	I understand that there are a lot of challenges in training EATS, and the authors have briefly discussed this in Section 5.	suggestion
2021-19	However, it is worthwhile to expand the discussion a bit by showing further experiments that demonstrate the potential benefit of end-to-end training.	rebuttal_process
2021-19	Finally, although end-to-end comes with the (potential) merits of improved data efficiency and improved quality, it also has its downsides.	strength
2021-19	Without a clearly interpretable hidden representation, it is harder to have direct control over prosody.	weakness
2021-19	How would prosody control be possible under the end-to-end framework?	weakness
2021-19	Despite the weaknesses, this paper makes sufficiently novel contributions in TTS, making it above the acceptance threshold.	decision
2021-19	I would look forward to further justifications of the EATS paradigm.	misc

2021-22	The authors addressed my concern so I increased my score to 8.	rebuttal_process
2021-22	This is a very interesting idea for controlling a pretrained model for some sort desired criteria.	abstract
2021-22	The authors argue that existing approaches for this have taken a pointwise view for instance using REINFORCE to optimize for a particular reward.	abstract
2021-22	This can lead models to over-optimize on the criteria and sacrifice diversity and other criteria.	abstract
2021-22	The authors instead propose to take a distributional view.	abstract
2021-22	Given the pretrained LM distribution a, they would like to find a distribution c as: p = arg min_{c∈C} D_KL(c, a)	abstract
2021-22	where C is a set of distributions that pass the constraints.	abstract
2021-22	Some of these constraints are point-wise but some are distributional.	abstract
2021-22	For instance when generating biographies, the authors would like a constraint e.g. X% should talk about a certain gender or occupation.	abstract
2021-22	The authors describe how their approach leads them to an EBM (energy based model) and subsequent derivations.	abstract
2021-22	I think some of this section could be better written for those who are not familiar with EBMs. The experiments are quite interesting and show how the author's "soft" approach allows them to elegantly adjust the distribution of the LM without degeneration.	strength
2021-22	Pros: -Very interesting idea. -Thorough experiments.	strength
2021-22	In addition to comparing with REINFORCE based methods,  the authors also compare with CTRL and PPLM in the appendix.	strength
2021-22	Cons: -I think the method section (especially the optimization part)  could be explained better for readers who are not familiar with EBM, and allow the paper to have more accessibility.	weakness
2021-22	The paper studies the controlled sequence generation problem based on pretrained language models, i.e., controlling a generic pretrained LM to satisfy certain constraints, e.g., removing certain biases in language models.	abstract
2021-22	Specifically, the paper proposes a distributional view and imposes constraints based on collective statistical properties.	abstract
2021-22	The problem is formalized as a constraint satisfaction problem, minimizing a divergence objective.	abstract
2021-22	The paper proposes to use KL-Adaptive DPG algorithm for approximating the optimal energy-based model distribution.	abstract
2021-22	Experiments were conducted over both pointwise constraints and distributional constraints, showing the effectiveness of the model over the compared baselines.	abstract
2021-22	Pros: The problem under study is an important problem and can have extensive impact on many downstream language generation applications.	strength
2021-22	This paper makes solid contributions by proposing a formal view on generation controlling.	strength
2021-22	It provides a framework to handle pointwise, distributional, and hybrid constraints.	strength
2021-22	The method proposed to sample from the sequential EBM makes sense and is empirically vilified to be effective.	weakness
2021-22	The experiments and analyses support the claims and conclusions.	strength
2021-22	Overall, the paper is well organized and easy to understand.	strength
2021-22	Cons: The paper may benefit from some human evaluation for text generation.	weakness
2021-22	It is somehow not easy to tell which model is better from figure 2, GDC or Ziegler.	weakness
2021-22	It seems that Ziegler is superior in generating attribute-related sentences while inferior in diversity.	weakness
2021-22	The sentence quality might be similar as the converged values of (π, a) are close.	weakness
2021-22	The current submission contains a number of typos, grammatical and other style issues, in both the main sections and appendixes, but these are rather easy to fix.	weakness
2021-22	Questions: For real-life applications, whether the proposed framework has scalability issue; e.g., if a task has a large number of constraints to consider or if the constraints are more complicated than what are tested in Section 3?	weakness
2021-22	Assuming one has already got an adjusted LM with some attributes based on GPT2, which would be better if she/he wants to add a new attribute to generation: starting scratch from GPT2 or continuing with the adjusted model?	weakness
2021-22	In this paper the authors have proposed a mechanism for controlled text generation both pointwise and distributional.	abstract
2021-22	That is they not only can generate each sentences bearing some specified contraint or attribute but also takes care of overall property distribution of the generates set of sentences.	abstract
2021-22	Though pointwise or per sentence level control is well explored, the distributional control is a new and promising direction which the authors have proposed.	strength
2021-22	The authors proposed a method Generation with Distributional Control (GDC),	abstract
2021-22	which is nothing but a constraint satisfaction problem over the probability distribution p representing the desired target Language Model.	abstract
2021-22	Overall I find the problem challenging and promising.	strength
2021-22	This is a nicely written paper.	strength
2021-22	However, I have some quetions regarding experimental evaluation.	weakness
2021-22	In the Figure 4, the authors have reported the generated sentences controlling sentiment and also report the frequency of the sentence present in the corpus.	weakness
2021-22	By corpus does it mean the original training corpus?	weakness
2021-22	or the generated corpus by GPT-2?	weakness
2021-22	The proposed method is imposing a constraint so that the generation distribution becomes closer to the original distribution (in this case GPT-2) and still satisfy the pointwise and distributional constraints.	weakness
2021-22	If the distributional constraints are not imposed,	weakness
2021-22	the generated sentences should be similar to that of the original GPT-2 generated sentences bearing which satisfy the pointwise constraint.	weakness
2021-22	What does the freq signifies here?	weakness
2021-22	The authors should provide some discussion regarding the same.	suggestion
2021-22	Are the sentences generated sequentially keeping the context of the previously generated sentences?	suggestion
2021-22	or they do not have any context of the  previously generated sentence?	weakness
2021-22	If each generated sentences are independent from previously generated sentences, how meaningful it is to impose distribution constraint on that ?	suggestion

2021-34	The authors target the unsupervised reinforcement learning problem.	abstract
2021-34	An opposite idea from the existing approaches by maximizing state entropy is adopted to minimize state entropy.	abstract
2021-34	It is interesting that such an idea has achieved good performance in unstable environments.	abstract
2021-34	A state distribution is fitted during the interaction with an environment and the probability of the current state is used as a virtual reward.	abstract
2021-34	The parameters or sufficient statistics are also applied to the policy.	abstract
2021-34	The motivation is clear and verified.	strength
2021-34	It is generally a good paper.	rating_summary
2021-34	It is surprising that the exploration is achieved in the long term even minimizing state entropy.	suggestion
2021-34	Is that possible the exploration events are from the 'unstable' environment?	weakness
2021-34	What if there are some patterns underlying the exploration events but only part of the 'unstable' environment?	weakness
2021-34	Is that OK to totally rely on unexpected events from the environment to explore the environment?	weakness
2021-34	Is that possible to add some exploration strategy in the developed model?	weakness
2021-34	This work proposes an RL approach SMiRL that is able to learn effective policies in unstable environments without the need for external reward.	abstract
2021-34	The idea at a high-level is almost the opposite of intrinsic motivation RL approaches, which encourage novelty-seeking behaviors.	abstract
2021-34	The proposed method instead aims to minimize surprise or state entropy.	abstract
2021-34	To train the agent, rewards come from state marginal estimates, but because this distribution is changing, the authors create an augmented MDP.	abstract
2021-34	Through experiments on game domains and robot control tasks, the authors show that SMiRL outperforms intrinsic motivation methods.	abstract
2021-34	The authors also show that SMiRL can be used to do imitation and can be combined with regular reward signals.	abstract
2021-34	Pros: The problem formulation is interesting and novel.	strength
2021-34	Intrinsic motivation is well studied, but this problem considers the setting where the environment is unstable rather than static, which requires new methods.	strength
2021-34	The paper is written well and is clear.	strength
2021-34	The motivation is described well.	strength
2021-34	The authors evaluate on many domains, highlighting the diversity of settings in which the approach can be applied.	strength
2021-34	Cons: It seems like this approach is only applicable to unstable environments.	weakness
2021-34	Does the approach fail for regular, static environments?	weakness
2021-34	I'm assuming the agent might just end up staying still because it's trying to seek stable states.	weakness
2021-34	It can be combined with the external reward signal but will the minimizing entropy objective hurt you?	weakness
2021-34	Without common sense knowledge, this approach would take many iterations to learn.	weakness
2021-34	So while this formulation might be more similar to the real world, the real world would only allow for a few interactions with the world.	weakness
2021-34	Comments: How is SMiRL doing better than the oracle in Figure 3 center?	weakness
2021-34	In Figure 3 left, it might be a problem that with minimal episodes, SMiRL does worse.	weakness
2021-34	If SMiRL is useful for more real-world unstable environments, this would require a simulator good enough to model the real world.	weakness
2021-34	Recommendation: Overall, the paper is interesting and novel.	strength
2021-34	The approach is reasonable and experiments show the value of the method in unstable environments.	strength
2021-34	I recommend "accept". Post-rebuttal response: The authors addressed most of my concerns so I continue to recommend acceptance of the paper.	decision
2021-34	Specifically, they answered my question about whether the approach will work in static environments and how prior data can be used to improve sample efficiency.	rebuttal_process
2021-34	They also conducted additional experiments to verify some of my questions.	rebuttal_process
2021-34	Summary: This paper proposes a new intrinsic objective for RL agents: surprise minimization.	abstract
2021-34	This may come as a surprise, as other related works usually propose to maximize surprise, or to maximize novelty.	abstract
2021-34	The authors present motivations and conduct an empirical study on several environments to support their idea.	abstract
2021-34	Strong points: Overall, I think it is a good paper.	strength
2021-34	Let me list some strong points: The idea is simple, novel and well motivated.	strength
2021-34	The paper positions this new intrinsic objective with respect to variants of novelty maximization objectives and brings a new perspective.	strength
2021-34	It's well written and organized	strength
2021-34	The algorithm is tested on a relevant selection of environments and against state-of-the-art algorithms using intrinsic objectives.	strength
2021-34	The empirical evidence seems to support the claims.	strength
2021-34	The related work is quite complete.	strength
2021-34	I liked the discussion about stable vs unstable environments.	strength
2021-34	It is the first time I see it discussed.	strength
2021-34	The website brings visualizations of trained policies.	strength
2021-34	Weak points: I will now list a few weak points of the paper.	misc
2021-34	Some descriptions of the results are missing.	weakness
2021-34	In figures, what does the shaded area represent?	weakness
2021-34	(std, sem, confidence intervals, etc).	weakness
2021-34	In Table 1, what are the numbers?	weakness
2021-34	(what is the central tendency, what is the error, how many seeds?).	weakness
2021-34	Same for Appendix D. The paper does not provide all necessary information to reproduce the results.	weakness
2021-34	There is no detail about the RL algorithms (TRPO and DQN), no description of the architectures, and no hyperparameters.	weakness
2021-34	This is important and should be contained somewhere in the Appendix.	weakness
2021-34	Will the code be released?	weakness
2021-34	If not, why so? Same questions for the environments, are they accessible somewhere?	weakness
2021-34	It seems to me that this approach could potentially tackle harder problems, but the paper is limited to a simplification of the tetris game, planar humanoid variants and Doom.	weakness
2021-34	The x-axes of the figures also tell us that only a few episodes were needed to solve them.	weakness
2021-34	I am not saying that I need the hardest games solved to find an algorithm interesting, but I am wondering whether it would scale.	weakness
2021-34	Could you tell us whether you attempted to tackle harder environments, and if so, why do you think SMiRL failed?	suggestion
2021-34	I think we can gain a lot of understanding by looking at negative results.	suggestion
2021-34	For example, I feel like testing this algorithm on a 3D humanoid would better demonstrate the power of this approach.	suggestion
2021-34	It seems to me that there might be a confounding factor that could partially explain the success of the surprise reward.	weakness
2021-34	Indeed, it seems that all environments presented here can terminate when the agent dies.	weakness
2021-34	This is a guess, as I could not find this information in the paper (please add it).	weakness
2021-34	If so, then the expected cumulative rewards is an increasing function of the lifetime of the agent in the game.	weakness
2021-34	Because of this, maximizing the cumulative surprise might be a good idea because it goes in the same direction (by construction) as maximizing survival.	weakness
2021-34	A counter-argument from the paper can be the performance of SMiRL + reward in Walk, that is superior to the performance of reward alone.	suggestion
2021-34	However, this is unclear as I could not find the description of the reward function in the paper (please add it).	weakness
2021-34	Another way to disprove this hypothesis would be to compare the performance of SMiRL to the performance of an agent maximizing a reward function that gives +1 whenever the agent is alive (survival bonus).	suggestion
2021-34	If it performs better, then surprise maximization brings something to the table, if it does not, then it might work because it is correlated to the survival time.	suggestion
2021-34	Another way could be to have episodes that do not include death-related resets (fixed length episodes).	suggestion
2021-34	Recommendation and justification: This is overall a strong paper.	strength
2021-34	However I'm concerned about the potential confounding factor of the survival time.	weakness
2021-34	I'll give a score of 6, but I would happily increase that score if	decision
2021-34	The authors convince me that the success of the surprise maximization is not due to the survival confounding factor.	rebuttal_process
2021-34	The authors include all necessary details for reproducibility and/or release the code.	rebuttal_process
2021-34	Discuss the scalability to harder problems (e.g. 3D Humanoid).	rebuttal_process
2021-34	Feedback to improve the paper (not part of assessment)	rebuttal_process
2021-34	I would move the discussion about how surprise minimization and novelty maximization can be complementary to the intro.	suggestion
2021-34	These two approaches seem to go in opposite directions and, as a reader, I would be happy to read this discussion early.	rebuttal_process
2021-34	It would be interesting to discuss how it plays out in natural agents.	suggestion
2021-34	The intuition is that minimizing surprise leads to finding a stable configuration and staying there.	suggestion
2021-34	In practice, it is probably balanced with other driving needs like the need for food.	suggestion
2021-34	I guess it is discussed in related papers like Friston 2009.	suggestion
2021-34	In natural agents, surprise minimization must also be model-based.	suggestion
2021-34	Indeed, animals do not need to jump out of cliffs several times to know that it's bad.	suggestion
2021-34	Not sure I understand the inequality in Eq1.	suggestion
2021-34	Maybe I missed something. Discuss the surprise maximization approach of Achiam et al and whether it differs from yours.	suggestion
2021-34	Table 1: the legend seems to disagree with the results.	weakness
2021-34	It seems to me that the entropy difference is as low in your environment as in the others, but the caption says "note the clear negative  entropy gap on our tasks, whereas this clear trend is absent on the Atari games".	weakness
2021-34	Fig 4. Is it really training in 80 episodes?	weakness
2021-34	There are very few images to train a VAE, especially if the episode resets when the agent falls (does it?) How many steps per episode?	weakness
2021-34	How do you get demos for humanoid tasks?	weakness
2021-34	I guess there are not human demos but previously trained agents?	weakness
2021-34	Results in Fig. 6 are not super satisfying, they are quite far from the target (although I guess it is a difficult task).	weakness
2021-34	I am not even sure the second example is achievable.	weakness
2021-34	In the traditional Tetris, wouldn't cubes fall due to gravity?	weakness
2021-34	Typos: "our results are available online" → missing full stop.	weakness
2021-34	"In such environments, which we believe are more reflective of the real world" → previous sentences do not discuss environments but intrinsic objectives.	weakness
2021-34	"unexpected events don't happen" → "do not"	weakness
2021-34	"deep DQN" → "Deep Q-Networks", or "DQN"	misc
2021-34	Update post-rebuttal The authors addressed most of my concerns, especially the one about the confounding factor.	rebuttal_process
2021-34	I am updating the score from 6 to 7.	rebuttal_process
2021-34	This paper presents and studies an unsupervised learning approach to the emergence of "meaningful/useful"	abstract
2021-34	behaviour in a Deep RL setting, based on an algorithm which objective consists in minimizing surprise.	abstract
2021-34	It is possible to see this paper as studying a Deep RL implementation of cognitive homeostasis, an old idea recently popularized through the work on the free energy principle by Friston and colleagues.	abstract
2021-34	The paper presents two versions of this algorithm, one without representation learning, and one with representation learning (VAE), and compares it to Deep RL algorithms that maximize prediction error/novelty from two perspectives: emergence of behaviours and utility as an auxiliary reward to solve sparse reward problems.	abstract
2021-34	The paper discusses the relevance of the approach in environments that are said to be "unstable" (with an attempt to formalize this concept).	abstract
2021-34	The paper also shortly presents an application of the algorithm for imitation learning.	abstract
2021-34	Experiments are presented in a variety of environments.	abstract
2021-34	Major strengths: The paper is globally clear and well-written	strength
2021-34	The qualitative results showing and analyzing emergent behaviour are stimulating	strength
2021-34	Implementation of the free-energy principle in high-dimensional spaces is known to be challenging and this paper contributes towards understanding how to do it (yet the precise formal articulation between SMIRL and the	strength
2021-34	FEP remains to be done, and this does not seem to be the primary objective of the paper)	strength
2021-34	The discussion of the complementarity of within-episode SMIRL and across-episode novelty seeking intrinsic rewards is interesting	strength
2021-34	The shortly described application for imitation learning is promising	strength
2021-34	The are many and varied environments in experiments	strength
2021-34	Major weaknesses: The environments chosen for experimentation are such that minimizing surprise aligns very well with either producing "interesting" behaviour or maximizing an external reward.	weakness
2021-34	One could easily consider slight modifications of most environments where a trivial solution to minimizing surprise could be found instead (e.g. giving the ability to push on the "pause" button in the Tetris game or having safe rooms in opposite directions than enemies in Vizdoom/HauntedHouse).	weakness
2021-34	In a real robot, this would lead a robot to hide in the corner of a room and just look at a uniform wall that does not move.	weakness
2021-34	This problem of trivial solutions to surprise minimization approaches has been called the Dark Room problem in the FEP literature.	weakness
2021-34	This is known to be a limit of this general approach when applied to the real world, and how to address it operationnally (in a tractable manner) under this paradigm remains an open question.	weakness
2021-34	Related to the point above, the fact that different environments are used in different experiments of the paper does not allow to get a good grasp of the general behaviour of the algorithm: I would recommend to run all experiments in all environments (maybe to report in Annex), and at least to justify why environmnets change across experiments.	weakness
2021-34	The paper is positioned in comparison with novelty seeking/prediction error maximizing agents, and motivates the surprise minimizing approach by arguing that real-world situations are problematic for novelty seeking agents,	weakness
2021-34	as the real world spontenously generates novelty through distractors that should be avoided rather than explored.	weakness
2021-34	This is accurate, but an incomplete discussion of the literature in the area of unsupervised learning of behaviours: approaches maximizing learning progress were precisely introduced to adress these limits of novelty seeking approaches and to scale to real world environments with two families of distractors: novel unlearnable parts of the environments	weakness
2021-34	(which is a problem for novelty seeking approaches) and trivial low-entropy parts of the environments (which are a problem in principle for surprise minimization approaches).	weakness
2021-34	Thus, the paper should position itself in comparison with LP-based approaches and show experiments with at least some of the existing LP-based algorithms (e.g. Schmidhuber, 1991; Lopes et al., 2012;	suggestion
2021-34	Kim et al., 2020). As the authors acknowledge, the version of SMIRL using represention learning is not fully "within episode surprise minmization",	suggestion
2021-34	as the learned representation depends on past episodes: as a consequence, it is unclear what one should conclude about the relation between within vs across episode mechanisms and the properties of emergent behaviours	weakness
2021-34	The paper lacks discussion of related work aiming to implement surprise minimization approaches that scale to high-dimensional spaces, especially in a RL framework, e.g. Tschantz et al., 2020; Annabi et al., 2020	weakness
2021-34	The paper does not provide code to enable reproducibility of results (and does not say it will)	weakness
2021-34	Overall, the topic of this paper is interesting and the work could make a valuabe contribution, especially from the perspective of studying incentives to emergent structured behaviour in unsupervised learning,.	strength
2021-34	However, the weaknesses mentionned above need to be addressed to better establish the contributions of this paper, both in terms of understanding the generality of the results and the contributions in relation to the existing litterature.	weakness
2021-34	References: Annabi, L., Pitti, A., & Quoy, M.	misc
2021-34	(2020). Autonomous learning and chaining of motor primitives using the Free Energy Principle.	misc
2021-34	arXiv preprint arXiv:2005.05151. Kim, K. H., Sano, M., De Freitas, J., Haber, N., & Yamins, D.	misc
2021-34	(2020, January). Active world model learning in agent-rich environments with progress curiosity.	misc
2021-34	In International Conference on Machine Learning (ICML).	misc
2021-34	Lopes, M., Lang, T., Toussaint, M., & Oudeyer, P.	misc
2021-34	Y. (2012). Exploration in model-based reinforcement learning by empirically estimating learning progress.	misc
2021-34	In Advances in neural information processing systems (pp.	misc
2021-34	206-214). J. Schmidhuber, "Curious model-building control systems," in Proc.	misc
2021-34	Int. Joint Conf. Neural Netw., Singapore, 1991, vol.	misc
2021-34	2, pp. 1458–1463. Tschantz, A., Baltieri, M., Seth, A.	misc
2021-34	K., & Buckley, C. L. (2020, July). Scaling active inference.	misc
2021-34	In 2020 International Joint Conference on Neural Networks (IJCNN) (pp.	misc
2021-34	1-8). IEEE. Tschantz, A., Millidge, B., Seth, A.	misc
2021-34	K., & Buckley, C. L. (2020).	misc
2021-34	Reinforcement Learning through Active Inference.	misc
2021-34	arXiv preprint arXiv:2002.12636.	misc

2021-38	The authors present new insights on PCA analysis by reconceiving it in terms of a Nash equilibrium among different players, related to the different components.	abstract
2021-38	The importance of an objective function minimizing the off-diagonal elements of R is emphasized.	abstract
2021-38	The insights lead to parallel algorithms and are demonstrated on large scale problems, which is nice.	abstract
2021-38	Overall the new insights can be very valuable and also inspiring for future work and for new developments,  from a broader perspective.	strength
2021-38	Points that can be improved: related to eqs (1)(2) the authors claim that there is a problem with some classical interpretation of PCA analysis.	weakness
2021-38	However, this statement is unclear and possibly incorrect: for coming to (2) the authors start from the solution i.e. the eigenvalue problem, while this should be the result of the derivation.	weakness
2021-38	This part should be clarified.	weakness
2021-38	Probably it is better to replace this part of the paper by a standard formulation and derivation of PCA analysis as given in standard textbooks.	suggestion
2021-38	Also one can find it component per component and add orthogonality constraints in each step.	suggestion
2021-38	it is nice that a parallel algorithm is obtained.	strength
2021-38	However, at this point the positioning of the result within the existing literature is not clear yet.	weakness
2021-38	In the areas of signal processing and neural networks, parallel algorithms and parallel implementations (systolic arrays, VLSI, etc.) of PCA analysis exist for more than 30 years, see e.g. the book Principal Component Neural Networks: Theory and Applications by K.I. Diamantaras, S.Y. Kung, and related work.	weakness
2021-38	section 5: missing methods are e.g. the power method, Lanczos method, Jacobi etc.	weakness
2021-38	related to future work the authors mention the role of PCA analysis with respect to VAE.	weakness
2021-38	There is existing work on generative kernel models, eigenvalue problems and disentanglement that is related to it, see Pandey et al  https://arxiv.org/abs/1906.08144 A.	weakness
2021-38	Summarize This paper proposes to not only maximizes the trace of the projected covariance matrix R but also minimizes the off-diagonal element of R, which helps to recover the real principal components(eigenvectors of the covariance matrix) from data, while the other large-scale algorithms only recover the top-k subspace.	weakness
2021-38	Furthermore, the authors utilize the hierarchical relation between eigenvectors and design a utility function for each eigenvector.	weakness
2021-38	Therefore, each eigenvector serves as a player in a game and they will achieve strict-Nash Equilibrium at the end, which enables a decentralized algorithm for large-scale PCA problems.	weakness
2021-38	In the experiment, the authors conduct experiments on synthetic data, moderate scale data, and large scale data by resnet activation maps.	abstract
2021-38	The first two experiments demonstrate that the proposed algorithm is competitive with Oja's algorithm and even better under some conditions.	abstract
2021-38	The large scale experiment on resnet activation maps is only feasible by the proposed algorithm and demonstrate that it is a powerful tool to achieve interpretable representation.	abstract
2021-38	B. Strength This paper is well organized and easy to follow.	strength
2021-38	The explanation about why other algorithms only recover the top-k subspace but not the principal components is step-by-step.	strength
2021-38	Based on this observation, the authors propose to minimize off-diagonal elements and derives the utility function, which adds a generalized Gram-Schmidt step to the gradient and can be decentralized naturally.	abstract
2021-38	Besides the algorithm itself, the proposed method also opens new doors for other interesting future research other than recovering principle components.	strength
2021-38	The experiments are simple yet sufficient to demonstrate the superiority of the proposed algorithm.	strength
2021-38	To the best of my knowledge, the proposed algorithm is the first that dealS with a problem as large as in the resnet-200 experiment.	strength
2021-38	C. Weakness: My only question is that the proposed algorithm focuses on recovering the real principle components and finding interpretable features.	weakness
2021-38	So it would be good if we can see some comparison of the lower-dimensional features in some downstream applications.	suggestion
2021-38	For example, can we cluster the input data into meaningful clusters better than other algorithms?	rebuttal_process
2021-38	D. Justification of score: This is a great paper that gives a new perspective on PCA and derives a novel decentralized large-scale algorithm and will inspire a lot of further research along this line.	strength
2021-38	So I vote to accept this paper.	decision
2021-38	Principal component analysis (PCA) is a well-known dimensionality reduction and feature learning technique in the literature that leads to uncorrelated features.	abstract
2021-38	While there are a plethora of algorithms for PCA, along with accompanying analysis, a majority of these works have been developed from an optimization perspective.	abstract
2021-38	This paper differs from existing works in that it motivates the k-PCA problem, which involves learning the k-dominant eigen vectors of the sample covariance matrix, as a competitive game between k players in which each player is supposed to estimate one of the eigen vectors and the PCA solution is the unique strict-Nash equilibrium.	abstract
2021-38	The main contributions of the paper in this regard are the following: Setting up the PCA problem as a competitive game between k players and showing that the Nash equilibrium corresponds to the PCA solution (Theorem 2.1)	strength
2021-38	Development of two games (algorithms), with one a sequential algorithm and the other a decentralized algorithm, for solving the PCA problem (Algorithms 1 and 2)	strength
2021-38	Convergence analysis of the sequential algorithm under a restrictive set of assumptions (Theorem 4.1)	strength
2021-38	Establishment of the equivalence between the decentralized algorithm and the Generalized Hebbian Algorithm (GHA) of Sanger (Proposition H.1)	strength
2021-38	Overall, this is a novel paper in that it offers an alternate view of the PCA problem, which might lead to further advances in our understanding of PCA-type algorithms in the future.	strength
2021-38	I therefore have a favorable view of this paper.	strength
2021-38	There are however several important aspects of this paper that need to be clarified by the authors in a subsequent revision before it becomes ready for publication.	decision
2021-38	Major Comments Theorem 2.1 is based on the assumption of the top-k eigenvalues being distinct.	weakness
2021-38	Algorithms such as Orthogonal Iteration ("subspace" power method), to the best of my understanding, only require an eigen gap between the k and k+1 eigenvalues and do not require the first k eigenvalues to be distinct.	weakness
2021-38	This needs to be discussed clearly in the paper.	weakness
2021-38	While Theorem 4.1 for the sequential game does not explicitly state it, it appears that it also requires the eigenvalues to be distinct (Theorem L.4, e.g.).	weakness
2021-38	This, once again, is a major assumption that is neither discussed clearly in the paper, nor compared to other works that do not seem to have this limitation.	weakness
2021-38	Majority of the works in the PCA literature require the initialization subspace to not be orthogonal to the k-PCA subspace.	weakness
2021-38	This work, however, requires the stringent assumption that each eigenvector is initialized to within π/4 radians of the original eigenvector.	weakness
2021-38	Not only is this a strict probabilistic assumption in the case of random initialization, but it also becomes harder to satisfy as k increases (as the authors also discuss).	weakness
2021-38	In light of this strict condition, this reviewer is confused by the claim in the paper that "these theoretical findings are strong relative to other claims." I would also have liked the authors to discuss this assumption up front in the paper.	weakness
2021-38	The sequential game appears to be very similar to other approaches that have been proposed in the literature that estimate an eigenvector, subtract its contributions from the data, and then estimate the next eigenvector (see, e.g., Allen-Zhu and Li, 2017 and Raja and Bajwa, 2020).	weakness
2021-38	Such approaches of course suffer from the fact that they require distinct eigenvalues, but they don't require any QR decomposition.	weakness
2021-38	There is however no discussion of the connections between such approaches and the proposed sequential game.	weakness
2021-38	Why is the decentralized game being called "decentralized"?	weakness
2021-38	Is there a distinction the authors are making between distributed and decentralized?	weakness
2021-38	What's the topology being considered by the authors in relation to this game and what exactly does "broadcast(v^i)" mean in terms of reaching out to other nodes?	weakness
2021-38	Some discussion of this would be useful.	weakness
2021-38	While the decentralized game has not been analyzed in this paper, the distributed variant of GHA has been analyzed in the literature; see "Fast and communication-efficient distributed PCA".	weakness
2021-38	It would be helpful for the authors to comment on the differences between their decentralized algorithm and this distributed GHA work.	suggestion
2021-38	Why is "the longest streak of consecutive vectors with angular error less than π/8 radians" the right metric for the experiments?	suggestion
2021-38	The claim in Figure 3 that "We omit Krasulina's as it is only designed to find the top-k subspace" is not clear to this reviewer.	weakness
2021-38	It would be useful for the authors to discuss the use of ∇vR, rather than ∇vi, for updates in both algorithms.	suggestion
2021-38	Minor Comments In my opinion, it is incorrect to say that PCA leads to interpretable features.	weakness
2021-38	The claim "An exponential convergence rate in the full-batch setting is possible using Riemannian acceleration techniques" is perhaps too ambitious, unless the authors are confident that this is doable, in which case one wonders why this was not shown in the paper.	weakness
2021-38	The experimental plots in the paper is too hard to see clearly.	weakness
2021-38	It might be a useful idea to add them to the appendix also, where they can be shown in larger sizes.	suggestion
2021-38	Post-discussion period comments The authors have satisfactorily addressed all of my comments as well as, in my opinion, comments of other reviewers.	rebuttal_process
2021-38	Based on the latest revised version of the paper, I am increasing my score to 8 (from 7).	rebuttal_process
2021-38	I believe this paper is worthy of publication in proceedings of *CONF* 2021 and I recommend it as such.	decision

2021-47	Summary The paper attempts to improve the interpretability of RL agents' action selection process by (a) proposing embedded self-prediction (ESP), a model that embeds generalised value functions (GVFs) in the action-value function of the agent with a "combining" function to and (b) ESP-DQN, an extension of DQN that augments experience replay tuples with a GVF feature vector and that decomposes the model into separate combining and GVF parameters.	abstract
2021-47	These enable to define action-values with respect to predefined feature maps, thus providing more "resolution" into the behaviour of the policy.	abstract
2021-47	Good stuff The idea of decomposing the policy into GVFs as a way to force explanations wrt.	strength
2021-47	some features is brilliant, and it is well executed when combined with the contrastive explanation system.	strength
2021-47	Sections 2-4 provide both a clear introduction to GVFs as well as a detailed and sound description of the overall framework.	strength
2021-47	The related work section is fairly tight, but actually covers a good amount of necessary and relevant related work, which makes it easy to scan through the literature.	strength
2021-47	The experimental section does employ mostly fairly similar environments, but it is clear in the hypotheses being tested, and it is fairly satisfactory considering what it is attempting to evaluate.	strength
2021-47	Uncategorised notes This is more of a meta-comment, but I enjoyed reading the paragraph about manually-designed features in Section 1.	strength
2021-47	I understand why the authors felt the need to write it, and it is a sad state of affairs that it is now often a requirement to argue what to many people is just reality.	strength
2021-47	I wonder if it'd be worth it to test the method on Atari, through possibly the use of MinAtar: https://github.com/kenjyoung/MinAtar	suggestion
2021-47	-- It feels like there's a gap in difficulty (of learning and analysis) between the ToW setting and the rest of the environments, and something aking to a middle ground would probably be a good setting to add.	weakness
2021-47	Complex gridworlds such as BabyAI and the NetHack Learning Environment are probably also good options.	weakness
2021-47	Final comments Overall, I'm extremely happy to strongly recommend this paper towards acceptance.	decision
2021-47	It is well written, it introduces a method that attempts to move forward towards solving an important problem in Reinforcement Learning, and there's a significant amount of details in the paper that would make it fairly straightforward to reproduce.	strength
2021-47	Typos Sec 1 RL agents explain its... -> their directly "embed"... -> embeds train those... -> trains Sec 3	strength
2021-47	Combination function update -> Combining?	weakness
2021-47	The manuscript is at times a little inconsistent.	weakness
2021-47	This paper proposes a method that offers explanation of action preference in a deep RL agent based on given features by the human.	abstract
2021-47	In other words, the model explains why action A is preferred to action B based on some given features.	abstract
2021-47	This is done through the embedded self-prediction (ESP) model.	abstract
2021-47	The authors also proposed a method for evaluation of importance of features in the learned policy.	abstract
2021-47	While the paper benefits from extended experimental result and interesting theoretical analysis in the tabular RL, I think its readability could be improved.	weakness
2021-47	For example, I believe generalized value functions should be explained more extensively as (I think) it is less known to the community compared to concepts like MDP and DQN.	weakness
2021-47	Also, an analysis  (or at least some discussion) on the effect of the number of features on learning QFs would be helpful.	weakness
2021-47	Another question I have, is about the dependence of features.	weakness
2021-47	What would happen to the evaluation if some given features are dependent?	weakness
2021-47	Some minor points: It would be helpful if the authors add a figure of their networks similar to what is common in the field, specifying input and output of the networks.	suggestion
2021-47	The phrase of "greedy policy" was a little confusing for me, especially because "greedy algorithm" is usually one-step look ahead search.	weakness
2021-47	Is it just argmax Q(s, a) (as suggested in page 3)?	weakness
2021-47	I think the defense on manually-designed features could be transferred to conclusion since the importance of interpretability has been already mentioned in the first paragraph.	weakness
2021-47	Edit: I have read the authors' response as well as the other reviews.	rebuttal_process
2021-47	Based on the additional results and added feature selection details, I now agree that ESP is generally applicable.	rebuttal_process
2021-47	Summary: The authors present ESP, an RL system that can then explain action choices in terms of future feature values.	abstract
2021-47	Generalized value functions (GVFs) are used to learn an estimate of total future discounted feature values.	abstract
2021-47	These estimated total feature values are then used to estimate each actions Q-value.	abstract
2021-47	Since Q-values are based on GVF outputs, these intermediate values can be used as an explanation.	abstract
2021-47	The authors select a subset of these values to form a Minimal Sufficient Explanation (MSX).	abstract
2021-47	The proposed system is evaluated using three domains.	abstract
2021-47	The authors show that performance is comparable to non-GVF agents.	abstract
2021-47	Reasons for score: Though the authors present a novel explanation format, the applicability of the method is uncertain.	weakness
2021-47	The results appear to rely on specific GVF feature choices.	weakness
2021-47	Non-general methods are still of interest, but the limited information on feature construction prevents a fair comparison to other approaches.	weakness
2021-47	Additionally, the explanations are not evaluated quantitatively.	weakness
2021-47	Pros: -The use of GVFs for explanations in terms of future feature values is a novel line of work.	strength
2021-47	MSXs are a natural way to then produce more concise explanations, and the authors extend MSX to their non-linear use case in a well-justified way.	strength
2021-47	-The analysis of the Tug of War explanations was thorough.	strength
2021-47	It clearly showed how ESP explanations would be used to investigate agent behavior.	strength
2021-47	Cons: -ESP is built upon the GVF features, but the choice of GVF features is suspect.	weakness
2021-47	Each environment uses a different style for its features.	weakness
2021-47	Lunar Lander and CartPole both have continuous features, yet the authors use "deltas" for Lunar Lander and region discretization for Cart Pole.	weakness
2021-47	Tug of War uses a bunch of features, including information about feature values when a game ends, and non-linearities are applied to the outputs of the GVF features.	weakness
2021-47	Note that these non-linearities are used only for Tug of War, and different non-linearities are used for different features (Table 2 in Appendix D).	weakness
2021-47	Effectively, each environment appears to use carefully engineered features.	weakness
2021-47	Given that DQN-Full performs very poorly for specific settings (i.e., in some cases, the agent cannot learn without the GVF features), the choice of features seems to be important.	weakness
2021-47	The authors should indicate the process used for selecting them and how these features should be chosen for other environments.	suggestion
2021-47	ESP may not be robust to GVF feature choice, but this is insufficiently addressed in the paper.	suggestion
2021-47	-In Section 6.3, the authors present potential conclusions that can be drawn from an ESP agent.	weakness
2021-47	These conclusions can be evaluated to determine whether valid conclusions can be drawn from the explanations.	suggestion
2021-47	Such an evaluation would allow the hypotheses of the authors to be tested.	weakness
2021-47	-A substantial reorganization of the paper would improve clarity.	weakness
2021-47	Various definitions and descriptions are provided a few sections after the terms/methods are first used.	weakness
2021-47	Terms are unnecessarily over-loaded (such as "sound").	weakness
2021-47	Questions During Rebuttal Period: -Please address and clarify the "Cons" above.	suggestion
2021-47	-In particular: a) How were the GVF features chosen?	suggestion
2021-47	Why does each environment use different features?	suggestion
2021-47	b) What happens when Lunar Lander is given "CartPole-style" (region discretization) features?	suggestion
2021-47	What happens when CartPole is given "Lunar-Lander-style" (change in features) features?	suggestion
2021-47	Minor comments: -This work would benefit from another editing pass for tense/plurality matching between subject and verb.	suggestion
2021-47	-The proofs in Appendix B would benefit from a re-write; currently, they are hard to parse.	weakness

2021-79	This paper shows that Mixup training is approximately certain kind of regularized loss minimization.	abstract
2021-79	Based on this, it provides some theoretical analysis on the advantages of Mixup training for the generalization and adversarial robustness over one-step attacks.	abstract
2021-79	This paper provides many insights on why Mixup works: e.g. connecting its 2nd order approximation with l2 adversarial loss; and shows that the Radmacher complexity of mixup adaptively characterize the intrinsic dimension of empirical data distribution.	abstract
2021-79	Though the techniques used in the paper were already developed by other works, the new results and insights on mixup in this work are worthy of being known by the community, particularly for that Mixup is such a popular data augmentation trick in deep learning.	strength
2021-79	Some questions: Could you provide any comments on Mixup and adversarial training, e.g. one-step and multi-step ones?	suggestion
2021-79	What about the generality of the analysis on L_infinity attacks?	weakness
2021-79	Summary This paper has extensive analysis on mixup augmentation, which focus on the effect of adversarial robustness and generalization.	abstract
2021-79	In adversarial robustness, They try to make a connection between mixup loss and adversarial loss, On the other hand of generalization,  they argue that mixup is a kind of data-apdaptive regularization.	abstract
2021-79	Comment Good contribution about author's careful analysis on connecting between mixup loss and adversarial loss.	strength
2021-79	It seems to be the first theoretical analysis on discussing their connection, since the past works just report the number to show how mixup and their variants to be robust to single-step adversarial attack.	strength
2021-79	Good to community about having a connection between Mixup and Rademacher complexity, I think it can make some impact to discuss the high-level connection between data augmentation and model generalization.	strength
2021-79	The paper theoretically studies the beneficial effect of mixup on robustness and generalization of machine models.	abstract
2021-79	The mixup loss is  rewritten to be the sum of the original empirical loss and a regularization term (plus a high order term).	abstract
2021-79	For robustness, the regularization term is proven to be upper bound of first and second order terms of the adversarial loss's Taylor expansion.	abstract
2021-79	Hence, the mixup loss can upper bound the approximate adversarial loss.	abstract
2021-79	For generalization, the regularization term is used to control the hypothesis to have small Rademacher complexity.	abstract
2021-79	The paper is clearly written and well organized.	strength
2021-79	pros: Rigorous theoretical analysis are conducted on non-linear models, specifically the neural network model.	strength
2021-79	The theoretical results are clean and insightful.	strength
2021-79	cons: When studying robustness, an approximated adversarial loss is considered.	weakness
2021-79	The approximated loss is the truncation of the Taylor expansion of the original loss.	weakness
2021-79	The quality of the approximation is not explored in the paper.	weakness
2021-79	It is better to provide numerical evidence that whether the bounds in Thm 3.1 and 3.3 still hold for original adversarial loss, and how tight the bounds are.	suggestion
2021-79	In the generalization part, only an indirect connection is built between mixup loss and the generalization gap.	weakness
2021-79	no result is provided concerning the generalization error of the solution found by minimizing the mixup loss.	weakness
2021-79	Paper summary The paper gives theoretical proof showing that the recently proposed data augmentation technique Mixup can indeed improve generalization and help in robustness.	abstract
2021-79	The theorems cover GLMs and certain classes of neural networks.	abstract
2021-79	The paper also contains numerical experiments supporting some aspects of the theory.	abstract
2021-79	Strengths Currently, there is only a limited theoretical understanding of why Mixup works.	weakness
2021-79	This paper shows that Mixup is essentially equal to regularizing the first and second derivatives (with respect to the input x).	weakness
2021-79	Intuitively, this means that changing the training samples slightly shouldn't change the output of the model much.	weakness
2021-79	Further, the paper proves that the mixup loss is an upper bound on the 2nd order Taylor approximation of the adversarial loss, and hence reducing mixup loss reduces adversarial loss.	abstract
2021-79	Finally, the paper proves that mixup helps in reducing the Rademacher complexity and hence improves generalization.	abstract
2021-79	The results seem fairly general and apply to many models such as GLMs and neural networks.	weakness
2021-79	The paper supports its approximations and claims by numerical experiments.	strength
2021-79	Concerns The regularizing term R3 looks like it is minimizing zT∇fθ(xi)z (for some z).	weakness
2021-79	This promotes the Hessian (wrt x) to have negative eigenvalues in the direction of z.	weakness
2021-79	Ideally, we would want the Hessian (and also the gradient) to be 0 around the training samples so that perturbing the input doesn't change the output much.	weakness
2021-79	Thus, I don't see how the R3 term helps regularize the Hessian properly.	weakness
2021-79	The paper claims that Assumption 3.1 holds when the minimizers are not too dispersed.	weakness
2021-79	Does it still hold for practical neural networks where the minimizers can possible be fairly far apart?	weakness
2021-79	Comments Although the paper seems well written, I have a few suggestions: The notation cos(θ,x) which refers to ⟨θ,x⟩|θ||x| should be explained in the preliminary section.	suggestion
2021-79	On page 6, the statement fθ(x)=∇fθ(x)Tx should be proven.	weakness
2021-79	It will save the reader some time if the proof is provided.	weakness
2021-79	In Remark 3.1, I think Theorem 3.2 should actually be Theorem 3.4	weakness
2021-79	Score justification There isn't much prior work on the theoretical understanding of Mixup.	weakness
2021-79	This paper provides theoretical guarantees for Mixup on two fronts - robustness and generalization; for both GLMs and ReLUs.	abstract

2021-85	Summary This paper proposes an improvement to how tokens are selected for masking in pre-training large masked language models (BERT and family).	abstract
2021-85	Specifically, it stipulates that purely random choice of words (or word pieces) makes the MLM task insufficiently hard.	abstract
2021-85	It then goes on to propose a data-driven approach for selecting n-grams to mask together.	abstract
2021-85	The approach, based on an extension of pointwise mutual information for n-grams, is shown to outperform random token and random spans masking strategies on performance of downstream tasks.	abstract
2021-85	Strong and weak points Strong points: The paper is very well written throughout, and easy to follow.	strength
2021-85	The problem is well motivated with empirical evidence.	strength
2021-85	I think Section 2 demonstrates well the case for random masking being too easy.	strength
2021-85	The multivariate version of PMI proposed is simple and well motivated.	strength
2021-85	The evaluation experiments are convincing the results are robust across the tasks shown,	strength
2021-85	Weak points: The one main drawback in this study is the lack of comparison with entity-based techniques for masking.	weakness
2021-85	In particular [1] has recently defined "salient span masking" based on named entity recognition and dates.	suggestion
2021-85	Salient span masking has been adopted in [2] where it is shown to boost performance of open-domain question answering by 9+ points (Table1 of [2], models tagged with "+ SSM").	abstract
2021-85	I think it would be extremely interesting to compare these techniques (SSM specifically, but entity-based techniques in general) with PMI-Masking.	suggestion
2021-85	Specifically, it is currently unclear whether the PMI based n-gram masking vocabulary simply ends up rediscovering popular named entity mentions, or whether there are more interesting sub-phrases (e.g., idiomatic sub-phrases) that a NER system would not select.	suggestion
2021-85	Finally, it would be interesting to empirically test whether these extra non-entity n-grams provide further performance boost over the entity-based "salient span masking".	suggestion
2021-85	[1] REALM: Retrieval-Augmented Language Model Pre-Training (https://arxiv.org/abs/2002.08909).	misc
2021-85	[2] How Much Knowledge Can You Pack Into the Parameters of a Language Model?	misc
2021-85	(https://arxiv.org/abs/2002.08910) Recommendation I recommend this paper for acceptance.	decision
2021-85	The analysis and ideas throughout the paper are well executed.	strength
2021-85	I also think the topic should be of high interest to the *CONF* and NLP communities, given the importance of MLM pre-training on most state-of-the-art models at the moment.	strength
2021-85	Despite the lack of comparison with entity-based techniques, having a statistically principled alternative, solely based on co-occurrence, without linguistic grounding, seems interesting.	strength
2021-85	Questions for authors The main question here related to the entity-based approaches discussed above.	strength
2021-85	I think it would be interesting to address this issue given how closely related it is to this work, and the good performance the cited papers demonstrate using it.	suggestion
2021-85	I can think of a couple of ways to address this comparison: (1) qualitative analysis of the masking vocabulary to better understand the differences between "PMI ngrams" and entity mentions, (2) experimental analysis incorporating some entity-based masking into the experiments in the paper.	suggestion
2021-85	Irrespectively of how you choose to address the entity-based comparison, I was interested in some analysis, or sampling, of the PMI-Masking vocabulary to understand what type of n-grams are being selected (entity mentions, idiomatic phrases, noun-phrases, etc.).	suggestion
2021-85	Would be interesting if you could make this vocab available or add a small sample in the appendix.	suggestion
2021-85	Throughout the paper there is an assumption that contiguous words are considered for masking.	weakness
2021-85	This was not immediately clear in the beginning of the paper (I realized it only in Section 3.2 with - "What about contiguous spans of more than two tokens?").	weakness
2021-85	But one question came to mind: what about correlated non-contiguous spans?	weakness
2021-85	For example "eigenvalue" and "eigenvector" are unlikely to be present in the same n-gram, but have reasonably high chances of showing up together in the same passage.	weakness
2021-85	Have you considered extending this work to non-contiguous spans?	weakness
2021-85	Is there any expectation that this would help learning, or is it just a bad idea?	weakness
2021-85	Was there an attempt to mix masking strategies during pre-training?	weakness
2021-85	Although Table 6 is convincing in demonstrating that single-token perplexity is not correlated with performance of downstream tasks, the differences seem curious.	weakness
2021-85	One is left wondering if there is any benefit in adding a small number of "easy" masking cases (i.e., random-tokens or random-spans)?	weakness
2021-85	This paper introduces an approach for masked language modeling, where they mask wordpieces together which have high PMI.	abstract
2021-85	The idea is relatively simple, has potential for high impact through broad adoption, and the paper is clearly written with extensive experiments.	strength
2021-85	The experiments on GLUE, SQuAD, and RACE are very well set up.	strength
2021-85	For example, evaluating multiple learning rates for each downstream task is expensive, but really adds to confidence I have in the results.	strength
2021-85	Reporting the best median dev score over five random initializations per hyperparameter, then evaluating that same model on the test set, definitely improves the reproducibility of the results.	strength
2021-85	In addition, showing how performance is affected by the amount of pretraining data is very useful, and the experiments range from small scale to large scale.	strength
2021-85	The ablations adjusting the vocabulary size (which, in turn, changes the size of wordpiece tokens) is a valuable contribution, and I would have asked to see something like this if it wasn't included.	strength
2021-85	Table 4 is a nice addition -- it's interesting that the MLM loss is not predictive of downstream performance.	strength
2021-85	I suspect this approach will become widely adopted (or built upon) in future work pretraining language models.	strength
2021-85	I give this paper an 8, only taking off points because the idea is relatively intuitive and doesn't really open a broad new area for future work.	weakness
2021-85	I don't see any obvious methodological flaws, which frankly, we can find in most papers.	weakness
2021-85	I would be interested in seeing if this reduced the variance of the fine-tuning process.	weakness
2021-85	That might be something the authors could include for the camera ready, maybe in the appendix.	suggestion
2021-85	Edit: After reviewing the author response, I will keep my score as it is.	rebuttal_process
2021-85	I believe the paper should be accepted.	decision
2021-85	Summary: The paper proposes a variant on the MLM training objective which uses PMI in order to determine which spans to mask.	abstract
2021-85	The idea is related to recently-proposed Whole Word Masking and Entity-based masking, but the authors argue the PMI-based approach is more principled.	abstract
2021-85	The method is straightforward--it involves computing PMIs for ngrams (in this case, up to length 5) over the training corpus, and then preferring to mask entire collocational phrases rather than single words during training.	abstract
2021-85	The intuition is that masking single words allows models to exploit simple collocations, thus optimizing their training objective without learning longer-range dependencies or higher level semantic features of the sentences, and this makes training less efficient than it could be.	abstract
2021-85	One contribution of the paper is a variant on the PMI metric that performs better for longer phrases by reducing the scores of phrases that happen to contain high-PMI subphrases, e.g. "George Washington is" should not have a high score despite the fact that "George Washington" does have a high score.	abstract
2021-85	The authors compare their method against vanilla BERT with random masking, as well as against recently proposed variants such as SpanBERT and AMBERT, and show consistent improvements in terms of final performance as well as better efficiency during training.	abstract
2021-85	By way of analysis, the authors also make an argument that token-level perplexity is not correlated with downstream performance.	abstract
2021-85	This is an interesting point to make, though they do not expound upon it in this paper.	abstract
2021-85	Strengths: The proposed method is simple and principled	strength
2021-85	The empirical results show consistent improvement on standard benchmark tasks	strength
2021-85	The proposed variation the PMI metric is a nice sub-contribution	strength
2021-85	Weaknesses: A somewhat marginal contribution, its not significantly different from the variants proposed previously (e.g., SpanBERT, entity masking)	weakness
2021-85	The evaluation focuses purely on benchmark tasks which are known to have flaws (e.g., the current "superhuman" performance on these tasks already makes gains on them suspect).	weakness
2021-85	I'd have liked to some more analysis/discussion of the linguistic consequences of this new objective.	suggestion
2021-85	See more specific comments below.	misc
2021-85	Additional Comments/Questions: I am curious about the more general effect of this training objective on the models linguistic (and particularly syntactic) knowledge.	weakness
2021-85	E.g., can you say more about how often the model sees unigrams being masked and how the distribution of these unigrams differs from what would be seen if we did random masking?	suggestion
2021-85	I ask because I could imagine that this objective has a noticeable effect on the masking of function words (e.g., preposition occurring more often in collocations, pronouns and determines maybe less often) and thus the model might get differing access to these words in isolation.	weakness
2021-85	Since function words carry a lot more signal about syntactic structure than do content words and phrases (of the type you are capturing in your PMI metric), I'm very curious if there are some tradeoffs (or, possibly, additional advantages) that comes with your method that are not reflected by the benchmark performance metrics.	weakness
2021-85	Squad and GLUE are going to favor knowledge of things like entities and events, and capture very little about more nuanced linguistic reasoning, so reporting performance on some more recently released challenge sets, or using some probing studies, or at least just giving some analysis of win/loss patterns, would be very informative for assessing the contribution of this paper to NLP more generally.	suggestion
2021-85	Summary: This paper presents a masking strategy for training masked language models (MLM).	abstract
2021-85	The proposed strategy builds on previous approaches that mask semantically coherent spans of tokens (such as entire words, named entities, or spans) rather than randomly masking individual tokens.	abstract
2021-85	Specifically, the proposed method computes the PMI of spans (and the generalization for spans of size >2) over the pretraining corpus, and randomly masks from among the 800K spans (lengths 2-5) with the highest PMI.	abstract
2021-85	Masking based on PMI removes the ability for the model to rely on highly local signals to fill in the mask and instead focus on learning higher level semantics.	abstract
2021-85	They motivate this hypothesis with an experiment demonstrating that as the size of the WordPiece vocabulary decreases (and words are more frequently split into multiple tokens rather than being their own token), the transfer performance of the resulting MLM decreases.	abstract
2021-85	However, using whole-word masking with this same vocabulary size recovers much of the original performance, indicating that allowing the model to rely on these strong local signals harms the transfer quality of the resulting model.	abstract
2021-85	Experiments: The paper evaluates on three standard NLU benchmarks: SQuAD2, GLUE, and RACE.	abstract
2021-85	They compare their PMI-based algorithm against random token masking, random span masking, and a naive version of their PMI masking strategy.	abstract
2021-85	All baselines are implemented within a single codebase.	abstract
2021-85	Their strategy outperforms random-span masking on SQuAD throughout pretraining, though the latter does close the gap on a smaller pretraining corpus (16GB).	abstract
2021-85	They show this gap remains if they use a larger pretraining corpus (54GB), as would likely be the case with a large-scale pretraining experiment.	abstract
2021-85	The resulting models also consistently outperform all other baselines on all tasks.	abstract
2021-85	They do additionally compare to outside models (AMBERT, SpanBERT, RoBERTa) and show their PMI-based models outperform or perform similarly to these models.	abstract
2021-85	Overall, this paper introduces a solid theoretical basis for existing and new methods for training masked language models.	strength
2021-85	They present robust experiments demonstrating the efficacy of their method under various settings.	strength
2021-85	Missing citation for RACE and original SQuAD dataset?	weakness
2021-85	I assume this doesn't happen often, but is it ever the case that a training example does not have any mask-able spans?	weakness

2021-120	Summary of my understanding The authors propose an unsupervised (or rather, I would say self-supervised) method to build a simulator of incompressible fluid flows based on neural networks.	abstract
2021-120	Their neural network is given the pressure and velocity (or its potential) fields at time t and is trained so that it outputs the fields at time t+dt with the "physics-constrained loss functions."	abstract
2021-120	The proposed loss functions are designed to make the outputs of the neural net fulfill the Navier-Stokes equation and boundary conditions.	abstract
2021-120	They train the neural net on randomly-generated domains and boundary conditions.	abstract
2021-120	They show that the learned model outputs qualitatively plausible flows, even if the domain is not exactly handled in the training phase.	abstract
2021-120	Evaluation The paper is easy to follow.	strength
2021-120	The proposed method is technically reasonable.	strength
2021-120	The related work section is comprehensive.	strength
2021-120	While the technical novelty of each component of the proposed method looks moderate, the overall framework would be valuable as a fast differentiable fluid dynamics solver, which the authors claim.	strength
2021-120	The experiment section is convincing to some extent, but it lacks comparison to classical solvers (i.e., somewhat inherently-faithful simulations), which limits our capability to assess the soundness of the results in Section 4.1 and Appendix D.	weakness
2021-120	Also, the paper lacks information on the range of hyperparameter search.	weakness
2021-120	Questions [Q1] How did you create the randomized boundary conditions (vd)k0?	weakness
2021-120	I could not find it in the paper.	weakness
2021-120	[Q2] The training strategy described in Section 3.7, especially the pool's update and random renew, seems essential to the performance of the proposed method.	weakness
2021-120	Did you performed some ablation study in this regard, or do you have some notes on what happened if the presented strategy was not adopted partially?	weakness
2021-120	Such a description would help a reader's understanding much.	weakness
2021-120	[Q3] Can you provide the range of hyperparameter search and information on how intensively the search was done?	weakness
2021-120	Such information is important for assessing training computational cost.	weakness
2021-120	Summary This paper presents a "physics-informed" deep learning model of fluid dynamics.	abstract
2021-120	The underlying deep learning architecture employed is a somewhat standard u-net, but one of the proposed method's distinguishing features is that it enforces its adherence to physical behavior at its loss terms, by penalizing predictions that are not incompressible or do not conserve momentum.	abstract
2021-120	Notably, this approach allows it to be trained unsupervisedly, without requiring the generation of ground-truth simulations.	abstract
2021-120	Pros The enforcement of the physical behavior as a core feature of the architecture (e.g., through the "a" vector and the conservation losses) makes the network to generate stable simulations.	strength
2021-120	This also brings with it the interesting benefit of being able to learn without the need for ground truth data, lowering the cost of training the model.	strength
2021-120	The proposed model performs favorably, both in terms of speed and error, to phiflow, the current "reference" differentiable fluid simulator.	strength
2021-120	Since the proposed method is still deep learning based, it is also differentiable.	strength
2021-120	Cons Most of the experiments serve only to validate the model's accuracy and its ability to learn a proper fluid simulation.	weakness
2021-120	The only practical application demonstrated is in a simple control task, which is not explored too deeply.	weakness
2021-120	Phiflow (Holl et al., 2020), for example, performs more extensive and diverse evaluation of control settings.	weakness
2021-120	Reasons for score Overall, given the "pros" described above, notably the distinct loss formulation that allows the model to learn unsupervisedly to perform efficient, differentiable fluid simulations, I recommend this paper for acceptance.	decision
2021-120	Additional comments Is training improved if actual simulations are used for data, instead of "cold starts"?	weakness
2021-120	If so how do these compare with training with cold starts (as presented in the paper), both in terms of training speed and in terms of final (test) results?	weakness
2021-120	Does starting from more realistic starting points fix the issue of the initial error "spike" in Fig 4?	suggestion
2021-120	This paper proposes to learn the dynamics of an incompressible fluid via a physics informed loss formulation using an unsupervised training framework.	abstract
2021-120	It employs a custom solver that is executed at training time to learn a Navier-Stokes residual with a incompressible (curl of a stream function) formulation.	abstract
2021-120	This setup is demonstrated for two dimensional karman vortex streets, and a control example for the magnus effect.	abstract
2021-120	The paper definitely targets an interesting and relevant direction for machine learning, but it is a (fairly identical) resubmission of a paper that was rejected at NeurIPS.	weakness
2021-120	It is of course possible to resubmit papers, but unfortunately in this case it was a fairly clear rejection, and while I cannot compare versions side by side, I think the submitted content is largely the same.	decision
2021-120	As both NeurIPS and *CONF* are currently mostly on par in terms of aims, content, and expectations for accepted submissions, I cannot recommend accepting this paper in its current form to *CONF*.	decision
2021-120	For the previous submission, I believe the following points were the most important issues, as raised in the reviews: Most importantly, the benefits of the proposed method over existing ones (ie supervised from regular solver) are not made clear enough.	weakness
2021-120	Then, there is the insufficient quantitative evaluation, that the stream function formulation does not properly extend to 3D, and that the direction was not fully clear (performance vs control applications).	weakness
2021-120	I'm aware that it must be frustrating for authors to be rejected for very similar reasons, but it is likewise not nice for reviewers to repeatedly look at papers searching for differences, and discovering that authors have not taken comments from previous submissions into account.	weakness
2021-120	I would be curious to hear in the rebuttal how or whether the authors have updated their submission after the NeurIPS reviews.	suggestion
2021-120	As already mentioned in the previous cycle: there is merit to the direction of the paper, but I think it is important to make a clear step forward for one or more of the issues raised with the submission (as outlined in the previous paragraph).	suggestion
2021-120	With such extensions, there should be the potential for an improved evaluation.	suggestion
2021-120	However, as it stands I don't think this submission is suitable for *CONF*.	decision
2021-120	A minor note, but the title seems somewhat inapproprate to me - physics-informed typically implies that derivatives are computed via autodiff from a network representing the solution.	weakness
2021-120	Here, the authors instead discretize the solution on a grid, while a network yields the solution for the next step, and the physical model is evaluated with FDs on the grid.	weakness
2021-120	"Physics-constrained" (as in the section title) would be a better choice for the title.	weakness
2021-120	The paper is generally a good contribution especially in the field of machine learning for physics.	strength
2021-120	However, I do have some questions about the novelty of this work and what makes it different from other physics informed approaches.	weakness
2021-120	Below are the pros and cons and the I list out a set of comments and additional results that I believe would make the contribution of the paper much more clear.	weakness
2021-120	Pros: -- A physics informed neural architecture that optimizes the governing PDEs to perform forward integration of the system.	strength
2021-120	Generally a great approach since it does not require high-res numerical simulations	strength
2021-120	-- Generalizes to new geometries	strength
2021-120	Cons -- I might be missing something, but I generally feel that there is a lack of novelty.	weakness
2021-120	Approach is very similar to certain papers I point out in the comments.	weakness
2021-120	That however, does not take anything away from the approach proposed and it is still a good contribution once certain concerns (in the comments) are resolved	weakness
2021-120	-- The authors claim that their network generalizes but I do not see an explanation for it.	weakness
2021-120	Even an intuitive explanation would be good.	weakness
2021-120	The introduction section of the paper does talk about different approaches to physics informed neural networks especially the Raissi et al., papers and mention that Raissi's approach (https://arxiv.org/pdf/1711.10566.pdf and https://arxiv.org/abs/1711.10561) does not generalize to new domains.	weakness
2021-120	Can the authors explain why they believe that their network would automatically generalize to new geometries?	weakness
2021-120	Does the introduction of Ω and ∂Ω in their input features make their framework generalizable?	weakness
2021-120	The use of vector potentials to ensure divergence free by construction inside neural architectures using fixed convolution operations has been shown before By Mohan et al., (https://arxiv.org/pdf/2002.00021.pdf).	weakness
2021-120	The authors should cite this paper.	weakness
2021-120	In Figure 2, I'm concerned about how well the network satisfies BCs. If possible, I would like to see a plot showing velocity with time during inference near the edges of the obstacle.	weakness
2021-120	There is a complete absence of baselines.	weakness
2021-120	Sure, the a-net outperforms v-net when it comes to the loss.	weakness
2021-120	However, the loss term is 10−3 for v-net and 10−5 for a-net.	weakness
2021-120	Both are pretty low. How much improvement in inference would one expect?	weakness
2021-120	It is evident that constraining divergence by construction definitely helps in reducing the loss and this has been shown in https://arxiv.org/pdf/2002.00021.pdf.	weakness
2021-120	I would like to see a comparison between the a-net prediction and a numerical simulation and possibly a comparison with general PINNs (https://arxiv.org/pdf/1711.10566.pdf)  for the rectangular obstacle.	suggestion
2021-120	It should clearly show that their approach is more accurate in terms of RMSE error between a-net and numerical simulation as compared to PINN and numerical simulation.	suggestion
2021-120	What would be more interesting to see is how well the authors satisfy BCs as compared to PINN.	suggestion
2021-120	Moreover according to the authors PINNs would not generalize to the airfoil shaped obstacle while their network would .	suggestion
2021-120	This would clearly show an advantage of the proposed mechanism.	weakness
2021-120	Without a baseline however, there is not much to prove that this is a better approach.	weakness
2021-120	The problem solved in this paper is a very simple problem.	strength
2021-120	Even numerical solvers are not expensive when it comes to solving this problem.	strength
2021-120	This physics informed approach is very promising because it can reduce the computational cost immensely (during inference).	strength
2021-120	Would it be possible for the authors to show this for a real turbulent flow, e.g. the system considered in the Mohan et al., paper (https://arxiv.org/pdf/2002.00021.pdf) or the Kolmogorov system as shown in this paper (https://advances.sciencemag.org/content/advances/3/9/e1701533.full.pdf) or even the 2D Kraichnan system shown in https://arxiv.org/abs/1808.02983.	suggestion
2021-120	Unless these approaches generalize to fully turbulent flow, the use for such architectures are limited for real applications	suggestion
2021-120	This is still an important contribution in the field of deep learning for computational physics, specifically in fluid dynamics.	strength

2021-138	########################################################################## Summary: This paper investigates data augmentation in the context of RL and proposes a novel augmentation algorithm to enabling robust learning directly from pixels without the need for auxiliary losses or pre-training.	abstract
2021-138	The authors propose to average both the Q function and its target over multiple image transformations.	abstract
2021-138	The experiments on DeepMind control suite and Atari 100k benchmark show that their method outperforms previous model-free, model-based and contrastive learning approaches.	abstract
2021-138	########################################################################## Pros: This paper tackles a valuable problem of improving RL by data augmentation.	strength
2021-138	It will have a broad impact on the area of both representation learning and reinforcement learning.	strength
2021-138	The idea of averaging both the Q function and its target over multiple image transformations is interesting and promising.	strength
2021-138	This approach is easy to use and can be combined with any model-free RL algorithm.	strength
2021-138	The paper is well written and the results section is well structured.	strength
2021-138	They outperform baseline methods on two popular benchmarks and conduct ablation studies to verify the contribution of each component.	strength
2021-138	########################################################################## Cons: The proposed idea is very similar to RAD, a concurrent work by Laskin et al. The performance is also similar between these two approaches.	weakness
2021-138	More discussions and comparisons will help the readers better understand the difference.	suggestion
2021-138	They claim data augmentation is all you need.	suggestion
2021-138	To support this strong claim.	suggestion
2021-138	I think the authors should conduct more experiments on more base algorithms.	suggestion
2021-138	Can we get the same conclusion if we add the data augmentation techniques to other model-free RL algorithms (e.g., PPO or TD3) or a model-based RL algorithm (e.g., PlaNet or Dreamer)?	suggestion
2021-138	########################################################################## Taken both pros and cons in to consideration, I vote for an acceptance because of the novelty of the proposed idea and large-scale comparisons to previous model-free, model-based and contrastive approaches.	decision
2021-138	However, the experiments in the paper are not sufficient to support their claims "data augmentation is all you need" and I do suggest the authors to include more experiments to make it clear.	weakness
2021-138	Summary: To enable robust policy learning with image observations, the paper proposes a simple data augmentation technique that can be used with existing model-free reinforcement learning algorithms.	abstract
2021-138	It defines a notion of optimality invariant state transformation which preserves the Q function.	abstract
2021-138	An example of such transformations can be random image translations.	abstract
2021-138	It uses these transformations to (i) transform the input images, (ii) average the target Q values, and (iii) average the Q function themselves.	abstract
2021-138	Using this simple technique, they are able to get SOTA on DM control tasks and Atari 100k benchmark.	abstract
2021-138	On DM control tasks, it's able to outperform SAC trained on state representations.	abstract
2021-138	Additionally, the paper provides ablation studies on different image transformations and robustness analysis with respect to hyperparameter settings.	abstract
2021-138	Novelty: While data augmentation techniques are common in computer vision, this was the first work (concurrently with RAD) to apply the technique in the context of reinforcement learning.	abstract
2021-138	Reasons for score: Overall, I vote for accepting.	decision
2021-138	The reasons are as follows.	misc
2021-138	(i) It's a simple technique which can be used with any RL algorithm to improve the performance of the algorithm (ii) Good and detailed evaluation  (iii) additional ablation studies and robustness analysis present	strength
2021-138	Questions: Will using different random image transformations in sequence help?	weakness
2021-138	(My hunch is it won't as shown in RAD but still wanted your view given this method also does the average of Q function and target Q values) This paper tackle the effectiveness of data augmentation in reinforcement learning.	abstract
2021-138	Authors have introduced a regularization technique, based on image shifts and Q-function augmentation,(DrQ) that significantly improves the performance of model-free RL algorithms trained directly from images.	abstract
2021-138	Here are two positive points in the paper: The authors have combined a model-free RL and regularized method, which shows a promising direction with a training strategy with augmented data.	strength
2021-138	The authors have solid improve in continuous control by image pixel.	strength
2021-138	The negative point is that their DrQ method is very rough and very intuitive, authors can tackle more on the transformations of the images.	weakness
2021-138	Generally the paper is acceptable and they have shown a promising direction.	strength
2021-138	PROS Finding that image augmentation helps in learning a good policy is indeed a substantial contributions.	strength
2021-138	Other two aspects to regularize Q-values are also helping to learn a better policy.	strength
2021-138	CONS The paper presents many experiments but there are a few crucial ones which are missing.	weakness
2021-138	For example, what is the impact on training time of DrQ as compared conventional DQN?	weakness
2021-138	How much augmentation is good?	weakness
2021-138	etc. Question that needs justification: Overall, the paper is well written and explain various things but the algo-1 needs explanation of all parameters.	weakness
2021-138	For example, \\math{D} (reader has to think that it's a replay buffer), \\math{u(T)} is not at all clear.	weakness
2021-138	Many notation are not explained even in text and needs a clear explanation for reader from RL and non-RL domain.	weakness
2021-138	The primary claim of the paper is that image augmentation improves the performance.	weakness
2021-138	Sec5.1 shows significant improvement when image augmentation is used with different methods but it is very strange to see the improvement is just by adding 4 pixels on image boundary.	weakness
2021-138	Is there implication when we do more augmentation by increasing the size of random shift?	weakness
2021-138	How does the augmented image compare visually to the original image?	weakness
2021-138	The figure-6 in appendix shows all the results but which one is the random shift?	weakness
2021-138	In figure4, the SAC state is significantly better than the DrQ and no explanation is provided.	weakness
2021-138	Why one should not used SAC compared to DrQ?	weakness
2021-138	Paper claims that the proposed method can work with any model-free RL algorithm.	weakness
2021-138	Any justification or experiments to support the claim?	weakness
2021-138	If not, the contribution needs a re-writing.	weakness

2021-146	SUMMARY: This work describes a new algorithm, SPRT-TANDEM, for classifying sequential data as early as possible.	abstract
2021-146	It builds upon several previous works (SPRT, KLIEP, and deep neural networks) to propose a well-engineered and well-motivated solution for the considered problem.	abstract
2021-146	STRENGTHS: This work is exceptionally documented, on all accounts: related work is multidisciplinary, broad and thorough; all losses and algorithms are derived from first principles; experiments are varied and include every last details regarding their setups, evaluation metrics or outcomes.	strength
2021-146	Albeit only briefly mentioned in the main, the method has strong theoretical foundations, as documented in Appendix A.	strength
2021-146	Experiments show stronger benchmark results than the considered baselines (LSTM-m/s, EARLIEST and 3DResNet), on a quite diverse set of experiments (images, videos).	strength
2021-146	Although the classification of sequential data is not among the most popular topics, I believe this contribution to be significant for the field.	strength
2021-146	WEAKNESSES: I believe the 8-page limit of *CONF* does make this work justice, given the extensive documentation that comes along in the supplementary materials.	strength
2021-146	The short format of the main paper makes it difficult at places to fully follow or appreciate the contributions presented in this work.	weakness
2021-146	(Should this paper be rejected, I would recommend it to be submitted to JMLR, which format is certainly a better fit.) [EDITED AFTER DISCUSSION: My concerns are largely addressed and the paper is now stronger.	decision
2021-146	I very much hope to see it at the conference, and have updated my review and rating accordingly.]	decision
2021-146	The paper proposes a new algorithm for early classification of sequential data, exploiting approaches to density ratio estimation to enable applying a sequential probability ratio test-type algorithm on perceptually rich data with no explicit likelihood.	abstract
2021-146	The algorithm is trained using a novel density ratio estimation loss alongside more standard cross-entropy, and shows strong performance on a number of provided benchmarks, including on a new variant of sequential MNIST.	abstract
2021-146	The paper claims ability to control speed-accuracy tradeoff in early classification, and applying the Wald SPRT on arbitrary sequential data.	abstract
2021-146	I enjoyed reading this paper: it combines two ideas (sequential likelihood ratio testing and density ratio estimation) in a way that appears obvious after the fact, but as often with these sort of post-hoc obvious ideas, is novel to my knowledge, and elegant.	strength
2021-146	Trying to get something like the SPRT working beyond pairs of simple hypotheses has been under investigation for over half a century, and this paper is a worthwhile attempt.	strength
2021-146	The empirical results look pretty good as well.	strength
2021-146	At the same time, I think the paper does overstate the benefit of the theoretical connection to the classical SPRT and previous non-i.i.d. extensions, and doesn't engage sufficiently with the challenges of stopping rules.	weakness
2021-146	I discuss these issues in more detail next, and conclude with some minor points about presentation, background work, and analysis.	weakness
2021-146	Connections to the classical SPRT	weakness
2021-146	The paper claims to extend Wald's SPRT to non-i.i.d. data, and in particular its optimality properties.	abstract
2021-146	This includes claims in section 1 (on approaching Bayes-optimality and extending the Wald SPRT to arbitrary sequential data), at the beginning of section 4 (on how the LLLR enables performing the SPRT, which is provably optimal), and in section 5 (on approaching if not reaching Bayes-optimality).	weakness
2021-146	As far as I can tell, the paper does not enable performing the Wald SPRT on arbitrary data, and does not provide a provably optimal sequential test.	weakness
2021-146	The claim that it approaches optimality in any formal sense is likewise not supported as far as I can tell.	weakness
2021-146	Broadly, a sequential test consists of an update rule and a decision rule -- for the SPRT the update rule and decision rule are both optimal.	weakness
2021-146	For most extensions (non-i.i.d., multi-hypothesis, deadlines, etc), the update still follows Bayes' rule, and the optimal decision rule can only be found numerically (if at all), so some heuristic is given.	weakness
2021-146	This heuristic is often a fixed threshold, with asymptotic optimality guarantees).	weakness
2021-146	SPRT-TANDEM seems to be in the family of such extensions: it still applies Bayes' rule sequentially, and the stopping is given based on a fixed threshold.	weakness
2021-146	Thus, its optimality is asymptotic at best, and stronger claims are not supported.	weakness
2021-146	Furthermore, the paper doesn't make it clear whether the standard conditions for asymptotic optimality apply to the SPRT-TANDEM either: in my rough understanding, the standard asymptotic result is as risk goes to 0 (or equivalently, the LLR goes to infinity, and the threshold goes to infinity).	weakness
2021-146	I'm not sure that we know the SPRT-TANDEM LLR to grow in this way, and empirically, it seems like the LLR saturates to some fixed value, especially with high-order N, which means high threshold values are not achievable and risk cannot go to 0.	weakness
2021-146	We should also expect the SPRT-TANDEM to depart further from optimality as the model approaches the end of the video (since the optimal thing to do there is to gradually collapse the decision boundary, as the appendix reminds us).	weakness
2021-146	I recommend moderating these claims regarding optimality, and / or strengthening the results if possible.	suggestion
2021-146	Stopping rules The paper does not provide guidance on stopping rules, which limits practical use, and does not report on the thresholds used to generate the speed-accuracy tradeoff figures.	weakness
2021-146	Presumably, the simplest thing is to set the threshold to the desired accuracy (which I think will do the right thing in the no-overshoot case?).	suggestion
2021-146	Does this work for SPRT-TANDEM to achieve a given accuracy?	suggestion
2021-146	If not, is there another heuristic that applies?	suggestion
2021-146	I recommend addressing this question in more detail.	suggestion
2021-146	Relatedly, the paper criticizes Mori et al. 2018 and Hartvigsen et al. 2020 for using a separate objective for determining stopping and accuracy, but in fact SPRT-TANDEM would likewise need some dynamic programming or RL solver to have an optimal stopping policy, similarly to that prior work.	weakness
2021-146	I recommend providing explicit guidance about stopping rules, and moderating the claims relative to prior work.	suggestion
2021-146	Solving for an optimal stopping policy would also strengthen the paper.	suggestion
2021-146	Presentation issues The paper tries to cram a lot into the short *CONF* format, supported by an extensive appendix.	weakness
2021-146	I appreciate the inclusion of classical SPRT results in the appendix, which may be unfamiliar to the *CONF* audience.	strength
2021-146	At the same time, the main text does not provide much intuition about the novel LLLR loss, which is given very little explanation considering it is presented as one of the paper's major contributions.	weakness
2021-146	The relationships and improvements relative to KLIEP are presented too tersely, and a reader not familiar with that precise method will not know what to make of them.	weakness
2021-146	The paper would do better to provide more exposition there, perhaps in favor of moving the results tables to the appendix (since they show the same information as figure 3 as far as I can tell).In addition, the SAT curves are too busy, small, and hard to read.	weakness
2021-146	For the main document, I would recommend increasing font and symbol sizes, and presenting fewer orders of SPRT-TANDEM models (e.g. just order-1 and best-order), and fewer hitting times per model (e.g. there is no need to present the accuracy at every one of the last 10 frames if the accuracies are all the same there).	suggestion
2021-146	Finally, the LLR trajectory figures can use partial transparency to make the individual traces easier to see.	suggestion
2021-146	Additional/minor points Background work. I appreciated the fairly detailed review of past work related to the SPRT.	strength
2021-146	A few notable missing pieces related to neuroscience are work predating Kira et al. 2015 in applying the SPRT to neural data (e.g. Gold & Shadlen 2002) and to human decision making more broadly (e.g. Stone, 1960; Edwards, 1965, Ashby 1983, and others).	weakness
2021-146	Missing work related to practical applications of the SPRT includes Johari et al. 2017, Ju et al. 2019, and others from the domain of internet experimentation.	weakness
2021-146	None of these are critical omissions, I only bring them up considering the already-broad review.	misc
2021-146	Statistical analysis. Given that the paper has a clear hypothesis (that SPRT-TANDEM outperforms competitors), it seems more sensible to perform repeated measures regression with planned contrasts rather than post-hoc testing for significance.	weakness
2021-146	This is a minor issue.	misc
2021-146	The authors propose how to use the neural networks to estimate the posteriors for each label y for the log likelihood ratio (LLR) estimation.	weakness
2021-146	The LLR estimation is performed using the multivariate inputs from the windows of time series, and it is used for the SPRT criterion without conventional iid assumption.	suggestion
2021-146	The paper contains an interesting idea of using the neural networks for the prediction of likelihoods and accumulating the information for the conventional sequantial probability ratio test (SPRT), which is well known for explaining the speed-accuracy tradeoff for decision making.	abstract
2021-146	The experimental results using various datasets show the relevance of the algorithm in terms of improving the speed-accuaracy tradeoff.	abstract
2021-146	The authors presented a reasonable combination of two different objective functions: LLLR and L_multiplet.	abstract
2021-146	Though the authors did not mention explicitly, one is the objective for LLR which is ill-posed because pairs of high-biased neural network outputs can result in a small LLR by preserving only the ratio of the outputs correct.	weakness
2021-146	The other is the posterior objective (L_multiplet) which will alleviate the ill-posedness of the first objective by making the outputs of neural networks as close as possible to the correct one though they do not use those posteriors once it can estimate the LLR correctly.	abstract
2021-146	My question about this paper is the learning procedure.	weakness
2021-146	For LLLR training, there is no need for the sync of x^(t) for different ys.	weakness
2021-146	I don't see the explanation about choosing the index t in LLLR.	weakness
2021-146	In the paragraph below Eq. (6), I don't understand what it means by the KL divergence between two ratios.	weakness
2021-146	The paper proposes a novel SPRT-TANDEM algorithm minimizing the divergence between estimated and true Log-Likelihood Ratios of SPRT and making it thereby Bayes optimal for various real-world applications.	abstract
2021-146	The paper is very well written, clear and scientifically sound and provides  extensive contributions, e.g. a database in addition to the algorithm.	strength
2021-146	Performance of the algorithm is demonstrated via three experiments.	strength
2021-146	Previous research is given sufficient credit.	strength
2021-146	The only thing I would still like to see more is the discussion at the conclusions.	weakness
2021-146	Why does this seemingly simple modification to the existing SPRT method provide so superior performance.	weakness
2021-146	The appendices are referred a lot in the text but they are missing from the paper?	weakness
2021-146	A very minor comment: The following sentence is a bit vaguely written: Long short-term memory (LSTM)-s/LSTM-m impose monotonicity on classification ...	weakness
2021-146	I guess it should be : Long short-term memory (LSTM) variants LSTM-S and LSTM-M impose monotonicity on classification ...	weakness
2021-146	Summary This work introduces SPRT-TANDEM an algorithm to train a sequential probability ratio test (SPRT) as a neural network.	abstract
2021-146	This network is then used to discriminate between two hypotheses as fast as possible (seeing the smallest number of observations in a sequence) while maintaining a certain level of accuracy.	abstract
2021-146	The main contribution of this work is to enable Wald's SPRT without actual knowledge of the ratio, learning a neural network to model it.	abstract
2021-146	Major comments Pros: The paper does a good job of introducing the problem statement, that is the "fast" classification of sequential data.	strength
2021-146	The algorithm introduced is well motivated and bridges the gap between "classic statistical" methods and machine learning approaches for sample-efficient time series classification.	strength
2021-146	The experimental, results though not outstanding, show that SPRT-TANDEM outperforms other deep learning methods.	strength
2021-146	These experiments are insightful by the fact that they compare the performance of the different methods for different mean hitting time.	strength
2021-146	Overall the paper is pleasant to read and introduce a new method that could be helpful for some practitioners.	strength
2021-146	Cons: The related work is quite superficial (even taking into account app B).	weakness
2021-146	In particular, I would have liked a deeper comparison with LSTM-s/m and EARLIEST, discussing the drawback/advantages of these methods with respect to SPRT-TANDEM.	suggestion
2021-146	In the proof of 4, just before eq 70 you say: "Let us assume that the process {x(s)}ts=1 is i.i.d., namely -> eq 70".	suggestion
2021-146	This seems wrong to me.	weakness
2021-146	The assumption you're making there is that the process has independent component conditionally to the class y (e.g for t = 2, the Bayesian network: x_1 <- y -> x_2).	weakness
2021-146	This is still a reasonable hypothesis however this is not equivalent to simply assuming the process is iid (which then would mean for t = 2, the Bayesian network: x_1 -> y <- x_2 and would not make eq 70 correct).	weakness
2021-146	The three tasks on which you test the models seem to be quite well solved after a few samples on average for all models.	weakness
2021-146	I think it would be worth testing the models on tasks that require more samples for reaching good performance and maybe where the temporality required (hyperparameter N) is larger.	suggestion
2021-146	It is not very clear to me what are the respective roles of LLLR and MCEL.	weakness
2021-146	I do not understand why both are useful, I would have thought that CE in itself would be sufficient.	weakness
2021-146	The ablation study you did is interesting but I would have liked to get further insights about what is happening there.	suggestion
2021-146	I believe that testing your method on a toy problem for which the correct ratio is known would be insightful about the "optimality" of your method.	suggestion
2021-146	Minor comments Page 2 "As an ..	weakness
2021-146	orDEr" Loss written with capital everywhere.	weakness
2021-146	How to choose N: You say that training the features extractor is faster than the integrator's however it is not clear to me how you can train these two parts independently from each other.	weakness
2021-146	For comparison on NMNIST it could be interesting to see the performance of a simple classifier on the 19th image alone.	weakness
2021-146	You're talking about Optuna for hyperparameter optimization, this is unknown to me.	weakness
2021-146	A word about how it is working could be nice.	weakness
2021-146	Why are the number of trials different?	weakness
2021-146	The purpose of SPRT-TANDEM is to be as fast as possible, it could be interesting to clearly state somewhere how comparable are the different methods in term of computing times even if they are very close to each other.	suggestion

2021-155	The paper proposes Fast Geometric Projections (FGP), a method to certify the robustness of neural networks with ReLU as activation exploiting the fact the such networks are piecewise affine functions.	abstract
2021-155	FGP can verify whether in an ℓp-ball around an input x adversarial examples exist or not.	abstract
2021-155	Since it's not guaranteed to find a conclusive solution, it has the option of returning "unknown" if the given point could not be certified.	abstract
2021-155	FGP aims at computationally efficiency at the price of incompleteness.	abstract
2021-155	Finally, it can provide lower bounds on the norm of the smallest adversarial perturbation.	abstract
2021-155	Pros Verifying efficiently the robustness of (large) neural networks is an interesting task and most of the research has primarily focused on the ℓ∞-threat model.	strength
2021-155	A fast methods which is effective in particular wrt the ℓ2-norm is valuable.	strength
2021-155	The proposed methods is geometrically justified and benefits from many heuristics to optimize efficiency.	strength
2021-155	In the experimental evaluation, for the ℓ2-threat model, FGP outperforms the methods for exact verification (GeoCert, MIP) with a timeout in terms of both verified robust accuracy and runtime (with a large margin).	strength
2021-155	Moreover, it scales better than other methods to larger and deeper architectures when tested  on models trained for verifiability (MMR, RS).	strength
2021-155	When comparing lower bounds on the size of the minimal adversarial perturbations, it provides better bounds than FastLin, although with higher computational cost.	strength
2021-155	Cons To achieve good provable robustness, it is necessary to train specifically for it and then usually the best bounds are given by the same technique deployed at training time (e.g. IBP training).	weakness
2021-155	An evaluation of FGP in such scenario seems to me particularly meaningful (and at the moment missing).	weakness
2021-155	If the proposed FGP outperformed the bounds given by the technique of, for example, (Wong & Kolter, 2018) on a model trained as in (Wong & Kolter, 2018), this would strengthen the proposed method.	suggestion
2021-155	The ℓ∞ version doesn't seem as competitive the ℓ2 one.	weakness
2021-155	Moreover, in that case ϵ=0.01 is used, which is very small for MNIST or F-MNIST (e.g. Zhang et al. (2019) have guaranteed up to ϵ=0.4).	weakness
2021-155	It seems that CROWN (Zhang et al., 2018) slightly improves upon FastLin, so it might make sense to include it in the comparison.	weakness
2021-155	Other comments Only one value for ϵ is tested for each dataset.	weakness
2021-155	It would be interesting to see how FGP scales to larger values.	suggestion
2021-155	The clean accuracy of the models used in the evaluation should be reported (it would be helpful to know whether they achieve reasonable performance).	suggestion
2021-155	Also, for completeness, using the different types of models (naturally trained, MMR and RS) for all the datasets (and also for the comparison to FastLin) would be good.	suggestion
2021-155	Overall, the method is clearly presented and achieves good empirical results.	strength
2021-155	As mentioned above, further experiments could show its effectiveness and applicability on models with state-of-the-art provable robustness.	suggestion
2021-155	Zhang et al., "Towards Stable and Efficient Training of Verifiably Robust Neural Networks", 2019	misc
2021-155	Update after rebuttal I thank the authors for their response.	misc
2021-155	After reading it, the revised version and the other reviews, I think that there's evidence of the effectiveness of the proposed method.	rebuttal_process
2021-155	As I tried to convey in the initial review, I consider the main weakness of the paper not showing the performance on models achieving state-of-the-art verified robustness.	rebuttal_process
2021-155	I think the case of ℓ∞ is emblematic, where ϵ=0.01 is used.	rebuttal_process
2021-155	Since there exist models achieving provable robustness >90% for ϵ=0.3 (on MNIST), the scenario considered in the evaluation doesn't seem interesting, regardless of how FGP compares to the other methods.	weakness
2021-155	While I agree with the authors that training for provable guarantees sacrifices clean accuracy in most of the cases, as far as I know that is currently the way to achieve good VRA at meaningful thresholds.	weakness
2021-155	Along this line, also the authors used FGP on larger models when those were trained with MMR or RS.	weakness
2021-155	In my opinion the most interesting application of a (incomplete) verification method like FGP is to provide better VRA than current methods for training (IBP, CROWN-IBP, Wong & Kolter's method), for example using less heavily regularized models which can retain higher clean accuracy.	strength
2021-155	I think this is the missing piece in the current version of the paper.	strength
2021-155	For these reasons, I keep my initial score.	strength
2021-155	Summary: This paper proposes an algorithm to verify whether or not there exists an adversarial example in an Lp ball of size espilon around a given training sample.	abstract
2021-155	As opposed to the more common used bound propagation method (Crown, fastlin, IBP...), it does not do so by overapproximating bounds on the output, but instead by exploring neighbouring activation patterns to the one where the sample to verify lie, in a form of exhaustive search.	abstract
2021-155	As opposed to previous similar algorithm (Geocert), it sacrifices completeness in order to gain efficiency.	abstract
2021-155	In addition, the authors explain how to take advantage of the Neural network structure to make the algorithm more efficient.	abstract
2021-155	Experimental results are very convincing, showing strong improvements in term of runtimes of the method with the proportions of "unknown" sample remaining moderate.	strength
2021-155	Main comment: It would be beneficial for the authors to make explicit the difference between the Fast Geometric Projections algorithm and the GeoCert algorithm.	suggestion
2021-155	As it is, it seems obvious to me that the two algorithms are clearly related but it would be beneficial to make the difference more apparent.	suggestion
2021-155	There is a short mention in the Related Work section but I found it pretty lacking.	weakness
2021-155	Why does GeoCert need to rely on projection to polytope solved by quadratic program and FGP does not?	suggestion
2021-155	As it is, my understanding of the difference is that GeoCert, when computing the projection, computes it only to the actual existing part of the decision boundary/activation constraint, rather than to its infinite prolongation as is done by FGP.	weakness
2021-155	Is that correct? Is there a intuition or a result that could be given of when GeoCert and FGP are going to return the same result?	weakness
2021-155	Or equivalently, a characterization of FGP incompleteness?	suggestion
2021-155	Is it something like "There exists an activation pattern A intersecting with the epsilon ball, with the decision boundary Ca going through it, but the projection of the center point onto the (extended to an infinite line) decision boundary Ca does not belong to A"	weakness
2021-155	I think that this would make the paper significantly stronger, if things were formulated in the way of "We perform this change, which leads to the loss in completeness due to not handling those cases well, but in exchange the computation that needs to be done becomes much simpler leading to X orders of magnitude improvement.	suggestion
2021-155	", rather than "here is an algorithm that works X order of magnitude better".	suggestion
2021-155	The first allows the reader to get an insight into the trade-off and the structure of the problem.	suggestion
2021-155	Smaller comment: It would be nice to have an ablation study / experimental evaluation of the benefits brought by 2.2 and taking advantage of the network structure if it's an experiment that could easily be run.	suggestion
2021-155	Experiment Section comments: I'm unclear as to what the upper bound on the verified robust accuracy tells.	weakness
2021-155	For FGP on cifar-CNN, why isn't the robust accuracy 86%?	weakness
2021-155	It seems like all samples have been successfully handled and 86 out fof the 100 have been found robust?	weakness
2021-155	Fastlin has mostly been superseded by Crown, which usually give tighter bounds.	weakness
2021-155	I assume that the tightness comparison would still go the same way but it would be a more appropriate baseline.	weakness
2021-155	It would also be good to provide the runtime for Fastlin/Crown for the benefit of the reader.	weakness
2021-155	FGP takes 60s on those networks , but I doubt that Fastlin takes more than 1s.	weakness
2021-155	There is even a lot of space on the side of Table 1.b to add a runtime column!	weakness
2021-155	Minor Notes: In 2.1, "if A0 contains a decision boundary" -> "contains a decision boundary C", maybe?	weakness
2021-155	C doesn't seem to be introduced anywhere (there is C_u for activation constraints).	weakness
2021-155	In 2.1,  at the end, it is not clear if the enqueued neighbouring activation regions will also potentially enqueue their neighbouring regions?	weakness
2021-155	I think that I figured out that it is based on the rest of the paper but it would be nice to make it more explicit.	weakness
2021-155	In 2.2, it would be nice, even if the proof is not there to have the intuitions behind it be in the main paper.	weakness
2021-155	The appendix sentence "To prove the correctness of our treebased exploration, we only need to prove that the additional regions we filter out either are unreachable, or are explored through a different path." is perfectly sufficient to convince the reader of the idea.	weakness
2021-155	Recommended Related works: The authors may potentially add a reference to "Measuring Neural Net Robustness with Constraints", by Bastani et al., NeurIPS 2016,   for context, although the method in that paper was only looking in a single linear piece of the network.	suggestion
2021-155	Not really necessary though. Opinion: The algorithm is great and its presentation from a point of view of "what is done" is quite clear but the paper would in my opinion be much better if it contextualize its contribution better in comparison to other algorithms.	weakness
2021-155	I would be happy to raise my rating if this was the case.	suggestion
2021-155	########################################### Update after discussion with the authors and reading the updated version: The authors have clarified the points that were unclear.	rebuttal_process
2021-155	I'm happy to raise my score.	rebuttal_process
2021-155	The authors propose a systematic search over the convex polyhedral regions on which the network is linear, to find the decision boundary, so to certify local ℓp robustness.	rebuttal_process
2021-155	The method is fairly general and evaluated for p=2 and p=∞.	strength
2021-155	A key point that makes the method feasible is leveraging of the compositional structure.	strength
2021-155	The proposed verification method is incomplete and returns, given an input either one of 2 certificates (robust or not_robust) or abstains from certification.	strength
2021-155	In the case of ℓ2 robustness, significant speed ups are gained compared to prior work.	rebuttal_process
2021-155	Certifying local ℓp robustness is in general an important problem.	abstract
2021-155	The scalability to large networks seems to be an issue, although the proposed method significantly outperforms prior work.	weakness
2021-155	Suggestions: A potential improvement for Section 2 would be to include a simple running example, that showcases the algorithm step by step.	suggestion
2021-155	Ideally, the same example would then also be used for Figure 2.	suggestion
2021-155	Questions: What would it take to make the proposed method complete?	suggestion
2021-155	What would the complexity / runtime be?	suggestion
2021-155	How large can the networks be grown for a more generous time budget (i.e. 5 min or 10 min instead of 2 min) so that the proposed method still performs well?	suggestion
2021-155	Given the generality of the approach and the speed up gained compared to prior work, i give this paper an accept.	decision
2021-155	=== I thank the authors for providing the answers to my questions.	misc
2021-155	I will not change my score.	misc
2021-155	Title before Rebuttal: Novel idea for accelerating verification hampered by concerns on correctness of algorithm	weakness
2021-155	Summary of Contributions The paper proposes a method to verify local robustness networks with piecewise-linear activation functions, providing tighter bounds than existing approximate verification approaches while scaling to deeper networks than existing complete verifiers.	abstract
2021-155	Furthermore, unlike other approximate verification techniques, the method is able to show the existence of adversarial examples (in some cases).	abstract
2021-155	The method works by exhaustively checking each of the activation regions (that is, the convex polytopes on which the network is linear) that are fully or partially within the ε-ball for the presence of a decision boundary.	abstract
2021-155	If there exists a region containing a decision boundary for which the projection p of x to the decision boundary is within the ε-ball: An adversarial example exists if p is in the activation region	abstract
2021-155	The robustness status is unknown if p lies outside of the activation region	abstract
2021-155	Otherwise, if no such region exists, the sample is robust to local perturbations.	abstract
2021-155	Score Recommendation The method in this paper is novel and the results presented are compelling.	strength
2021-155	However, it is currently marginally below the acceptance threshold (5) for me, as I am concerned about the correctness of the optimized tree-based exploration variant of FGP presented.	decision
2021-155	Notwithstanding the other issues identified, if my concerns around the correctness are addressed and the presented results stand, this paper would be among the top 50% of accepted papers (8).	decision
2021-155	Otherwise, it would be a reject (3).	decision
2021-155	For the tree-based exploration approach to be correct, we need to prove that regions filtered out are unreachable or explored through a different path.	weakness
2021-155	The proof in A.2 needs to be expanded upon, and does not currently provide sufficient detail to demonstrate that this is the case.	weakness
2021-155	Specifically, consider the potential counterexample in the image linked: https://www.geogebra.org/geometry/ajzcrves.	suggestion
2021-155	We have four polytopes A_prev, A, A_next, A bounded by ECG, DCE, FCD, GCF respectively.	suggestion
2021-155	Every region that differs by one neuron is adjacent to each other.	suggestion
2021-155	A_next is discarded when exploring from A, it will not be reached by any other path since A' will not be explored from A_prev (the boundary GC does not pass inside the ε-ball).	suggestion
2021-155	I expect that some other property of the network means that this is not an actual counterexample, but do not see which specifically.	weakness
2021-155	Strengths Novelty: The paper presents a novel approach for verification that can be accelerated on GPUs.	strength
2021-155	While other work to accelerate verification on GPUs exist (e.g. predicting dual variables for optimization using a neural network or expressing SDP solver algorithms as passes through the network being verified), the method presented simply exhaustively searches (very efficiently) each activation region.	weakness
2021-155	Results: To the best of my knowledge, the method: Advances the state-of-the-art in certifying local stability over perturbations of bounded l2-norm, with significant improvements in runtime (1-2 orders of magnitude) over GeoCert.	strength
2021-155	Advances the state-of-the-art in providing tight certified lower bounds (up to 2 orders of magnitude) on l2-robustness radius	strength
2021-155	Overall Clarity: This paper was a pleasure to read, and I wanted to make special note of this.	strength
2021-155	I appreciated that thought had been put in in naming and defining terms (e.g. activation pattern, activation region), and found the diagrams in particular helped me understand the algorithm.	strength
2021-155	Results were organized clearly Reproducibility: The authors took pains to provide details that make their results more reproducible (architecture, training hyperparameters and method, time budget, computational setup for verification).	rebuttal_process
2021-155	Weaknesses (The relative length of this section should not be taken as an indictment of this paper.)	strength
2021-155	In addition to the issue with the optimized variant of FGP, here are some additional weaknesses that should be addressed.	suggestion
2021-155	Clarity of Algorithm Description With reference to Figure 2b and focusing on C1, the penultimate paragraph of page 3 says "For each constraint at distance less than ε from x, we enqueue the corresponding activation region if it has not been searched yet".	suggestion
2021-155	It would be helpful to emphasize in the paper that we are checking if the line C1 intersects the ε-ball, not just the line segment between A11 and A01.	suggestion
2021-155	I could not find an explanation in the paper for how the decision boundaries are computed; this seems core to the efficiency of the approach.	weakness
2021-155	As far as I can tell, the method described linearizes the network f about the activation region (obtaining f′), and computes the decision boundary in f′ (not f), and then checks the distance to x.	weakness
2021-155	If this is the case, it should be explicitly specified, and a concise proof that this is correct added.	weakness
2021-155	Generalization to Other Norms The time comparison ("up to 5x faster than ERAN on models trained with PGD, and one to two orders of magnitude faster than ERAN on models trained using MMR") is misleading without noting that the FGP returns "unknown" for a significant proportion of samples (6-15%) while ERAN+MIP returns a result in all cases.	weakness
2021-155	This should be noted in Section 3.3, rather than readers having to head to the appendix to determine this.	weakness
2021-155	While ERAN is a reasonably competitive verification method, I'd be interested to see a comparison with nnenum (CAV '20: https://github.com/stanleybak/nnenum) if possible, as I expect that nnenum may provide stronger results than ERAN.	suggestion
2021-155	I would also love to see a comparison with VeriNet (ECAI '20 but this is not necessary as ECAI '20 did occur after Aug 2, 2020.	suggestion
2021-155	Certified Lower Bounds Given that FastLin is designed to be quick but provide loose bounds, it would be reasonable to provide the mean runtime for FastLin (presumably ~1s) and FGP_LB (60s) in Table 1b, so that someone scanning the paper can understand that FGP provides better bounds at the cost of longer runtime)	suggestion
2021-155	CROWN provides better certified lower bounds than FastLin without a significantly higher cost to runtime (see, for example, Table 4 of the original paper).	suggestion
2021-155	Computing the mean bounds in Table 1b using CROWN (rather than FastLin) would be a better comparison of FGP to the state of the art.	suggestion
2021-155	Reproducibility The first paragraph of page 6 states that "measurements are obtained by evaluating on 100 arbitrary instances".	suggestion
2021-155	It would be helpful for reproducibility to provide the indexes of these instances.	suggestion
2021-155	It would be nice to have the relevant code released.	suggestion
2021-155	Questions for Authors With reference to Figure 1b: it seems possible to distinguish between the left and right cases by computing the projection of point p onto the ε-ball, p′, and checking whether p′ is within the activation region.	weakness
2021-155	This would allow the algorithm to avoid returning unknown in cases like the right.	weakness
2021-155	Is this incorrect, or was it merely challenging to implement?	weakness
2021-155	How did you handle the presence of multiple decision boundaries within a single activation region (e.g. one between category 1 and 2, and one between category 1 and 3) or does this provably not occur?	weakness
2021-155	Additional Feedback Figure 1 combines two related but different ideas (an illustration of the basic FGP algorithm, and an example of a case requiring FGP to return unknown when analyzing a boundary constraint) into a single figure.	weakness
2021-155	As a result, the figure was slightly confusing for me when I initially looked at it.	weakness
2021-155	I understand that this is probably done for space constraints, and the authors do attempt to use the spacing between the squares to distinguish between the two groups of squares, but perhaps a light vertical line (or some other visual aid) would help to distinguish more clearly.	suggestion
2021-155	(I have a similar recommendation for Figure 2 is even less clear).	suggestion
2021-155	The use of "enriched" in third paragraph of Page 5 is slightly confusing (is the queue somehow enriched, or is it the regions that are?).	weakness
2021-155	What about "Instead, it is sufficient to use a regular queue of regions, augmenting each region with a field storing the layer the last flipped neuron belongs to"?	weakness
2021-155	The paragraph on "depth scalability" discussing Figure 3 seems to emphasize the ratio of regions searched between FGP and GeoCert.	weakness
2021-155	Given this, showing the y-axis of regions searched on a log scale might be appropriate since it enables readers to do the comparison themselves.	weakness
2021-155	The layout of the paper means that the first introduction in text of verified robust accuracy occurs after its use in Table 1.	weakness
2021-155	Given that it looks like there is enough space in the caption, it might be worth it to spell out what VRA is in the caption.	weakness
2021-155	For reproducibility, it would be helpful to indicate which commit / release of the MIP code you used to obtain the results.	suggestion
2021-155	Post-Rebuttal Comments I've increased the rating for the paper from 5 -> 8 as the authors have addressed all my substantive concerns.	rebuttal_process
2021-155	Most critically, the authors demonstrated that, even without the tree-based exploration variant, FGP's performance improves significantly over the state of the art particularly for l2 networks.	abstract
2021-155	The authors have also significantly improved the clarity of the algorithm description, made the comparison in the section on generalization to other norms more clear, provided information in the paper allowing readers to understand that FastLin is significantly faster (but also significantly looser), and provided details that make reproducing these results far simpler.	rebuttal_process

2021-295	This paper studies the problem of representation learning for first-stage retrieval in text ranking/matching.	abstract
2021-295	Specifically, it investigates the role of negative instances and how to select them in the quality of representations.	abstract
2021-295	The paper is well-written and it does a fair job in motivating the problem and discussing related works.	strength
2021-295	The main pros are as follows: Authors attempt to theoretically study the importance of negative instances and their impact on the gradients of objective function for text retrieval.	strength
2021-295	Specifically, they posit that non-informative negative instances provide small gradients n the course of training, and thus adversely affect the convergence of gradient-based learnings.	strength
2021-295	Through various experiments, the authors show that by exploiting the top retrieved texts from the corpus, using the current learned representations, as 'hard' negative instances, the performance of text ranking methods can be significantly improved.	abstract
2021-295	There are however some ambiguities and room for improvement: As equation (10) suggests, the optimal distribution over negative instances is the one that minimizes the variance of gradients, or equivalently the norm of loss function.	weakness
2021-295	However, at many places it is pointed out that small losses prevent the model from learning and slow down the convergence rate.	weakness
2021-295	This is a kind of contradiction, and needs to be clarified,	weakness
2021-295	Existing results from literature such as equation (11) are leveraged to connect the norm of loss function and the gradient of last layer of representation learning models.	weakness
2021-295	However, the arguments here are not quite rigorous.	weakness
2021-295	It should be clarified that these results hold for what types of ranking objectives, and what other assumptions such as smoothness, etc.	weakness
2021-295	are required. The main bottleneck of using top retrieved documents as negative instances is the computational burden of updating the ANN indexing per batch.	weakness
2021-295	Therefore, the authors propose to perform this updating less frequently.	weakness
2021-295	It would be nice to have more comprehensive experiment to show that how sensitive is the performance vs the frequency of index updates.	suggestion
2021-295	########################################################################## Summary: This paper studies the problem of dense text retrieval, which represents texts as dense vectors for approximate nearest neighbors (ANN) search.	abstract
2021-295	Dense text retrieval has two phases.	abstract
2021-295	The first phase learns a representation model to project semantically similar texts to vectors of large similarity scores (e.g. inner products or cosine similarity scores).	abstract
2021-295	The second phase adopts an ANN search algorithm to index these vectors and process queries.	abstract
2021-295	The paper claims key contributions at the first phase.	abstract
2021-295	Specifically, (1) The paper introduces a better negative sampling method to sample good dissimilar text pairs for training.	abstract
2021-295	(2) The new method enables faster converge of model learning.	abstract
2021-295	(3) The new method leads to 100x faster efficiency than a BERT-based baseline, while achieving almost the same accuracy as the baseline.	abstract
2021-295	########################################################################## Reasons for score: Overall, I like the idea of this paper and opt for a weak accept.	decision
2021-295	A carefully designed negative sampling method should be able to outperform baselines that use simple heuristics.	weakness
2021-295	The efficiency improvement 100X is very promising.	strength
2021-295	However, the paper can be better in experimental comparison and presentation.	weakness
2021-295	For experimental comparison, a stronger baseline using dense vectors should be included to strengthen the performance claim.	weakness
2021-295	For presentation, many important terms require clear definitions, without which the performance gain is not understandable.	weakness
2021-295	It will be good if the authors can address the above two issues in the rebuttal.	suggestion
2021-295	########################################################################## Pros: The paper proposes a novel negative sampling method.	strength
2021-295	Based on the method, the paper proposes a new dense text retrieval framework ANCE.	abstract
2021-295	ANCE introduces an asynchronous index refresh to select the most dissimilar text pairs for training in a timely manner.	abstract
2021-295	The proposed ANCE achieves faster model training and equally accurate text retrieval when compared with a number of baselines.	abstract
2021-295	In a TREC 2019 task, ANCE achieves the best NDCG score against 11 baselines.	abstract
2021-295	The authors promise to make code open source.	rebuttal_process
2021-295	That will greatly improve the reproducibility of the work.	suggestion
2021-295	The code, together with its performance, will serve as a new state-of-the-art for future study.	rebuttal_process
2021-295	########################################################################## Cons: An important baseline is missing.	weakness
2021-295	In section 5, the paper describes Baselines.	abstract
2021-295	According to the descriptions, all baseline use BM25 to retrieve samples for training.	abstract
2021-295	BM25 may not be the best for a strong baseline since it relies on sparse word tokens.	weakness
2021-295	An alternative is to use BERT [CLS] dense vectors of all texts and a similarity search algorithm such as locality sensitive hashing as the retriever.	weakness
2021-295	It will be good if the authors can add the baseline to the paper.	suggestion
2021-295	The paper does not explain clearly why the proposed method runs faster than baselines.	weakness
2021-295	The experimental results support that the proposed method outperforms several baselines.	strength
2021-295	However, the paper does not explain the performance superiority.	weakness
2021-295	I am not sure about which of dissimilar text pairs selection or index refresher or others in the proposed negative sampling leads to the superiority.	weakness
2021-295	The reason for uncertainty may be due to the lacking of definitions in the paper.	weakness
2021-295	For example, "BERT rerank" refers to a baseline but it has not a definition in the paper.	weakness
2021-295	The input and output of "BERT rerank" remain unclear.	weakness
2021-295	Similarly, "TREC 2019" is an important benchmark but it has not definitions related to the inputs and outputs.	weakness
2021-295	It is necessary to explain important concepts for the best readability of the paper.	weakness
2021-295	########################################################################## Questions during rebuttal: I would like to see some experiments or discussions to clarify the above cons.	rebuttal_process
2021-295	The paper explores how to effectively do negative sampling for dense retrieval.	abstract
2021-295	The paper shows that negatives sampled locally in batch are not informative, and proposes ANCE, a learning mechanism that selects hard training negatives globally from the entire corpus, using an asynchronously updated ANN index.	abstract
2021-295	Strength: Experiments show that the proposed method significantly outperforms state-of-the-art approaches such as DPR on MSMARCO, TRECDL, NaturalLanguages and TriviaQA.	strength
2021-295	Weakness: The analysis on why in-batch local negatives are ineffective (in section 3) does not seem to be very insightful.	weakness
2021-295	Also, I did not see a big connection between this analysis and the negative sampling technique proposed in the following section.	weakness
2021-295	The idea of maintaining a set of global negatives is not new, and refreshing index asynchronously has also been explored in [Guu et al. 2020]	weakness
2021-295	Overall, I think this is a borderline paper.	misc
2021-295	The experiments show improved performance over baseline methods, but the idea seems a bit incremental (a combination of existing tricks for training retrieval model).	weakness
2021-295	########################################################################## Summary: Authors start from an assumption: "local negative sampling is the bottleneck of dense retrieval's effectiveness".	weakness
2021-295	To overcome this limitation, authors propose ANCE (Approximate nearest neighbour Negative Contrastive Estimation), a new contrastive representation learning mechanism for dense retrieval.	weakness
2021-295	The basic idea is that of constructing negatives exploiting the being trained deep retrieval module.	weakness
2021-295	The idea is that the model considers as negatives borderline cases.	abstract
2021-295	They also show, theoretically, that this improves the variance of the stochastic gradient estimation thus leading to faster convergence.	abstract
2021-295	########################################################################## Reasons for score: I honestly enjoyed reading the paper.	misc
2021-295	It has a theoretical justification that explains the intuition of using hard negatives.	abstract
2021-295	Experiments are thorough and they show improvements over the state of the art.	abstract
2021-295	The discussion section is thorough.	misc
2021-295	I believe that this research results are very important also in practice	strength
2021-295	########################################################################## Pros: The paper addresses a timely and important problem	strength
2021-295	The paper gives a nice theoretical justification for the reasons why they have to use hard negatives	abstract
2021-295	Experiments are thorough and nicely done.	strength
2021-295	Results are very good. I particularly enjoyed seeing that both on public datasets and on real world search systems the novel retrieval mechanism helps greatly.	strength
2021-295	########################################################################## Cons: The only one thing that I believe might impair the utilisation of this method is that you need to reconstruct the embeddings every m batches.	weakness
2021-295	It takes 10h every reconstruction and it is not clear what happens every m batches.	weakness
2021-295	Do you start a novel reconstruction?	weakness
2021-295	Do you replace the current embedding version with a new one as soon as one finishes training?	weakness
2021-295	This aspect, in my opinion, is the weakest of the paper and it would deserve more attention by the authors.	weakness
2021-295	########################################################################## Questions during rebuttal period: Please address and clarify the cons above	suggestion
2021-295	######################################################################### Some minor issues (1) In equation (2) I would call D^+ and D^-, D_q^+ and D_q^- in order to remark that negatives and positives are per-query.	weakness
2021-295	(2) an negative —> a negative	rating_summary
2021-295	(3) Citation Luan et al. —> It's Toutanova not Toutanove	weakness

2021-312	Summary This work proposes a new stochastic algorithm for distributed optimization.	abstract
2021-312	I really enjoyed reading the paper.	misc
2021-312	It has a strong theory, nice experiments with several objectives and parameter-sensitivity tests, and the writing is sufficiently good.	strength
2021-312	This authors did a great job comparing both theoretically and numerically to the most relevant literature.	strength
2021-312	The paper builds on top of the recent successes in distributed optimization and two particularly popular approaches: quantization and decentralized topology of the network.	abstract
2021-312	The paper's main weakness, in my opinion, is that the algorithm is a bit hard to understand at first, and it has a number of extra parameters, but this seems to be a common issue for decentralized algorithms, and the experiments show that their tuning is not difficult.	weakness
2021-312	Another issue is that the paper might be too technical for an unprepared reader, and the theorem statement is hard to parse.	weakness
2021-312	Detailed comments The paper presents a clear motivation for developing a new algorithm by explaining the shortcomings of the existing ones.	strength
2021-312	For instance, D-PSGD is explained to not converge precisely when the data is heterogeneous; error-feedback has a slow rate due to the delayed compensation of errors; DCD-PSGD is mentioned in Remark 1 to be too aggressive and to perform worse numerically.	weakness
2021-312	I think that the thorough comparison to the prior work is one of the main strengths of this paper.	strength
2021-312	I think the algorithm is not explained well.	weakness
2021-312	Its design consists of multiple pieces: a consensus algorithm based on NIDS/D^2, a compression scheme based on Diana, and the gradient updates of SGD, all of which are given together immediately.	weakness
2021-312	The authors present the algorithm without any warm-up, and there are several ways to fix it.	weakness
2021-312	Firstly, the meaning of the variables that are presented in the algorithm could help to understand it.	weakness
2021-312	Before presenting the full algorithm, it makes sense to mention that D^k is a dual variable needed to ensure stationarity at the optimum and that its role is to estimate the gradient there.	weakness
2021-312	Similarly, the communication procedure is not obvious and the meaning of H^k is unclear when just looking at the algorithmic steps.	weakness
2021-312	I do think that the paper would benefit from introducing some of the concepts before the algorithm.	suggestion
2021-312	My first impression was a lot of confusion, mainly because of the communication procedure.	weakness
2021-312	Maybe it's best if the authors explain what happens when the algorithm is fully centralized, what it boils down to, and why it works in that case.	suggestion
2021-312	Theorem 1 assumes that the gradients are bounded uniformly over the space.	suggestion
2021-312	A number of recent papers showed that SGD works even if the variance is bounded only at the optimum, for example, (Gower et al., "SGD: General Analysis and Improved Rates").	suggestion
2021-312	Do the authors think that it's possible to relax the assumptions for LEAD as well?	suggestion
2021-312	Can the authors present a complexity bound based on Theorem 1 that would show explicitly every term?	suggestion
2021-312	I think it should be something O((C*kappa+beta)log(1/eps) + kappasigma^2/eps) and I'd hope to see a comparison to the bound of (Koloskova et al., 2019).	suggestion
2021-312	The authors should provide a reference for the claims around Assumption 1, such as the eigenvalue bounds.	suggestion
2021-312	For instance, (Xiao and Boyd, "Fast linear iterations for distributed averaging") or any other material where the properties of mixing matrices are explained.	suggestion
2021-312	The convergence rate recovers that of NIDS as mentioned in Remark 3.	suggestion
2021-312	Does it also recover the right rate in other special cases, for instance, when the algorithm is fully centralized (W=I)?	suggestion
2021-312	In figure 4, please clarify if you use the running average of the losses or the full train loss.	suggestion
2021-312	Typos: Corollary 1: the expression for condition number has u instead of \\mu in the denominator.	weakness
2021-312	p.7, "before uniformly partitioned": should be "before being uniformly partitioned" The paper introduces a novel decentralized algorithm (LEAD) incorporated with compression that achieves linear convergence rate in strongly convex setting.	weakness
2021-312	The main idea is to apply and communicate the compression of an auxiliary variable instead of the primal or dual iterates.	weakness
2021-312	Convergence analysis is provided for both deterministic and stochastic variants.	weakness
2021-312	Experiments shows the state-of-the-art performance.	weakness
2021-312	The paper is well written and the results are presented clearly.	strength
2021-312	I only have a few questions regarding the presentation of the algorithm.	strength
2021-312	a) It would be better to discuss in more details that the algorithm reduces to NIDS when there is no compression.	weakness
2021-312	In the current presentation, this point is not clearly stated in the main paper (only in appendix B).	weakness
2021-312	b) The role of the auxiliary variables Y and H deserves a better  explanation.	weakness
2021-312	It is unclear to me why we apply the quantization on the difference between Y and H, in other words, what does this difference represent?	weakness
2021-312	Providing more intuitions on these points will be helpful for further understanding.	weakness
2021-312	c) According to the equation on the top of page 5, it seems like the quantization only introduces an additive noise in the update.	weakness
2021-312	Can it be viewed as an inexact variant of NIDS where the noise is bounded by some iterate dependent quantity?	weakness
2021-312	d) Does the result transfer to the convex but non-strongly convex setting?	weakness
2021-312	e) I am wondering whether the parameter C in the contractive operator is dimension dependent, in which case would the stepsize also be dimension dependent?	weakness
2021-312	f) In Figure 1 b) and Figure 2 b), some curves stop very early, it would be better to fix them.	weakness
2021-312	Overall, I am very positive about the paper and hence recommend acceptance.	decision
2021-312	This paper introduces a novel algorithm for decentralized optimization when nodes can only communicate a compressed signal with their neighbors.	abstract
2021-312	Unlike most decentralized methods with compression that are inspired by primal methods (DGD type methods), this paper introduces a new primal-dual algorithm with compression.	abstract
2021-312	The proposed method's main idea is borrowed from the NIDS algorithm, which converges linearly when the local loss functions are smooth and strongly convex.	abstract
2021-312	As the proposed LEAD method is based on primal-dual methods, it succeeds in improving the sublinear rate of primal-based methods.	abstract
2021-312	To the best of my knowledge, this is the first decentralized method that achieves a linear convergence rate in the setting that nodes use compressed signals.	strength
2021-312	Overall, the paper is well-written, and the authors explain the intuition behind each step of their proposed method very well.	strength
2021-312	Although the main proposed method and its convergence analysis are similar to the ones in the paper that introduce the NIDS algorithm, the final theoretical result is strong.	strength
2021-312	Therefore, the reviewer recommends the acceptance of this paper.	rating_summary
2021-312	A few minor comments: It is acceptable that the authors provide a linear convergence rate for the proposed method, but it would be better to characterize the overall complexity bound for their proposed method to achieve an ϵ accurate solution.	weakness
2021-312	In particular, it would be great if they could study the dependency of the overall complexity (number of communication rounds) on the graph connectivity  parameter and the objective function parameters (strong convexity and smoothness constants.)	suggestion
2021-312	For the stochastic case, there has been a recent line of work that shows that if each local function can be written as a finite sum of a large number of functions (which is often the case in most machine learning problems), it is possible to obtain exact linear convergence rate.	suggestion
2021-312	The current result for the stochastic setting is a bit trivial, considering the result for the deterministic setting.	weakness
2021-312	It would be interesting if the authors could use the tools used in the following papers to improve their results for the stochastic setting.	suggestion
2021-312	1- Dual-Free Stochastic Decentralized Optimization with Variance Reduction	misc
2021-312	2- An accelerated decentralized stochastic proximal algorithm for finite sums	misc
2021-312	3- DSA: Decentralized Double Stochastic Averaging Gradient Algorithm	misc
2021-312	4- Towards More Efficient Stochastic Decentralized Learning: Faster Convergence and Sparse Communication	misc
2021-312	5- An Optimal Algorithm for Decentralized Finite Sum Optimization	misc

2021-450	Summary The paper studies the relation between the geometry of solutions of continual (CL) and multi-task learning (MTL).	abstract
2021-450	Towards this end, the authors empirically identify that all the solutions of CL (i.e. solutions obtained after each task) and MTL are connected by a linear region of low error.	abstract
2021-450	This is a very interesting finding and, to my knowledge, has not been studied previously in the CL literature.	strength
2021-450	Based on this observation, the authors propose a memory and regularization-based CL method, MC-SGD, that ensures that the final CL solution is linearly connected to all the task's solutions.	strength
2021-450	The authors further demonstrate that the solution of the MTL lies in the region where the Hessian of the loss function is low and hence the regularization-based approaches that make use of curvature information (e.g.) EWC, are a promising direction for CL.	strength
2021-450	Experiments are conducted on Permuted and Rotated MNIST, Split CIFAR benchmarks.	strength
2021-450	MC-SGD performs strongly compared to other baselines.	strength
2021-450	Positives 1- I quite enjoyed reading the paper.	strength
2021-450	It is very well-written and insightful.	strength
2021-450	2- Sections 2.1 and 3 are very nice.	strength
2021-450	The finding, albeit empirical, that the solutions of multi-task and continual learning are linearly connected could prove to be very important for future research in CL.	strength
2021-450	3- Experimental results are very strong.	strength
2021-450	I am frankly quite surprised that the gain on top of ER is that much.	strength
2021-450	Although the authors mention it in Section 5 that they use a similar setup as in the other works, I just want to clarify the number of epochs here.	strength
2021-450	If my understanding of their work is correct then Chaudhry et al., in all their work use a single-epoch setup where Farajtabar et al., used multiple epochs.	weakness
2021-450	Do you use single or multiple epochs?	weakness
2021-450	Negatives I don't have any major concerns about the work except for a few nitpicks and questions.	misc
2021-450	1- See the multiple-epochs remark above.	misc
2021-450	2- Fig.5: Can you compute all the eigenvectors and show whether the cosine similarity in the eigenvectors corresponding to the smallest eigenvalues actually increase when you go towards the multi-task solution.	suggestion
2021-450	You can reduce network size if compute is the problem.	rebuttal_process
2021-450	3- Eq.5 (or similarly Eq. 3): It seems that one needs the solution of task t w^t for this loss to work.	weakness
2021-450	If one just receives the task t how would one obtain this solution?	weakness
2021-450	Do you do this in two steps?	weakness
2021-450	Where, in step 1, you just compute the wt^ starting from w¯t−1, and then, in step 2, you use the wt^ obtained from step 1 to compute the final w¯t?	weakness
2021-450	4- Fig. 7: Might want to highlight in the legend which path is CL and which is MC.	weakness
2021-450	5- Page 5, 6th to last line, oneself itself ############## Summary ##############	weakness
2021-450	This submission asks the question of how the minima found by batch multi-task learning compare to those of continual learning.	abstract
2021-450	It empirically finds the they are connected via linear interpolation through a manifold of low error, and leverages this fact to come up with a clever new algorithm for continual learning that performs better than various existing continual learning baselines on three benchmark data sets.	abstract
2021-450	############## Strengths ############## The question of how to connect multi-task to continual learning solutions is well motivated via simple introductory experiments.	strength
2021-450	The answer to this question, that there is a linear mode connectivity, motivates a simple, elegant, and effective algorithm.	strength
2021-450	############## Weaknesses ############## The paper could benefit from substantial editing to make it clearer and easier to follow.	weakness
2021-450	I found myself having to re-read various sections to properly understand how the different parts of the paper were connected.	weakness
2021-450	The empirical evaluation is done only on three benchmarks.	weakness
2021-450	It could be valuable to add evaluations on additional data sets, like Omniglot (https://github.com/brendenlake/omniglot).	suggestion
2021-450	############## Recommendation ############## I recommend this paper for acceptance, but urge the authors to substantially revise their manuscript to make it more approachable.	decision
2021-450	I believe this paper to be self-contained, with a clear question being asked, which hadn't been asked before: how are the solutions to multi-task and continual learning methods connected.	weakness
2021-450	The authors find that there is linear connectivity between these solutions, and use this fact to motivate a simple yet effective continual learning algorithm.	weakness
2021-450	############## Arguments ############## The question of whether and how the solutions to multi-task and continual learning are connected is highly relevant.	strength
2021-450	While most prior literature had assumed that some distance metric in the parameter space was the correct way to measure their connection, this work is motivated by the experiments in Fig. 2, which show that these metrics are not quite appropriate.	abstract
2021-450	Instead, the authors show that linear mode connectivity better explains how multi-task and continual learning solutions are related.	abstract
2021-450	The manuscript then deviates to an analysis of when this type of connectivity holds by analyzing second-order Taylor approximations.	abstract
2021-450	I had to re-read this section (Section 3) multiple times in order to find what the relevance of it was to the submission.	abstract
2021-450	My conclusion was that the point is that the fact that the parameter vectors move in directions of low curvature means that interpolation in those directions doesn't increase the loss by much.	abstract
2021-450	This fact seems to be somewhat hidden in the text.	abstract
2021-450	I encourage the authors to place emphasis on what their analysis is attempting to find before diving into it in depth, as it is easy to lose the reader if they are not aware of where the analysis is going from the start.	suggestion
2021-450	The proposed algorithm is clever and simple: it leverages past data not only to approximate the loss of the previous tasks on the new solution, but also to add a regularization encouraging a low-loss linear path between the solutions.	strength
2021-450	Although the authors experiment with very few data sets, I believe they sufficiently show the applicability of their method and the fact that it performs well.	strength
2021-450	It would be interesting to see how differently the method would perform if instead of the MC regularization, the authors used the EWC one.	suggestion
2021-450	This would help avoid conflating the claim "regularization + replay is best" from "MC regularization + replay is best".	suggestion
2021-450	Similarly, it would be relevant to reproduce Figure 7 with the solutions found by baselines, to assess whether they also find linear connectivity solutions.	suggestion
2021-450	The claims would be stronger if the authors showed that baselines don't find linearly connected solutions.	suggestion
2021-450	############## Additional feedback ############## The following points are provided as feedback to hopefully help better shape the submitted manuscript, but did not impact my recommendation in a major way.	misc
2021-450	Intro It seems like the authors interchangeably used en-dash and em-dash.	weakness
2021-450	They also used en-dash to open, but not close, a statement.	weakness
2021-450	What confounding factors are removed by doing w1 --> w1,w2 other than initialization?	weakness
2021-450	The text makes it sound like there's more but no other is discussed.	weakness
2021-450	Contribution 3: benchmark --> benchmarks	misc
2021-450	I believe compressed related work sections or those pushed to the appendix make it hard to place the contribution in context.	weakness
2021-450	I encourage the authors to expand this section in the main paper.	suggestion
2021-450	Sec 3 I had two main questions when reading this section: Why doesn't EWC find such a low curvature path, if it precisely penalizes deviations in directions of high curvature?	weakness
2021-450	Why can't we just use the proper Taylor expansion instead of Euclidean distance then, to measure forgetting, instead of mode connectivity?	weakness
2021-450	These two questions were answered towards the end of the section by showing that this is not a sufficient condition, and are then explicitly addressed by suggesting that second-order approximations are a promising direction for future work.	rebuttal_process
2021-450	I encourage the authors to clarify this before diving into the analysis, so the reader knows what to look for when reading this section.	suggestion
2021-450	The caption for Fig. 5 doesn't explain difference between b and c, which is only somewhat explained in text later.	weakness
2021-450	Sec 4 Regularization only considers low-loss path between the solution to the immediately previous task and the current solution (but not the solutions to all past tasks), assuming that the immediately previous solution contains sufficient information.	weakness
2021-450	Was this empirically tested? The EWC authors claim that using only the previous model in their setting is insufficient [1], so it would be interesting to see if there's a similar effect here.	weakness
2021-450	Appendices Very complete: additional results, justification of experimental setting.	strength
2021-450	Style, grammar: appendix X --> Appendix X	misc
2021-450	second order Taylor --> second-order Taylor expansion/approximation minima is often used as a singular, which should be minimum regularization based --> regularization-based rehearsal based --> rehearsal-based	misc
2021-450	Inconsistent italization of i.e. few shot learning --> few-shot learning	weakness
2021-450	[1] Kirkpatrick, J., Pascanu, R., Rabinowitz, N., Veness, J., Desjardins, G., Rusu, A.	misc
2021-450	A., ... & Hassabis, D. (2018).	misc
2021-450	Reply to Huszár: The elastic weight consolidation penalty is empirically valid.	strength
2021-450	Proceedings of the National Academy of Sciences, 115(11), E2498-E2498.	misc
2021-450	Chicago The paper starts by that observing the local minima obtained in a multi task scenario are connected with a linear path of low error regime to the local minima of each task in a continual learning scenario in contrast to the path between the different minima of tasks incrementally learned, provided the both training of multi task and continual learning have started from the same initialization.	abstract
2021-450	The paper studies and shows this mode connectivity empirically.	abstract
2021-450	It further discusses and analyses the factors behind this connectivity while noting that this is valid when tasks have shared structure in which local minima can be found nearby.	abstract
2021-450	Motivated by these observations, the paper proposes a new solution to the continual learning problem.	abstract
2021-450	This is done by defining a new loss that forces this connectivity between the minima of  the previous task and the current task.	abstract
2021-450	As this requires evaluating the loss of a previous task, an experience replay of stored previous samples is used.	abstract
2021-450	The paper shows improved performance in comparison to existing methods on different benchmarks of 20 tasks long each.	abstract
2021-450	While I enjoy reading the paper, I think the clarity of the text can be enhanced specifically when referring to figures.	weakness
2021-450	The second reference to figure 2, comments on the Euclidean distance without explaining where this is shown in the figure and that was not so clear in the figure caption either.	weakness
2021-450	Figure 7, it is not clear what corresponds to the Naïve SGD and what corresponds to the MC SGD.	weakness
2021-450	On the empirical evaluation, I wonder how stable SGD would perform if was given access to the same replay buffer?	weakness
2021-450	It would be also interesting to show the comparison of the path with Stable SGD since it is supposed to find wider local minima where other tasks minima are likely to be nearby.	suggestion
2021-450	I assume that split cifar 100 is multi-head, would the proposed solution shows similar advantages in the shared head scenario?	suggestion

2021-513	(I will keep my review short, as I don't have much constructive criticisms to share)	misc
2021-513	The authors propose a new activation function that guarantees sparsity, which can reduce interference/forgetting when learning on non-stationary data.	abstract
2021-513	The Leaky Tiling Activations (LTA) are relaxations of the binning operation to make it differentiable.	abstract
2021-513	The authors proved that LTA guarantees sparse features.	abstract
2021-513	Next, LTA is experimentally evaluated in online supervised learning on non-stationary data, as well as in reinforcement learning (RL).	abstract
2021-513	In the supervised learning experiment, LTA is shown to outperform ReLU on a toy task.	abstract
2021-513	In the RL experiments, the story is repeated in discrete action domains (with a DQN backbone) and in continuous action domains (with a DDPG backbone).	abstract
2021-513	The paper is well written and an enjoyable read.	strength
2021-513	It is well motivated given that sparsity is an important characteristic for learning on non-stationary data.	strength
2021-513	The empirical section is extensive.	strength
2021-513	I am not really knowledgeable of those experiments, however.	weakness
2021-513	This is why I chose a confidence score of 4.	rating_summary
2021-513	My main criticism would be that no code was shared.	weakness
2021-513	I am willing to keep my score and fight for acceptance if the authors commit to releasing the code upon acceptance.	decision
2021-513	I would have appreciated, however, that the codebase would be available during the review period.	strength
2021-513	This is why I chose a score of 7 instead of 8.	rating_summary
2021-513	Furthermore, I encourage the authors to build simple PyTorch and/or TF packages such that one can easily use the LTA without much overhead.	suggestion
2021-513	Post rebuttal I am happy with the response.	rebuttal_process
2021-513	This paper proposes a novel activation function based on tiling, with a careful consideration of the tradeoff between differentiable regions of the tiling (which would otherwise be an undifferentiable one-hot) and sparsity.	abstract
2021-513	Using such activations in domains where we expect sparsity to help, online learning and deep RL, shows some encouraging improvements.	abstract
2021-513	Strengths: The paper is fairly well written and easy to understand	strength
2021-513	The method is simple and addresses important problems in deep RL	strength
2021-513	Results are encouraging Weaknesses [the two following weaknesses we addressed in the rebuttal]: The central motivation for this method is the reduction in gradient interference, yet, it is never measured, nor other similar values.	weakness
2021-513	Sparsity is measured, but it is expected to be lower, by construction.	weakness
2021-513	Proposing new methods without understanding why they work is detrimental to progress.	weakness
2021-513	"My number is bigger than your number" is not a great way to do science.	weakness
2021-513	The comparisons and baselines may not be entirely fair (although the methodology is otherwise good, with repeated runs and confidence intervals)	weakness
2021-513	I gave the paper a score of 6 because while the proposed method is interesting and possibly useful, more needs to be done to understand it, and advance our understanding of deep RL.	decision
2021-513	[Rebuttal update: the extra results clear up some uncertainty about this method.	rebuttal_process
2021-513	I think the success of this method is interesting and teaches us a thing or two about deep RL.]	strength
2021-513	Comments: "past Xt−1, Xt−2", that would be a 2-Markov problem, and is needlessly specific?	weakness
2021-513	I'd suggest just writing "the past Xt−i" since general online problems can have arbitrary time-dependencies.	suggestion
2021-513	"such encodings can enable faster learning and reduce interference", that may be true, but the authors need to test this.	weakness
2021-513	"LTA itself does not introduce any new training parameters", that's not true.	weakness
2021-513	If a layer has a 100 units and 4 LTA bins, then the output of LTA is 400 units, and the following layers has to be 4 times as big, and thus introduces 300 new training parameters.	weakness
2021-513	"One potentially surprising point is that the LTA reaches a lower error, even on iid data", this should be a warning sign.	weakness
2021-513	When a new method outperforms an old method in settings where it shouldn't, it's often because the setting is unfair to the old method.	weakness
2021-513	I see in Figure 17 that the authors compared LTA to a wider ReLU network, that's good, although it's still not clear to me that the comparison is fair, as values of k and eta are not given, nor how many units and layers there are.	weakness
2021-513	It would be good to have all hyperparameters clearly laid out, especially in the appendix (which can be arbitrarily long).	suggestion
2021-513	(The RL section is much clearer in that regard!)	suggestion
2021-513	Is the (presumed) reduction in gradient interference really due to the sparse encoding?	weakness
2021-513	or due to LTA still being mostly 0-gradient everywhere (e.g. in Figure 1c, with k=3 and eta=0.1, 60% of the x-axis has gradient 0)?	weakness
2021-513	This should be tested. If it's not clear what I mean, let's say we have 100 LTA units with k=4 and so 400 outputs.	weakness
2021-513	By construction we're going to have at most a 25% sparsity, let's say for argument's sake that all LTA inputs are positive and so there exactly 25% or 100 units that are on.	weakness
2021-513	Some proportion of these 100 active units will be in the 0-gradient regime, even though they are active.	weakness
2021-513	If that proportion is 5%, then activation sparsity and gradient sparsity will be roughly the same, but if that proportion is 80%, then the gradients will be even more sparse than the activations.	weakness
2021-513	That seems like something that's important to know to understand how/why the proposed method works.	weakness
2021-513	The RL experiments compare DQN-LTA with DQN-Large, this is good, but perhaps misleading it's not clear how k/delta/eta were chosen (Figure 13 hints at good values, but the legend is missing and it's not clear if this was done post-hoc or if it informed the choice in the main experiments, I assume the latter)	weakness
2021-513	it's not clear that the k that's optimal for DQN-LTA is the same that's optimal for DQN-Large.	weakness
2021-513	Considering Figure 13, it seems that the authors spent more time doing hyperparameter search for DQN-LTA than for DQN-Large, in that sense it's likely an unfair comparison.	weakness
2021-513	(in any case it would be interesting to see a plot similar to Figure 13 for DQN) Update during review period	suggestion
2021-513	The reproducibility of the paper is now much better.	suggestion
2021-513	It's great that the authors promised to release the LTA code.	weakness
2021-513	I hope that this includes the code for the experiments.	suggestion
2021-513	Based on the above, I changed my review score to 7.	rebuttal_process
2021-513	Summary The paper presents a novel activation function (Leaky Tiling Activation - LTA) to produce sparse activations, which have been found to stabilize learning in continual learning and RL settings.	abstract
2021-513	The new nonlinearity and its theoretical properties are described well, and the authors present convincing experiments demonstrating that the method yields practical benefits on synthetic datasets and RL games (e.g. Atari).	strength
2021-513	Reasons for score: The paper should certainly be published somewhere, but maybe in a workshop that focuses on continual learning or RL.	weakness
2021-513	While the proposed new activation function may be useful in some settings, there is not enough evidence in the paper that it would become a go-to solution, or significantly change the way we think about interference in continual learning and RL.	weakness
2021-513	In particular, the authors compare LTA variants of DQN, but it might have been useful to compare with e.g. Rainbow too.	weakness
2021-513	While I agree that interference is still an issue in modern RL with function approximation, it would be useful for potential users of LTA to know whether LTA provides benefits when used in conjunction with an existing state-of-the-art RL algorithm.	suggestion
2021-513	Adding an experiment to this effect to the appendix would make this paper stronger.	suggestion
2021-513	Pros The paper addresses a key problem in continuous learning and RL: interference and catastrophic forgetting.	strength
2021-513	It presents a novel method to combat this problem, and demonstrates its usefulness in a number of experiments.	strength
2021-513	The paper is generally well-written.	strength
2021-513	The experiments on synthetic data were compelling, and made for a very nice controlled experiment.	strength
2021-513	Cons The authors could have benchmarked against stronger baselines.	weakness
2021-513	The authors might be able to do this during the review process.	suggestion
2021-513	The precise set-ups that the authors used for their experiments should be described more clearly.	weakness
2021-513	As it is, the paper's work is not reproducible.	weakness
2021-513	See my the section "Questions during rebuttal period" below for details.	misc
2021-513	Questions during rebuttal period: Have the authors considered to benchmark using stronger (state-of-the-art) baselines?	rebuttal_process
2021-513	Someone who considers using LTAs would likely use a state-of-the-art method already, not DQN, which the authors benchmarked against.	weakness
2021-513	The question is then whether LTA yields benefits when used in conjunction with state-of-the-art methods, not when used in conjunction with DQN.	weakness
2021-513	I did not fully understand what architecture the authors used in their experiments.	weakness
2021-513	The architecture appears to be described mostly in this single sentence: "All the algorithms use a two-layer neural network, with the primary difference being the activation used on the last layer.	weakness
2021-513	Between what and what is this difference?	weakness
2021-513	Between baselines and the LTA experiments?	weakness
2021-513	Later on the page from which I quoted above, the authors mention that they experimented with DQN-like networks.	rebuttal_process
2021-513	The original DQN paper used 3 layers (https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf).	rebuttal_process
2021-513	Did the authors mean to write ""All the algorithms use a two-layer neural network"?	rebuttal_process
2021-513	Do the authors insert the LTA before the third layer?	rebuttal_process
2021-513	More generally, it would be helpful if the authors could describe the architectures used in more detail and maybe ask a colleague who is not yet familiar with the paper to review for clarity and reproducibility.	suggestion
2021-513	Some suggestions I would rephrase the first sentence of the abstract in order to introduce "interference" in a gentler way.	suggestion
2021-513	While interference is an active research topic in the RL community, many members of the *CONF* community might wonder "what kind of interference"?	suggestion
2021-513	A half-sentence like "where updates for some inputs degrade accuracy for others" (copied from the paper's introduction)  could suffice here.	suggestion
2021-513	In section 5.1, the authors have lines beginning with bolded "DQN", "DQN-LTA", "DQN-Large" et cetera.	suggestion
2021-513	For readability's sake, it might be useful to format these as bulleted lists.	suggestion
2021-513	A small grammatical issue: "This issue is usually called gradient vanish".	weakness
2021-513	Maybe rephrase this as "This issue is known as the vanishing gradient problem".	suggestion
2021-513	The authors list ReLUs as an example of an activation function with vanishing gradients.	suggestion
2021-513	As vanishing gradient problems go, ReLU is a bit different from tanh and other nonlinearities that saturate, so I would avoid listing it here to avoid unnecessary debates.	suggestion
2021-513	The authors write "Mnist" in a number of places.	suggestion
2021-513	The correct spelling is "MNIST": this is an acronym for "Modified National Institute of Standards and Technology".	suggestion
2021-513	Summary: This paper presents a new activation function "Leaky Tiling Activation", designed with the goal of learning sparse representations (where only a few units activate for a given input).	abstract
2021-513	This new activation function, rather than mapping a scalar to another scalar, maps a scalar to a smoothed one-hot representation based on bins., which encourages sparse representations.	abstract
2021-513	Reasons for score: Although I have some issues with the sparsity claims of this paper (see my comments below), I found the paper intriguing and I am curious to try similar ideas in other problems.	strength
2021-513	So, with some work on the sparsity argument sections, I think this could be an interesting paper with ideas people would want to play with.	strength
2021-513	Additional feedback: page 3: "In the following theorem" -> which theorem?	strength
2021-513	[edit after reading further: Do you mean Theorem 1 in Section 3.3?	suggestion
2021-513	(probably not, as that's for LTA, not TA)]	weakness
2021-513	I am not sure if "leaky" is the best name for LTA, as more than "leaky", it is almost like the operators in fuzzy logic, with the term "leaky", I was imagining it more like a Leaky ReLU, where derivative is non zero everywhere.	weakness
2021-513	page 4: about the "guaranteed sparsity".	weakness
2021-513	I do not think it's very fair to compare the sparsity achieved by this operation with that achieved via regularization for the following reason: LTA increases the number of outputs of the unit.	weakness
2021-513	So, if in a regular layer with, say, a ReLU activation, you would have n outputs, with an LTA activation you would have k*n outputs.	weakness
2021-513	Assuming TA, instead of LTA, where will be exactly "n" outputs that are non zero, the same as in the original network with ReLU.	weakness
2021-513	So, LTA is not increasing sparsity, but just encoding the output in a different way that makes it look sparse as it increases the number of outputs (Theorem 1 and its Corollary feel totally unnecessary, as the network is sparse by construction, no need to prove it).	weakness
2021-513	It's like claiming that a OHE representation of a discrete label is "sparser" than the label itself, which is an iffy argument, as they encode the same information.	weakness
2021-513	I am not implying that LTA is not useful (I still find the paper very intriguing!), but just that the sparsity argument is questionable, and in my view it is not fair.	weakness
2021-513	page 5: it seems all results are in the appendix, which is unfortunate, as papers should stand on their own and appendix only used for additional explanation, not for main results.	weakness
2021-513	So, I am going to judge the paper just based on what is on the first 10 pages, to be fair to other papers.	misc
2021-513	I'd expect sentences like "as figure X shows, blah, blah (more details in appendix)", but not "as figure X in the appendix shows, blah blah main result".	suggestion
2021-513	This is trying to game the page limitations.	strength

2021-534	The paper presents a post-hoc calibration method for deep neural net classification.	abstract
2021-534	The method proposes to first reduces the well-known ECE score to a special case of the Kolmogorov-Smirnov (KS) test, and this way solves the dependency of ECE on the limiting binning assumption.	abstract
2021-534	The method proposes next to recalibrate the classification probabilities by fitting a cubic spline to the KS test score.	abstract
2021-534	Strengths: The proposed approach is a clearly novel solution to an essential problem for the safety-critical use of deep learning.	strength
2021-534	The paper presents the material in a very clear and neat way, following an excellent scientific writing practice.	strength
2021-534	Both the KS test based treatment of network calibration and the elegant way to improve the calibration by cubic spline fitting are interesting and likely to attract the attention of the deep learning uncertainty community.	strength
2021-534	The paper presents an exhaustive set of experiments that report performances of quite deep state-of-the-art network architectures on four different benchmark tasks.	abstract
2021-534	It also provides comparisons against a decent number of state-of-the-art calibration methods.	abstract
2021-534	The results demonstrate the benefits of the proposed method.	abstract
2021-534	(Minor) Weaknesses: The paper can be improved with a few small adjustments in the presentation: The so-called "python semantics" notation in Sec 2 only introduces complexity and does not bring any concrete value to the story line.	weakness
2021-534	What is wrong with denoting the r-th top score simply as f^{(n)}.	weakness
2021-534	Why do we need the minus?	weakness
2021-534	We can simply assume that the first element of the sorted list is the top score.	weakness
2021-534	Why do we need Proposition 4.1 and its proof (sketch and full) in this paper?	weakness
2021-534	It only echoes the standard relationship between a PDF and CDF.	weakness
2021-534	The former is the derivative of the latter.	ac_disagreement
2021-534	Although the results in Tables 1 and 2 are groundbreaking in favor of the proposed method, they are calibration scores based on the KS score.	weakness
2021-534	That is only one way of measuring the calibratedness of a network, and furthermore that is the score the proposed method is maximizing.	weakness
2021-534	It would be interesting to see how well the proposed method generalizes across other calibration scores that may highlight complementary aspects of the uncertainty treatment capabilities of the predictor, such as the Brier score.	suggestion
2021-534	Overall, this is a solid piece of work that qualifies to appear at the *CONF*'21 proceedings.	decision
2021-534	Pros: originality, clarity, technical correctness.	strength
2021-534	Cons: experiments need some clarifications.	weakness
2021-534	Calibration typically relies heavily on binning the data, both for the calibration itself  (Histogram Binning) and how to measure its quality (ECE).	weakness
2021-534	Thus both operations suffer from sampling issues that are a cause for both bias and variance.	weakness
2021-534	The idea to rather perform the analysis using cumulative distributions would seem obvious, as it has been tried on so many other problems, but I have not seen it used for calibration.	weakness
2021-534	They do it for both calibration and its measure: The use of the Kolmogorov-Smirnov test to measure calibration between the target and the output distributions.	weakness
2021-534	Spline fitting of the cumulative distribution to compute its derivative	weakness
2021-534	As the implementation details are far from obvious , there are several original contributions, especially in implementing the splines.	strength
2021-534	I found the description both very clear and concise, switching between intuition and equations.	strength
2021-534	The only part I found confusing is the second paragraph of section 4.2 ("One method of calibration..").	weakness
2021-534	Fortunately, the next paragraph gives a very simple intuitive explanation by just stating how it is implemented.	weakness
2021-534	Experiments are very comprehensive and show improvements over Temp scaling and other methods.	weakness
2021-534	However, there are also some results that contradict previously reported experiments and need to be clarified: Besides Temp scaling, the main other methods are borrowed from Kull et al, so one could expect some consistency.	weakness
2021-534	However ECE results from Table 6 look very different from Table 3 in Kull et al. I assume these are different types of ECE: confidence vs.	weakness
2021-534	class-wise? In Table 1, KS result for the ODIR methods of Kull et Al are much worse than Temp scaling, in particular for CIFAR-100 and Imagenet.	weakness
2021-534	This contradicts results reported in Table 2 (this paper) and Table 3 (Kull et al) and should be explained.	weakness
2021-534	I am no expert in image classification,  but the 70% accuracy reported for CIFAR-100 seems way below current numbers, which have exceeded 80% since 2017, in particular for the proposed architectures (for instance Wide Resnet or DenseNet) https://benchmarks.ai/cifar-100.	weakness
2021-534	However these numbers seem to be consistent with what is reported by Kull et al (Table 18 in https://arxiv.org/pdf/1910.12656.pdf), so I assume the issue comes from borrowing their architecture and scores.	weakness
2021-534	While this should not impact comparative results, it would have been more satisfactory to use baseline architectures that match the state-of-the-art.	suggestion
2021-534	Additional experiments or discussions on the following would greatly help: As the spline method is not always better than Temp scaling, how do they compare from a computational viewpoint?	suggestion
2021-534	How long does the binary search over thousands of calibration examples take compared to the DNN feed-forward?	suggestion
2021-534	From Tables 1,2 and 6, KS error and ECE rank methods quite differently.	suggestion
2021-534	One reason one should trust KS more is that it does not depend on binning choices, and rely on a time-proven test.	suggestion
2021-534	But could one come up with an experiment that shows that KS is provably more reliable?	suggestion
2021-534	Post rebuttal: I have read the authors responses with to my 4 questions, and appreciate how detailed and honest they are.	rebuttal_process
2021-534	They satisfy my concerns. The authors present a binning-free calibration measure from a Kolmogorov-Smirnov-based test.	rebuttal_process
2021-534	Besides, the cumulative probability distribution is estimated using a spline-based fitting from percentiles.	abstract
2021-534	The approach allows correcting the probability estimation from trained deep learning models.	abstract
2021-534	The paper is clear and well-founded.	strength
2021-534	The paper proposes a post re-calibration spline-based approach for the re-calibration of multiclass predictions.	abstract
2021-534	Experiment results on image datasets are provided and evaluated according to the Kolmogorov–Smirnov (KS) statistic.	abstract
2021-534	Strengths: The proposed binning-free calibration measure is a good idea and widely used in binary classification problems	strength
2021-534	As expected, experimental results demonstrate that optimizing calibration with a spline-based approach results in better calibration	strength
2021-534	Weaknesses: Inconsistent notation: The authors should stick to either x or X	weakness
2021-534	The definition of the KS statistic is inconsistent with standard formulations Eq (8).	weakness
2021-534	Also, the paper claims to obtain the maximum value at σ=1; this is a strong assumption.	weakness
2021-534	Weak experiments: Why KS statistic and not Wasserstein or calibration slope?	weakness
2021-534	Eq (11) assumes fk(xi) are distinct, which is not always the case.	weakness
2021-534	Calibration and accuracy are orthogonal concerns, where the proposed post re-calibration approach is prone to overfit on calibration at the expense of accuracy (and this is the case, as experimental results show a loss in accuracy)	weakness
2021-534	Demonstrating an approach that accounts for the calibration-accuracy tradeoff is crucial	suggestion
2021-534	Extending the proposed approach to regression problems would strengthen the submission	suggestion
2021-534	A  qualitative discussion on: Why Temp.	suggestion
2021-534	Scaling is better calibrated in some instances	weakness
2021-534	Why calibration results of proposed solution (and alternatives) vary across different network architectures for the same dataset	weakness
2021-534	Why the proposed approach drops in performance between top-1 and top-2 predictions	weakness
2021-534	The proposed re-calibration approach may not scale well with large datasets; a computational complexity comparison against alternatives is crucial	weakness

2021-631	Initial Review The paper proposes a new algorithm for de-novo molecular design, which uses a model to extract explanatory subgraphs from a set of support molecules which "explain" high scores wrt a scoring function, and a generative model which is conditioned on these subgraphs to produce full molecules.	abstract
2021-631	All in all, I think this is an interesting combination of several existing approaches in generative models for molecules (all referenced in the paper).	strength
2021-631	The aspect of explainability is novel.	strength
2021-631	The presentation of the paper is mostly clear.	strength
2021-631	The approach is quite geared to molecule generation, but can potentially also inspire applications in other domains, which makes it interesting from a general ML perspective as well.	strength
2021-631	I like the paper from the theoretical side, which alone warrants acceptance of the paper at *CONF* in my opinion.	decision
2021-631	However, I have a few minor concerns / comments: I am a bit on the fence with the validation method.	weakness
2021-631	Since almost every new paper in the field proposes a new validation approach, it has become pretty much impossible to assess what the state of the art of the field is (or if the concept of SOTA is even something meaningful), and this paper is no exception in this regard.	weakness
2021-631	But I assume the authors will disagree here.	weakness
2021-631	In practice, generating 20k molecules is a lot.	weakness
2021-631	Looking at the statistics of the top100 molecules would probably be sufficient.	weakness
2021-631	Also, I find it somewhat surprising that some of the baseline algorithms (in particular the Winter et al MSO model), which are less constrained than algorithm presented here, are not achieving higher scores, in particular when the algorithms can query the scoring function 5 M times.	weakness
2021-631	Maybe this is something the authors could comment on in the rebuttal.	suggestion
2021-631	As an additional baseline, I would suggest to report the "best in dataset", I.e. run the scoring function on the seeds and all molecules used to train the generator, and pick the top molecules.	suggestion
2021-631	Related work: I would suggest to additionally cite https://arxiv.org/abs/1701.01329 which was the first paper to apply neural models to molecule generation in drug discovery, and the first of such papers which has been prospectively validated in laboratory experiments by scientists not affiliated with the authors.	suggestion
2021-631	Update 1 after discussion: Score raised.	rebuttal_process
2021-631	The authors propose a two step procedure for generating molecular graphs that optimize some desirable properties.	abstract
2021-631	The method consists of a rationale extraction phase, where the subgraph "important" for the desired property is identified and a graph completion step.	abstract
2021-631	When reading the paper for the first time, I found it a bit hard to follow the approach.	weakness
2021-631	The paper might be easier to read when the individual model components are introduced directly with the E- and M-step.	suggestion
2021-631	Currently, the graph completion model is introduced in Sec 3.1, then the E- and M-steps are described in Sec. 3.2, while the "explainer" is only mentioned and referred to Sec. 3.3.	weakness
2021-631	An easier to follow structure might be: 1) Rationale extraction 2) Graph completion 3) E/M iteration.	suggestion
2021-631	Beyond that, the terms "evolution" and "explanation" might be misleading here: The method is not an evolutionary algorithm, as the title might suggest.	weakness
2021-631	Even if subgraphs are extracted that lead to the generation of promising molecules in the graph completion step, this does not show that these subgraphs responsible.	weakness
2021-631	To warrant the term explaination, a  thorough analysis of the extracted rationales would be necessary, in particular since they might not even be connected graphs.	weakness
2021-631	The evaluation of the method in Table 1 and Fig. 3 show that the method outperforms previous appraoches and is capable of shifting the generated distribution shifts to higher scores.	weakness
2021-631	The human evaluation is weak, since only a single expert was asked.	weakness
2021-631	A panel of experts with reported agreement among the panel would improve the paper.	misc
2021-631	However, since the scores of the molecules are available, it does not become quite clear to me, how human experts benefit the performance evaluation under those same criteria.	weakness
2021-631	Pros Interesting approach to update the pool of rationales	strength
2021-631	Outperforms previous approaches, Fig 3.	strength
2021-631	show that desired properties improve over rounds	strength
2021-631	Ablation study demonstrates improvement by proposed procedure	strength
2021-631	Cons Structure of the paper could be improved	weakness
2021-631	The paper states that in the explainer a "subgraph s of k vertices" is extracted, therefore I assume the size of the rationale is fixed.	weakness
2021-631	This would severely restrict the space of rationales.	weakness
2021-631	The notion of explainability is not sufficiently discussed in the paper and the claim(?) that the rationales are somehow meaningful is not examined.	weakness
2021-631	Update: I read the reply and thank the authors for the clarifications.	misc
2021-631	SUMMARY: The authors propose a method that "explains" molecular properties based on molecular fragments and call it Molecular Evolution.	abstract
2021-631	The subgraph "explanations" then are used to explore larger swaths of chemical space.	abstract
2021-631	PROS: As far as the reviewer notes, this approach is novel in the (now increasingly crowded) set of alternatives for molecular generative models.	strength
2021-631	The authors have a model that compares favorably to the baselines	strength
2021-631	The authors use a very relevant set of optimization parameters for the multiobjective task.	strength
2021-631	The paper is well explained.	strength
2021-631	CONS: The reviewer believes that there is much more to explain why a molecule is better for a task than identifying a subgraph.	weakness
2021-631	This should be made clear in the manuscript as materials scientists want to know for example quantum properties of the fragment(s) and how they influence the given property to provide a valuable explanation.	weakness
2021-631	The paper tackles the problem of molecule property optimisation.	abstract
2021-631	To this end, the authors proposes an alternating approach consisting of an explainer model and a molecule completion model.	abstract
2021-631	The explainer model takes a complete molecule as input and outputs a subgraph that represents the part that contributes most to property prediction.	abstract
2021-631	Then, the molecule completion model uses the subgraphs to sample a complete graph that can maximise the property scores.	abstract
2021-631	The loss function of molecule completion model directly maximises the properties, which is non-differentiable so that the authors use a REINFORCE algorithm for optimisation.	abstract
2021-631	Pros: The paper proposes use subgraphs that contributes most to the property prediction for searching better molecules.	strength
2021-631	The subgraphs are learned supervisedly through the signals from the property to be optimised.	strength
2021-631	Compared to unconditional VAE models, this approach might be easier to optimise, since the subgraphs can serve as templates.	strength
2021-631	The method is novel and the experiments demonstrate the effectiveness of the methods compared to previous methods.	strength
2021-631	The paper is well-written and the idea is articulated in a formal description.	strength
2021-631	Cons (or questions): Some convergence analysis shall be needed, i.e. why this method will converge to the optimal values of the objective.	weakness
2021-631	The authors claim the method is an EM algorithm, and some proofs about convergence might be needed.	weakness
2021-631	Otherwise, some learning curves might be helpful, since the REINFORCE algorithm is known to suffer from high variances.	weakness

2021-746	This paper delves deeper into understanding shape-based representation of CNNs in an empirical way.	abstract
2021-746	Based on the stylized images, it proposes to use edge maps to more explicitly feed shape information to learning models.	abstract
2021-746	Besides, the common way to let models learn the shape-based representation is to train on the dataset contained the shape information while the texture information is severely distorted.	abstract
2021-746	The paper takes the point of changing the statistics of feature maps would result in style changes and proposed style-randomization to help CNNs better focus on shape information.	abstract
2021-746	Also, it connects the biasing degree on shape information of models with the defensive performance against common corruptions, like Gaussian additive noise, blur, and etc.	abstract
2021-746	An intuitive conclusion was drawn that there is no clear correlation between shape bias and robustness against common corruptions, and justified by extensive experiments.	abstract
2021-746	The studied direction is important for understanding the learned representation of CNNs. The proposed points, using edge maps and style randomization, are simple and effective for making CNNs focus on shape information.	abstract
2021-746	Also, the discussion with the common corruptions is intriguing and helps us better understand the relationships between the shape-based representation and the robustness.	strength
2021-746	The experiments are technically sound and results are sufficient to support their arguments, though more analysis would add support to the claims.	strength
2021-746	The experimental setups are described in detail that makes it easy to replicate the experiments.	strength
2021-746	Overall this paper is easy to follow.	strength
2021-746	The reviewer would encourage the author to share their codes and datasets.	suggestion
2021-746	Some concerns and comments are listed below: Since most of the experiments are conducted on ImageNet-20 and ResNet-18 (appendix lists few results on ImageNet-200 and ResNet-50), the reviewer doubts if the conclusions are still valid on different datasets and architectures.	weakness
2021-746	Especially for the aspects of the architecture, different architectures have different inductive biases and may result in different phenomena.	weakness
2021-746	According to Table 2, 3 and discussion on the bottom of the 6th page, the Stylized-Edge data would increase shape bias larger than Edge.	weakness
2021-746	This is a little bit counter-intuitive due to Stylized-edge only contain more uncorrelated texture information.	weakness
2021-746	The reviewer wonders if the quality of the Edge data is not very high and result in the phenomenon.	weakness
2021-746	To verify this, the author better to do experiments on the high-quality edge data with human annotations or performing edge algs on simple data.	weakness
2021-746	Table 3 and A5 list the results of E-SIN and SE+IN.	weakness
2021-746	The reviewer wonders if there are any "intermediate" results such as E-IN, E+IN to better compare.	weakness
2021-746	Also, the reviewer is curious about the useful of superposition.	weakness
2021-746	Will the concat or mix data bring similar benefits, compared to superposition?	weakness
2021-746	Currently, to test the degree of focusing on shape information, the author runs the random-shuffled images and tests on stylized-imagenet.	weakness
2021-746	This is good but the reviewer wonders if we directly test on the edge maps of validation sets.	weakness
2021-746	To sum up, the reviewer thinks this paper would bring some new understandings to the community.	strength
2021-746	--------------after rebuttal------------------- I've read all reviews and the rebuttal, and thank the authors for their efforts and extra experiments.	misc
2021-746	I think it is ok to be accepted due to provide further understandings about the learning representation with solid experimental results.	decision
2021-746	##Updated Review## I'd like to thank the reviewers for their responses, and for updating to the much clearer naming scheme for the different methodologies!	rebuttal_process
2021-746	Much easier to follow with those names.	suggestion
2021-746	I maintain that this is high quality novel work that contradicts a widely held belief within the field and as such is a clear accept.	decision
2021-746	Main Idea The main idea is that training to reduce texture bias in convnets in favor of shape bias is often thought to cause the concomitant increase in corruption robustness, but this has not been tested directly.	weakness
2021-746	The authors propose a systemic study of this relationship and conclude that both shape bias and increase corruption robustness are both byproducts of style-variation during training, that is they share a common causal mechanism, but that shape bias does not itself cause corruption robustness.	abstract
2021-746	They accomplish this by creating a new augmented dataset which encourages learning shape features (created from the edge maps of the training images) but does not also induce robustness against common corruptions.	abstract
2021-746	Additional findings from the study: Shape bias gets maximized when edge information and stylization are combined without including any texture information.	abstract
2021-746	Corruption robustness is maximized by superimposing the image (and its textures) on the above stylized edges.	abstract
2021-746	They propose that corruption robustness seems to benefit most from style variation in the vicinity of the image manifold.	abstract
2021-746	General Strengths I think the paper makes a compelling case.	strength
2021-746	The Geirhos result, while elegant and interesting, has always struck me as a roundabout way to get to the shape bias question.	weakness
2021-746	These authors more directly attack it, and show that the relationship doesn't hold, while showing the real.	rebuttal_process
2021-746	Value of the Geirhos result was in showing that manifold-local variation increased robustness (and also consequently shape bias).	strength
2021-746	In addition to the elegance of the result, the authors use of "texture randomization via texture feature randomization" is a an elegant (and much faster) implementation than regenerating an entire dataset with many different textures.	abstract
2021-746	The insight is good, the image itself doesn't need to be rerendered, but the networks interpretation of its texture needs to be scrambled.	weakness
2021-746	This doesn't restrict you to the statistics of any database on which you might extract textures	weakness
2021-746	Weaknesses Doesn't report I-SIN results across ImageNetC corruption dataset.	weakness
2021-746	Would have preferred to see the specific corruption type results broken out for more than just the two shown and the average robustness (the variation and performance on different types of corruption could be useful information).	suggestion
2021-746	Also make sure the names are consistent across graphs and paragraphs.	suggestion
2021-746	Not the best naming scheme (too many very similar acronyms which aren't immediately evocative of the underlying point).	weakness
2021-746	The paper disproves the hypothesis that addressing shape bias improves robustness to corruptions of neural networks, which has been stated by the previous studies [1, 2].	weakness
2021-746	The paper demonstrates that the degree of shape bias of a model is not correlated with classification accuracy on corrupted images via experiments.	abstract
2021-746	For the experiments, this paper presents two novel methods to encourage CNNs to be shape-biased: 1) edge dataset and 2) style randomization (SR).	abstract
2021-746	In the experiments, the authors train CNNs to be shape-biased to various degrees based on the proposed methods.	abstract
2021-746	Additionally, they compare test accuracies of the models to evaluate shape bias and robustness to corruption.	abstract
2021-746	In addition, this paper shows that through fine-tuning the affine parameters of the normalization layers, a CNN trained on original images can achieve comparable, if not better, performance than a CNN trained with data augmentation.	abstract
2021-746	Pros: The paper clearly demonstrates that shape bias is not correlated with robustness to corrupted images.	strength
2021-746	The test accuracy on texture-shape cue conflict images effectively indicates the degree of a model's shape bias.	strength
2021-746	Also, the authors visualize the results that explicitly compare the models' accuracy on corrupted images.	strength
2021-746	The authors present an interesting analysis towards Stylized ImageNet (SIN) dataset by separating the dataset into different factors that are used to generate SIN.	strength
2021-746	According to this paper, the newly made datasets based on each factor leads a CNN to be shape-biased to various degrees.	strength
2021-746	This provides insightful perspectives on shape bias and corruption robustness.	strength
2021-746	Specifically, it is intriguing that a model trained on a stylized dataset using styles from in-distribution images achieves comparable performance compared to what uses styles from out-of-distribution images, such as paintings.	strength
2021-746	Cons: The novelty of the two proposed methods, edge dataset and SR, is limited.	weakness
2021-746	First, the edge dataset is created based on an existing model.	weakness
2021-746	The authors simply convert the non-binary edge map into a binary one.	weakness
2021-746	Furthermore, there are not enough experiments or evidence that validate the effectiveness of this method compared to the original method.	weakness
2021-746	Additionally, SR is also similar to existing methods, except that they change the target distribution from training samples' distribution to a uniform distribution.	weakness
2021-746	According to this paper, a CNN trained on edge dataset is more shape-biased but achieves lower accuracy on corrupted images compared to a CNN trained on SIN.	weakness
2021-746	The authors state that the result indicates that shape bias is not correlated to corruption robustness.	weakness
2021-746	However, it seems to be unfair to compare edge dataset and SIN to disprove the correlation between shape bias and corruption robustness.	weakness
2021-746	Edge dataset, unlike SIN, does not contain any information except edges.	weakness
2021-746	Therefore, a CNN trained on edge dataset 'learns only shape information, while a SIN-trained CNN learns other information as well, but 'focuses more on the shape'.	weakness
2021-746	Since the edge dataset provides much less information of the original images, it is trivial that a CNN trained on edge dataset achieves lower performance compared to a SIN-trained CNN.	weakness
2021-746	The previous approaches [1, 2] mentioned in this paper also imply that shape bias is basically related to how much the model focuses on shape information, not to how much shape information the model is given.	weakness
2021-746	Therefore, it would be more reasonable to compare a SIN-trained CNN with other CNNs that are trained on datasets containing a similar amount of information, but have different levels of concentration on shape information.	weakness
2021-746	[1] Geirhos et al., "ImageNet-trained CNNs are biased towards texture; increasing shape bias improves accuracy and robustness." *CONF*'19	misc
2021-746	[2] Michaelis et al., "Benchmarking Robustness in Object Detection: Autonomous Driving when Winter is Coming." *CONF*'20	misc
2021-746	After rebuttal: Thank you for the additional explanation.	rebuttal_process
2021-746	The comments by the authors effectively address my concerns.	rebuttal_process
2021-746	Although the edge dataset and SR are quite similar to the existing methods, the authors clearly present the usefulness of them as well as their additional contributions.	rebuttal_process
2021-746	Also, additional training details support the adequacy of the comparison in the experiments.	rebuttal_process
2021-746	Therefore, I would like to increase my final score to 7: Good paper, accept.	decision
2021-746	Summary of the paper: This paper tries to study whether increasing shape-bias of a neural network trained with imagined will make it more robust to common corruptions such as gaussian noises.	abstract
2021-746	The paper falsified this point by producing a data augmentation method which leads to more shape biased network yet less susceptive to common corruptios.	abstract
2021-746	The paper further hypothesize that it's the stylization augmentation that leads to increase robustness of the network by ablation studies.	abstract
2021-746	Strength: The paper does provide a counter example to the common hypothesis that increase shape bias can lead to more robust network.	strength
2021-746	This provides insight to future researches on understanding how shape-bias and texture-bias affects on neural network robustness.	strength
2021-746	If the results of the counter example is reproducible and significance, then the claim is convincing and the paper did a good job verifying such hypothesis.	strength
2021-746	Weakness: The experiment results seemed to be limited in this dataset.	weakness
2021-746	In order to make such general claim, I would expect results for different datasets (e.g. CIFAR), large scale datasets (e.g. the whole imagined), and different network and training procedure (e.g. not just resent).	suggestion
2021-746	This might be some minor things, but it would be nice if there are statistic significance test for the results (or at least show the variance of couple runs).	suggestion
2021-746	When I looked at the difference of the number, it would be nice that one can make sure such results is statistically significant with respect with difference runs.	suggestion
2021-746	Most of the paper is very empirical, and there is little insight or theory or principal ways that organize the results.	weakness
2021-746	Justifications: While I do like the paper that provide insight and show negative results falsifying some prevalent claim, but the paper provides rather limited evaluation without theoretical insights.	weakness
2021-746	Since I'm not an expert in this field, I will recommend borderline scores to hear about the authors' response.	decision
2021-746	UPdate: the authors' reply address my concerns well, so I raise my rating to the acceptance side.	rebuttal_process

2021-844	Summary This work proposed a BO based NAS method using Weisfeiler-Lehman kernel.	abstract
2021-844	The idea is novel and natural considering the neural network architectures as acyclic directed graphs.	strength
2021-844	I am a bit surprised to see no one tried it before in the NAS field and it is great to know that using WL kernel leads to competitive NAS performance comparing to other NAS methods and at the same time improves interpretability.	strength
2021-844	Pros The proposed idea is novel and natural, given the graph natures of network architecture.	strength
2021-844	The notes on the interpretability is very interesting and differ the work from other methods.	strength
2021-844	Extensive empirical studies and ablation studies.	strength
2021-844	Extensive detail for reproducibility. The paper is very well written.	strength
2021-844	Minor comments I think this is a really nice work and I only have some minor comments: There is another line of work using BO for NAS: Ru, Binxin, Pedro Esperanca, and Fabio Carlucci.	weakness
2021-844	"Neural Architecture Generator Optimization." arXiv preprint arXiv:2004.01395 (2020).	misc
2021-844	Would be nice to know how does the proposed method compared to it.	suggestion
2021-844	The appendix C mentioned about using MKL to combine WL and MLP kernels.	weakness
2021-844	But in the end the author used 0.7 and 0.3 as the weights for them.	weakness
2021-844	I am wondering whether some simple MKL algorithm such as ALIGNF  can improve the performance here.	weakness
2021-844	You can find more detail in this paper: Cortes C, Mohri M, Rostamizadeh A.	misc
2021-844	Algorithms for learning kernels based on centered alignment[J].	misc
2021-844	The Journal of Machine Learning Research, 2012, 13(1): 795-828.	misc
2021-844	Reason for score I liked this work a lot, it bridged NAS and BO through the usage of graph kernels (WL kernel).	strength
2021-844	As a result, NAS becomes more sample efficient, which is empirically verified by extensive study in this work.	strength
2021-844	The author did a very good job on the empirical evaluations, they are thorough, solid and contains many ablation studies to understand their methods.	strength
2021-844	Post rebuttal comments I thank the authors for their responses.	misc
2021-844	I encourage the authors to continue the line of work on replacing the random sampling of NAGO.	suggestion
2021-844	Given NAS-BOWL surrogate, one can do Thompson Sampling instead of random sampling.	suggestion
2021-844	On the MKL side, the same weights for all the kernels might be the cause of worse performance.	weakness
2021-844	I would also encourage the authors to verify that.	suggestion
2021-844	Nevertheless, those are minor comments and I still think this is an important work to bridge BO and NAS.	strength
2021-844	I will keep my score.	strength
2021-844	Summary of the paper The paper presents a new Bayesian optimization strategy based on Weisfeiler-Lehman Kernels for neural architecture search.	abstract
2021-844	The proposed method is more sample efficient than other state-of-the-art Bayesian optimization (BO) methods and allows to identify repeating motifs in well performing architectures.	abstract
2021-844	Overall, I really enjoyed reading the paper and I am somewhat surprised that nobody has tried this before.	strength
2021-844	The usage of the Weisfeiler-Lehman kernel is well motivated and enables users to obtain a better intuition which parts of an architecture lead to a good performance.	strength
2021-844	Also, the proposed BO method outperforms other state-of-the-art BO methods across a range of competitive benchmarks.	strength
2021-844	However a few points need to be clarified, and it would be great if the authors could address them in the rebuttal.	suggestion
2021-844	Merits The paper is clearly written and the approach is well motivated	strength
2021-844	The proposed method achieves  strong results compared to other Bayesian optimization strategies based on Gaussian process surrogate models.	strength
2021-844	Also, the paper presents a thorough empirical evaluation to other state-of-the-art BO method on well established benchmarks	strength
2021-844	As far as I know, this is the first Bayesian optimization for NAS that provides interpretable features to explain which motifs of neural networks architectures work well.	strength
2021-844	Concerns Looking at Figure 12 in the appendix, it seems that the proposed method GPWL gets its main boost from the mutation strategy used  to  optimize the acquisition function.	weakness
2021-844	This makes me wonder whether the model is actually better than, for example BANANAS, or whether the gain in performance is mostly due to the mutation strategy?	weakness
2021-844	How well would BANANAS works with this mutation strategy?	weakness
2021-844	Could you also add the plots in Figure 3 with the regret on the y-axis (as in the original papers)?	suggestion
2021-844	This would show how far away from the optimum an optimizer actually is.	suggestion
2021-844	This is somewhat hidden with ranking plots, where an optimizer might have found an architecture with a negligible performance difference to the global optimum but has a lower rank.	suggestion
2021-844	Why does start GPWL at a higher rank than the other baselines in Figure 3?	suggestion
2021-844	And why does it stop earlier?	suggestion
2021-844	Also the dashed red horizontal line is not explained in the caption.	weakness
2021-844	Post Rebuttal: I thank the authors for taking the time to address my concerns.	rebuttal_process
2021-844	The paper is well written and the proposed approached is promising.	strength
2021-844	I therefor recommend acceptance. The authors propose a new neural architecture search algorithm combining Bayesian optimization with the expressive and popular Weisfeiler-Lehman (WL) Graph Kernel.	decision
2021-844	One advantage of using WL is the interpretable results that stem from the nature of how the kernel is computed, namely a propagation scheme through the graph.	strength
2021-844	Combined the derivative of Eq. 3.2, one can extract subgraphs that are directly responsible for increased performance.	strength
2021-844	In a variety of experiments, the authors show not only increased performance of detected architectures but also find subgraphs that are found by other algorithms as well.	strength
2021-844	Even though my expertise does not lie in the field of NAS, I find this work quite appealing.	strength
2021-844	It is an innovative application for graph kernels, which suffer from scalability which in this setting is less of a problem.	strength
2021-844	I find the aspects of novelty, interpretability, and quantitative results convincing enough to recommend acceptance.	decision
2021-844	Furthermore, the work is largely well structured and written, and the figures are legible and relevant.	strength
2021-844	W.r.t whether the comparison to other SOTA NAS algorithms is of good quality and fair, I think the input from reviewers with a NAS background would be valuable.	strength
2021-844	Minor comments: •    By itself a graph kernel is a similarity measure and does not perform any subgraph selection.	weakness
2021-844	It happens that due to the WL propagation scheme, the WL graph kernel consists of interpretable features while computing the similarity.	weakness
2021-844	I would clarify this a little more in 3.1.	weakness
2021-844	Since this is, in my opinion, the most innovative part of the manuscript, I would even consider bringing Figure 5 from the appendix into the main paper (maybe in a condensed form).	suggestion
2021-844	To get some space for this you could shorten some descriptions of experiment parameters and comparison methods to the appendix.	suggestion
2021-844	Also, I think Figure 5 is not 100% complete.	weakness
2021-844	To make it even easier to parse, I would put boxes around the subgraphs in the h=1features box and annotate them with their respective index.	suggestion
2021-844	E.g. the upper graph with 4 outgoing edges should be annotated with 5 and so on.	suggestion
2021-844	This also makes it a little clearer how WL leads to larger networks in each round of propagation.	suggestion
2021-844	•    I would be interested in some statistics about the chosen h parameter (do you mostly find small subnetworks to lead to high performance?) and how it is being optimized (due to its discrete nature).	suggestion
2021-844	•    Structure: I would move 3.2 into a subsection under the experiments.	suggestion
2021-844	In general, the manuscript reads a little squished as you have a lot of references to the appendix.	weakness
2021-844	It is not easy to remedy this as you don't want the work you did go unnoticed but maybe you can leave some references out and submit a longer version of this work to a journal where you don't suffer the space constraints of a conference paper.	suggestion
2021-844	•    From Table 1 it seems like the Avg. error gains are not significant as they overlap (in terms of standard deviations) for example with the DARTS results which is not bad as you still save one day of training.	weakness
2021-844	---------- After feedback ---------- First of all, I greatly appreciate the authors patient response to me during the feedback period.	misc
2021-844	The discussion was really fruitful.	misc
2021-844	Unfortunately, I still have a concern about interpretability of the proposed method, which is a central topic in the paper.	weakness
2021-844	First, we find it a bit strange when the reviewer says "it is difficult to find importance/meaning of comparing motifs is unclear", we clearly show our method does find importance in NAS-Bench-101, all 3 tasks of NAS-Bench-201 and DARTS search space (Fig 1 and	weakness
2021-844	7) -- if we can't find importance/distinguish different motifs, none of the results we've shown would've been possible.	weakness
2021-844	Even when a method works empirically, if a rationale behind the procedure is not clarified, a paper would not be scientifically convincing.	weakness
2021-844	Thus, I still do not think my claim is strange.	weakness
2021-844	Second, the example the reviewer gives is not a case when averaged gradient fails.	weakness
2021-844	On the contrary, it is exactly an example of when averaged gradient works.	weakness
2021-844	A motif with high and diverse local gradient magnitudes but average to near-0 is not important for the purpose of interpretability, as it doesn't consistently explain the network performance by itself (just based on such motifs, one cannot conclusively deduce the impact on performance of an arbitrary, unseen architecture in general) ...	weakness
2021-844	In the last response, the authors explained the interpretability issue through combination of motifs, but it did not resolve my concern.	rebuttal_process
2021-844	To simplify the discussion, consider a bit extreme case in which only one motif is employed in a network simultaneously, and assume WL parameter h = 0.	rebuttal_process
2021-844	Let g(c) = d \\mu / d \\phi^j |_\\phi^j=c.	rebuttal_process
2021-844	Then, consider a hypothetical case as follows: motif a) g(1) = 10, g(2) = 10 ...	rebuttal_process
2021-844	g(10) = 10, g(11) = -10, ....	rebuttal_process
2021-844	g(20) = -10 : AG = 0	rebuttal_process
2021-844	motif b) g(1) = 1, g(2) = 1 ...	rebuttal_process
2021-844	g(10) = 1, g(11) = 1, ....	rebuttal_process
2021-844	g(20) = 1 : AG = 20	rebuttal_process
2021-844	In this example, b) has a larger AG, but a) can have larger importance in practice, and now, since only one kind of motif is employed simultaneously, the explanation of the authors cannot be applied.	rebuttal_process
2021-844	For the exploration purpose, I do not find any rationale to consider that b) is more important than a).	rebuttal_process
2021-844	I know that these are extreme examples and may depend on an application scenario, but my point is that these examples reveal difficulty of interpretation of AG.	weakness
2021-844	The authors introduce AG at Section3.2 as an importance measure without carefully discussing how it can be interpreted in the context of the WL based exploration (just referring other papers without discussing details in a sense of the above averaging).	weakness
2021-844	The explanation through marginalization also does not get rid of this question.	weakness
2021-844	Since the interpretability is a main theme of the paper, providing a better interpretability of AG would be desired.	suggestion
2021-844	---------- Before feedback ---------- The paper proposes to use Weisfeiler-Lehman (WL) kernels for neural architecture search.	abstract
2021-844	WL kernel can incorporate the topological structure of the network, and the authors combines WL kernels with Bayesian optimization (BO) to optimize the validation performance of the network.	abstract
2021-844	Further, the authors also claim that WL kernels provide a useful interpretation about good / bad network structures by using the derivative of Gaussian process (GP), which can also be used for 'pruning' architectures.	abstract
2021-844	The performance is shown for several benchmark datasets.	abstract
2021-844	Overall, the idea would be reasonable, and the approach would be useful.	strength
2021-844	However, I'd have to say that the technical novelty and depth would be somewhat weak because the standard WL kernel is directly used without any significant modification, and a gradient-based importance evaluation is also a known technique (and its interpretation in this context is a bit difficult).	weakness
2021-844	Further, in my understanding, the paper should have provided more general discussions, not only to show data-specific observations.	weakness
2021-844	Detailed comments are as follows.	misc
2021-844	The proposal of the paper is not fully clear for me because the strategies are described for each one of datasets, separately.	weakness
2021-844	I couldn't find general procedures for the architecture search, from the main text of the paper.	weakness
2021-844	In practice, of-course, tuning on each dataset would be required, but showing specific tunings for well-known benchmark datasets is not attractive.	weakness
2021-844	A strategy applicable to wide range of tasks would be required for a methodology paper.	weakness
2021-844	Interpretation of the gradient-based motif identification is difficult for me.	weakness
2021-844	Even when a motif has a large positive or negative gradient value, it only implies 'local' importance around the given architecture.	weakness
2021-844	To derive general insight, more careful treatment would be required.	weakness
2021-844	The authors provide the motif discovery procedure in D.1, but it should be shown in main text because interpretability is one of main theme of this paper.	weakness
2021-844	In Section D.1, the authors described an approach taking average of all possible values, but the average is also difficult to interpret importance because it compresses the entire space, and as the author admitted, the computation would be often intractable in practical settings.	weakness
2021-844	Although the authors claim that good/bad motif identification is useful for 'pruning', no detailed general pruning procedure is shown in the main text.	weakness
2021-844	Providing a general algorithm would be required.	weakness
2021-844	What does 'prune' mean in this context?	weakness
2021-844	If a motif is regarded as 'bad' once, it is discarded forever, or can revive somewhere?	weakness
2021-844	As I mentioned above, gradient information is only local information.	weakness
2021-844	Even when a motif is 'bad', simply discarding it completely would be risky.	weakness
2021-844	Even when the 'average' gradient is used, the problem would not be mitigated, because even if the average gradient suggests a motif is useless, it may help to improve accuracy locally.	weakness
2021-844	For me, the rationale behind the exploration strategy with the pruning in the paper is quite unclear.	weakness
2021-844	The authors claim that the identified motif is trasferable.	weakness
2021-844	However, evidence of this claim is not fully clear.	weakness
2021-844	It seems empirical suggestions only from a few (similar) datasets.	weakness
2021-844	When is transferring effective, how do you know it holds when, and can it harm in some case?	weakness
2021-844	I think that a general discussion is missing in the paper.	weakness
2021-844	Another difficulty of the gradient-based importance evaluation is that the lack of uncertainty evaluation.	weakness
2021-844	The gradient (3.2) is the expected value of the predictive distribution of GP.	weakness
2021-844	Therefore, the variance is not considered.	weakness
2021-844	For example, if GP does not have any observations, the expected gradient would be 0 (when prior is f(x) = 0 with the unit variance for any x, which is a standard setting), but variance of gradient would be large, meaning that a motif is still has a potential to become important.	weakness
2021-844	Again, discarding a motif by the expected gradient without considering uncertainty is seemingly risky, though the paper lacks this kind of discussion on uncertainty, though the uncertainty evaluation is a central issue on in the context of BO.	weakness

2021-1615	The paper establish the average leave-one-out stability bound for the interpolation solutions, and show the above bound depends on condition number and spectral norm of kernel matrices.	abstract
2021-1615	The authors establishes a nice connection between numerical and statistical stability.	abstract
2021-1615	A nice property is that among all interpolation solutions, the upper bound on stability achieves the minimum at the solution with the minimal norm.	strength
2021-1615	The authors then comment that the interpolation solution with minimal norm may generalize better than other interpolation solutions.	strength
2021-1615	The paper is clearly and well written.	strength
2021-1615	Comments: In Theorem 7, the authors show that the stability can be bounded by the spectral norm of K,K\\dag, the condition number of K and the norm of y.	weakness
2021-1615	It seems that this upper bound would diverge as we increase the dimension or sample size.	weakness
2021-1615	This means that the upper bound is vacuous and may not explain the true generalization behavior of the interpolation solutions.	weakness
2021-1615	If the upper bound is loose, then even if the interpolation solution with the minimal norm achieves the minimal upper bound, this may not convincingly show that it outperforms other interpolation solutions.	weakness
2021-1615	I have doubts on eq (10).	weakness
2021-1615	I think the left-hand side and right-hand side of the third identity differ by the term 2K\\dagKvi. If K\\dagKvi≠0, then this identity would not hold.	weakness
2021-1615	Since vi can be any vector, the deduction is not convincing.	weakness
2021-1615	I would suggest the authors to take a close look at it.	suggestion
2021-1615	Lemma 5 is standard in the literature.	suggestion
2021-1615	I would suggest the authors to indicate its connection with existing results, e.g., Lemma 11 in "Learnability, Stability and Uniform Convergence" UPDATE: As the authors were already aware of the zero loss case and analyzed this previously, I am confident that the authors can address this to the point in an updated version.	suggestion
2021-1615	With this I think this is a good paper that should be accepted.	decision
2021-1615	########################## Summary: The papers addresses the setting of overparametrized models that interpolate the training data, and the related double descent observation in a kernel setting.	abstract
2021-1615	The overparametrized case of interpolating models is not yet that well understood, but of importance as the success of neural networks is closely related to that setting.	abstract
2021-1615	This paper shows that the minimum norm interpolating solution is optimal (among all interpolating solutions) with respect to a derived bound on the expected leave-one-out stability, and thus also optimal in the same sense with respect to the excess risk.	abstract
2021-1615	########################## ########################## Pros: The paper is well written, to the point, and technically (mostly!	strength
2021-1615	see cons) sound. To the best of my knowledge this particular stability analysis is novel, and thus warrants a publication, in particular as the overparametrized case is not that well understood as of yet.	decision
2021-1615	########################## ########################## Cons: I actually don't have many cons, I enjoyed the paper.	misc
2021-1615	There is however one thing that the authors seemed to have missed: The term V(hat(f_S),z_i) is zero, as hat(f_S) interpolates the training data and z_i is part of it.	weakness
2021-1615	That doesn't mean that any of the theory is wrong, but that creates two problems in my opinion: The story about leave-one-out stability does not make sense anymore.	weakness
2021-1615	In fact the expected leave-one-out stability is just the expected risk for interpolating solutions.	weakness
2021-1615	I would imagine that most of the results can be simplified because of that.	weakness
2021-1615	I could imagine that all the results hold with essentially all terms regarding hat(f_s) being removed.	weakness
2021-1615	I think the qualitative conclusions would remain the same though.	weakness
2021-1615	My suggestion would be to leave the paper as is (the results as far as I see also hold for not interpolating solutions), and then discuss the interpolating solutions as an extra case.	suggestion
2021-1615	########################## ########################## Scoring: For now I will have to vote for a rejection, as I am not sure if the problem that I mention can be addressed in one rebuttal phase.	decision
2021-1615	But I am happily convinced otherwise, or convinced that I am wrong in any other way.	weakness
2021-1615	########################## ########################## Additional feedback: When I first read the title I thought that you wanted to show minimal norm solutions are NOT stable, as it has 'minimal' stability.	weakness
2021-1615	I understand now that minimal refers to the numerical value of the stability definition, but still the wording was somewhat confusing.	weakness
2021-1615	(Just to consider, no need to change for me if you think it is correct like that)	weakness
2021-1615	Equation (2) and also bit later.	weakness
2021-1615	You use a comma to separate an index "S,i", fairly unusual I would say.	weakness
2021-1615	Remark 4, rework first sentence.	weakness
2021-1615	Equation (7), in the very basic property of RKHS the kappa would depend on x, for you that seems to follow with one of your assumptions + cauchy-schwarz.	weakness
2021-1615	I would not consider that a basic property.	weakness
2021-1615	########################## This paper investigates kernel ridge-less regression from a stability viewpoint by deriving its risk bounds.	abstract
2021-1615	Using stability arguments to derive risk bounds have been widely adopting in machine learning.	rebuttal_process
2021-1615	However, related studies on kernel ridge-less regression are still sparse.	rebuttal_process
2021-1615	The present study fills this gap, which, in my opinion, is also one of the main contributions of the present study.	strength
2021-1615	Pros: As mentioned above, this study presents some novel research into kernel ridge-less regression from a stability viewpoint.	strength
2021-1615	The study presented here brings some novel insights into the relationship between minimizing the norm of the ERM solution and minimizing a bound on stability and also reveals the role that the condition number of the kernel matrix plays in kernel ridge-less regression, see formula (6).	abstract
2021-1615	The paper is well presented and well polished.	strength
2021-1615	The analysis conducted in this paper seems to be sound.	strength
2021-1615	Just one minor concern: what would happen if the boundedness assumption on the output variable, which excludes the most common Gaussian noise, is not imposed?	strength
2021-1615	I understand that this condition is common in learning theory but are expecting more comments.	misc
2021-1615	In this paper, they provide the risk bounds of  kernel ridge-less regression (the regularization λ−→0) based on the CV_{loo} stability.	abstract
2021-1615	They show  that the interpolating solution with minimum norm is the minimal bound of CV_{loo} stability, and can be controlled by the condition number of the empirical kernel matrix, which establishes an  elegant link between numerical and statistical stability.	abstract
2021-1615	Pros: -1:  The stability of Tikhonov regularization has been well studied, but the study for the unregularized regression probelms is lacking.	strength
2021-1615	This paper fills the gap for unregularized regression.	abstract
2021-1615	-2:  They provide an upper bound on the stability of interpolating solutions, and show that among all interpolating solutions, the minimum norm solution has the best test error.	abstract
2021-1615	They further provide emprical experiments to verify their theoretical findings.	abstract
2021-1615	-3: They show that CV_{loo} stability can be bounded by the condition number of the empirical kernel matrix, which establishes an  elegant link between numerical and statistical stability.	abstract
2021-1615	Cons: In Theorem 7, the CV_{loo} stability of minimum norm solutions  can be bound by C1β1+Cβ2.	weakness
2021-1615	Note that the kernel matrix K converges to the integral operator LK:LK(f)(x)=∫Xf(x)K(x,y)dy when n→∞, so |K1/2|  and |K+|  may converge to  two constants.	weakness
2021-1615	Note that the  condition number of the empirical kernel matrix cond(K)≥1,  and |y| grows with the increase of n, thus the β1 and β2 may grow  with the the increase of n.	weakness
2021-1615	However, intuitively,  we think the CV_{loo} stability should decrease with the increase of n.	weakness
2021-1615	So, we think the upper bound proposed in this paper may be too loose.	weakness
2021-1615	If you can answer my concern at this point, I will increase my score.	suggestion

