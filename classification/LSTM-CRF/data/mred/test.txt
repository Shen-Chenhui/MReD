2018-4	High quality paper, appreciated by reviewers, likely to be of substantial interest to the community.	B-strength
2018-4	It's worth an oral to facilitate a group discussion.	B-decision

2018-5	Important problem (analyzing the properties of emergent languages in multi-agent reference games), a number of interesting analyses (both with symbolic and pixel inputs), reaching a finding that varying the environment and restrictions on language result in variations in the learned communication protocols (which in hindsight is that not surprising, but that's hindsight).	B-strength
2018-5	While the pixel experiments are not done with real images, it's an interesting addition the literature nonetheless.	I-strength

2018-38	This paper presents a sampling inference method for learning in multi-modal demonstration scenarios.	B-abstract
2018-38	Reference to imitation learning causes some confusion with the IRL domain, where this terminology is usually encountered.	B-weakness
2018-38	Providing a real application to robot reaching, while a relatively simple task in robotics, increases the difficulty and complexity of the demonstration.	I-weakness
2018-38	That makes it impressive, but also difficult to unpick the contributions and reproduce even the first demonstration.	I-weakness
2018-38	It's understandable at a meeting on learning representations that the reviewers wanted to understand why existing methods for learning multi-modal distributions would not work, and get a better understanding of the tradeoffs and limitations of the proposed method.	I-weakness
2018-38	The CVAE comparison added to the appendix during the rebuttal period just pushed this paper over the bar.	B-rebuttal_process
2018-38	The demonstration is simplified, so much easier to reproduce, making it more feasible others will attempt to reproduce the claims made here. <sep>	B-strength

2018-41	The paper presents a modification of the Winograd convolution algorithm that reduces the number of multiplications in a forward pass of a CNN with minimal loss of accuracy.	B-abstract
2018-41	The reviewers brought up the strong results, the readability of the paper, and the thoroughness of the experiments.	B-strength
2018-41	One concern brought up was the applicability to deeper network structures.	B-rebuttal_process
2018-41	This was acknowledged by the authors to be a subject of future work.	I-rebuttal_process
2018-41	Another issue raised was the question of theoretical vs. actual speedup.	I-rebuttal_process
2018-41	Again, this was acknowledged by the authors to be an eventual goal but subject to further systems work and architecture optimizations.	I-rebuttal_process
2018-41	The reviewers were consistent in their support of the paper.	B-rating_summary
2018-41	I follow their recommendation: Accept. <sep>	B-decision

2018-57	The paper proposes a unique network architecture that can learn divide-and-conquer strategies to solve algorithmic tasks, mimicking a class of standard algorithms.	B-abstract
2018-57	The paper is clearly written, and the experiments are diverse.	B-strength
2018-57	It also seems to point in the direction of a wider class of algorithm-inspired neural net architectures.	I-strength

2018-80	The reviewers concerns regarding novelty and the experimental evaluation have been resolved accordingly and all recommend acceptance.	B-rating_summary
2018-80	I would recommend removing the term "unsupervised" in clustering, as it is redundant.	B-suggestion
2018-80	Clustering is, by default, assumed to be unsupervised. <sep>	I-suggestion
2018-80	There is some interest in extending this to non-vision domains, however this is beyond the scope of the current work.	I-suggestion

2018-91	Thank you for submitting you paper to ICLR.	O
2018-91	The reviewers are all in agreement that the paper is suitable for publication, each revising their score upwards in response to the revision that has made the paper stronger. <sep>	B-rating_summary
2018-91	The authors may want to consider adding a discussion about whether the simple standard Gaussian prior, which is invariant under transformation by an orthogonal matrix, is a sensible one if the objective is to find disentangled representations.	B-suggestion
2018-91	Alternatives, such as sparse priors, might be more sensible if a model-based solution to this problem is sought.	I-suggestion

2018-92	Thank you for submitting you paper to ICLR.	O
2018-92	The idea is simple, but easy to implement and effective.	B-strength
2018-92	The paper examines the performance fairly thoroughly across a number of different scenarios showing that the method consistently reduces variance.	B-abstract
2018-92	How this translates into final performance is complex of course, but faster convergence is demonstrated and the revised experiments in table 2 show that it can lead to improvements in accuracy.	B-rebuttal_process

2018-95	This paper analyzes mathematically why weights of trained networks can be replaced with ternary weights without much loss in accuracy.	B-abstract
2018-95	Understanding this is an important problem, as binary or ternary weights can be much more efficient on limited hardware, and we've seen much empirical success of binarization schemes.	I-abstract
2018-95	This paper shows that the continuous angles and dot products are well approximated in the discretized network.	I-abstract
2018-95	The paper concludes with an input rotation trick to fix discretization failures in the first layer. <sep>	I-abstract
2018-95	Overall, the contribution seems substantial, and the reviewers haven't found any significant issues.	B-strength
2018-95	One reviewer wasn't convinced of the problem's importance, but I disagree here.	B-ac_disagreement
2018-95	I think the paper will plausibly be helpful for guiding architectural and algorithmic decisions.	B-strength
2018-95	I recommend acceptance. <sep>	B-decision

2018-99	Meta score: 6 <sep>	B-decision
2018-99	The paper approaches the problem of identifying out-of-distribution data by modifying the objective function to include a generative term.	B-abstract
2018-99	Experiments on a number of image datasets. <sep>	I-abstract
2018-99	Pros: <sep> - clearly expressed idea, well-supported by experimentation <sep>	B-strength
2018-99	- good experimental results <sep>	I-strength
2018-99	- well-written <sep>	I-strength
2018-99	Cons: <sep> - slightly limited novelty <sep>	B-weakness
2018-99	- could be improved by linking to work on semi-supervised learning approaches using GANs <sep>	B-suggestion
2018-99	The authors note that ICLR submission 267 (https://openreview.net/forum?id=H1VGkIxRZ) covers similar ground to theirs.	B-rebuttal_process

2018-104	This paper presents a new multi-document summarization task of trying to write a wikipedia article based on its sources.	B-abstract
2018-104	Reviewers found the paper and the task clear to understand and well-explained.	B-strength
2018-104	The modeling aspects are clear as well, although lacking justification.	I-strength
2018-104	Reviewers are split on the originality of the task, saying that it is certainly big, but wondering if that makes it difficult to compare with.	B-weakness
2018-104	The main split was the feeling that "the paper presents strong quantitative results and qualitative examples. "	I-weakness
2018-104	versus a frustration that the experimental results did not take into account extractive baselines or analysis.	I-weakness
2018-104	However the authors provide a significantly updated version of the work targeting many of these concerns, which does alleviate some of the main issues.	B-rebuttal_process
2018-104	For these reasons, despite one low review, my recommendation is that this work be accepted as a very interesting contribution. <sep>	B-decision

2018-122	The reviewers find the work interesting and well made, but are concerned that ICLR is not the right venue for the work.	B-rating_summary
2018-122	I will recommend that the paper be accepted, but ask the authors to add the NMT results to the main paper (any other non-synthetic applications they could add would be helpful).	B-decision

2018-125	This paper adapts (Nachum et al 2017) to continuous control via TRPO.	B-abstract
2018-125	The work is incremental (not in the dirty sense of the word popular amongst researchers, but rather in the sense of "building atop a closely related work"), nontrivial,  and shows empirical promise.	B-strength
2018-125	The reviewers would like more exploration of the sensitivity of the hyper-parameters.	B-suggestion

2018-127	Some reviewers seem to assign novelty to the compression and classification formulation; however, semi-supervised autoencoders have been used for a long time.	B-ac_disagreement
2018-127	Taking the compression task more seriously as is done in this paper is less explored. <sep>	B-strength
2018-127	The paper provides some extensive experimental evaluation and was edited to make the paper more concise at the request of reviewers.	B-rebuttal_process
2018-127	One reviewer had a particularly strong positive rating, due to the quality of the presentation, experiments and discussion.	B-rating_summary
2018-127	I think the community would like this work and it should be accepted. <sep>	B-decision

2018-130	A well written paper proposing some reasonable approaches to counter adversarial images.	B-strength
2018-130	Proposed approaches include non-differentiable and randomized methods.	B-abstract
2018-130	Anonymous commentators pushed upon and cleared up some important issues regarding white, black and gray "box" settings.	B-weakness
2018-130	The approach appears to be a plausible defence strategy.	B-strength
2018-130	One reviewers is a hold out on acceptance, but is open to the idea.	B-rating_summary
2018-130	The authors responded to the points of this reviewer sufficiently.	B-rebuttal_process
2018-130	The AC recommends accept.	B-decision

2018-131	This paper explores what might be characterized as an adaptive form of ZoneOut. <sep>	B-abstract
2018-131	With the improvements and clarifications added to the paper during the rebuttal the paper could be accepted. <sep>	B-decision

2018-138	The paper presents an interesting view of ResNets and the findings should be of broad interest.	B-strength
2018-138	R1 did not update their score/review, but I am satisfied with the author response, and recommend this paper for acceptance.	B-decision

2018-141	Initially this paper received mixed reviews.	O
2018-141	After reading the author response, R1 and and R3 recommend acceptance. <sep>	B-rating_summary
2018-141	R2, who recommended rejecting the paper, did not participate in discussions, did not respond to author explanations, did not respond to AC emails, and did not submit a final recommendation.	B-rebuttal_process
2018-141	This AC does not agree with the concerns raised by R2 (eg I don't find this model to be unprincipled). <sep>	B-ac_disagreement
2018-141	The concerns raised by R1 and R3 were important (especially eg comparisons to NMS) and the authors have done a good job adding the required experiments and providing explanations. <sep>	B-rebuttal_process
2018-141	Please update the manuscript incorporating all feedback received here, including comparisons reported to the concurrent ICLR submission on counting.	B-suggestion

2018-149	this work adapts cycle GAN to the problem of decipherment with some success.	B-abstract
2018-149	it's still an early result, but all the reviewers have found it to be interesting and worthwhile for publication.	B-rating_summary

2018-150	this submission proposes an efficient parametrization of a recurrent neural net by using two transition functions (one large and one small) to reduce the amount of computation (though, without actual improvement on GPU.)	B-abstract
2018-150	the reviewers found the submission very positive. <sep>	B-rating_summary
2018-150	please, do not forget to include all the result and discussion on the proposed approach's relationship to VCRNN which was presented at the same conference just a year ago.	B-suggestion

2018-162	Paper presents a way in which linear RNNs can be computed (fprop, bprop) using parallel scan.	B-abstract
2018-162	They show big improvements in speedups and show application on really long sequences.	I-abstract
2018-162	Reviews were generally favorable.	B-rating_summary

2018-167	+ Empirically convincing and clearly explained application: a novel deep learning architecture and approach is shown to significantly outperform state-of-the-art in unsupervised anomaly detection. <sep>	B-strength
2018-167	- No clear theoretical foundation and justification is provided for the approach <sep>	B-weakness
2018-167	- Connexion and differentiation from prior work on simulataneous learning representation and fitting a Gaussian mixture to it would deserve a much more thorough discussion / treatment. <sep>	I-weakness

2018-182	This paper initially received borderline reviews.	B-rating_summary
2018-182	The main concern raised by all reviewers was a limited experimental evaluation (synthetic only).	B-weakness
2018-182	In rebuttal, the authors provided new results on the CelebA dataset, which turned the first reviewer positive.	B-rebuttal_process
2018-182	The AC agrees there is merit to this approach, and generally appreciates the idea of compositional concept learning.	B-strength

2018-190	The paper extends the earlier work on Prototypical networks to semi-supervised setting.	B-abstract
2018-190	Reviewers largely agree that the paper is well-written.	B-strength
2018-190	There are some concerns on the incremental nature of the paper wrt to the novelty aspect but in the light of reported empirical results which show clear improvement over earlier work and given the importance of the topic, I recommend acceptance.	B-decision

2018-194	The paper received mostly positive comments from experts.	B-rating_summary
2018-194	To summarize: <sep> Pros: <sep> -- The paper provides complex counterparts for typical architectures / optimization strategies used by real valued networks. <sep>	B-strength
2018-194	Cons: <sep> -- Although the authors include plots explaining how nonlinearities transform phase, intuition about how phase gets processed can be improved. <sep>	B-weakness
2018-194	-- Improving evaluations: Wisdom et al computes log magnitude; real valued networks may not be suited for computing real / complex numbers which have a large dynamic range, like the complex spectra.	I-weakness
2018-194	So please compare performance by estimating magnitude as in Wisdom et al <sep>	B-suggestion
2018-194	-- Please add computational cost, in terms of the number of multiplies and adds, to the final version of the paper. <sep>	I-suggestion
2018-194	I am recommending that the paper be accepted based on these reviews. <sep>	B-decision

2018-198	The idea of using wavelet pooling is novel and will bring many interesting research work in this direction.	B-strength
2018-198	But more thorough experimental justification such as those recommended by the reviewers would make the paper better.	B-suggestion
2018-198	Overall, the committee feels this paper will bring value to the conference.	B-decision

2018-210	Pros: <sep> + Clearly written paper. <sep>	B-strength
2018-210	+ Easily implemented algorithm that appears to have excellent scaling properties and can even improve on validation error in some cases. <sep>	I-strength
2018-210	+ Thorough evaluation against the state of the art. <sep>	I-strength
2018-210	Cons: <sep> - No theoretical guarantees for the algorithm. <sep>	B-weakness
2018-210	This paper belongs in ICLR if there is enough space. <sep>	B-decision

2018-235	Authors propose an approach to generation of adversarial examples that jointly examine the effects to classification within a local neighborhood, to yield a more robust example.	B-abstract
2018-235	This idea is taken a step further for defense, whereby the classification boundaries within a local neighborhood of a presented example are examined to determine if the data was adversarially generated or not. <sep>	I-abstract
2018-235	Pro: <sep> - The idea of examining local neighborhoods around data points appears new and interesting. <sep>	B-strength
2018-235	- Evaluation and investigation is thorough and insightful. <sep>	I-strength
2018-235	- Authors made reasonable attempts to address reviewer concerns. <sep>	I-strength
2018-235	Con <sep> - Generation of adversarial examples an incremental improvement over prior methods	B-weakness

2018-241	The paper provides an interesting take on GAN training based on Coulomb dynamics.	B-abstract
2018-241	The proposed formulation is theoretically well motivated and authors provide guarantees for convergence.	I-abstract
2018-241	Reviewers agree that the theoretical analysis is interesting but are not completely impressed by the results.	B-weakness
2018-241	The method addresses mode collapse issue but still lacks in sample quality.	I-weakness
2018-241	Nevertheless, reviewers agree that this is a good step towards the understanding of GAN training.	B-strength

2018-250	PROS: <sep> 1. Interesting and clearly useful idea <sep>	B-strength
2018-250	2. The paper is clearly written. <sep>	I-strength
2018-250	3. This work doesn't seem that original from an algorithmic point of view since Reed & De Freitas (2015) and Cai et al (2017) among others have considered using execution traces.	B-weakness
2018-250	However the application to program repair is novel (as far as I know). <sep>	B-strength
2018-250	4. This work can be very useful for an educational platform though a limitation is the need for adding instrumentation print statements by hand. <sep>	I-strength
2018-250	CONS: <sep> 1. The paper has some clarity issues which the authors have promised to fix. <sep>	B-weakness
2018-250	---	O

2018-253	PROS: <sep> 1. Overall, the paper is well-written, clear in its exposition and technically sound. <sep>	B-strength
2018-253	2. With some caveats, an independent team concluded that the results were "largely reproducible" <sep>	I-strength
2018-253	3. The key idea is a smart evolution scheme.	I-strength
2018-253	It circumvents the traditional tradeoff between search space size and complexity of the found models. <sep>	I-strength
2018-253	4. The implementation seems technically sound. <sep>	I-strength
2018-253	CONS: <sep> 1. The results were a bit over-stated (the authors promise to correct) <sep>	B-weakness
2018-253	2. Could benefit from more comparison with other approaches (eg RL)	I-weakness

2018-254	PROS: <sep> 1. well-written and clear <sep>	B-strength
2018-254	2. added extra comparison to dagger which shows success <sep>	I-strength
2018-254	3. SOTA results on open ai benchmark problem and comparison to relevant related work (Shi 2017) <sep>	I-strength
2018-254	4. practical applications <sep>	I-strength
2018-254	5. created new dataset to test harder aspects of the problem <sep>	I-strength
2018-254	CONS: <sep> 1. the algorithmic novelty is somewhat limited <sep>	B-weakness
2018-254	2. some indication of scalability to real-world tasks is provided but it is limited	I-weakness

2018-256	This paper extends last year's paper on PATE to large-scale, real-world datasets.	B-abstract
2018-256	The model works by training multiple "teacher" models -- one per dataset, where a dataset might be for example, one user's data -- and then distilling those models into a student model.	I-abstract
2018-256	The teachers are all trained on disjoint data.	I-abstract
2018-256	Differential privacy is guaranteed by aggregating the teacher responses with added noise.	I-abstract
2018-256	The paper shows improved teacher consensus by adding more concentrated noise and allowing the teacher to simply not respond to a student query.	I-abstract
2018-256	The new results beat the old results convincingly on a variety of measures. <sep>	B-strength
2018-256	Quality and Clarity: The reviewers and I thought the paper was well written. <sep>	I-strength
2018-256	Originality: In some sense this work is incremental, extending and improving the existing PATE framework.	I-strength
2018-256	However, the extensions and new analysis are non-trivial and the results are good. <sep>	I-strength
2018-256	PROS: <sep> 1. Well written though difficult in places for somebody like myself who is not involved in this area. <sep>	I-strength
2018-256	2. Much improved scalability to real datasets <sep>	I-strength
2018-256	3. Good theoretical analysis supporting the extensions. <sep>	I-strength
2018-256	4. Comparison to related work (with a new comparison to UCI medical datasets used in the original paper and better results) <sep>	I-strength
2018-256	CONS: <sep> 1. Perhaps a little dense for the non-expert <sep>	B-weakness

2018-257	The effectiveness of active learning techniques for training modern deep learning pipelines in a label efficient manner is certainly a very well motivated topic.	B-strength
2018-257	The reviewers unanimously found the contributions of this paper to be of interest, particularly nice empirical gains over several natural baselines.	I-strength

2018-267	Reviewers agree that the paper is well done and addresses an interesting problem, but uses fairly standard ML techniques. <sep>	B-weakness
2018-267	The authors have responded to rebuttals with careful revisions, and improved results.	B-rebuttal_process

2018-285	This paper offers a theoretical and empirical analysis of the expressivity of RNNs, in particular in comparison to TT decomposition.	B-abstract
2018-285	The reviewers argued the results was interesting and important, although there were issues with clarity of some of the explanations.	B-strength
2018-285	More critical reviewers argued the comparison basis with CP networks was not "fair" in that their shallowness restricted their expressivity wrt TT.	B-weakness
2018-285	The experiments could be strengthened by making the explanations surrounding the set up clearer.	B-suggestion
2018-285	This paper is borderline acceptable, and would have benefited from a more active discussion between the reviewers and the author.	B-decision
2018-285	From reading the reviews and the author responses, I am leaning towards recommending acceptance to the main conference rather than the workshop track, as it is important to have theoretical work of this nature discussed at ICLR.	I-decision

2018-298	This is a very interesting paper that also seems a little underdeveloped.	B-weakness
2018-298	As noted by the reviewers, it would have been nice to see the idea applied to domains requiring function approximation to confirm that it can scale -- the late addition of Freeway results is nice, but Freeway is also by far the simplest exploration problem in the Atari suite.	B-suggestion
2018-298	There also seems to be a confusion between methods such as UCB, which explore/exploit, and purely exploitative methods.	B-weakness
2018-298	The case gamma_E > 0 is also less than obvious.	I-weakness
2018-298	Given the theoretical leanings of the paper, I would strongly encourage the authors to focus on deriving an RMax-style bound for their approach. <sep>	B-suggestion

2018-302	The reviewers agree that the problem of learning learning credit assignment from terminal rewards is interesting, and that the presented approach is promising.	B-strength
2018-302	There are some concerns regarding the rigor and correctness of the theoretical results, and I ask the authors to improve those aspects of the paper.	B-weakness
2018-302	I also ask the authors to the result figures easier to read.	I-weakness
2018-302	The chosen colors are not ideal and the use of log-scale x-axis is not standard.	I-weakness
2018-302	Finally, including DAgger in the same plot is confusing assuming that DAgger user more information.	I-weakness

2018-303	Given the changes to the paper, the reviewers agree that the paper meets the bar for publication at ICLR.	B-decision
2018-303	There are some concerns regarding the practical impact on CPUs and GPUs.	B-weakness
2018-303	I ask the authors to clearly discuss the impact on different hardware.	B-suggestion
2018-303	One can argue if adaptive quantization techniques are helpful, then there is a chance that future hardware will support them.	I-suggestion
2018-303	All of the experiments are conducted on toy datasets.	B-weakness
2018-303	Please consider including some experiments on Imagenet as well.	B-suggestion

2018-307	Dear authors, <sep>	O
2018-307	The reviewers all appreciated your work and agree that this a very good first step in an interesting direction.	B-strength

2018-315	This paper is a timely application of linear algebra to propose a method for reducing catastrophic interference by training a new task in a subspace of the parameter space using conceptors.	B-abstract
2018-315	The conceptors are deployed in the backprop, making this a valuable alternative to recent continual learning methods such as EWC.	I-abstract
2018-315	The paper is clearly written and the results give a clear validation of the method.	B-strength
2018-315	The reviewers agree as to the merits of the paper.	O

2018-329	The paper provides a useful analysis of the role of gradient penalties and the performance of the proposed approach in semi-supervised cases.	B-abstract

2018-344	The pros and cons of the paper under consideration can be summarized below: <sep>	O
2018-344	Pros: <sep> * Reviewers thought the underlying model is interesting and intuitive <sep>	B-strength
2018-344	* Main contributions are clear <sep>	I-strength
2018-344	Cons: <sep> * There is confusion between keywords and topics, which is leading to a somewhat confused explanation and lack of clear comparison with previous work.	B-weakness
2018-344	Because of this, it is hard to tell whether the proposed approach is clearly better than the state of the art. <sep>	I-weakness
2018-344	* Typos and grammatical errors are numerous <sep>	I-weakness
2018-344	As the authors noted, the concerns about the small dataset are not necessarily warranted, but I would encourage the authors to measure the statistical significance of differences in results, which would help alleviate these concerns. <sep>	B-suggestion
2018-344	An additional comment: it might be worth noting the connections to query-based or aspect-based summarization, which also have a similar goal of performing generation based on specific aspects of the content. <sep>	I-suggestion
2018-344	Overall, the quality of the paper as-is seems to be somewhat below the standards of ICLR (although perhaps on the borderline), but the idea itself is novel and results are good.	B-decision
2018-344	I am not recommending it for acceptance to the main conference, but it may be an appropriate contribution for the workshop track.	I-decision

2018-351	This submission is a continuation of a line of theoretical work that seeks to characterize optimization landscapes of neural networks by the presence or absence of spurious local minima.	B-abstract
2018-351	As the number of critical points grows combinatorially for larger networks, it is very challenging to show such results.	I-abstract
2018-351	The present submission extends slightly previous work by considering two hidden units and their proof technique goes beyond that of Brutzkus and Globerson, 2017, potentially leading to more interesting results if they can be extended to more complex networks. <sep>	I-abstract
2018-351	The setting of two hidden units is quite limited - far from any practical setting.	B-weakness
2018-351	If this were the stepping stone to proving optimality of certain optimization strategies for more complex networks, this may be of some interest, but it seems doubtful.	I-weakness
2018-351	One indication is given in sec 7 / Fig.	I-weakness
2018-351	1 in which it is shown that for even quite small numbers of hidden units, spurious local optima do occur and are reached 40% of the time for random initializations even with only 11 nodes. <sep>	I-weakness

2018-355	The paper analyzes neural network with hidden layer of piecewise linear units, a single output, and a quadratic loss.	B-abstract
2018-355	The reviewers find the results incremental and not "surprising", and also complained about comparison with previous work.	B-weakness
2018-355	I think the topic is very pertinent, and definitely more relevant compared to studying multi-layer linear networks.	B-strength
2018-355	Hence, I recommend the paper be presented in the workshop track.	B-decision

2018-356	The paper nicely unifies previous results and develops the property of local openness.	B-abstract
2018-356	While interesting, I find the application to multi-layer linear networks extremely limiting.	B-weakness
2018-356	There appears to be a sub-field in theory now focusing on solely multi-layer linear networks which is meaningless in practice.	I-weakness
2018-356	I can appreciate that this could give rise to useful proof techniques and hence, I am recommending it to the workshop track with the hope that it can foster more discussions and help researchers move away from studying multi-layer linear networks.	B-decision

2018-372	the problem is interesting, and the approach is also interesting.	B-strength
2018-372	however, the reviewers have found that this manuscript would benefit from more experiments, potentially involving some real data (even at least for evaluation) in addition to largely synthetic data sets used in the submission.	B-suggestion
2018-372	i also agree with them and encourage authors to consider this option.	I-suggestion

2018-376	The author's propose to use swish and show that it performs significantly better than Relus on sota vision models.	B-abstract
2018-376	Reviewers and anonymous ones counter that PRelus should be doing quite well too.	B-weakness
2018-376	Unfortunately, the paper falls in the category where it is hard to prove the utility of the method through one paper alone, and broader consensus relies on reproduction by the community.	I-weakness
2018-376	As a results, I'm going to recommend publishing to a workshop for now.	B-decision

2018-394	The paper introduces an interesting family of two-player zero-sum games with tunable complexity, called Erdos-Selfridge-Spencer games, as a new domain for RL.	B-abstract
2018-394	The authors report on extensive empirical results using a wide variety of training methods, including supervised learning and several flavors of RL (PPO, A2C, DQN) as well as single-agent vs. multi-agent training.	I-abstract
2018-394	The reviewers also appear to agree that the method appears to be technically correct, clearly written, and easy to read. <sep>	B-strength
2018-394	A drawback of the paper is that it does not make a *significant* contribution to the field.	B-weakness
2018-394	In combing through the reviewer comments, none of them identify a significant contribution.	I-weakness
2018-394	Even in the text of the paper, the authors do not anywhere claim to have made a significant contribution.	I-weakness
2018-394	As the paper is still interesting, the committee would like to recommend this for the workshop track. <sep>	B-decision
2018-394	Pros: <sep> Interesting domain with tunable complexity <sep>	B-strength
2018-394	High-quality extensive empirical results <sep>	I-strength
2018-394	Writing is clear <sep>	I-strength
2018-394	Cons: <sep> Lacks a significant contribution <sep>	B-weakness
2018-394	Appears to overlook self-play, the dominant RL training paradigm for decades (multiagent training appears to be related but different) <sep>	I-weakness
2018-394	Per Reviewer3, "I remain unconvinced that these games are good general tests for Deep RL	I-weakness

2018-397	This paper turned out to be quite difficult to call.	O
2018-397	My take on the pros/cons is: <sep> 1. The research topic, how and why humans can massively outperform DQN, is unanimously viewed as highly interesting by all participants. <sep>	B-strength
2018-397	2. The authors present an original human subject study, aiming to reveal whether human outperformance is due to human knowledge priors.	I-strength
2018-397	The study is well conceived and well executed.	I-strength
2018-397	I consider the study to be a contribution by itself. <sep>	I-strength
2018-397	3. The study provides prima facie evidence that human priors play a role in human performance, by changing the visual display so that the priors cannot be used. <sep>	B-abstract
2018-397	4. However, the study is not definitive, as astutely argued by AnonReviewer2.	B-weakness
2018-397	Experiments using RL agents (with presumably no human priors) yield behavior that is similar to human behavior.	I-weakness
2018-397	So it is possible that some factor other than human prior may account for the behavior seen in the human experiments. <sep>	I-weakness
2018-397	5. It would indeed be better, as argued by AnonReviewer2, to use some information-theoretic measure to distinguish the normal game from the modified games. <sep>	B-suggestion
2018-397	6. The paper has been substantially improved and cleaned up from the original version. <sep>	B-rebuttal_process
2018-397	7. AnonReviewer1 provided some thoughtful detailed discussion of how the authors may be overstating the conclusions that one can draw from the paper. <sep>	B-weakness
2018-397	Bottom line: Given the procs and cons of the paper, the committee recommends this for workshop. <sep>	B-decision

2018-401	Authors present a method for representing DNA sequence reads as one-hot encoded vectors, with genomic context (expected original human sequence), read sequence, and CIGAR string (match operation encoding) concatenated as a single input into the framework.	B-abstract
2018-401	Method is developed on 5 lung cancer patients and 4 melanoma patients. <sep>	I-abstract
2018-401	Pros: <sep> - The approach to feature encoding and network construction for task seems new. <sep>	B-strength
2018-401	- The target task is important and may carry significant benefit for healthcare and disease screening. <sep>	I-strength
2018-401	Cons: <sep> - The number of patients involved in the study is exceedingly small.	B-weakness
2018-401	Though many samples were drawn from these patients, pattern discovery may not be generalizable across larger populations.	I-weakness
2018-401	Though the difficulty in acquiring this type of data is noted. <sep>	I-weakness
2018-401	- (Significant) Reviewer asked for use of public benchmark dataset, for which authors have declined to use since the benchmark was not targeted toward task of ultra-low VAFs.	B-rebuttal_process
2018-401	However, perhaps authors could have sourced genetic data from these recommended public repositories to create synthetic scenarios, which would enable the broader research community to directly compare against the methods presented here.	B-suggestion
2018-401	The use of only private datasets is concerning regarding the future impact of this work. <sep>	B-weakness
2018-401	- (Significant) The concatenation of the rows is slightly confusing.	I-weakness
2018-401	It is unclear why these were concatenated along the column dimension, rather than being input as multiple channels.	I-weakness
2018-401	This question doesn't seem to be addressed in the paper. <sep>	I-weakness
2018-401	Given the pros and cons, the commitee recommends this interesting paper for workshop.	B-decision

2018-441	This method makes a connection between evolutionary and variational methods in a particular model.	B-abstract
2018-441	This is a good contribution, but there has been little effort to position it in comparison to standard methods that do the same thing, showing relative strengths and weaknesses. <sep>	B-weakness
2018-441	Also, please shorten the abstract.	B-suggestion

2018-500	The reviewers all outlined concerns regarding novelty and the maturity of this work.	B-weakness
2018-500	It would be helpful to clarify the relation to doubly stochastic kernel machines as opposed to random kitchen sinks, and to provide more insight into how this stochasticity helps.	B-suggestion
2018-500	Finally, the approach should be tried on more difficult image datasets.	I-suggestion

2018-503	The problem addressed here is an important one: What is a good evaluation metric for generative models?	B-abstract
2018-503	A good selection of popular metrics are analyzed for their appropriateness for model selection of GANs.	I-abstract
2018-503	Two popular approaches are recommended: the kernel Maximum Mean Discrepancy (MMD) and the 1-Nearest-Neighbour (1-NN) two-sample test.	I-abstract
2018-503	This seems reasonable, but the present work was not recommended for acceptance by 2 reviewers who raised valid concerns. <sep>	B-rating_summary
2018-503	From a readability perspective, it would be nice to simply list the answer to question (1) directly in the introduction.	B-suggestion
2018-503	One must read more than a few pages to get to the answer of why the metrics that are advocated were picked.	B-weakness
2018-503	It need not read like a mystery. <sep>	I-weakness
2018-503	R4: "The evaluations rely on using a pre-trained imagenet model as a representation.	I-weakness
2018-503	The authors point out that different architectures yield similar results for their analysis, however it is not clear how the biases of the learned representations affect the results.	B-rebuttal_process
2018-503	The use of learned representations needs more rigorous justification" <sep>	I-rebuttal_process
2018-503	R2: "First, it only considers a single task for which GANs are very popular.	B-weakness
2018-503	Second, it could benefit from a deeper (maybe theoretical analysis) of some of the questions."	I-weakness
2018-503	- the first point of which is also related to a concern of R4. <sep>	O
2018-503	Given the overall high selectivity of ICLR, the present submission falls short.	B-decision

2018-508	The reviewers were unanimous in their assessment that the paper was not ready for publication in ICLR.	B-rating_summary
2018-508	Their concerns included: <sep> - lack of novelty over Niepert, Ahmed, Kutzkov, ICML 2016 <sep>	B-weakness
2018-508	- The approach learns combinations of graph kernels and its expressive capacity is thus limited <sep>	I-weakness
2018-508	- The results are close to the state of the art and it is not clear whether any improvement is statistically significant. <sep>	I-weakness
2018-508	The authors have not provided a response to these concerns.	B-rebuttal_process

2018-530	This paper presents a memory architecture for RL based on reservoir sampling, and is meant to be an alternative to RNNs.	B-abstract
2018-530	The reviewers consider the idea to be potentially interesting and useful, but have concerns about the mathematical justification.	B-weakness
2018-530	They also point out limitations in the experiments: in particular, use of artificial toy problems, and a lack of strong baselines.	I-weakness
2018-530	I don't think the paper is ready for ICLR publication in its current form. <sep>	B-decision

2018-537	meta score: 4 <sep>	B-decision
2018-537	This paper proposes an activation function, called displaced ReLU (DReLU), to improve the performance of CNNs that use batch normalization. <sep>	B-abstract
2018-537	Pros <sep> - good set of experiments using CIFAR, with good results <sep>	B-strength
2018-537	- attempt to explain the approach using expectations <sep>	I-strength
2018-537	Cons <sep> - theoretical explanations are not so convincing <sep>	B-weakness
2018-537	- limited novelty <sep>	I-weakness
2018-537	- CIFAR is relatively limited set of experiments <sep>	I-weakness
2018-537	- does not compare with using bn after relu, which is now well-studied and seems to address the motivation of this paper (and thus questions the conclusions)	I-weakness

2018-538	Meta score: 4 <sep>	B-decision
2018-538	The paper concerns the development of a density network for estimating uncertainty in recommender systems.	B-abstract
2018-538	The submitted paper is not very clear and it is hard to completely understand the proposed method from the way it is presented.	B-weakness
2018-538	This makes assessing the contribution of the paper  difficult. <sep>	I-weakness
2018-538	Pros: <sep> - addresses an interesting and important problem <sep>	B-strength
2018-538	- possible novel contribution <sep>	I-strength
2018-538	Cons: <sep> - poorly written, hard to understand precisely what is done <sep>	B-weakness
2018-538	- difficult to compare with the state-of-the-art, not helped by disorganised literature review <sep>	I-weakness
2018-538	- experimentation could be improved <sep>	I-weakness
2018-538	The paper needs more work before being ready for publication.	B-decision

2018-552	The reviewers point out that most of the results are already known and are not novel.	B-weakness
2018-552	There are also issues with the presentation.	I-weakness
2018-552	Studying only depth 2 and depth 3 networks is very limiting.	I-weakness

2018-561	The authors propose a technique for weight pruning that leaves block diagonal weights, instead of unstructured sparse weights, leading to faster inference.	B-abstract
2018-561	However, the experiments demonstrating the quality of the pruned models are insufficient.	B-weakness
2018-561	The authors also discuss connections to random matrix theory; but these connections are not worked out in detail.	I-weakness

2018-565	While the reviewers considered the basic idea of adding supervision intermediate to differentiable programming style architectures to be interesting and worthy of effort, they were unsure if <sep> 1: the proposed abstractions for discussing ntm and nram are well motivated/more generally applicable <sep>	B-weakness
2018-565	2: the methods used in this work to give intermediate supervision are more generally applicable <sep>	I-weakness

2018-577	This paper resulted in significant discussion -- both between R2 and the authors, and between the AC, PCs, and other solicited experts. <sep>	O
2018-577	The problem of language grounding (and instruction following) in virtual environments is clearly important, this work was one of the first in the recent resurgence, and the goal of understand what the agents have learned is clearly noble and important.	B-strength
2018-577	In terms of raw recommendations, the majority reviewer recommendation is negative, but since concerns raised by R2 seemed subjective (which in principle is not a problem), out of abundance of caution, we solicited additional input.	B-rating_summary
2018-577	Unfortunately, we received feedback consistent with the concerns raised here: <sep> -- The lack of generality of the behavior found.	B-weakness
2018-577	Even if we ignore the difficult question of why the agent prefers what it does, it's unclear how the conclusions here generalize much farther than the model and environment used; the manuscript does not provide any novel or transferable principals of the form "this kind of bias in the environment leads to this kind of bias in models with these properties". <sep>	I-weakness
2018-577	-- We realize even providing that concrete a statement might be hard, but also missing are thorough comparisons to other kinds of models (eg non-deep, as asked by R1) to establish that this is a general phenomenon. <sep>	I-weakness
2018-577	Ultimately, there is a sense that this is too narrow an analysis, too soon.	I-weakness
2018-577	If there was one architecture for learning embodied agents in 3d environments that was clearly successful and useful, then studying its properties might be interesting (even crucial).	I-weakness
2018-577	But the dust in this space isn't settled.	I-weakness
2018-577	Our current agents are fairly poor, and so the impact of understanding the biases of a specific model trained in a specific environment seems fairly low. <sep>	I-weakness
2018-577	Finally -- this not taken into consideration in making the decision -- it is not okay to list personal homepage domains (that may reveal author identity to ACs) as conflict domains; those are meant for institutional conflicts/domains.	I-weakness

2018-580	This paper received divergent ratings (7, 3, 3).	B-rating_summary
2018-580	While there is value in thorough evaluation papers, this manuscript has significant presentation issues.	B-weakness
2018-580	As all three reviewers point out, the way it is currently written, it misrepresents the claims made by Mirowski et al 2016 and over-reaches in its findings.	I-weakness
2018-580	Unfortunately, we cannot make a decision on what the manuscript may look like in future once these issues are fixed, and must reject.	B-decision

2018-589	This paper studies to what extent adversarial training affects the properties of adversarial examples in object classification. <sep>	B-abstract
2018-589	Reviewers found the work going in the right direction, but agreed that it needs further evidence/focus in order to constitute a significant contribution to the ICLR community.	B-weakness
2018-589	In particular, the AC encourages authors to relate their work to the growing body of (mostly concurrent) work on robust optimization and adversarial learning.	B-suggestion
2018-589	For the above reasons, the AC recommends rejection at this time.	B-decision

2018-591	This paper addresses the question of how to solve image super-resolution, building on a connection between sparse regularization and neural networks. <sep>	B-abstract
2018-591	Reviewers agreed that this paper needs to be rewritten, taking into account recent work in the area and significantly improving the grammar.	B-weakness
2018-591	The AC thus recommends rejection at this time.	B-decision

2018-605	The paper proposes a new convolutional network architecture, called CrescendoNet.	B-abstract
2018-605	Whilst achieving competitive performance on CIFAR-10 and SVHN, the accuracy of the proposed model on CIFAR-100 is substantially lower than that of state-of-the-art models with fewer parameters; the paper presents no experimental results on ImageNet.	B-weakness
2018-605	The proposed architecture does not provide clear new insights or successful new design principles.	I-weakness
2018-605	This makes it unlikely the current manuscript will have a lot of impact.	I-weakness

2018-607	The paper presents a boosting method and uses it to train an ensemble of convnets for image classification.	B-abstract
2018-607	The paper lacks conceptual and empirical comparisons with alternative boosting and ensembling methods.	B-weakness
2018-607	In fact, it is not even clear from the experimental results whether or not the proposed method outperforms a simple baseline model that averages the predictions of T independently trained convolutional networks.	I-weakness

2018-613	The authors propose to use identity + some weights in the recurrent connections to prevent vanishing gradients.	B-abstract
2018-613	The reviewers found the experiments to have weak baselines, weakening the claims of the paper.	B-weakness

2018-616	+ The paper proposes an interesting empirical measure of "learnability" of a trained network: how well the predictive function it represents can be learned by another network.	B-abstract
2018-616	And shows it empirically seems to correlate with better generalization. <sep>	I-abstract
2018-616	- The work is purely empirical: it features no theory relating this learnability to generalization <sep>	B-weakness
2018-616	- Learnability measure is somewhat ad-hoc with moving parts left to be specified (learning network, data splits,..) <sep>	I-weakness
2018-616	- as pointed out by a reviewer, learnability doesn't really provide any answers for now. <sep>	I-weakness
2018-616	- the work would be much stronger if it went beyond a mere correlation study, and if learnability considerations allowed to derive a new approach/regularization scheme that was convincingly shown to improve generalization. <sep>	B-suggestion

2018-622	Proposed network compression method offers limited technical novelty over existing approaches, and empirical evaluations do not clearly demonstrate an advantage over current state-of-the-art. <sep>	B-weakness
2018-622	Paper presentation quality also needs to be improved.	I-weakness

2018-623	The paper makes overly strong claims, too weakly supported by a hard to follow and insufficiently rigorous mathematical argument.	B-weakness
2018-623	Connections with a large body of relevant prior literature are missing.	I-weakness

2018-626	The paper received borderline-negative scores (6,5,5) with R1 and R2 having significant difficulty with the clarity of the paper.	B-rating_summary
2018-626	Although R3 was marginally positive, they pointed out that the experiments are "extremely weak".	B-weakness
2018-626	The AC look at the paper and agrees with R3 on this point.	O
2018-626	Therefore the paper cannot be accepted in its current form.	B-decision
2018-626	The experiments and clarity need work before resubmission to another venue.	I-decision

2018-628	The paper received scores of 5,5,5, with the reviewers agreeing the paper was marginally below the acceptance threshold.	B-rating_summary
2018-628	The main issue, raised by both R2 and R3 was that connection between representation learning in deep nets and coding theory was not fully justified/made.	B-weakness
2018-628	With no reviewer advocating acceptance, it is not possible to accept the paper unfortunately.	B-decision

2018-631	The paper received weak scores: 4,4,5.	B-rating_summary
2018-631	R2 complained about clarity.	B-weakness
2018-631	R3's point about the lack of fully connected layers in current SOA deepnets is very valid and the authors response far from convincing.	B-rebuttal_process
2018-631	Unfortunately the major revision provided by the authors was not commented on by the reviewers, but many of the major shortcomings of the work still remain. <sep>	I-rebuttal_process
2018-631	Generally, the paper is below the acceptance threshold, so cannot be accepted.	B-decision

2018-651	This paper tackles a very important problem: evaluating natural language generation.	B-abstract
2018-651	The paper presents an overview of existing unsupervised metrics, and looks at how they correlate with human evaluation scores.	I-abstract
2018-651	This is important work and the empirical conclusions are useful to the community, but the datasets used are too limited and the authors agree it would be better to use newer bigger and more diverse datasets suggested by reviewers for drawing more general conclusions.	B-weakness
2018-651	This work would indeed be much stronger if it relied on better, more recent datasets; therefore publication as is seems premature. <sep>	B-decision

2018-653	While using self-play for training a goal-oriented dialogue system makes sense, the contribution of this paper compared to previous work (that the paper itself cites) seems too minor, and the limitations of using toy synthetic data further weaken the work. <sep>	B-weakness

2018-658	The paper builds on earlier work by Wang et al (2015) on Visual Concepts (VCs) and explores the use of VCs for few-shot learning setting for novel classes. <sep>	B-abstract
2018-658	The work, as pointed out by two reviewers is somewhat incremental in nature, with main novelty being the demonstration of utilities of VCs for few shot learning.	B-weakness
2018-658	This would not have been a big limitation if the paper had a carefully conducted empirical evaluation providing insights on the effect of various configuration settings/hyperparameters on the performance in few shot learning, which two of the reviewers (Anon3, Anon2) state are missing.	I-weakness
2018-658	The paper falls short of the acceptance threshold in its current form. <sep>	B-decision
2018-658	PS: The authors posted a github link to the code on Jan 12 which may potentially compromise the anonymity of the submission (though it was after all the reviews were already in) https://openreview.net/forum?id=BJ_QxP1AZ&noteId=BJaIDpBEM <sep>	B-weakness

2018-672	Pros <sep> -- Competitive results on LibriSpeech. <sep>	B-strength
2018-672	Cons <sep> -- Limited novelty, and lacks enough comparisons. <sep>	B-weakness
2018-672	-- Comparison with other end-to-end approaches, and on other commonly used datasets, like WSJ, missing. <sep>	I-weakness
2018-672	-- Gated convnets have already been proposed. <sep>	I-weakness
2018-672	-- Letter based systems have been shown to be competitive to phone based systems. <sep>	I-weakness
2018-672	-- Optimization criterion is quite similar to lattice-free MMI proposed by Povey et al, but with a letter based LM and a slightly different HMM topology. <sep>	I-weakness
2018-672	Given the cons pointed out by reviews, the AC is recommending that the paper be rejected.<sep>	B-decision

2018-694	Pros: <sep> + The idea of end-to-end training that simultaneously learns the weights and appropriate precision for those weights is very appealing. <sep>	B-strength
2018-694	Cons: <sep> - Experimental results are far from the state-of-the-art, which makes the empirical evaluation unconvincing. <sep>	B-weakness
2018-694	- More justification is needed for the update of the number of bits using the sign of the gradient. <sep>	I-weakness

2018-698	The reviewers agree that the authors have made an interesting contribution studying the effect of data augmentation, but they also agree that the claims made by the paper require a broader empirical study beyond the limited number of tasks surveyed in the current revision.	B-weakness
2018-698	I urge the authors to follow this advice and see what they find.	B-suggestion

2018-703	This paper presents a neural compositional model for visual question answering.	B-abstract
2018-703	The overall idea may be exciting but the committee agrees with the evaluation of Reviewer 1:  the experimental section is a bit thin and it only evaluates against an artificial dataset for visual QA that does not really need a knowledge base.	B-weakness
2018-703	It would have been better to evaluate on more traditional question answering settings where the answer can be retrieved from a knowledge base (WebQuestions, Free917, etc.	B-suggestion
2018-703	), and then compare with state of the art on those.	I-suggestion

2018-712	All reviewers agree that the contribution of this paper, a new way of training neural nets to execute Monte-Carlo Tree Search, is an appealing idea.	B-strength
2018-712	For the most part, the reviewers found the exposition to be fairly clear, and the proposed architecture of good technical quality.	I-strength
2018-712	Two of the reviewers point out flaws in implementing in a single domain, 10x10 Sokoban with four boxes and four targets.	B-weakness
2018-712	Since their training methodology uses supervised training on approximate ground-truth trajectories derived from extensive plain MCTS trials, it seems unlikely that the trained DNN will be able to generalize to other geometries (beyond 10x10x4) that were not seen during training.	I-weakness
2018-712	Sokoban also has a low branching ratio, so that these experiments do not provide any insight into how the methodology will scale at much higher branching ratios. <sep>	I-weakness
2018-712	Pros: Good technical quality, interesting novel idea, exposition is mostly clear.	B-strength
2018-712	Good empirical results in one very limited domain. <sep>	I-strength
2018-712	Cons: Single 10x10x4 Sokoban domain is too limited to derive any general conclusions. <sep>	B-weakness
2018-712	Point for improvement: The paper compares performance of MCTSnet trials vs. plain MCTS trials based on the number of trials performed.	I-weakness
2018-712	This is not an appropriate comparison, because the NN trials will be much more heavyweight in terms of CPU time, and there is usually a time limit to cut off MCTS trials and execute an action.	I-weakness
2018-712	It will be much better to plot performance of MCTSnet and plain MCTS vs. CPU time used.	B-suggestion

2018-713	All three reviewers are in agreement that this paper is not ready for ICLR in its current state.	B-rating_summary
2018-713	Given the pros/cons, the committee feels the paper is not ready for acceptance in its current form.	B-decision

2018-721	The paper presents an approach for learning continuous-valued vector representations combining multiple input feature sets of different types, in both unsupervised and supervised settings.	B-abstract
2018-721	The revised paper is a merger of the original submission and another ICLR submission.	B-weakness
2018-721	This meta-review takes into account all of the comments on both submissions and revisions. <sep>	O
2018-721	The merged paper is an improvement over the two separate ones.	B-rebuttal_process
2018-721	However, the contribution over previous work is still a bit unclear.	I-rebuttal_process
2018-721	It still does not sufficiently compare to/discuss in the context of other recent work on combining multiple feature groups. <sep>	I-rebuttal_process
2018-721	The experiments are also quite limited.	I-rebuttal_process
2018-721	The idea is introduced as extremely general, but the experiments focus on a small number of specific tasks, some of them non-standard.	I-rebuttal_process

2018-730	The paper addresses the training time of CNNs, in the common setting where a CNN is trained on one domain and then used to extract features for another domain.	B-abstract
2018-730	The paper proposes to speed up the CNN training step via a particular proposed training schedule with a reduced number of epochs.	I-abstract
2018-730	Training time of the pre-trained CNN is not a huge concern, since this is only done once, but optimizing training schedules is a valid and interesting topic of study.	B-strength
2018-730	However, the approach here does not seem novel; it is typical to adjust training schedules according to the desired tradeoff between training time and performance.	B-weakness
2018-730	The experimental validation is also thin, and the writing needs improvement.	I-weakness

2018-731	This paper applies a form of recurrent autoencoder for a specific type of industrial sensor signal analysis.	B-abstract
2018-731	The application is very narrow and the data set is proprietary.	B-weakness
2018-731	The approach is not clearly described, but seems very straightforward and is not placed in context of prior work.	I-weakness
2018-731	It is therefore not clear how to evaluate the contribution of the method.	I-weakness
2018-731	The authors have revised the paper to include more details and prior work, but it still needs a lot more work on all of the above dimensions before it can make a significant contribution to the ICLR community.	B-rebuttal_process

2018-735	Pro: <sep> - Interesting approach to tie together reinforcement Q-learning with CNN for prediction and reward function learning in predicting downstream effects of chemical structures, while providing relevant areas for decision-making. <sep>	B-strength
2018-735	Con: <sep> - Datasets are small, generalizability not clear. <sep>	B-weakness
2018-735	- Performance is not high (although performance wasn't the goal necessarily) <sep>	I-weakness
2018-735	- Sometimes test performance is higher than training performance, making results questionable. <sep>	I-weakness
2018-735	- Should include comparison to other wrapper-based combinatorial approaches. <sep>	I-weakness
2018-735	- Too targeted an appeal/audience (better for chemical journal)	I-weakness

2018-736	Authors present a method for modeling neurodegenerative diseases using a multitask learning framework that considers "censored regression" problems (to model where the outputs have discrete values and ranges).	B-abstract
2018-736	Given the pros/cons, the committee feels this paper is not ready for acceptance in its current state. <sep>	B-decision
2018-736	Pro: <sep> - This approach to modeling discrete regression problems is interesting and may hold potential, but the evaluation is not in a state where strong meaningful conclusions can be made. <sep>	B-strength
2018-736	Con: <sep> - Reviewers raise multiple concerns regarding evaluation and comparison standards for tasks.	B-weakness
2018-736	While authors have added some model comparisons in response, in other areas comparisons don't appear complete.	B-rebuttal_process
2018-736	For example, when using MRI data, networks compared all use features derived from images, rather than systems that may learn from images themselves.	I-rebuttal_process
2018-736	Authors claim dataset is too small to learn directly from pixels in this data (in comments), but transfer learning and data augmentation have been successfully applied to learn from datasets of this size.	I-rebuttal_process
2018-736	In addition, new multitask techniques in the imaging domain have also been presented that dynamically learn the network structure, rather than relying on a hand-crafted neural network design.	I-rebuttal_process
2018-736	How this approach would compare is not addressed. <sep>	I-rebuttal_process

2018-739	Reviewers unanimous in assessment that manuscript has merits, but does not satisfy criteria for publication. <sep>	B-rating_summary
2018-739	Pros: <sep> - Potentially novel application of neural networks to survival analysis with competing risks, where only one terminal event from one risk category may be observed. <sep>	B-strength
2018-739	Cons: <sep> - Incomplete coverage of other literature. <sep>	B-weakness
2018-739	- Architecture novelty may not be significant. <sep>	I-weakness
2018-739	- Small performance gains (though statistically significant)	I-weakness

2018-755	This paper does not meet the bar for ICLR - neither in terms of the quality of the write-up, nor in experimental design.	B-decision
2018-755	The two confident reviewers agree to reject the paper, the weak accept comes from a less confident reviewer who did not write a good review at all.	B-rating_summary
2018-755	The rebuttal does not change this assessment.	B-rebuttal_process

2018-759	PROS: <sep> 1. All the reviewers thought that the work was interesting and showed promise <sep>	B-strength
2018-759	2. The paper is relatively well written <sep>	I-strength
2018-759	CONS: <sep> 1. Limited experimental evaluation (just MNIST) <sep>	B-weakness
2018-759	The reviewers were all really on the fence about this but in the end felt that while the idea was a good one and the authors were responsive in their rebuttal, the experimental evaluation needed more work.	B-rebuttal_process

2018-773	The reviewers are unanimous in their opinion that the theoretical results in this paper are of limited novelty and significance.	B-weakness
2018-773	Several parts of the paper are not presented clearly enough.	I-weakness
2018-773	As such the paper is not ready for ICLR-2018 acceptance.	B-decision

2018-781	The paper proposes a method to map input probability distributions to output probability distributions with few parameters.	B-abstract
2018-781	They show the efficacy of their method on synthetic and real stock data.	I-abstract
2018-781	After revision they seemed to have added another dataset, however, it is not carefully analyzed like the stock data.	B-rebuttal_process
2018-781	More rigorous experimentation needs to be done to justify the method.	B-weakness

2018-789	The paper proposes a new model called differential decision tree which captures the benefits of decision trees and VAEs.	B-abstract
2018-789	They evaluate the method only on the MNIST dataset.	B-weakness
2018-789	The reviewers thus rightly complain that the evaluation is thus insufficient and one also questions its technical novelty.	I-weakness

2018-804	The proposed conditional variance regularizer looks interesting and the results show some promise.	B-strength
2018-804	However, as the reviewers pointed out, the connection between the information-theoretic argument provided and the final form of the regularizer is too tenuous in its current form.	B-weakness
2018-804	Since this argument is central to the paper, the authors are urged to either provide a more rigorous derivation or motivate the regularizer more directly and place more emphasis on its empirical evaluation.	B-suggestion

2018-811	The reviewers agreed that the paper was too long (more than twice the recommended page limit not counting the appendix) and difficult to follow.	B-weakness
2018-811	They also pointed out that its central idea of learning the noise distribution in a VAE was not novel.	I-weakness
2018-811	While the shortened version uploaded by the authors looks like a step in the right direction, it was not sufficient to convince the reviewers.	B-rebuttal_process

2018-812	The reviewers are in agreement, that the paper is a big hard to follow and incorrect in places, including some claims not supported by experiments.	B-weakness

2018-828	The authors present GraphVAE, a method for fitting a generative deep model, a variational autoencoder, to small graphs.	B-abstract
2018-828	Fitting deep learning models to graphs remains challenging (although there is relevant literature as brought up by the reviewers and anonymous comments) and this paper is a strong start. <sep>	B-strength
2018-828	In weighing the various reviews, AnonReviewer3 is weighed more highly than AnonReviewer1 and AnonReviewer2 since that review is far more thorough and the reviewer is more expert on this subject.	O
2018-828	Unfortunately, the review from AnonReviewer1 is extremely short and of very low confidence.	B-rating_summary
2018-828	As such, this paper sits just below the borderline for acceptance.	B-decision
2018-828	In general, the main criticisms of the paper are that some claims are too strong (eg non-differentiability of discrete structures), treatment of related work (missing references, etc.)	B-weakness
2018-828	and weak experiments and baselines.	I-weakness
2018-828	The consensus among the reviews (even AnonReviewer2) is that the paper is preliminary.	I-weakness
2018-828	The paper is close, however, and addressing these concerns will make the paper much stronger. <sep>	B-suggestion
2018-828	Pros: <sep> - Proposes a method to build a generative deep model of graphs <sep>	B-strength
2018-828	- Addresses a timely and interesting topic in deep learning <sep>	I-strength
2018-828	- Exposition is clear <sep>	I-strength
2018-828	Cons: <sep> - Treatment of related literature should be improved <sep>	B-weakness
2018-828	- Experiments and baselines are somewhat weak <sep>	I-weakness
2018-828	- "Preliminary" <sep>	I-weakness
2018-828	- Only works on rather small graphs (ie O(k^4) for graphs with k nodes)	I-weakness

2018-839	The reviewers agreed that this paper is not quite ready for publication at ICLR.	B-rating_summary
2018-839	One of the reviewers thought the paper was well written and easy to follow while the two others said the opposite.	B-strength
2018-839	One of the main criticisms was issues with the composition.	B-weakness
2018-839	The paper seems to lack a clear formal explanation of the problem and the proposed methodology.	I-weakness
2018-839	The reviewers in general weren't convinced by the experiments, complaining about the lack of a required baseline and that the proposed method doesn't seem to significantly help in the experiment presented. <sep>	I-weakness
2018-839	Pros: <sep> - The proposed idea is interesting <sep>	B-strength
2018-839	- The problem is timely and of interest to the community <sep>	I-strength
2018-839	- Addresses multiple important problems at the intersection of ML and RL in sequence generation <sep>	I-strength
2018-839	Cons: <sep> - Novel but somewhat incremental <sep>	B-weakness
2018-839	- The experiments are not compelling (ie the results are not strong) <sep>	I-weakness
2018-839	- A necessary baseline is missing <sep>	I-weakness
2018-839	- Significant issues with the writing - both in terms of clarity and correctness.	I-weakness

2018-845	Regarding clarity, while the paper definitely needs work if it is to be resubmitted to an ML venue, different revisions would be appropriate for a physics audience.	B-suggestion
2018-845	And given the above comment, any suggested changes are likely to be superfluous.	O

2018-869	The reviewers agree that the paper studies and interesting problem with an interesting approach.	B-strength
2018-869	The reviewers raised some concerns regarding the theoretical and empirical results.	B-weakness
2018-869	The authors have made changes to the paper, but given the theoretical nature of the paper and the extent of changes, another review is needed before publication.	B-rebuttal_process

2018-878	Dear authors, <sep>	O
2018-878	The reviewers appreciated your work and recognized the importance of theoretical work to understand the behaviour of deep nets.	B-strength
2018-878	That said, the improvement over existing work (especially Montufar, 2017) is minor.	B-weakness
2018-878	This, combined with the limited attraction of such work, means that the paper will not be accepted. <sep>	B-decision
2018-878	I acknowledge the major modifications done but it is up to the reviewers to decide whether or not they agree to re-review a significantly updated version.	B-rebuttal_process

2018-902	This paper looks at  building new density estimation methods and new methods for tranformations and autoregressive models.	B-abstract
2018-902	The request from reviewers for comparison improves the paper.	B-rebuttal_process
2018-902	These models have seen a wide range of applications and have been highly successful, needing the added benefits shown and their potential impact to be expanded further.	B-suggestion

2018-903	This is an interesting paper and addresses an important problem of neural networks with memory constrains.	B-abstract
2018-903	New experiments have been added that add to the paper, but the full impact of the paper is not yet realised, needing further exploration of models of current practice, wider set of experiments and analysis, and additional clarifying discussion.	B-suggestion

2018-908	The subject of model evaluation will always be a contentious one, and the reviewers were not yet fully-convinced by the discussion.	B-rebuttal_process
2018-908	The points you bring up at the end of your rresponse already point to directions for improvement as well as a greater degree of precision and control.	I-rebuttal_process

2019-4	Even though the results are very preliminary we still accept them for the purpose of fostering interesting discussions.	B-decision

2019-117	The manuscript presents a promising new algorithm for learning geometrically-inspired embeddings for learning hierarchies, partial orders, and lattice structures.	B-abstract
2019-117	The manuscript builds on the build on the box lattice model, extending prior work by relaxing the box embeddings via Gaussian convolutions.	I-abstract
2019-117	This is shown to be particularly effective for non-overlapping boxes, where the previous method fail. <sep>	I-abstract
2019-117	The primary weakness identified by reviewers was the writing, which was thought to be lacking some context, and may be difficult to approach for the non-domain expert.	B-weakness
2019-117	This can be improved by including an additional general introduction.	B-suggestion
2019-117	Otherwise, the manuscript was well written. <sep>	B-strength
2019-117	Overall, reviewers and AC agree that the general problem statement is timely and interesting, and well executed.	I-strength
2019-117	In our opinion, this paper is a clear accept.	B-decision

2019-130	The authors posit and investigate a hypothesis -- the "lottery ticket hypothesis" -- which aims to explain why overparameterized neural networks are easier to train than their sparse counterparts.	B-abstract
2019-130	Under this hypothesis, randomly initialized dense networks are easier to train because they contain a larger number of "winning tickets". <sep>	I-abstract
2019-130	This paper received very favorable reviews, though there were some notable points of concern.	B-rating_summary
2019-130	The reviewers and the AC appreciated the detailed and careful experimentation and analysis.	B-strength
2019-130	However, there were a couple of points of concern raised by the reviewers: 1) the lack of experiments conducted on large-scale tasks and models, and 2) the lack of a clear application of the idea beyond what has been proposed previously. <sep>	B-weakness
2019-130	Overall, this is a very interesting paper with convincing experimental validation and as such the AC is happy to accept the work.	B-decision

2019-138	This method proposes a criterion (SNIP) to prune neural networks before training.	B-abstract
2019-138	The pro is that SNIP can find the architecturally important parameters in the network without full training.	B-strength
2019-138	The con is that SNIP only evaluated on small datasets (mnist, cifar, tiny-imagenet) and it's uncertain if the same heuristic works on large-scale dataset.	B-weakness
2019-138	Small datasets can always achieve high pruning ratio, so evaluation on ImageNet is quite important for pruning work.	I-weakness
2019-138	The reviewers have consensus on accept.	B-rating_summary
2019-138	The authors are recommended to compare with previous work [1][2] to make the paper more convincing. <sep>	B-suggestion
2019-138	[1] Song Han, Jeff Pool, John Tran, and William Dally.	O
2019-138	Learning both weights and connections for efficient neural network.	O
2019-138	NIPS, 2015. <sep>	O
2019-138	[2] Yiwen Guo, Anbang Yao, and Yurong Chen.	O
2019-138	Dynamic network surgery for efficient dnns.	O
2019-138	NIPS, 2016.	O

2019-139	The reviewers acknowledge the value of the careful analysis of Gaussian encoder/decoder VAE presented in the paper.	B-strength
2019-139	The proposed algorithm shows impressive FID scores that are comparable to those obtained by state of the art GANs.	I-strength
2019-139	The paper will be a valuable addition to the ICLR program. <sep>	B-decision

2019-140	All reviewers agree that the proposed method interesting and well presented.	B-strength
2019-140	The authors' rebuttal addressed all outstanding raised issues.	B-rebuttal_process
2019-140	Two reviewers recommend clear accept and the third recommends borderline accept.	B-rating_summary
2019-140	I agree with this recommendation and believe that the paper will be of interest to the audience attending ICLR.	B-decision
2019-140	I recommend accepting this work for a poster presentation at ICLR.	I-decision

2019-148	Reviewers are in a consensus and recommended to accept after engaging with the authors.	B-rating_summary
2019-148	Please take reviewers' comments into consideration to improve your submission for the camera ready. <sep>	B-suggestion

2019-160	This paper proposes an approach for imitation learning from unsegmented demonstrations.	B-abstract
2019-160	The paper addresses an important problem and is well-motivated.	B-strength
2019-160	Many of the concerns about the experiments have been addressed with follow-up comments.	B-rebuttal_process
2019-160	We strongly encourage the authors to integrate the new results and additional literature to the final version.	B-suggestion
2019-160	With these changes, the reviewers agree that the paper exceeds the bar for acceptance.	B-rating_summary
2019-160	Thus, I recommend acceptance.	B-decision

2019-193	The reviewers unanimously agreed that the paper was a significant advance in the field of machine learning on graph-structured inputs.	B-strength
2019-193	They commented particularly on the quality of the research idea, and its depth of development.	I-strength
2019-193	The results shared by the researchers are compelling, and they also report optimal hyperparameters, a welcome practice when describing experiments and results. <sep>	I-strength
2019-193	A small drawback the reviewers highlighted is the breadth of the content in the paper, which gave the impression of a slight lack of focus.	B-weakness
2019-193	Overall, the paper is a clear advance, and I recommend it for acceptance.	B-decision

2019-201	Lean in favor <sep>	B-rating_summary
2019-201	Strengths:  The paper tackles the difficult problem of automatic robot design.	B-strength
2019-201	The approach uses graph neural networks to parameterize the control policies, which allows for weight sharing / transfer to new policies even as the topology changes.	I-strength
2019-201	Understanding how to efficiently explore through non-differentiable changes to the body is an important problem (AC).	I-strength
2019-201	The authors will release the code and environments, which will be useful in an area where there are currently no good baselines (AC). <sep>	I-strength
2019-201	Weaknesses: There are concerns (particularly R2, R1) over the lack of a strong baseline, and with the results being demonstrated on a limited number of environments (R1) (fish, 2D walker).	B-weakness
2019-201	In response, the authors clarified the nomenclature and description of a number of the baselines, and added others.	B-rebuttal_process
2019-201	AC: there is no submitted video (searches for "video" on the PDF text produces no hits); this is seen by the AC as being a real limitation from the perspective of evaluation. <sep>	B-weakness
2019-201	AC agrees with some of the reviewer remarks that some of the original stated claims are too strong. <sep>	I-weakness
2019-201	AC: the simplified fluid model of Mujoco (http://mujoco.org/book/computation.html#gePassive) is unable to model the fluid state, in particular the induced fluid vortices that are responsible for a good portion of fish locomotion, ie, "Passive and active flow control by swimming fishes and mammals" and other papers.	I-weakness
2019-201	Acknowledging this kind of limitation will make the paper stronger, not weaker; <sep>	B-suggestion
2019-201	the ML community can learn from much existing work at the interface of biology and fluid mechancis. <sep>	I-suggestion
2019-201	There remain points of contention, ie, the sufficiency of the baselines.	B-weakness
2019-201	However, the reviewers R2 and R3 have not responded to the detailed replies from the authors, including additional baselines (totaling 5 at present) <sep>	B-rebuttal_process
2019-201	and pointing out that baselines such as CMA-ES (R2) in a continuous space and therefore do not translate in any obvious way to the given problem at hand. <sep>	I-rebuttal_process
2019-201	On balance, with the additional baselines and related clarifications, the AC feels that this paper makes a useful and valid contribution to the field, and will help establish a benchmark in an important area. <sep>	B-strength
2019-201	The authors are strongly encouraged to further state caveats and limitations, and to emphasize why some candidate baseline methods are not readily applicable. <sep>	B-suggestion

2019-207	The authors present a theoretical and practical study on low-precision training of neural networks.	B-abstract
2019-207	They introduce the notion of variance retention ratio (VRR) that determines the accumulation bit-width for precise tailoring of computation hardware.	I-abstract
2019-207	Empirically, the authors show that their theoretical result extends to practical implementation in three standard benchmarks. <sep>	I-abstract
2019-207	A criticism of the paper has been certain hyperparameters that a reviewer found to be chosen rather arbitrarily, but I think the reviewers do a reasonable job in rebutting it. <sep>	B-weakness
2019-207	Overall, there is consensus that the paper presents an interesting framework and does both practical and empirical analysis, and it should be accepted.	B-rating_summary

2019-228	The proposed method suggests a way to do robust conditional image generation with GANs.	B-abstract
2019-228	The premise is to make the image to image translation model resilient to noise by leveraging structure in the output space, with an unsupervised "pathway". <sep>	I-abstract
2019-228	In general, the qualitative results seem reasonable on a a number of datasets, including those suggested by reviewers.	B-strength
2019-228	The method appears simple, novel and easy to try.	I-strength
2019-228	The main concerns seem to be that the idea is maybe too simple, but I'm not particularly bothered by that.	B-ac_disagreement
2019-228	The authors showed it working well on a variety of tasks (synthetic and natural), provide SSIM numbers that look compelling (despite SSIM's short-comings) and otherwise give compelling arguments for the technical soundness of the approach. <sep>	B-rebuttal_process
2019-228	Thus, I recommend acceptance.	B-decision

2019-231	I agree with the reviewers that this is a strong contribution and provides new insights, even if it doesn't quite close the problem. <sep>	B-strength
2019-231	p.s.: It seems that centering the weight matrices at initialization is a key idea.	B-abstract
2019-231	The authors note that Dziugaite and Roy used  bounds that were based on the distance to initialization, but that their reported numerical generalization bounds also increase with the increasing network size.	I-abstract
2019-231	Looking back at that work, they look at networks where the size increases by a very large factor (going from eg 400,000 parameters roughly to over 1.2 million, so a factor of 2.5), at the same time the bound increases by a much smaller factor.	I-abstract
2019-231	The type of increase also seems much less severe than those pictured in Figures 3/5.	I-abstract
2019-231	Since Dzugate and Roy's bounds involved optimization, perhaps the increase there is merely apparent.	I-abstract

2019-237	The reviewers that provided extensive and technically well-justified reviews agreed that the paper is of high quality.	B-strength
2019-237	The authors are encouraged to make sure all concerns of these reviewers are properly addressed in the paper.	B-suggestion

2019-239	While the reviews of this paper were somewhat mixed (7,6,4), I ended up favoring acceptance because of the thorough author responses, and the novelty of what is being examined. <sep>	B-decision
2019-239	The reviewer with a score of 4, argues that this work is not a good fit for iclr, but, although tailoring new metrics may not be a common area that is explored, I don't believe that it's outside the range of iclr's interest, and therefore also more unique.	B-ac_disagreement

2019-257	Strong paper on hierarchical RL with very strong reviews from people expert in this subarea that I know well. <sep>	B-strength

2019-258	AR1 is concerned about lack of downstream applications which show that higher-order interactions are useful and asks why not to model higher-order interactions for all (a,b) pairs.	B-weakness
2019-258	AR2 notes that this submission is a further development of Arora et al and is satisfied with the paper.	B-strength
2019-258	AR3 is the most critical regarding lack of explanations, eg why linear addition of two word embeddings is bad and why the corrective term proposed here is a good idea.	B-weakness
2019-258	The authors suggest that linear addition is insufficient when final meaning differs from the individual meanings and show tome quantitative results to back up their corrective term. <sep>	B-rebuttal_process
2019-258	On balance, all reviewers find the theoretical contributions sufficient which warrants an accept.	B-rating_summary
2019-258	The authors are asked to honestly reflect all uncertain aspects of their work in the final draft to reflect legitimate concerns of reviewers.	B-suggestion

2019-261	The overall consensus after an extended discussion of the paper is that this work should be accepted to ICLR.	B-rating_summary
2019-261	The back-and-forth between reviewers and authors was very productive, and resulted in substantial clarification of the work, and modification (trending positive) of the reviewer scores.	B-rebuttal_process

2019-269	Pros: <sep> - A useful and well-structured dataset which will be of use to the community <sep>	B-strength
2019-269	- Well-written and clear (though see Reviewer 2's comment concerning the clarity of the model description section) <sep>	I-strength
2019-269	- Good methodology <sep>	I-strength
2019-269	Cons: <sep> - There is a question about why a new dataset is needed rather than a combination of previous datasets and also why these datasets couldn't be harvested from school texts directly.	B-weakness
2019-269	Presumably it would've been a lot more work but please address the issue in your rebuttal. <sep>	B-suggestion
2019-269	- Evaluation: Reviewer 3 is concerned that the evaluation should perhaps have included more mathematics-specific models (a couple of which are mentioned in the text).	B-weakness
2019-269	On the other hand, Reviewer 2 is concerned that the specific choices (eg "thinking steps") made for the general models are non-standard in seq-2-seq models.	I-weakness
2019-269	I haven't heard about the thinking step approach but perhaps it's out there somewhere.	O
2019-269	It would be helpful generally to have more discussion about the reasoning involved in these decisions. <sep>	B-suggestion
2019-269	I think this is a useful contribution to the community, well written and thoughtfully constructed.	B-strength
2019-269	I am tentatively accepting this paper with the understanding that you will engage directly with the reviewers to address their concerns about the evaluation section.	B-decision
2019-269	Please in particular use the rebuttal period to focus on the clarity of the model description and the motivation for the particular models chosen.	B-suggestion
2019-269	Also consider adding additional experiments to allay the concerns of the reviewers.	I-suggestion

2019-274	The paper proposes a new method that builds on the Bayesian modelling framework for GANs and is supported by a theoretical analysis and an empirical evaluation that shows very promising results.	B-abstract
2019-274	All reviewers agree, that the method is interesting and the results are convincing, but that the model does not really fit in the standard Bayesian setting due to a data dependency of the priors.	B-weakness
2019-274	I would therefore encourage the authors to reflect this by adapting the title and making the differences more clear in the camera ready version.	B-suggestion

2019-283	This paper makes the intriguing observation that a density model trained on CIFAR10 has higher likelihood on SVHN than CIFAR10, ie, it assigns higher probability to inputs that are out of the training distribution.	B-abstract
2019-283	This phenomenon is also shown to occur for several other dataset pairs.	I-abstract
2019-283	This finding is surprising and interesting, and the exposition is generally clear.	B-strength
2019-283	The authors provide empirical and theoretical analysis, although based on rather strong assumptions.	I-strength
2019-283	Overall, there's consensus among the reviewers that the paper would make a valuable contribution to the proceedings, and should therefore be accepted for publication.	B-rating_summary

2019-296	Alternating minimization is surprisingly effective for low-rank matrix factorization and dictionary learning problems.	B-abstract
2019-296	Better theoretical characterization of these methods is well motivated.	B-strength
2019-296	This paper fills up a gap by providing simultaneous guarantees for support recovery as well as coefficient estimates for  linearly convergence to the true factors, in the online learning setting.	B-abstract
2019-296	The reviewers are largely in agreement that the paper is well written and makes a valuable contribution.	B-strength
2019-296	The authors are advised to address some of the review comments around relationship to prior work highlighting novelties.	B-suggestion

2019-301	This paper introduces a class of deep neural nets that provably have no bad local valleys.	B-abstract
2019-301	By constructing a new class of network this paper avoids having to rely on unrealistic assumptions and manages to provide a relatively concise proof that the network family has no strict local minima.	I-abstract
2019-301	Furthermore, it is demonstrated that this type of network yields reasonable experimental results on some benchmarks.	I-abstract
2019-301	The reviewers identified issues such as missing measurements of the training loss, which is the actual quantity studied in the theoretical results, as well as some issues with the presentation of the results.	B-weakness
2019-301	After revisions the reviewers are satisfied that their comments have been addressed.	B-rebuttal_process
2019-301	This paper continues an interesting line of theoretical research and brings it closer to practice and so it should be of interest to the ICLR community.	B-strength

2019-310	Reviewers agree the paper should be accepted. <sep>	B-rating_summary
2019-310	See reviews below.	O

2019-312	The paper proposes a novel differential way to output brush strokes, taking a few ideas from model-based learning.	B-abstract
2019-312	The method is efficient in that one can train it in an unsupervised manner and does not require paired data.	B-strength
2019-312	The strengths of the paper are the qualitative results that demonstrate nice interpolations among other things, on a number of datasets (esp.	I-strength
2019-312	post-rebuttal). <sep>	I-strength
2019-312	The weaknesses of the paper are the writing (which I think is relatively easy to improve if the authors make an honest effort) and some of the quantitative evaluation.	B-weakness
2019-312	I would encourage the authors to get in touch with the SPIRAL paper authors in order to get access to the SPIRAL generated MNIST test data and then perhaps the classification metric could be updated. <sep>	B-suggestion
2019-312	In summary, from the discussion, the major points of contention were the somewhat lacking initial evaluation (which was fixed to a large extent) and the quality of writing (which could be fixed more).	B-weakness
2019-312	I believe the submission is genuinely novel, interesting (esp.	B-decision
2019-312	the usage of world model-like techniques) and valuable for the ICLR audience so I recommend acceptance.	I-decision

2019-314	This paper presents a method for measuring the degree to which some representation for a composed object effectively represents the pieces from which it is composed.	B-abstract
2019-314	All three authors found this to be an important topic for study, and found the paper to be a limited but original and important step toward studying this topic.	B-strength
2019-314	However, two reviewers expressed serious concerns about clarity, and were not fully satisfied with the revisions made so far.	B-rebuttal_process
2019-314	I'm recommending acceptance, but I ask the authors to further revise the paper (especially the introduction) to make sure it includes a blunt and straightforward presentation of the problem under study and the way TRE addresses it. <sep>	B-decision
2019-314	I'm also somewhat concerned at R2's mention of a potential confound in one experiment.	B-weakness
2019-314	The paper has been updated with what appears to be a fix, though, and R2 has not yet responded, so I'm presuming that this issue has been resolved. <sep>	B-rebuttal_process
2019-314	I also ask the authors to release code shortly upon de-anonymization, as promised.	B-suggestion

2019-316	The submission proposes a method to construct adversarial attacks based on deforming an input image rather than adding small peturbations.	B-abstract
2019-316	Although deformations can also be characterized by the difference of the original and deformed image, it is qualitatively and quantitatively different as a small deformation can result in a large difference. <sep>	I-abstract
2019-316	On the positive side, this paper proposes an interesting form of adversarial attack, whose success can give additional insights on the forms of existing adversarial attacks.	B-strength
2019-316	The experiments on MNIST and ImageNet are reasonably comprehensive and allow interesting interpretation of how the image deforms to allow the attack.	I-strength
2019-316	The paper is also praised for its clarity, and cleaner formulation compared to Xiao et al (see below).	I-strength
2019-316	Additional experiments during rebuttal phase partially answered reviewer concerns, and provided more information eg about the effect of the smoothness of the deformation. <sep>	B-rebuttal_process
2019-316	There were some concerns that the paper primarly presents one idea, and perhaps missed an opportunity for deeper analysis (R1).	B-weakness
2019-316	R2 would have appreciated more analysis on how to defend against the attack. <sep>	I-weakness
2019-316	A controversial point is the relation /  novelty with respect to Xiao et al, ICLR 2018.	I-weakness
2019-316	As eg pointed out by R1: "The paper originates from a document provably written in late 2017, which is before the deposit on arXiv of another article (by different authors, early 2018) which was later accepted to ICLR 2018 [Xiao and al.].	I-weakness
2019-316	This remark is important in that it changes my rating of the paper (being more indulgent with papers proposing new ideas, as otherwise the novelty is rather low compared to [Xiao and al.])." <sep>	I-weakness
2019-316	On the balance, all three reviewers recommended acceptance of the paper.	B-rating_summary
2019-316	Regarding novelty over Xiao et al, even ignoring the arguable precedence of the current submission, the formulation is cleaner and will likely advance the analysis of adversarial attacks.	B-strength

2019-318	The paper is well written and easy to follow.	B-strength
2019-318	The experiments are adequate to justify the usefulness of an identity for improving existing multi-Monte-Carlo-sample based gradient estimators for deep generative models.	I-strength
2019-318	The originality and significance are acceptable, as discussed below. <sep>	I-strength
2019-318	The proposed doubly reparameterized gradient estimators are built on an important identity shown in Equation (5).	I-strength
2019-318	This identity appears straightforward to derive by applying both score-function gradient and reparameterization gradient to the same objective function, which is expressed as an expectation.	I-strength
2019-318	The AC suspects that this identity might have already appeared in previous publications / implementations, though not being claimed as an important contribution / being explicitly discussed.	B-weakness
2019-318	While that identity may not be claimed as the original contribution of the paper if that suspicion is true, the paper makes another useful contribution in applying that identity to the right problem: improving three distinct training algorithms for deep generative models.	B-strength
2019-318	The doubly reparameterized versions of IWAE and reweighted wake-sleep (RWS) further show how IWAE and RWS are related to each other and how they can be combined for potentially further improved performance. <sep>	I-strength
2019-318	The AC believes that the paper makes enough contributions by well presenting the identity in (5) and applying it to the right problems.	I-strength

2019-333	The paper presents an adversarial learning framework for active visual tracking, a tracking setup where the tracker has camera control in order to follow a target object.	B-abstract
2019-333	The paper builds upon Luo et al 2018 and proposes jointly learning  tracker and target policies (as opposed to tracker policy alone).	I-abstract
2019-333	This automatically creates a curriculum of target trajectory difficulty, as opposed to the engineer designing the target trajectories.	I-abstract
2019-333	The paper further proposes a method for preventing the target to fast outperform the tracker and thus cause his policy to plateau.	I-abstract
2019-333	Experiments presented justify the problem formulation and design choices, and outperform Luo et al .	I-abstract
2019-333	The task considered is  very important, active surveillance with drones is just one sue case. <sep>	I-abstract
2019-333	A downside of the paper is that certain sentences have English mistakes, such as this one:  "The authors learn a policy that maps raw-pixel observation to control signal straightly with a Conv-LSTM network.	B-weakness
2019-333	Not only can it save the effort in tuning an extra camera controller, but also does it outperform the..." However, overall the manuscript is well written, well structured, and easy to follow.	B-strength
2019-333	The authors are encouraged to correct any remaining English mistakes in the manuscript.	B-suggestion

2019-350	The paper investigates mixed-integer linear programming methods for neural net robustness verification in presence of adversarial attckas.	B-abstract
2019-350	The paper addresses and important problem, is well-written, presents a novel approach and demonstrates empirical improvements; all reviewers agree that this is a solid contribution to the field.	B-strength

2019-351	This paper proposes a novel method of solving inverse problems that avoids direct inversion by  first reconstructing various piecewise-constant projections of the unknown image (using a different CNN to learn each)  and then combining them via optimization to solve the final inversion. <sep>	B-abstract
2019-351	Two of the reviewers requested more intuitions into why this two stage process would  fight the inherent ambiguity. <sep>	B-weakness
2019-351	At the end of the discussion, two of the three reviewers are convinced by the derivations and empirical justification of the paper. <sep>	B-rebuttal_process
2019-351	The authors also have significantly improved the clarity of the manuscript throughout the discussion period. <sep>	I-rebuttal_process
2019-351	It would be interesting to see if there are any connections between such inversion via optimization with deep component analysis methods, eg "Deep Component Analysis via Alternating Direction Neural Networks " of Murdock et al , that train neural architectures to effectively carry out the second step of optimization, as opposed to learning a feedforward mapping. <sep>	B-suggestion

2019-356	To borrow the succinct summary from R1, "the paper suggests a method for generating representations that are linked to goals  in reinforcement learning.	B-abstract
2019-356	More precisely, it wishes to learn a representation so that two states are similar if the policies leading to them are similar."	I-abstract
2019-356	The reviewers and AC agree that this is a novel and worthy idea. <sep>	B-strength
2019-356	Concerns about the paper are primarily about the following. <sep>	O
2019-356	(i) the method already requires good solutions as input, ie, in the form of goal-conditioned policies, (GCPs) <sep>	B-weakness
2019-356	and the paper claims that these are easy to learn in any case. <sep>	I-weakness
2019-356	As R3 notes, this then begs the question as to why the actionable representations are needed. <sep>	I-weakness
2019-356	(ii) reviewers had questions regarding the evaluations, ie, fairness of baselines, additional comparisons, and additional detail. <sep>	I-weakness
2019-356	After much discussion, there is now a fair degree of consensus.	O
2019-356	While R1 (the low score) still has a remaining issue with evaluation, particularly hyperparameter evaluation, they are also ok with acceptance.	B-rating_summary
2019-356	The AC is of the opinion that hyperparameter tuning is of course an important issue, but does not see it as the key issue for this particular paper. <sep>	O
2019-356	The AC is of the opinion that the key issue is issue (i), raised by R3.	O
2019-356	In the discussion, the authors reconcile the inherent contradiction in (i) based on the need of additional downstream tasks that can then benefit from the actionable representation, and as demonstrated in a number of the evaluation examples (at least in the revised version).	B-rebuttal_process
2019-356	The AC believes in this logic, but believes that this should be stated more clearly in the final paper.	I-rebuttal_process
2019-356	And it should be explained the extent to which training for auxiliary tasks implicitly solve this problem in any case. <sep>	I-rebuttal_process
2019-356	The AC also suggests nominating R3 for a best-reviewer award.	O

2019-358	This paper presents a novel method for synthesizing fluid simulations, constrained to a set of parameterized variations, <sep>	B-abstract
2019-358	such as the size and position of a water ball that is dropped.	I-abstract
2019-358	The results are solid; there is little related work to compare to, in terms of methods that can "compute"/recall simulations at that speed. <sep>	B-strength
2019-358	The method is 2000x faster than the orginal simulations.	I-strength
2019-358	This comes with the caveats that: <sep> (a) the results are specific to the given set of parameterized environments; the method is learning a compressed version of the original animations; (b) there is a loss of accuracy, and therefore also a loss of visual plausibility. <sep>	B-weakness
2019-358	The AC notes that the paper should use the ICLR format for citations, ie, "(foo et al)" rather than "(19)". <sep>	I-weakness
2019-358	The AC also suggests that limitations should also be clearly documented, ie, as seen from the perspective of those working in the fluid simulation domain. <sep>	I-weakness
2019-358	The principle (and only?)	I-weakness
2019-358	contentious issue relates to the suitability of the paper for the ICLR audience, <sep>	I-weakness
2019-358	given its focus on the specific domain of fluid simulations.	I-weakness
2019-358	The AC is of two minds on this: <sep> (i) the fluid simulation domain has different characteristics to other domains, and thu understanding the ICLR audience can benefit from the specific nature of the predictive problems that come the fluid simulation domain; new problems can drive new methods.	B-strength
2019-358	There is a loose connection between the given work and residual nets, and of course res-nets have also been recently reconceptualized as PDEs. <sep>	B-weakness
2019-358	(ii) it's not clear how much the ICLR audience will get out of the specific solutions being described; <sep>	I-weakness
2019-358	it requires understanding spatial transformer networks and a number of other domain-specific issues. <sep>	I-weakness
2019-358	A problem with this type of paper in terms of graphics/SIGGRAPH is that it can also be seen as "falling short" <sep>	I-weakness
2019-358	there, simply because it is not yet competitive in terms of visual quality or the generality of fluid simulators; it really fulfills a different niche than classical fluid simulators. <sep>	I-weakness
2019-358	The AC leans slightly in favor of acceptance, but is otherwise on the fence. <sep>	B-decision

2019-363	This paper proposes an input-dependent baseline function to reduce variance in policy gradient estimation without adding bias.	B-abstract
2019-363	The approach is novel and theoretically validated, and the experimental results are convincing.	B-strength
2019-363	The authors addressed nearly all of the reviewer's concerns.	B-rebuttal_process
2019-363	I recommend acceptance.	B-decision

2019-375	BMIs need per-patient and per-session calibration, and this paper seeks to amend that.	B-abstract
2019-375	Using VAEs and RNNs, it relates sEEG to sEMG, in principle a ten-year old approach, but do so using a novel adversarial approach that seems to work. <sep>	I-abstract
2019-375	The reviewers agree the approach is nice, the statements in the paper are too strong, but publication is recommended.	B-rating_summary
2019-375	Clinical evaluation is an important next step.	B-suggestion

2019-401	This paper presents good empirical results on an important and interesting task (translation between several language pairs with a single model).	B-abstract
2019-401	There was solid communication between the authors and the reviewers leading to an improved updated version and consensus among the reviewers about the merits of the paper.	B-rebuttal_process

2019-420	The reviewers lean to accept, and the authors clearly put a significant amount of time into their response.	B-rebuttal_process
2019-420	I will also lean to accept.	B-decision
2019-420	However, the comments of reviewer 2 should be taken seriously, and addressed if possible, including an attempt to cut the paper length down.	B-suggestion

2019-467	This paper provides interesting results on convergence and stability in general differentiable games.	B-abstract
2019-467	The theory appears to be correct, and the paper reasonably well written.	B-strength
2019-467	The main concern is in connections to an area of related work that has been omitted, with overly strong statements in the paper that there has been little work for general game dynamics.	B-weakness
2019-467	This is a serious omission, since it calls into question some of the novelty of the results because they have not been adequately placed relative to this work.	I-weakness
2019-467	The authors should incorporate a thorough discussion on relations to this work, and adjust claims about novelty (and potentially even results) based on that literature.	B-suggestion

2019-479	This heavily disputed paper discusses a biologically motivated alternative to back-propagation learning.	B-abstract
2019-479	In particular, methods focussing on sign-symmetry rather than weight-symmetry are investigated and, importantly, scaled to large problems.	I-abstract
2019-479	The paper demonstrates the viability of the approach.	I-abstract
2019-479	If nothing else, it instigates a wonderful platform for debate. <sep>	B-strength
2019-479	The results are convincing and the paper is well-presented.	I-strength
2019-479	But the biological plausibility of the methods needed for these algorithms can be disputed.	B-weakness
2019-479	In my opinion, these are best tackled in a poster session, following the good practice at neuroscience meetings. <sep>	B-suggestion
2019-479	On an aside note, the use of the approach to ResNet should be questioned.	B-weakness
2019-479	The skip-connections in ResNet may be all but biologically relevant.	B-suggestion

2019-495	This paper proposes a VAE model with arbitrary conditioning.	B-abstract
2019-495	It is a novel idea, and the model derivation and training approach are technically sound.	B-strength
2019-495	Experiments are thoughtfully designed and include comparison with latest related works. <sep>	I-strength
2019-495	R1 and R3 suggested the original version of the paper was lack of comparison with relevant work and the authors provided new experiments in the revision.	B-rebuttal_process
2019-495	The rebuttal also addressed a few other concerns about the novelty and clarity raised by R3. <sep>	I-rebuttal_process
2019-495	Based on the novel contribution in handling missing feature imputation with VAE, I would recommend to accept the paper.	B-decision
2019-495	It is worth noticing that there is another submission to ICLR (https://openreview.net/forum?id=ByxLl309Ym) that shares a similar idea of constructing the inference network with binary masking, although it is designed for a pre-trained VAE model. <sep>	O
2019-495	There are still two weaknesses pointed out by R3 that would help improve the paper by addressing them: <sep> 1. The paper does not handle different kinds of missingness beyond missing at random. <sep>	B-weakness
2019-495	2. VAE model makes the trade-off between computational complexity and accuracy. <sep>	I-weakness
2019-495	Point 1 would be a good direction for future research, and point 2 is a common problem for all VAE approaches.	I-weakness
2019-495	While the latter should not become a reason to reject the paper, I encourage the authors to take MCMC methods into account in the evaluation section. <sep>	B-suggestion

2019-503	The paper proposes an approach to hyperparameter tuning based on bilevel optimization, and demonstrates promising empirical results.	B-abstract
2019-503	Reviewer's concerns seem to be addressed well in rebuttals and extended version of the paper.	B-rebuttal_process

2019-509	The proposed ENorm procedure is a normalization scheme for neural nets whereby the weights are rescaled in a way that minimizes the sum of L_p norms while maintaining functional equivalence.	B-abstract
2019-509	An algorithm is given which provably converges to the globally optimal solution.	I-abstract
2019-509	Experiments show it is complementary to, and perhaps slightly better than, other normalization schemes. <sep>	I-abstract
2019-509	Normalization issues are important for DNN training, and normalization schemes like batch norm, weight norm, etc.	I-abstract
2019-509	have the unsatisfying property that they entangle multiple issues such as normalization, stochastic regularization, and effective learning rates.	I-abstract
2019-509	ENorm is a conceptually cleaner (if more algorithmically complicated) approach.	B-strength
2019-509	It's a nice addition to the set of normalization schemes, and possibly complementary to the existing ones. <sep>	I-strength
2019-509	After a revision which included various new experiments, the reviewers are generally happy with the paper.	B-rebuttal_process
2019-509	While there's still some controversy over whether it's really better than things like batch norm, I think the paper would be worth publishing even if the results came out negative, since it is a very natural idea which took some algorithmic insight in order to actually execute. <sep>	B-decision

2019-513	The paper presents a variational inequality perspective on the optimization problem arising in GANs.	B-abstract
2019-513	Convergence of stochastic gradient descent methods (averaging and extragradient variants) is given under monotonicity (or convex) assumptions.	I-abstract
2019-513	In particular, binlinear saddle point problem is carefully studied with batch and stochastic algorithms.	I-abstract
2019-513	Experiments on CIFAR10 with WGAN etc.	I-abstract
2019-513	show that the proposed averaging and extrapolation techniques improve the GAN training in such a nonconvex optimization practices. <sep>	I-abstract
2019-513	General convergence results in the context of general non-monotone VIPs is still an open problem for future exploration.	I-abstract
2019-513	The questions raised by the reviewers are well answered.	B-rebuttal_process
2019-513	The reviewers unanimously accept the paper for ICLR publication.	B-rating_summary

2019-552	This paper considers the information bottleneck Lagrangian as a tool for studying deep networks in the common case of supervised learning (predicting label Y from features X) with a deterministic model, and identifies a number of troublesome issues.	B-abstract
2019-552	(1) The information bottleneck curve cannot be recovered by optimizing the Lagrangian for different values of  because in the deterministic case, the IB curve is piecewise linear, not strictly concave.	I-abstract
2019-552	(2) Uninteresting representations can lie on the IB curve, so information bottleneck optimality does not imply that a representation is useful.	I-abstract
2019-552	(3) In a multilayer model with a low probability of error, the only tradeoff that successive layers can make between compression and prediction is that deeper layers may compress more.	I-abstract
2019-552	Experiments on MNIST illustrate these issues, and supplementary material shows that these issues also apply to the deterministic information bottleneck and to stochastic models that are nearly deterministic.	I-abstract
2019-552	There was a substantial degree of disagreement between the reviewers of this paper.	B-rating_summary
2019-552	One reviewer (R3) suggested that all the conclusions of the paper are the consequence of P(X,Y) being degenerate.	B-weakness
2019-552	The authors responded to this criticism in their response and revision quite effectively, in the opinion of the AC.	B-rebuttal_process
2019-552	Because R3 failed to participate in the discussion, this review has been discounted in the final decision.	I-rebuttal_process
2019-552	The other two reviewers were considerably more positive about the paper, with one (R1) having basically no criticisms and the other (R2) expression some doubts about the novelty of the observations being made in the paper and their importance for practical machine learning scenarios.	B-rating_summary
2019-552	Following the revision and discussion, R2 expressed general satisfaction with the paper, so the AC is recommending acceptance.	B-decision
2019-552	The AC thinks that the final paper would be clearer if the authors were to carefully distinguish between ground-truth labels used in training and the labels estimated by the model for a given input.	B-suggestion
2019-552	At the moment, the symbol Y appears to be overloaded, standing for both.	B-weakness
2019-552	Perhaps the authors should place a hat over Y when it is standing for estimated labels?	B-suggestion

2019-574	The paper presents generative models to produce multi-agent trajectories.	B-abstract
2019-574	The approach of  using a simple heuristic labeling function that labels variables that would otherwise be latent in training data is novel and and results in higher quality than the previously proposed baselines. <sep>	B-strength
2019-574	In response to reviewer suggestions, authors included further results with models that share parameters across agents as well as agent-specific parameters and further clarifications were made for other main comments (ie, baselines that train the hierarchical model by maximizing an ELBO on the marginal likelihood?).	B-rebuttal_process

2019-594	This paper combines probabilistic models, VAEs, and self-organizing maps to learn interpretable representations on time series.	B-abstract
2019-594	The proposed contributions are a novel and interesting combination of existing ideas, in particular, the extension to time-series data by modeling the cluster dynamics.	I-abstract
2019-594	The empirical results show improved unsupervised clustering performance, on both synthetic and real datasets, compared to a number of baselines.	I-abstract
2019-594	The resulting 2D embedding also provides an interpretable visualization. <sep>	I-abstract
2019-594	The reviewers and the AC identified a number of potential weaknesses in the presentation in the original submission: (1) there was insufficient background on SOMs, leaving the readers unable to comprehend the contributions, (2) some of the details about the experiments were missing, such as how the baselines were constructed, (3) additional experiments were needed in regards to the hyper-parameters, such as number of clusters and the weighting in the loss, and (4) Figure 4d required a description of the results. <sep>	B-weakness
2019-594	The revision and the comments by the authors addressed most of these comments, and the reviewers felt that their concerns had been alleviated. <sep>	B-rebuttal_process
2019-594	Thus, the reviewers felt the paper should be accepted.	B-rating_summary

2019-598	The reviewers all agreed that the problem application is interesting, and that there is little new methodology, but disagreed as to how that should translate into a score.	B-rating_summary
2019-598	The highest rating seemed to heavily weight the importance of the method to biological application, whereas the lowest rating heavily weighted the lack of technical novelty.	I-rating_summary
2019-598	However, because the ICLR call for papers clearly calls out applications in biology, and all reviewers agreed on its strength in that regard, and it was well-written and executed, I would recommend it for acceptance.	B-decision

2019-628	Pros: <sep> - a method that obtains convergence results using a using time-dependent (not fixed or state-dependent) softmax temperature. <sep>	B-strength
2019-628	Cons: <sep> - theoretical contribution is not very novel <sep>	B-weakness
2019-628	- some theoretical results are dubious <sep>	I-weakness
2019-628	- mismatch of Boltzmann updates and epsilon-greedy exploration <sep>	I-weakness
2019-628	- the authors seem to have intended to upload a revised version of the paper, but unfortunately, they changed only title and abstract, not the pdf -- and consequently the reviewers did not change their scores. <sep>	I-weakness
2019-628	The reviewers agree that the paper should be rejected in the submitted form.	B-rating_summary

2019-651	This paper proposes a new method for graph representation in sequence-to-sequence models and validates its results on several tasks.	B-abstract
2019-651	The overall results are relatively strong. <sep>	B-strength
2019-651	Overall, the reviewers thought this was a reasonable contribution if somewhat incremental.	I-strength
2019-651	In addition, while the experimental comparison has greatly improved from the original version, there are still a couple of less satisfying points: notably the size of the training data is somewhat small.	B-weakness
2019-651	In addition, as far as I can tell all comparisons with other graph-based baselines actually aren't implemented in the same toolkit with the same hyperparameters, so it's a bit difficult to tell whether the gains are coming from the proposed method itself or from other auxiliary differences. <sep>	I-weakness
2019-651	I think this paper is very reasonable, and definitely on the borderline for acceptance, but given the limited number of slots available at ICLR this year I am leaning in favor of the other very good papers in my area.	B-decision

2019-658	The paper proposes a new method for adversarial attacks, MarginAttack, which finds adversarial examples with small distortion and runs faster than the CW baseline, but slower than other methods.	B-abstract
2019-658	The authors provide theoretical guarantees and a broad set of experiments. <sep>	I-abstract
2019-658	In the discussion, a consistent concern has been that, experimentally, the method does not perform noticeably better than previous approaches.	B-weakness
2019-658	The authors mention that the lines are too thick to reveal the difference.	I-weakness
2019-658	It has been pointed out that this might be related to the way the experiments are conducted, but the proposed method still does better than other methods.	B-strength
2019-658	AnonReviewer1 mentions that the assumptions needed for the theoretical part might be too strong, meaning that the main contribution of the paper is in the experimental side. <sep>	B-weakness
2019-658	The comparisons with other methods and the assumptions made in the theorems seem to have caused quite some confusion and there was a fair amount of discussion.	I-weakness
2019-658	Following the discussion session, AnonReviewer1 updated his rating from 5 to 6 with high confidence. <sep>	B-rebuttal_process
2019-658	The referees all rate the paper as not very strong, with one marginally above acceptance threshold and two marginally below the acceptance threshold. <sep>	B-rating_summary
2019-658	Although the paper seems to propose valuable ideas, and it appears that the discussion has clarified many questions from the initial submission, the paper has not provided a clear, convincing, selling point at this time.	B-weakness

2019-676	The work presents a method of imposing harmonic structural regularizations to layers of a neural network.	B-abstract
2019-676	While the idea is interesting, the reviewers point out multiple issues. <sep>	O
2019-676	Pros: <sep> + Interesting method <sep>	B-strength
2019-676	+ Hidden layer coherence tends to improve <sep>	I-strength
2019-676	Cons: <sep> - Deficient comparisons to baselines or context with other works. <sep>	B-weakness
2019-676	- Insufficient assessment of impact to model performance. <sep>	I-weakness
2019-676	- Lack of strategy to select regularizers <sep>	I-weakness
2019-676	- Lack of evaluation on more realistic datasets	I-weakness

2019-685	The paper can also improved thorough a more thorough evaluation.	B-weakness

2019-690	This paper proposes a method for improving robustness to black-box adversarial attacks by replacing the cross-entropy layer with an output vector encoding scheme.	B-abstract
2019-690	The paper is well-written, and the approach appears to be novel.	B-strength
2019-690	However, Reviewer 4 raises very relevant concerns regarding the experimental evaluation of the method, including (a) lack of robustness without AT in the whitebox case (which is very relevant as we still lack good understanding of blackbox vs whitebox robustness) (b) comparison with Kannan et al and (c) lack of some common strong attacks.	B-weakness
2019-690	Reviewer 1 echoes many of these concerns.	O

2019-707	This paper proposes to quantify the uncertainty of neural network models with Beta, Dirichlet and Dirichlet-Multinomial likelihood.	B-abstract
2019-707	This paper is clearly written with a sound main idea.	B-strength
2019-707	However, it is a common practice to model different types of data with different likelihood, although the proposed distributions are not usually used for network output.	B-weakness
2019-707	All the reviewers therefore considered this paper to be of limited novelty.	I-weakness
2019-707	Reviewer 2 also had a concern about the mixed experimental results of the proposed method. <sep>	I-weakness
2019-707	Reviewer 3 raised the concern that this paper did not model the uncertainty of prediction from the uncertainty of the model parameters.	I-weakness
2019-707	It is a common consideration in a Bayesian approach and I encourage the authors to discussed different sources of uncertainty in future revisions.	B-decision

2019-725	This method proposes a primal approach to minimizing Wasserstein distance for generative models.	B-abstract
2019-725	It estimates WD by computing the exact WD between empirical distributions. <sep>	I-abstract
2019-725	As the reviewers point out, the primal approach has been studied by other papers (which this submission doesn't cite, even in the revision), and suffers from a well-known problem of high variance.	B-weakness
2019-725	The authors have not responded to key criticisms of the reviewers.	B-rebuttal_process
2019-725	I don't think this work is ready for publication in ICLR. <sep>	B-decision

2019-745	In this work, the authors conduct experiments using variants of RNNs and Gated CNNs on a speech recognition task, motivated by the goal of reducing the computational requirements when deploying these models on mobile devices. <sep>	B-abstract
2019-745	While this is an important concern for practical deployment of ASR systems, the main concerns expressed by the reviewers is that the work lacks novelty.	B-weakness
2019-745	Further, the authors choice to investigate CTC based systems which predict characters.	I-weakness
2019-745	These models are not state-of-the-art for ASR, and as such it is hard to judge the impact of this work on a state-of-the-art embedded ASR system.	I-weakness
2019-745	Finally, it would be beneficial to replicate results on a much larger corpus such as Librispeech or Switchboard.	B-suggestion
2019-745	Based on the unanimous decision from the reviewers, the AC agrees that the work, in the present form, should be rejected. <sep>	B-decision

2019-764	This paper focuses on scaling up neural theorem provers, a link prediction system that combines backward chaining with neural embedding of facts, but does not scale to most real-world knowledge bases.	B-abstract
2019-764	The authors introduce a nearest-neighbor search-based method to reduce the time/space complexity, along with an attention mechanism that improves the training.	I-abstract
2019-764	With these extensions, they scale NTP to modern benchmarks for the task, including ones that combine text and knowledge bases, thus providing explanations for such models. <sep>	I-abstract
2019-764	The reviewers and the AC note the following as the primary concerns of the paper: (1) the novelty of the contributions is somewhat limited, as nearest neighbor search and attention are both well-known strategies, as is embedding text+facts jointly, (2) there are several issues in the evaluation, in particular around analysis of benefits of the proposed work on new datasets.	B-weakness
2019-764	There were a number of other potential weaknesses, such the performance on some benchmarks (Fb15k) and clarity and writing quality of a few sections. <sep>	I-weakness
2019-764	The authors provided significant revisions to the paper that addressed many of the clarity and evaluation concerns, along with providing sufficient comments to better contextualize some of the concerns.	B-rebuttal_process
2019-764	However, the concerns with novelty and analysis of the results still hold.	I-rebuttal_process
2019-764	Reviewer 3 mentions that it is still unclear in the discussion why the accuracy of the proposed approach matches/outperforms that of NTP, ie why is there not a tradeoff.	I-rebuttal_process
2019-764	Reviewer 4 also finds the analysis lacking, and feels that the differences between the proposed work and the single-link approaches, in terms of where each excels, are described in insufficient detail.	I-rebuttal_process
2019-764	Reviewer 4 focused more on the simplicity of the text encoding, which restricts the novelty as more sophisticated text embeddings approaches are commonplace. <sep>	I-rebuttal_process
2019-764	Overall, the reviewers raised different concerns, and although all of them appreciated the need for this work and the revisions provided by the authors, ultimately feel that the paper did not quite meet the bar.	B-decision

2019-774	This paper presents a model for question answering, where the idea is to have a collaborative model that aligns queries and sentences on a small supervised dataset and also uses semi-supervised information from a weakly supervised corpus to answer open domain questions resulting in short answer spans. <sep>	B-abstract
2019-774	The main criticism of the paper is regarding its novelty, and reviewers cite the similarities with prior work such as Chen et al and Min et al  There is relative consensus between the reviewers that further work using the semi-supervised outlook with stronger results could strengthen the paper further.	B-weakness

2019-786	The reviewers agreed that there are some promising ideas in this work, and useful empirical analysis to motivate the approach.	B-strength
2019-786	The main concern is in the soundness of the approach (for example, comments about cumulative learning and negative samples).	B-weakness
2019-786	The authors provided some justification about using previous networks as initialization, but this is an insufficient discussion to understand the soundness of the strategy.	B-rebuttal_process
2019-786	The paper should better discuss this more, even if it is not possible to provide theory.	B-weakness
2019-786	The paper could also be improved with the addition of a baseline (though not necessarily something like DeepStack, which is not publicly available and potentially onerous to reimplement).	B-suggestion

2019-791	Reviewer ratings varied radically (from a 3 to an 8).	B-rating_summary
2019-791	However, the reviewer rating the paper as 8 provided extremely little justification for their rating.	O
2019-791	The reviewers providing lower ratings gave more detailed reviews, and also engaged in  discussion with the authors.	B-rebuttal_process
2019-791	Ultimately neither decided to champion the paper, and therefore, I cannot recommend acceptance.	B-decision

2019-797	there is a disagreement among the reviewers, and i am siding with the two reviewers (r1 and r3) and agree with r3 that it is rather unconventional to pick learning-to-learn to experiment with modelling variable-length sequences (it's not like there's no other task that has this characteristics, eg, language modelling, translation,..)	B-weakness

2019-798	The paper proposes a novel approach to interfacing robots with humans, or rather vv: by mapping instructions to goals, and goals to robot actions.	B-abstract
2019-798	A possibly nice idea, and possibly good for more efficient learning. <sep>	B-strength
2019-798	But the technical realisation is less strong than the initial idea.	B-weakness
2019-798	The original idea merits a good evaluation, and the authors are strongly encouraged to follow up on this idea and realise it, towards a stronger publication. <sep>	B-suggestion
2019-798	It be noted that the authors refrained from using the rebuttal phase.	B-rebuttal_process

2019-801	This paper presents an extensive empirical study to sentence-level pre-training.	B-abstract
2019-801	The paper compares pre-trained language models to other potential alternative pre-training options, and concludes that while pre-trained language models are generally stronger than other alternatives, the robustness and generality of the currently available method is less than ideal, at least with respect to ELMO-based pretraining. <sep>	I-abstract
2019-801	Pros: <sep> The paper presents an extensive empirical study that offers new insights on pre-trained language models with respect to a variety of sentence-level tasks. <sep>	B-strength
2019-801	Cons: <sep> The primarily contributions of this paper is empirical and technical novelty is relatively weak.	B-weakness
2019-801	Also, the insights are based just on ELMO, which may have a relatively weak empirical impact.	I-weakness
2019-801	The reviews were generally positive but marginally positive, which reflect that insights are interesting but not overwhelmingly interesting.	I-weakness
2019-801	None of these is a deal-breaker per say, but the paper does not provide sufficiently strong novelty, whether based on insights or otherwise, relative to other papers being considered for acceptance. <sep>	I-weakness
2019-801	Verdict: <sep> Leaning toward reject due to relatively weak novelty and empirical impact. <sep>	B-decision
2019-801	Additional note on the final decision: <sep> The insights provided by the paper are valuable, thus the paper was originally recommended for an accept.	B-strength
2019-801	However, during the calibration process across all areas, it became evident that we cannot accept all valuable papers, each presenting different types of hard work and novel contributions.	O
2019-801	Consequently, some papers with mostly positive (but marginally positive) reviews could not be included in the final cut, despite their unique values, hard work, and novel contributions.	O

2019-806	The paper proposes an approach to remedying mode collapse problem in GANs.	B-abstract
2019-806	This approach relies on using multiple discriminators and assigning a different portion of each minibatch to each discriminator. <sep>	I-abstract
2019-806	+ preventing mode collapse in GAN training is an important problem <sep>	B-strength
2019-806	- the exact motivation for the proposed techniques is not fully fleshed out <sep>	B-weakness
2019-806	- the evaluation and baselines used are lacking	I-weakness

2019-812	The authors propose using a SVM, trained as a last layer of a neural network, to identify exemplars (support vectors) to save and use to prevent forgetting as the model is trained on further tasks.	B-abstract
2019-812	The method is effective on several supervised benchmarks and is compared to several other methods, including VCL, iCARL, and GEM.	I-abstract
2019-812	The reviewers had various objections to the initial paper that centered around comparisons to other methods and reporting of detailed performance numbers, which the authors resolved convincingly in their revised paper.	B-rebuttal_process
2019-812	However, the AC and 2 of the reviewers were unconvinced of the contribution of the approach.	B-weakness
2019-812	Although no one has used this particular strategy, of using support vectors to prevent forgetting, the approach is a simplistic composition of the NN and the SVM which is heuristic, at least in how the authors present it.	I-weakness
2019-812	Most importantly, the approach is limited to supervised classification problems, yet catastrophic forgetting is not commonly considered to be a problem for the supervised classifier setting; rather it is a problem for inherently sequential learning environments such as RL (MNIST and CIFAR are just commonly used in the literature for ease of evaluation).	I-weakness

2019-837	The paper studies the problem of reinforcement learning under certain constraints on action sequences.	B-abstract
2019-837	The reviewers raised important concerns regarding (1) the general motivation, (2) the particular formulation of constraints in terms of action sequences and (3) the relevance and significance of experimental results.	B-rebuttal_process
2019-837	The authors did not submit a rebuttal.	I-rebuttal_process
2019-837	Given the concerns raised by the reviewers, I encourage the authors to improve the paper to possibly resubmit to another venue.	B-decision

2019-841	The method under consideration uses parallel convolutional filter groups per layer, where activations are averaged between the groups, forming "inner ensembles". <sep>	B-abstract
2019-841	Reviewers raised a number of concerns, including the increased computational cost for apparently little performance gain, the choice of base architecture (later addressed with additional experiments using WideResNet and ResNeXt), issues of clarity of presentation (some of which were addressed).	B-weakness
2019-841	One reviewer was unconvinced without direct comparison to full ensembles.	I-weakness
2019-841	Another reviewer raised the issue of a missing direct comparison to the most similar method in the literature, maxout (Goodfellow et al, 2013).	I-weakness
2019-841	Authors rebutted this by claiming that maxout is difficult to implement and offering vague arguments for its inferiority to their method. <sep>	B-rebuttal_process
2019-841	The AC agrees that a maxout baseline is important here, as it is extremely close to the proposed method and also trivially implemented, and that in light of maxout (and other related methods) the degree of novelty is limited.	B-weakness
2019-841	The AC also concurs that a full ensemble baseline would strengthen the paper's claims.	I-weakness
2019-841	In the absence of either of these the AC concurs with the reviewers that this work is not suitable for publication at this time.	B-decision

2019-842	The authors propose a scheme to learn a mapping between the discrete space of network architectures into a continuous embedding, and from the continuous embedding back into the space of network architectures.	B-abstract
2019-842	During the training phase, the models regress the number of parameters, and expected accuracy given the continuous embedding.	I-abstract
2019-842	Once trained, the model can be used for compression by first embedding the network structure and then performing gradient descent to maximize accuracy by minimizing the number of parameters.	I-abstract
2019-842	The optimized representation can then be mapped back into the discrete architecture space. <sep>	I-abstract
2019-842	Overall, the main idea of this work is very interesting, and the experiments show that the method has some promise.	B-strength
2019-842	However, as was noted by the reviewers, the paper could be significantly strengthened by performing additional experiments and analyses.	B-weakness
2019-842	As such, the AC agrees with the reviewers that the paper in its present form is not suitable for acceptance, but the authors are encouraged to revise and resubmit this work to a future venue. <sep>	B-decision

2019-848	I appreciate the willingness of the authors to engage in vigorous discussion about their paper.	O
2019-848	Although several reviewers support accepting this submission, I do not find their arguments for acceptance convincing.	B-ac_disagreement
2019-848	The paper considers automated methods for finding errors in text classification models.	B-abstract
2019-848	I believe it is valuable to study the errors our models make in order to understand when they work well and how to improve them.	B-strength
2019-848	Crucially, in the later case, we should demonstrate how to use the errors we find to close the loop and create better models. <sep>	B-suggestion
2019-848	A paper about techniques to find errors for text models should make a sufficiently large contribution to be accepted.	O
2019-848	I view the following hypothetical contributions as the most salient in this specific case thus my decision reduces to determining if any of these conditions have been met.	O
2019-848	A paper need not achieve all of these things, any one of them would suffice: <sep> 1. Show that the errors found can be used to meaningfully improve the models. <sep>	B-suggestion
2019-848	This requires building a better model than the one probed by the method and convincingly demonstrating that it is superior in an important way that is relevant to the original goals of the application.	I-suggestion
2019-848	Ideally it would also consider alternative, simpler ways to improve the models (eg making them larger). <sep>	I-suggestion
2019-848	2. Show that errors are difficult to find, but that the proposed method is nonetheless capable of finding errors and that the method is non-obvious to a researcher in the field. <sep>	I-suggestion
2019-848	This is not applicable here because errors are extremely easy to find on the test set and from labeling more data.	I-suggestion
2019-848	If we demand an automated method, then the greedy algorithm does not qualify as sufficiently non-obvious and it seems to work fine, making the Gumbel method unnecessary. <sep>	I-suggestion
2019-848	3. Show that the particular specific errors found are qualitatively different from other errors in their implications and that they provide a unique and important insight. <sep>	I-suggestion
2019-848	I do not believe this submission attempts to show this type of contribution.	B-weakness
2019-848	One example of this type of paper would be a paper that does a comparative study of the errors that different models make and finds something interesting (potentially yielding a path to improved models). <sep>	B-suggestion
2019-848	4. Generate a new, more difficult/interesting, dataset by finding errors of one or more trained models <sep>	I-suggestion
2019-848	Given that the authors use human labelers to validate examples this is potentially another path.	I-suggestion
2019-848	Here is an example of a paper using adversarial techniques in this way: https://arxiv.org/abs/1808.05326 <sep>	I-suggestion
2019-848	However, I believe the paper would need to be rethought and rewritten to make this sort of contribution. <sep>	I-suggestion
2019-848	Ultimately, the authors and reviews supporting acceptance must explain the contribution succinctly and convincingly.	I-suggestion
2019-848	The reviewers most strongly advocating for accepting this submission seem to be saying that there is a valuable new method and probabilistic framework proposed here for finding model errors.	B-strength
2019-848	I believe researchers in the field could have easily come up with the greedy algorithm (a standard approach to discrete optimization problems) proposed here without needing to read the paper.	B-ac_disagreement
2019-848	Furthermore, I believe the other more complicated Gumbel algorithm proposed is not necessary given the similarly effective and simpler greedy algorithm.	B-suggestion
2019-848	If the authors believe that the Gumbel algorithm provides application-relevant advantages over the greedy algorithm, then they should specify how these errors will be used and rewrite the paper to make the greedy algorithm a baseline.	I-suggestion
2019-848	However, I do not believe the experimental results support this idea. <sep>	B-weakness

2019-862	This paper present a framework for creating meaning-preserving adversarial examples.	B-abstract
2019-862	It then proposes two attacks within this framework: one based on k-NN in the word embedding space, and another one based on character swapping. <sep>	I-abstract
2019-862	Overall, the goal of constructing such meaning-preserving attacks is very interesting.	B-strength
2019-862	However, it is unclear how successful the proposed approach really is in the context of this goal. <sep>	B-weakness
2019-862	Additionally, it is not clear how much novelty there is compared to already existing methods that have a very similar aim.	I-weakness

2019-864	Average score of 3.33, highest score of 4. <sep>	B-rating_summary
2019-864	The AC recommends rejection. <sep>	B-decision

2019-872	The paper proposes to take into accunt the label structure for classification tasks, instead of a flat N-way softmax.	B-abstract
2019-872	This also lead to a zero-shot setting to consider novel classes.	I-abstract
2019-872	Reviewers point to a lack of reference to prior work and comparisons.	B-weakness
2019-872	Authors have tried to justify their choices, but the overall sentiment is that it lacks novelty with respect to previous approaches. <sep>	B-rebuttal_process
2019-872	All reviewers recommend to reject, and so do I.	B-decision

2019-875	The authors propose a scheme to compress models using student-teacher distillation, where training data are augmented using examples generated from a conditional GAN. <sep>	B-abstract
2019-875	The reviewers were generally in agreement that 1) that the experimental results generally support the claims made by the authors, and 2) that the paper is clearly written and easy to follow. <sep>	B-strength
2019-875	However, the reviewers also raised a number of concerns: 1) that the experiments were conducted on small-scale tasks, 2) the use of the compression score might be impractical since it would require retraining a compressed model, and is affected by the effectiveness of the compression algorithm which is an additional confounding factor.	B-weakness
2019-875	The authors in their rebuttal address 2) by noting that the student training was not too expensive, but I believe that this cost is task specific.	B-rebuttal_process
2019-875	Overall, I think 1) is a significant concern, and the AC agrees with the reviewers that an evaluation of the techniques on large-scale datasets would strengthen the paper. <sep>	B-weakness

2019-881	This paper presents a reinforcement learning approach to hierarchical text classification. <sep>	B-abstract
2019-881	Pros: A potentially interesting idea to drive the search process over a hierachical set of labels using reinforcement learning. <sep>	B-strength
2019-881	Cons: The major concensus among all reviewers was that there were various concerns about experimental results, eg, apple-to-apple comparisons against prior art (R1), proper tuning of hyper-parameters (R1, R2), the label space is too small (539) to have practical significance compared to tens of thousands of labels that have been used in other related work (R3), and other missing baselines (R3).	B-weakness
2019-881	In addition, even after the rebuttal, some of the technical clarity issues have not been fully resolved, eg, what the proposed method is actually doing (optimizing F1 metric vs the ability to fix inconsistent labeling problem). <sep>	B-rebuttal_process
2019-881	Verdict: <sep> Reject.	B-decision
2019-881	While authors came back with many detailed responses, they were not enough to address the major concerns reviewers had about the empirical significance of this work.	B-rebuttal_process

2019-890	The paper proposes a new graph-based regularizer to improve the robustness of deep nets.	B-abstract
2019-890	The idea is to encourage smoothness on a graph built on the features at different layers.	I-abstract
2019-890	Experiments on CIFAR-10 show that the method provides robustness over very different types of perturbations such as adversarial examples or quantization.	I-abstract
2019-890	The reviewers raised concerns around the significance of the results, the reliance on a single dataset and the unexplained link between adversarial examples and the regularization.	B-weakness
2019-890	Despite the revision, the reviewers maintain their concerns.	B-rebuttal_process
2019-890	For this reason this work is not ready for publication.	B-decision

2019-899	This paper seeks to shed light on why seq2seq models favor generic replies.	B-abstract
2019-899	The problem is an important one, unfortunately the responses proposed in the paper are not satisfactory.	B-weakness
2019-899	Most reviewers note problems and general lack of rigorousness in the assumptions used to produce the theoretical part of the paper (eg, strong assumption of independence of generated words).	I-weakness
2019-899	The experiments themselves are not convincing enough to warrant acceptance by themselves.	B-decision

2019-902	Unfortunately, this paper fell just below the bar for acceptance.	B-decision
2019-902	The reviewers all saw significant promise in this work, stating that it is intriguing, "novel and provides an interesting solution to a challenging problem" and that "many interesting use cases are clear".	B-strength
2019-902	AnonReviewer2 particularly argued for acceptance, arguing that the proposed approach provides a very flexible method for incorporating constraints in neural network training.	I-strength
2019-902	A concern of AnonReviewer2 was that there was no guarantee that this loss would be convex or converge to an optimum while statisfying the constraints.	B-weakness
2019-902	The other two reviewers unfortunately felt that while the proposed approach was "interesting", "promising" and "intriguing", the quality of the paper, in terms of exposition, was too low to justify acceptance.	I-weakness
2019-902	Arguably, it seems the writing doesn't do the idea justice in this case and the paper would ultimately be significantly more impactful if it was carefully rewritten.	B-suggestion

2019-906	The reviewers agree the paper is not ready for publication.	B-rating_summary

2019-919	It is a simple but good idea to consider the choice of mini-batch size as a multi-armed bandit problem.	B-strength
2019-919	Experiments also show a slight improvement compared to the best fixed batch size. <sep>	I-strength
2019-919	The main concerns from the reviewers are that (1) treating the choice of hyper-parameters as a bandit problem is known and has been exploited in different context, and this paper is limited to the choice of the mini-batch size, (2) the improvement in the test error is not significant.	B-weakness
2019-919	The authors' feedback did not solve the concerns raised by R2. <sep>	B-rebuttal_process
2019-919	This paper conveys a nice idea but as the current form it falls slightly below the standard of the ICLR publications.	B-decision
2019-919	One direction for improvement, as suggested by the reviewer, would be extending the idea for a wider hyper-parameter selection problems.	B-suggestion

2019-931	This paper extends the DiCE estimator with a better control variate baseline for variance reduction. <sep>	B-abstract
2019-931	The reviewers all think the paper is fairly clear and well written.	B-strength
2019-931	However, as the reviews and discussion indicates,  there are several critical issues, including lack of explanation of the choice of baseline, the lack more realistic experiments and a few misleading assertions.	B-weakness
2019-931	We encourage the authors to rewrite the paper to address these criticism.	B-suggestion
2019-931	We believe this work will make a successful submission with proper modification in the future. <sep>	I-suggestion

2019-938	-pros: <sep> - good, sensible idea <sep>	B-strength
2019-938	- good evaluations on the domains considered <sep>	I-strength
2019-938	- good analysis <sep>	I-strength
2019-938	-cons: <sep> - novelty, broader evaluation <sep>	B-weakness
2019-938	I think this is a good and interesting paper and I appreciate the authors' engagment with the reviewers.	O
2019-938	I agree with the authors that it is not fair to compare their work to a blog post which hasn't been published and I have taken this into account.	B-ac_disagreement
2019-938	However, there is still concern among the reviewers about the strength of the technical contribution and the decision was made not to accept for ICLR this year.	B-decision

2019-945	This paper was reviewed by three experts.	O
2019-945	Initially, the reviews were mixed with several concerns raised.	B-rebuttal_process
2019-945	After the author response, there continue to be concerns about need for significantly more experiments.	I-rebuttal_process
2019-945	If this were a journal, it is clear that recommendation would be "major revision".	B-decision
2019-945	Since that option is not available and the paper clearly needs another round of reviews, we must unfortunately reject.	I-decision
2019-945	We encourage the authors to incorporate reviewer feedback and submit a stronger manuscript at a future venue.	I-decision

2019-977	I appreciate that the authors are refuting a technical claim in Poole et al, however the paper has garnered zero enthusiasm the way it is written.	B-weakness
2019-977	I suggest to the authors that they rewrite the paper as a refutation of Poole et al, and name it as such.	B-suggestion

2019-982	This paper studies the role of pooling in the success underpinning CNNs.	B-abstract
2019-982	Through several experiments, the authors conclude that pooling is neither necessary nor sufficient to achieve deformation stability, and that its inductive bias can be mostly recovered after training. <sep>	I-abstract
2019-982	All reviewers agreed that this is a paper asking an important question, and that it is well-written and reproducible.	B-strength
2019-982	On the other hand, they also agreed that, in its current form, this paper lacks a 'punchline' that can drive further research.	B-weakness
2019-982	In words of R6, "the paper does not discuss the links between pooling and aliasing", or in words of R4, "it seems to very readily jump to unwarranted conclusions".	I-weakness
2019-982	In summary, the AC recommends rejection at this time, and encourages the authors to pursue the line of attack by exploring the suggestions of the reviewers and resubmit.	B-decision

2019-992	This paper proposes a VAE-based model which is able to perform musical timbre transfer.<sep>	B-abstract
2019-992	The reviewers generally find the approach well-motivated.	B-strength
2019-992	The idea to perform many-to-many transfer within a single architecture is found to be promising.	I-strength
2019-992	However, there have been some unaddressed concerns, as detailed below. <sep>	O
2019-992	R3 has some methodological concerns  regarding negative transfer and asks for more extended experimental section.	B-weakness
2019-992	R1 and R2 ask for more interpretable results and, ultimately, a more conclusive study.	I-weakness
2019-992	R2 specifically finds the results to be insufficient. <sep>	I-weakness
2019-992	The authors have agreed with some of the reviewers' feedback but have left most of it unaddressed in a new revision.	B-rebuttal_process
2019-992	That could be because some of the recommendations require significant extra work. <sep>	I-rebuttal_process
2019-992	Given the above, it seems that this paper needs more work before being accepted in ICLR.	B-decision

2019-993	The paper received mixed ratings.	O
2019-993	The proposed idea is quite reasonable but also sounds somewhat incremental.	B-weakness
2019-993	While the idea of separating foreground/background is reasonable, it also limits the applicability of the proposed method (ie, the method is only demonstrated on aligned face images).	I-weakness
2019-993	In addition, combining AdaIn with foreground mask is a reasonable idea but doesn't sound groundbreakingly novel.	I-weakness
2019-993	The comparison against StarGAN looks quite anecdotal  and the proposed method seems to cause only hairstyle changes (but transfer with other attributes are not obvious).	I-weakness
2019-993	In addition, please refer to detailed reviewers' comments for other concerns.	O
2019-993	Overall, it sounds like a good engineering paper that might be better fit to computer vision venue, but experimental validation seems somewhat preliminary and it's unclear how much novel insight and general technical contributions that this work provides. <sep>	B-weakness

2019-1000	The authors propose a GAN-based anomaly detection method based on simulating anomalies (low density regions of the data space) in order to train an anomaly classifier. <sep>	B-abstract
2019-1000	While the paper addresses an interesting take on an important problem, there are many concerns raised by reviewers including novelty, clarity, attribution, reproducibility, the use of exclusively proprietary data, and a multitude of textual mistakes.	B-weakness
2019-1000	Overall, the paper shows promise but does not seem to be a mature and polished piece of work.	I-weakness
2019-1000	As there has been no rebuttal or update to the paper I have no choice but to concur with the reviewers' initial assessments and reject.	B-decision

2019-1014	The reviewers find the per difficult to read.	B-weakness
2019-1014	Reviewers also had concerns regarding the correctness of various claims in the paper.	I-weakness
2019-1014	The paper was also found lacking in experimental analysis, as it only tested on relatively small datasets, and only no a CNN architecture.	I-weakness
2019-1014	Overall, the paper appears to be lacking in quality and clarity, and questionable in correctness and originality.	I-weakness

2019-1020	This paper provides further insight into using RL for active learning, particularly by formulating AL as an MDP and then using RL methods for that MDP.	B-abstract
2019-1020	Though the paper has a few insights, it does not sufficiently place itself amongst the many other similar strategies using an MDP formulation.	B-weakness
2019-1020	I recommend better highlighting what is novel in this work (eg, more focus on the reward function, if that is key).	B-suggestion
2019-1020	Additionally, avoid general statements like "To this end, we formalize the annotation process as a Markov decision process", which suggests that this is part of the contribution, but as highlighted by reviewers, has been a standard approach.	I-suggestion

2019-1023	The paper conveys interesting idea but need more work in terms of fair empirical study and also improvement of the writing.	B-weakness
2019-1023	The AC based her summary only on the technical argumentation presented by reviewers and authors.	O

2019-1026	The authors propose a framework for compressing neural network models which involves applying a weight distortion function periodically as part of training.	B-abstract
2019-1026	The proposed approach is relatively simple to implement, and is shown to work for weight pruning, low-rank compression and quantization, without sacrificing accuracy. <sep>	B-strength
2019-1026	However, the reviewers had a number of concerns about the work.	O
2019-1026	Broadly, the reviewers felt that the work was incremental.	B-weakness
2019-1026	Further, if the proposed techniques are important to get the approach to work well in practice, then the paper would be significantly strengthened by further analyses.	I-weakness
2019-1026	Finally, the reviewers noted that the paper does not consider whether the specific weight pruning strategies result in a reduction of computational resources beyond potential storage savings, which would be important if this method is to be used in practice. <sep>	I-weakness
2019-1026	Overall, the AC tends to agree with the reviewers criticisms.	O
2019-1026	The authors are encouraged to address some of these issues in future revisions of the work. <sep>	B-suggestion

2019-1028	This paper presents a new defense against adversarial examples using random permutations and a Fourier transform.	B-abstract
2019-1028	The technique is clearly novel, and the paper is clearly written. <sep>	B-strength
2019-1028	However, as the reviewers and commenters pointed out, there is a significant degradation in natural accuracy, which does not seem to be easily recoverable.	B-weakness
2019-1028	This degradation is due to the random permutation of the images, which effectively disallows the use of convolutions. <sep>	I-weakness
2019-1028	Furthermore, Reviewer 1 points out that the baselines are insufficient, as the authors do not explore (a) learning the transformation, or (b) using expectation over transformation to attack the model. <sep>	I-weakness
2019-1028	This concern is further validated by the fact that Black-box attacks are often the best-performing, which is a sign of gradient masking.	I-weakness
2019-1028	The authors try to address this by performing an attack against an ensemble of models, and against a substitute model attack.	B-rebuttal_process
2019-1028	However, attacking an ensemble is not equivalent to optimizing the expectation, which would require sampling a new permutation at each step. <sep>	I-rebuttal_process
2019-1028	The paper thus requires significantly stronger baselines and attacks.	B-suggestion

2019-1033	The authors have presented an empirical study of generalization and regularization in DQN.	B-abstract
2019-1033	They evaluate generalization on different variants of Atari games and show that dropout and L2 regularization are beneficial.	I-abstract
2019-1033	The paper does not contain any major revelations, nor does it propose new algorithms or approaches, but it is a well-written and clear demonstration, and it would be interesting to the deep RL community.	B-strength
2019-1033	However, the reviewers did not feel that the paper met the bar for publication at ICLR because the experiments were not more comprehensive, which would be expected for an empirical study.	B-rating_summary
2019-1033	The AC will side with the reviewers but hopes that the authors will expand their study and resubmit to another venue in the future.	B-decision

2019-1034	This paper proposes a faster approximation to batch norm, which avoids summing over the entire batch by subsampling either random examples or random image locations.	B-abstract
2019-1034	It analyzes some of the tradeoffs of computation time vs. statistical efficiency of gradient estimation, and proposes schemes for decorrelating the samples to make good use of smaller numbers of samples. <sep>	I-abstract
2019-1034	The proposal is a reasonable one, and seems to give a noticeable improvement in efficiency.	B-strength
2019-1034	However, it's not clear there is a substantial enough contribution for an ICLR paper.	B-weakness
2019-1034	The idea of subsampling is fairly obvious, and various other methods have already been proposed which decouple the computation of BN statistics from the training batch.	I-weakness
2019-1034	From a practical standpoint, it's not clear that the observed benefit is large enough to justify the considerable complexity of an efficient implementation. <sep>	I-weakness

2019-1051	The authors present an interesting approach for combining finite state automata to compose new policies using temporal logic.	B-abstract
2019-1051	The reviewers found this contribution interesting but had several questions that suggests that the current paper presentation could be significantly clarified and situated with respect to other literature.	B-weakness
2019-1051	Given the strong pool of papers, this paper was borderline and the authors are encouraged to revise their paper to address the reviewers' feedback. <sep>	B-suggestion

2019-1068	The paper provides an interesting combination of existing techniques (such as GCN and and the Bernoulli-Poisson link) to address the problem of overlapping community detection.	B-abstract
2019-1068	However, there were concerns about lack of novelty, evaluation metrics, and missing comparisons with previous work.	B-weakness
2019-1068	The authors did not provide a response to address these concerns.	B-rebuttal_process

2019-1070	The reviewers appreciated the clarity of writing, and the importance of the problem being addressed.	B-rebuttal_process
2019-1070	There was a moderate amount of discussion around the paper, but the two reviewers who responded to the author discussion were split in their opinion, with one slightly increasing their score to a 6, and the other remaining unconvinced.	I-rebuttal_process
2019-1070	The scores overall are borderline for ICLR acceptance, and given that, no reviewer stepped forward to champion the paper.	B-rating_summary

2019-1081	This paper heavily modifies standard time-series-VAE models to improve their representation learning abilities.	B-abstract
2019-1081	However, the resulting model seems like an ad-hoc combination of tricks that lose most of the nice properties of VAEs.	B-weakness
2019-1081	The resulting method does not appear to be useful enough to justify itself, and it's not clear that the same ends couldn't be pursued using simpler, more general, and computationally cheaper approaches.	I-weakness

2019-1098	`This paper tackles the problem of learning with one hidden layer non-overlapping conv net for XOR detection problem.	B-abstract
2019-1098	For this problem the paper shows that over parametrized models perform better, giving insights into why larger neural networks generalize better - an interesting question to study.	I-abstract
2019-1098	However reviews opined that the setting considered in this paper is too specific to this XOR problem and the simplified network architecture,  and the techniques are not generalizable to other models.	B-weakness
2019-1098	Generalizing these results to more complex architectures or other learning problems will make the paper more interesting.	B-suggestion

2019-1120	The reviewers raised a number of concerns including the usefulness of the presented dataset given that the collected data is acted rather than naturalistic (and the large body of research in affective computing explains that models trained on acted data cannot generalise to naturalistic data), no methodological novelty in the presented work, and relatively uninteresting application with very limited real-world application (it remains unclear whether having better empathetic dialogues would be truly crucial for any real-life application and, in addition, all work is based on acted rather than real-world data).	B-weakness
2019-1120	The authors' rebuttal addressed some of the reviewers' concerns but not fully (especially when it comes to usefulness of the data).	B-rebuttal_process
2019-1120	Overall, I believe that the effort to collect the presented database is noble and may be useful to the community to a small extent.	B-strength
2019-1120	However, given the unrealism of the data and, in turn, very limited (if any) generalisability of the presented to real-world scenarios, and lack of methodological contribution, I cannot recommend this paper for presentation at ICLR.	B-decision

2019-1142	The submission proposes a hierarchical clustering approach (nested-means clustering) to determine good quantization intervals for non-uniform quantization.	B-abstract
2019-1142	An empirical validation shows improvement over a very closely related approach (Zhu et al, 2016). <sep>	I-abstract
2019-1142	There was an overall consensus that the literature review was insufficient in its initial form.	B-weakness
2019-1142	The authors have proposed to extend it somewhat.	B-rebuttal_process
2019-1142	Other concerns are related to the novelty of the technique (R4 was particularly concerned about novelty over Zhu et al, 2016). <sep>	B-weakness
2019-1142	Two reviewers were against acceptance, and one was positive about the paper.	B-rating_summary
2019-1142	Due to the overall concerns about the novelty of the approach, and that these concerns were confirmed in discussion after the rebuttal, this paper is unlikely to meet the threshold for acceptance to ICLR.	B-decision

2019-1152	This work presents a reconstruction GAN with an additional classification task in the objective loss function.	B-abstract
2019-1152	Evaluations are carried out on medical and non-medical datasets. <sep>	I-abstract
2019-1152	Reviewers raise multiple concerns around the following: <sep> - Novelty (all reviewers) <sep>	B-weakness
2019-1152	- Inadequate comparison baselines (all reviewers) <sep>	I-weakness
2019-1152	- Inadequate citations.	I-weakness
2019-1152	(R2 & R3) <sep>	O
2019-1152	Authors have not offered a rebuttal.	B-rebuttal_process
2019-1152	Recommendation is reject.	B-decision
2019-1152	Work may be more suitable as an application paper for a medical conference or journal.	B-suggestion

2019-1162	It seems that the reviewers reached a consensus that the paper is not ready for publication in ICLR.	B-rating_summary
2019-1162	(see more details in the reviews below. )	O

2019-1163	This paper is extending the meta-learning MAML method to the mixture case.	B-abstract
2019-1163	Specifically, the global parameters of the method are now modeled as a mixture.	I-abstract
2019-1163	The authors also derive the elaborate associated inference for this approach. <sep>	I-abstract
2019-1163	The paper is well written although Rev2 raises some presentation issues that can surely improve the quality of the paper, if addressed in depth. <sep>	B-strength
2019-1163	The results do not convince any of the three reviewers.	B-weakness
2019-1163	Rev3 asks for a clearer exposition of the results to increase convincingness.	I-weakness
2019-1163	Rev2 and Rev1 also make similar comments. <sep>	O
2019-1163	Rev1 also questions the motivation of the approach, although the other two reviewers seem to find the approach well motivated.	B-weakness
2019-1163	Although it certainly helps to prove the motivation within a very tailored to the method application, the AC weighted the opinion of all reviewers and did not consider the paper to lack in the motivation aspect. <sep>	B-ac_disagreement
2019-1163	The reviewers were overall not very impressed with this paper and that does not seem to stem from lack of novelty or technical correctness.	B-weakness
2019-1163	Instead, it seems that this work is rather inconclusive (or at least it is presented in an inconclusive manner): Rev1 says that the important questions (like trade-offs and other practical issues) are not answered, Rev2 suggests that maybe this paper is trying to address too much, and all three reviewers are not convinced by the experiments and derived insights. <sep>	I-weakness
2019-1163	Finally, Rev2 points out some inherent caveats of the method; although they do not seem to be severe enough to undermine the overall quality of the approach, it would be instructive to have them investigated more thoroughly (even if not completely solving them).	I-weakness

2019-1174	The reviewers found that paper is well written, clear and that the authors did a good job placing the work in the relevant literature.	B-strength
2019-1174	The proposed method for using multiple discriminators in a multi-objective setting to train GANs seems interesting and compelling.	I-strength
2019-1174	However, all the reviewers found the paper to be on the borderline.	B-rating_summary
2019-1174	The main concern was the significance of the work in the context of existing literature.	B-weakness
2019-1174	Specifically, the reviewers did not find the experimental results significant enough to be convinced that this work presents a major advance in GAN training.	I-weakness

2019-1179	Reviewers are in a consensus and recommended to reject after engaging with the authors.	B-rating_summary
2019-1179	Please take reviewers' comments into consideration to improve your submission should you decide to resubmit. <sep>	B-suggestion

2019-1203	Following the unanimous vote of the submitted reviews, this paper is not ready for publication at ICLR.	B-decision
2019-1203	Among other concerns raised, the experiments need significant work, and the exposition needs clarification.	B-weakness

2019-1214	This was a borderline paper and a very difficult decision to make. <sep>	O
2019-1214	The paper addresses a potentially interesting problem in approximate POMDP planning, based on simplifying assumptions that perception can be decoupled from action and that a set of sensors exhibits certain conditional independence structure.	B-abstract
2019-1214	As a result, a simple approach can be devised that incorporates a simple greedy perception method within a point-based value iteration scheme. <sep>	I-abstract
2019-1214	Unfortunately, the assumptions the paper makes are so strong and seemingly artificial to the extent that they appear reverse engineered to the use of a simple perception heuristic.	B-weakness
2019-1214	In principle, such a simplification might not be a problem if the resulting formulation captured practically important scenarios, but that was not convincingly achieved in this paper---indeed, another major limitation of the paper is its weak motivation.	I-weakness
2019-1214	In more detail, the proposed approach relies on decoupling of perception and action, which is a restrictive assumption that bypasses the core issue of exploration versus exploitation in POMDPS.	I-weakness
2019-1214	As model of active perception, the proposal is simplistic and somewhat artificial; the motivation for the particular cost model (cardinality of the sensor set) is particularly weak---a point that was not convincingly defended in the discussion.	I-weakness
2019-1214	Perhaps the biggest underlying weakness is the experimental evaluation, which is inadequate to support a claim that the proposed methods show meaningful advantages over state-of-the-art approaches in important scenarios.	I-weakness
2019-1214	A reviewer also raised legitimate questions about the strength of the theoretical analysis. <sep>	I-weakness
2019-1214	In the end, the reviewers did not disagree on any substantive technical matter, but nevertheless did disagree in their assessments of the significance of the contribution.	O
2019-1214	This is clearly a borderline paper, which on the positive side, was competently executed, but on the negative side, is pursuing an artificial scenario that enables a particularly simple algorithmic approach. <sep>	B-weakness
2019-1214	Despite the lack of consensus, a difficult decision has to be made nonetheless.	O
2019-1214	In the end, my judgement is that the paper is not yet strong enough for publication.	B-decision
2019-1214	I would recommend the authors significantly strengthen the experimental evaluation to cover off at least two of the major shortcomings of the current paper: (1) The true utility of the proposed method needs to be better established against stronger baselines in more realistic scenarios.	B-suggestion
2019-1214	(2) The relevance of the restrictive assumptions needs to be more convincingly established by providing concrete, realistic and more challenging case studies where the proposed techniques are still applicable.	I-suggestion
2019-1214	The paper would also be improved if the theoretical analysis could be strengthened to better address the criticisms of Reviewer 4.	I-suggestion

2019-1247	* Strengths <sep> The paper proposes a novel and interesting method for detecting adversarial examples, which has the advantage of being based on general "fingerprint statistics" of a model and is not restricted to any specific threat model (in contrast to much of the work in the area which is restricted to adversarial examples in some L_p norm ball).	B-strength
2019-1247	The writing is clear and the experiments are extensive. <sep>	I-strength
2019-1247	* Weaknesses <sep> The experiments are thorough.	B-weakness
2019-1247	However, they contain a subtle but important flaw.	I-weakness
2019-1247	During discussion it was revealed that the attacks used to evaluate the method fail to reduce accuracy even at large values of epsilon where there are simple adversarial attacks that should reduce the accuracy to zero.	I-weakness
2019-1247	This casts doubt on whether the attacks at small values of epsilon really are providing a good measure of the method's robustness. <sep>	I-weakness
2019-1247	* Discussion <sep> There was substantial disagreement about the paper, with R1 feeling that the evaluation issues were serious enough to merit rejection and R3 feeling that they were not a large issue.	B-rating_summary
2019-1247	In discussion with me, both R1 and R3 agreed that if an attack were demonstrated to break the method, that would be grounds for rejection.	I-rating_summary
2019-1247	They also both agreed that there probably is an attack that breaks the method.	I-rating_summary
2019-1247	A potential key difference is that R3 thinks this might be quite difficult to find and so merits publishing the paper to motivate stronger attacks. <sep>	I-rating_summary
2019-1247	I ultimately agree with R1 that the evaluation issues are indeed serious.	B-weakness
2019-1247	One reason for this is that there is by now a long record of adversarial defense papers posting impressive numbers that are often invalidated within a short period (often less than a month or so) of the paper being published.	I-weakness
2019-1247	The "Obfuscated Gradients" paper of Athalye, Carlini, and Wagner suggests several basic sanity checks to help avoid this.	I-weakness
2019-1247	One of the sanity checks (which the present paper fails) is to test that attacks work when epsilon is large.	I-weakness
2019-1247	This is not an arbitrary test but gets at a key issue---any given attack provides only an *upper bound* on the worst-case accuracy of a method.	I-weakness
2019-1247	For instance, if an attack only brings the accuracy of a method down to 80% at epsilon=1 (when we know the true accuracy should be 0%), then at epsilon=0.01 we know that the measured accuracy of the attack comes 80% from the over-optimistic accuracy at epsilon=1 and at most 20% from the true accuracy at epsilon=0.01.	I-weakness
2019-1247	If the measured accuracy at epsilon=1 is close to 100%, then accuracy at lower values of epsilon provides basically no information.	I-weakness
2019-1247	This means that the experiments as currently performed give no information about the true accuracy of the method, which is a serious issue that the authors should address before the paper can be accepted.	B-decision

2019-1258	The reviewers agree that providing more insights on why batch normalization work is an important topic of investigation, but they all raised several problems with the current submission which need to be addressed before publication.	B-rating_summary
2019-1258	The AC thus proposes "revise and sesubmit".	B-decision

2019-1268	The submission proposes a setting of two agents, one of them probing the other (the latter being the "demonstrator").	B-abstract
2019-1268	The probing is done in a way that learns to imitate the expert's behavior, with some curiosity-driven reward that maximizes the chance that the probing agents has the expert do trajectories that the probing agent hasn't seen before. <sep>	I-abstract
2019-1268	All the reviewers found the idea and experiments interesting.	B-strength
2019-1268	The major concern is whether the setup and the environments are too contrived.	B-weakness
2019-1268	At least 2 reviewers commented on the fact that the environments/dataset seemed engineered for success of the given method, which is a concern about how this method would generalize to something other than the proposed setup. <sep>	I-weakness
2019-1268	I also share the concern with R3 regarding the practicality of the proposed method: it is not obvious to me what problems this would actually be *useful* for, given that the method requires online interaction with an expert agent in order to succeed.	I-weakness
2019-1268	The space of such scenarios where we can continuously probe an expert agent many many times for free/cheap is very small and frankly I'm not entirely sure why you would need to do imitation learning in that case at all (if the method was shown to work using only a state, rather than requiring a state/action pair from the expert, then maybe it'd be more useful). <sep>	I-weakness
2019-1268	It's a tough call, but despite the nice results and interesting ideas, I think the method lacks generality and practical utility/significance and thus at this point I cannot recommend acceptance in its current form.	B-decision

2019-1291	The paper presents an action conditioned video prediction method that combines previous losses in the literature, such as, perceptual, adversarial and infogan type of losses.	B-abstract
2019-1291	The reviewers point out the lack of novelty in the formulation, as well as the lack of experiments that would verify its usefulness in model based RL.	B-weakness
2019-1291	There is no rebuttal thus no ground for discussion or acceptance.	B-decision

2019-1295	The paper studies how to construct infinitely deep infinite-width networks from a theoretical point of view, and uses the results of its theoretical analysis to design a weight initialization scheme for finite-width networks.	B-abstract
2019-1295	While the idea is interesting and the paper may contain novel theoretical contributions, the experimental results are weak, as pointed out by all three reviewers from several different perspectives.	B-weakness
2019-1295	In particular, it seems that the presented theoretical analysis is useful mainly for weight initialization and hence has limited potential impacts.	I-weakness
2019-1295	In addition, the authors have responded to neither the AC's question, nor a detailed anonymous comment that challenges the value of Proposition 1 given the previous work by Aronszajn.	B-rebuttal_process

2019-1298	The paper proposes a quantity to monitor learning on an information plane which is related to the information curves considered in the bottleneck analysis but is more reliable and easier to compute. <sep>	B-abstract
2019-1298	The main concern with the paper is the lack of interpretation and elaboration of potential uses.	B-weakness
2019-1298	A concern is raised that the proposed method abstracts away way too much detail, so that the shapes of the curves are to be expected and contain little useful information (see AnonReviewer2 comments).	I-weakness
2019-1298	The authors agree to some of the main issues, as they pointed out in the discussion, although they maintain that the method could still contain useful information. <sep>	B-rebuttal_process
2019-1298	The reviewers are not very convinced by this paper, with ratings either marginally above the acceptance threshold, marginally below the acceptance threshold, or strong reject. <sep>	B-rating_summary

2019-1305	The paper presents an architecture search method which jointly optimises the architecture and its weights.	B-abstract
2019-1305	As noted by reviewers, the method is very close to Shirakawa et al, with the main innovation being the use of categorical distributions to model the architecture.	B-weakness
2019-1305	This is a minor innovation, and while the results are promising, they are not strong enough to justify acceptance based on the results alone.	B-decision

2019-1321	This paper addresses the problem of recommendations within user sessions from a reinforcement learning perspective.	B-abstract
2019-1321	The problem is naturally modeled as an RL problem, given its sequential nature and inherent uncertainty of any model over user preferences.	I-abstract
2019-1321	The problem suffers from delayed and sparse rewards, which the authors propose to address using self-supervised prediction.	I-abstract
2019-1321	The approach is empirically validated in a simulated setting, using data from the 2015 ACM RecSys Challenge. <sep>	I-abstract
2019-1321	The reviewers and AC note that the problem studied is an important application area where RL has high potential to improve over current research results and industry practice.	B-strength
2019-1321	The proposed idea is interesting, and the strong empirical evaluation on a publicly available data set is highlighted.	I-strength
2019-1321	R1 also commends the authors' decision to address the challenging cold-start problem. <sep>	I-strength
2019-1321	The reviewers and AC also note several potential weaknesses.	O
2019-1321	The choice of addressing the problem from a reinforcement learning perspective is not clearly motivated.	B-weakness
2019-1321	This is needed, as many supervised learning (and other types) approaches to the problem exist.	I-weakness
2019-1321	A performance comparison to current state-of-the-art RL baselines is missing.	I-weakness
2019-1321	The proposed approach is related to both imagination augmented (I2A, Racaniere et al 2017) and agents with auxiliary rewards (UNREAL, Jaderberg et al 2016), but does not compare to either method.	I-weakness
2019-1321	Neither does the related work section sufficiently clarify why the proposed approach is expected to improve over these prior approaches.	I-weakness
2019-1321	A thorough comparison to these baselines in a real-world application like session-based recommendation would be a strong contribution in itself, but without the contributions of the paper are hard to assess.	I-weakness
2019-1321	Reviewers also noted lack of clarity.	I-weakness
2019-1321	Some concerns are addressed by the authors, but the consensus is that the paper would benefit from a major revision to clearly work out the method, as well as it's conceptual and empirical differences from existing reinforcement learning approaches.	B-rebuttal_process
2019-1321	R3 mentions missing related work, some of which the authors include in the revision.	I-rebuttal_process
2019-1321	The AC recommends also following up on references in cited papers to ensure a future revision of the paper is well placed in the context of prior work on recommender systems, especially when modeled as a reinforcement learning problem. <sep>	B-suggestion
2019-1321	Overall, the paper was assessed as borderline by the reviewers.	B-rating_summary
2019-1321	The ACs view is that there are too many concerns for acceptance at ICLR in the present form, and that the paper will benefit from a thorough revision.	B-decision

2019-1338	The paper introduces a form of variational auto encoder for learning disentangled representations.	B-abstract
2019-1338	The idea is to penalise synergistic mutual information.	I-abstract
2019-1338	The introduction of concepts from synergy to the community is appreciated. <sep>	B-strength
2019-1338	Although the approach appears interesting and forward looking in understanding complex models, at this point the paper does not convince on the theoretical nor on the experimental side.	B-weakness
2019-1338	The main concepts used in the paper are developed elsewhere, the potential value of synergy is not properly examined. <sep>	I-weakness
2019-1338	The reviewers agree on a not so positive view on this paper, with ratings either ok, but not good enough, or clear rejection.	B-rating_summary
2019-1338	There is a consensus that the paper needs more work. <sep>	B-weakness

2019-1339	The reviewers are polarized on this paper and the overall feeling is that it is not quite ready for publication.	B-rating_summary
2019-1339	There is also an interesting interpretability aspect that, while given as a motivation for the approach, is never really explored beyond showing some figures of alignments.	B-weakness
2019-1339	One of the main concerns of the method's effectiveness in practice is the computational cost.	I-weakness
2019-1339	There is also concern from one of the reviewers that the formulation could result in creating sparse matching maps where only a few pixels get matched.	I-weakness
2019-1339	The authors provide some justification for why this wouldn't happen, and this should be put in a future draft.	B-rebuttal_process
2019-1339	Even better would be to show statistics to demonstrate empirically that this doesn't happen. <sep>	B-suggestion
2019-1339	There were a number of clarifications that were brought up during the discussion, and the authors should go over this carefully and update the draft to resolve these issues.	B-rebuttal_process
2019-1339	There is also a typo in the title that should be fixed. <sep>	B-weakness

2019-1362	The authors propose implementing intrinsic motivation as a differentiable supervised loss coming from the error of a forward model, rather than the black box style of curiosity reward.	B-abstract
2019-1362	The motivation is that this approach will lead to more sample efficient exploration for real robots.	I-abstract
2019-1362	The use of a differentiable loss for policy optimization is interesting and has some novelty.	B-strength
2019-1362	However, the reviewers were unanimous in their criticism of the paper for poor baselines, unclear experiments and results, and unsupported claims.	B-weakness
2019-1362	Even after substantial revisions to the paper, the AC and reviewers were unconvinced of the basic claims of the paper.	B-rebuttal_process

2019-1368	This paper proposes TopicGAN, a generative adversarial approach to topic modeling and text generation.	B-abstract
2019-1368	TopicGAN operates in two steps: it first generates latent topics and produces bag-of-words corresponding to those latent topics.	I-abstract
2019-1368	In the second step, the model generates text conditioning on those topic words. <sep>	I-abstract
2019-1368	Pros: <sep> It combines the strength of topic models (interpretable topics that are learned unsupervised) with GAN for text generation. <sep>	B-strength
2019-1368	Cons: <sep> There are three major concerns raised by reviewers: (1) clarity, (2) relatively thin experimental results, and (3) novelty.	B-weakness
2019-1368	Of these, the first two were the main concerns.	I-weakness
2019-1368	In particular, R1 and R2 raised concerns about insufficient component-wise evaluation (eg, text classification from topic models) and insufficient GAN-based baselines.	I-weakness
2019-1368	Also, the topic model part of TopicGAN seems somewhat underdeveloped in that the model assumes a single topic per document, which is a relatively strong simplifying assumption compared to most other topic models (R1, R3).	I-weakness
2019-1368	The technical novelty is not extremely strong in that the proposed model combines existing components together.	I-weakness
2019-1368	But this alone would have not been a deal breaker if the empirical results were rigorous and strong. <sep>	I-weakness
2019-1368	Verdict: <sep> Reject.	B-decision
2019-1368	Many technical details require clarification and experiments lack sufficient comparisons against prior art.	B-weakness

2019-1378	This work proposes to use the MAML meta-learning approach in order to tackle the typical problem of insufficient demonstrations in IRL. <sep>	B-abstract
2019-1378	All reviewers found this work to contain a novel and well-motivated idea and the manuscript to be well-written.	B-strength
2019-1378	The combination of MAML and MaxEnt IRL is straightforward, as R2 points out, however the AC does not consider this to be a flaw given that the main novelty here is the high-level idea rather than the technical details. <sep>	B-ac_disagreement
2019-1378	However, all reviewers agree that for this paper to meet the ICLR standards, there has to be an increase in rigorousness through (a) a more close examination of assumptions, sensitivity of parameters and connections to imitation learning (b) expanding the experimental section.	B-suggestion

2019-1414	This is an interesting direction but multiple reviewers had concerns about the amount of novelty in the current work, and given the strong pool of other papers, this didn't quite reach the threshold. <sep>	B-decision

2019-1424	The paper proposes adversarial sampling for pool-based active learning. <sep>	B-abstract
2019-1424	The reviewers and AC note the critical potential weaknesses on experimental results: it is far from being surprising the proposed method is better than random sampling.	B-weakness
2019-1424	Ideally, one has to reduce the complexity under keeping the state-of-art performance.	I-weakness
2019-1424	Otherwise, it is hard to claim the proposed method is fundamentally better than prior ones, although their targets might be different. <sep>	I-weakness
2019-1424	AC thinks the proposed method has potential and is interesting, but decided that the authors need more works to publish.	B-decision

2019-1432	I enjoyed reading the paper myself and I appreciate the unifying framework connecting RAML and SPG.	O
2019-1432	While I do not put a lot of weight on the experiments, I agree with the reviewers that the experimental results are not very strong, and I am not convinced that the theoretical contribution meets the bar at ICLR. <sep>	B-weakness
2019-1432	In the interpolation algorithm, there seems to be an additional annealing parameter and two tuning parameters.	I-weakness
2019-1432	It is important to describe how the parameters are tuned.	I-weakness
2019-1432	Given the additional hyper-parameters, one may consider giving all of the algorithms the same budget of hyper-parameter tuning.	B-suggestion
2019-1432	I also agree with reviewers that the policy gradient baseline seems to underperform typical results.	B-weakness
2019-1432	One possible way to strengthen the experiments is to try to replicate the results of SPG or RAML and discuss the behavior of each algorithm as a function of hyper-parameters. <sep>	B-suggestion

2019-1434	This paper does two things.	B-abstract
2019-1434	First, it proposes an approach to estimating the mutual information between the input, X, or target label, Y, and an internal representation in a deep neural network, L, using MINE (for I(Y;L)) or a variation on MINE (for I(X;L)) and noise regularization (estimating I(X;L+), where  is isotropic Gaussian white noise) to avoid the problem that I(X;L) is infinite for deterministic networks and continuous X.	I-abstract
2019-1434	Second, it attempts to validate the information bottleneck theory of deep learning (Tishby and Zaslavsky, 2015) by exploring an approach to training DNNs that optimizes the information bottleneck Lagrangian, I(Y;L)  I(X;L+), layerwise instead of using cross-entropy and backpropagation.	I-abstract
2019-1434	Experiments on MNIST and CIFAR-10 show improvements for the layerwise training over cross-entropy training.	I-abstract
2019-1434	The penalty on I(X;L+) is described as being analogous to weight decay.	I-abstract
2019-1434	The reviewers raised a number of concerns about the paper, the most serious of which is that the claim that the layerwise training results validate the information bottleneck theory of deep learning is too strong.	B-weakness
2019-1434	In the AC's opinion, R1's critique that "[i]f the true mutual information is infinite and the noise regularized estimator is only meant for comparative purposes, why then are the results of the training trajectories interpreted so literally as estimates of the true mutual information?"	B-rebuttal_process
2019-1434	is critical, and the authors' reply that "this quantity is in fact a more appropriate measure for "compactness" or "complexity" than the mutual information itself" undermines their claim that they are validating the information bottleneck theory of deep nets because the information bottleneck theory claims to be using mutual information.	I-rebuttal_process
2019-1434	The AC also suggests that if the authors wish to continue this work and submit it to another venue, they (1) discuss the fact that MINE estimates only a lower bound that may be quite loose in practice and (2) say in their experimental section whether or not the variance of the regularizing noise was tuned as a hyperparameter, and if so, how results varied with different amounts of noise.	B-suggestion
2019-1434	Finally, the AC regrets that only one reviewer participated in the discussion (in a very minimal way), despite the reviewers' receiving several reminders that the discussion is a defining feature of the ICLR review process.	B-rebuttal_process

2019-1440	All reviewers agree in their assessment that this paper has merits but is not yet ready for acceptance into ICLR.	B-rating_summary
2019-1440	The area chair commends the authors for their responses to the reviews.	B-rebuttal_process

2019-1446	This paper presents a dataset for measuring disentanglement in learned representations.	B-abstract
2019-1446	It consists of MNIST digits, sometimes transformed in various ways, and labeled with a variety of attributes.	I-abstract
2019-1446	This dataset is used to measure statistics of various learned models. <sep>	I-abstract
2019-1446	Measuring disentanglement is certainly an important problem in our field.	B-strength
2019-1446	This dataset seems to be well designed, and I would recommend its use for papers studying disentanglement.	I-strength
2019-1446	The experiments are well-designed.	I-strength
2019-1446	While the reviewers seem bothered by the fact that it's limited to MNIST, this doesn't strike me as a problem.	B-ac_disagreement
2019-1446	We continue to learn a lot from MNIST, even today. <sep>	I-ac_disagreement
2019-1446	But producing a useful dataset isn't by itself a significant enough research contribution for an ICLR paper.	B-weakness
2019-1446	I'd recommend publication if (a) it were very different from currently existing datasets, (b) constructing it required overcoming significant technical obstacles, or (c) the dataset led to particularly interesting findings. <sep>	O
2019-1446	Regarding (a), there are already datasets of similar complexity which have ground-truth attributes useful for measuring disentanglement, such as dSprites and 3D Faces.	B-weakness
2019-1446	Regarding (b), the construction seems technically straightforward.	I-weakness
2019-1446	Regarding (c), the experimental findings are plausible and consistent with past findings (which is a good validation of the dataset) but not obviously interesting in their own right. <sep>	I-weakness
2019-1446	So overall, this seems like a useful dataset, but I cannot recommend publication at ICLR. <sep>	B-decision

2019-1448	This paper studies the properties of L1 regularization for deep neural network.	B-abstract
2019-1448	It contains some interesting results, eg the stationary point of an l1 regularized layer has bounded number of non-zero elements.	B-strength
2019-1448	On the other hand, the majority of reviewers has concerns on that experimental supports are weak and suggests rejection.	B-rating_summary
2019-1448	Therefore, a final rejection is proposed.	B-decision

2019-1458	The authors propose a technique for pruning networks by using second-order information through the Hessian.	B-abstract
2019-1458	The Hessian is approximated using the Fisher Information Matrix, which is itself approximated using KFAC.	I-abstract
2019-1458	The paper is clearly written and easy to follow, and is evaluated on a number of systems where the authors find that the proposed method achieves good compression ratios without requiring extensive hyperparameter tuning. <sep>	B-strength
2019-1458	The reviewers raised concerns about 1) the novelty of the work (which builds on the KFAC work of Martens and Grosse), 2) whether zeroing out individual connections as opposed to neurons will have practical runtime benefits, 3) the lack of comparisons against baselines on overall training time/complexity, 4) comparisons to work which directly prune as part of training (instead of the train-prune-finetune scheme adopted by the authors). <sep>	B-weakness
2019-1458	In the view of the AC,  4) would be an interesting comparison but was not critical to the decision.	B-ac_disagreement
2019-1458	Ultimately, the decision came down to the concern of lack of novelty and whether the proposed techniques would have an impact on runtime in practice. <sep>	B-weakness

2019-1468	This paper tackles the problem of using auxiliary losses to help regularize and aid the learning of a "goal" task.	B-abstract
2019-1468	The approach proposes avoiding the learning of irrelevant or contradictory details from the auxiliary task at the expense of the "goal" tasks by observing cosine similarity between the auxiliary and main tasks and ignore those gradients which are too dissimilar. <sep>	I-abstract
2019-1468	To justify such a setup one must first show that such negative interference occurs in practice, warranting explicit attention.	B-suggestion
2019-1468	Then one must show that their algorithm effectively mitigates this interference and at the same time provides some useful signal in combination with the main learning objective. <sep>	I-suggestion
2019-1468	During the review process there was a significant discussion as to whether the proposed approach sufficiently justified its need and usefulness as defined above.	O
2019-1468	One major point of contention is whether to compare against the multi-task literature.	B-rebuttal_process
2019-1468	The authors claim that prior multi-task learning literature is out of scope of this work since their goal is not to measure performance on all tasks used during learning.	I-rebuttal_process
2019-1468	However, this claim does not invalidate the reviewer's request for comparison against multi-task learning work.	I-rebuttal_process
2019-1468	In fact, the authors *should* verify that their method outperforms state-of-the-art multi-task learning methods.	B-suggestion
2019-1468	Not because they too are studying performance across all tasks, but because their method which knows to prioritize one task during training should certainly outperform the learning paradigms which have no special preference to one of the tasks. <sep>	I-suggestion
2019-1468	A main issue with the current draft centers around the usefulness of the proposed algorithm.	B-weakness
2019-1468	First, whether the gradient co-sine similarity is a necessary condition to avoid negative interference and 2) to show at least empirically that auxiliary losses do offer improved performance over optimizing the goal task alone.	I-weakness
2019-1468	Based on the experiments now available the answers to these questions remains unclear and thus the paper is not yet recommended for publication.	B-decision

2019-1474	The paper proposes a new method for training generative models by minimizing general f-divergences.	B-abstract
2019-1474	The main technical idea is to optimize f-divergence between joint distributions which is rightly observed to be the upper bound of the f-divergence between the marginal distributions and address the disjoint support problem by convolving the data with a noise distribution.	I-abstract
2019-1474	The basic ideas in this work are not completely novel but are put together in a new way. <sep>	B-strength
2019-1474	However, the key weakness of this work, as all the reviewer noticed, is that the empirical results are too week to support the usefulness of the proposed approach.	B-weakness
2019-1474	The only quantitive results are in table 2, which is only a simple Gaussian example.	I-weakness
2019-1474	It essential to have more substantial empirical results for supporting the new algorithm. <sep>	B-suggestion

2019-1491	mnist and small picture variants are not that impressive. <sep>	B-weakness
2019-1491	it is a minor extension of VAEs which also are not common in sota systems.	I-weakness

2019-1536	The paper contains useful information and shows relative improvements compared to mixup.	B-strength
2019-1536	However, some of the main claims are not substantiated enough to be fully convincing.	B-weakness
2019-1536	For example, the claims that manifold mixup can prevent can manifold collision issue where the interpolation between two samples collides with a sample from other class is incorrect.	I-weakness
2019-1536	The authors are encouraged to incorporate remarks of the reviewers.	B-suggestion

2019-1565	This paper shows convergence of stochastic gradient descent  for the problem of learning weight matrices for a linear dynamical system  with non-linear activation.	B-abstract
2019-1565	Reviewers agree that the problem considered is both interesting and challenging.	B-strength
2019-1565	However the paper makes many simplifying assumptions - 1) both input and hidden state are observed, a very non standard assumption, 2) analysis requires increasing activation functions, cannot handle ReLU functions.	B-weakness
2019-1565	I agree with R2 and think these assumptions make the results significantly weaker.	I-weakness
2019-1565	R1 and R3 are more optimistic, but authors response does not give an insight into how one might extend this analysis to the setting where hidden state is not observed.	B-rebuttal_process
2019-1565	Relaxing these assumptions will make the paper more interesting.	B-suggestion

2019-1571	The paper proposes two simple generator architecture variants enabling the use of GAN training for the tasks of denoising (from known noise types) and demixing (of two added sources).	B-abstract
2019-1571	While the denoising approach is very similar to AmbientGAN and could thus be considered somewhat incremental, all reviewers and the AC agree that the developed use of GANs for demixing is an interesting novel direction.	B-strength
2019-1571	The paper is well written, and the approach is supported by encouraging experimental results on MNIST and Fashion-MNIST. <sep>	I-strength
2019-1571	Reviewers and AC noted the following weaknesses of the paper: a) no theoretical support or analysis is provided for the approach, this makes it primarily an empirical study of a nice idea. <sep>	B-weakness
2019-1571	b) For an empirical study, the experimental evaluation is very limited, both in terms of dataset/problems it is tested on; and in terms of algorithms for demixing/source-separation that it is compared against. <sep>	I-weakness
2019-1571	Following these reviews, the authors added the experiments on Fashion-MNIST and comparison with ICA which are steps in the right direction.	B-rebuttal_process
2019-1571	This improvement moved one reviewer to positively update his score, but not the others. <sep>	I-rebuttal_process
2019-1571	Taking everything into account, the AC judges that it is a very promising direction, but that more extensive experiments on additional benchmark tasks for demixing and comparison with other demixing algorithms are needed to make this work a more complete contribution. <sep>	B-suggestion

2019-1598	The paper tried to introduce a new interpretation of dropout and come with improved algorithms.	B-abstract
2019-1598	However, the reviewers were not convinced that the presented arguments were correct/novel, and they found the paper difficult to follow.	B-weakness
2019-1598	The authors are encouraged to carefully revise their paper to address these concerns.	B-suggestion

2019-1603	This paper shows how to implement a low-rank version of the Adagrad preconditioner in a GPU-friendly manner.	B-abstract
2019-1603	A theoretical analysis of a "hard-window" version of the proposed algorithm demonstrates that it is not worse than SGD at finding a first-order stationary point in the nonconvex setting.	I-abstract
2019-1603	Experiments on CIFAR-10 classification using a ConvNet and Penn Treebank character-level language modeling using an LSTM show that the proposed algorithm improves training loss faster than SGD, Adagrad, and Adam (measuring time in epochs) and has better generalization performance on the language modeling task.	I-abstract
2019-1603	However, if wall-clock time is used to measure time, there is no speedup for the ConvNet model, but there is for the recurrent model.	I-abstract
2019-1603	The reviewers liked the simplicity of the approach and greatly appreciated the elegant visualization of the eigenspectrum in Figure 4.	B-strength
2019-1603	But, even after discussion, critical concerns remained about the need for more focus on the practical tradeoffs between per-iteration improvement and per-second improvement in the loss and the need for a more careful analysis of the relationship of this method to stochastic L-BFGS.	B-rebuttal_process
2019-1603	A more minor concern is that the term "full-matrix regularization" seems somewhat deceptive when the actual regularization is low rank.	B-weakness
2019-1603	The AC also suggests that, if the authors plan to revise this paper and submit it to another venue, they consider the relationship between GGT and the various stochastic natural gradient optimization algorithms in the literature that differ from GGT primarily in the exponent on the Gram matrix.	B-suggestion

2019-1624	The work proposes a method for smoothing a non-differentiable machine learning pipeline (such as the Faster-RCNN detector) using policy gradient.	B-abstract
2019-1624	Unfortunately, the reviewers identified a number of critical issues, including no significant improvement beyond existing works.	B-weakness
2019-1624	The authors did not provide a rebuttal for these critical issues.	B-rebuttal_process

2019-1638	The authors propose a generative model based on variational autoencoders that provides means to manipulate the high-level attributes of a given input.	B-abstract
2019-1638	The attributes can be either pre-defined ground truth attributes or unknown attributes automatically discovered from the data. <sep>	I-abstract
2019-1638	While the reviewers acknowledged the potential usefulness of the proposed approach, they raised important concerns that were viewed by AC as a critical issue: (1) very limited experimental evaluation (eg no baseline or ablation results, no quantitative results); comparisons on other more complex datasets and more in-depth analysis would substantially strengthen the evaluation and would allow to assess the scope of the contribution of this work   see, for example, R3's suggestion to use other dataset like dSprites or CelebA, where the ground truth attributes are known; (2) lack of presentation clarity  see R2's latest comment how to improve. <sep>	B-weakness
2019-1638	A general consensus among reviewers and AC suggests, in its current state the manuscript is not ready for a publication.	B-decision
2019-1638	It needs clarification, more empirical studies and polish to achieve the desired goal. <sep>	B-weakness

2019-1650	The manuscript proposes a novel estimation technique for generative models based on fast nearest neighbors and inspired by maximum likelihood estimation.	B-abstract
2019-1650	Overall, reviewers and AC agree that the general problem statement is timely and interesting, and the subject is of interest to the ICLR community <sep>	B-strength
2019-1650	The reviewers and ACs note weakness in the evaluation of the proposed method.	B-weakness
2019-1650	In particular, reviewers note that the Parzen-based log-likelihood estimate is known to be unreliable in high-dimensions.	I-weakness
2019-1650	This makes a quantitative evaluation of the results challenging, thus other metrics should be evaluated.	I-weakness
2019-1650	Reviewers also expressed concerns about the strengths of the baselines compared.	I-weakness
2019-1650	Additional concerns are raised with regards to scalability which the authors address in the rebuttal.	B-rebuttal_process

2019-1675	While the paper contains interesting ideas, the reviewers suggest improving the clarity and experimental study of the paper.	B-weakness
2019-1675	The work holds promises but is not ready for publication at ICLR.	B-decision

2019-1686	This paper proposes a new method to mine sentence from Wikipedia and use them to train an MT system, and also a topic-based loss function.	B-abstract
2019-1686	In particular, the first contribution, which is the main aspect of the proposal is effective, outperforming methods for fully unsupervised learning. <sep>	B-strength
2019-1686	The main concern with the proposed method, or at least it's description in the paper, is that it isn't framed appropriately with respect to previous work on mining parallel sentences from comparable corpora such as Wikipedia.	B-weakness
2019-1686	Based on interaction in the reviews, I feel that things are now framed a bit better, and there are additional baselines, but still the explanation in the paper isn't framed with respect to this previous work, and also the baselines are not competitive, despite previous work reporting very nice results for these previous methods. <sep>	I-weakness
2019-1686	I feel like this could be a very nice paper at some point if it's re-written with the appropriate references to previous work, and experimental results where the baselines are done appropriately.	B-suggestion
2019-1686	Thus at this time I'm not recommending that the paper be accepted, but encourage the authors to re-submit a revised version in the future.	B-decision

2020-6	The paper proposed an operation called StructPool for graph-pooling by treating it as node clustering problem (assigning a label from 1. .k to each node) and then use a pairwise CRF structure to jointly infer these labels.	B-abstract
2020-6	The reviewers all think that this is a well-written paper, and the experimental results are adequate to back up the claim that StructPool offers advantage over other graph-pooling operations.	B-strength
2020-6	Even though the idea of the presented method is simple and it does add more (albeit by a constant factor) to the computational burden of graph neural network, I think this would make a valuable addition to the literature.	I-strength

2020-24	Most reviewers seems in favour of accepting this paper, with the borderline rejection being satisfied with acceptance if the authors take special heed of their comments to improve the clarity of the paper when preparing the final version.	B-rating_summary
2020-24	From examination of the reviews, the paper achieves enough to warrant publication.	B-decision
2020-24	Accept.	I-decision

2020-64	This paper presents a case study of training a video classifier and subsequently analyzing the features to reduce reliance on spurious artifacts.	B-abstract
2020-64	The supervised learning task is zebrafish bout classification which is relevant for biological experiments.	I-abstract
2020-64	The paper analyzed the image support for the learned neural net features using a previously developed technique called Deep Taylor Decomposition.	I-abstract
2020-64	This analysis showed that the CNNs when applied to the raw video were relying on artifacts of the data collection process, which spuriously increased classification accuracies by a "clever Hans" mechanism.	I-abstract
2020-64	By identifying and removing these artifacts, a retrained CNN classifier was able to outperform an older SVM classifier.	I-abstract
2020-64	More importantly, the analysis of the network features enabled the researchers to isolate which parts of the zebrafish motion were relevant for the classification. <sep>	I-abstract
2020-64	The reviewers found the paper to be well-written and the experiments to be well-designed.	B-strength
2020-64	The reviewers suggested a some changes to the phrasing in the document, which the authors adopted.	B-rebuttal_process
2020-64	In response to the reviewers, the authors also clarified their use of ImageNet for pre-training and examined alternative approaches for building saliency maps. <sep>	I-rebuttal_process
2020-64	This paper should be published as the reviewers found the paper to be a good case study of how model interpretability can be useful in practice.	B-decision

2020-75	The authors provide an empirical and theoretical exploration of Nesterov momentum, particularly in the over-parametrized settings.	B-abstract
2020-75	Nesterov momentum has attracted great interest at various times in deep learning, but its properties and practical utility are not well understood.	I-abstract
2020-75	This paper makes an important step towards shedding some light on this approach for training models with a large number of parameters.	I-abstract

2020-81	This paper provides a theoretical background for the expressive power of graph convolutional networks.	B-abstract
2020-81	The results are obviously useful, and the discussion went in the positive way.	B-strength
2020-81	All reviewers recommend accepting, and I am with them.	B-decision

2020-101	The paper proposes a using pixel-adaptive convolutions to leverage semantic labels in self-supervised monocular depth estimation.	B-abstract
2020-101	Although there were initial concerns of the reviewers regarding the technical details and limited experiments, the authors responded reasonably to the issues raised by the reviewers.	B-rebuttal_process
2020-101	Reviewer2, who gave a weak reject rating, did not provide any answer to the authors comments.	I-rebuttal_process
2020-101	We do not see any major flaws to reject this paper.	B-decision

2020-107	This paper examines the correspondence between topological similarity of languages (correlation between the message space and object space) and ability to learn quickly in a situation of emergent communication between agents. <sep>	B-abstract
2020-107	While this paper is not without issues, it does seem to present a nice contribution that all of the reviewers appreciated to some extent.	B-strength
2020-107	I think it will spark further discussions in this area, and thus can recommend it for acceptance.	B-decision

2020-126	The paper proposed a new pretrained language model which can take visual information into the embeddings.	B-abstract
2020-126	Experiments showed state-of-the-art results on three downstream tasks.	I-abstract
2020-126	The paper is well written and detailed comparisons with related work are given.	B-strength
2020-126	There are some concerns about the clarity and novelty raised by the reviewers which is answered in details and I think the paper is acceptable.	B-decision

2020-167	The authors address the important and understudied problem of tuning of unsupervised models, in particular variational models for learning disentangled representations.	B-abstract
2020-167	They propose an unsupervised measure for model selection that correlates well with performance on multiple tasks.	I-abstract
2020-167	After significant fruitful discussion with the reviewers and resulting revisions, many reviewer concerns have been addressed.	B-rebuttal_process
2020-167	There are some remaining concerns that there may still be a gap in the theoretical basis for the application of the proposed measure to some models, that for different downstream tasks the best model selection criteria may vary, and that the method might be too cumbersome and not quite reliable enough for practitioners to use it broadly.	I-rebuttal_process
2020-167	All of that being said, the reviewers (and I) agree that the approach is sufficiently interesting, and the empirical results sufficiently convincing, to make the paper a good contribution and hopefully motivation for additional methods addressing this problem.	B-strength

2020-186	This works improves the MixMatch semi-supervised algorithm along the two directions of distribution alignment and augmentation anchoring, which together make the approach more data-efficient than prior work. <sep>	B-abstract
2020-186	All reviewers agree that the impressive empirical results in the paper are its main strength, but express concern that the method is overly complicated and hacking together many known pieces, as well as doubt as to the extent of the contribution of the augmentation method itself, with requests for better augmentation controls. <sep>	B-weakness
2020-186	While some of these concerns have not been addressed by authors in their response, the strength of empirical results seems enough to justify an acceptance recommendation.	B-decision

2020-192	The authors propose to enforce interpretability and controllability on latent variables, like affect and speaking rate, in a speech synthesis model by training in a semi-supervised way, with a small amount of labeled data with the variables of interest labeled.	B-abstract
2020-192	The idea is sensible and the results are very encouraging, and the authors have addressed the initial concerns brought up by the reviewers.	B-strength

2020-195	This paper introduces a way to augment memory in recurrent neural networks with order-independent aggregators.	B-abstract
2020-195	In noisy environments this results in an increase in training speed and stability.	I-abstract
2020-195	The reviewers considered this to be a strong paper with potential for impact, and were satisfied with the author response to their questions and concerns.	B-rebuttal_process

2020-233	This paper proposes techniques to improve training with batch normalization.	B-abstract
2020-233	The paper establishes the benefits of these techniques experimentally using ablation studies.	I-abstract
2020-233	The reviewers found the results to be promising and of interest to the community.	B-strength
2020-233	However, this paper is borderline due in part due to the writing (notation issues) and because it does not discuss related work enough.	B-rating_summary
2020-233	We encourage the authors to properly address these issues before the camera ready.	B-suggestion

2020-240	The submission proposes an approach to accelerate network training by modifying the precision of individual weights, allowing a substantial speed up without a decrease in model accuracy.	B-abstract
2020-240	The magnitude of the activations determines whether it will be computed at a high or low bitwidth. <sep>	I-abstract
2020-240	The reviewers agreed that the paper should be published given the strong results, though there were some salient concerns which the authors should address in their final revision, such as how the method could be implemented on GPU and what savings could be achieved. <sep>	B-rating_summary
2020-240	Recommendation is to accept.	B-decision

2020-243	Main content: <sep> Blind review #2 summarizes it well: <sep> Summary: This paper deals with the representation degeneration problem in neural language generation, as some prior works have found that the singular value distribution of the (input-output-tied) word embedding matrix decays quickly.	B-abstract
2020-243	The authors proposed an approach that directly penalizes deviations of the SV distribution from the two prior distributions, as well as a few other auxiliary losses on the orthogonality of U and V (which are now learnable).	I-abstract
2020-243	The experiments were conducted on small and large scale language modeling datasets as well as the relatively small IWSLT 2014 De-En MT dataset. <sep>	I-abstract
2020-243	Pros: <sep> + The paper is well-written with great clarity.	B-strength
2020-243	The dimensionality of the involved matrices (and their decompositions) are clearly provided, and the approach is clearly described.	I-strength
2020-243	The authors also did a great job providing the details of their experimental setup. <sep>	I-strength
2020-243	+ The experiments seem to show consistent improvements over the baseline methods (at least the ones listed by the authors) on a relatively extensive set of tasks (eg, of both small and large scales, of two different NLP tasks).	I-strength
2020-243	Via WT2 and WT103, the authors also showed that their method worked on both LSTM and Transformers (which it should, as the SVD on word embedding should be independent of the underlying architecture). <sep>	I-strength
2020-243	+ I think studying the expressivity of the output embedding matrix layer is a very interesting (and important) topic for NLP.	I-strength
2020-243	(eg, While models like BERT are widely used, the actual most frequently re-used module of BERT is its pre-trained word embeddings.) <sep>	I-strength
2020-243	-- <sep>	O
2020-243	Discussion: <sep> The reviewers agree that it is a very well written paper, and this is important as a conference paper to illuminate readers. <sep>	B-rating_summary
2020-243	The one main objection is that spectrum control regularization was previously proposed and applied to GANs (Jiang et al ICLR 2019).	B-weakness
2020-243	However the authors convincingly point out that the technique is widely used, not only for GANs, and that application to neural language generation has quite different characteristics requiring a different, new approach: "our proposed prior distributions as shown in Figure 2 in our paper are fundamentally different from the singular value distributions learned using their penalty functions (See Figure 1 and Table 7 in Jiang et al's paper).	B-rebuttal_process
2020-243	Figure 1 in their paper suggests that their penalty function, ie, D-optimal Reg, will encourage all the singular values close to 1, which is well aligned with their motivation for training GAN.	I-rebuttal_process
2020-243	However, if we use such penalty function to train neural language models, the learned word representations will lose the power of modeling contextual information, and can result in much worse results than the baseline methods." <sep>	I-rebuttal_process
2020-243	-- <sep>	O
2020-243	Recommendation and justification: <sep> I concur with the majority of reviewers that this paper is a weak accept.	B-decision
2020-243	Though not revolutionary, it is well written, has usefully broad application, and is supported well empirically.	B-strength

2020-248	This paper proposes a bidirectional joint image-text model using a variational hetero-encoder (VHE) randomized generative adversarial network (GAN).	B-abstract
2020-248	The proposed VHE-GAN model encodes an image to decode its associated text.	I-abstract
2020-248	Three reviewers have split reviews.	O
2020-248	Reviewer #3 is overall positive about this work.	B-rating_summary
2020-248	Reviewer #1 rated weak acceptance, while request more comparison with latest works.	I-rating_summary
2020-248	Reviewer  #2 rated weak reject raised concerns on the motivation of the approach, the lack of ablation and lack of comparison with the latest work.	I-rating_summary
2020-248	During the rebuttal, the authors provide additional comparison and ablation, which seem to address the major concerns.	B-rebuttal_process
2020-248	Given the overall positive feedback and the quality of rebuttal, the AC recommends acceptance.	B-decision

2020-250	This paper applies reinforcement learning to text adventure games by using knowledge graphs to constrain the action space.	B-abstract
2020-250	This is an exciting problem with relatively little work performed on it.	B-strength
2020-250	Reviews agree that this is an interesting paper, well written, with good results.	I-strength
2020-250	There are some concerns about novelty but general agreement that the paper should be accepted.	B-rating_summary
2020-250	I therefore recommend acceptance.	B-decision

2020-251	This paper studies optimal control with low-dimensional representation.	B-abstract
2020-251	The paper presents interesting progress, although I urge the authors to address all issues raised by reviewers in their revisions.	B-suggestion

2020-255	This paper studies the role of topology in designing adversarial defenses.	B-abstract
2020-255	Specifically , the authors study defense strategies that rely on the assumption that data lies on a low-dimensional manifold, and show theoretical and empirical evidence that such defenses need to build a topological understanding of the data. <sep>	I-abstract
2020-255	Reviewers were initially positive, but had some concerns pertaining to clarity and limited experimental setup.	B-rating_summary
2020-255	After a productive rebuttal phase, now reviewers are mostly in favor of acceptance, thanks to the improved readibility and clarity.	I-rating_summary
2020-255	Despite the small-scale experimental validation, ultimately both reviewers and AC conclude this paper is worthy of publication.	B-decision

2020-256	The paper proposes to compress convolutional neural networks via weight sharing across filters of each convolution layer.	B-abstract
2020-256	A fast convolution algorithm is also designed for the convolution layer with this approach.	I-abstract
2020-256	Experimental results show (i) effectiveness in CNN compression, (ii) acceleration on the tasks of image classification, object detection and neural architecture search.	I-abstract
2020-256	While the authors addressed most of reviewers' concerns, the weakness of the paper which remains is that no wall-clock runtime numbers (only FLOPS) are reported - so efficiency of the approach in practice in uncertain. <sep>	B-rebuttal_process

2020-284	Main description:  paper focuses on training neural networks using 8-bit floating-point numbers (FP8).	B-abstract
2020-284	The goal is highly motivated: training neural networks faster, with smaller memory footprint and energy consumption. <sep>	I-abstract
2020-284	Discussions <sep> reviewer 3: gives a very short review and is not knowledagble in this area (rating is weak accept) <sep>	B-rating_summary
2020-284	reviewer 4: well written and convincing paper, some minor technical flaws (not very knowledgable) <sep>	B-strength
2020-284	reviewer 1: interesting paper but argues not very practical (not very knowledgable) <sep>	I-strength
2020-284	reviewer 2: this is the most thorough and knowledable review, and here the authors like the scope of the paper and its interest to ICLR. <sep>	I-strength
2020-284	Recommendation: going mainly by reviewer 2, i vote to accept this as a poster	B-decision

2020-286	Sleep" is introduced as a way of increasing robustness in neural network training.	B-abstract
2020-286	To sleep, the network is converted into a spiking network and goes through phases of more and less intense activation.	I-abstract
2020-286	The results are quite good when it comes to defending against adversarial examples.	I-abstract
2020-286	Reviewers agree that the method is novel and interesting.	B-strength
2020-286	Authors responded to the reviewers' questions (one of the reviewers had a quite extensive set of questions) satisfactorily, and improved the paper significantly in the process.	B-rebuttal_process
2020-286	I think the paper should be accepted on the grounds of novelty and good results.	B-decision

2020-287	This is an interesting contribution that sheds some light on a well-studied but still poorly understood problem.	B-strength
2020-287	I think it might be of interest to the community.	I-strength

2020-291	This paper studies the question of why a network trained to reproduce a single image often de-noises the image early in training.	B-abstract
2020-291	This an interesting question and, post discussion, all three reviewers agree that it will be of general interest to the community and is worth publishing.	B-rating_summary
2020-291	Therefore I recommend it be accepted.	B-decision

2020-296	The authors propose to learn space-aware 3D feature abstractions of the world given 2.5D input, by minimizing 3D and 2D view contrastive prediction objectives.	B-abstract
2020-296	The work builds upon Tung et al (2019) but extends it by removing some of the limitations, making it thus more general.	I-abstract
2020-296	To do so, they learn an inverse graphics network which takes as input 2.5D video and maps to a 3D feature maps of the scene.	I-abstract
2020-296	The authors present experiments on both real and simulation datasets  and their proposed approach is tested on feature learning, 3D moving object detection, and 3D motion estimation with good performance.	I-abstract
2020-296	All reviewers agree that this is an important problem in computer vision and the papers provides a working solution.	B-strength
2020-296	The authors have done a good job with comparisons and make a clear case about their superiority of their model (large datasets, multiple tasks).	I-strength
2020-296	Moreover, the rebuttal period has been quite productive, with the authors incorporating reviewers' comments in the manuscript, resulting thus in a stronger submission.	B-rebuttal_process
2020-296	Based in reviewer's comment and my own assessment, I think this paper should get accepted, as the experiments are solid with good results that the CV audience of ICLR would find relevant. <sep>	B-decision

2020-302	This paper studies learning with noisy labels by integrating the idea of curriculum learning. <sep>	B-abstract
2020-302	All reviewers and AC are happy with novelty, clear write-up and experimental results. <sep>	B-strength
2020-302	I recommend acceptance. <sep>	B-decision

2020-316	This paper presents a detailed comparison of different bonus-based exploration methods on a common evaluation framework (Rainbow) when used with the ATARI game suite.	B-abstract
2020-316	They find that while these bonuses help on Montezuma's Revenge (MR), they underperform relative to epsilon-greedy on other games.	I-abstract
2020-316	This suggests that architectural changes may be a more important factor than bonus-based exploration in recent advances on MR. <sep>	I-abstract
2020-316	The reviewers commented that this paper makes no effort to present new techniques, and the insights discovered could be expanded on.	B-weakness
2020-316	Despite this, it is an interesting paper that is generally well argued and would be a useful contribution to the field.	B-strength
2020-316	I recommend acceptance.	B-decision

2020-328	Four reviewers have assessed this paper and they have scored it as 6/6/6/6 after rebuttal.	B-rating_summary
2020-328	Nonetheless, the reviewers have raised a number of criticisms and the authors are encouraged to resolve them for the camera-ready submission.	B-suggestion
2020-328	Especially, the authors should take care to make this paper accessible (understandable) to the ML community as ICLR is a ML venue (rather than quantum physics one).	I-suggestion
2020-328	Failure to do so will likely discourage the generosity of reviewers toward this type of submissions in the future.	I-suggestion

2020-343	Main content: Paper proposes a fast network adaptation (FNA) method, which takes a pre-trained image classification network, and produces a network for the task of object detection/semantic segmentation <sep>	B-abstract
2020-343	Summary of discussion: <sep> reviewer1: interesting paper with good results, specifically without the need to do pre-training on Imagenet.	B-strength
2020-343	Cons are better comparisons to existing methods and run on more datasets. <sep>	B-weakness
2020-343	reviewer2:  interesting idea on adapting source network network via parameter re-mapping that offers good results in both performance and training time. <sep>	B-strength
2020-343	reviewer3: novel method overall, though some concerns on the concrete parameter remapping scheme.	I-strength
2020-343	Results are impressive <sep>	I-strength
2020-343	Recommendation: Interesting idea and good results.	I-strength
2020-343	Paper could be improved with better comparison to existing techniques.	B-weakness
2020-343	Overall recommend weak accept.	B-decision

2020-378	The paper proposes an algorithm for learning a transport cost function that accurately captures how two datasets are related by leveraging side information such as a subset of correctly labeled points.	B-abstract
2020-378	The reviewers believe that this is an interesting and novel idea.	B-strength
2020-378	There were several questions and comments, which the authors adequately addressed.	B-rebuttal_process
2020-378	I recommend that the paper be accepted.	B-decision

2020-380	Congratulations on getting your paper accepted to ICLR.	B-decision
2020-380	Please make sure to incorporate the reviewers' suggestions for the final version.	B-suggestion

2020-384	Under the optimization formulation of adversarial attack, this paper proposes two methods to improve the transferability of adversarial examples, namely Nesterov Iterative Fast Gradient Sign Method (NI-FGSM) and Scale-Invariant attack Method (SIM).	B-abstract
2020-384	NI-FGSM adapts Nesterov accelerated gradient into the iterative attacks to effectively look ahead and avoid the "missing" of the global maximum, and SIM optimizes the adversarial perturbations over the scale copies of the input images so as to avoid "overfitting" on the white-box model being attacked and generate more transferable adversarial examples.	I-abstract
2020-384	Empirical results demonstrate the effectiveness of the proposed methods.	I-abstract
2020-384	The ideas are sensible, and the empirical studies were strengthened during rebuttal.	B-rebuttal_process

2020-401	The paper proposes a very simple but thoroughly evaluated and investigated idea for improving generalization in GCNs.	B-abstract
2020-401	Though the reviews are mixed, and in the post-rebuttal discussion the two negative reviewers stuck to their ratings, the area chair feels that there are no strong grounds for rejection in the negative reviews.	B-rebuttal_process
2020-401	Accept.	B-decision

2020-412	The paper proposes a "compressive transformer", an extension of the transformer, that keeps a compressed long term memory in addition to the fixed sized memory.	B-abstract
2020-412	Both memories can be queried using attention weights.	I-abstract
2020-412	Unlike TransfomerXL that discards the oldest memories, the authors propose to "compress" those memories.	I-abstract
2020-412	The main contribution of this work is that that it introduces a model that can handle extremely long sequences.	I-abstract
2020-412	The authors also introduces a new language modeling dataset based on text from Project Gutenberg that has much longer sequences of words than existing datasets.	I-abstract
2020-412	They provide comprehensive experiments comparing against different compression strategies and compares against previous methods, showing that this method is able to result in lower word-level perplexity.	I-abstract
2020-412	In addition, the authors also present evaluations on speech, and image sequences for RL. <sep>	I-abstract
2020-412	Initially the paper received weak positive responses from the reviewers.	B-rebuttal_process
2020-412	The reviewers pointed out some clarity issues with details of the method and figures and some questions about design decisions.	I-rebuttal_process
2020-412	After rebuttal, all of the reviewers expressed that they were very satisfied with the authors responses and increased their scores (for a final of 2 accepts and 1 weak accept). <sep>	I-rebuttal_process
2020-412	The authors have provided a thorough and well-written paper, with comprehensive and convincing experiments.	B-strength
2020-412	In addition, the ability to model long-range sequences and dependencies is an important problem and the AC agrees that this paper makes a solid contribution in tackling that problem.	I-strength
2020-412	Thus, acceptance is recommended.	B-decision

2020-419	This paper proposes a solid (if somewhat incremental) improvement on an interesting and well-studied problem.	B-strength
2020-419	I suggest accepting it.	B-decision

2020-432	The authors present an algorithm CHOCO-SGD to make use of communication compression in a decentralized setting.	B-abstract
2020-432	This is an interesting problem, and the paper is well-motivated and well-written.	B-strength
2020-432	On the theoretical side, the authors prove the convergence rate of the algorithm on non-convex smooth functions, which shows a nearly linear speedup.	B-abstract
2020-432	The experimental results on several benchmark datasets validate the algorithm achieves better performance than baselines.	I-abstract
2020-432	These can be made more convincing by comparing with more baselines (including DeepSqueeze and other centralized algorithms with a compression scheme), and on larger datasets.	B-suggestion
2020-432	The authors should also clarify results on consensus.	B-weakness

2020-452	This paper studies generalizations of Variational Autoencoders to Non-Euclidean domains, modeled as products of constant curvature Riemannian manifolds.	B-abstract
2020-452	The framework allows to simultaneously learn the latent representations as well as the curvature of the latent domain. <sep>	I-abstract
2020-452	Reviewers were unanimous at highlighting the significance of this work at developing non-Euclidean tools for generative modeling.	B-strength
2020-452	Despite the somewhat preliminary nature of the empirical evaluation, there was consensus that the paper puts forward interesting tools that might spark future research in this direction.	I-strength
2020-452	Given those positive assessments, the AC recommends acceptance. <sep>	B-decision

2020-467	This paper proposes methodology to train binary neural networks. <sep>	B-abstract
2020-467	The reviewers and authors engaged in a constructive discussion.	B-rebuttal_process
2020-467	All the reviewers like the contributions of the paper. <sep>	O
2020-467	Acceptance is therefore recommended.	B-decision

2020-490	This paper presents a sampling-based approach for generating compact CNNs by pruning redundant filters.	B-abstract
2020-490	One advantage of the proposed method is a bound for the final pruning error. <sep>	I-abstract
2020-490	One of the major concerns during review is the experiment design.	B-weakness
2020-490	The original paper lacks the results on real work dataset like ImageNet.	I-weakness
2020-490	Furthermore, the presentation is a little misleading.	I-weakness
2020-490	The authors addressed most of these problems in the revision. <sep>	B-rebuttal_process
2020-490	Model compression and purring is a very important field for real world application, hence I choose to accept the paper. <sep>	B-decision

2020-506	Although some criticism remains for experiments, I suggest to accept this paper.	B-decision

2020-510	This paper considers hierarchical reinforcement learning, and specifically the case where the learning and use of lower-level skills should not be decoupled.	B-abstract
2020-510	To this end the paper proposes Hierarchical Proximal Policy Optimization (HiPPO) to jointly learn the different layers of the hierarchy.	I-abstract
2020-510	This is compared against other hierarchical RL schemes on several Mujoco domains. <sep>	I-abstract
2020-510	The reviewers raised three main issues with this paper.	O
2020-510	The first concerns an excluded baseline, which was included in the rebuttal.	B-rebuttal_process
2020-510	The other issues involve the motivation for the paper (in that there exist other methods that try and learn different levels of hierarchy together) and justification for some design choices.	B-weakness
2020-510	These were addressed to some extent in the rebuttal, but I believe this to still be an interesting contribution to the literature, and should be accepted. <sep>	B-decision

2020-513	The paper proposes an attention mechanism for equivariant neural networks towards the goal of attending to co-occurring features.	B-abstract
2020-513	It instantiates the approach with rotation and reflection transformations, and reports results on rotated MNIST and CIFAR-10.	I-abstract
2020-513	All reviewers have found the idea of using self-attention on top of equivariant feature maps technically novel and sound.	B-strength
2020-513	There were some concerns about readability which the authors should try to address in the final version.	B-suggestion

2020-518	Three reviewers have assessed this paper and they have scored it as 6/6/6/6 after rebuttal.	B-rebuttal_process
2020-518	Nonetheless, the reviewers have raised a number of criticisms and the authors are encouraged to resolve them for the camera-ready submission.	B-suggestion

2020-521	The paper provides theoretical justification for a previously proposed method for uncertainty estimation based on sampling from a prior distribution (Osband et al, Burda et al). <sep>	B-abstract
2020-521	The reviewers initially raised concerns about significance, clarity and experimental evaluation, but the author rebuttal addressed most of these concerns. <sep>	B-rebuttal_process
2020-521	In the end, all the reviewers agreed that the paper deserves to be accepted.	B-rating_summary

2020-541	The paper proposes a method to control dynamical systems described by a partial differential equations (PDE).	B-abstract
2020-541	The method uses a hierarchical predictor-corrector scheme that divides the problem into smaller and simpler temporal subproblems.	I-abstract
2020-541	They illustrate the performance of their method on 1D Burger's PDE and 2D incompressible flow. <sep>	I-abstract
2020-541	The reviewers are all positive about this paper and find it well-written and potentially impactful.	B-rating_summary
2020-541	Hence, I recommend acceptance of this paper.	B-decision

2020-557	The paper presents a new take on exploration in multi-agent reinforcement learning settings, and presents two approaches, one motivated by information theoretic, the other by decision theoretic influence on other agents.	B-abstract
2020-557	Reviewers consider the proposed approach "pretty elegant, and in a sense seem fundamental", the experimental section "thorough", and expect the work to "encourage future work to explore more problems in this area".	B-strength
2020-557	Several questions were raised, especially regarding related work, comparison to single agent exploration approaches, and several clarifying questions.	B-weakness
2020-557	These were largely addressed by the authors, resulting in a strong submission with valuable contributions.	B-rebuttal_process

2020-564	The paper considers an interesting algorithm on zeorth-order optimization and contains strong theory.	B-strength
2020-564	All the reviewers agree to accept.	B-rating_summary

2020-584	Main content: <sep> Blind review #1 summarizes it well: <sep> This paper presents a semantic parser that operates over passages of text instead of a structured data source.	B-abstract
2020-584	This is the first time anyone has demonstrated such a semantic parser (Siva Reddy and several others have essentially used unstructured text as an information source for a semantic parser, similar to OpenIE methods, but this is qualitatively different).	I-abstract
2020-584	The key insight is to let the semantic parser point to locations in the text that can be used in further symbolic operations.	I-abstract
2020-584	This is excellent work, and it should definitely be accepted.	B-decision
2020-584	I have a ton of questions about this method, but they are good questions. <sep>	O
2020-584	-- <sep>	O
2020-584	Discussion: <sep> The reviews all agree on a generally positive assessment, and focus on details that have been addressed, rather than major problems. <sep>	B-rating_summary
2020-584	-- <sep>	O
2020-584	Recommendation and justification: <sep> This paper should be accepted.	B-decision
2020-584	Even though novelty in terms of fundamental machine learning components is minimal, but the architecture employing neural models to do symbolic work is a good contribution in a crucial direction (especially in the theme of ICLR).	B-strength

2020-593	This is a very interesting paper which extends natural gradient to output space metrics other than the Fisher-Rao metric (which is motivated by approximating KL divergence).	B-abstract
2020-593	It includes substantial mathematical and algorithmic insight.	I-abstract
2020-593	The method is shown to outperform various other optimizers on a neural net optimization problem that's artificially made ill-conditioned; while it's not clear how practically meaningful this setting is, it seems like a good way to study optimization.	B-strength
2020-593	I think this paper will be of interest to a lot of researchers and could open up new research directions, so I recommend acceptance as an Oral. <sep>	B-decision

2020-612	The authors present a hierarchical explanation model for understanding the underlying representations produced by LSTMs and Transformers.	B-abstract
2020-612	Using human evaluation, they find that their explanations are better, which could lead to better trust of these opaque models. <sep>	I-abstract
2020-612	The reviewers raised some issues with the derivations, but the author response addressed most of these.	B-rebuttal_process

2020-622	The paper generalizes several existing results for structured linear transformations in the form of K-matrices.	B-abstract
2020-622	This is an excellent paper and all reviewers confirmed that.	O

2020-639	This paper is consistently supported by all three reviewers and thus an accept is recommended.	B-decision

2020-640	The paper proposed a new synthetically generated video dataset (CATER) for benchmarking temporal reasoning.	B-abstract
2020-640	The dataset is based on the CLEVR dataset and provides videos make up of primitive actions ("rotate", "pick-place", "slide", "contain") that can be combined to form for complex actions. <sep>	I-abstract
2020-640	The paper also benchmarks a variety of methods on three proposed tasks (atomic action classification, composite action classification, and 'snitch' localization) and demonstrates that while it is possible to get high performance on atomic action classification, the other two task are still challenging and requires temporal modeling. <sep>	I-abstract
2020-640	Overall, all reviewers found the paper to be well written and easy to follow, with care given to the dataset construction, as well as the task definitions and experiment setup and analysis.	B-strength
2020-640	The paper received strong scores from all reviewers (3 accepts).	B-rating_summary
2020-640	Based on the reviewer comments, the authors further improved the paper by adding additional relevant datasets for comparison and providing missing details pointed out by the reviewers.	B-rebuttal_process
2020-640	After the rebuttal, the reviewers remained positive.	B-rating_summary

2020-670	This paper proposes a RNA structure prediction algorithm based on an unrolled inference algorithm.	B-abstract
2020-670	The proposed approach overcomes limitations of previous methods, such as dynamic programming (which does not work for molecular configurations that do not factorize), or energy-based models (which require a minimization step, eg by using MCMC to traverse the energy landscape and find minima). <sep>	I-abstract
2020-670	Reviewers agreed that the method presented here is novel on this application domain, has excellent empirical evaluation setup with strong numerical results, and has the potential to be of interest to the wider deep learning community.	B-strength
2020-670	The AC shares these views and recommends an enthusiastic acceptance.	B-decision

2020-677	All three reviewers strongly recommend accepting this paper.	B-rating_summary
2020-677	It is clear, novel, and a significant contribution to the field.	B-strength
2020-677	Please take their suggestions into account in a camera ready version.	B-suggestion
2020-677	Thanks!	O

2020-725	This paper presents a novel option discovery mechanism through incrementally learning reusasble options from a small number of policies that are usable across multiple tasks. <sep>	B-abstract
2020-725	The primary concern with this paper was with a number of issues around the experiments.	B-weakness
2020-725	Specifically, the reviewers took issue with the definition of novel tasks in the Atari context.	I-weakness
2020-725	A more robust discussion and analysis around what tasks are considered novel would be useful.	B-suggestion
2020-725	Comparisons to other option discovery papers on the Atari domains is also required. <sep>	I-suggestion
2020-725	Additionally, one reviewer had concerns on the hard limit of option execution length which remain unresolved following the discussion. <sep>	B-rebuttal_process
2020-725	While this is really promising work, it is not ready to be accepted at this stage.	B-decision

2020-727	All reviewers recommend reject, and there is no rebuttal.	B-rating_summary

2020-733	The paper proposes a method for performing active learning on graph convolutional networks.	B-abstract
2020-733	In particular, instead of performing uncertainty-based sampling based on an individual node level, the authors propose to look at regional based uncertainty.	I-abstract
2020-733	They propose an efficient algorithm based on page rank.	I-abstract
2020-733	Empirically, they compare their method to several other leading methods, comparing favorably. <sep>	I-abstract
2020-733	Reviewers found the work poorly organized and difficult to read.	B-weakness
2020-733	The idea to use region based estimates is intuitive but feels like nothing more than just that.	I-weakness
2020-733	It's not clear if there is a mathematical basis to justify such a method (eg an analysis of sample complexity as has been accomplished in other graph active learning problems, Dasarathy, Nowak, Zhu 2015). <sep>	I-weakness
2020-733	The idea requires further study and justification, and the paper needs an improved exposition.	I-weakness
2020-733	Finally, the authors were not anonymized on the PDF.	O

2020-734	The paper aims to find locally interpretable models, such that the local models are fit (wrt the ground truth) and faithful (wrt the global underlying black box model). <sep>	B-abstract
2020-734	The contribution of the paper is that the local model is trained from a subset of points, selected via an optimized importance weight function.	I-abstract
2020-734	The difference compared to Ren et al (cited) is that the IW function is non-differentiable and optimized using Reinforcement Learning. <sep>	I-abstract
2020-734	A first concern (Rev#1, Rev#2) regards the positioning of the paper wrt RL, as the actual optimization method could be any black-box optimization method: one wants to find the IW that maximizes the faithfulness.	B-rebuttal_process
2020-734	The rebuttal makes a good job in explaining the impact of using a non-differentiable IW function. <sep>	I-rebuttal_process
2020-734	A second concern (Rev#2) regards the interpretability of the IW underlying the local interpretable model. <sep>	I-rebuttal_process
2020-734	There is no doubt that the paper was considerably improved during the rebuttal period.	I-rebuttal_process
2020-734	However, the improvements raise additional questions (eg about selecting the IW depending on the distance to the probes).	I-rebuttal_process
2020-734	I encourage the authors to continue on this promising line of search.	O

2020-760	This paper conducted a number of empirical studies to find whether units in object-classification CNN can be used as object detectors.	B-abstract
2020-760	The claimed conclusion is that there are no units that are sufficient powerful to be considered as object detectors.	I-abstract
2020-760	Three reviewers have split reviews.	B-rating_summary
2020-760	While reviewer #1 is positive about this work, the review is quite brief.	I-rating_summary
2020-760	In contrast, Reviewer #2 and #3 both rate weak reject, with similar major concerns.	I-rating_summary
2020-760	That is, the conclusion seems non-conclusive and not surprising as well.	B-weakness
2020-760	What would be the contribution of this type of conclusion to the ICLR community?	I-weakness
2020-760	In particular, Reviewer #2 provided detailed and well elaborated comments.	O
2020-760	The authors made efforts to response to all reviewers' comments.	B-rebuttal_process
2020-760	However, the major concerns remain, and the rating were not changed.	I-rebuttal_process
2020-760	The ACs concur the major concerns and agree that the paper can not be accepted at its current state.	B-decision

2020-791	This submission has been assessed by three reviewers who scored it 3/1/3, and they have remained unconvinced after the rebuttal.	B-rebuttal_process
2020-791	The main issues voiced are the difficult readability of the paper, cryptic at times due to a mix of physical and DL notations, and a lack of sufficient experimentation to support all claims.	B-weakness
2020-791	The reviewers acknowledge the authors' efforts to resolve the main issues but find these efforts insufficient.	B-rebuttal_process
2020-791	Thus, this paper cannot be accepted to ICLR2020.	B-decision

2020-794	Thanks to reviewers and authors for an interesting discussion.	O
2020-794	It seems the central question is whether learning to identify correct bijections should be part of graph classification problems, or whether this leads to bias and overfitting.	B-rebuttal_process
2020-794	Reviews are generally negative, putting this in the lower third of the submissions.	B-rating_summary
2020-794	The paper, however, inspired an interesting discussion, and I would encourage the authors to continue this line of work, addressing the question of bias and overfitting more directly, possibly going beyond dataset evaluation and, for example, thinking about how to evaluate whether training on non-isomorphic graphs leads to better off-training set generalization.	B-suggestion

2020-802	This paper presents a method for speeding up Gaussian process inference by leveraging locality information through k-nearest neighbours. <sep>	B-abstract
2020-802	The key idea is well-motivated intuitively, however the way in which it is implemented seems to introduce new complications.	B-weakness
2020-802	One such issue is KNN overhead in high dimensions, but R1 outlines other potential issues too.	I-weakness
2020-802	Moreover, the method's merit is not demonstrated in a convincing way through the experiments.	I-weakness
2020-802	The authors have provided a rebuttal for those issues, but it does not seem to solve the concerns entirely. <sep>	B-rebuttal_process

2020-805	The work proposes to learn neural networks using a homotopy-based continuation method.	B-abstract
2020-805	Reviewers found the idea interesting, but the manuscript poorly written, and lacking in experimental results.	B-weakness
2020-805	With no response from the authors, I recommend rejecting the paper.	B-decision

2020-823	This paper presents a new metric for adversarial attack's detection.	B-abstract
2020-823	The reviewers find the idea interesting, but the some part has not been clearly explained, and there are questions on the reproducibility issue of the experiments.	B-weakness

2020-839	Based on the Bayesian approach to filtering problem, the paper proves that RNN are universal approximators for the filtering problem.	B-abstract
2020-839	Two reviewers, however, have doubts about the novelty and difficulty to get the result.	B-weakness
2020-839	Although I do not fully agree that Reviewer3 that the proof is just "DNN can fit anything" - it is not this case, but the concerns of Reviewer2 are more strong, especially about the usage of the term "recurrent neural network".	I-weakness
2020-839	The paper is purely theoretical and does not have any numerical experiments, which probably makes it too weak for ICLR in this form.	I-weakness
2020-839	However, I encourage the authors to continue to work on the subject, since the approach looks very interesting but it still very far from practice.	O

2020-849	Two reviewers are negative on this paper while the other one is slightly positive.	B-rating_summary
2020-849	Overall, this paper does not make the bar of ICLR.	B-decision
2020-849	A reject is recommended.	I-decision

2020-850	After the rebuttal, the reviewers agree that this paper would benefit from further revisions to clarify issues regarding the motivation of the DP-based security definition,  any relationship it may have to standard definitions of privacy, and the role of dimensionality in the theoretical guarantees.	B-rebuttal_process

2020-860	This submission studies an interesting problem.	B-strength
2020-860	However, as some of the reviewers point out, the novelty of the proposed contributions is fairly limited.	B-weakness

2020-866	The paper investigates why adversarial training can sometimes degrade model performance on clean input examples. <sep>	B-abstract
2020-866	The reviewers agreed that the paper provides valuable insights into how adversarial training affects the distribution of activations.	B-strength
2020-866	On the other hand, the reviewers raised concerns about the experimental setup as well as the clarity of the writing and felt that the presentation could be improved. <sep>	B-weakness
2020-866	Overall, I think this paper explores a very interesting direction and such papers are valuable to the community.	B-strength
2020-866	It's a borderline paper currently but I think it could turn into a great paper with another round of revision.	B-rating_summary
2020-866	I encourage the authors to revise the draft and resubmit to a different venue. <sep>	B-decision

2020-879	The reviewers were unanimous that this submission is not ready for publication at ICLR in its current form. <sep>	B-rating_summary
2020-879	Concerns raised included that the method was not sufficiently general, including in choice of experiments reported, and the lack of discussion of some lines of significantly related work.	B-weakness

2020-880	This paper introduces a method for building interpretable classifiers, along with a measure of "concept accuracy" to evaluate interpretability, and primarily applies this method to text models, but includes a proof of concept on images in the appendix. <sep>	B-abstract
2020-880	The main contributions are sensible enough, but the main problems the reviewers had were: <sep> A) The performance of the proposed method <sep>	B-weakness
2020-880	B) The lack of human evaluation of interpretability, and <sep>	I-weakness
2020-880	C) Lack of background and connections to other work. <sep>	I-weakness
2020-880	The authors improved the paper considerably during the rebuttal period, and might have addressed point C) satisfactorily, but only after several back and forths, and at this point it's too late to re-evaluate the paper.	B-rebuttal_process
2020-880	I expect that a more polished version of this paper would be acceptable in a future conference. <sep>	B-decision
2020-880	I mostly ignored R1's review as they didn't seem to put much thought into their review and didn't respond to requests for clarifications.	B-rebuttal_process

2020-883	The authors received reviews from true experts and these experts felt the paper was not up to the standards of ICLR. <sep>	B-rating_summary
2020-883	Reviewer 3 and Reviewer 1 disagree as to whether the new notion of generalization error is appropriate.	O
2020-883	I think both cases can be defended.	O
2020-883	I think the authors should aim to sharpen their argument in this regard.Several reviewers at one point remark that the results follow from standard techniques: shouldn't this be the case?	B-suggestion
2020-883	I believe the actual criticism being made is that the value of these new results do not go above and beyond existing ones.	B-weakness
2020-883	There is also the matter of what value should be attributed to technical developments on their own.	I-weakness
2020-883	On this matter, the reviewers seem to agree that the derivations lean heavily on prior work.	I-weakness

2020-894	This paper proposes to mitigate mode collapse in GANs by encouraging distribution matching in the latent space.	B-abstract
2020-894	Reviewers 1 and 3 expressed concerns that the methodology is too incremental in the context of the existing literature (VEEGAN, VAE-GAN, AAE).	B-weakness
2020-894	This, combined with the lack of up-to-date baselines, makes it difficult to access the significance of the proposed modifications.	I-weakness
2020-894	The quality and precision of the writing can also be improved to meet the standards of publication at a top-tier conference. <sep>	B-decision

2020-895	The submission describes a new two-stage training scheme for multi-modal image-to-image translation.	B-abstract
2020-895	The new scheme is compared to a single-stage end-to-end baseline, and the advantage of the new scheme is demonstrated empirically.	I-abstract
2020-895	All three reviewers appreciate the proposed contribution and the quality improvement it brings over the baseline.	B-strength
2020-895	At the same time, the reviewers see the contribution as incremental and not sufficient for an ICLR paper.	B-rating_summary
2020-895	The author response and paper adjustment have not changed the opinion of the reviewers, so the overall recommendation is to reject.	B-decision

2020-897	This paper presents sparse attention mechanisms for image captioning.	B-abstract
2020-897	In addition to recent sparsemax based method, authors proposed to extend it by incorporating structural constraints in 2D images, which is called TVMAX.	I-abstract
2020-897	The proposed methods are shown to improve the quality of captioning, particularly in terms of fewer erroneous repetitions, and obtain better human evaluation scores. <sep>	I-abstract
2020-897	Through reviewer discussion, one reviewer updated the score to rejection.	B-rating_summary
2020-897	A major concern raised by the reviewers is that the motivation of introducing sparse attention is not clear, and the reason why it improves the quality (particularly, why it can reduce repetition) is not convincing.	B-weakness
2020-897	While we understand it is plausible for long sequences as in text domain, we are not convinced that it is really necessary for image captioning problems.	I-weakness
2020-897	Although authors seem to have some ideas, we cannot see how they will be reflected in the paper so I'd like to recommend rejection. <sep>	B-decision
2020-897	I recommend authors to polish the paper with a clearer description of the motivation and high-level analysis of the method as well as testing on other visual tasks to show its generality. <sep>	B-suggestion

2020-899	This paper proposes augmentation of the state exploration strategy that is interesting and has a potential to lead to improvement.	B-abstract
2020-899	However, the current presentation makes it difficult to properly assess that.	B-weakness
2020-899	In particular, the way the authors convey both the underlying intuition and its implementation is fairly vague and does not build confidence in the grounding of the underlying methodology.	I-weakness

2020-916	The paper proposes a domain-adaptive filter decomposition method via separating domain-specific and cross-domain features, towards learning invariant representations for unsupervised domain adaptation. <sep>	B-abstract
2020-916	Overall, this well-written paper is well motivated with a better technique for learning invariant representations using convolutional filters.	B-strength
2020-916	Nonetheless, reviewers still have major concerns: 1) the novelty of the paper may be marginal given the significant line of recent work on learning domain-invariant representations; 2) when the label distributions differ, learning invariant representations can only lead to worse target generalizations; 3) the provided theory has an unclear connection to the presented filter decomposition method.	B-weakness
2020-916	The paper can be strengthened by further discussions on how to mitigate the aforementioned negative results. <sep>	B-suggestion
2020-916	Hence I recommend rejection.	B-decision

2020-928	In this paper dense layers in deep neural networks representing policies are replaced by tensor regression layers, also by a scattering layer, and second-order optimization is considered.	B-abstract
2020-928	The paper does not have a single consistent message, and combines different techniques for unclear reason.	B-weakness
2020-928	Important related work is not cited.	I-weakness
2020-928	The presentation was found unclear by the reviewers.	I-weakness

2020-943	This paper studies tradeoffs in the design of attention-based architectures.	B-abstract
2020-943	It argues and formally establishes that the expressivity of an attention head is determined by its dimension and that fixing the head dimension, one gains additional expressive power by using more heads. <sep>	I-abstract
2020-943	Reviewers were generally positive about the question under study here, but raised important concerns about the significance of the results and the take-home message in the current manuscript.	B-rating_summary
2020-943	The AC shares these concerns, and recommends rejection, while encouraging the authors to address the concerns raised during this discussion.	B-decision

2020-949	This work claims two primary contributions: first a new saliency method "expected gradients" is proposed, and second the authors propose the idea of attribution priors to improve model performance by integrating domain knowledge during training.	B-abstract
2020-949	Reviewers agreed that the expected gradients method is interesting and novel, and experiments such as Table 1 are a good starting point to demonstrate the effectiveness of the new method.	B-strength
2020-949	However, the claimed "novel framework, attribution priors" has large overlap with prior work [1].	B-weakness
2020-949	One suggestion for improving the paper is to revise the introduction and experiments to support the claim "expected gradients improve model explainability and yield effective attribution priors" rather than claiming to introduce attribution priors as a new framework.	B-suggestion
2020-949	One possibility for strengthening this claim is to revisit experiments in [1] and related follow-up work to demonstrate that expected gradients yield improvements over existing saliency methods.	I-suggestion
2020-949	Additionally, current experiments in Table 1 only consider integrated gradients as a baseline saliency method, there are many others worth considering, see for example the suite of methods explored in [2]. <sep>	I-suggestion
2020-949	Finally, I would add that the current section on distribution shift provides an overly narrow perspective on model robustness by only considering robustness to additive Gaussian noise.	B-weakness
2020-949	It is known that it is easy to improve robustness to Gaussian noise by biasing the model towards low frequency statistics in the data, however this typically results in degraded robustness to other kinds of noise types.	I-weakness
2020-949	See for example [3], where it was observed that adversarial training degrades model robustness to low frequency noise and the fog corruption.	I-weakness
2020-949	If the authors wish to pursue using attribution priors for improving robustness to distribution shift, it is important that they evaluate on a more varied suite of corruptions/noise types [4].	B-suggestion
2020-949	Additionally, one should compare against strong baselines in this area [5]. <sep>	I-suggestion
2020-949	1. https://arxiv.org/abs/1703.03717 <sep>	O
2020-949	2. https://arxiv.org/abs/1810.03292 <sep>	O
2020-949	3. https://arxiv.org/abs/1906.08988 <sep>	O
2020-949	4. https://arxiv.org/abs/1807.01697 <sep>	O
2020-949	5. https://arxiv.org/abs/1811.12231 <sep>	O

2020-954	This paper presents an encoder-decoder based approach to construct a compressed latent space representation of each molecule.	B-abstract
2020-954	Then a second neural network segments the output and assigns an atomic number.	I-abstract
2020-954	Unlike previous works using 1D or 2D representations, the proposed method focuses on the 3D representations. <sep>	I-abstract
2020-954	The reviewers have several major concerns.	O
2020-954	Firstly, the novelty of the paper seems to be limited as the proposed method mainly use the existing techniques.	B-weakness
2020-954	Secondly, there is no clear baseline to compare with.	I-weakness
2020-954	Finally, there is no clear quantitative results to measure the proposed method.	I-weakness
2020-954	The rebuttal did not well address these problems. <sep>	B-rebuttal_process
2020-954	Overall, this paper did not meet the standard of ICLR and I choose to reject the paper. <sep>	B-decision

2020-965	This paper presents an empirical analysis of the reasons behind BatchNorm vulnerability to adversarial inputs, based on the hypothesis that such vulnerability may be caused by using different statistics during the inference stage as compared to the training stage.	B-abstract
2020-965	While the paper is interesting and clearly written, reviewers point out insufficient empirical evaluation in order to make the claim more convincing.	B-weakness

2020-975	The paper is develops a self-training framework for graph convolutional networks where we have partially labeled graphs with a limited amount of labeled nodes.	B-abstract
2020-975	The reviewers found the paper interesting.	B-strength
2020-975	One reviewer notes the ability to better exploit available information and raised questions of computational costs.	B-weakness
2020-975	Another reviewer felt the difference from previous work was limited, but that the good results speak for themselves.	B-strength
2020-975	The final reviewer raised concerns on novelty and limited improvement in results.	B-weakness
2020-975	The authors provided detailed responses to these queries, providing additional results. <sep>	B-rebuttal_process
2020-975	The paper has improved over the course of the review, but due to a large number of stronger papers, was not accepted at this time.	B-decision

2020-1002	This paper studies the robust reinforcement learning problem in which the constraint on model uncertainty is captured by the Wasserstein distance.	B-abstract
2020-1002	The reviewers expressed concerns regarding novelty with respect to prior work, the presentation or the results, and unconvincing experiments.	B-weakness
2020-1002	In its current form the paper is not ready for acceptance to ICLR-2020.	B-decision

2020-1003	This paper proposes to use stacked layers of Gaussian latent variables with a maxent objective function as a regulariser.	B-abstract
2020-1003	I agree with the reviewers that there is very little novelty and the experiments are not very convincing.	B-weakness

2020-1021	This paper has, at its core, a potential for constituting a valuable contribution.	B-strength
2020-1021	However, there was a shared belief among reviewers (that I also share) that the paper still has much room for improvement in terms of presentation and justification of the claims.	B-weakness
2020-1021	I hope that the authors will be able to address the feedback they received to make this submission get where it should be. <sep>	B-suggestion

2020-1033	The paper proposed a new metric to define the quality of optimizers as a weighted average of the scores reached after a certain number of hyperparameters have been tested. <sep>	B-abstract
2020-1033	While reviewers (and myself) understood the need to better be able to compare optimizers, they failed to be convinced by the proposed solutions.	B-weakness
2020-1033	In particular (setting aside several complaints of the reviewers with which I disagree), by defining a very versatile metric, this paper lacks a strong conclusion as the ranking of optimizers would clearly depend on the instantiation of that metric. <sep>	I-weakness
2020-1033	Although that is to be expected, by the very behaviour of these optimizers, it makes it unclear what the added value of the metric is.	I-weakness
2020-1033	As one reviewer pointed out,  all the points made could have been similarly made with other, more common plots. <sep>	I-weakness
2020-1033	Ultimately, it wasn't clear to me what the paper was trying to achieve beyond defining a mathematical formula encompassing all "standard" evaluation metric, which I unfortunately see of limited value.	I-weakness

2020-1038	The paper is well-motivated by neuroscience that our brains use information from outside the receptive field of convolutive processes through top-down mechanisms.	B-abstract
2020-1038	However, reviewers feel that the results are not near the state of the art and the paper needs further experiments and need to scale to larger datasets.	B-weakness

2020-1042	Thanks to the reviewers and the authors for an interesting discussion.	O
2020-1042	The reviewers are mixed, learning toward positive, but a few shortcomings were left unaddressed: (i) Turning the task into a mention-pair classification problem ignores the mention detection step, and synergies from joint modeling are lost.	B-weakness
2020-1042	(ii) Lee et al (2018) has been surpassed by some margin by BERT and spanBERT, models ignored in this paper.	I-weakness
2020-1042	(iii) Several approaches to aggregating structured annotations have already been introduced, eg, for sequence labelling tasks.	I-weakness
2020-1042	[0] Overall, the limited novelty, the missing baselines, and the missing related work lead me to not favor acceptance at this point. <sep>	B-decision
2020-1042	[0] https://www.aclweb.org/anthology/P17-1028/	O

2020-1056	This paper interprets adaptive gradient methods as trust region methods, and then extends the trust regions to axis-aligned ellipsoids determined by the approximate curvature.	B-abstract
2020-1056	It's fairly natural to try to extend the algorithms in this way, but the paper doesn't show much evidence that this is actually effective.	B-weakness
2020-1056	(The experiments show an improvement only in terms of iterations, which doesn't account for the computational cost or the increased batch size; there doesn't seem to be an improvement in terms of epochs.)	I-weakness
2020-1056	I suspect the second-order version might also lose some of the online convex optimization guarantees of the original methods, raising the question of whether the trust-region interpretation really captures the benefits of the original methods.	I-weakness
2020-1056	The reviewers recommend rejection (even after discussion) because they are unsatisfied with the experiments; I agree with their assessment. <sep>	B-decision

2020-1084	All the reviewers recommend rejecting the submission.	B-rating_summary
2020-1084	There is no basis for acceptance.	B-decision

2020-1087	This paper proposes an attack method to improve the transferability of adversarial examples under black-box attack settings. <sep>	B-abstract
2020-1087	Despite the simplicity of the proposed idea, reviewers and AC commonly think that the paper is far from being ready to publish in various aspects: (a) the presentation/writing quality, (b) in-depth analysis and (c) experimental results. <sep>	B-decision
2020-1087	Hence, I recommend rejection.	I-decision

2020-1092	The paper proposes to combine RL and Imitation Learning.	B-abstract
2020-1092	It defines a regularized reward function that minimizes the KL distance between the policy and the expert action.	I-abstract
2020-1092	The formulation is similar to the KL regularized MDPs, but with the difference that an additional indicator function based on the support of the expert's distribution is multiplied to the regularized term. <sep>	I-abstract
2020-1092	Several issues have been brought up by the reviewers, including: <sep> * Comparison with pre-deep learning literature on the combination of RL and imitation learning <sep>	B-weakness
2020-1092	* Similarity to regularized MDP framework <sep>	I-weakness
2020-1092	* Assumption 1 requiring a stochastic expert policy, contradicting the policy invariance claim <sep>	I-weakness
2020-1092	* Difficulty of learning the indicator function of the support of the expert's data distribution <sep>	I-weakness
2020-1092	Some of these issues have been addressed, but at the end of the day, one of the expert reviewers was not convinced that the problem of learning an indicator function is going to be easy at all.	B-rebuttal_process
2020-1092	The reviewer believes that learning such a function requires "learning a harsh approximation of the density of visits of the expert for every state which is a quite hard task, especially in stochastic environments." <sep>	I-rebuttal_process
2020-1092	Another issue is related to the policy invariance under the optimal expert policy.	B-weakness
2020-1092	In most MDPs, the optimal policy is not stochastic and does not satisfy Assumption 1, so the optimal policy invariance proof seems to contradict Assumption 1. <sep>	I-weakness
2020-1092	Overall, it seems that even though this might become a good paper, it requires some improvements.	I-weakness
2020-1092	I encourage the authors to address the reviewers' comments as much as possible.	B-suggestion

2020-1095	This paper analyzes the non-convergence issue in Adam in a simple non-convex case.	B-abstract
2020-1095	The authors propose a new adaptive gradient descent algorithm based on exponential long term memory, and analyze its convergence in both convex and non-convex settings.	I-abstract
2020-1095	The major weakness of this paper pointed out by many reviewers is its experimental evaluation, ranging from experimental design to missing comparison with strong baseline algorithms.	B-weakness
2020-1095	I agree with the reviewers' evaluation and thus recommend reject.	B-decision

2020-1111	This paper presents a new link prediction framework in the case of small amount labels using meta learning methods.	B-abstract
2020-1111	The reviewers think the problem is important, and the proposed approach is a modification of meta learning to this case.	B-strength
2020-1111	However, the method is not compared to other knowledge graph completion methods such as TransE, RotaE, Neural Tensor Factorization in benchmark dataset such as Fb15k and freebase.	B-weakness
2020-1111	Adding these comparisons can make the paper more convincing.	B-suggestion

2020-1114	This paper presents a method which creates a representative subset of testing examples so that the model can be tested quickly during the training.	B-abstract
2020-1114	The procedure makes use of the famous HGS selection algorithm which identifies and then eliminates the redundant and obsolete test cases based on two criteria: (1) structural coverage as measured by the number of neurons activated beyond a certain threshold, and (2) distribution mismatch (as measured by KL divergence) of the last layer activations.	I-abstract
2020-1114	The algorithm has two-phases: (1) a greedy subset selection based on the coverage, and (2) an iterative phase were additional test examples are added until the KL divergence (as defined above) falls below some threshold. <sep>	I-abstract
2020-1114	This approach is incremental in nature -- the resulting multi-objective optimisation problem is not a significant improvement over BOT.	B-weakness
2020-1114	After the discussion phase, we believe that the advantages over BOT were not clearly demonstrated and that the main drawback of BOT (requiring the number of samples) is not hindering practical applications.	B-rebuttal_process
2020-1114	Finally, the empirical evaluation is performed on very small data sets and I do not see an efficient way to apply it to larger data sets where this reduction could be significant.	B-weakness
2020-1114	Hence, I will recommend the rejection of this paper.	B-decision
2020-1114	To merit acceptance to ICLR the authors need to provide a cleaner presentation (especially of the algorithms), with a focus on the incremental improvements over BOT, an empirical analysis on larger datasets, and a detailed look into the computational aspects of the proposed approach. <sep>	B-suggestion

2020-1116	This paper investigated the effect of network width on learned features using activation atlases.	B-abstract
2020-1116	From the current view of deep learning, the novelty of the paper is limited. <sep>	B-weakness
2020-1116	As all reviews rejected the paper and the authors gave up rebuttal, I choose to reject the paper. <sep>	B-decision

2020-1135	This paper proposes a new model, the Routing Transformer, which endows self-attention with a sparse routing module based on online k-means while reducing the overall complexity of attention from O(n^2) to O(n^1.5).	B-abstract
2020-1135	The model attained very good performance on WikiText-103 (in terms of perplexity) and similar performance to baselines (published numbers) in two other tasks. <sep>	I-abstract
2020-1135	Even though the problem addressed (reducing the quadratic complexity of self-attention) is extremely relevant and the proposed approach is very intuitive and interesting, the reviewers raised some concerns, notably: <sep> - How efficient is the proposed approach in practice.	B-weakness
2020-1135	Even though the theoretical complexity is reduced, more modules were introduced (eg, forced clustering, mix of local heads and clustering heads, sorting, etc.) <sep>	I-weakness
2020-1135	- Why is W_R fixed random?	I-weakness
2020-1135	Since W_R is orthogonal, it's just a random (generalized) "rotation" (performed on the word embedding space).	I-weakness
2020-1135	Does this really provide sensible "routing"? <sep>	I-weakness
2020-1135	- The experimental section can be improved to better understand the impact of the proposed method.	I-weakness
2020-1135	Adding ablations, as suggested by the reviewers, would be an important part of this work. <sep>	I-weakness
2020-1135	- Not clear why the work needs to be motivated through NMF, since the proposed method uses k-means. <sep>	I-weakness
2020-1135	Unfortunately several points raised by the reviewers (except R2) were not addressed in the author rebuttal, and therefore it is not clear if some of the raised issues are fixable in camera ready time, which prevents me from recommend this paper to be accepted. <sep>	B-decision
2020-1135	However, I *do* think the proposed approach is very interesting and has great potential, once these points are clarified.	B-strength
2020-1135	The gains obtained in WikiText-103 are promising.	I-strength
2020-1135	Therefore, I strongly encourage the authors to resubmit this paper taking into account the suggestions made by the reviewers.	B-decision

2020-1143	This paper proposes a GA-based method for optimizing the loss function a model is trained on to produce better models (in terms of final performance).	B-abstract
2020-1143	The general consensus from the reviewers is that the paper, while interesting, dedicates too much of its content to analyzing one such discovered loss (the Baikal loss), and that the experimental setting (MNIST and Cifar10) is too basic to be conclusive.	B-weakness
2020-1143	It seems this paper can be so significantly improved with some further and larger scale experiments that it would be wrong to prematurely recommend acceptance.	B-decision
2020-1143	My recommendation is that the authors consider the reviewer feedback, run the suggested further experiments, and are hopefully in the position to submit a significantly stronger version of this paper to a future conference.	I-decision

2020-1160	This paper is an empirical studies of methods to stabilize offline (ie, batch) RL methods where the dataset is available up front and not collected during learning.	B-abstract
2020-1160	This can be an important setting in eg safety critical or production systems, where learned policies should not be applied on the real system until their performance and safety is verified.	I-abstract
2020-1160	Since policies leave the area where training data is present, in such settings poor performance or divergence might result, unless divergence from the reference policy is regularized.	I-abstract
2020-1160	This paper studies various methods to perform such regularization. <sep>	I-abstract
2020-1160	The reviewers are all very happy about the thoroughness of the empirical work.	B-strength
2020-1160	The work only studies existing methods (and combination thereof), so the novelty is limited by design.	B-weakness
2020-1160	The paper was also considered well written and easy to follow.	B-strength
2020-1160	The results were very similar between the considered regularizers, which somehow limits the usefulness of the paper as practical guideline (although at least now we know that perhaps we do not need to spend a lot of time choosing the best between these).	B-weakness
2020-1160	Bigger differences were observed between "value penalties" versus "policy regularization".	I-weakness
2020-1160	This seems to correspond to theoretical observations by Neu et al (https://arxiv.org/abs/1705.07798, 2017), which is not cited in the manuscript.	I-weakness
2020-1160	Although unpublished, I think that work is highly relevant for the current manuscript, and I'd strongly recommend the authors to consider its content.	B-suggestion
2020-1160	Some minor comments about the paper are given below. <sep>	O
2020-1160	On the balance, the strong point of the paper is the empirical thoroughness and clarity, whereas novelty, significance, and theoretical analysis are weaker points.	B-weakness
2020-1160	Due to the high selectivity of ICLR, I unfortunately have to recommend rejection for this manuscript. <sep>	B-decision
2020-1160	I have some minor comments about the contents of the paper: <sep> - The manuscript contains the line:  "Under this definition, such a behavior policy b is always well-defined even if the dataset was collected by multiple, distinct behavior policies".	B-weakness
2020-1160	Wouldn't simply defining the behavior as a mixture of the underlying behavior policies (when known) work equally well? <sep>	I-weakness
2020-1160	- The paper mentions several earlier works that regularize policies update using the KL from a reference policy (or to a reference policy).	I-weakness
2020-1160	The paper of Peters is cited in this context, although there the constraint is actually on the KL divergence between state-action distributions, resulting in a different type of regularization.	I-weakness

2020-1162	This work proposes a dynamical systems model to allow the user to better control sequence generation via the latent z. Reviewers all agreed the that the proposed method is quite interesting.	B-strength
2020-1162	However, reviewers also felt that current evaluations were weak and were ultimately unconvinced by the author rebuttal.	B-rebuttal_process
2020-1162	I recommend the authors resubmit with a stronger set of experiments as suggested by Reviewers 2 and 3.	B-decision

2020-1164	The authors observe that batch normalization using the statistics computed from a *test* batch significantly improves out-of-distribution detection with generative models.	B-abstract
2020-1164	Essentially, normalizing an OOD test batch using the test batch statistics decreases the likelihood of that batch and thus improves detection of OOD examples.	I-abstract
2020-1164	The reviewers seemed concerned with this setting and they felt that it gives a significant advantage over existing methods since they typically deal with single test example.	B-weakness
2020-1164	The reviewers thus wanted empirical comparisons to methods designed for this setting, ie traditional statistical tests for comparing distributions.	B-suggestion
2020-1164	Despite some positive discussion, this paper unfortunately falls below the bar for acceptance.	B-decision
2020-1164	The authors added significant experiments and hopefully adding these and additional analysis providing some insight into how the batchnorm is helping would make for a stronger submission to a future conference.	I-decision

2020-1172	The authors propose an invertible flow-based model for molecular graph generation.	B-abstract
2020-1172	The reviewers like the idea but have several concerns: in particular, overfitting in the model, need for more experiments and missing related work.	B-weakness
2020-1172	It is important for authors to address them in a future submission	B-suggestion

2020-1173	The paper proposes a tensor-based extension to graph convolutional networks for prediction over dynamic graphs. <sep>	B-abstract
2020-1173	The proposed model is reasonable and achieves promising empirical results.	B-strength
2020-1173	After discussion, it is agreed that while the problem of handling dynamic graphs is interesting and challenging, the proposed tensor method lacks novelty, the theoretical analysis is artificial, and the empirical study does not cover enough benchmarks. <sep>	B-weakness
2020-1173	The current version of the paper is not ready for publication.	B-decision
2020-1173	Addressing the issues above could lead to a strong publication in the future.	B-suggestion

2020-1178	This paper proposes a new dimensionality reduction technique that tries to preserve the global structure of the data as measured by the relative distances between triplets.	B-abstract
2020-1178	As Reviewer 1 noted, the construction of the TriMap algorithm is fairly heuristic, making it difficult to determine how TriMap ought to behave "better" than existing dimensionality reduction approaches other than through qualitative assessment.	B-weakness
2020-1178	Here, I share Reviewer 2's concern that the qualitative behavior of TriMap is difficult to distinguish from existing methods in many of the figures. <sep>	I-weakness

2020-1179	The authors propose a notion of feature robustness, provide a straightforward decomposition of risk in terms of this robustness measure, and then provide some empirical evidence for their perspective.	B-abstract
2020-1179	Across the board, the reviewers raised issues with missing related work, which the authors then addressed.	B-rebuttal_process
2020-1179	I will point out that some things the authors say about PAC-Bayes are false.	B-weakness
2020-1179	eg, in the rebuttal the authors say that PAC-Bayes is limited to 0-1 error.	I-weakness
2020-1179	It is generally trivial to obtain bounds for bounded loss.	I-weakness
2020-1179	For unbounded loss functions, there are bounds based on, eg, sub gaussian assumptions. <sep>	I-weakness
2020-1179	Despite improvements in connections with related work, reviewers continued to find the theoretical contributions to be marginal.	B-rebuttal_process
2020-1179	Even the empirical contributions were found to be marginal.	I-rebuttal_process

2020-1242	The submission presents an approach to uncovering causal relations in an environment via interaction.	B-abstract
2020-1242	The topic is interesting and the work is timely.	B-strength
2020-1242	However, the experimental setting is quite simplistic and the approach makes strong assumptions that limit its applicability.	B-weakness
2020-1242	The reviewers are split.	O
2020-1242	R2 raised their rating from 3 to 6 following the authors' responses and revision, but R1 maintained their rating of 3 and posted a response that justifies this position.	B-rebuttal_process
2020-1242	In light of the limitations of the work, the AC recommends against accepting the submission.	B-decision

2020-1258	The authors integrate an interpolation based regularization to develop a graph neural network for semi-supervised learning.	B-abstract
2020-1258	While reviewers enjoyed the paper, and the authors have provided a thoughtful response, there were remaining questions about clarity of presentation and novelty remaining after the rebuttal period.	B-rebuttal_process
2020-1258	The authors are encouraged to continue with this work, accounting for reviewer comments in future revisions.	B-suggestion

2020-1281	This paper studies the problem of mode collapse in GANs.	B-abstract
2020-1281	The authors present new metrics to judge the model's diversity of the generated faces.	I-abstract
2020-1281	The authors present two black-box approaches to increasing the model diversity.	I-abstract
2020-1281	The benefit of using a black box approach is that the method does not require access to the weights of the model and hence it is more easily usable than white-box approaches.	I-abstract
2020-1281	However, there are significant evaluation problems and lack of theoretical and empirical motivation on why the methods proposed by the paper are good.	B-weakness
2020-1281	The reviewers have not changed their score after having read the response and there is still some gaps in evaluation which can be improved in the paper.	B-rebuttal_process
2020-1281	Thus, I'm recommending a Rejection.	B-decision

2020-1290	This paper shows a nice idea to transfer knowledge from larger sequence models to small models.	B-abstract
2020-1290	However, all the reivewers find that the contribution is too limited and the experiments are insufficient.	B-weakness
2020-1290	All the reviewers agree to reject.	B-rating_summary

2020-1307	This paper has been reviewed by three reviewers and received scores such as 3/3/6.	B-rating_summary
2020-1307	The reviewers took into account the rebuttal in their final verdict.	O
2020-1307	The major criticism concerned the somewhat ad-hoc notion of interpretability, the analysis of vanishing/exploding gradients in  TPRU is experimental lacking theory.	B-weakness
2020-1307	Finally,  all reviewers noted the paper is difficult to read and contains grammar issues etc.	I-weakness
2020-1307	which does not help.	I-weakness
2020-1307	On balance, we regret that this paper cannot be accepted to ICLR2020. <sep>	B-decision

2020-1328	The paper is rejected based on unanimous reviews.	B-decision

2020-1335	The paper presents a framework named Wasserstein-bounded GANs which generalizes WGAN.	B-abstract
2020-1335	The paper shows that WBGAN can improve stability. <sep>	I-abstract
2020-1335	The reviewers raised several questions about the method and the experiments, but these were not addressed. <sep>	B-rebuttal_process
2020-1335	I encourage the authors to revise the draft and resubmit to a different venue.	B-decision

2020-1338	The authors present a new approach to improve performance for retro-synthesis using a seq2seq model, achieving significant improvement over the baseline.	B-abstract
2020-1338	There are a number of lingering questions regarding the significance and impact of this work.	B-weakness
2020-1338	Hence, my recommendation is to reject.	B-decision

2020-1341	This paper constitutes interesting progress on an important topic; the reviewers identify certain improvements and directions for future work (see in particular the updates from AnonReviewer1), and I urge the authors to continue to develop refinements and extensions.	B-strength

2020-1347	The paper proposes a Bayesian optimization approach to creating adversarial examples.	B-abstract
2020-1347	The general idea has been in the air for some years, and over the last year especially there have been a number of approaches using BayesOpt for this purpose.	I-abstract
2020-1347	Reviewers raised concerns about differences between this approach and related work, and practical challenges in general for using BayesOpt in this domain (regarding dimensionality, etc.).	B-weakness
2020-1347	The authors provided thoughtful responses, although some of these concerns still remain.	B-rebuttal_process
2020-1347	The authors are encouraged to address all comments carefully in future revisions, which a sufficiently substantial that the paper would benefit from additional review. <sep>	B-suggestion

2020-1350	The consensus of reviewers is that this paper is not acceptable in present form, and the AC concurs.	B-decision

2020-1381	The paper investigates quantization for speeding up RL.	B-abstract
2020-1381	While the reviewers agree that the idea is a good one (it should definitely help), they also have a number of concerns about the paper and presentation.	B-weakness
2020-1381	In particular, the reviewers feel that the authors should have provided more insight into the challenges of quantization in RL and the tradeoffs involved.	I-weakness
2020-1381	After having read the rebuttals, the reviewers believe that the authors are on the right track, but that the paper is still not ready for publication.	B-rating_summary
2020-1381	If the authors take the reviewer comments and concerns seriously and update their paper accordingly, the reviewers believe that this could eventually result in a strong paper.	B-suggestion

2020-1387	This paper introduces a model that learns a slot-based representation and its transition model to predict the representation changes over time.	B-abstract
2020-1387	While all the reviewers agree that this paper is focusing on an important problem, they expressed multiple concerns regarding the novelty of the approach as well as lacking experiments.	B-weakness
2020-1387	It certainly is missing multiple important relevant works, thereby overclaiming at a few places.	I-weakness
2020-1387	The authors provided a short general response to compare their approach with some of the previous works and conduct stronger experiments for a future submission.	B-rebuttal_process
2020-1387	We believe this paper is not at the stage to be published at this point.	B-decision

2020-1388	The paper describes an approach for learning context dependent entity representations that encodes fine-grained entity types.	B-abstract
2020-1388	The paper includes some good empirical results and observations, but the proposed approach is very simple but lacks technical novelty needed to top ML conference; the clarify of the presentation can also be improved.	B-weakness

2020-1395	This paper studies the effect of training image classifier with masked images to exclude distraction regions in the image and avoid formation of spurious correlation between them and predicted labels.	B-abstract
2020-1395	The paper proposes actdiff regularizer and demonstrates that it prevents such overfitting phenomenon on synthetic data.	I-abstract
2020-1395	However, there was no success on real data.	I-abstract
2020-1395	This is important as it shows that the improvement reported in some saliency-map based approaches in the literature may be due to other regularization effects such as cutout. <sep>	I-abstract
2020-1395	This was a unique submission in my batch, as it embraces its negative results.	O
2020-1395	Among our internal discussions, all reviewers that and we all believe that negative results are important and should be encouraged.	B-strength
2020-1395	However, in order for the negative results to be sufficiently insightful for the entire community, they need to be examined under well-organized experiments.	B-weakness
2020-1395	This is the aspect that the reviewers think the paper needs to improve on.	O
2020-1395	In particular, R2 believes the paper could consider a larger set of possible regularizations as well as a broader range of  applications.	B-suggestion
2020-1395	The insights in such setting may then lead to solid insights on why the current approaches are not very helpful, and in which direction the follow-up researches should focus on.	I-suggestion

2020-1419	The paper proposes an approach for unsupervised learning of keypoint landmarks from images and videos by decomposing them into the foreground and static background.	B-abstract
2020-1419	The technical approach builds upon related prior works such as Lorenz et al 2019 and Jakab et al 2018 by extending them with foreground/background separation.	I-abstract
2020-1419	The proposed method works well for static background achieving strong pose prediction results.	B-strength
2020-1419	The weaknesses of the paper are that (1) the proposed method is a fairly reasonable but incremental extension of existing techniques; (2) it relies on a strong assumption on the property of static backgrounds; (3) video prediction results are of limited significance and scope.	B-weakness
2020-1419	In particular, the proposed method may work for simple data like KTH but is very limited for modeling videos as it is not well-suited to handle moving backgrounds, interactions between objects (eg, robot arm in the foreground and objects in the background), and stochasticity.	I-weakness

2020-1420	The general consensus amongst the reviewers is that this paper is not quite ready for publication, and needs to dig a little deeper in some areas.	B-rating_summary
2020-1420	Some reviewers thought the contributions are unclear, or unsupported.	B-weakness
2020-1420	I hope these reviews will help you as you work towards finding a home for this work.	O

2020-1421	In this work, the authors develop a method for providing frequentist confidence intervals for a range of deep learning models with coverage guarantees.	B-abstract
2020-1421	While deep learning models are being used pervasively, providing reasonable uncertainty estimates from these models remains challenging and an important open problem.	I-abstract
2020-1421	Here, the authors argue that frequentist statistics can provide confidence intervals along with rigorous guarantees on their quality.	I-abstract
2020-1421	They develop a jack-knife based procedure for deep learning.	I-abstract
2020-1421	The reviews for this paper were all borderline, with two weak accepts and two weak rejects (one reviewer was added to provide an additional viewpoint).	B-rating_summary
2020-1421	The reviewers all thought that the proposed methodology seemed sensible and well motivated.	B-strength
2020-1421	Among the cited issues, major topics of discussion were the close relation to related work (some of which is very recent, Giordano et al) and that the reviewers felt the baselines were too weak (or weakly tuned).	B-weakness
2020-1421	The reviewers ultimately did not seem convinced enough by the author rebuttal to raise their scores during discussion and there was no reviewer really willing to champion the paper for acceptance.	B-rebuttal_process
2020-1421	Unfortunately, this paper falls below the bar for acceptance.	B-decision
2020-1421	It seems clear that there is compelling work here and addressing the reviewer comments (relation to related work, ie Robbins, Giordano and stronger baselines) would make the paper much stronger for a future submission.	B-suggestion

2020-1431	The authors address the problem of training an NMT model on a really massive parallel data set of 40 billion Chinese-English sentence pairs, an order of magnitude bigger than other cz-en experiments.	B-abstract
2020-1431	To address noise and training time problems they propose pretraining + a couple of different ways of creating a fine-tuning data set.	I-abstract
2020-1431	Two of the reviewers assert that the technical contribution is thin, and the results are SOTA but not really as good as you might hope with this amount of data.	B-weakness
2020-1431	This combined with the fact that the data set is not released, makes me think that this paper is not a good fit with ICLR and would more appropriate for an application focussed conference.	B-decision
2020-1431	The authors engaged strongly with the reviewers, adding more backtranslation results.	B-rebuttal_process
2020-1431	The reviewers took their responses into account but did not change their scores.	I-rebuttal_process

2020-1435	This paper builds a connection between MixUp and adversarial training.	B-abstract
2020-1435	It introduces untied MixUp (UMixUp), which generalizes the methods of MixUp.	I-abstract
2020-1435	Then, it also shows that DAT and UMixUp use the same method of MixUp for generating samples but use different label mixing ratios.	I-abstract
2020-1435	Though it has some valuable theoretical contributions, I agree with the reviewers that it's important to include results on adversarial robustness, where both adversarial training and MixUp are playing an important role.	B-strength

2020-1436	The paper proposes to augment the conditional GAN discriminator with an attention mechanism, with the aim to  help the generator, in the context of image to image translation.	B-abstract
2020-1436	The reviewers raise several issues in their reviews.	O
2020-1436	One theoretical concern has to do with how the training of the attention mechanism (which seems to be collaborative) would interact with the minimax, zero-sum nature of a GAN objective; another with the discrepancy in how the attention map is used during training and testing.	B-weakness
2020-1436	The experimental results were not significant enough, and the reviewers also recommend additional experiment results to clearly demonstrate the benefit of the method.	I-weakness

2020-1440	This paper proposes to overcome the issue of inconsistent availability of longitundinal data via the combination of leveraging principal components analysis and locality preserving projections.	B-abstract
2020-1440	All three reviewers express significant reservations regarding the technical writing in the paper.	B-weakness
2020-1440	As it stands, this paper is not ready for publication. <sep>	B-decision

2020-1441	This paper presents to integrate the codes based on multiple hashing functions with Transformer networks to reduce vocabulary sizes in input and output spaces.	B-abstract
2020-1441	Compared to non-hashed models, it enables training more complex and powerful models with the same number of overall parameters, thus leads to better performance. <sep>	I-abstract
2020-1441	Although the technical contribution is limited considering hash-based approach itself is rather well-known and straightforward, all reviewers agree that some findings in the experiments are interesting.	B-strength
2020-1441	On the cons side, two reviewers were concerned about unclear presentation regarding the details of the method.	B-weakness
2020-1441	More importantly, the proposed method is only evaluated on non-standard tasks without comparison to other previous methods.	I-weakness
2020-1441	Considering that the main contribution of the paper is in empirical side, I agree it is necessary to evaluate the method on more standard benchmarking tasks in NLP where there should be many other state-of-the-art methods of model compression.	B-suggestion
2020-1441	For these reasons, I'd like to recommend rejection.	B-decision

2020-1442	The paper introduces a method for removing what they call representation error and apply the method to super resolution and compressive sensing. <sep>	B-abstract
2020-1442	The reviewers have provided constructive feedback.	O
2020-1442	The reviewers like aspects of the paper but are also concerned with various shortcomings.	O
2020-1442	The consensus is that the paper is not ready for publication as it stands. <sep>	B-rating_summary
2020-1442	Rejection is therefore recommended with strong encouragement to keep working on the method and submit elsewhere.	B-decision

2020-1445	The paper presents an efficient approach to computer saliency measures by exploiting saliency map order equivalence (SMOE), and visualization of individual layer contribution by a layer ordered visualization of information. <sep>	B-abstract
2020-1445	The authors did a good job at addressing most issues raised in the reviews.	B-rebuttal_process
2020-1445	In the end, two major concerns remained not fully addressed: one is the motivation of efficiency, and the other is how much better SMOE is compared with existing statistics.	I-rebuttal_process
2020-1445	I think these two issue also determines how significance the work is. <sep>	I-rebuttal_process
2020-1445	After discussion, we agree that while the revised draft pans out to be a much more improved one, the work itself is nothing groundbreaking.	I-rebuttal_process
2020-1445	Given many other excellent papers on related topics, the paper cannot make the cut for ICLR.	B-decision

2020-1455	Thank you very much for the detailed feedback to the reviewers, which helped us better understand your paper. <sep>	O
2020-1455	Thanks also for revising the manuscript significantly; many parts were indeed revised. <sep>	O
2020-1455	However, due to the major revision, we find more points to be further discussed, which requires another round of reviews/rebuttals. <sep>	B-rebuttal_process
2020-1455	For this reason, we decided not to accept this paper. <sep>	B-decision
2020-1455	We hope that the reviewers' comments are useful for improving the paper for potential future publication. <sep>	O

2020-1477	Thanks for an interesting discussion.	O
2020-1477	The paper introduces a sound question generation technique for QA.	B-abstract
2020-1477	Reviewers are moderately positive, with low confidence.	B-rating_summary
2020-1477	Some issues remain unresolved, though: While the UniLM comparison is currently not apples-to-apples, for example, nothing prevents the authors from using their method to pretrain UniLM.	B-rebuttal_process
2020-1477	Currently, QA results are low-ish, and it is hard to accept a paper based solely on BLEU scores (questionable metric) for question generation (the task is but a means to an end).	I-rebuttal_process
2020-1477	Moreover, the authors do not really discuss how their method relates to previous work (see Review 2 and the related work cited there; there's more, eg, [0]).	I-rebuttal_process
2020-1477	I also find it a little problematic that the paper completely ignores all work prior to 2017: The NLP community started organizing workshops on question generation in 2010. [1]	I-rebuttal_process

2020-1480	The authors propose an agent that can act in an RL environment to verify hypotheses about it, using hypotheses formulated as triplets of pre-condition, action sequence, and post-condition variables.	B-abstract
2020-1480	Training then proceeds in multiple stages, including a pretraining phase using a reward function that encourages the agent to learn the hypothesis triplets. <sep>	I-abstract
2020-1480	Strengths: Reviewers generally agreed it's an important problem and interesting approach <sep>	B-strength
2020-1480	Weaknesses: There were some points of convergence among reviewer comments: lack of connection to existing literature (ie to causal reasoning and POMDPs), and concerns about the robustness of the results (which were only reporting the max seeds).	B-weakness
2020-1480	Two reviewers also found the use of natural language to unnecessarily complicate their setup.	I-weakness
2020-1480	Overall, clarity seemed to be an issue.	I-weakness
2020-1480	Other comments concerned lack of comparisons, analyses, and suggestions for alternate methods of rewarding the agent (to improve understandability). <sep>	I-weakness
2020-1480	The authors deserve credit for their responsiveness to reviewer comments and for the considerable amount of additional work done in the rebuttal period.	B-rebuttal_process
2020-1480	However, these efforts ultimately didn't satisfy the reviewers enough to change their scores.	I-rebuttal_process
2020-1480	Although I find that the additional experiments and revisions have significantly strengthened the paper, I don't believe it's currently ready for publication at ICLR.	B-decision
2020-1480	I urge the authors to focus on clearly presenting and integrating these new results in a future submission, which I look forward to. <sep>	B-suggestion

2020-1491	The paper proposes learning a latent embedding for image manipulation for PixelCNN by using Fisher scores projected to a low-dimensional space. <sep>	B-abstract
2020-1491	The reviewers have several concerns about this paper: <sep> * Novelty <sep> * Random projection doesn't learn useful representation <sep>	B-weakness
2020-1491	* Weak evaluations <sep>	I-weakness
2020-1491	Since two expert reviewers are negative about this paper, I cannot recommend acceptance at this stage. <sep>	B-decision

2020-1522	This paper proposes an approach to handle the problem of unsmoothness while modeling spatio-temporal urban data.	B-abstract
2020-1522	However all reviewers have pointed major issues with the presentation of the work, and whether the method's complexity is justified.	B-weakness

2020-1531	This paper proposes an outlier detection method that maps outliers to low probability regions of the latent space.	B-abstract
2020-1531	The novelty is in proposing a weighted reconstruction error penalizing the mapping of outliers into high probability regions.	I-abstract
2020-1531	The reviewers find the idea promising. <sep>	B-strength
2020-1531	They have also raised several questions.	O
2020-1531	It seems the questions are at least partially addressed in the rebuttal, and as a result one of our expert reviewers (R5) has increased their score from WR to WA.	B-rebuttal_process
2020-1531	But since we did not have a champion for this paper and its overall score is not high enough, I can only recommend a reject at this stage.	B-decision

2020-1533	The paper proposes metrics for comparing explainability metrics. <sep>	B-abstract
2020-1533	Both reviewers and authors have engaged in a thorough discussion of the paper and feedback.	B-rebuttal_process
2020-1533	The reviewers, although appreciating aspects of the paper, all see major issues with the paper. <sep>	O
2020-1533	All reviewers recommend reject.	B-rating_summary

2020-1541	The reviewers generally reached a consensus that the work is not quite ready for acceptance in its current form.	B-rating_summary
2020-1541	The central concerns were about the potentially limited novelty of the method, and the fact that it was not quite clear how good the annotations needed to be (or how robust the method would be to imperfect annotations).	B-weakness
2020-1541	This, combined with an evaluation scenario that is non-standard and requires some guesswork to understand its difficulty, leaves one with the impression that it is not quite clear from the experiments whether the method really works well.	I-weakness
2020-1541	I would recommend for the authors to improve the evaluation in the next submission.	B-suggestion

2020-1547	The paper proposed the use of a lossy transform coding approach to to reduce the memory bandwidth brought by the storage of intermediate activations.	B-abstract
2020-1547	It has shown the proposed method can bring good memory usage while maintaining the the accuracy. <sep>	I-abstract
2020-1547	The main concern on this paper is the limited novelty.	B-weakness
2020-1547	The lossy transform coding is borrowed from other domains and only the use of it on CNN intermediate activation is new, which seems insufficient.	I-weakness

2020-1583	The authors extend the framework of randomized smoothing to handle non-Gaussian smoothing distribution and use this to show that they can construct smoothed models that perform well against l2 and linf adversarial attacks.	B-abstract
2020-1583	They show that the resulting framework can obtain state-of-the-art certified robustness results improving upon prior work. <sep>	I-abstract
2020-1583	While the paper contains several interesting ideas, the reviewers were concerned about several technical flaws and omissions from the paper: <sep> 1) A theorem on strong duality was incorrect in the initial version of the paper, though this was fixed in the rebuttal.	B-weakness
2020-1583	However, the reasoning of the authors on the "fundamental trade-off" is specific to the particular framework they consider, and is not really a fundamental trade-off. <sep>	I-weakness
2020-1583	2) The justification for the new family of distributions constructed by the author is not very clear and the experiments only show marginal improvements over prior work.	I-weakness
2020-1583	Thus, the significance of this contribution is not clear. <sep>	I-weakness
2020-1583	Some of the issues were clarified during the rebuttal, but the reviewers remained unconvinced about the above points. <sep>	B-rebuttal_process
2020-1583	Thus, the paper cannot be accepted in its current form. <sep>	B-decision

2020-1587	This manuscript investigates and characterizes the tradeoff between fairness and accuracy in neural network models.	B-abstract
2020-1587	The primary empirical contribution is to investigate this tradeoff for a variety of datasets. <sep>	I-abstract
2020-1587	The reviewers and AC agree that the problem studied is timely and interesting.	B-strength
2020-1587	However, this manuscript also received quite divergent reviews, resulting from differences in opinion about the novelty of the results.	B-weakness
2020-1587	IN particular, it is not clear that the idea of a fairness/performance tradeoff is a new one.	I-weakness
2020-1587	In reviews and discussion, the reviewers also noted issues with clarity of the presentation.	I-weakness
2020-1587	In the opinion of the AC, the manuscript is not appropriate for publication in its current state.	B-decision

2020-1595	The paper contains interesting ideas for giving simple explanations to a NN; however, the reviewers do not feel the contribution is sufficiently novel to merit acceptance.	B-rating_summary

2020-1602	The reviewers agree that this is an interesting paper but it required major modifications.	B-weakness
2020-1602	After rebuttal, thee paper is much improved but unfortunately not above the bar yet.	B-decision
2020-1602	We encourage the authors to iterate on this work again.	B-suggestion

2020-1610	This paper proposes a sensor placement strategy based on maximising the information gain.	B-abstract
2020-1610	Instead of using Gaussian process, the authors apply neural nets as function approximators.	I-abstract
2020-1610	A limited empirical evaluation is performed to assess the performance of the proposed strategy. <sep>	I-abstract
2020-1610	The reviewers have raised several major issues, including the lack of novelty, clarity, and missing critical details in the exposition.	B-weakness
2020-1610	The authors didn't address any of the raised concerns in the rebuttal.	B-rebuttal_process
2020-1610	I will hence recommend rejection of this paper.	B-decision

2020-1611	The paper proposes to study what information is encoded in different layers of StyleGAN.	B-abstract
2020-1611	The authors do so by training classifiers for different layers of latent codes and investigating whether changing the latent code changes the generated output in the expected fashion. <sep>	I-abstract
2020-1611	The paper received borderline reviews with two weak accepts and one weak reject.	B-rating_summary
2020-1611	Initially, the reviewers were more negative (with one reject, one weak reject, and one weak accept).	B-rebuttal_process
2020-1611	After the rebuttal, the authors addressed most of the reviewer questions/concerns. <sep>	I-rebuttal_process
2020-1611	Overall, the reviewers thought the results were interesting and appreciated the care the authors took in their investigations.	B-strength
2020-1611	The main concern of the reviewers is that the analysis is limited to only StyleGAN.	B-weakness
2020-1611	It would be more interesting and informative if the authors applied their methodology to different GANs.	B-suggestion
2020-1611	Then they can analyze whether the methodology and findings holds for other types of GANs as well.	I-suggestion
2020-1611	R1 notes that given the wide interest in StyleGAN-like models, the work maybe of interest to the community despite the limited investigation.	B-strength
2020-1611	The reviewers also point out the writing can be improved to be more precise. <sep>	B-weakness
2020-1611	The AC agrees that the paper is mostly well written and well presented.	B-strength
2020-1611	However, there are limitations in what is achieved in the paper and it would be of limited interest to the community.	B-weakness
2020-1611	The AC recommends that the authors consider improving their work, potentially broadening their investigation to other GAN architectures, and resubmit to an appropriate venue.	B-decision

2020-1623	The paper improves the previous method for detecting out-of-distribution  (OOD) samples. <sep>	B-abstract
2020-1623	Some theoretical analysis/motivation is interesting as pointed out by a reviewer.	B-strength
2020-1623	I think the paper is well written in overall and has some potential. <sep>	I-strength
2020-1623	However, as all reviewers pointed out, I think experimental results are quite below the borderline to be accepted (considering the ICLR audience), ie, the authors should consider non-MNIST-like and more realistic datasets.	B-decision
2020-1623	This indicates the limitation on the scalability of the proposed method. <sep>	B-weakness
2020-1623	Hence, I recommend rejection.	B-decision

2020-1626	Apologies for only receiving two reviews.	O
2020-1626	R2 gave a WR and R3 gave an A.	B-rating_summary
2020-1626	Given the lack of 3rd review and split nature of the scores, the AC has closely scrutinized the paper/reviews/comments/rebuttal.	O
2020-1626	Thoughts: <sep> - Paper is on interesting topic. <sep>	B-strength
2020-1626	- AC agrees with R2's concern about the evaluation not using more complex environments like Mujoco.	B-weakness
2020-1626	Without evaluation on a standard benchmark, it is difficult to know objectively if the approach works. <sep>	I-weakness
2020-1626	- AC agrees with authors that the DISTRAL approach forms a strong baseline. <sep>	B-strength
2020-1626	- Nevertheless, the experiments aren't super compelling either. <sep>	B-weakness
2020-1626	- AC has some concerns about scaling issues wrt model size & #tasks. <sep>	I-weakness
2020-1626	The paper is very borderline, but the AC sides with R2's concerns and unfortunately feels the paper cannot be accepted without a stronger evaluation.	B-decision
2020-1626	With this, it would make a compelling paper.	O

2020-1631	The paper introduces additional layers on top BERT type models for disentangling of semantic and positional information.	B-abstract
2020-1631	The paper demonstrates (small) performance gains in transfer learning compared to pure BERT baseline. <sep>	I-abstract
2020-1631	Both reviewers and authors have engaged in a constructive discussion of the merits of the proposed method.	B-rebuttal_process
2020-1631	Although the reviewers appreciate the ideas and parts of the paper the consensus among the reviewers is that the evaluation of the method is not clearcut enough to warrant publication. <sep>	B-rating_summary
2020-1631	Rejection is therefore recommended.	B-decision
2020-1631	Given the good ideas presented in the paper and the promising results the authors are encouraged to take the feedback into account and submit to the next ML conference.	I-decision

2020-1637	This is an observational work with experiments for comparing iterative pruning methods. <sep>	B-abstract
2020-1637	I agree with the main concerns of all reviewers: <sep> (a) Experimental setups are of too small-scale or with easy datasets, so hard to believe they would generalize for other settings, eg, large-scale residual networks.	B-weakness
2020-1637	This aspect is very important as this is an observational paper. <sep>	B-suggestion
2020-1637	(b) The main take-home contribution/message is weak considering the high-standard of ICLR. <sep>	B-weakness
2020-1637	Hence, I recommend rejection. <sep>	B-decision
2020-1637	I would encourage the authors to consider the above concerns as it could yield a valuable contribution.	B-suggestion

2020-1660	In this work, the authors focus on the high-dimensional regime in which both the dataset size and the number of features tend to infinity.	B-abstract
2020-1660	They analyze the performance of a simple regression model trained on the random features and revealed several interesting and important observations. <sep>	I-abstract
2020-1660	Unfortunately, the reviewers could not reach a consensus as to whether this paper had sufficient novelty to merit acceptance at this time.	B-rating_summary
2020-1660	Incorporating their feedback would move the paper closer towards the acceptance threshold.	B-suggestion

2020-1675	The paper proofs that reinforcement learning (using two different algorithms) converge to iterative dominance solutions for a class of multi-player games (dominance solvable games). <sep>	B-abstract
2020-1675	There was a lively discussion around the paper.	O
2020-1675	However, two of the reviewers remain unconvinced of the novelty of the approach,  pointing to [1] and [2], with [1] only pertaining to supermodular games.	B-weakness
2020-1675	The exact contribution over such existing results is currently not addressed in the manuscript.	I-weakness
2020-1675	There were also concerns about the scaling and applicability of the results, as dominance solvable games are limited. <sep>	I-weakness
2020-1675	[1] http://www.parisschoolofeconomics.eu/docs/guesnerie-roger/milgromroberts90. pdf <sep>	O
2020-1675	[2] Friedman, James W., and Claudio Mezzetti.	O
2020-1675	"Learning in games by random sampling."	O
2020-1675	Journal of Economic Theory 98.1 (2001): 55-84.	O

2020-1686	All reviewers rated this submission as a weak reject and there was no author response. <sep>	B-rating_summary
2020-1686	The AC recommends rejection.	B-decision

2020-1688	This paper proposes  architectural modifications to transformers, which are promising for sequential tasks requiring memory but can be unstable to optimize, and applies the resulting method to the RL setting, evaluated in the DMLab-30 benchmark. <sep>	B-abstract
2020-1688	While I thought the approach was interesting and the results promising, the reviewers unanimously felt that the experimental evaluation could be more thorough, and were concerned with the motivation behind of some of the proposed changes. <sep>	B-weakness

2020-1698	The paper proposed the use of dynamic convolutional kernels as a way to reduce inference computation cost, which is a linear combination of static kernels and fused after training for inference to reduce computation cost.	B-abstract
2020-1698	The authors evaluated the proposed methods on a variety models and shown good FLOPS reduction while maintaining accuracy. <sep>	I-abstract
2020-1698	The main concern for this paper is the limited novelty.	B-weakness
2020-1698	There have been many works use dynamic convolutions as pointed out by all the reviewers.	I-weakness
2020-1698	The most similar ones are SENet and soft conditional computation.	I-weakness
2020-1698	Although the authors claim that soft conditional computation "focus on using more parameters to make models to be more expressive while we focus on reducing redundant calculations", the methods are pretty the same and moreover in the abstract of soft conditional computation they have "CondConv improves the performance and inference cost trade-off".	I-weakness

2020-1720	The authors take inspiration from regulatory fit theory and propose a new parameter for policy gradient algorithms in RL that can manage the "regulatory focus" of an agent.	B-abstract
2020-1720	They hypothesize that this can affect performance in a problem-specific way, especially when trading off between broad exploration and risk.	I-abstract
2020-1720	The reviewers expressed concerns about the usefulness of the proposed algorithm in practice and a lack of thorough empirical comparisons or theoretical results.	B-weakness
2020-1720	Unfortunately, the authors did not provide a rebuttal, so no further discussion of these issues was possible; thus, I recommend to reject.	B-decision

2020-1725	This paper provides and analyzes an interesting approach to "de-biasing" a predictor from its training set.	B-abstract
2020-1725	The work is valuable, however unfortunately just below the borderline for this year.	B-decision
2020-1725	I urge the authors to continue their investigations, for instance further addressing the reviewer comments below (some of which are marked as coming after the end of the feedback period).	B-suggestion

2020-1730	The paper addresses the setting of learning with rejection while incorporating the ideas from learning with adversarial examples to tackle adversarial attacks.	B-abstract
2020-1730	While the reviewers acknowledged the importance to study learning with rejection in this setting, they raised several concerns: (1) lack of technical contribution -- see R1's and R2's related references, see R3's suggestion on designing c(x); (2) insufficient empirical evidence -- see R3's comment about the sensitivity experiment on the strength of the attack, see R1's suggestion to compare with a baseline that learns the rejection function such as SelectiveNet;  (3) clarity of presentation -- see R2's suggestions how to improve clarity. <sep>	B-weakness
2020-1730	Among these, (3) did not have a substantial impact on the decision, but would be helpful to address in a subsequent revision.	O
2020-1730	However, (1) and (2) make it very difficult to assess the benefits of the proposed approach, and were viewed by AC as critical issues. <sep>	B-weakness
2020-1730	AC can confirm that all three reviewers have read the author responses and have revised the final ratings.	O
2020-1730	AC suggests, in its current state the manuscript is not ready for a publication.	B-decision
2020-1730	We hope the reviews are useful for improving and revising the paper. <sep>	O

2020-1759	The paper proposes a technique for incorporating prior knowledge as relations between training instances. <sep>	B-abstract
2020-1759	The reviewers had a mixed set of concerns, with one common one being an insufficient comparison with / discussion of related work.	B-weakness
2020-1759	Some reviewers also found the clarity lacking, but were satisfied with the revision.	B-rebuttal_process
2020-1759	One reviewer found the claim of the approach being general but only tested and valid for the VQA dataset problematic. <sep>	I-rebuttal_process
2020-1759	Following the discussion, I recommend rejection at this time, but encourage the authors to take the feedback into account and resubmit to another venue.	B-decision

2020-1772	This paper proposes to represent the distribution wrt which neural architecture search (NAS) samples architectures through a variational autoencoder, rather than through a fully factorized distribution (as previous work did). <sep>	B-abstract
2020-1772	In the discussion, a few things improved (causing one reviewer to increase his/her score from 1 to 3), but it became clear that the empirical evaluation has issues, with a different search space being used for the method than for the baselines.	B-rebuttal_process
2020-1772	There was unanimous agreement for rejection.	B-rating_summary
2020-1772	I agree with this judgement and thus recommend rejection.	B-decision

2020-1775	This paper addresses the problem of exploration in challenging RL environments using self-imitation learning.	B-abstract
2020-1775	The idea behind the proposed approach is for the agent to imitate a diverse set of its own past trajectories.	I-abstract
2020-1775	To achieve this, the authors introduce a policy conditioned on trajectories.	I-abstract
2020-1775	The proposed approach is evaluated on various domains including Atari Montezuma's Revenge and MuJoCo. <sep>	I-abstract
2020-1775	Given that the evaluation is purely empirical, the major concern is in the design of experiments.	B-weakness
2020-1775	The amount of stochasticity induced by the random initial state alone does not lead to convincing results regarding the performance of the proposed approach compared with baselines (eg Go-Explore).	I-weakness
2020-1775	With such simple stochasticity, it is not clear why one could not use a model to recover from it and then rely on an existing technique like Go-Explore.	I-weakness
2020-1775	Although this paper tackles an important problem (hard-exploration RL tasks), all reviewers agreed that this limitation is crucial and I therefore recommend to reject this paper.	B-decision

2020-1782	The authors show that in a reinforcement learning setting, partial models can be causally incorrect, leading to improper evaluation of policies that are different from those used to collect the data for the model.	B-abstract
2020-1782	They then propose a backdoor correction to this problem that allows the model to generalize properly by separating the effects of the stochasticity of the environment and the policy.	I-abstract
2020-1782	The reviewers had substantial concerns about both issues of clarity and the clear, but largely undiscussed, connection to off-policy policy evaluation (OPPE). <sep>	B-weakness
2020-1782	In response, the authors made a significant number of changes for the sake of clarity, as well as further explained the differences between their approach and the OPPE setting.	B-rebuttal_process
2020-1782	First, OPPE is not typically model-based.	I-rebuttal_process
2020-1782	Second, while an importance sampling solution would be technically possible, by re-training the model based on importance-weighted experiences, this would need to be done for every evaluation policy considered, whereas the authors' solution uses a fundamentally different approach of causal reasoning so that a causally correct model can be learned once and work for all policies. <sep>	I-rebuttal_process
2020-1782	After much discussion, the reviewers could not come to a consensus about the validity of these arguments.	I-rebuttal_process
2020-1782	Futhermore, there were lingering questions about writing clarity.	I-rebuttal_process
2020-1782	Thus, in the future, it appears the paper could be significantly improved if the authors cite more of the off policy evaluation literature, in addition to their added textual clairifications of the relation of their work to that body of work.	B-suggestion
2020-1782	Overall, my recommendation at this time is to reject this paper.	B-decision

2020-1804	This manuscript describes a continual learning approach where individual instances consist of sequences, such as language modeling.	B-abstract
2020-1804	The paper consists of a definition of a problem setting, tasks in that problem setting, baselines (not based on existing continual learning approaches, which the authors argue is to highlight the need for such techniques, but with which the reviewers took issue), and a novel architecture. <sep>	I-abstract
2020-1804	Reviews focused on the gravity of the contribution.	O
2020-1804	R1 and R2, in particular, argued that the paper is written as though the problem/benchmark definition is the main contribution.	B-weakness
2020-1804	R2 mentions that in spite of this, the methods section jumps directly into the candidate architecture.	I-weakness
2020-1804	As mentioned above, several reviewers also took issue with the fact that existing CL techniques are not employed as baselines.	I-weakness
2020-1804	The authors engaged with reviewers and promised updates, but did not take the opportunity to update their paper. <sep>	B-rebuttal_process
2020-1804	As many of the reviewers' comments remain unaddressed and the authors' updates did not materialize, I recommend rejection, and encourage the authors to incorporate the feedback they have received in a future submission.	B-decision

2020-1810	This paper proposes a new black-box adversarial attack based on tiling and evolution strategies.	B-abstract
2020-1810	While the experimental results look promising, the main concern of the reviewers is the novelty of the proposed algorithm, and many things need to be improved in terms of clarity and experiments.	B-weakness
2020-1810	The paper does not gather sufficient support from the reviewers even after author response.	B-rebuttal_process
2020-1810	I encourage the authors to improve this paper and resubmit to future conference.	B-decision

2020-1813	The author propose a method called global momentum compression for sparse communication setting, and provided some theoretical results on the convergence rate.	B-abstract
2020-1813	The convergence result is interesting, but the underlying assumptions used in the analysis appear very strong.	B-weakness
2020-1813	Moreover, the proposed algorithm has limited novelty as it is only a minor modification.	I-weakness
2020-1813	Another main concern is that the proposed algorithm shows little performance improvement in the experiments.	I-weakness
2020-1813	Moreover, more related algorithms should be included in the experimental comparison.	I-weakness

2020-1819	This paper investigates the practical and theoretical consequences of speeding up training using incremental gradient methods (such as stochastic descent) by calculating the gradients with respect to a specifically chosen sparse subset of data. <sep>	B-abstract
2020-1819	The reviewers were quite split on the paper. <sep>	O
2020-1819	On the one hand, there was a general excitement about the direction of the paper.	B-strength
2020-1819	The idea of speeding up gradient descent is of course hugely relevant to the current machine learning landscape.	I-strength
2020-1819	The approach was also considered novel, and the paper well-written. <sep>	I-strength
2020-1819	However, the reviewers also pointed out multiple shortcomings.	O
2020-1819	The experimental section was deemed to lack clarity and baselines.	B-weakness
2020-1819	The results on standard dataset were very different from expected, causing worry about the reliability, although this has partially been addressed in additional experiments.	I-weakness
2020-1819	The applicability to deep learning and large dataset, as well as the significance of time saved by using this method, were other worries. <sep>	I-weakness
2020-1819	Unfortunately, I have to agree with the majority of the reviewers that the idea is fascinating, but that more work is required for acceptance to ICLR.	B-decision

2020-1823	The paper proposes a generative model that jointly trains an implicit generative model and an explicit energy based model using Stein's method.	B-abstract
2020-1823	There are concerns about technical correctness of the proofs and the authors are advised to look carefully into the points raised by the reviewers.	B-weakness

2020-1831	The paper analyses the effect of different loss functions for TransE and argues that certain limitations of TransE can be mitigated by choosing more appropriate loss functions.	B-abstract
2020-1831	The submission then proposes TransComplEx to further improve results.	I-abstract
2020-1831	This paper received four reviews, with three recommending rejection, and one recommending weak acceptance.	B-rating_summary
2020-1831	A main concern was in the clarity of motivating the different models.	B-weakness
2020-1831	Another was in the relatively low performance of RotatE compared with [1], which was raised by multiple reviewers.	I-weakness
2020-1831	The authors provided extensive responses to the concerns raised by the reviewers.	B-rebuttal_process
2020-1831	However, at least the implementation of RotatE remains of concern, with the response of the authors indicating "Please note that we couldn't use exactly the same setting of RotatE due to limitations in our infrastructure."	I-rebuttal_process
2020-1831	On the balance, a majority of reviewers felt that the paper was not suitable for publication in its current form.	B-rating_summary

2020-1832	This paper proposes a new graph Hierarchy representation (HAG) which eliminates the redundancy during the aggregation stage and improves computation efficiency.	B-abstract
2020-1832	It achieves good speedup and also provide theoretical analysis.	B-strength
2020-1832	There has been several concerns from the reviewers; authors' response addressed them partially.	B-rebuttal_process
2020-1832	Despite this, due to the large number of strong papers, we cannot accept the paper at this time.	B-decision
2020-1832	We encourage the authors to further improve the work for a future version. <sep>	O

2020-1833	The paper proposes a definition of the sensitivity of the output to random perturbations of the input and its link to generalization. <sep>	B-abstract
2020-1833	While both reviewers appreciated the timeliness of this research, they were taken aback by the striking similarity with the work of Novak et al I encourage the authors to resubmit to a later conference with a lengthier analysis of the differences between the two frameworks, as they started to do in their rebuttal.	B-decision

2020-1837	This paper proposes a curriculum-based reinforcement learning approach to improve theorem proving towards longer proofs.	B-abstract
2020-1837	While the authors are tackling an important problem, and their method appears to work on the environment it was tested in, the reviewers found the experimental section too narrow and not convincing enough.	B-weakness
2020-1837	In particular, the authors are encouraged to apply their methods to more complex domains beyond Robinson arithmetic.	B-suggestion
2020-1837	It would also be helpful to get a more in depth analysis of the role of the curriculum.	I-suggestion
2020-1837	The discussion period did not lead to improvements in the reviewers' scores, hence I recommend that this paper is rejected at this time. <sep>	B-decision

2020-1855	This paper proposes EXAID, a method to detect adversarial attacks by building on the advances in explainability (particularly SHAP), where activity-map-like explanations are used to justify and validate decisions.	B-abstract
2020-1855	Though it may have some valuable ideas, the execution is not satisfying, with various issues raised in comments.	B-weakness
2020-1855	No rebuttal was provided.	B-rebuttal_process

2020-1856	This works presents a method for inferring the optimal bit allocation for quantization of weights and activations in CNNs.	B-abstract
2020-1856	The formulation is sound and the experiments are complete.	B-strength
2020-1856	However, the main concern is that the paper is very similar to a recent work by the authors, which is not cited.	B-weakness

2020-1860	This paper extends the idea of influence functions (aka the implicit function theorem) to multi-stage training pipelines, and also adds an L2 penalty to approximate the effect of training for a limited number of iterations. <sep>	B-abstract
2020-1860	I think this paper is borderline.	B-rating_summary
2020-1860	I also think that R3 had the best take and questions on this paper. <sep>	O
2020-1860	Pros: <sep> - The main idea makes sense, and could be used to understand real training pipelines better. <sep>	B-strength
2020-1860	- The experiments, while mostly small-scale, answer most of the immediate questions about this model. <sep>	I-strength
2020-1860	Cons: <sep> - The paper still isn't all that polished.	B-weakness
2020-1860	eg on page 4: "Algorithm 1 shows how to compute the influence score in (11).	I-weakness
2020-1860	The pseudocode for computing the influence function in (11) is shown in Algorithm 1" <sep>	I-weakness
2020-1860	- I wish the image dataset experiments had been done with larger images and models. <sep>	I-weakness
2020-1860	Ultimately, the straightforwardness of the extension and the relative niche applications mean that although the main idea is sound, the quality and the overall impact of this paper don't quite meet the bar.	B-decision

2020-1872	The main concern raised by the reviewers is that the paper is difficult to read and potentially unclear.	B-weakness
2020-1872	Therefore, the area chair read the paper, and also found it fairly dense and challenging to read.	I-weakness
2020-1872	While there may be important discoveries in the paper, the paper in its current form makes it too difficult to read.	I-weakness
2020-1872	Since four reviewers (including the AC) struggled to understand the paper, we believe the presentation of the paper should be improved.	I-weakness
2020-1872	In particular, the claims of the paper should be better put into context.	I-weakness

2020-1880	Since there were only two official reviews submitted, I reviewed the paper to form a third viewpoint.	O
2020-1880	I agree with reviewer 2 on the following points, which support rejection of the paper: <sep> 1) Only CIFAR is evaluated without Penn Treebank; <sep>	B-weakness
2020-1880	2) The "faster convergence" is not empirically justified by better final accuracy with same amount of search cost; and <sep>	I-weakness
2020-1880	3) The advantage of the proposed ACSA over SBMD is not clearly demonstrated in the paper. <sep>	I-weakness
2020-1880	The scores of the two official reviews are insufficient for acceptance, and an additional review did not overturn this view.	B-rebuttal_process

2020-1899	Unfortunately, the reviewers of the paper are all not certain about their review, none of them being RL experts.	O
2020-1899	Assessing the paper myselfnot being an RL expert but having experiencethe authors have addressed all points of the reviewers thoroughly. <sep>	B-rebuttal_process

2020-1907	This paper proposes to split the GNN operations into two parts and study the effects of each part.	B-abstract
2020-1907	While two reviewers are positive about this paper, the other reviewer R1 has raised some concerns.	B-rating_summary
2020-1907	During discussion, R1 responded and indicated that his/her concerns were not addressed in author rebuttal.	B-rebuttal_process
2020-1907	Overall, I feel the paper is borderline and lean towards reject.	B-decision

2020-1920	This work tackles the problem of catastrophic forgetting by using Gaussian processes to identify "memory samples" to regularize learning. <sep>	B-abstract
2020-1920	Although the approach seems promising and well-motivated, the reviewers ultimately felt that some claims, such as scalability, need stronger justifications.	B-weakness
2020-1920	These justifications could come, for example, from further experiments, including ablation studies to gain insights.	B-suggestion
2020-1920	Making the paper more convincing in this way is particularly desirable since the directions taken by this paper largely overlap with recent literature (as argued by reviewers). <sep>	B-weakness

2020-1932	This submission proposes a method to pass sanity checks on saliency methods for model explainability that were proposed in a prior work. <sep>	B-abstract
2020-1932	Pros: <sep> -The method is simple, intuitive and does indeed pass the proposed checks. <sep>	B-strength
2020-1932	Cons: <sep> -The proposed method aims to pass the sanity checks, but is not well-evaluated on whether it provides good explanations.	B-weakness
2020-1932	Passing these checks can be considered as necessary but not sufficient. <sep>	I-weakness
2020-1932	-All reviewers agreed that the evaluation could be improved and most reviewers found the evaluation insufficient. <sep>	I-weakness
2020-1932	Given the shortcomings, AC agrees with the majority recommendation to reject. <sep>	B-decision

2020-1936	This work proposes a self-supervised segmentation method: building upon Crawford and Pineau 2019, this work adds a Monte-Carlo based training strategy to explore object proposals. <sep>	B-abstract
2020-1936	Reviewers found the method interesting and clever, but shared concerns about the lack of a better comparison to Crawford and Pineau, as well as generally a lack of care in comparisons to others, which were not satisfactorily addressed by authors response. <sep>	B-weakness
2020-1936	For these reasons, we recommend rejection.	B-decision

2020-1940	This paper investigates theories related to networks sparsification, related to mode connectivity and the so-called lottery ticket hypothesis.	B-abstract
2020-1940	The paper is interesting and has merit, but on balance I find the contributions not sufficiently clear to warrant acceptance.	B-decision
2020-1940	The authors made substantial changes to the paper which are admirable and which bring it to borderline status. <sep>	B-rebuttal_process

2020-1951	This paper proposes a multiresolution spectral geometric loss called the zoomout loss to help with matrix completion, and show state-of-the-art results on several recommendation benchmarks, although experiments also show that the result improvements are not always dependent upon the geometric loss itself. <sep>	B-abstract
2020-1951	Reviewers find the idea interesting and the results promising but also have important concerns about the experiments not establishing how the approach truly works.	B-weakness
2020-1951	Authors have clarified their explanations in the revisions and provided requested experiments (eg, on the importance of the initialization size), however important reservations re.	B-rebuttal_process
2020-1951	why the approach works are still not sufficiently addressed, and would require more iterations to fulfill the potential of this paper. <sep>	I-rebuttal_process
2020-1951	Therefore, we recommend rejection.	B-decision

2020-1954	All three reviewers felt the paper should be rejected and no rebuttal was offered.	B-rating_summary
2020-1954	So the paper is rejected.	B-decision

2020-1966	This paper studies deep neural network (DNN) learning curves by leveraging recent connections of (wide) DNNs to kernel methods such as  Gaussian processes. <sep>	B-abstract
2020-1966	The bulk of the arguments contained in this paper are, thus, for the "kernel regime" rather than "the problem of non-linearity in DNNs", as one reviewer puts it. <sep>	I-abstract
2020-1966	When it comes to scoring this paper, it has been controversial.	O
2020-1966	However a lot of discussion has taken place.	O
2020-1966	On the positive side, it seems that there is a lot of novel perspectives included in this paper.	B-strength
2020-1966	On the other hand, even after the revision, it seems that this paper is still very difficult to follow for non-physicists. <sep>	B-weakness
2020-1966	Overall, it would be beneficial to perform a more careful revision of the paper such that it can be better appreciated by the targeted scientific community. <sep>	B-suggestion

2020-1970	The paper proposes a combination graph neural networks and graph generation model (GraphRNN) to model the evolution of dynamic graphs for predicting the topology of next graph given a sequence of graphs. <sep>	B-abstract
2020-1970	The problem to be addressed seems interesting, but lacks strong motivation.	B-weakness
2020-1970	Therefore it would be better if some important applications can be specified. <sep>	B-suggestion
2020-1970	The proposed approach lacks novelty.	B-weakness
2020-1970	It would be better to point out why the specific combination of two existing models is the most appropriate approach to address the task. <sep>	B-suggestion
2020-1970	The experiments are not fully convincing.	B-weakness
2020-1970	Bigger and comprehensive datasets (with the right motivating applications) should be used to test the effectiveness of the proposed model. <sep>	B-suggestion
2020-1970	In short, the current version failed to raise excitement from readers due to the reasons above.	B-weakness
2020-1970	A major revision addressing these issues could lead to a strong publication in the future.	B-suggestion

2020-1984	The paper explores an initialization scheme for the recently introduced linear memory network (LMN) (Bacciu et al, 2019) that is better than random initialization and the approach is tested on various MNIST and TIMIT data sets with positive results. <sep>	B-abstract
2020-1984	Reviewer 3 raised concerns about the breadth of experiments and novelty.	B-weakness
2020-1984	Reviewer 2 recognized that the model performs well on its MNIST baselines and had concerns about applicability to larger settings.	I-weakness
2020-1984	Reviewer 1 acknowledges a very well written paper, but again raises concerns about the thoroughness of the experiments.	I-weakness
2020-1984	The authors responded to all three reviewers, responding that the tasks were chosen to match existing work and that the approach is complementary to LSTMs to solve different tasks.	B-rebuttal_process
2020-1984	Overall the reviewers did not re-adjust their ratings. <sep>	I-rebuttal_process
2020-1984	There remains questions on scalability and generality, which makes the paper not yet ready for acceptance.	B-decision
2020-1984	We hope that the reviews support the authors further research.	O

2020-1985	This paper offers a new approach to cross-modal embodied learning that aims to overcome limited vocabulary and other issues.	B-abstract
2020-1985	Reviews are mixed.	O
2020-1985	I concur with the two reviewers who say the work is interesting but the contribution is not sufficiently clear for acceptance at this time.	B-decision

2020-1988	This paper presents a differentiable coarsening approach for graph neural network.	B-abstract
2020-1988	It provides the empirical demonstration that the proposed approach is competitive to existing pooling approaches.	I-abstract
2020-1988	However, although the paper shows an interesting observation, there are remaining novelty as well as clarity concerns.	B-weakness
2020-1988	In particular, the contribution of the proposed work over the graph kernels based on other forms of coarsening such as the early work of Shervashidze et al as well as higher-order WL (pointed out by Reviewer1) remains unclear.	I-weakness
2020-1988	We believe the paper currently lacks comparisons and discussions, and will benefit from additional rounds of future revisions. <sep>	I-weakness

2020-1992	This submission has been assessed by three reviewers and scored 3/6/1.	B-rating_summary
2020-1992	The reviewers also have not increased their scores after the rebuttal.	B-rebuttal_process
2020-1992	Two reviewers pointed to poor experimental results that do not fully support what is claimed in contributions and conclusions.	B-weakness
2020-1992	Theoretical support for the reconstruction criterion was considered weak.	I-weakness
2020-1992	Finally, the paer is pointed to be a special case of (Zhang 2019).	I-weakness
2020-1992	While the paper has some merits, all reviewers had a large number of unresolved criticism.	B-rebuttal_process
2020-1992	Thus, this paper cannot be accepted by ICLR2020. <sep>	B-decision

2020-1999	This paper tackles the interesting problem of meta-learning in problem spaces where training "tasks" are scarce.	B-abstract
2020-1999	Two criticisms that seems to shared across reviewers are that (i) it is debatable how "novel" the space of meta learning with "few" tasks is, especially since there aren't established standard for how many training tasks should be available, and (ii) the paper could use more comparisons with baseline methods and ablations to understand the contributions.	B-weakness
2020-1999	As an AC, I down-weight criticism (i) because I don't feel the paper has to be creating a new problem definition; it's acceptable to make advances within an existing space.	B-ac_disagreement
2020-1999	However, criticism (ii) seems to remain.	O
2020-1999	After conferring with reviewers it seems that the rebuttal was not strong enough to significantly alter the reviewer's opinions on this issue, and so the paper does not have enough support to justify acceptance.	B-decision
2020-1999	The paper certainly addresses interesting issues, and I look forward to seeing a revised/improved version at another venue.	I-decision

2020-2000	This work introduces a simple and effective method for ensemble distillation.	B-abstract
2020-2000	The method is a simple extension of earlier "prior networks": it differs in which, instead of fitting a single network to mimic a distribution produced by the ensemble, this work suggests to use multi-head (one head per individual ensemble member) in order to better capture the ensemble diversity.	I-abstract
2020-2000	This paper experimentally shows that multi-head architecture performs well on MNIST and CIFAR-10 (they added CIFAR-100 in the revised version) in terms of accuracy and uncertainty. <sep>	I-abstract
2020-2000	While the method is effective and the experiments on CIFAR-100 (a harder task) improved the paper, the reviewers (myself included) pointed out in the discussion phase that the limited novelty remains a major weakness.	B-rebuttal_process
2020-2000	The proposed method seems like a trivial extension of the prior work, and does not provide much additional insight.	I-rebuttal_process
2020-2000	To remedy this shortcoming, I suggest the authors provide extensive experimental supports including various datasets and ablation studies. <sep>	B-suggestion
2020-2000	Another concern mentioned in the discussion is the fact that these small improvements are in spite of the fact that the proposed method ends up using many more parameters than the baselines.	B-weakness
2020-2000	Including and comparing different model sizes in a full fledged experimental evaluation would better convey the trade-offs of the proposed approach. <sep>	B-suggestion

2020-2003	There is no author response for this paper.	B-rebuttal_process
2020-2003	The paper addresses the issue of catastrophic forgetting in continual learning.	B-abstract
2020-2003	The authors build upon the idea from [Zheng,2019], namely finding gradient updates in the space perpendicular to the input vectors of the previous tasks resulting in less forgetting, and propose an improvement, namely to use principal component analysis to enable learning new tasks without restricting their solution space as in [Zheng,2019]. <sep>	I-abstract
2020-2003	While the reviewers acknowledge the importance to study continual learning, they raised several concerns that were viewed by the AC as critical issues: (1) convincing experimental evaluation -- an analysis that clearly shows how and when the proposed method can solve the issue that [Zheng,2019] faces with (task similarity/dissimilarity scenario) would substantially strengthen the evaluation and would allow to assess the scope and contributions of this work; also see R3's detailed concerns and questions on empirical evaluation, R2's suggestion to follow the standard protocols, and R1's suggestion to use PackNet and HAT as baselines for comparison;  (2) lack of presentation clarity -- see R2's concerns how to improve, and R1's suggestions on how to better position the paper. <sep>	B-weakness
2020-2003	A general consensus among reviewers and AC suggests, in its current state the manuscript is not ready for a publication.	B-decision
2020-2003	It needs clarifications, more empirical studies and polish to achieve the desired goal. <sep>	B-suggestion

2020-2015	This paper proposed a very general idea called Atomic Compression Networks (ACNs) to construct neural networks.	B-abstract
2020-2015	The idea looks simple and effective.	B-strength
2020-2015	However, the reason why it works is not well explained.	B-weakness
2020-2015	The experiments are not sufficient enough to convince the reviewers.	I-weakness

2020-2022	The paper propose a new quantization-friendly network training algorithm called GQ (or DQ) net.	B-abstract
2020-2022	The paper is well-written, and the proposed idea is interesting.	B-strength
2020-2022	Empirical results are also good.	I-strength
2020-2022	However, the major performance improvement comes from the combination of different incremental improvements.	B-weakness
2020-2022	Some of these additional steps do seem orthogonal to the proposed idea.	I-weakness
2020-2022	Also, it is not clear how robust the method is to the various hyperparameters / schedules.	I-weakness
2020-2022	For example, it seems that some of the suggested training options are conflicting each other.	I-weakness
2020-2022	More in-depth discussions and analysis on the setting of the regularization parameter and schedule for the loss term blending parameters will be useful.	B-suggestion

2020-2034	The paper tackles the key question of achieving high prediction performances with few labels.	B-abstract
2020-2034	The proposed approach builds upon Contrastive Predictive Coding (van den Oord et al 2018).	I-abstract
2020-2034	The contribution lies in i) refining CPC along several axes including model capacity, directional predictions, patch-based augmentation; ii) showing that the refined representation learned by the called CPC.v2 supports an efficient classification in a few-label regime, and can be transferred to another dataset; iii) showing that the auxiliary losses involved in the CPC are not necessarily predictive of the eventual performance of the network. <sep>	I-abstract
2020-2034	This paper generated a hot discussion.	O
2020-2034	Reviewers were not convinced that the paper contributions are sufficiently innovative to deserve being published at ICLR.	B-rating_summary
2020-2034	Authors argued that novelty does not have to lie in equations, and that the new ideas and evidence presented are worth. <sep>	B-rebuttal_process
2020-2034	The area chair thinks that the paper raises profound questions (eg, what auxiliary losses are most conducive to learning a good representation; how to divide the computational efforts among the preliminary phase of representation learning and the later phase of classifier learning), but given the number of options and details involved, these results may support several interpretations besides the authors'. <sep>	B-weakness
2020-2034	The authors might also want to leave the claim about the generality of the CPC++ principles (eg, regarding audio) for further work - or to bring additional evidence backing up this claim. <sep>	B-suggestion
2020-2034	In conclusion, this paper contains brilliant ideas and I hope to see them published with a strengthened analysis of its components.	B-decision

2020-2040	Main content: <sep> Blind review #3 summarizes it well: <sep> This paper presents a technique for encoding the high level "style" of pieces of symbolic music.	B-abstract
2020-2040	The music is represented as a variant of the MIDI format.	I-abstract
2020-2040	The main strategy is to condition a Music Transformer architecture on this global "style embedding".	I-abstract
2020-2040	Additionally, the Music Transformer model is also conditioned on a combination of both "style" and "melody" embeddings to try and generate music "similar" to the conditioning melody but in the style of the performance embedding. <sep>	I-abstract
2020-2040	-- <sep>	O
2020-2040	Discussion: <sep> The reviewers questioned the novelty.	B-weakness
2020-2040	Blind review #2 wrote: "Overall, I think the paper presents an interesting application and parts of it are well written, however I have concerns with the technical presentation in parts of the paper and some of the methodology.	I-weakness
2020-2040	Firstly, I think the algorithmic novelty in the paper is fairly limited.	I-weakness
2020-2040	The performance conditioning vector is generated by an additional encoding transformer, compared to the Music Transformer paper (Huang et al 2019b).	I-weakness
2020-2040	However, the limited algorithmic novelty is not the main concern.	I-weakness
2020-2040	The authors also mention an internal dataset of music audio and transcriptions, which can be a major contribution to the music information retrieval (MIR) community.	I-weakness
2020-2040	However it is not clear if this dataset will be publicly released or is only for internal experiments." <sep>	I-weakness
2020-2040	However, after revision, the same reviewer has upgraded the review to a weak accept, as the authors wrote "We emphasize that our goal is to provide users with more fine-grained control over the outputs generated by a seq2seq language model.	B-rebuttal_process
2020-2040	Despite its simplicity, our method is able to learn a global representation of style for a Transformer, which to the best of our knowledge is a novel contribution for music generation.	I-rebuttal_process
2020-2040	Additionally, we can synthesize an arbitrary melody into the style of another performance, and we demonstrate the effectiveness of our results both quantitatively (metrics) and qualitatively (interpolations, samples, and user listening studies)." <sep>	I-rebuttal_process
2020-2040	-- <sep>	O
2020-2040	Recommendation and justification: <sep> This paper is borderline for the reasons above, and due to the large number of strong papers, is not accepted at this time.	B-decision
2020-2040	As one comment, this work might actually be more suitable for a more specialized conference like ISMIR, as its novel contribution is more to music applications than to fundamental machine learning approaches.	B-suggestion

2020-2054	This paper propose a method to train DNNs using 8-bit floating point numbers, by using an enhanced loss scaling method and stochastic rounding method.	B-abstract
2020-2054	However, the proposed method lacks novel and both the paper presentation and experiments need to be improved throughout.	B-weakness

2020-2068	The reviewers equivocally reject the paper, which is mostly experimental and the results of which are limited.	B-rating_summary
2020-2068	The authors do not react to the reviewers' comments.	B-rebuttal_process

2020-2076	The main contribution is a Bayesian neural net algorithm which saves computation at test time using a vector quantization approximation.	B-abstract
2020-2076	The reviewers are on the fence about the paper.	B-rating_summary
2020-2076	I find the exposition somewhat hard to follow.	B-weakness
2020-2076	In terms of evaluation, they demonstrate similar performance to various BNN architectures which require Monte Carlo sampling.	I-weakness
2020-2076	But there have been lots of BNN algorithms that don't require sampling (eg PBP, Bayesian dark knowledge, MacKay's delta approximation), so it seems important to compare to these.	I-weakness
2020-2076	I think there may be promising ideas here, but the paper needs a bit more work before it is to be published at a venue such as ICLR. <sep>	B-decision

2020-2078	This paper has been assessed by three reviewers scoring it as follows: 6, 3, 8.	B-rating_summary
2020-2078	The submission however attracted some criticism post-rebuttal from the reviewers eg, why concatenating teacher to student is better than the use l2 loss or how the choice of transf.	B-rebuttal_process
2020-2078	layers has been made (ad-hoc).	B-weakness
2020-2078	Similarly, other major criticism includes lack of proper referencing to parts of work that have been in fact developed earlier in preceding papers.	I-weakness
2020-2078	On balance, this paper falls short of the expectations of ICLR 2020, thus it cannot be accepted at this time.	B-decision
2020-2078	The authors are encouraged to work through major comments and resolve them for a future submission.	B-suggestion

2020-2084	The authors introduce a method for disentangling effects of correlated predictors in the context of high dimensional outcomes.	B-abstract
2020-2084	While the paper contains interesting ideas and has been substantially improved from its original form, the paper still does not meet the quality bar of ICLR due to its limitations in terms of limited applicability and experiments.	B-decision
2020-2084	The paper will benefit from a revision and resubmission to another venue.	I-decision

2020-2089	This paper proposes an out-of-distribution detection (OOD) method without assuming OOD in validation. <sep>	B-abstract
2020-2089	As reviewers mentioned, I think the idea is interesting and the proposed method has potential.	B-strength
2020-2089	However, I think the paper can be much improved and is not ready to publish due to the followings given reviewers' comments: <sep>	B-decision
2020-2089	(a) The prior work also has some experiments without OOD in validation, ie, use adversarial examples (AE) instead in validation.	B-weakness
2020-2089	Hence, the main motivation of this paper becomes weak unless the authors justify enough why AE is dangerous to use in validation. <sep>	I-weakness
2020-2089	(b) The performance of their replication of the prior method is far lower than reported.	I-weakness
2020-2089	I understand that sometimes it is not easy to reproduce the prior results.	I-weakness
2020-2089	In this case, one can put the numbers in the original paper.	B-suggestion
2020-2089	Or, one can provide detailed analysis why the prior method should fail in some cases. <sep>	I-suggestion
2020-2089	(c) The authors follow exactly same experimental settings in the prior works.	B-weakness
2020-2089	But, the reported score of the prior method is already very high in the settings, and the gain can be marginal.	I-weakness
2020-2089	Namely, the considered settings are more or less "easy problems".	I-weakness
2020-2089	Hence, additional harder interesting OOD settings, eg, motivated by autonomous driving, would strength the paper. <sep>	B-suggestion
2020-2089	Hence, I recommend rejection.	B-decision

2020-2107	The paper introduces a distributed algorithm for training deep nets in clusters with high-latency (ie very remote) nodes.	B-abstract
2020-2107	While the motivation and clarity are the strengths of the paper, the reviewers have some concerns regarding novelty and insufficient theoretical analysis.	B-weakness

2020-2114	This paper studies over-parameterization for unsupervised learning.	B-abstract
2020-2114	The paper does a series of empirical studies on this topic.	I-abstract
2020-2114	Among other things the authors observe that larger models can increase the number latent variables recovered when fitting larger variational inference models.	I-abstract
2020-2114	The reviewers raised some concern about the simplicity of the models studied and also lack of some theoretical justification.	B-weakness
2020-2114	One reviewer also suggests that more experiments and ablation studies on more general models will further help clarify the role over-parameterized model for latent generative models.	B-suggestion
2020-2114	I agree with the reviewers that this paper is "compelling reason for theoretical research on the interplay between overparameterization and parameter recovery in latent variable neural networks trained with gradient descent methods".	B-strength
2020-2114	I disagree with the reviewers that theoretical study is required as I think a good empirical paper with clear conjectures is as important.	B-ac_disagreement
2020-2114	I do agree with the reviewers however that for empirical paper I think the empirical studies would have to be a bit more thorough with more clear conjectures.	B-weakness
2020-2114	In summary, I think the paper is nice and raises a lot of interesting questions but can be improved with more through studies and conjectures.	I-weakness
2020-2114	I would have liked to have the paper accepted but based on the reviewer scores and other papers in my batch I can not recommend acceptance at this time.	B-decision
2020-2114	I strongly recommend the authors to revise and resubmit.	I-decision
2020-2114	I really think this is a nice paper and has a lot of potential and can have impact with appropriate revision.	O

2020-2121	This manuscript investigates the posterior collapse in variational autoencoders and seeks to provide some explanations from the phenomenon.	B-abstract
2020-2121	The primary contribution is to propose some previously understudied explanations for the posterior collapse that results from the optimization landscape of the log-likelihood portion of the ELBO. <sep>	I-abstract
2020-2121	The reviewers and AC agree that the problem studied is timely and interesting, and closely related to a variety of recent work investigating the landscape properties of variational autoencoders and other generative models.	B-strength
2020-2121	However, this manuscript also received quite divergent reviews, resulting from differences in opinion about the technical difficulty and importance of the results.	O
2020-2121	In reviews and discussion, the reviewers noted issues with clarity of the presentation and sufficient justification of the results.	B-weakness
2020-2121	There were also concerns about novelty.	I-weakness
2020-2121	In the opinion of the AC, the manuscript in its current state is borderline, and should ideally be improved in terms of clarity of the discussion, and some more investigation of the insights that result from the analysis.	I-weakness

2020-2125	The submission is a detailed and extensive examination of overfitting in vision-and-language navigation domains.	B-abstract
2020-2125	The authors evaluate several methods across multiple environments, using different splits of the environment data into training, validation-seen, and validation-unseen.	I-abstract
2020-2125	The authors also present an approach using semantic features which is shown to have little or no gap between training and validation performance. <sep>	I-abstract
2020-2125	The reviewers had mixed reviews and there was substantial discussion about the merits of the paper.	O
2020-2125	However, a significant issue was observed and confirmed with the authors, relating to tuning the semantic features and agent model on the unseen validation data.	B-weakness
2020-2125	This is an important flaw, since the other methods were not tuned in this way, and there was no 'test' performance given in the paper.	I-weakness
2020-2125	For this reason, the recommendation is to reject the paper.	B-decision
2020-2125	The authors are encouraged to fairly compare all models and resubmit their paper at another venue.	I-decision

2020-2130	Unfortunately the paper is confusingly written, and there is only agreement by all reviewers on the rejection of the paper.	B-rating_summary
2020-2130	Indeed, if all reviewers and the area chair do not interpret the paper well, the authors' best response would be to rewrite the papers rather than disagree with all reviewers. <sep>	O
2020-2130	In the area chair's opinion, the current form the paper does not merit publication.	B-decision
2020-2130	The authors are advised to address the reviewers' concerns, rework the paper, and submit to a conference again.	I-decision

2020-2138	This paper explores training CNNs with labels of differing granularity, and finds that the types of information learned by the method depends intimately on the structure of the labels provided. <sep>	B-abstract
2020-2138	Thought the reviewers found value in the paper, they felt there were some issues with clarity, and didn't think the analyses were as thorough as they could be.	B-weakness
2020-2138	I thank the authors for making changes to their paper in light of the reviews, and hope that they feel their paper is stronger because of the review process.	O

2020-2148	The reviewers initially gave scores of 1,1,3 citing primarily weak empirical results and a lack of theoretical justification.	B-rating_summary
2020-2148	The experiments are presented on synthetic examples, which is a great start but the reviewers found that this doesn't give strong enough evidence that the methods developed in the paper would work well in practice.	B-weakness
2020-2148	The authors did not submit an author response to the reviewers and as such the scores did not change during discussion.	B-rebuttal_process
2020-2148	This paper would be significantly strengthened with the addition of experiments on actual problems eg related to drug discovery which is the motivation in the paper.	B-suggestion

2020-2149	This paper proposes a fractional graph convolutional networks for semi-supervised learning, using a classification function repurposed from previous work, as well as parallelization and weighted combinations of pooling function.	B-abstract
2020-2149	This leads to good results on several tasks. <sep>	I-abstract
2020-2149	Reviewers had concerns about the part played by each piece, the lack of comparison to recent related work, and asked for better explanation of the rationale of the method and more experimental details.	B-rebuttal_process
2020-2149	Authors provided explanations and details, and a more thorough set of comparison to other work, showing better performance in some but not all cases. <sep>	I-rebuttal_process
2020-2149	However, concerns that the proposed innovations are too incremental remain. <sep>	I-rebuttal_process
2020-2149	Therefore, we cannot recommend acceptance.	B-decision

2020-2152	The authors propose a model-based RL algorithm, consisting of learning a deterministic multi-step reward prediction model and a vanilla CEM-based MPC actor. <sep>	B-abstract
2020-2152	In contrast to prior work, the model does not attempt to learn from observations nor is a value function learned. <sep>	I-abstract
2020-2152	The approach is tested on task from the mujoco control suit.<sep>	I-abstract
2020-2152	The paper is below acceptance threshold. <sep>	B-decision
2020-2152	It is a variation on previous work form Hafner et al <sep>	B-weakness
2020-2152	Furthermore, I think the approach is fundamentally limited: All the learning derives from the immediate, dense reward signal, whereas the main challenges in RL are found in sparse reward settings that require planning over long horizons, where value functions or similar methods to assign credit over long time windows are absolutely essential.	I-weakness

2020-2153	This paper pursues an ambitious goal to provide a theoretical analysis HRL in terms of regret bounds.	B-abstract
2020-2153	However, the exposition of the ideas has severe clarity issues and the assumptions about HMDPs used are overly simplistic to have an impact in RL research. <sep>	B-weakness
2020-2153	Finally, there is agreement between the reviewers and AC that the novelty of the proposed ideas is a weak factor and that the paper needs substantial revision.	I-weakness

2020-2161	This paper presents a method for providing uncertainty for deep learning regressors through assigning a notion of evidence to the predictions.	B-abstract
2020-2161	This is done by putting priors on the parameters of the Gaussian outputs of the model and estimating these via an empirical Bayes-like optimization.	I-abstract
2020-2161	The reviewers in general found the methodology sensible although incremental in light of Sensoy et al and Malinin & Gales but found the experiments thorough.	B-strength
2020-2161	A comment on the paper pointed out that the approach was very similar to something presented in the thesis of Malinin (it seems unfair to expect the authors to have been aware of this, but the thesis should be cited and not just the paper which is a different contribution).	B-weakness
2020-2161	In discussion, one reviewer raised their score from weak reject to weak accept but the highest scoring reviewer explicitly was not willing to champion the paper and raise their score to accept.	B-rebuttal_process
2020-2161	Thus the recommendation here is to reject.	B-decision
2020-2161	Taking the reviewer feedback into account, incorporating the proposed changes and adding more careful treatment of related work would make this a much stronger submission to a future conference.	B-suggestion

2020-2174	This paper proposes to use a mixture of Gaussians to variationally encode high-dimensional data through a latent space.	B-abstract
2020-2174	The latent codes are constrained using the variational information bottleneck machinery. <sep>	I-abstract
2020-2174	While the paper is well-motivated and relatively well-written, it contains minimal novel ideas.	B-weakness
2020-2174	The consensus in reviews and lack of rebuttal make it clear that this paper should be significantly augmented with novel material before being published to ICLR. <sep>	B-decision

2020-2188	The paper proposes to use the representation learned via CPC to do reward shaping via clustering the embedding and providing a reward based on the distance from the goal. <sep>	B-abstract
2020-2188	The reviewers point out some conceptual issues with the paper, the key one being that the method is contingent on a random policy being able to reach the goal, which is not true for difficult environments that the paper claims to be motivated by.	B-weakness
2020-2188	One reviewer noted limited experiment runs and lack of comparisons with other reward shaping methods. <sep>	I-weakness
2020-2188	I recommend rejection, but hope the authors find the feedback helpful and submit a future version elsewhere.	B-decision

2020-2191	The present paper establishes uniform approximation theorems (UATs) for PointNet and DeepSets that do not fix the cardinality of the input set. <sep>	B-abstract
2020-2191	Two nonexperts read the paper and came away not understanding what this exercise has taught us and why the weakening of the hypotheses was important.	B-weakness
2020-2191	The authors made no attempt to argue these points in their rebuttals and so I went looking at the paper to find the answer in their revisions, but did not find it after scanning through the paper.	B-rebuttal_process
2020-2191	I think a paper like this needs to explain what is gained and what obstructions earlier approaches met, and why the current techniques side step those.	B-suggestion
2020-2191	One of the reviewers felt that the fixed cardinality assumption was mild.	B-weakness
2020-2191	I'm really not sure why the authors didn't attack this idea.	B-rebuttal_process
2020-2191	Maybe it is mild in some technical sense? <sep>	B-suggestion
2020-2191	What I read of the paper seemed excellent in term of style and clarity.	B-strength
2020-2191	I think the paper simply needs to make a better case that it is not merely an exercise in topology.	B-suggestion
2020-2191	I think the result here is publishable on its own grounds, but for the paper to effectively communicate those findings, the authors should have revised it to address these issues.	I-suggestion
2020-2191	They chose not to and so I recommend ICLR take a pass.	B-decision
2020-2191	Once the reviewers revised the framing and scope/impact, provided it doesn't sound trivial, I think it'll be ready for publication. <sep>	B-suggestion

2020-2197	The authors tackle an interesting and important problem, developing numerical common-sense.	B-abstract
2020-2197	They use a crowdsourcing service to collect a dataset and use regression from word embeddings to numerical common sense. <sep>	I-abstract
2020-2197	Reviewers were concerned with the size and quality of the dataset, the quality of the prediction methods used, and the analysis of the experimental results. <sep>	B-weakness
2020-2197	Given the many concerns, I recommend rejecting the paper, but I encourage the authors to revise the paper to address the concerns and resubmit to another venue.	B-decision

2020-2203	The reviewers reached a consensus that the paper was not ready to be accepted in its current form.	B-rating_summary
2020-2203	The main concerns were in regard to clarity, relatively limited novelty, and a relatively unsatisfying experimental evaluation.	B-weakness
2020-2203	Although some of the clarity concerns were addressed during the response period, the other issues still remained, and the reviewers generally agreed that the paper should be rejected.	B-rating_summary

2021-1	There is a clear consensus over all reviewers that this is a very strong empirical analysis, with actionable insights that should prove quite useful both to researchers and practitioners.	B-strength
2021-1	I have no doubt that many will use it as a reference when implementing and using RL algorithms (especially since the authors said they would release their code). <sep>	I-strength
2021-1	This is thus a clear accept, that in my opinion would deserve an oral presentation, so as to better disseminate its key findings. <sep>	B-decision

2021-33	Two knowledgeable reviewers were positive 7 and very positive 10 about this paper, considering it an important contribution that illuminates previously unknown aspects of two classic models, namely RBMs and Hopfield networks.	B-rating_summary
2021-33	They considered the work very well developed, theoretically interesting and also of potential practical relevance.	B-strength
2021-33	A third reviewer initially expressed some reservations in regard to the inverse map from RBMs to HNs and the experiments.	B-rebuttal_process
2021-33	Following the authors' responses, which the reviewer found detailed and informative, he/she significantly raised his/her score to 7, also emphasizing that he/she hoped to see the paper accepted.	I-rebuttal_process
2021-33	With the unanimously positive feedback, I am recommending the paper to be accepted. <sep>	B-decision

2021-36	The paper proposes a method to grow deep network architectures over the course of training.	B-abstract
2021-36	The work has been extremely well received and has clear novelty and solid experiment validation. <sep>	B-strength

2021-43	All reviewers agree that this is a well-written and interesting paper that will be of interest to the ICLR and broader ML community. <sep>	B-rating_summary

2021-65	This work presents a method to combine EBMs and VAEs in two stages.	B-abstract
2021-65	First, the VAE model is learned; second, an EBM-based correction term is learned via MLE.	I-abstract
2021-65	The methodology is novel and of interest to the ICLR community. <sep>	B-strength

2021-105	Three reviewers are positive, while one is negative.	B-rating_summary
2021-105	The negative reviewer is well-qualified, but the review is not persuasive.	O
2021-105	Overall, this paper should be published as a wake-up call to the research community.	B-decision
2021-105	Unfortunately, the lesson of this paper is similar to that of several previous papers, in particular <sep>	B-weakness
2021-105	Armstrong, T. G., Moffat, A., Webber, W., & Zobel, J.	O
2021-105	(2009, November).	O
2021-105	Improvements that don't add up: ad-hoc retrieval results since 1998.	O
2021-105	In Proceedings of the 18th ACM conference on Information and knowledge management (pp.	O
2021-105	601-610). <sep>	O
2021-105	This submission should be a spotlight, to maximize the chance that future researchers learn its lesson. <sep>	B-decision

2021-109	All three reviewers are positive, and the authors have addressed essentially all the questions raised by the reviewers.	B-rating_summary
2021-109	The main insight of the paper is clear, and the empirical results are good, so a spotlight is deserved. <sep>	B-decision

2021-110	The paper analyzes the gradient flow dynamics of deep equilibrium models with linear activations and establishes linear convergence for quadratic loss and logistic loss; several exciting results and connections, solid contribution, accept! <sep>	B-decision

2021-113	This work proposes a simple and intuitive way to improve how to learn a communication protocol off-policy in the non-stationary situation in which messages received in the past do not reflect an agent's current policy.	B-abstract
2021-113	The authors introduce a communication correction that relabels the received message adjusting it to the current policy.	I-abstract
2021-113	The authors show that this method, besides being simple, is effective in a number of experiments.	I-abstract
2021-113	As observed by some reviewers, an issue with the method is that it is not clear how it would scale up to more complex environments than those considered.	B-weakness
2021-113	However, the authors addressed the concerns during the response phase, both adding new experiments, and with a clear statement of what are the outstanding issues.	B-rebuttal_process
2021-113	The paper is certainly a clever and solid contribution to the area of multi-agent communication learning, and I am strongly in favour of accepting it. <sep>	B-decision

2021-115	This paper advances the idea that recent "influence estimation" methods for supervised learning cannot be trivially applied to GANs.	B-abstract
2021-115	Based on Hara et al's method, the authors propose a novel influence estimation for GANs, and an evaluation scheme based on popular GAN evaluation methods, exploiting the fact that they are differentiable with respect to their input data.	I-abstract
2021-115	The paper demonstrates empirically that the proposed influence estimation method correlates to true influence.	I-abstract
2021-115	It also shows that removing "harmful" instances using the average log-likelihood, Inception Score, and Frechet Inception Distance versions of the proposed metric improves the quality of generated examples. <sep>	I-abstract
2021-115	All reviewers were positive about the paper.	B-rating_summary
2021-115	R2 pointed out that it was well-written and appreciated the detailed analysis.	B-strength
2021-115	They thought it thoroughly explained the similarities between it and the most closely-related recent work (Hara et al and Koh & Liang).	I-strength
2021-115	Concerns expressed by the reviewer were: the amount of samples needed to be removed to obtain a statistically significant result, lack of qualitative results, and an outdated baseline for anomaly detection.	B-weakness
2021-115	The reviewer also stated that they had some concerns with practical applicability and would like to see more GAN metrics, like Precision & Recall.	I-weakness
2021-115	The authors added qualitative results to the paper which partially satisfied the reviewer. <sep>	B-rebuttal_process
2021-115	R1 also thought that the paper was well-written and contributed to the interpretability of GAN training.	B-strength
2021-115	Like R2, they pointed out the lack of visual examples (addressed in rebuttal), and asked for more insight into what kind of characteristics make a data point influential.	B-rebuttal_process
2021-115	They also requested that the authors add a metric that trades fidelity and diversity like P&R.	I-rebuttal_process
2021-115	The reviewer originally felt that the paper was below the bar, because it was "like a story without a satisfying conclusion".	I-rebuttal_process
2021-115	However, the authors responded with additional analysis which satisfied the reviewer, and they upgraded their score by two points. <sep>	I-rebuttal_process
2021-115	R3 also found the paper well-written and interesting, like the other reviewers.	B-strength
2021-115	The reviewer raised some similar concerns as the other reviewers (eg qualitative results), as well as the scalability of the method to relevant architectures, which I thought was surprising that the other reviewers didn't mention.	B-weakness
2021-115	The authors responded that they believe their method succeeded in improving diversity of the generated samples but not their visual quality.	B-rebuttal_process
2021-115	This is an important point. <sep>	I-rebuttal_process
2021-115	The additions in Appendix D have addressed the main concerns of R1 and R2, as well as R3's concern about lack of visual analysis.	I-rebuttal_process
2021-115	R1 seems quite convinced now, and R2, though not changing their score, was already in favour of acceptance.	B-rating_summary
2021-115	It is an interesting finding that "harmful" instances seem to come from regions of distributional mismatch. <sep>	B-weakness
2021-115	I would like to see a fidelity-diversity tradeoff like P&R added to a paper, and a discussion of this work in relation to DeVries et al "Instance Selection" that appears to be similarly motivated though executed differently.	B-suggestion
2021-115	I think one major thing holding back this paper is the scale of the experimental analysis (Gaussians & MNIST); I hope the authors can scale the method in future work. <sep>	B-weakness

2021-128	This paper has received four positive reviews.	B-rating_summary
2021-128	The main intellectual contribution of the paper is the introduction of a novel readout mechanism that allows models to be shared fully across neurons which in turn helps transfer learning across neurons and even across animals.	B-strength
2021-128	The reviewers commented on the technical strength of the paper.	I-strength
2021-128	At the same time, the main contribution remains relatively incremental from a technical standpoint, and while the approach may be of value to future work, the impact of the current study on neuroscience (which is the target here) is quite limited.	B-weakness
2021-128	Nonetheless, there seems to be sufficient enthusiasm from the reviewers to recommend this paper be accepted. <sep>	B-rating_summary

2021-139	This paper presents a new NAS benchmarks for hardware-aware NAS.	B-abstract
2021-139	For each of the architectures in the search space of NAS-Bench-201, it measures hardware performance (energy cost and latency) for six different hardware devices.	I-abstract
2021-139	This is extremely useful for the NAS research community, since it takes very specialized hardware domain knowledge (including machine learning development frameworks, device compilation, embedded systems, and device measurements) as well as the hardware to make these hardware-aware measurements on as many as six (very different) devices. <sep>	I-abstract
2021-139	The code has been made available to the reviewers during the author response window and has been checked by the reviewers in the meantime.	B-rebuttal_process
2021-139	All reviewers appreciated the paper and gave (clear) acceptance scores. <sep>	B-rating_summary
2021-139	Before this work, it was very hard for the average NAS researcher to assess their method properly in a hardware-aware setting, and I expect this work to change this, and to open up the very important field of hardware-aware NAS to many more researchers.	O
2021-139	For this reason I recommend to accept this paper as a spotlight. <sep>	B-decision

2021-141	This paper proposes an efficient attention mechanism linear in time and space using random features. <sep>	B-abstract
2021-141	The approach has some similarities with the simultaneous ICLR 2021 submission "Rethinking Attention with Performers", with a key difference of a gating mechanism present in this work, motivated by recency bias.	I-abstract
2021-141	This paper is a valuable contributions to the efficient attention research topic.	B-strength
2021-141	The reviewers appreciate the experiments and the in-depth analysis.	I-strength
2021-141	I recommend acceptance. <sep>	B-decision
2021-141	A noteworthy concern brought up in the discussion period has to do with whether the attention mechanism dominates the feed-forward computations in the neural network, and how much this is architecture-specific.	B-weakness
2021-141	The authors provide TPU timings, but I encourage the authors to add a discussion and timings of relative performance of feed-forward vs. attention layers that covers GPU and CPU optimizers as well. <sep>	B-suggestion

2021-148	This paper provides a novel generalization bound for neural networks using knowledge distillation.	B-abstract
2021-148	In particular, they argue that "test error <= training error + distillation error + distillation complexity" where the distillation complexity is typically much smaller than the original complexity of the neural network.	I-abstract
2021-148	This is motivated by the empirical findings that neural networks can typically be significantly compressed in practice using KD without losing too much accuracy. <sep>	I-abstract
2021-148	I found this result novel and the direction is very promising.	B-strength
2021-148	This is a clear accept for ICLR. <sep>	B-decision

2021-166	This paper addresses a crucial problem with graph convolutions on meshes. <sep>	B-abstract
2021-166	The authors identify the issues related to existing networks and devise a sensible approach. <sep>	I-abstract
2021-166	The work presents a novel message passing GNN operator for meshes that is equivariant under gauge transformations. <sep>	I-abstract
2021-166	The reviewers unanimously agree on the both the importance of the problem and the impact the proposed work could have. <sep>	B-strength
2021-166	Suggestions for next version: <sep> The paper is unreadable without the appendix and somehow it would be better to make it self-contained <sep> 	B-weakness
2021-166	Additional references as suggested in the reviews. <sep>	B-suggestion
2021-166	Expanded experiments as suggested by R4, will also improve reader's confidence in the method.<sep>	I-suggestion
2021-166	I would recommend acceptance.	B-decision
2021-166	I would request the authors to release a sufficiently documented and easy to use implementation.	B-suggestion
2021-166	This not only allows readers to build on this work but also increase the overall impact of this method. <sep>	I-suggestion

2021-187	Reviewers generally agree that the main result of the paper, which generalizes the classical Wigner-Eckart Theorem and provides a  basis for the space of G-steerable kernels for any compact group G, is a significant result.	B-strength
2021-187	There are also several concerns that need to be addressed.	O
2021-187	R4 notes that the use of the Dirac delta function (eg Theorem C.7) is informal and mathematically imprecise and needs to be fixed.	B-weakness
2021-187	R1 notes that it would be helpful to at least describe how this general formulation can be applied in machine learning. <sep>	B-suggestion
2021-187	Presentation and accessibility: the current version of the paper will be accessible to only  a small part of the machine learning audience, ie those already with advanced knowledge in mathematics and/or theoretical physics, in particular in representation theory.	B-weakness
2021-187	If the authors aim to make it more accessible, the writing would need to be substantially improved. <sep>	I-weakness

2021-196	This is a fairly technical paper bridging deep learning with uncertainty propagation in computations (ie probabilistic numerics).	B-abstract
2021-196	It is well structured, but it could benefit from further improvements in readability given that there are only very few researchers that are experts in all sub-domains associated with this work.	B-suggestion
2021-196	Given the above, as well as low overall confidence by the reviewers, I attempted a more thorough reading of the paper (even if not an expert myself), and I was also happy to see that the discussion clarified important points.	B-rebuttal_process
2021-196	Overall, the idea is novel, convincing and seems well executed, with good results.	B-strength
2021-196	The technical advancements needed to make the idea work are fairly complicated and are appreciated as contributions, because they are expected to be useful in other applications too (beyond irregular sampled data) where uncertainty propagation matters. <sep>	I-strength

2021-202	This paper analyzes a version of optimistic value iteration with generalized linear function approximation.	B-abstract
2021-202	Under an optimistic closure assumption,  the algorithm is shown to enjoy sublinear regret.	I-abstract
2021-202	The paper also studies error propagation through backups that do not require closed-form characterization of dynamics and reward functions. <sep>	I-abstract
2021-202	Overall, this is a solid contribution and the consensus is to accept. <sep>	B-decision

2021-215	CausalWorld is a benchmark for robotic manipulation to address transfer and structural learning.	B-abstract
2021-215	The benchmark includes (i) a variety of tasks (picking, pushing, tower, etc) relating to manipulating blocks, (ii) configurable properties for environments (properties of blocks, gravity, etc), (iii) customizable learning settings involving intervention actors, which can change the environment to induce a curriculum. <sep>	I-abstract
2021-215	The reviewers found the paper compelling and with many strengths, including 'interesting and important ideas' (R4), 'simple API with a standardized interface' for 'procedural generation of goals' (R5), 'strongly motivated and tackles a real and practical problem' (R3), and 'benchmark with many good properties' (R2).	B-strength
2021-215	By and large, the reviewers agree that the paper presents an important benchmark satisfying several desiderata, which I certainly agree with. <sep>	I-strength
2021-215	On the other hand, most of the reviewers (3 out of 4) also raised serious concerns, more prominently, about the experimental results and the causal inference component.	B-weakness
2021-215	For instance, R5 commented that "all the SOTA algorithms fail," and it is hard to quantify how agents would perform well in different tasks.	I-weakness
2021-215	R3 pointed out the lack of "qualitative results exploring the relationship between the identified and proposed causal variables," emphasizing that 'the benchmark is well-motivated, but not backed up with strong experimental results.''	I-weakness
2021-215	R2 identified the lack of clear causal component in the paper while the paper mentions "opportunity to investigate causality" and "underlying structural causal model (SCM)."	I-weakness
2021-215	All in all, these are valid concerns. <sep>	O
2021-215	The authors' rebuttal was quite detailed, and appreciated, but left some important questions unanswered.	B-rebuttal_process
2021-215	The first and critical issue is about the causal nature of the simulator.	I-rebuttal_process
2021-215	The simulator's name is "causalworld" and its stated goal is to provide "a benchmark for causal structure and transfer learning in a robotic manipulation environment."	I-rebuttal_process
2021-215	Also, the first bullet in the list of contributions is: "We propose CausalWorld, a new benchmark comprising a parametrized family of robotic manipulation environments for advancing out-of-distribution generalization and causal structure learning in RL."	I-rebuttal_process
2021-215	After reading the paper, I was quite surprised to realize there is no single example of a causal model, in any shape or form (eg, SCM, DAG, Physics) or a structural learning benchmark.	I-rebuttal_process
2021-215	In other words, there is a serious, somewhat nontrivial gap between the claimed contributions and what was realized in the paper.	I-rebuttal_process
2021-215	One way to address this issue would be to make the causality more explicit in the paper, for example, by sharing the underlying structural causal model, how variables form causal relationships, what causal structures are being learned, and how these learned structures compare with the ground truth.	B-suggestion
2021-215	I think these would be reasonable expectations of a simulator that aims to disentangle the causal aspect of the learning process. <sep>	I-suggestion
2021-215	The second issue is about the experimental results in terms of generalizability.	B-rebuttal_process
2021-215	The authors emphasized on different occasions that "The primary goal of this work is to provide the tools to build and evaluate generalizable agents in a more systematic fashion, rather than building generalizable agents for the tasks specified," or "the experiments is to showcase the flexibility regarding curricula and performance evaluation schemes offered with CausalWorld, rather than solving new tasks or proposing new algorithms."	I-rebuttal_process
2021-215	These responses are somewhat not satisfactory given that the goal of the paper is providing tools to build generalizable agents, while the authors seem to suggest they are not committed to actually building such agents.	I-rebuttal_process
2021-215	Specifically, the experiments did not demonstrate the simulator as a benchmark but only showcased its flexibility (ie, offering a large number of degrees of freedom).	I-rebuttal_process
2021-215	One suggestion would be to evaluate how algorithms (agents) with varying degrees of "generalizability" power perform across tasks with various difficulty levels.	B-suggestion
2021-215	As it currently stands, the tasks are too easy or too hard for the standard, uncategorized algorithms, which makes it difficult to learn any lessons from running something in the simulator. <sep>	B-rebuttal_process
2021-215	Lastly, I should mention that the work has a great potential to introduce causal concepts and causal reasoning to robotics, there is a natural and compelling educational component here.	B-strength
2021-215	Still, the complete absence of any discussion of causality and the current literature results hurt this connection and the realization of this noble goal.	B-rebuttal_process
2021-215	I believe that after reading the paper, the regular causal inference researcher will not be able to understand what assumptions and types of challenges are entailed by this paper and robotics research.	I-rebuttal_process
2021-215	On the other hand, the robotics researcher will not be able to understand what a causal model is and the tools currently available in causal reasoning that may be able to help solve the practical challenges of robotics.	I-rebuttal_process
2021-215	In other words, this is a huge missed opportunity since there is a complementary nature of what the paper is trying to do in robotics and the results available in causal inference.	I-rebuttal_process
2021-215	I believe readers expect and would benefit from having this connection clearly articulated and realized in a more explicit fashion. <sep>	B-suggestion
2021-215	If the issues listed above are addressed, I believe the paper can be a game-changer in understanding and investigating robotics & causality.	I-suggestion
2021-215	Given the aforementioned potential and reasons, I recommend the paper's acceptance under the assumption that the authors will take the constructive feedback provided in this meta-review into account and revise the manuscript accordingly. <sep>	B-decision

2021-220	The paper is proposing a new framework for understanding generalization in the deep learning.	B-abstract
2021-220	The main idea is considering the difference of stochastic optimization on a population risk and optimization on an empirical risk.	I-abstract
2021-220	The classical theory considers the difference of empirical risk and population risk.	I-abstract
2021-220	This basically translates the practical motivation from finding good function classes to finding good optimizers which can re-use the data effectively.	I-abstract
2021-220	Although the paper provides no theoretical result, it provides an interesting empirical study.	B-strength
2021-220	The paper somewhat demonstrates that SGD on deep networks is somehow good at re-using the same data.	I-strength
2021-220	I believe this angle is very novel and might hope to future theoretical discoveries.	I-strength
2021-220	The paper is reviewed by four reviewers and two of them argue its acceptance and two of them argue rejection.	B-rating_summary
2021-220	After discussion, this status remained and I carefully read and reviewed the paper.	O
2021-220	Here are the major issues raised by the reviewers: <sep> R#1: The paper is missing a theoretical study.	B-weakness
2021-220	The implications on the practical deep learning is not clear. <sep>	I-weakness
2021-220	R#2: Choice of the soft-error is particular to the task and how to go beyond soft-max is not clear. <sep>	I-weakness
2021-220	R#3: Finds the paper not novel as well as trivial or hard to understand. <sep>	I-weakness
2021-220	R#4: The choice of soft error is ad-hoc.<sep>	I-weakness
2021-220	I believe the issues raised by R#3 are not justified.	B-ac_disagreement
2021-220	First of all, novelty is very clear and.	I-ac_disagreement
2021-220	appreciated by other reviewers.	I-ac_disagreement
2021-220	Moreover, the paper is rather easy to understand and the results are very farm from trivial.	I-ac_disagreement
2021-220	However, the other issues raised by other reviewers are valid.	O
2021-220	Specifically, soft-error seems to be a limitation of the study.	B-weakness
2021-220	However, the authors respond to this concern and reviewer increases their score.	B-rebuttal_process
2021-220	I believe the theory is lacking but the paper is simply showing this novel approach and its empirical validity.	B-weakness
2021-220	A theory to explain this phenomenon would be amazing but not necessary for publication.	B-suggestion
2021-220	Similarly, without a theory it is hard to expect any practical implication.	B-weakness
2021-220	Overall, I believe the paper is an interesting and novel one which will likely to lead additional work in the area.	B-strength
2021-220	Considering we are still far from a satisfying theory of generalization for deep learning and the role of the optimization is clear, this angle worth sharing with the community.	I-strength
2021-220	Hence, I decide to accept.	B-decision
2021-220	However, I have some concerns which should be addressed by the camera-ready.<sep>	O
2021-220	Claims should be revised and authors should make sure they have enough evidence for them.	B-suggestion
2021-220	For example, authors provide no satisfying evidence for random labels or very limited evidence for pre-training.	I-suggestion
2021-220	I strongly recommend authors to either remove some of these discussions or present in a fashion which is not a result but part of the discussion for future research. <sep>	I-suggestion
2021-220	A section about limitations should be added.	I-suggestion
2021-220	Specifically, the soft-error choice should be discussed in this limitation section. <sep>	I-suggestion
2021-220	Discussion section should be extended with the pointers to the relevant work on bootstrap literature as well as suggestions to the theoreticians.	I-suggestion
2021-220	Not providing any theoretical result is always fine but authors should understanding why is it hard to make theoretical statements and where to search them.<sep>	I-suggestion

2021-221	This paper presents a hierarchical version of -TCVAE that promotes disentanglement in the latent space and improves the robustness of VAEs over adversarial attacks, without (much) degeneration on the quality of reconstructions.	B-abstract
2021-221	The analysis on the relationship between disentanglement and adversarial robustness is valuable and the method is new.	B-strength
2021-221	The results look promising.	I-strength
2021-221	The comments were properly addressed. <sep>	B-rebuttal_process

2021-234	Summary: <sep> This paper introduces a different, interesting definition of safety in RL.	B-abstract
2021-234	The paper does a nice job of showing success with empirical results and providing bounds.	B-strength
2021-234	I think it provides a nice contribution to the field. <sep>	I-strength
2021-234	Discussion: <sep> The reviewers agree this paper should be accepted.	B-rating_summary
2021-234	The initial points brought up against the paper have been successfully addressed or mitigated. <sep>	B-rebuttal_process

2021-254	Most of the reviewers agree that this paper presents interesting ideas for an important problem.	B-strength
2021-254	The paper could be further improved by having a thorough discussion of related works (eg Placeto) and construct proxy baselines that reflect these approaches. <sep>	B-suggestion
2021-254	The meta-reviewer decided to accept the paper given the positive aspects, and encourages the author to further improve the paper per review comments. <sep>	B-decision
2021-254	Thank you for submitting the paper to ICLR. <sep>	O

2021-270	The work tackles the task to convert an artificial neural networks (ANN) to a spiking neural network (SNN).	B-abstract
2021-270	The topic is potentially important for energy-efficient hardware implementations of neural networks.	I-abstract
2021-270	There is already quite some literature available on this topic. <sep>	I-abstract
2021-270	Compared to these, the manuscript exhibits a number of strong contributions: It presents a theoretical analysis of the conversion error and consequently arrives at a principled way to reduce the conversion error.	I-abstract
2021-270	The authors test the performance of the conversion on a number of challenging data sets.	I-abstract
2021-270	Their method achieves excellent performances with reduced simulation time / latency (usually, in order to achieve comparable performance to ANNs, one needs to run the SNN for many simulated time steps- this simulation time is reduced by their model). <sep>	I-abstract
2021-270	One reviewer criticized that the article was hard to read, but this opinion was not shared by other reviewers and the authors have improved the readability in a revision. <sep>	B-rebuttal_process
2021-270	In summary, I believe that this manuscript presents a very good contribution to the field. <sep>	B-strength

2021-287	The reviewers have  different views on the papers but agreed that the paper can be accepted.	B-rating_summary
2021-287	However, they suggested some points of improvements including the writing (clarity and style) and experiments showing strong improvements compared to WGAN. <sep>	B-suggestion

2021-297	There is some positive consensus on this paper, which improved somewhat after the very detailed rebuttal comments by the authors.	B-rebuttal_process
2021-297	The use of limited amounts of OOD data is interesting and novel.	B-strength
2021-297	There were some experimental design problems, but these were well-addressed in rebuttal. <sep>	B-rebuttal_process
2021-297	A reviewer points out that anomaly/outlier detection does not explicitly assume that there is only one class within the normal class (or in-distribution data).	B-weakness
2021-297	The one-class assumption is mainly made in some popular anomaly detection methods, such as one-class classification-based approaches for anomaly detection.	I-weakness
2021-297	The authors should take this into careful consideration when preparing a final version of this work. <sep>	B-suggestion

2021-301	The paper closes an important gap in our understanding of neural tangent kernels. <sep>	B-strength
2021-301	In addition, the used techniques are novel. <sep>	I-strength
2021-301	My low confidence is mainly based on the fact, that the review process at conference is not perfectly suited to deal with such papers, since their review would actually require both expert reviewers and substantially longer reviewing periods. <sep>	O

2021-304	This paper studies how to improve the worst-case subgroup error in overparameterized models using two simple post-hoc processing techniques.	B-abstract
2021-304	All reviewers were positive about the paper, though R5 questioned the novelty of the paper which built heavily on a few previous papers (in particular, it builds heavily on Sagawa et al 2020a,b).	B-rating_summary
2021-304	The AC is satisfied with the authors`' response clarifying the novelty.	B-rebuttal_process
2021-304	Given that this topic is quite timely and of interest to the ICLR community, and that this paper presented a clean investigation on it, the AC recommends acceptance. <sep>	B-decision

2021-315	This paper proposed an ensemble of diverse models as a mechanism to protect models from theft. <sep>	B-abstract
2021-315	The idea is quite novel.	B-strength
2021-315	There are some concerns regarding the robustness of the hashing function (that I share), however not every paper has to be perfect, especially when it introduces a novel setup. <sep>	B-ac_disagreement
2021-315	AC <sep>	O

2021-333	This paper proposes a new implementation of a previously proposed two-stage process for video prediction: first predict future segmentation maps, then map them to video frames.	B-abstract
2021-333	Combined with other advances in video prediction and image generation, this simple idea is shown empirically to work very well, producing video predictions up to many hundreds of frames into the future in real stochastic settings with unprecedented quality.	I-abstract
2021-333	Strong ablation studies over the course of the review process further serve to confirm the value of various design choices involved in the implementation. <sep>	B-rebuttal_process

2021-338	This paper considers a new setting of robustness, where multiple predictions are simultaneously made based on a single input.	B-abstract
2021-338	Different from existing robustness certificates which independently consider perturbation of each prediction, the authors propose collective robustness certificate that computes the number of predictions which are simultaneously guaranteed to remain stable under perturbation.	I-abstract
2021-338	This yields more optimistic results.	B-strength
2021-338	Most reviewers think this is a very interesting work and the authors present an effective method to combine individual certificate.	I-strength
2021-338	The experimental results are convincing.	I-strength
2021-338	I recommend accept. <sep>	B-decision

2021-339	This paper studies the link between generalization behavior and "flatness" of the loss landscape in deep networks.	B-abstract
2021-339	Specifically, the authors study two measures of flatness (local entropy and local energy), and show that these two measurements are strongly correlated with one another.	I-abstract
2021-339	Moreover they show via a careful set of numerical experiments that two previously proposed algorithms (entropy SGD and replica SGD) that optimize for local entropy tend to both find flatter minima as well as provide better generalization. <sep>	I-abstract
2021-339	Despite the fact that the paper proposes no new models or algorithms, the experiments are compelling and provide non-trivial insights into predicting generalization behavior of deep networks, as well as solid evidence on the benefits of entropy regularization in SGD.	B-strength
2021-339	The authors also seem to have satisfactorily answered the (numerous) initial concerns raised by the authors.	B-rebuttal_process
2021-339	Overall, I recommend an accept. <sep>	B-decision

2021-360	Dear Authors, <sep>	O
2021-360	Thank you very much for your very detailed feedback to the reviewers.	O
2021-360	They have highly contributed to clarifying some of the concerns raised by the reviewers and improved their understanding of this paper. <sep>	B-rebuttal_process
2021-360	Overall, all the reviewers acknowledge the merit of this paper and thus I suggest acceptance of this paper. <sep>	B-decision
2021-360	However, as Reviewer #4 pointed out, there are conceptual and theoretical issues that need to be more carefully addressed. <sep>	B-weakness
2021-360	Please clarify these issues in the final version of the paper. <sep>	B-suggestion

2021-369	This paper proposed a new method to prune neural networks using a continuous penalty function.	B-abstract
2021-369	All reviewers suggest acceptance (some are on borderline though) as the authors did a good job in the rebuttal phase.	B-rating_summary
2021-369	AC also could not find any particular reason to reject the paper (in particular, the overall writing is clear) and thinks that this paper is a meaningful addition to ICLR 2021. <sep>	B-decision

2021-383	Four reviewers have reviewed this paper and after rebuttal, they were overall positive about the proposed idea.	B-rating_summary
2021-383	We congratulate authors on the paper. <sep>	O

2021-405	This paper presents an approach to domain adaptation in reinforcement learning.	B-abstract
2021-405	The main idea behind this approach, DARC, is to modify the reward function in the source domain so that the learned policy is optimal in the target domain.	I-abstract
2021-405	This is achieved by learning a classifier that learns to discriminate between the data from the source domain and those from the target domain. <sep>	I-abstract
2021-405	Overall, reviewers appreciated the intuitiveness of the approach as well as its formal analysis.	B-strength
2021-405	They had some concerns with respect to experiments, which was sorted out in the author response period.	B-rebuttal_process
2021-405	Given the overall positive reviews, I recommend accepting the paper. <sep>	B-decision

2021-414	The paper aims at understanding why self-supervised/contrastive learning methods  transfer well when used as pretraining for fine-tuning downstream tasks  (compared to eg, supervised pretraining based on the cross-entropy loss).	B-abstract
2021-414	Three reviewers recommend acceptance, whereas one reviewer recommends borderline rejection, arguing the take home message of the paper is not very clear.	B-rating_summary
2021-414	While this is a legitimate concern, the AC agrees with the majority that the paper does shed light on the differences between supervised and self-supervised pretraining (based on interesting empirical findings) and recommends acceptance. <sep>	B-decision

2021-429	This paper considers meta-learning based on MAML.	B-abstract
2021-429	The authors use Neural Tangent Kernels (NTKs) to develop two meta-learning algorithms that avoid the inner-loop adaptation, which makes MAML computationally intensive.	I-abstract
2021-429	Experimental results demonstrate favorable empirical performance over existing methods. <sep>	I-abstract
2021-429	The paper is generally well written and readable.	B-strength
2021-429	The proposed methods are well motivated and based on solid theoretical ground.	I-strength
2021-429	The emprirical performance shows advantages in efficiency and quality.	I-strength
2021-429	This work is worth acceptence in ICLR 2021. <sep>	B-decision

2021-440	This paper presents an empirical study focusing on Bayesian inference on NNGP - a Gaussian process where the kernel is defined by taking the width of a Bayesian neural network (BNN) to the infinity limit.	B-abstract
2021-440	The baselines include a finite width BNN with the same architecture, and a proposed GP-BNN hybrid (NNGP-LL) which is similar to GPDNN and deep kernel learning except that the last-layer GP has its kernel defined by the width-limit kernel.	I-abstract
2021-440	Experiments are performed on both regression and classification tasks, with a focus on OOD data.	I-abstract
2021-440	Results show that NNGP can obtain competitive results comparing to their BNN counterpart, and results on the proposed  NNGP-LL approach provides promising supports on the hybrid design as to combine the best from both GP and deep learning fields. <sep>	I-abstract
2021-440	Although the proposed approach is a natural extension of the recent line of work on GP-BNN correspondence, reviewers agreed that the paper presented a good set of empirical studies, and the NNGP-LL approach, evaluated in section 5 with SOTA deep learning architectures, provides a promising direction of future for scalable uncertainty estimation.	B-strength
2021-440	This is the main reason that leads to my decision on acceptance. <sep>	B-decision
2021-440	Concerns on section 3's results on under-performing CNN & NNGP results on CIFAR-10 has been raised, which hinders the significance of the results there (since they are way too far from expected CNN accuracy).	B-weakness
2021-440	The compromise for model architecture in order to enable NNGP posterior sampling is understandable, although this does raise questions about the robustness of posterior inference for NNGP in large architectures. <sep>	I-weakness

2021-453	The paper proposes a method for SLAM like dense 3D mapping (colored occupancy grid) based on differentiable rendering with a possibility to provide a probabilistic generative predictive distribution, evaluated on UAVs. <sep>	B-abstract
2021-453	Initially this paper has a wide spread of reviews, with ratings between 4 and 9.	B-rating_summary
2021-453	Reviewers appreciated the elegant and principled formulation and the interest of the predictive distribution.	B-strength
2021-453	On the downside, several issues were raised on the incremental nature wrt to DVBF-LM; presentation and writing being very dense and difficult to follow; positioning wrt to prior art; performance with respect to known visual SLAM SOTA baselines; limited evaluations. <sep>	B-weakness
2021-453	The authors provided responses to many of this questions and also updates to the paper, which convinced several reviewers, who unanimously recommended acceptance after discussion. <sep>	B-rating_summary
2021-453	The AC concurs. <sep>	B-decision

2021-459	This work develops a novel framework for online continual learning, which they authors name Contextual Transformation Networks (CTN).	B-abstract
2021-459	This framework comprises a base network, which learns to map inputs to a shared feature representation, and a controller that efficiently transforms this shared feature vector to task specific features given a task identifier.	I-abstract
2021-459	Both of these components have access to their own memory.	I-abstract
2021-459	The optimization of the both the controller and base network parameters is framed as a bi-level optimization framework. <sep>	I-abstract
2021-459	Pros: <sep> important and challenging problem <sep> 	B-strength
2021-459	strong results<sep>	I-strength
2021-459	Cons: <sep> Currently the writing creates the impression of limited novelty from a technical perspective.	B-weakness
2021-459	I would encourage the authors to more crisply highlight the technical novelty of their method.<sep>	I-weakness

2021-468	The reviewers raised a number of concerns about the novelty of the paper and comparisons.	B-weakness
2021-468	The authors were able to address the concerns regarding the comparisons in the response, and the reviewers unanimously agree that the paper should be published.	B-rating_summary
2021-468	I do think however that this paper is quite borderline.	O
2021-468	I agree with the reviewers that the updated experiments are convincing in terms of the provided comparisons.	B-rebuttal_process
2021-468	However, the reservations I have about the work can perhaps best be stated as follows: There is quite a bit of work in the area of imitation from observations, which makes a range of different assumptions and utilizes a variety of different domain adaptation techniques.	B-weakness
2021-468	Much of this work is in the robotics domain (which is cited in the paper), and much of it demonstrates results in fairly realistic settings, often with real humans and real robots.	I-weakness
2021-468	In comparison, the experiments in this paper are quite simplistic, using toy domains and "demonstrations" obtained from a computational oracle (ie, another policy).	I-weakness
2021-468	Given the maturity of this field and the current state of the art, I am skeptical of this evaluation, and I think TPIL is a very weak baseline.	I-weakness
2021-468	That said, I would  defer to the reviewers in this case -- I do think the particular technical contributions that the paper makes are a valuable addition to the literature, though somewhat incremental.	B-strength
2021-468	I am also sympathetic to the authors in that much of the more successful prior work in this area that does evaluate under realistic conditions makes subtly different assumptions, or utilizes different techniques for which it is difficult to provide an apples-to-apples comparison. <sep>	O
2021-468	One thing I would request of the authors for the camera ready though is: Please tone down the claims.	B-suggestion
2021-468	"Human-like 7 DOF Striker" is not human-like, it's a crudely simulated robotic arm that was recolored.	I-suggestion
2021-468	It would of course be better to have a realistic evaluation (as many prior papers in this field indeed have), but in the absence of that, it is best not to overclaim and be upfront that the evaluation is on relatively simple simulated tasks under conditions that are not necessarily realistic (and have nothing to do with actual humans), but meant rather to evaluate in an apples-to-apples manner the particular algorithmic innovations in the method. <sep>	I-suggestion

2021-472	Three out of four reviewers are positive about the paper after the author response and during the discussion. <sep>	B-rating_summary
2021-472	Strengths include<sep> The proposed method for parameter reduction in transformers allows end-2-end learning cross-modal representations especially on long videos, which has not been possible before <sep>	B-strength
2021-472	Good performance on audio and video understanding <sep>	I-strength
2021-472	Extensive set of ablations<sep>	I-strength
2021-472	Concerns include a somewhat incremental nature of the paper and the still large computational resources to run the experiments. <sep>	B-weakness
2021-472	I think, both, the ideas and results are interesting to the community and recommend accept. <sep>	B-decision

2021-486	This paper presents two new representation learning tasks (losses) based on contrastive learning that---when combined with a language modeling loss---result in a better multilingual model.	B-abstract
2021-486	Experiments on machine translation and XTREME demonstrate the benefit of the proposed method compared to strong baselines. <sep>	I-abstract
2021-486	I think this is an interesting paper that advances multilingual representation learning.	B-strength
2021-486	The authors have incorporated many suggestions from the reviewers to improve the paper during the rebuttal period.	B-rebuttal_process
2021-486	I recommend to accept the paper, but also strongly suggest the authors to make an official submission to XTREME to validate their results. <sep>	B-decision

2021-505	The paper shows that using graph neural networks to address multi-task control problems with incompatible environments does not provide benefits to the learning process.	B-abstract
2021-505	The authors instead propose to use Transformers as a simpler mechanism to be able to train and discover the helpful morphological distinctions between agents in order to better solve multitask reinforcement learning problems. <sep>	I-abstract
2021-505	The paper is well written and the analysis of the literature has been appreciated.	B-strength
2021-505	The contribution is original and relevant to the community. <sep>	I-strength
2021-505	All the reviewers agree that this paper deserves acceptance.	B-rating_summary
2021-505	We invite the authors to modify the paper by following the suggestions provided by the reviewers.	B-suggestion
2021-505	In particular: <sep> improve the analysis of the empirical results <sep> 	I-suggestion
2021-505	update the plots <sep>	I-suggestion
2021-505	add the suggested references<sep>	I-suggestion

2021-515	First as a procedural point, the paper got 7, 7, 5, 5.	B-rating_summary
2021-515	AnonReviewer3 gave it a 5, but seemed satisfied by the discussion and promised to raise their score.	B-rebuttal_process
2021-515	They did not do so, but I must interpret their last messages as indicating they now support the paper.	I-rebuttal_process
2021-515	AnonReviewer2, the other 5, had some concerns that other reviewers seem to have helped address during rebuttal.	I-rebuttal_process
2021-515	They did not update their score, but were happy to leave their certainty low and defer to other reviewers' recommendation.	I-rebuttal_process
2021-515	As such, although the average score looks low in the system, the paper is of an acceptable standard according to reviews. <sep>	B-rating_summary
2021-515	The paper adapts a method from tabular RL to Deep RL, allowing (as the title aptly says), agents to learn What to do by simulating the past.	B-abstract
2021-515	Reviewers speaking in support of the paper found that the paper was clear and sound in its evaluation, providing interesting results and a useful and reusable method.	B-strength
2021-515	It is my feeling that after discussion, the case for the paper has been clearly made, and in the absence of any strong objections from the reviewers, I am happy to go with the consensus and recommend acceptance. <sep>	B-decision

2021-520	The paper proposes a variant of Kanerva Machine Wu et al (2018) by introducing a spatial transformer to index the memory storage and Temporal Shift Module Lin et al, (2019).	B-abstract
2021-520	The KM++ model learns to encode an exchangeable sequence locally via the spatial transformer.	I-abstract
2021-520	The proposed method is evaluated on conditional image generation tasks.	I-abstract
2021-520	The empirical results demonstrated the nearby keys in the memory encoded related and similar images.	I-abstract
2021-520	Several issues of clarity and the correctness of the main theoretical result were addressed during the rebuttal period in a way that satisfied the reviewers.	B-rebuttal_process
2021-520	The basic ideas in the paper are interesting to both the machine learning and the wider cognitive science communities.	B-strength
2021-520	However, additional experiments should be included in Table 1 to complete the "DKM w/TSM (our impl)" row on Fashion MNIST, CIFAR-10, and DMLab in the final revision for completeness. <sep>	B-weakness

2021-527	This paper presents work on efficient video analysis.	B-abstract
2021-527	The reviewers appreciated the clear formulation and effective methodology.	B-strength
2021-527	Concerns were raised over empirical validation.	B-weakness
2021-527	The authors' responses added additional material that assisted in clarifying these points.	B-rebuttal_process
2021-527	After the discussion the reviewers converged on a unanimous accept rating.	B-rating_summary
2021-527	The paper contains solid advances in efficient inference for video analysis and is ready for publication. <sep>	B-decision

2021-528	The paper proposes a novel method for greed layer-wise training by considering the learning signal from either backprop or from the additional auxiliary losses.	B-abstract
2021-528	SEarching for DecOupled Neural Architecture learns to identify the decoupled blocks by learning the gating parameters similar to gradient-based architecture search algorithms, such as DARTs.	I-abstract
2021-528	The empirical experiments demonstrated the effectiveness of SEDONA on CIFAR and TinyImageNet using various ResNet architectures.	I-abstract
2021-528	Several issues of clarity and the correctness of the main theoretical result were addressed during the rebuttal period in the way that satisfied the reviewers.	B-rebuttal_process
2021-528	The ideas in this paper are interesting and are broadly applicable.	B-strength
2021-528	Additional experiments / discussions on the tradeoff between initial search cost and accuracy should be included in the final version. <sep>	B-suggestion

2021-539	This paper proposes an approach to data augmentation to train image recognition models called SaliencyMix, which involves pasting salient regions (as judged by some saliency detector) of one image into another, and mixing the two labels accordingly.	B-abstract
2021-539	Most reviewers generally agreed that the proposed approach is simple -- it is easy to understand the method and its motivation, especially in the context of related augmentation approaches like CutMix -- and has solid experimental results demonstrating its effectiveness. <sep>	B-strength
2021-539	The main objection the more negative reviewers had to the work is a perceived lack of novelty.	B-weakness
2021-539	In my view it is a new method (even if similar to prior work like CutMix), and as AR5 argues: "this approach probably starts to get to the heart of why these previous strategies work: they are probably less effective ways of doing what this paper suggests."	B-ac_disagreement
2021-539	The improvements in Table 1, columns 1 & 3 (CIFAR-10 & CIFAR-100) especially speak to this -- these improvements with traditional augmentation disabled are quite substantial, even though the differences become marginal when moving to the "+" augmented versions of the dataset (as well as in ImageNet).	I-ac_disagreement
2021-539	So although the method is indeed similar to CutMix, I agree that it offers valuable insight into why these previous methods work.	I-ac_disagreement
2021-539	Besides which the results do show improvements over similar methods, even if the improvements are marginal. <sep>	I-ac_disagreement
2021-539	Overall, I recommend accepting the paper as it provides useful insight into why prior methods work and proposes a new one that in practice works slightly better.	B-decision
2021-539	Minor comments for the camera-ready version: <sep> Please revise the writing based on AR4's good suggestions. <sep>	B-suggestion
2021-539	Highlighting a comment from AR4: <sep> BAsNet for example, was trained on 10k images.	I-suggestion
2021-539	Why not simply include these (and their mask) as part of the pretraining when considering some of the baselines ?<sep>	I-suggestion
2021-539	I recommend including discussion of this important point in the final version of the paper.	I-suggestion
2021-539	The learning-based approaches are effectively using additional training data.	B-strength
2021-539	It's good that a non-learning-based method happens to perform best so that the results remain comparable with prior work, but this should nonetheless be discussed if the learning-based approaches are to be included. <sep>	B-suggestion
2021-539	Please remove the blue text coloring (if not already planned -- I'm not sure if this was done as a "diff" for the response).<sep>	I-suggestion
2021-539	Figure 3(a-b) show that Montabone & Soto (2010) performs better on both the datasets and the effects are identical on CIFAR10 and ImageNet datasets<sep>	B-weakness
2021-539	I do not see how Figure 3 shows this.	I-weakness
2021-539	Is "OpenCV Saliency" in the figure using the method from Montabone & Soto (2010)?	I-weakness
2021-539	Please clarify this by making the connection between the bar labels in the figure and the discussion in the text clear for the camera-ready version of the paper. <sep>	B-suggestion

2021-557	The paper proposes a new teacher-student framework where the teacher network guides the student network in learning useful information from trajectories of a dynamical system.	B-abstract
2021-557	The proposed framework is inspired by the Knowledge Distillation method.	I-abstract
2021-557	The teacher learns what information should be used from the trajectories and distills this information for the student in the form of target activations.	I-abstract
2021-557	In a nutshell, the framework allows the student to interpolate between model-based and model-free approaches in an automated fashion.	I-abstract
2021-557	Experimental evaluation on both the hand-crafted and simulated tasks demonstrate the effectiveness of the proposed framework.	I-abstract
2021-557	The reviewers had borderline scores in their initial reviews and raised several questions for the authors.	B-rating_summary
2021-557	The reviewers appreciated the rebuttal, which helped in answering their key questions -- I want to thank the authors for engaging with the reviewers during the discussion phase.	B-rebuttal_process
2021-557	The reviewers have an overall positive assessment of the paper, and believe that the proposed teacher-student framework is novel and potentially useful for many real-world problems.	B-rating_summary
2021-557	The reviewers have provided detailed feedback in their reviews, and I would like to strongly encourage the authors to incorporate this feedback when preparing the final version of the paper. <sep>	B-suggestion

2021-560	The paper presents a theoretical analysis of the expressive power of equivariant models for point clouds with respect to translations, rotations and permutations.	B-abstract
2021-560	The authors provide sufficient conditions for universality, and prove that recently introduced architectures, eg Tensor Field Networks(TFN), do fulfil this property. <sep>	I-abstract
2021-560	The submission received positive reviews ; after rebuttal, all reviewers recommend acceptance and highlight the valuable paper modifications made by the authors to clarify the intuitions behind the proofs. <sep>	B-rating_summary
2021-560	The AC also considers that this paper is a solid contribution for ICLR, which will draw interest for both theoreticians and practitioners in the community.	B-decision
2021-560	Therefore, the AC recommends acceptance. <sep>	I-decision

2021-561	The paper focuses on the task of learning efficient representation models for video classification.	B-abstract
2021-561	To avoid the excessive computational cost of performing 3D convolutions on video, the authors propose to break the channel dimension of video representations into sub-dimensions that are treated separately.	I-abstract
2021-561	This cuts down on computation and improves classification performance over many methods in the literature.	B-strength
2021-561	Extensive experiments were run on well-known benchmarks to justify the claims of the model.	B-abstract
2021-561	Such backbone architectures can be very useful in the realm of video understanding.	B-strength
2021-561	The authors should be commended for the amount of work they did in the rebuttal period to address the comments and inquiries brought up by the reviewers.	B-rebuttal_process
2021-561	Extra experiments were done and more in-depth analysis was made possible. <sep>	I-rebuttal_process

2021-562	This paper was unanimously rated above the acceptance threshold by the reviewers.	B-rating_summary
2021-562	While all reviewers agree it is worth accepting, they differed in their enthusiasm.	I-rating_summary
2021-562	Most reviewers agree that major limitations of the paper include that the paper provides no insight into why Dale's principle exists and the actual results are not truly state-of-the-art.	B-weakness
2021-562	Nevertheless there is agreement that the paper presents results worth publicizing to the ICLR audience.	B-rating_summary
2021-562	The comparison of the inhibitory network to normalization schemes is interesting. <sep>	B-strength
2021-562	Also, please reference the Neural Abstraction Pyramid work. <sep>	B-suggestion

2021-570	The paper proposed weighted-MOCU, a novel objective-oriented data acquisition criterion for active learning.	B-abstract
2021-570	The propositions are well-motivated, and all reviewers find the analysis of the drawbacks of several popular myopic strategies (eg ELR tends to stuck in local optima; BALD tends to be overly explorative)) interesting and insightful.	B-strength
2021-570	Reviews also appreciate the novelty of the proposed weighted strategy for addressing the convergence issue of MOCU-based approaches.	I-strength
2021-570	Overall I share the same opinions and believe the paper offers useful insights for the active learning community. <sep>	I-strength
2021-570	In the meantime, there were shared concerns among several reviewers in the readability (structure and intuition), lack of empirical results on more realistic active learning tasks, and limited discussion on the modeling assumptions.	B-weakness
2021-570	Although the rebuttal revision does improve upon many of these points, the authors are strongly encouraged to take into account the reviews, in particular, to further strengthen the empirical analysis and discussions, when preparing a revision. <sep>	B-suggestion

2021-572	This paper proposes a new method for learning a model for spatio-temporal data described by an (unknown) spatio-temporal PDE.	B-abstract
2021-572	The model learns a continuous time PDE using the adjunct method and uses graph networks to perform message passing between different discrete time steps on a grid obtained with Delaunay triangulation. <sep>	I-abstract
2021-572	The method initially 3 favorable and 1 unfavorable ratings, but convincing responses to some of the raised issues led to unanimous recommendations for acceptance (not all reviewer feedback after the rebuttal has been made public, but feedback has been made to the privately AC on these issues by different reviewers). <sep>	B-rating_summary
2021-572	The reviewers appreciated novelty of the method and numerous ablations. <sep>	B-strength
2021-572	Initially perceived weaknesses were some key experiments on generalization over different grid discretizations; the simplicity of some experiments, and links to different prior art - many of these points have been dealt with by authors in their response. <sep>	B-rebuttal_process
2021-572	The AC concurs and proposes acceptance. <sep>	B-decision

2021-573	This paper analyses the signal propagation through residual architectures; then suggests a scaling method which, together with weight standardization, allows to train such networks to high accuracy with batch-norm; it demonstrates that the method performs better than previous methods (Fixup, SkipInit), and can be used on more advanced architectures. <sep>	B-abstract
2021-573	The reviewers initially had several concerns, but after the author's revision, these concerns were addressed and most reviewers recommended acceptance.	B-rebuttal_process
2021-573	One reviewer did not respond, but I think these concerns were addressed.	I-rebuttal_process
2021-573	I think it will help to further convince the readers on the usefulness of the method readers if the authors would check the sensitivity to the learning rate with the current method and compare with other methods (SkipInit, Fixup, BN).	B-suggestion
2021-573	The reason I'm suggesting this is that I think one of the main reasons BN is still in popular use is that it commonly tends to make training more robust to changes in hyper-parameters, such as the learning rate (while other methods, like SkipInit and Fixup, require more hyper-parameter tuning). <sep>	I-suggestion
2021-573	Overall the analysis and the suggested method seem useful, especially at a small batch size and the writing is mostly clear, so I recommend acceptance. <sep>	B-decision

2021-589	The authors present a new theoretical framework that establishes that any network can be approximated by pruning a polynomially larger random binary networks, and also an algorithm for pruning binary nets.	B-abstract
2021-589	The results are important in the general context of the "strong" lottery ticket hypothesis, and are of both theoretical and practical interest.	B-strength
2021-589	Although some of the ideas and technical contributions can be seen as a combination of prior tools and algorithms, the experimental findings are very novel.	I-strength
2021-589	Some further concerns of clarity and novelty were addressed by the authors. <sep>	B-rebuttal_process

2021-605	The authors propose an intriguing alternative to IFT or unrolled GD as a method for optimizing through arg min layers in a neural net, by using a differentiable sampling-based optimization approach.	B-abstract
2021-605	I found the general idea in the paper to be intriguing and thought-provoking.	B-strength
2021-605	The reviewers generally seem to have also appreciated the method, and many of the reviewers' concerns were addressed by the authors during the rebuttal.	B-rebuttal_process
2021-605	Although the paper does have a number of flaws -- in particular, the evaluation is a bit hard to appreciate, since improvement over prior work is either unclear, or no meaningful comparison is offered, -- I think in this case the benefits outweigh the downsides.	B-weakness
2021-605	The work is far from perfect, but the ideas that are presented are interested and valuable to the community, and I think that ICLR attendees will appreciate learning about this work.	B-decision
2021-605	I would encourage the authors however to improve the paper, and especially the empirical evaluation, as much as possible for the camera-ready, and to take reviewer comments into account insofar as feasible.	B-suggestion
2021-605	I'm also not sure how much I buy the "overfitting to hyperparameters" argument for unrolled GD, and a less charitable interpretation is that the authors present this issue largely to make up for the comparative lack of other benefits.	I-suggestion
2021-605	That's not necessarily a bad thing, but I think making such a big deal of it is a bit strange.	I-suggestion
2021-605	It's probably fair to say at this stage that the actual benefits of this approach are a bit modest (though improvements in runtime are a good thing...), but the idea is interesting, and may spur future research. <sep>	I-suggestion

2021-624	This paper provides a clear and useful empirical study of how the initialization scale and activations function affects the generalization capability of neural networks.	B-abstract
2021-624	Previous works showing the effect of the initialization scale (Chizat and Bach (2018), Geiger et al (2019), Woodorth et al (2020)) had a more limited set of experiments.	I-abstract
2021-624	Moreover, here an extreme case is shown, wherewith sin activation function no generalization is possible at a large init scale (there the kernel regime is useless for generalization since the hidden layer output becomes very sensitive to any small perturbation in the input).	I-abstract
2021-624	Lastly, two alignment measures are suggested, which are correlated with the generalization across several architectures and initialization scales. <sep>	I-abstract
2021-624	All the reviewers argued for acceptance, and one strongly so.	B-rating_summary
2021-624	I agree that the paper is sufficiently interesting and clear to be accepted.	B-decision
2021-624	However, despite the high scores, I only recommend a poster and not spotlight/oral: I think the novelty of the empirical study is not groundbreaking, given the experiments in previous works, and the usefulness of the suggested measures are not completely clear without a thorough comparison against previously suggested measures. <sep>	I-decision

2021-632	This paper proposes a method to cope with large vocabulary sizes.	B-abstract
2021-632	The idea is to find a small number of anchor words and to express every other word as a sparse nonnegative linear combination of them.	I-abstract
2021-632	They give an end-to-end method for training, and give a statistical interpretation of their algorithm as a Bayesian nonparametric prior (in particular an Indian restaurant process).	I-abstract
2021-632	They give extensions that allow them to deduce the optimal number of anchors which allows them to avoid needing to tune this hyperparameter.	I-abstract
2021-632	Finally they give a variety of experiments, particularly in language and recommendation tasks.	I-abstract
2021-632	The results on language are particularly impressive, and in the author response period, at the behest of a reviewer, they were able to extend the experiments to the Amazon Review dataset which contains 233M reviews on 43.5 M items by 15.2 M users. <sep>	B-rebuttal_process
2021-632	This paper is a nice combination of a simple but powerful idea, and a range of experiments demonstrating its utility.	B-strength
2021-632	Other papers have proposed related ideas, but here the main novelty is in (1) using a small number of anchors that can incorporate domain knowledge and (2) using a sparse linear transformation to express other words in this basis.	I-strength
2021-632	One reviewer did not find the Bayesian nonparametric interpretation to be fruitful, since it does not lead to techniques for handling growing datasets (eg if the ideal number of anchors changes over time). <sep>	B-weakness

2021-635	The paper provides a study of the impact of preconditioning/second-order methods on generalization by giving a precise analysis in tractable regression settings. <sep>	B-abstract
2021-635	It illustrates conditions under which preconditioning might be useful for better generalization. <sep>	I-abstract
2021-635	The readability issues raised by the reviewers have been taken into account, as well as some missing references, except Wu, D. and Xu, J.	B-weakness
2021-635	"On the Optimal Weighted Regularization in Overparameterized Linear Regression" NeurIPS 2020, raised by reviewer (though it is a really recent reference). <sep>	O
2021-635	Overall the contributions are significant enough to accept the paper for publication. <sep>	B-decision

2021-646	I agree with the reviewers' comments.	O
2021-646	The technique proposed in the paper is very interesting, and although the method itself is not particularly surprising (it's "just" chaining two compressors), it's a really nice way of framing and studying the problem.	B-strength
2021-646	On the other hand, the experiments are relatively weak, and I think there is significant potential for improvement here (especially with an added 9th page of text).	B-weakness
2021-646	I encourage the authors to add some more convincing experiments in future versions of the paper. <sep>	B-suggestion

2021-647	This paper is a variant of the large growing class of Neural ODEs, and adds dependency on a time delay to the baseline, which allows to model a larger class of physical systems, in particular adding the possibility of crossing paths in phase space. <sep>	B-abstract
2021-647	After initial evaluation, the paper was on the fence, with 2 reviewers providing favorable reviews, and 2 reviewers recommending rejection.	B-rating_summary
2021-647	A particular important issue raised was positioning with respect to prior art, [Dupont 2019], with some substantial overlap between the papers; requests of theoretical discussions of the class of studied systems and its properties. <sep>	B-weakness
2021-647	Most of these remarks have been addressed by the authors, in particular positioning and experimental comparisons. <sep>	B-rebuttal_process
2021-647	The AC judged that the paper had been sufficiently improved and recommends acceptance. <sep>	B-decision

2021-652	This paper proposes to enhance the robustness of RL and supervised learning algorithms to noise in the observations by dropping input features that are irrelevant for the task.	B-abstract
2021-652	It relies on the information bottleneck framework (well derived in the paper) and learns a parametric compression of the input features that sets them to zero if they are not relevant for the taskn.	I-abstract
2021-652	The method is extensively evaluated on several RL tasks (exploration in VizDoom and DMLab with a noisy "TV" distractor) and supervised tasks (ImageNet or CIFAR-10 classification with noise). <sep>	I-abstract
2021-652	Reviewers have praised the idea, derivation and writing, as well as the extensive experiments on RL and supervised tasks.	B-strength
2021-652	Critique focused on: <sep> the contrived nature of the TV noise (localised always in the same corner of the image -- a standard evaluation according to the authors), <sep> 	B-weakness
2021-652	lack of comparison with other feature selection methods, <sep>	I-weakness
2021-652	lack of comparison with Conditional Entropy Bottleneck (done during rebuttal), <sep>	I-weakness
2021-652	more general noise than just specific pixels (clarified by the authors as being the features coming out of a convnet)<sep>	I-weakness
2021-652	Given that the reviewers' comments were largely addressed by the authors, and given the final scores of the paper, I will recommend acceptance. <sep>	B-decision

2021-654	The paper presents a novel method for learning with noisy labels based on an interesting insight into the learning dynamics of deep neural networks. <sep>	B-abstract
2021-654	Reviewers unanimously vote for acceptance.	B-rating_summary
2021-654	I agree with their assessment, and it is my pleasure to recommend the paper for acceptance. <sep>	B-decision
2021-654	If I can draw attention to one comment, I strongly agree with R1 that the criterion in eq (3) is somewhat poorly motivated.	B-weakness
2021-654	I believe the paper would benefit from a clearer exposition of this part. <sep>	I-weakness
2021-654	Please make sure to address all reviewers' remarks in the camera-ready version.	B-suggestion
2021-654	Thank you for submitting your work to ICLR. <sep>	O

2021-666	The authors proposed a meta learning framework for NAS, namely MetaD2A (Meta Dataset-to-Architecture), that can stochastically generate graphs (architectures) from a given set (dataset) via a dataset-architecture latent space learned with amortized meta-learning.	B-abstract
2021-666	Each dataset is encoded via a set encoder and the architecutres are obtained via a graph decoder.	I-abstract
2021-666	MetaD2A is trained once on a database consisting of datasets and pretrained networks and can rapidly search a neural architecture for a novel dataset.	I-abstract
2021-666	While the set encoder and graph decoder for NAS have been introduced by existing work, the main contribution of the paper is to show that the meta-learning of a "dataset-conditioned architecture generation" framework can enable fast generation of a good architecture without training on the target dataset.	I-abstract
2021-666	The proposed method is interesting and effective, however it requires an existing pool of good architectures for a given task, which may limit its applicability to a diverse set of real-world problems.	B-weakness
2021-666	I strongly encourage the authors to include experiments on a larger pool of architectures than the NAS-Bench-201 search space to show the strength of their proposed method in generating good architectures.	B-suggestion
2021-666	While training MetaD2A with pairs of MetaImageNet and randomly sampled graph shows that the proposed framework can generate graphs with different types of edges, it doesn't show that it can successfully meta-learn to produce better architectures for a new task from an existing pool of good architectures. <sep>	B-weakness
2021-666	We believe that many of the reviewers comments were addressed in the rebuttal, so while the scores are low, they do not reflect neither the contribution nor the reviewers opinion well (eg, R3, in his last post, seems to suggest that his review should be updated but it has not happened). <sep>	B-rebuttal_process

2021-671	The authors present a Bayesian approach for context aggregation in neural processes based models.	B-abstract
2021-671	The article is well written, and provides a nice and comprehensive framework.	B-strength
2021-671	The reviewers raised some issues regarding the lack of comparisons to proper baselines.	B-weakness
2021-671	The authors provided additional comparisons in the revised version.	B-rebuttal_process
2021-671	The comparisons were found satisfactory by some some reviewers, who increased their scores.	I-rebuttal_process
2021-671	Based on the revised version, I recommend acceptance. <sep>	B-decision

2021-672	This paper revisits the under-explored "implicit" variant of Variational Intrinsic Control introduced by Gregor et al They identify a flaw that biases the original formulation in stochastic environments and propose a fix. <sep>	B-abstract
2021-672	Reviewers agree that there is a [at least a potential, R4] contribution here: "even the description of what implicit VIC is trying to do is a novel contribution of this work", in the words of R2, and "the derivation has theoretical value and is not a simple re-derivation of VIC", in R4's post-rebuttal remarks.	B-strength
2021-672	Several reviewers raised significant concerns around clarity, which were addressed in an updated manuscript, which also provided new visualizations and new experiments which reviewers found compelling.	B-rebuttal_process
2021-672	All reviewers agreed that the revised manuscript was considerably improved. <sep>	I-rebuttal_process
2021-672	R4's score stands at the 5, with the other reviewers all standing at 6.	B-rating_summary
2021-672	R4's main concerns are around whether the missing term in the mutual information identified by the authors is a problem in practice on non-toy tasks (echoing somewhat R3's concerns re: high-dimensional tasks).	B-weakness
2021-672	While this is a valid concern, the function of a conference paper needn't necessarily be to (even attempt) to provide the final word on a matter.	B-ac_disagreement
2021-672	Identifying subtle issues such as the one brought forth in this manuscript and re-examining old ideas is a valuable service to the community, and this paper will serve as a beginning to a conversation rather than an end.	B-strength
2021-672	The AC also considers themselves rather familiar with the original VIC paper, and found the results herein somewhat surprising and noteworthy. <sep>	I-strength
2021-672	I recommend acceptance, but encourage the authors to incorporate remaining feedback in the camera-ready. <sep>	B-decision

2021-680	Dear authors, <sep>	O
2021-680	all reviewers found many interesting contributions in your paper and also pointed out some minor/major issues.	O
2021-680	In your rebuttal discussions, you addressed most of them to their satisfaction and I hope you will incorporate them carefully also in your final submission. <sep>	B-rebuttal_process
2021-680	I hence recommend accepting this paper <sep>	B-decision

2021-682	The paper analyses several approaches to pruning at initialization, compared to after training.	B-abstract
2021-682	There was a large gap in reviewers appreciation of the paper, but I think that the pros outdo the cons as the paper show a lot of insights overall.	B-strength
2021-682	I recommend accepting the paper. <sep>	B-decision

2021-684	This paper presents a new, large-scale, open-domain dataset for on-screen audio-visual separation, and provides an initial solution to this task.	B-abstract
2021-684	As the setting is quite specialized, the authors proposed a neural architecture based on spatial-temporal attentions (while using existing learning objective for audio separation).	I-abstract
2021-684	The reviewers were initially concerned that, while reasonably motivated, the architecture seemed some arbitrary.	B-rebuttal_process
2021-684	The authors then provided extensive ablation studies to evaluate the significance of each component with existing datasets, and these efforts are appreciated by reviewers.	I-rebuttal_process
2021-684	The authors may consider re-organizing the paper and moving some ablation studies to the main text.	B-suggestion
2021-684	On the other hand, the reviewers believe that the dataset will be very useful for the community due to its diversity in content and label quality. <sep>	B-strength

2021-687	The paper is about adapting a voice generation model to new speakers with minimal amount of training data.	B-abstract
2021-687	The key insight in this paper is that the voice can be adapted using a small set of variables -- the bias and the variance associated with the layer that normalizes the mel-spectrogram associated with the decoder.	I-abstract
2021-687	Additionally, they characterize voice at the utterance level to capture stationary factors like background acoustic conditions and at the phoneme level to capture factors such as prosody, though there are no explicit constraints to force such representation. <sep>	I-abstract
2021-687	The strength of the paper are: <sep> Simplicity of the approach <sep> 	B-strength
2021-687	Empirical evaluation that demonstrates its effectiveness<sep>	I-strength
2021-687	The weakness of the paper are: <sep> analysis of what the crucial parameters of the model represent <sep> 	B-weakness
2021-687	lack of clarity that is obvious from several back-and-forths between the reviewers and the author.<sep>	I-weakness
2021-687	A few examples include: <sep> "There is also a phoneme-level acoustic embedding which is used in the same way, which at inference is taken from random sentences (why not in training?	I-weakness
2021-687	), and, I guess, is supposed to cover phoneme-level idiosyncrasies of the speaker, although this isn't clear to me." <sep>	I-weakness
2021-687	" it is only the speaker embedding that is the input to fine-tuning, and this only via the normalization parameters.	I-weakness
2021-687	(Both the normalization parameters and the speaker embedding itself are fine-tuned.)	I-weakness
2021-687	I am not sure what the speaker-embedding is left to do with all this acoustic-level input, but OK."<sep>	I-weakness

2021-688	The reviewers had a number of concerns which seem to have been addressed by the authors in the discussion phase.	B-rebuttal_process
2021-688	All the reviewers are in favor of accepting the paper.	B-rating_summary
2021-688	The paper provides an interesting/novel idea for federated learning with heterogenous clients/devices. <sep>	B-strength

2021-699	The paper considers the problem of learning interpretable, low-dimensional representations from high-dimensional multimodal input via weak supervision in a learning from demonstration (LfD) context.	B-abstract
2021-699	To mitigate the disparity between the abstractions that humans reason over and the robot's low-level action and observation spaces, the paper argues for learning a low-dimensional embedding that captures the underlying concepts.	I-abstract
2021-699	The primary contribution of the paper is the ability to learn disentangled low-dimensional representations that are interpretable from weak supervision using conditional latent variable models. <sep>	I-abstract
2021-699	The paper was reviewed by three knowledgeable referees, who read the author response and discussed the paper.	O
2021-699	The paper considers a challenging problem in learning from demonstration, namely dealing with the disparity that exists between the ways in which humans and robots model and observe the world, a problem that is exacerbated when reasoning over high-dimensional multimodal observations.	B-abstract
2021-699	As the reviewers note, the use of variational inference to learn low-dimensional interpretable representations from weak supervision is compelling.	B-strength
2021-699	The primary concerns are that the contributions need to be more clearly scoped and that the experimental evaluation is a bit narrow.	B-weakness
2021-699	The authors make an effort to resolve some of these issues, in part through the inclusion of an additional experiment that considers pouring tasks.	B-rebuttal_process
2021-699	However, the extent to which this second task mitigates concerns about the narrow evaluation is not fully clear.	I-rebuttal_process
2021-699	The paper would be strengthened by the inclusion of experiments in a less contrived setting (and one for which the concepts are not necessarily disjoint) as well as a clearer discussion of the primary contributions. <sep>	B-suggestion

2021-713	The major criticism of this paper after the initial reviews was a lack of experimental results on deeper and more modern architectures that include skip connections.	B-weakness
2021-713	The authors added results to the paper to address these issues. <sep>	B-rebuttal_process

2021-723	This paper tackles a very important topic in deep RL, namely automatic (non-differentiable) hyper-parameter tuning.	B-abstract
2021-723	It does so by combining ideas from genetic algorithms and neural architecture search with shared experience replay in order to obtain the key property of sample efficiency.	I-abstract
2021-723	The proposed solution is communicated clearly, and the results are compelling (often 10x improvements), as well as qualitatively interesting. <sep>	B-strength
2021-723	Unfortunately for the authors, their original submission contained only part of the intended results, hence the borderline scores by some reviewers.	B-rating_summary
2021-723	In the meanwhile, a second suite of experiments have been added, which I think are compelling enough evidence to validate the paper's approach. <sep>	B-rebuttal_process

2021-725	This paper takes a step towards understanding the role of nonlinear function approximation--- more specifically, function approximation via (two-layer) neural nets---in some variants of the policy-gradient algorithms.	B-abstract
2021-725	The authors borrow the mean field analysis idea recently popularized in studying shallow neural nets, and investigate the mean-field limits of the training dynamics in the current RL settings.	I-abstract
2021-725	The results and analyses are interesting as they nicely complement another line of linearization-based analyses (ie, the one based on neural tangent kernels) towards understanding non-linear function approximation.	B-strength
2021-725	As suggested by a reviewer, it would be nice to add discussions in the revised paper regarding when the dynamics can be guaranteed to converge to a stationary point. <sep>	B-suggestion

2021-727	This paper proposes a simple generalization to epsilon-greedy exploration that induces temporally extended probes and can leverage options.	B-abstract
2021-727	The idea and analysis are trivial.	B-weakness
2021-727	Computational results demonstrate when this sort of exploration is helpful.	B-abstract
2021-727	The paper is well written and the authors offer a fair assessment of when these ideas do or do not address challenging exploration tasks.	B-strength
2021-727	A range of computational results support and offer insight into the concepts. <sep>	I-strength

2021-739	This paper proposes to use high dimensional representation for labels to strengthen the adversarial robustness of deep neural networks.	B-abstract
2021-739	Experimental results demonstrate that the proposed method improve adversarial robustness.	I-abstract
2021-739	All reviewer agree that the authors propose an interesting idea and this direction deserves further exploration.	B-strength
2021-739	On the other hand, the reviewers also raise a serious question: There is a lack of explanation of why high dimensional representation of labels improve adversarial robustness.	B-weakness
2021-739	Therefore, it is not clear if the proposed method can defend refined attacks tailored to such dimensional label representation.	I-weakness
2021-739	The authors are highly encouraged to conduct deeper analysis, especially on the robustness against finer attacks. <sep>	B-suggestion

2021-742	The paper describes a cool application of online learning from bandit feedback -- creating personalized, adaptive typing interfaces for users with sensorimotor impairments.	B-abstract
2021-742	The problem is well-motivated -- the interface can observe users' gaze (eg via a webcam image), predict a character as an action, and bandit feedback can be collected by observing whether users use the backspace key after the interface's action.	I-abstract
2021-742	Prior work showed that gaze-to-text can be less burdensome than typing, but this can quickly become untrue the more mistakes the interface makes.	I-abstract
2021-742	So, the goal is to personalize the interaction policy so that it makes fewer mistakes than the default interaction policy trained using a fixed dataset of expert demonstrations. <sep>	I-abstract
2021-742	The high-point of the paper is the empirical user study with 12-60 participants -- the study convincingly demonstrates that indeed a simple bandit algorithm can improve over the default interface; moreover, users exhibit intriguing co-adaptation patterns with the adaptive interfaces.	B-strength
2021-742	These findings may prove to be an interesting point for future studies in user co-adaptation. <sep>	I-strength
2021-742	The low-point of the paper is its algorithmic development.	B-weakness
2021-742	There is a vast literature on bandit/RL algorithms, and incorporating human feedback into their operation (the paper rightly cites TAMER, COACH, etc.)	I-weakness
2021-742	but it is very unclear why any one of these algorithms could not be used for the paper's application.	I-weakness
2021-742	COACH (human feedback gives an explicit view of the action's advantage -- which in the contextual bandit setting exactly matches the paper's assumptions) seems particularly appropriate.	I-weakness
2021-742	Although the algorithm proposed in the paper is simple, how applicable is it in any other context?	I-weakness
2021-742	how does it compare to COACH/etc.?	I-weakness
2021-742	when should we prefer this algorithm over others?	I-weakness
2021-742	Furthermore, given that X2T trains a reward model from observed user-behavior, a natural baseline would use an epsilon-greedy strategy (fraction of the time, pick actions greedily according to the reward model) -- this might isolate the benefit of the approximately Boltzmann exploration being conducted on top of the reward estimates in Eqn 2.	I-weakness
2021-742	Finally, since X2T trains a reward model per user it could be particularly informative to visualize what the models have learned to illustrate qualitatively how X2T is personalizing across its user base. <sep>	B-suggestion
2021-742	The paper could have a much bigger impact if the authors can figure out some creative way to enable the broader research community to work on this problem domain.	I-suggestion
2021-742	A testbed or environment (like RecSim for content recommendation https://github.com/google-research/recsim) with configurable but realistic reward models could allow researchers to test several bandit algorithms, MDP vs CB formulations, other ways to interpret user feedback etc. <sep>	I-suggestion

2021-758	This paper considers a new and practical setting of meta-learning for out-of-domain task adaptation where a pretrained model exists but the original meta-training data is not available.	B-abstract
2021-758	The authors incorporate several ideas including deep ensembles, adversarial training and uncertainty-based step sizes, and achieve competitive performance under this particular setting. <sep>	I-abstract
2021-758	The combination of various methods appears complicated, but the authors provide detailed ablation study to show the effectiveness of each component empirically.	B-strength
2021-758	During rebuttal and discussion, they addressed many of the concerns from the reviewers.	B-rebuttal_process
2021-758	As pointed out by a reviewer, their proposed method would have a value in the domain adaptation area beyond meta-learning. <sep>	B-strength
2021-758	The remaining concern is on the somewhat ad-hoc combination of multiple methods and lack of a clear single solution for addressing the OOD few-shot learning problem.	B-rebuttal_process
2021-758	Nonetheless, the proposed methods show a convincing empirical improvement on the vanilla MAML baseline in the experiments. <sep>	B-strength

2021-761	This paper shows how multiple tasks can be encoded in a single neural network without the need for explicit modular construction for each task.	B-abstract
2021-761	The idea is very interesting and the research work presented is of high quality. <sep>	B-strength
2021-761	All the reviewers underline their interest in the presented work.	I-strength
2021-761	However, there is a deviation in the reviewers' score with half voting for acceptance and the other half for rejection.	B-rating_summary
2021-761	The main concern of the fellow reviewers with the below acceptance threshold score was the difficulty in grapsing the theory of the research presented due to the lack of important content from the main manuscript due to space limitations.	I-rating_summary
2021-761	The authors have an extended supplementary material that covers the whole magnitude of their work. <sep>	B-rebuttal_process
2021-761	I understand the reviewers' concern on how such a dense presentation does not do justice and harms the presented effort itself.	B-weakness
2021-761	However, given the edits the authors added to address the issue rasied and the interest and potential of this work - acknowledged by all the reviewers and myself I recommend acceptance.	B-decision
2021-761	This is a work of a quality I would like to keep seeing in ICLR. <sep>	I-decision

2021-766	This paper was controversial amongst the reviewers.	O
2021-766	There is clear utility to the ICLR community: a new model of grid cells based on well-known technique (SR) used frequently in ML; good science---careful analysis showing the proposed model exhibits key properties and useful in synthetic navigation domains;  such work reminds of the important concerns in natural learning systems which is relevant to those that wish to simulate and build intelligence.	B-strength
2021-766	Two of the reviewers with subject matter experience in the area advocated for acceptance. <sep>	B-rating_summary
2021-766	On the other hand, many readers of ICLR may find the paper confusing and unsatisfying as some of the reviewers did.	B-weakness
2021-766	The empirical work was limited to small domains and mostly in the form of demonstrations---a typically ICLR reader would expect a performance improvement claim or a scientific hypothesis tested by each experiment.	I-weakness
2021-766	Presented as a new algorithm for ML the paper might appear too limited and simple (eg, relying on state aggregation).	I-weakness
2021-766	The reviewers with neuro background found the paper clear and well organized, while the ML reviewers found it confusing.	I-weakness
2021-766	The relevance of the work will be limited to a smaller subset of researchers---but this is true of many ML works also.	I-weakness
2021-766	Finally, ML readers might be more familar with neuro work which propose computational models and then validate those models against real neural activity data from brains.	I-weakness
2021-766	This is work is not like that, rather using synthetic data to demonstrate important properties and explore empirical conjectures about the model. <sep>	I-weakness
2021-766	In the end the paper is boarder line: the subject matter experts both listed issues that should be addressed (eg, band cells issue), while the reaction of the ML reviewers suggests the impact of the work might be reduced at ICLR (compared to other venues).	I-weakness
2021-766	Additional text clearly articulating the scope and managing reader expectation could mitigate this concern, but it's not a small task to change the tone and pitch this way.	I-weakness
2021-766	Scientific conferences are about insights and understanding, this paper provides both.	B-suggestion
2021-766	Please consider the suggested edits to maximize the impact of your work at ICLR this year. <sep>	I-suggestion

2021-770	The paper provides a method for constructing PAC confidence scores for pre-trained deep learning classifiers.	B-abstract
2021-770	The reviewers were all positive about the paper. <sep>	B-rating_summary
2021-770	Pros: <sep> Has provable guarantees on the reliability of the prediction.	B-strength
2021-770	Such guarantees are quite desirable in practice. <sep>	I-strength
2021-770	The problem of neural network uncertainty is important and timely problem, especially in safety-critical applications. <sep>	I-strength
2021-770	The method is simple and well-motivated. <sep>	I-strength
2021-770	Strong empirical performance. <sep>	I-strength
2021-770	Interesting applications to fast DNN inference and safe planning.<sep>	I-strength
2021-770	Cons: <sep> Lack of generalization guarantees-- the guarantees in the paper only hold on the training set; but in practice, performance in test is what's important. <sep> 	B-weakness
2021-770	Only a handful of baselines tested against, most of which (if not all) were naive.<sep>	I-weakness

2021-773	In this paper, the authors combine ideas from SLAM (using an Extended Kalman Filter and a state with nonlinear transitions and warping) and differentiable memory networks that store a spherical representation of the state (from the ego-centric point of view of an RL agent moving in an environment) with depth and visual features stored at each pixel and dynamics transitions corresponding to warping. <sep>	B-abstract
2021-773	The main idea in the paper is very simple and elegant, but I will concur with the reviewers that the writing of the first version of the paper was extremely hard to understand and that the experimental section was too dense.	B-weakness
2021-773	Two subsequent revisions of the paper have dramatically improved the paper. <sep>	B-rebuttal_process
2021-773	Given the spread of scores (R1: 6, R2: 7 and R3: 4) and the fact that only R1 and R2 have acknowledged the revisions, I will veer towards acceptance. <sep>	B-decision

2021-778	Please clarify as early as the abstract that you refine the analysis of the algorithm proposed by Shalev-Shwartz et al (which is a great contribution given the importance of the problem). <sep>	B-suggestion

2021-784	Pros: <sep> Provides a practical technique which can dramatically speed up PDE solving -- this is an important and widely applicable contribution. <sep> 	B-strength
2021-784	Paper is simultaneously clearly written and mathematically sophisticated. <sep>	I-strength
2021-784	The experimental results as impressive.<sep>	I-strength
2021-784	Cons: <sep> There were concerns that the paper lacks novelty compared to Li et al 2020b, where the underlying theoretical framework was developed.	B-weakness
2021-784	The primary novelty would seem to be: <sep> using Fourier transforms as the specific neural operator <sep> 	B-strength
2021-784	the strength of the experimental results<sep>	I-strength
2021-784	Overall, I recommend acceptance.	B-decision
2021-784	I believe the techniques in this paper will be practically useful for future research. <sep>	B-strength

2021-785	This paper analyses the interaction between data-augmentation strategies  and model ensembles with regards to calibration performance.	B-abstract
2021-785	The authors note how strategies such as mixup and label smoothing, which reduce a single model's over-confidence, lead to degradation in calibration performance when such models are combined as an ensemble.	I-abstract
2021-785	They propose a simple solution.	I-abstract
2021-785	The paper merits publication. <sep>	B-decision

2021-801	This paper proposes "HyperDynamics" a framework that takes into account the history of an agents recent interactions with the environment to predict physical parameters such as mass and friction.	B-abstract
2021-801	These parameters are fed into a forward dynamics model, represented as a neural network, that is used for control. <sep>	I-abstract
2021-801	Pros: <sep> addresses an important problem (adapting dynamics models to "new" environments) and provides strong baselines <sep> 	B-strength
2021-801	well written and authors have improved clarity even further based on reviewers comments<sep>	I-strength
2021-801	Cons: <sep> I agree with the reviewer that it is currently unclear how well this will transfer to the real world <sep> 	B-weakness
2021-801	The idea of predicting physical parameters from a history of environment interactions is not not novel in itself (although the proposed framework is, as far as I know).	I-weakness
2021-801	The authors should include related work along the lines of (1) (this is just one paper that comes to mind, others exist)<sep>	B-suggestion
2021-801	(1) Preparing for the Unknown: Learning a Universal Policy with Online System Identification <sep>	O

2021-846	The authors study co-ordination in multi-agent systems.	B-abstract
2021-846	Specifically they propose a scheme where agents model future trajectories through the environment dynamics and other agents' actions, they then use this to form a plan which forms the agents' intention which is then communicated to the other agents. <sep>	I-abstract
2021-846	The major concerns raised by the reviewers were around novelty, lack of ablations and significance of results as improvements were modest.	B-weakness
2021-846	During the rebuttal, the authors have extended their work with ablations and have conducted a statistical test.	B-rebuttal_process
2021-846	While it is true the current results present a small improvement, i think this is an interesting contribution in the field of emergent communication <sep>	B-strength

2021-853	The paper suggests a procedure to efficiently adapting a learned neural compression model to a new test distribution.	B-abstract
2021-853	If this test distribution has low entropy (eg, a video as a sequence of interrelated frames), large compression gains can be expected.	I-abstract
2021-853	To achieve these gains, the method adapts the decoder model to the new instance, transmitting not only the data but also a compressed model update.	I-abstract
2021-853	Experiments are carried out on compressing I-frames from videos, while comparisons comprise baseline approaches that finetune the latent representations of videos as opposed to the decoder. <sep>	I-abstract
2021-853	The paper's main contribution is very timely and relevant.	B-strength
2021-853	While it was well-known in the classical compression literature that model updates could be sent along with the data (eg, as already done in "optimized JPEG"), this is the first time the idea was implemented in neural compression.	I-strength
2021-853	The experiments are arguably the paper's weaker part and were originally a concern, but they have been significantly improved during the review period such that all reviewers voted for acceptance.	B-rating_summary
2021-853	We encourage the authors to further strengthen their experimental results by adding more challenging baselines on well-established tasks (eg, image compression). <sep>	B-suggestion

2021-863	The reviewers have ranked this paper as borderline accept.	B-rating_summary
2021-863	On the negative side, the main claim of the paper (the more categories for training a one-shot detector, the better) has already been observed in several works and very intuitive.	B-weakness
2021-863	However, the paper has done significant experimental work to support this claim.	I-weakness
2021-863	The paper is very well written, it carefully explores the existing setups for one-shot detection and highlights their weaknesses.	B-strength
2021-863	The paper also gives advice on how to construct better datasets for one-shot detection (the conclusion "add more diverse categories" is somewhat obvious but the paper demonstrates how important that is). <sep>	I-strength

2021-874	This paper describes a method called 'stochastic' inverse reinforcement learning.	B-abstract
2021-874	It is somewhat unclear how this differs from other probabilistic approaches to IRL.	B-weakness
2021-874	In particular Bayesian approaches have been used in the past to obtain distributions over reward functions.	I-weakness
2021-874	However, SIRL tries to estimate a generative model over such distributions.	I-weakness
2021-874	All the reviewers foudn the paper suffering from lack of clarity, in particular with respect to how the model/algorithm is constructed.	I-weakness
2021-874	There are some possible technical problems with respect to claims about inferring demonstrations by different experts (cf.	I-weakness
2021-874	work on multi-task IRL).	I-weakness
2021-874	The experiments also seem to be insufficient. <sep>	I-weakness

2021-881	This paper studies the problem of adversarial training for graph neural networks.	B-abstract
2021-881	The proposed method is build on the free training approach, and more specifically FreeLB, with some additional tricks including bias perturbation (for node-classification) and unbounded attacks.	I-abstract
2021-881	While these additions are potentially useful, there are only limited investigation into their effect.	B-weakness
2021-881	Putting aside the technical distinctions of the method with prior work, this paper can also be viewed as an empirical study of adversarial training techniques on graph data with various GNN architectures.	B-abstract
2021-881	It is worth noting that overall the conclusions on "adversarial training" are positive, we do see consistent improvement over a variety of architectures and tasks.	B-strength
2021-881	The issues however, is that it is unclear whether these improvements can be similarly achieved using prior technique like FreeLB (oblation is only done on one single task, where biased perturbation is shown to lead to minor improvement).	B-weakness
2021-881	The paper also provides some results showing the effect of depth of the network as well as different training strategies such as batch norm, dropout with general adversarial training.	B-abstract
2021-881	These results are interesting to see but do seem to be limited in both scope and depth.	B-weakness
2021-881	It appears that the authors have two goals in mind, one is to propose FLAG and demonstrate its usefulness, and the other is to provide a better understanding of how adversarial training works for GNNs in general.	B-abstract
2021-881	Given the limited novelty of FLAG compared to prior methods, the main contribution actually comes from the later part, which unfortunately is somewhat underdeveloped. <sep>	B-weakness

2021-884	All reviewers agreed on the major shortcomings of this submission, the most important of which is that the contributions are insufficiently evaluated.	B-weakness
2021-884	There was no author response. <sep>	B-rebuttal_process

2021-892	This paper explores a network that has a parvo (fine, detailed, slow) <sep>	B-abstract
2021-892	and magno (low-res, quick) stream.	I-abstract
2021-892	The ideas are interesting and the results intriguing, and one reviewer is in favor of acceptance. <sep>	B-rating_summary
2021-892	Several reviewers criticized the clarity of the paper.	B-weakness
2021-892	and the lack of details for, explanations of, and critical evaluation of, the design decisions.	I-weakness
2021-892	For example, how do the results depend on certain design decisions?	I-weakness
2021-892	I think that with a bit more work, this paper has potential to be a very impactful paper.	O
2021-892	I would encourage the authors to follow the detailed suggestions and resubmit the work to a high-impact conference or journal. <sep>	B-decision

2021-894	One referee supports acceptance, whereas three referees lean towards rejection.	B-rating_summary
2021-894	All referees agree that the idea introduced in the paper is interesting but find that the motivation and evaluation of the proposed aggregation functions could be significantly strengthened.	B-weakness
2021-894	The rebuttal addresses R1's concerns about novelty and unfair comparisons, R2's concerns about computational efficiency of the methods, R3's concerns about motivation of the proposed approach and some missing baselines, and R4's concerns about motivation.	B-rebuttal_process
2021-894	However, the rebuttal does not address the reviewers' concerns related to improvements achieved by the proposed approach, statistical significance nor appropriate comparison with SOTA.	I-rebuttal_process
2021-894	I agree with the reviewers that the paper tries to address a relevant problem and proposes interesting ideas, which are worth exploring.	B-strength
2021-894	However, after discussion, the referees agree that further work should be devoted to strengthen the contribution.	B-rebuttal_process
2021-894	I agree with their assessment and hence must reject.	B-decision
2021-894	In particular, I would strongly recommend to follow their suggestions to either provide strong theoretical motivation to support the claims of the paper or work on a strengthened empirical evaluation, following OGB guidelines to report the std of the results and including a proper comparison with the state of the art. <sep>	B-suggestion

2021-910	The paper initially had mixed reviews (4,5,6).	B-rebuttal_process
2021-910	The main issues raised were: <sep> limited novelty (re-using/integrating components) [R2]; <sep> 	B-weakness
2021-910	limited generalization ability since the model needs to be retrained on every video [R2, R3]; <sep>	I-weakness
2021-910	limited applicability - experiments limited to certain domain of video, while results on videos with large motion are not convincing [R2, R3]; <sep>	I-weakness
2021-910	missing ablation studies / experiments [R3, R4].<sep>	I-weakness
2021-910	The author response partially addressed some concerns, but the main points 1-3 are still problematic.	B-rebuttal_process
2021-910	In addition, the AC noted that the technical aspect was lacking: <sep> 	B-weakness
2021-910	Training with contrastive loss on a single video may likely overfit the embedding to the video, which leads to a meaningless embedding where all non-neighboring segments are orthogonal in the embedding space.	I-weakness
2021-910	While changing the softmax temperature can yield higher entropy transition probabilities, the induced probability distribution is probably highly noisy.	I-weakness
2021-910	It would be better to train this on a large video corpus, which will prevent overfitting.	B-suggestion
2021-910	Also contrastive loss is typically used to build a discriminative embedding space for classification/recognition, not a smooth embedding space for generation (where distances between embedding vectors are strongly correlated to similarity).	B-weakness
2021-910	Thus some other embedding smoothness terms could be added during contrastive learning. <sep>	B-suggestion
2021-910	The learning is only on the transition probabilities, while the video generation is separate.	B-weakness
2021-910	It would have been more convincing to learn the transition probabilities with the video generation process in an end-to-end manner.	B-suggestion
2021-910	Perhaps a discriminator could be placed after the video generator so that the transition probabilities could be learned so as to better mimic real video.	I-suggestion
2021-910	Other loss terms based on video temporal smoothness could also be added ensure smoother transitions between clips (eg, motion consistency).<sep>	I-suggestion
2021-910	The negative reviewers remained unconvinced by the author response, and the AC agreed with their concerns.	B-rebuttal_process
2021-910	Thus, the paper was recommended for rejection. <sep>	B-decision

2021-935	While it's commonly acknowledged that the paper is well written, the reviews are a bit split: R3 and R1 are mildly positive/negative, respectively, R2 and R4 both voted for reject.	B-rating_summary
2021-935	R2 asked many questions regarding experiments, which were addressed in the details in the rebuttal.	B-rebuttal_process
2021-935	R4 raised 6 questions regarding the bound, and the authors only answered some of them in the rebuttal.	I-rebuttal_process
2021-935	R4 felt "the method is lacking in a theoretic proof of a strict bound, which is the primary contribution of the paper".	B-weakness
2021-935	Both R1 and R4 pointed out the proposed algorithm is not practical as expected, especially the results on larger scale such as ImageNet are missing. <sep>	I-weakness
2021-935	The AC cannot agree with the authors' argument that the contribution of the paper is "a conceptual framework that it is possible to certify a watermark for neural networks" in responding to such criticisms.	B-rebuttal_process
2021-935	It's indeed very important for this conceptual framework to be proven valuable through thorough experiments and solid comparisons. <sep>	B-suggestion

2021-936	This paper considers the problem of searching over the joint space of hardware and neural architectures to trade-off accuracy and latency. <sep>	B-abstract
2021-936	Reviewers raised some valid questions about the following aspects: <sep> 	O
2021-936	Low technical novelty <sep>	B-weakness
2021-936	Prior work on hardware and neural architecture co-design, and closely related work are not addressed <sep>	I-weakness
2021-936	Lacking details on hardware platform and discussion on physical constraints to determine invalid hardware designs (addressed somewhat, but the response is not satisfactory)<sep>	I-weakness
2021-936	One additional comment: if we care about latency for a particular hardware platform, it is possible to automatically configure adaptive inference techniques to meet the latency constraints. <sep>	B-suggestion
2021-936	Overall, my assessment is that the paper requires more work before it is ready for publication. <sep>	B-decision

2021-954	This paper proposes a weighted balanced accuracy metric to evaluate the performance of imbalanced multiclass classification.	B-abstract
2021-954	The metric is based on a one-versus-all decomposition from multi-class to binary, and then aggregating the metrics on the binary classification sub-problems in a weighted manner.	I-abstract
2021-954	The authors hope to argue that the new metric is more flexible for evaluating classifiers in the imbalanced and importance-varying setting. <sep>	I-abstract
2021-954	The reviewers agree that the proposed framework is simple and applicable to an important problem.	B-strength
2021-954	Somehow the novelty and significance of the work is pretty limited, as many related metrics (eg micro/macro-averaged metrics) exist in the literature.	B-weakness
2021-954	The authors are encouraged to think about stronger reasoning on how useful the "new" metric is.	B-suggestion
2021-954	The experiments are also not convincing nor complete enough to verify the benefits of the proposed metric. <sep>	B-weakness

2021-955	This paper is a study in optimizing the Donsker-Varadhan lower bound on mutual information focusing on a "drift" problem.	B-abstract
2021-955	The bound is a difference between terms which appears to have an extra degree of freedom where the two terms increase or decrease together.	I-abstract
2021-955	They propose a fix for this problem.	I-abstract
2021-955	The authors state that the DV bound is of practical value but in most cases it is replaced by discriminative lower bounds as in contrastive predictive coding (CPC) which are biased but have lower variance.	I-abstract
2021-955	The paper does not address the variance (convergence) issues with the DV bound. <sep>	I-abstract
2021-955	We have a well informed reviewer who feels that the paper is not sufficiently novel and has other issues supporting rejection.	B-rating_summary
2021-955	Other reviews are not very enthusiastic.	O
2021-955	I will side with rejection. <sep>	B-decision

2021-984	The paper proposes a method for data augmentation by cross-modal data generation.	B-abstract
2021-984	While the reviewers agree that the paper addresses a relevant and important problem in medical imaging, they also agree on that the paper has limited novelty over the state of the art.	B-weakness
2021-984	Also the setup of experimental validation to comparison methods is questioned. <sep>	I-weakness

2021-1028	This paper proposes an algorithm to address the disparate effect that DP has on the accuracy of minority/low-frequency sub-populations.	B-abstract
2021-1028	Unfortunately the work does not actually guarantee or analyze the resulting privacy guarantees.	B-weakness
2021-1028	In particular it may provide much worse privacy (or no privacy at all) to the minority subpopulation. <sep>	I-weakness
2021-1028	The paper also calls their algorithm "fair" without using an accepted term or a careful discussion of what an algorithm needs to satisfy to be considered "fair".	I-weakness
2021-1028	Using a more technical term such "reducing the accuracy disparity" would make much more sense. <sep>	B-suggestion

2021-1035	The paper proposes a new distributed training method for graph convolutional networks, using subgraph approximation.	B-abstract
2021-1035	The reviewers raised multiple challenges, such as novelty, validity of experiments, and some technical issues.	B-weakness
2021-1035	The authors did not respond to the reviewers' comments.	B-rebuttal_process
2021-1035	The AC agreed with the reviewers that the paper, in the current form, is not ready for publication. <sep>	B-decision

2021-1057	The paper proposes a variant of MAML for meta-learning on tasks with a hierarchical tree structure.	B-abstract
2021-1057	The proposed algorithm is evaluated on synthetic datasets, and it compares favorably to MAML.	I-abstract
2021-1057	The reviewers identified several significant weaknesses, including: (1) the experimental evaluation is limited, and it only includes small synthetic datasets; (2) the proposed algorithm is incremental over MAML.	B-weakness
2021-1057	The reviewers agreed that the paper cannot be accepted in its current form.	B-rating_summary
2021-1057	I recommend reject. <sep>	B-decision

2021-1082	It is important to develop efficient training methods for BERT like models since they have been widely used in real-world natural language processing tasks.	B-abstract
2021-1082	The proposed approach is interesting.	B-strength
2021-1082	It speeds up BERT training via identifying lottery tickets in the early stage of training.	B-abstract
2021-1082	We agree with the authors's rebuttal that autoML is not that related to the work here.	B-rebuttal_process
2021-1082	Our main concern on this work  is its worse-than-BERT performance showed in Table 2.	B-weakness
2021-1082	The performance gap is significant.	I-weakness
2021-1082	Sufficiently more training steps would fill the performance gap but the proposed method may have no advantage any more over the normal training procedures.	I-weakness
2021-1082	To make this work more convincing, we would like to suggest to include experiments on comparing different methods under similar prediction performance.	B-suggestion
2021-1082	In addition, since the main claim of this work is for training efficiency,  it will be helpful to show the advantage of this method by directly presenting the training curves/ results of different methods.	I-suggestion
2021-1082	Overall this paper is pretty much on the boundary.	O
2021-1082	We encourage the authors to resubmit this work once these issues are well resolved. <sep>	B-decision

2021-1084	This paper introduces an ensemble method to few-shot learning. <sep>	B-abstract
2021-1084	Although the introduced method yields competitive results, it is fair to say it is more complicated than much simpler algorithms and does not necessarily perform better.	B-weakness
2021-1084	Given that ensembling for few-shot learning has been around for a while, it is not clear that this paper will have a significant audience at ICLR. <sep>	O
2021-1084	Sorry about the bad news, <sep>	O
2021-1084	AC. <sep>	O

2021-1085	This paper empirically investigates the gradient dynamic of two-layer network nets with ReLU activations on synthetic datasets under L2 loss.	B-abstract
2021-1085	The empirical results show that for a specific type of initialization and less overparametrized neural nets, the gradient dynamics experience two phases: a phase that follows the random features model where all the neurons are quenched and another phase where there are a few activated neurons.	I-abstract
2021-1085	As pointed out by Reviewer 1, this paper lacks mathematical support and did not distinguish between random features model and neural tangent model.	B-weakness
2021-1085	Reviewer 3 and Reviewer 4 also complained that the paper is purely experimental.	I-weakness
2021-1085	Therefore, this paper may benefit from proposing an at least heuristic or high-level conjecture/interpretation/argument that tries to explain the empirical results. <sep>	B-suggestion

2021-1116	The reviewers are unanimous that the submission does not clear the bar for ICLR. <sep>	B-rating_summary

2021-1119	This paper proposes a suite of benchmark visual model-based RL tasks to evaluate causal discovery approaches under systematically varying causal graphs.	B-abstract
2021-1119	Despite some disagreement on this point among reviewers, I would come down on the side of saying that a better-executed version of this paper would have been a good fit at ICLR.	O
2021-1119	However, its current drawbacks make this a borderline reject.	B-decision
2021-1119	The most important of these drawbacks is: it is unclear to what extent results on these simple environments translate to more realistic complex ones. <sep>	B-weakness
2021-1119	Reviewers have also pointed to omitted relevant work that could be discussed in future versions, such as PHYRE.	I-weakness
2021-1119	Another relevant benchmark in this vein: https://arxiv.org/abs/1907.09620 <sep>	O

2021-1136	Reviewers liked the concept of the zero-day attack and yet raised different concerns about the other parts of the paper.	O
2021-1136	In general, Reviewers wanted to see more thorough experimental evaluations (eg, against blackbox attack and adaptive attack) and improved clarity of the theoretical analyses.	B-weakness
2021-1136	AC encourages authors to incorporate Reviewers' comments when preparing the paper for elsewhere. <sep>	B-suggestion

2021-1139	The paper introduces a simple and interesting method that adaptively smoothes the labels of augmented data based on a distance to the "clean" training data.	B-abstract
2021-1139	The reviewers have raised concerns about limited novelty, minor improvement over baselines, and insufficient experiments.	B-weakness
2021-1139	The author's response was not sufficient to eliminate these concerns.	B-rebuttal_process
2021-1139	The AC agrees with the reviewers that the paper does not pass the acceptance bar of ICLR. <sep>	B-decision

2021-1141	This paper proposes a method for out-of-distribution (OOD) detection by introducing a K+1 abstention class for outliers, in addition to the in-distribution classes.	B-abstract
2021-1141	While the method has shown promising performance compared to the Outlier Exposure (OE), the novelty is limited given the idea is almost identical to an AAAI'20 paper (Mohseni et al 2020).	B-weakness
2021-1141	In addition, several reviewers have raised concerns regarding the lack of fairness in the experimental setting.	I-weakness
2021-1141	Authors are encouraged to address them in a future submission. <sep>	B-suggestion
2021-1141	The AC believes an interesting and valuable contribution to the community would be showing conceptual and theoretical reasoning for the pros and cons of using K+1 class vs. entropy regularization.	I-suggestion
2021-1141	Currently, the tradeoff between these two types of training objectives is not well understood. <sep>	B-weakness
2021-1141	Overall, three knowledgeable reviewers in this area have indicated rejections.	B-rating_summary
2021-1141	The AC discounted R2's rating due to the less familiarity in this area and lack of participation in the post-rebuttal discussion among reviewers. <sep>	B-ac_disagreement
2021-1141	Lastly, the AC would like to greatly thank R1, R3, and R4 for the active engagement and participation in the paper discussion period.	O
2021-1141	It was very helpful for the decision-making process. <sep>	O

2021-1142	There are some interesting ideas raised on continuous-time models with latent variables in machine learning.	B-strength
2021-1142	However, the reviewers argue, and I agree, that the connection to causal models as typically required in applications about the effects of interventions is not addressed with as much care as it might have been needed. <sep>	B-weakness

2021-1146	This work analyses the impact of mini-batch size on the variance of the gradients during SGD, in the context of linear models.	B-abstract
2021-1146	It shows an inverse relationship between the variance of the gradient and the batch size for such models, under certain assumptions.	I-abstract
2021-1146	Reviewers generally agree that the work is theoretically sound.	B-strength
2021-1146	However, all reviewers believe that the contributions of this work are limited.	B-weakness
2021-1146	This concern was not adequately addressed during the discussion phase and led to the ultimate decison to reject. <sep>	B-decision

2021-1153	This paper proposes a semi-supervised graph classification technique that unifies feature and label propagation techniques.	B-abstract
2021-1153	The resulting algorithm is a simple extension that attains strong performance.	I-abstract
2021-1153	Reviewers were divided on this submission.	O
2021-1153	Some reviewers felt the proposed algorithm did not constitute a sufficient technical contribution given that it was a simple combination of existing techniques.	B-weakness
2021-1153	I tend to agree with other reviewers that the simplicity is a benefit.	B-ac_disagreement
2021-1153	However, despite the methods simplicity there was significant confusion about the details of the method and multiple reviewers flagged that the paper was difficult to read and understand.	B-weakness
2021-1153	It further could benefit from additional discussion and some clarification/cleanup of the experimental results.	I-weakness
2021-1153	Finally, multiple reviewers asked for better situating of the proposed method with respect to prior work.	I-weakness
2021-1153	Given these concerns, I do not think the paper is ready for publication.	B-decision
2021-1153	I would recommend the reviewers do a thorough re-write of the paper to address these concerns and consider resubmitting. <sep>	I-decision

2021-1156	Four knowledgeable referees have indicated reject.	B-rating_summary
2021-1156	I agree with the most critical reviewer R4 that the model design lacks a clear and transparent motivation and that the experimental setup is not convincing, and so must reject. <sep>	B-decision

2021-1157	The paper considers using local spectral graph clustering methods such at the PPR-Nibble method for graph neural networks.	B-abstract
2021-1157	These local spectral methods are widely used in social networks, and understanding neural networks from them is interesting. <sep>	I-abstract
2021-1157	In many ways, the results are interesting and novel, and they deserve to be more widely known, but there are several directions to make the work more useful to the community.	B-weakness
2021-1157	These are outlined in the reviewer comments, which the authors answered partially but not completely satisfactorily.	B-rebuttal_process
2021-1157	Much of this has to do with explaining how/where these (these very fundamental and ubiquitous) methods are useful in a particular application (GNNs here, and node embeddings below).	I-rebuttal_process
2021-1157	An example of a paper that successfully did this is "LASAGNE: Locality And Structure Aware Graph Node Embedding, E. Faerman, et al  Proc.	I-rebuttal_process
2021-1157	2018 Conference on Web Intelligence."	I-rebuttal_process
2021-1157	(That is mentioned not since it is directly relevant to this paper, but since it provides an example of how to present the use of a method such as PPR-Nibble for the community. <sep>	I-rebuttal_process

2021-1173	This paper presents an interesting method dubbed quotient manifold modeling to handle the "multi-manifold" structure of natural data and generalize to new manifolds that arise from novel discrete combinations.	B-abstract
2021-1173	While some of the methods and ideas were appreciated by reviewers, there were a number of experimental and clarity concerns.	B-weakness
2021-1173	The authors's did not submit a rebuttal, and the many unaddressed concerns (especially around experimental baselines) lead me to recommend rejecting this work. <sep>	B-rebuttal_process

2021-1176	This paper studies two techniques for handling high dimensional action spaces in deep RL, namely selecting action components independently or selecting components sequentially in an autoregressive approach.	B-abstract
2021-1176	The methods are developed for two deep RL algorithms (PPO and SAC) and tested on multiple domains. <sep>	I-abstract
2021-1176	The reviewers recognized the significance of this research topic but found significant problems in the presentation.	B-weakness
2021-1176	The reviewers raised concerns on the relationship to prior work in the literature (R2), baseline comparisons that are missing in the experiments (R2, R4), and a lack of clarity in the intended message of the experiments (R3).	I-weakness
2021-1176	The authors responded favorably to the reviews, answered clarification questions, and acknowledged the limitations of the submitted work.	B-rebuttal_process
2021-1176	The authors expressed their intent to release a stronger paper sometime in the future.	I-rebuttal_process
2021-1176	The reviewers acknowledged the author response and were in consensus that the submission needs more work. <sep>	I-rebuttal_process
2021-1176	Three reviewers have indicated reject for the reasons described above.	B-rating_summary
2021-1176	The paper is therefore rejected. <sep>	B-decision

2021-1189	The paper considers a problem of weak mean estimation under a differential privacy like constraint.	B-abstract
2021-1189	Specifically, estimating the signs of a (sparse) mean, and not the actual values. <sep>	I-abstract
2021-1189	The reviewers brought up a number of concerns, including the weak privacy guarantee (a type of average-case privacy).	B-weakness
2021-1189	Other lesser concerns include inaccuracies in comparisons with the literature and lack of interest in the algorithm/method itself. <sep>	I-weakness
2021-1189	As there was no response from the authors, there was little further discussion afterwards, and the reviewers remained in their opinion to reject the paper. <sep>	B-rating_summary

2021-1191	This paper provides a new perspective on deep networks by showing that NPK is composed of base kernels and their dependence on the architecture is explicitized.	B-abstract
2021-1191	It is further shown that learning the gates can perform better than random gates. <sep>	I-abstract
2021-1191	While the paper provides interesting understanding neural networks, it is unclear what practical benefit can be drawn from it.	B-weakness
2021-1191	On the architectures considered such as FC, ResNet and CNN (btw, it seems restricted to 1-D), it will be important to show that such insights lead to new models or learning algorithms that improve upon the standard practice in deep learning (or get very close to).	B-suggestion
2021-1191	It is debatable whether drawing such a nontrivial insight alone warrants publication at ICLR, while "nontrivial" itself is a subjective judgement.	O
2021-1191	I understand people differ in their opinions, and the NTK paper has been impactful.	O
2021-1191	Unfortunately since there are quite a few other papers that are stronger, I have to recommend not accepting this paper to ICLR this time. <sep>	B-decision

2021-1209	This paper proposes a method for differentiable pruning that replaces the hard thresholding of standard pruning, with a soft version that permits taking the gradients of the pruning threshold.	B-abstract
2021-1209	The proposed benefits are an accuracy that is better or competitive with alternative methods as well.	I-abstract
2021-1209	Moreover, the paper suggests the technique to be efficient. <sep>	I-abstract
2021-1209	The pros of this paper are that it is working in an interesting setting of differentiable pruning, with the hope of -- in some sense -- simplifying the pruning process or at least unifying the process with standard training.	B-strength
2021-1209	The technique is plausibly justified in its technical development.	I-strength
2021-1209	The paper also follows with a significant number of experiments. <sep>	I-strength
2021-1209	The cons of this paper are that the conceptual framework -- beyond the initial idea -- is not fully clear.	B-weakness
2021-1209	In particular, this paper does not elucidate a clear set of claims and hence, results in the difficulty on the Reviewers part in detangling the claims and identifying the appropriate comparisons. <sep>	I-weakness
2021-1209	For example, the paper doesn't take up a simple claim that it is state-of-the-art in accuracy vs parameter measures (and would seem not to given the results of Renda et al (2020)).	I-weakness
2021-1209	It need not necessarily make this claim, but there are suggestions to such a claim early in the paper.	I-weakness
2021-1209	If this is not an intended claim, then the paper can remove any suggestions to such (ie, the claims around new SoTA for networks not evaluated in prior work). <sep>	B-suggestion
2021-1209	The paper has a somewhat tentative claim that it is more efficient (in the total number of epochs of training) versus other techniques (Table 3).	B-weakness
2021-1209	However, the presented results are only at a single-point versus other methods.	I-weakness
2021-1209	Renda et al (2020) directly consider accuracy versus retraining cost trade-offs.	I-weakness
2021-1209	Appendix E of that paper provides one-shot pruning results for ResNet-50 showing accuracy on par with that presented here.	I-weakness
2021-1209	The number of retraining epochs is also similar to here.	I-weakness
2021-1209	This paper, however, only compares against the most expensive iterative pruning data point in the other paper. <sep>	I-weakness
2021-1209	In sum, my recommendation is Reject.	B-decision
2021-1209	This is promising work that needs only (1) to include a few testable claims and (2) to re-organize the results (and perhaps run a limited set of new results) to thoroughly explore those claims.	B-suggestion
2021-1209	For example, if the most important claim is accuracy vs retraining cost, then it needs to show a more complete trade-off curve of the two results.	I-suggestion
2021-1209	Of course, this, in principle, opens the door to comparisons to many other techniques in the literature. <sep>	I-suggestion

2021-1215	The paper analyzes the space of mixed sample data augmentation approaches, and proposes a new variant, FMix, based on a new masking strategy.	B-abstract
2021-1215	Reviewers point to the fact that FMix is only marginally better than previous approaches, that the experimental setup is unconvincing, and that the proposed analysis might not be grounded.	B-weakness
2021-1215	This is a really borderline paper but I see the issues as more important than the benefits, so I recommend rejection. <sep>	B-decision

2021-1241	Though the observation regarding the importance of the low end of the spectrum is interesting in its own right, the paper would be better substantiated by experiments on more datasets and a more thorough characterization of the paper novelty/contrast to state of the art. <sep>	B-suggestion

2021-1249	This paper provides approximation results for functions that can be represented by hybrid quantum-classical circuits.	B-abstract
2021-1249	It is felt that venues such as QIP would be a more suitable venue, and perhaps some experiments/simulations could be added. <sep>	B-decision

2021-1261	The reviewers positively valued the proposed idea of performing permutation selection in permutation decoding via combining node embedding and self-attention, which seems to be of high originality.	B-strength
2021-1261	I found that this paper is mostly clearly written, except Section 3.2 as AnonReviewer5 commented.	I-strength
2021-1261	The main concern among the reviewers is regarding applicability of the proposal beyond the BCH codes. <sep>	B-weakness
2021-1261	Pros: <sep> The proposal of utilizing self-attention for permutation selection in permutation decoding is novel and interesting. <sep> 	B-strength
2021-1261	Computational complexity in the decoding phase is only slightly increased compared with random permutation selection, and is far smaller than performing decoding for all permutations.	I-strength
2021-1261	The GPS classifier can be parallelizable to further reduce latency. <sep>	I-strength
2021-1261	The proposal should be applicable beyond the BCH codes to those with decoding based on the Tanner graph, including polar codes.<sep>	I-strength
2021-1261	Cons: <sep> Only the BCH codes were considered, whereas in the authors responses they will add a short analysis on polar codes. <sep> 	B-weakness
2021-1261	It seems that systematic enumeration of the PG is required, which would limit applicability of the proposal. <sep>	I-weakness
2021-1261	There is a room for improvement in presentation: <sep> 	I-weakness
2021-1261	In Section 3.2, the description of "positional encodings" was unclear to me, in that the ordering of the codeword entries is arbitrary, unlike typical sequence transduction problems to which attention mechanism is being applied.<sep>	I-weakness
2021-1261	Dependence of the input vector sequences of the attention head on the permutation  is not clearly explained.<sep>	I-weakness
2021-1261	Performance of the proposed method might depends on choices of the parity-check matrix, which is however not discussed in this paper at all.<sep>	I-weakness
2021-1261	Based on the above concerns, the paper is not yet ready for publication in its current form. <sep>	B-decision
2021-1261	Minor points: <sep> In page 3, line 14, "that" should be deleted. <sep>	B-weakness
2021-1261	In references list, "hdpc" should be in capital.	I-weakness
2021-1261	"reed-muller" should be "Reed-Muller". <sep>	I-weakness

2021-1271	The paper proposes hybrid discriminative + generative training of energy-based models (HDGE) building on JEM.	B-abstract
2021-1271	By connecting contrastive loss functions to generative loss, HDGE proposes an alternative loss function that reduces computational cost of training EBMs. <sep>	I-abstract
2021-1271	The reviewers agree that this is an interesting idea and that the empirical results look promising. <sep>	B-strength
2021-1271	However, multiple reviewers raised concerns that the theoretical justification was incomplete and felt that some of the claims about the equivalence between the two, as well as some of the practical approximations introduced, need more justification. <sep>	B-weakness
2021-1271	I encourage the authors to revise the paper and resubmit to a different venue. <sep>	B-decision

2021-1274	Four reviewers evaluated your work and provided a detailed review with many suggestions.	O
2021-1274	I also think that there is an interesting idea and encouraging results but there is a lack of numerical results and still some parts are still unclear and need to be polished.	B-weakness
2021-1274	Consequently in its  current form, the paper can not be accepted for publication.	B-decision
2021-1274	I would advise you to carefully follow the remarks of reviewer 1 to improve  the paper. <sep>	B-suggestion

2021-1280	The work studies the transferability of perturbations/adversarial attacks on DRL agents.	B-abstract
2021-1280	As a way to mitigate the cost of generating individual perturbation for each state in each episode, the authors proposed several variants to use same perturbation across different states across different episodes.	I-abstract
2021-1280	While reviewers recognize the potential of the direction, they are not comfortable accepting the paper at its current state.	B-rating_summary
2021-1280	The experiment results in its current form does not provide enough support to the claim.	B-weakness
2021-1280	In particular, it is unclear how much the shared perturbation changes the original perception in comparison to the individual comparison, and how should the impact number differences be interpreted.	I-weakness
2021-1280	Reviewers brought up concerns regarding all the experiments being evaluated on a DDQN agent, and not enough clarities has been provided on the different design choices.	I-weakness
2021-1280	If perceptual similarity is not the indicator of environment transferability, do the authors have intuition on what does? <sep>	I-weakness

2021-1293	the paper undoubtedly tackles an interesting problem in the mainstream of learning with partial / unknown / weak / noisy / complementary labels.	B-abstract
2021-1293	The authors have had a set of constructive suggestions and questions from the reviewers (and external comments), some positive, some negative.	O
2021-1293	I find it a bit unsettling that to several major questions, the main feedback from the authors was a citation in the paper with no further action; (a) ablation tests of R2 end up in citing papers from a public comment, (b) R4 raised a key point in comment 2, with the links to partial labels learning.	B-rebuttal_process
2021-1293	The authors' answer is not satisfying as one would have hoped at least of a partial justification of the author's approach in this context.	I-rebuttal_process
2021-1293	The authors would have had time to develop at least elements of a formal comparison.	I-rebuttal_process
2021-1293	Just citing the work is not enough; <sep>	I-rebuttal_process

2021-1300	This paper studies RL with low switching cost under the deep RL setting.	B-abstract
2021-1300	It provides new heuristics for doing so.	B-strength
2021-1300	The reviewers are worrying about whether the problem is important in practice, whether the policies obtained can be used in practice, and the theories might not be strong enough.	B-weakness
2021-1300	The paper can be strengthened if better theory and more experiments are provided. <sep>	I-weakness

2021-1301	This paper considers a new model of input data specific for image classification problems.	B-abstract
2021-1301	In particular, the high level idea is that each image contains certain patterns, and which patterns it contain decides its label.	I-abstract
2021-1301	In this framework, under some stronger assumptions (eg, patterns are orthogonal, one positive pattern and one negative pattern, PSI assumption, etc.)	I-abstract
2021-1301	the authors showed that SGD on a 3-layer overparametrized convolutional network will be able to have a small sample complexity, while the VC dimension would be at least exponential in d. The paper also provided some empirical evidence on a modified MNIST dataset.	I-abstract
2021-1301	While the idea seems to be an interesting first step, the reviewers find that the current version of theory still relies on fairly strong assumptions. <sep>	B-weakness

2021-1303	This paper investigates how to deploy adaptive learning rates in multi-agent RL (MARL).	B-abstract
2021-1303	In particular, the learning rates are adaptively chosen based on which directions maximally affect the Q-function, and take into account the interplay and balance between the actors and the critics.	I-abstract
2021-1303	The topic is certainly of great interest when designing fast-convergent MARL algorithms.	B-strength
2021-1303	However, the reviewers point out the inadequacy and insufficiency of empirical gains in the reported experiments.	B-weakness
2021-1303	Also, larger-scale experimental settings are needed in order to provide more convincing evidence about the practical benefits of the proposed scheme. <sep>	I-weakness

2021-1320	This work presents a distributed SVGD (DSVGD) algorithm as a new non-parametric Bayesian framework for federated learning.	B-abstract
2021-1320	The reviewers concerned with the practical advantages of the proposed method, including the communication cost and the constraint of updating one agent per time.	B-weakness
2021-1320	The authors rebuttal helped addressing some of the concerns, including proposing a new Parallel-DSVGD algorithm.	B-rebuttal_process
2021-1320	This is very much appreciated.	O
2021-1320	However, given the significant modification needed over the original version, we think it is better for the authors to further improve the work and submit to the next conference. <sep>	B-decision

2021-1327	The submitted paper is well written and easy to follow and also the idea of using VAEs for making inferences about the opponents on which a policy can be conditioned on is sensible.	B-strength
2021-1327	Also the reported performance in comparison to two baselines is good (although I have concerns about the selection of the baselinessee below).	I-strength
2021-1327	Acceptance of the paper was suggested by 3 of the reviewers and rejection by one of the reviewers.	B-rating_summary
2021-1327	While I don't share all concerns of the negative reviewer, I also suggest to reject the paper. <sep>	B-decision
2021-1327	My suggestion to reject the paper is mainly based on seeing concerns of the positive reviewers more critical as these reviewers themselves and some concerns I have on my own.	I-decision
2021-1327	In particular, I don't think that all MARL approaches can simply be discarded for comparisonno matter whether the opponents are learning or not.	B-weakness
2021-1327	Regarding the evaluation, I think that an environment with real opponents must be considered and that robustness is a key property that should be studied (otherwise an approach with a fixed set of best response policies and inference about the opponent might perform as well).	B-suggestion
2021-1327	In that regard I also find the selection of baselines insufficientthe minimum I would expect is to consider a NOM baseline using an RNN (which as far as I can tell is not the case) such that it could make inferences about the opponent. <sep>	B-weakness
2021-1327	I want to acknowledge that the paper improved quite a lot during the rebuttal period in which the authors extended their discussion of related work on opponent modeling. <sep>	B-rebuttal_process
2021-1327	In summary, the paper could be improved substantially by an extended empirically study (more environments + baselines + "mismatch" settings).	B-weakness
2021-1327	If the currently observed performance gains also hold in these settings, this can become a good paper but in its current form I think the paper is not demonstrating that the proposed approach performs favorably over natural baselines and works well against real opponents. <sep>	I-weakness

2021-1334	Summary of discussions: R1 was positive on the paper in their initial evaluation, and although dissatisfied with the author's feedback, continued to support the paper.	B-rebuttal_process
2021-1334	I agree with R1's assessment that other reviewers' call for more theory is somewhat unfair, considering the fact that very similar papers don't usually include theoretical justification beyond intuitive motivation. <sep>	B-ac_disagreement
2021-1334	By contrast, R3 is the most negative on the paper, leaning towards rejection.	B-rating_summary
2021-1334	The main concern is that open questions remain as to whether the reported performance can be attributed to the architecture, or the loss function proposed.	B-weakness
2021-1334	This is an important point to clarify, and further ablation studies would make the paper stronger. <sep>	B-suggestion
2021-1334	After considering the strengths and weaknesses of this work, the final decision was to reject.	B-decision
2021-1334	Authors are encouraged to improve this promising work and resubmit to a future venue. <sep>	I-decision

2021-1336	The paper proposes an intriguing approach for "individual treatment effect" estimation from an observational dataset.	B-abstract
2021-1336	The approach is developed for multiple discrete actions (beyond binary treatments as typically studied in ITE literature) and discrete outcomes (a special case compared to related literature).	I-abstract
2021-1336	The idea is to use the "direct method" (ie learn a probabilistic classifier using the observed dataset) and sample imputed outcomes for all unobserved action-outcomes.	I-abstract
2021-1336	Then, learn a probabilistic classifier that fits the observed+imputed dataset well, and iterate the procedure.	I-abstract
2021-1336	This intriguing idea seems to converge empirically on a few different problems, and sampling the imputations rather than using deterministic imputations seems to be an important detail. <sep>	I-abstract
2021-1336	Proof of convergence is however shown for deterministic imputations.	B-weakness
2021-1336	The generalization error bound (Theorem 1) also does not show adequate motivation for the proposed method -- even with infinite data (n->infinity), the excess risk could scale with the empirical risk of the returned model on the imputed dataset.	I-weakness
2021-1336	Without an additional step proving that empirical risk on hat{D} (the imputed dataset) converges to 0 during successive iterations of the procedure, the generalization error bound is incomplete. <sep>	I-weakness
2021-1336	Consider the example of Figure 1, but where customer A has arrived to the system twice.	I-weakness
2021-1336	So, the dataset contains {x1, $2, 1} and {x1, $3, 0}.	I-weakness
2021-1336	When constructing the imputed dataset, the first data-point would create 2 regression examples {x1, $1,.}	I-weakness
2021-1336	and {x1, $3,.}	I-weakness
2021-1336	while the second data-point would create 2 regression examples {x1, $1,.}	I-weakness
2021-1336	and {x1, $2,.}. <sep>	I-weakness
2021-1336	Now, if the two {x1, $1,.}	I-weakness
2021-1336	examples have different imputation labels sampled from the model, this sets up an unrealizable learning problem and the empirical risk on hat{D} cannot be 0 for any predictor. <sep>	I-weakness
2021-1336	In this toy example, we might know that we should "collapse" the two data-points (eg, de-duplicate the dataset to only have unique x's with aggregated action-outcomes across all observations) in the original data-set and only create one set of imputed labels -- but similar unrealizability can happen for x's that are "close" to each other that no model has capacity to label them differently. <sep>	I-weakness
2021-1336	The strength of the paper is its intriguing approach to ITE estimation.	B-strength
2021-1336	It is a form of an iterative S-learner (vanilla S-learners have been widely used in ITE estimation). <sep>	I-strength
2021-1336	The low-point of the paper is this weakness in theory and analysis -- it is unclear if the proposed procedure with sampling imputations (which seems to be important for empirical performance) is even a consistent algorithm. <sep>	B-weakness
2021-1336	The paper would be much stronger with a more rigorous analysis of when the method will reliably work, and importantly, its limitations -- such a study will help practitioners know when to use self-training over direct method, targeted max likelihood, S-learners, etc. <sep>	B-suggestion

2021-1355	The authors study "robustness curves" which are plots of the robust error versus the radius used in the corresponding l_p-ball threat model. <sep>	B-abstract
2021-1355	Pro: I completely agree with the authors that the current evaluation purely based on evaluation for a single radius is insufficient and one should report the complete curve. <sep>	B-strength
2021-1355	Con: The authors are overclaiming that they have come up with robustness curves.	B-weakness
2021-1355	Very early papers eg even in the adversarial training paper of Madry there are plots of robust accuracy versus chosen threshold.	I-weakness
2021-1355	Moreover, I agree with one of the reviewers that using PGD for the purpose of a robustness curve is inaccurate and in particular inefficient as several attacks for different radii have to be done.	I-weakness
2021-1355	There have been several attacks developed which aim to find the adversarial sample with minimum norm and thus compute the robustness curve in one run. <sep>	I-weakness
2021-1355	The additional insights eg intersection of robustness curves are partially to be expected and I don't find them sufficient to move the paper over the bar for ICLR.	B-decision
2021-1355	As these insights are additionally  only shown for relatively small models which seem far away from the state of the art, it is unclear if they generalize.	B-weakness
2021-1355	However, I encourage to follow some of the reviewer's suggestions to improve the paper. <sep>	B-suggestion

2021-1379	This promising work proves that the proposed contrastive learning approach to representation learning can recover the underlying topic posterior information given standard topic modelling assumptions.	B-abstract
2021-1379	The work provides detailed proof and detailed experiments.	B-strength
2021-1379	The analysis is interesting and yields interesting insights.	I-strength
2021-1379	However, the experimental results are somewhat weak by lacking comparison with more recent document representation work. <sep>	B-weakness
2021-1379	Pros: <sep> Good detailed proofs and experiments. <sep> 	B-strength
2021-1379	Interesting idea of using topic modelling to understand representation learning.<sep>	I-strength
2021-1379	Cons: <sep> The description of DirectNCE is somewhat hidden and could be better introduced in the paper. <sep> 	B-weakness
2021-1379	Experimental baselines are weak lacking a comparison to recent document representation work such as Arora et al 2019. <sep>	I-weakness
2021-1379	Stronger classification baselines could be incorporated.<sep>	I-weakness

2021-1396	This paper addresses the real-world problem of semi-supervised learning where the distribution from which the labeled examples are drawn is different from the distribution from which the unlabeled examples are drawn.	B-abstract
2021-1396	The task is motivated by structure-activity prediction for drug design (quantitative structure activity prediction, or QSAR).	I-abstract
2021-1396	Examples represent molecules, and we wish to predict a real-valued measure of binding affinity.	B-weakness
2021-1396	Exactly the general problem of data skew arose with exactly this task for example in one of the KDD Cup 2001 tasks.	I-weakness
2021-1396	While the authors here mention that labeled data may be focused more on active molecules (those with a high continuous-valued response), in the KDD Cup 200`1 data the reverse was true, and the unlabeled test data were skewed to higher activity level.	I-weakness
2021-1396	I say all this to agree with the authors about the real-world nature of the problem they address.	O
2021-1396	Also, some reviewers felt more empirical evaluation was needed, so that may be an additional data set for the authors to consider using. <sep>	B-weakness
2021-1396	Reviewer concerns including that the approach was simplistic, the empirical results were insufficient, and the claims were oversold.	I-weakness
2021-1396	The author replies and revisions, and the discussion, moved the reviews to be more favorable but still not strong enough to justify acceptance yet.	B-decision
2021-1396	Nevertheless, the consensus is that the paper addresses an important problem and the revisions are headed in the right direction to make a strong future paper, and that the authors should be encouraged to continue this work. <sep>	O

2021-1408	The paper received 5 reviews, one of which had positive feedback.	B-rating_summary
2021-1408	Although there are merits associated with the paper, several concerns raised in the reviews and the discussion period that prevents the paper to be accepted.	B-rebuttal_process
2021-1408	It appears that experiments on noisy graphs are not properly done and competitive baselines are not used for validations.	B-weakness
2021-1408	The quality of the learned graph structure is not adequately analyzed.	I-weakness
2021-1408	and the experimental setup was not clearly explained.	I-weakness
2021-1408	All these indicate that there is a need for a major revision before the paper can be considered for acceptance. <sep>	B-decision

2021-1413	This work was deemed interesting by the reviewers, but they highlighted the following weaknesses in this version of the paper: <sep> Lack of comparison to other methods. <sep> 	B-weakness
2021-1413	Lack of novelty compared to previous work.<sep>	I-weakness
2021-1413	Fundamental problem with training only on one dataset (MNIST), issue with possible overfitting.<sep>	I-weakness

2021-1430	The paper proposed a two-stage method to select instances from a set, involving candidate selection (learning a function  to determine a Bernoulli probability for each input) and AutoRegressive subset selection (learning a function  to generate probabilities for sampling elements from a reduced set); both stages use the Concrete distribution to ensure differentiability.	B-abstract
2021-1430	The experiments show the performance of the proposed method on several use-cases, including reconstruction of an image from a subset of its pixels, selecting sparse features for a classification task, and dataset distillation for few-shot classification.	I-abstract
2021-1430	I read the paper and I agree with the reviewers that in its current format the paper is hard to follow.	B-weakness
2021-1430	I strongly encourage the authors to add more discussion and intuition on the proposed method and extend the experiments with more baseline comparison and ablation studies in the revised version. <sep>	B-suggestion

2021-1443	The paper proposes a fast, nearly-linear time, algorithm for finding a sparsifier for general directed and undirected graphs that approximately preserves the spectral properties of the original graph.	B-abstract
2021-1443	The reviewers appreciated the main contribution of the paper, but they were concerned about the correctness and clarity of the paper, as well as the relevance of the contribution to machine learning.	B-weakness
2021-1443	Following the discussion with the authors, the reviewers still felt that these concerns had not been fully addressed by the authors' responses and the subsequent revision of the paper.	B-rebuttal_process
2021-1443	After taking these concerns into account as well as evaluating the paper relative to other ICLR submissions, I recommend reject. <sep>	B-decision

2021-1446	In this paper the authors propose an approach to improving the accuracy of the classification problem based on deep neural networks by detecting the in-domain data from background/noise.	B-abstract
2021-1446	The strategy is designed in such a way that the detector and the classifier share the bottom layers of the network.	I-abstract
2021-1446	Theoretical proof is given and experiments are conducted on a variety of datasets.	I-abstract
2021-1446	The novelty of the work is to come up with a better estimate the pdf of the data and use it to help the classification based on the deep neural networks.	I-abstract
2021-1446	There are concerns raised by the reviewers regarding the related work, the exposition and the experimental design.	B-weakness
2021-1446	After the rebuttal from the authors, which is meticulous, some of the issues unfortunately still stand.	B-rebuttal_process
2021-1446	The paper needs to make a stronger case in order to be accepted, especially, for instance, the theoretical and empirical comparison with the existing techniques sharing the similar idea. <sep>	B-decision

2021-1456	This work proposes a method to discover neighboring local optima around an existing one.	B-abstract
2021-1456	Reviewers all found the idea interesting but argued that the paper needed more work.	B-weakness
2021-1456	In particular, some of the claims are too informal or not sufficiently supported and the reviewers found the key section were difficult to follow.	I-weakness
2021-1456	The paper should be resubmitted after improving the presentation of the results. <sep>	B-decision

2021-1471	This paper proposes OpenCos for semi-supervised learning that can leverage unsupervised information in open-set scenarios where samples can be out-of-class.	B-abstract
2021-1471	They first pre-train by learning an unsupervised representation using SimCLR on both the labeled and unlabeled data.	I-abstract
2021-1471	Then, they detect out-of-class samples in the unlabeled set based on similarity measures on the representation learned in the previous step.	I-abstract
2021-1471	The unlabeled data can now be split into in-class and out-of-class.	I-abstract
2021-1471	OpenCos optimizes (8) which combines a semi-supervised loss for in-class unlabeled data and an auxiliary cross-entropy loss with soft-labels for the out-of-class samples.	I-abstract
2021-1471	Finally, they perform an auxiliary batch normalization. <sep>	I-abstract
2021-1471	The paper is easy to read and clearly structured.	B-strength
2021-1471	It also places the work well with respect to related work. <sep>	I-strength
2021-1471	The proposed approach makes sense; however, as pointed out by the reviewers, the novelty is marginal.	B-weakness
2021-1471	The technical innovation seems to be an extension of SimCLR and the auxiliary batch norm of Xie et al <sep>	I-weakness

2021-1481	While the reviewers appreciated the aim of the work, they found the technical contribution to be too incremental to be of sufficient interest and the exploration of the problem and its significance to be incomplete in the paper's current state. <sep>	B-weakness

2021-1494	The authors study the problem of (insufficient) generalization in gossip-type decentralized deep learning.	B-abstract
2021-1494	Specifically, they establish an upper bound on the square of the consensus parameter distance, which the authors identify as a key quantity that influences both optimization and generalization.	I-abstract
2021-1494	This upper bound (called the critical consensus distance) can be monitored and controlled during the training process via (eg) learning rate scheduling and tweaking the amount of gossip.	I-abstract
2021-1494	A series of empirical results on decentralized image classification and neural machine translation are presented in support of this observation. <sep>	I-abstract
2021-1494	Initial reviews were mixed.	O
2021-1494	While all reviewers liked the approach, concerns were raised about the novelty of the results, the lack of theoretical depth, and the mismatch between theory and experiments.	B-weakness
2021-1494	Overall, the idea of tracking consensus distance to control generalization seems to be a practically useful concept.	B-strength
2021-1494	During the discussion phase the authors were been able to (convincingly, in the area chair's view) respond to a subset of the criticisms. <sep>	B-rebuttal_process
2021-1494	Unfortunately, concerns remained regarding the mismatch between the theoretical and empirical results, and in the end the paper fell just short of making the cut. <sep>	B-decision
2021-1494	The authors are encouraged to carefully consider the reviewers' concerns while preparing a future revision. <sep>	B-suggestion

2021-1499	The authors present a method for self-supervised learning of representations of 2D projections of 3D objects.	B-abstract
2021-1499	By performing known 3D transformations of an object of interest, a encoder/decoder network is trained to estimate the applied transformation from a series of 2D projections.	I-abstract
2021-1499	The proposed method is used as a regularizer and experiments are performed on supervised 3D object classification and retrieval. <sep>	I-abstract
2021-1499	After seeing each others' reviews, one of the main concerns from the reviewers was the relationship between the proposed method and Zhang et al, CVPR 2019 (ie AET).	B-weakness
2021-1499	The two methods are conceptually very similar, and the consensus from the reviewers is that the authors did not acknowledge the overlap sufficiently and also did not provide a convincing argument as to why they think the approaches are different. <sep>	I-weakness
2021-1499	In their rebuttal the authors provided some additional results on real data which is a valuable and welcome addition.	B-rebuttal_process
2021-1499	However, there were still other concerns that the reviewers had eg R2 wanted to know why the model could not be applied directly to 3D shapes instead of 2D projections. <sep>	I-rebuttal_process
2021-1499	Given the above concerns (specifically the relationship to AET), there is currently not enough support for accepting the paper in its current form.	B-decision
2021-1499	The authors have received detailed feedback and are encouraged to take it onboard when revising the paper in future. <sep>	B-suggestion

2021-1513	Like the reviewers, I find this paper extremely borderline.	O
2021-1513	On the one hand, it is clearly written, about a topic I find fascinating, and generally well motivated if not shockingly novel (ie removing some of the simplifying assumptions from Zhong et al 2020, eg requiring grounding to be learned, use of real language rather than synthetically generated).	B-strength
2021-1513	On the other hand, I agree with the leitmotiv present amongst the reviews that the problem at the centre of the experimental setting is very, very simple (3 objects, 3 descriptions).	B-weakness
2021-1513	I am mindful of the fact that access to computational resources is unevenly distributed, and am not expecting a paper like this to immediately scale their experiments to highly complex settings with photorealism, etc, but I can't help but feel that a more challenging task, with a deeper analysis of the problems presented by both grounding and the use of non-synthetic language, would both have been highly desirable to make this paper uncontroversially worth accepting. <sep>	I-weakness
2021-1513	As a result, the decision is to not accept the paper in its present form.	B-decision
2021-1513	Work on this topic should definitely be presented at ICLR, but it's a shame this paper did not make a stronger case for itself. <sep>	O

2021-1516	This paper proposes an interesting new direction for low-cost NAS.	B-abstract
2021-1516	However, the paper is not quite ready for acceptance in its current form.	B-decision
2021-1516	The main area of improvement is around the generalizability of the score presented, both empirically and (ideally) theoretically.	B-weakness
2021-1516	The two main directions of generalizability that would be worth investigating are 1) different image datasets (see comments around Imagenet-16) 2) different/larger search spaces.	I-weakness
2021-1516	Even simple search spaces consisting of a few architectural modification starting from standard architectures (eg resnets) would go a long way in convincing the community that the proposed method generalizes past NasBench. <sep>	I-weakness

2021-1524	The paper studies personalized federated learning, mixing a global model with locally trained models.	B-abstract
2021-1524	Reviewers agreed on the relevance of the problem and that the work contains valuable contributions, such as the generalization bounds. <sep>	B-strength
2021-1524	After discussion, unfortunately consensus remained that the paper remains narrowly below the bar in the current form. <sep>	B-rating_summary
2021-1524	Concerns remained on novelty over the Mapper optimization algorithm which also has adaptivity to the local/global combination of models, the dependence of the generalization bound on the mixing parameter as it converges to the global model, <sep>	B-weakness
2021-1524	as well as on the strength of the experimental findings compared to well-known FedAvg and related method in a realistic benchmark environment (such as eg Leaf), since the dataset choice (and even more its partition among clients) is a crucial aspect for measuring personalization in a fair way.	I-weakness
2021-1524	We hope the feedback helps to strengthen the paper for a future occasion. <sep>	O

2021-1532	This paper is about training a discrete policy that maps an image representation through a differentiable time-dependent path planning module.	B-abstract
2021-1532	The method is based on [1] and the reviewers are concerned about lack of novelty with respect to this work, and also with [2], however the latter only appeared a few weeks before the ICLR deadline, so I am not factoring it in my recommendation.	B-weakness
2021-1532	Unfortunately, in light of [1], 3/4 reviewers do not recommend acceptance, and I agree with them. <sep>	B-decision
2021-1532	[1] Vlastelica, Paulus, Musil, Martius, Rolinek.	O
2021-1532	Differentiation of Blackbox Combinatorial Solvers (2020). <sep>	O
2021-1532	[2] Yonetani, Taniai, Barekatain, Nishimura, Kanezaki.	O
2021-1532	Path Planning using Neural A* Search (2020). <sep>	O

2021-1550	I found the main algorithmic contributions to be interesting and of potential value to practitioners, as highlighted by Reviewer 3.	B-strength
2021-1550	Like several reviewers, I found the causal framing to be confusing, or at least not really to be framed in a framework like Pearl's: the word "confounding" is thrown a few times in the manuscript, but there is no formal sense by which it is linked to what we commonly understand as confounding.	B-weakness
2021-1550	We are still talking about what happens inside a predictive model (a deterministic function), not what happens in the real world (the authors are not alone as targets of my observation: my point applies to a lot of the papers in the references, where the causal interpretation is hardly illuminating for those coming from a causal inference background, for instance).	I-weakness
2021-1550	The reply to Reviewer 2, for instance, cites [1], which is about Granger causality and has little to do with Pearl's framework.	I-weakness
2021-1550	Despite its name, Granger "causality" is a probabilistic concept (or, at best, an idea for identifying non-causality) with a very minimal causal basis besides the use of time ordering.	I-weakness
2021-1550	A much more rigorous explanation of confounding in this paper's context needs to be provided. <sep>	B-suggestion
2021-1550	That been said: as helpfully highlighted by Reviewer 3 (and summarized without any need to resort to a causal framing), there are several positive contributions added here, which might be of interest to the ICLR audience.	B-strength
2021-1550	The causal framing unfortunately gets in the way without adding clarity. <sep>	B-weakness
2021-1550	In its present state, the paper is not yet ready for publication.	B-decision
2021-1550	We hope that the reviewer comments prove helpful for preparing a strong future submission. <sep>	O

2021-1566	This paper tackles a problem of resource allocation using reinforcement learning.	B-abstract
2021-1566	An important invariant - permutation invariant - is identified as an important characteristic of this problem.	I-abstract
2021-1566	Then it is shown that taking advantage of such an invariant should  dramatically improve the sample efficiency. <sep>	I-abstract
2021-1566	On behalf of the reviewers, I would like to thank the authors for addressing many concerns raised in the initial reviews.	O
2021-1566	Unfortunately, a further examination revealed several other potential issues that require further clarification: <sep> It seems that real-data experiments do not really demonstrate whether the benefits of the approach come from multi-task learning or from permutation invariance.	B-weakness
2021-1566	It would make sense to run an ablation study.	I-weakness
2021-1566	In particular, if the benefit is really coming from multi-task learning, then the theory part of the paper becomes less relevant.<sep>	I-weakness
2021-1566	The metric used for finance application appear to be in-adequate.	I-weakness
2021-1566	It is typical in finance academic literature to look some form of risk-adjusted returns.	I-weakness
2021-1566	Is the MTL strategy just taking more risk?	I-weakness
2021-1566	How statistically significant are the results?<sep>	I-weakness
2021-1566	Given these concerns the paper can not be accepted in its current form but we encourage authors to address these and resubmit. <sep>	B-decision

2021-1567	This paper investigates an improvement to the direct feedback alignment (DFA) algorithm where the "backward weights" are learned instead of being fixed random matrices.	B-abstract
2021-1567	The proposed approach essentially applies the technique of DFA to Kolen-Pollack learning.	I-abstract
2021-1567	While reviewers found the paper reasonably clear and thought the experiments were acceptable, there were significant concerns about the novelty of the approach and the fact that the proposed approach was a straightforward combination of existing ideas.	B-weakness
2021-1567	Further, the paper could have done a better job situating (and applying) the proposed method to DFA variants that have been proposed since the original DFA paper came out. <sep>	B-suggestion

2021-1582	This is a borderline case (quite comparable to the other borderline case in my batch).	B-rating_summary
2021-1582	The paper has received careful reviews and based on my weighting of the different arguments I arrive at an average score between 5.75 and 6. .	B-decision
2021-1582	The authors present some worthwhile ideas related to disentanglement that deserves more attention and that could spark more research in this direction.	B-strength
2021-1582	At the same time, the level of novelty and significance of this work remains a bit limited.	B-weakness
2021-1582	Taken together the paper is likely not compelling enough to be among the top papers to be selected for publication at ICLR. <sep>	B-decision

2021-1586	The paper introduces a new formulation for learning low-dimensional manifold representations via autoencoder mappings that are (locally) isometric by design.	B-abstract
2021-1586	The key technical ingredient is the use of a particular (theoretically motivated) weight-tied architecture coupled with isometry-promoting loss terms that can be approximated via Monte Carlo sampling.	I-abstract
2021-1586	Representative results on simple manifold learning experiments are shown in support of the proposed formulation. <sep>	I-abstract
2021-1586	The paper was generally well-received; all reviewers appreciated the theoretical elements as well as the presentation of the ideas. <sep>	B-strength
2021-1586	However, there were a few criticisms.	O
2021-1586	First, the fact that the approach requires Monte Carlo sampling in very high dimensions automatically limits its scope.	B-weakness
2021-1586	Second, the experiments seemed somewhat limited to simple (by ICLR standards) datasets.	I-weakness
2021-1586	Third and most crucially, the approach lacks a compelling-enough use case.	I-weakness
2021-1586	It is not entirely clear what local isometry enables, beyond nice qualitative visualizations (and moreover, what the isometric autoencoder provides over other isometry-preserving manifold learning procedures such as ISOMAP).	I-weakness
2021-1586	Some rudimentary results are shown on k-NN classification and linear SVMs, but the gains seem to be in the margins. <sep>	I-weakness
2021-1586	The authors are encouraged to consider the above concerns (and in particular, identifying a unique use case for isometric autoencoders) while preparing a future revision. <sep>	B-suggestion

2021-1589	The paper makes an attempt towards byzantine resilient federated learning, in the pressneece of backdoor attacks. <sep>	B-abstract
2021-1589	The method presented combines a clustering step with a poison elimination step, and seems to be effective against a range of current attacks. <sep>	I-abstract
2021-1589	Both steps are a bit ad hoc in nature, and do not come with provable guarantees. <sep>	B-weakness
2021-1589	Moreover, the algorithms presented will have a big negative impact on personalization as several models may be incorrectly discarded during and FL round. <sep>	I-weakness
2021-1589	The authors further point in their response that " no existing defense against backdoor attacks preserves the privacy of the clients' data."	B-rebuttal_process
2021-1589	This is in fact not true, as the differential privacy defense presented by the "Can you really backdoor FL" paper is in fact fully respective of user privacy. <sep>	I-rebuttal_process
2021-1589	At the same time, the work on backdoor attacks and defenses is reminiscent of the "cat and mouse" work in adversarial examples: an attack comes out, then a defense claims to protect against it, then an attack that incorporates that defense can be made stronger, and so on.	B-weakness
2021-1589	This is similar in the context of backdoor attacks. <sep>	I-weakness
2021-1589	In fact, a recent work [1] proposes that detecting backdoors is in the general computationally unlikely, rendering the generality of the proposed algorithm questionable, and also suggest a set of attacks that seem very hard to defend against.	I-weakness
2021-1589	(it is fine that the authors do not reference this work as it was published just recently) <sep>	O
2021-1589	As the paper lacks significant algorithmic novelty, solid guarantees, and also is unclear whether it is universally sound, the overall contribution is limited. <sep>	B-weakness
2021-1589	[1] Wang et al Attack of the tails: Yes, you really can backdoor federated learning, neurips 2020 <sep>	O
2021-1589	https://papers.nips.cc/paper/2020/file/b8ffa41d4e492f0fad2f13e29e1762eb-Paper.pdf <sep>	O

2021-1597	The reviewers agree that this paper has some interesting ideas.	B-strength
2021-1597	However, they believe it needs more work before it is ready for publication, especially so with regards to presentation (SDEs as GANs) and the experiments (backpropagating through the solver rather than using the adjoint dynamics).	B-rating_summary
2021-1597	These would significantly strengthen the paper, but would probably require another round of reviews. <sep>	O

2021-1664	The paper presents a hierarchical version of NMF for the CP decomposition of tensors. <sep>	B-abstract
2021-1664	The idea is similar to Chinocki et al 2007 and extends Gao et al 2019, and in Chinocki was presented for the standard linear formulation with regularisation terms.	I-abstract
2021-1664	The extension here doesn't use the standard ALS algorithm but rather presents a neural network analogue, though the functions are still linear, its just that back-prop etc.	I-abstract
2021-1664	are used for the computation.	I-abstract
2021-1664	The authors point out their formulation is a more flexible representation and optimisation (in response to AnonReviewer5), and thus represents an improvement.	I-abstract
2021-1664	While this is an interesting implementation, in NNs, the model is still fairly simple. <sep>	B-weakness
2021-1664	Moreover the experimental results are restricted to a few data sets.	I-weakness
2021-1664	There are literally hundreds of NMF variants in publication and many different evaluations are done.	I-weakness
2021-1664	The experimental work here, while showcasing the work, is not extensive.	I-weakness
2021-1664	For instance, more empirical comparisons should have been made against prior hierarchical NMF on a battery of data. <sep>	I-weakness
2021-1664	So this is good, publishable work, and the authors have repaired many of the issues raised by the reviewers.	B-rebuttal_process
2021-1664	The work, however, is borderline in empirical work and the contribution is not strong. <sep>	B-weakness

2021-1674	This work studies the question of universal approximation with neural networks under general symmetries.	B-abstract
2021-1674	For this purpose, the authors first leverage existing universal approximation results with shallow fully connected networks defined on infinite-dimensional input spaces, that are then upgraded to provide Universal Approximation of group-equivariant functions using group equivariant  convolutional networks. <sep>	I-abstract
2021-1674	Reviewers were all appreciative of the scope of this paper, aimed at unifying different UAT results under the same underlying 'master theorem', bringing a much more general perspective on the problem of learning under symmetry.	B-strength
2021-1674	However, reviewers also expressed concerns about the accessibility and readibility of the current manuscript, pointing at the lack of examples and connections with existing models/results.	B-weakness
2021-1674	Authors did a commendable job at adding these examples and incorporating reviewers feedback into a much improved revision. <sep>	B-rebuttal_process
2021-1674	After taking all the feedback into account, this AC has the uncomfortable job of recommending rejection at this time.	B-decision
2021-1674	Ultimately, the reason is that this AC is convinced that this paper can be made even better by doing an extra revision that helps the reader navigate through the levels of generality.	B-weakness
2021-1674	As it turns out, this paper was reviewed by three top senior experts at the interface of ML and groups/invariances, who themselves found that the treatment could be made more accessible --- thus hinting a difficult read for non-experts.	I-weakness
2021-1674	In particular, the main result of this work (theorem 9) is based on a rather intuitive idea (that one can leverage UAT for generic neural nets on the generator of an equivariant function), that requires some technical 'care' in order to be fleshed out.	I-weakness
2021-1674	The essence of the proof can be conveyed in simple terms, after which following through the proof is much easier.	I-weakness
2021-1674	Similarly, the paper quickly adopts an abstract (yet precise) formalism in terms of infinite-dimensional domains, which again clouds the essential ideas in technical details.	I-weakness
2021-1674	While the paper now contains several examples, this AC believes the authors can go to the extra mile of connecting them together, and further discussing the shortcomings of the result --- in particular, the remarks on tensor representations and the invariant case are of great importance in practice, and should be discussed more prominently.	I-weakness
2021-1674	Finally, while this work is only concerned with universal approximation, an important aspect that is not mentioned here is the quantitative counterpart, ie what are the approximation rates for symmetric functions under the considered models. <sep>	I-weakness

2021-1703	The paper proposes a novel way to have weight decay-like update rule.	B-abstract
2021-1703	Empirically, the authors claim that it improves generalization when applied to momentum-based optimizers and optimizers with coordinate-wise learning rates. <sep>	I-abstract
2021-1703	This paper has been thoroughly discussed, both in public and private mode. <sep>	O
2021-1703	The strength of this paper lies in the possible gain in generalization performance due the proposed change. <sep>	B-strength
2021-1703	The weaknesses are: <sep> the very confusing and not scientific motivation of the proposed change <sep> 	B-weakness
2021-1703	the experiments are not fully convincing<sep>	I-weakness
2021-1703	More in details, we all found the discussion on "stable" and "unstable" weight decay extremely confusing.	I-weakness
2021-1703	The claim of the paper is that "stable" weight decay should be preferred over "unstable" one.	I-weakness
2021-1703	However, to validate a scientific claim it is necessary to carry out an empirical or theoretical evaluation.	I-weakness
2021-1703	The theoretical one is simply missing: a number of proposition and corollaries are stated with some simple mathematical facts completely disconnected from the optimization or generalization issues.	I-weakness
2021-1703	As it is, removing these arguments would actually make the paper better. <sep>	B-suggestion
2021-1703	On the empirical side, there is no experiment that supports the simple claim that "the unstable weight decay problem may undesirably damage performance".	B-weakness
2021-1703	Instead, what we see are experiments in which the modified update rule seem to perform better, but they don't actually show that "stability" or "instability" are the specific issues at play here.	I-weakness
2021-1703	Indeed, any other explanation is equally valid and the experiments do not support any specific one, but rather they can only support the claim that the proposed algorithm might be better than some other optimization algorithms.	I-weakness
2021-1703	The specific reason why this is happening is not clear. <sep>	I-weakness
2021-1703	Turning to the empirical evaluation, the discussion elicited the fact that, a part for CIFAR10, the experiments are carried out without tuning of the learning rates.	I-weakness
2021-1703	Hence, it is difficult for us to even validate the claim of superiority of the method.	I-weakness
2021-1703	I don't subscribe to the idea that a deep learning paper requires experiments on ImageNet to be valid.	I-weakness
2021-1703	Yet, given that there is no supporting theory in this paper, the empirical evaluation should be solid and thorough. <sep>	I-weakness
2021-1703	For the above reasons, the paper cannot be published at ICLR. <sep>	B-decision

2021-1710	This paper proposes a method for predicting higher-order structure in time-varying graphs.	B-abstract
2021-1710	The paper was reviewed by three expert reviewers, and while they expressed appreciation for the sensible solution, they have remaining concerns about the novel contributions and comparisons (analytical and empirical) with previous approaches.	B-weakness
2021-1710	Also, the paper would be clearer if examples are used to illustrate the important points of the paper.	I-weakness
2021-1710	The authors are encouraged to continue research, taking into consideration the detailed comments provided by the reviewers. <sep>	B-suggestion

2021-1714	While the submission has promising components, the reviewers were not able to reach a consensus to recommend acceptance.	B-rating_summary
2021-1714	The main concerns is that (1) theorem statements and assumptions are not clearly explained, and (2) the novelty of the approach is not made clear, and (3) there remain concerns on whether the experimental results are due to hyperparameter search or improvements due to the model. <sep>	B-weakness

2021-1723	The authors investigate different tokenization methods for the translation between French and Fon (an African low-resource language).	B-abstract
2021-1723	Low-resource machine translation is a very important topic and it is great to see work on African languages - we need more of this! <sep>	B-strength
2021-1723	Unfortunately, the reviewers unanimously agree that this work might be better suited for a different conference, for example LREC, since the machine learning contributions are small.	B-rating_summary
2021-1723	The AC encourages the authors to consider submitting this work to LREC or a similar conference. <sep>	B-decision

2021-1737	The paper proposed a useful incremental extension to the monotonic GMM attention by incorporating source content. <sep>	B-abstract
2021-1737	It has shown comparable performance for online and long-form speech recognition, but falls behind on the machine translation task.	I-abstract
2021-1737	For online ASR, it would be more convincing to include latency comparisons across different streaming models besides WERs. <sep>	B-suggestion
2021-1737	The presentation of the paper can be further improved although it already got better based on reviewers' comments. <sep>	B-rebuttal_process
2021-1737	As in the discussion, a more accurate description of the method would be "multi-head Gaussian attention" instead of GMM attention. <sep>	B-weakness
2021-1737	The main factor for the decision is limited novelty and clarity can be further improved. <sep>	I-weakness

2021-1738	This paper proposes a method for neural architecture search (NAS) based on adversarial methods.	B-abstract
2021-1738	It uses a discriminator trained to distinguish between random vs. good architectures, letting the discriminator's scores serve as a reward signal for an autoregressive generator.	I-abstract
2021-1738	I agree with AR1: this is a nice and clever idea.	B-strength
2021-1738	Reviewers generally agreed that the method was interesting, eg it's quite flexible in that it's able to incorporate constraints, and that the evaluation is rather extensive and shows that the method performs well across the board.	I-strength
2021-1738	Many minor criticisms were raised and addressed well by the authors in their responses and manuscript updates. <sep>	B-rebuttal_process
2021-1738	The major criticism shared by most reviewers was the high methodological complexity of the proposed approach, and the proportionally small gains shown over much simpler baselines.	B-weakness
2021-1738	This criticism remained despite the authors' responses.	B-rebuttal_process
2021-1738	The method is indeed complex: the same method without any adversarial component already performs well, and many important details of the model are relegated to Appendix A.2.	I-rebuttal_process
2021-1738	(I would recommend, for example, moving Fig.	B-suggestion
2021-1738	2 to the main text if at all possible.	I-suggestion
2021-1738	Also, the Appendix can/should be included in the main PDF for ICLR, rather than in supplementary material, as AR1 mentions.)	I-suggestion
2021-1738	It was not clear to reviewers that the adversarial component of the approach has a significant benefit.	B-rebuttal_process
2021-1738	The authors respond by pointing to Table 7 showing that the discriminator reduces the number of queries and points out that in reality these queries correspond to expensive evaluations.	I-rebuttal_process
2021-1738	If this is a major selling point of the method (it sounds like it could be), it should be highlighted and analyzed far more -- at least moved to the main text rather than an Appendix -- ideally with a real-world evaluation showing a practical large improvement in overall wall-clock time, rather than a benchmark where these evaluations are free.	B-suggestion
2021-1738	Perhaps the exclusive reliance on these benchmarks, though undoubtedly useful for quick experimentation, in the end holds back the paper and prevents the method's benefits from becoming apparent to the readers. <sep>	B-weakness
2021-1738	As a minor point (also raised by AR1), the paper is formatted incorrectly for ICLR: the font color is off, and more importantly the PDF is unsearchable (text cannot be selected, ctrl-F does not work), which makes it very difficult to quickly reference and review.	I-weakness
2021-1738	Please try not to stray from the conference-provided style file for future submissions. <sep>	B-suggestion
2021-1738	I appreciate the cleverness of the method, the extent of the evaluation, and the thorough responses to the reviews.	B-strength
2021-1738	However, unfortunately with the current presentation, it is too difficult to discern the benefit of the proposed approach from the manuscript.	B-weakness
2021-1738	The approach is nonetheless intuitively appealing and seems quite promising, and I hope the authors will take the reviewers' good feedback into account and resubmit the paper in the future. <sep>	B-decision

2021-1739	There are two main contributions in this paper.	O
2021-1739	First, the use of NN from the same cluster as "views" of the data as understood in classical contrastive learning.	B-strength
2021-1739	Second, the use of additional augmentation techniques, namely cutMix and multi-resolution.	I-strength
2021-1739	The reviewers noted that the paper is written well and easy to understand, that the ablation study is conducted well and that the model shows good empirical performance on several tasks. <sep>	I-strength
2021-1739	At the same time, the somewhat limited novelty of the paper was also discussed.	B-weakness
2021-1739	As noted by R4, all aspects of the present paper have been discussed in previous work.	I-weakness
2021-1739	The difference with previously published clustering-based SSL methods was also not very clear.	I-weakness
2021-1739	This was discussed in the rebuttal but without strong evidence supporting the claims.	B-rebuttal_process
2021-1739	Moreover, the ablation study is conducted on models that are trained for 200 epochs.	B-weakness
2021-1739	While this is understandable from a pragmatic point of view, the conclusions may be completely different when the model is fully optimised. <sep>	I-weakness
2021-1739	Because of all the points raised in the discussions, this paper is a too close to borderline to be accepted.	B-decision
2021-1739	We recommend the authors improve the manuscript given the feedback provided in the reviews and discussion and resubmit to another venue. <sep>	I-decision

2021-1806	The authors propose a new approach to topology optimization to address over-smoothing in GCNs.	B-abstract
2021-1806	This is a borderline paper.	B-rating_summary
2021-1806	Topology optimization is clearly important and relevant and the approach tries to optimize the topology (add/delete edges) by viewing the problem as a latent variable model and aiming to optimize the graph together with the GCN parameters to maximize the likelihood of observed node labels.	B-strength
2021-1806	A number of related joint topology optimization approaches exist, however, as discussed in the reviews and the responses.	B-rebuttal_process
2021-1806	The proposed methodology is termed variational EM but is a bit heuristic in the sense that E and M steps do not follow a consistent criterion (the direction of KL is flipped between the steps).	B-weakness
2021-1806	A number of comparisons are provided with consistent gains though the gains appear relatively small.	I-weakness
2021-1806	No error bars are provided despite request to add them to better assess the significance of these results.	I-weakness
2021-1806	It remains unclear whether the gains are worth the added complexity. <sep>	I-weakness

2021-1822	This paper shows that linear layers can be replaced by butterfly networks.	B-abstract
2021-1822	Put simple, the paper follows the idea of sketching to design new architectures that can reduce the number of trainable parameters and also gives the theoretical and empirical analysis to validate this claim.	I-abstract
2021-1822	In this regard, the paper would be  appealing.	B-strength
2021-1822	But the theoretical results given in this paper are incremental. <sep>	B-weakness

2021-1830	The authors propose a pretraining strategy learning inductive biases in transformers for deduction, induction, and abduction.	B-abstract
2021-1830	Further, the claims and results seem to indicate that such pretraining is more successful in transformers which provide a more malleable architecture for learning inductive (structural) biases.	I-abstract
2021-1830	There are open questions that remain, specifically surrounding disentangling high performance from structural bias learning (ie is pretraining doing what we think it is) and whether datasets are the "correct" mechanism for imparting such biases/knowledge. <sep>	B-weakness

2021-1832	The paper proposes a unification of three popular baseline regularizers in continual learning.	B-abstract
2021-1832	The unification is realized through a claim that they all regularize (surprisingly) related objectives. <sep>	I-abstract
2021-1832	The key strengths of the paper highlighted by the reviewers were: <sep> The established connection is valuable and interesting, even if weaker than suggested originally <sep> 	B-strength
2021-1832	Good motivation (unifying different regularization methods is useful for the community) <sep>	I-strength
2021-1832	Clear writing<sep>	I-strength
2021-1832	The key weakness of the paper is a weak empirical validation of the claim that these three regularizers work because they regularize the norm of the gradient (as mentioned in the discussion by R3).	B-weakness
2021-1832	Rather, the key claims are correlational.	I-weakness
2021-1832	The authors correctly say that (1) the three regularizers all regularize related objects (namely different norms of the gradient) and (2) they reduce forgetting.	I-weakness
2021-1832	However, it is not sufficiently well demonstrated that (1) => (2).	I-weakness
2021-1832	Relatedly, given that the paper does not have a very clear theoretical contribution, it would be really helpful to demonstrate utility.	I-weakness
2021-1832	It would be useful to extend experiments that apply these insights to developing novel regularizers or improving/simplifying hyperparameter tuning. <sep>	B-suggestion
2021-1832	Additionally, in the review process, the link was discovered to be weaker than originally suggested.	B-weakness
2021-1832	The paper casts the relation in terms of the Fisher Information Matrix, which suggests it is theoretical and sound.	I-weakness
2021-1832	After the discussion, it seems that viewing this relationship in terms of the Fisher Information Matrix is somewhat misleading.	I-weakness
2021-1832	The three different regularization methods all regularize different norms of the gradient (L1 or L2), which are empirically, and under some assumption theoretically, related.	I-weakness
2021-1832	More precisely, EWC regularizes the trace of the Empirical Fisher, which is equivalent to the L2 norm of the gradient of the loss function.	I-weakness
2021-1832	SI regularizes a term similar to the L1 norm of the gradient.	I-weakness
2021-1832	These effects were seen by the reviewers to be somewhat loosely related to the Fisher Information Matrix. <sep>	I-weakness
2021-1832	Based on the above, I have to recommend rejection.	B-decision
2021-1832	I would like to thank the Authors for submitting the work for consideration to ICLR.	O
2021-1832	I hope the feedback will be useful for improving the work. <sep>	O

2021-1858	This paper proposed an additional training objective for unsupervised neural machine translation (UNMT).	B-abstract
2021-1858	They first train two UNMT models and use these models to generate pseudo parallel corpora.	I-abstract
2021-1858	These parallel corpora are used to optimize the UNMT training objective.	I-abstract
2021-1858	The experiments are conducted on several language pairs and they also compared with several alternative works. <sep>	I-abstract
2021-1858	All the reviewers admit that the proposed method is straightforward and effective.	B-strength
2021-1858	The authors claim that the new training objective is used to enhance the "data diversification".	B-rebuttal_process
2021-1858	This point has been questioned by the reviewers.	I-rebuttal_process
2021-1858	Some reviewers are convinced by the response and some still have different opinions.	I-rebuttal_process
2021-1858	From my point of view, the proposed method can also be considered as a kind of combination of  (pseudo) supervised NMT and unsupervised NMT. <sep>	B-weakness
2021-1858	The presentation and description of its key contributions seem unclear.	I-weakness
2021-1858	However, we encourage the authors to modify their paper and we believe this proposed method can inspire the MT community for further research.	B-suggestion
2021-1858	At the moment, the paper is seen as not yet ready for publication at this time. <sep>	B-decision

2021-1903	Three reviewers recommend rejecting or weak reject.	B-rating_summary
2021-1903	The studied problem is interesting, but as one reviewer pointed out, it is not that clear how this work changes our theoretical understanding of those methods or what they imply for applications.	B-weakness
2021-1903	Overall, I feel this work is on the borderline (probably it deserves higher score than the current score), but probably below the acceptance bar at the current form. <sep>	B-decision

2021-1904	This paper examines adversarially trained robust models, and finds that accuracy disparity is higher than for standard models.	B-abstract
2021-1904	The authors introduce a method they call Fair Robust Learning using Lagrange multipliers to minimize overall robust error while constraining the accuracy discrepancy between classes. <sep>	I-abstract
2021-1904	In discussion, consensus was reached that the observations and approach are interesting but the paper is not yet ready for publication.	B-rating_summary
2021-1904	The main concern is that it is not clear if the class accuracy disparity is due to adversarial training, or simply due to lower accuracy in general.	B-weakness
2021-1904	Please see reviews and public discussion for further details. <sep>	O

2021-1906	The paper proposes a new framework for online hypothesis testing aimed at detecting causal effects (of treatments on outcomes) within subgroups in online settings where treatments are randomized.	B-abstract
2021-1906	Such settings occur in online advertising where different versions of the same website may be presented to a set of otherwise exchangeable users via A/B testing. <sep>	I-abstract
2021-1906	Under the standard causal assumptions of SUTVA, and sequential ignorability, in addition to a set of regularity conditions, the authors derive a result (Theorem 1) leading to an online test (Theorem 2).	I-abstract
2021-1906	Since the resulting test's limiting distribution does not have an exact analytic form, the authors instead propose a bootstrap approach to determine a set of parameters to properly control the error rate. <sep>	I-abstract
2021-1906	The author validate their approach by a simulation study, as well as via a user click log data from Yahoo! <sep>	I-abstract
2021-1906	The reviewer opinion was somewhat split on this paper, in particular some reviewers raised concern about some (conceptually significant) typos, interpretability of assumptions, and the need for parametric assumptions (the dichotomy between linear models and neural networks is surely a false one -- the semi-parametric literature obtains nice parametric style results, although perhaps not always for tests, without assuming parametric likelihoods all the time). <sep>	B-weakness

2021-1907	This paper first makes the observation that incidental supervisory data can be used to define a new prior from which to calculate a PAC-Bayes generalization guarantee.	B-abstract
2021-1907	This observation can be applied to any setting where there is unsupervised or semi-supervised pre-training followed by fine-tuning on labeled data.	I-abstract
2021-1907	The PAC-Bayes bound is valid when applied to the fine-tuning.	I-abstract
2021-1907	For example, one could use an L2 bound (derived from PAC-Bayes) on the difference between the fine-tuned parameters and pre-trained parameters. <sep>	I-abstract
2021-1907	But the paper proposes evaluating the value of pre-training before looking at any labeled data.	I-abstract
2021-1907	Let 0 be the prior before unsupervised or semi-supervised training and let ~ be the prior after pre-training.	I-abstract
2021-1907	The paper proposes using the entropy ratio H(0)/H(~) as a measure of the value of the pre-training.	I-abstract
2021-1907	As the reviewers note, this is not really related to PAC-Bayes bounds.	B-weakness
2021-1907	Furthermore, it is clearly possible that the pre-training greatly focuses the prior but in a way that is detrimental to learning the task at hand. <sep>	I-weakness
2021-1907	I have to side with the reviewers that feel that this is below threshold. <sep>	B-decision

2021-1912	This paper addresses an interesting problem and all reviewers agree.	B-strength
2021-1912	Most reviewers found the paper difficult to understand and it was hard to see the novel contributions.	B-weakness
2021-1912	The paper will need a significant revision before publication. <sep>	B-decision

2021-1918	The paper has good contributions to a challenging problem, leveraging a Faster-RCNN framework with a novel self-supervised learning loss.	B-strength
2021-1918	However reviewer 4 and other chairs (in calibration) considered that the paper does not meet the bar for acceptance.	B-rating_summary
2021-1918	The other reviewers did not champion the paper either, hence i am proposing rejection. <sep>	B-decision
2021-1918	Pros: <sep> R1 and R3 agree that the proposed model improves over related models such as MONET. <sep> 	B-strength
2021-1918	The value of the proposed self-supervised loss connecting bounding boxes and segmentations is well validated in experiments.<sep>	I-strength
2021-1918	Cons: <sep> R4 gives good suggestions that may be useful to reach a broader readership, namely introducing more of the concepts used in the paper., eg "stick breaking, spatial broadcast decoder, multi-otsu thresholding" so it becomes more self-contained.	B-weakness
2021-1918	R4 also suggests improving the writing more generally. <sep>	I-weakness
2021-1918	R4 still finds the proposed "method quite complex yet derivative" after the rebuttal. <sep>	B-rebuttal_process
2021-1918	All reviewers complain about lack of experiments in real data, but the authors did revise their paper and add some coco results in the appendix.	I-rebuttal_process
2021-1918	These could be part of the main paper in a future version.<sep>	O

2021-1928	Three reviewers are mildly positive, while one is negative.	B-rating_summary
2021-1928	The substantive comments of the reviewers are consistent with each other; it is merely their evaluations that differ. <sep>	O
2021-1928	One contribution of the paper is that it shows how using temperature tuning can yield similar accuracy to using batch normalization; this is useful because batch normalization is not always possible.	B-strength
2021-1928	The revised paper shows improvements, and we appreciate the engagement of the authors with the reviewer comments.	B-rebuttal_process
2021-1928	However, there are remaining weaknesses such as a weak argument based on the empirical.results. <sep>	I-rebuttal_process
2021-1928	This paper can be improved based on the comments made by the reviewers.	B-suggestion
2021-1928	We encourage the authors to resubmit to a future venue. <sep>	B-decision

2021-1983	This paper proposes a method for modeling higher-order interactions in Poisson processes.	B-abstract
2021-1983	Unfortunately, the reviewers do not feel that the paper, in its current state, meets the bar for ICLR.	B-rating_summary
2021-1983	In particular, reviewers found the descriptions unclear and the justifications lacking.	B-weakness
2021-1983	While the responses did aid the reviewers understanding, the paper would benefit from rewriting and more careful thought given to the experimental design. <sep>	B-rebuttal_process

2021-2005	This work proposes an efficient method for modelling long-range connections in point-cloud data.	B-abstract
2021-2005	Reviewers found the paper to be generally well-written.	B-strength
2021-2005	On the less positive side, reviewers felt that the novelty of the work was marginal, and that the experimentation, limited to synthetic data in one domain, was too limited.	B-weakness
2021-2005	These concerns remain after the discussion phase.	B-rebuttal_process
2021-2005	In addition, the authors stated during the discussion that "Our goal is indeed to develop an efficient strategy to model LRIs in real chemical and materials systems", which conflicts with the presentation of the work as motivated by more general point cloud modelling problems.	I-rebuttal_process
2021-2005	Given these weaknesses, the final decision was to reject. <sep>	B-decision

2021-2007	The authors proposed to pre-process the original input features into a low dimensional term and its corresponding residual term via SVD.	B-abstract
2021-2007	The paper empirically demonstrated the neural networks trained on such factorized exhibit faster convergence in training.	I-abstract
2021-2007	Several issues of clarity were addressed during the rebuttal period by the authors. <sep>	B-rebuttal_process
2021-2007	However, the reviewers still felt that there were some remaining fundamental issues with the paper,<sep>	B-weakness
2021-2007	The motivation is not echoed in the experiments, namely most of the experiments on CIFAR and CatDog dataset using a low dimensional factorization of d=1 which is trivial and often part of the whitening preprocessing.<sep>	I-weakness
2021-2007	The proposed factorization via SVD will be difficult to scale up to high dimensional features, large training sets and higher d >> 1. <sep>	I-weakness
2021-2007	The empirical experiments show a marginal improvement in the training speed, especially in the image recognition tasks, yet there seems an early plateau in test performance when compared to the baselines.<sep>	I-weakness
2021-2007	The theoretical analysis in Section 2 studied linear models.	I-weakness
2021-2007	Yet, the rest of the paper focuses on non-linear neural networks.	I-weakness
2021-2007	It is difficult to see the connection between the analysis and the rest of the paper.<sep>	I-weakness
2021-2007	Thus, I recommend rejection of the paper at this time as the current version of the paper needs further development, and non-trivial modifications, to be broadly applicable. <sep>	B-decision

2021-2013	The paper presents a self-supervised model based on a contrastive autoencoder that can make use of a small training set for upstream multi-label/class tasks. <sep>	B-abstract
2021-2013	Reviewers have several concerns, including the lack of comparisons and justification for the setting, as well as the potentially narrow setting.	B-weakness
2021-2013	Overall, I found the paper to be borderline, the cons slightly greater than the pros, so I recommend to reject it. <sep>	B-decision

2021-2016	This paper analyzes several neighbor embedding methods-- t-SNE, UMAP, and ForceAtlas2-- by considering their objectives as consisting of attractive and repulsive terms.	B-abstract
2021-2016	The main hypothesis is that stronger repulsive terms contribute towards learning discrete structures, while stronger attractive terms contribute towards learning continuous/manifold structures.	I-abstract
2021-2016	The paper empirically explored the space parameterized by the relative weighting of the attractive and repulsive terms for the t-SNE and UMAP algorithms, using several data sets, and qualitatively confirmed their conclusions about the impact of the attractive and repulsion terms as the relative weights vary. <sep>	I-abstract
2021-2016	The experimental validation of the paper's main hypothesis is thorough and the use of diverse data sets and neighbor embedding methods is appreciated-- as the authors point out, several reviewers missed this contribution.	B-rebuttal_process
2021-2016	However, several reviewers point out that the insight presented in the paper is already largely present in the literature, and that beyond its analysis the paper does not present new algorithms based on this insight.	I-rebuttal_process
2021-2016	The authors rebut this claim by arguing that the novelty of the paper lies in it: (1) showing the contrary to the established opinion, UMAP works despite, instead of because, it uses cross-entropy loss, and (2) the paper offers for the first time a theoretical understanding of why ForceAtlas2 highlights continuous developmental trajectories, and (3) prior work has not made the connection between UMAP, ForceAtlas2, and t-SNE or suggested using exaggeration throughout the optimization process for t-SNE rather than simply as a warm-up.	I-rebuttal_process
2021-2016	The paper does indeed present intuitions for (1)-(3) based on the attraction-repulsion ideas, and makes the connection between these neighbor embedding algorithms by viewing them as variations on the theme of attraction-repulsion, but these intuitions are not significant steps forward with respect to what is already known about how neighbor embeddings balance attraction and repulsion.	B-weakness
2021-2016	The mathematical analyses consist of stating the gradient for the algorithms and explaining how weighing the attraction and repulsion terms differently lead to different qualitative observations.	B-abstract
2021-2016	The use of exaggeration throughout the optimization process is straightforward, and no strong mathematical characterization of the properties of the resulting algorithm is given. <sep>	B-weakness
2021-2016	It is recommended that this paper be rejected, as it consists of a thorough empirical validation of an understanding of the trade-off between attractive and repulsive forces in neighbor embedding methods that was already present in the literature, along with some straightforward arguments connecting several popular neighbor embedding methods, but does not introduce any significantly new actionable insights or novel algorithms. <sep>	B-decision

2021-2025	This paper considers convex optimization problems whose solutions involve the solution of linear systems defined in terms of the Hessian.	B-abstract
2021-2025	It presents algorithms that reduce the runtime of standard iterative approaches to solving these problems by iteratively sketching the Hessian; the novelty lies in the fact that the authors use the idea of learned sketches which have been used prior for problems in data mining.	I-abstract
2021-2025	In particular, the authors use the approach to learning sketches of Liu et al, 2020 to learn the entries in sparse sketching matrices for the Hessian, and propose using the Iterative Hessian Sketch algorithms of Pilanci and Wainright, 2016 to iteratively solve the concerned optimization problem.	I-abstract
2021-2025	The advantages of learned sketches are that they may allow using smaller sketch sizes while making progress on the problem, as they are learned to work well on the distribution of Hessians from which the problem instance is drawn. <sep>	I-abstract
2021-2025	The consensus of the reviews is that the idea of using learned sketches for convex optimization seems to be novel, and this paper is an interesting attempt, but falls short of the level of contribution required for publication in ICLR.	B-rating_summary
2021-2025	The main concern is that the theory provided for the use of the learned sketches is incremental: the analysis does not reflect the fact that the sketches are learned; instead, the algorithm builds in a safeguard by using both a random sketch and a learned sketch, and the analysis uses the properties of the random sketch to proceed.	B-weakness
2021-2025	The empirical results are suggestive, but the convergence rates of the learned and random sketches do not vary much, so the benefits seem marginal for most of the problems considered (with the exception of a standard least squares problem, for which we know learned sketching performs well). <sep>	I-weakness
2021-2025	The paper is recommended to be rejected, as the theory is weak, and the empirical results are borderline. <sep>	B-decision

2021-2027	The paper has two contributions.	O
2021-2027	A novel benchmark for clinical multi-modal multi-task learning based on the already released MIMIC III and a multi-modal multi-task machine learning model.	B-strength
2021-2027	While the paper does show value in providing a curated benchmark and combining/unifying existing approaches to a timely problem, the reviewers agree that the paper provides insufficient novelty to warrant publication. <sep>	B-rating_summary

2021-2045	This paper is a systematic study of how assumptions that are present recent theoretical meta-learning bounds are satisfied in practical methods, and whether promoting these assumptions (by adding appropriate regularization terms) can improve performance of existing methods.	B-abstract
2021-2045	The authors review common themes in theoretical frameworks for a meta learning setting that involves a feature learning step, based on which linear predictors for a variety of tasks are trained.	I-abstract
2021-2045	Statistical guarantees for such a framework (that is, statistical guarantees for the performance of trained on an additional target task) are based on the assumption that the set of weight vectors of the linear predictors span the space (ie exhibit variety) and that the training tasks all enjoy a similar margin separability (that is, that the representation is not significantly better suited for some of the tasks than others). <sep>	I-abstract
2021-2045	The current submission, cleanly reviews the existing literature, distills out these two properties and then proposes a regularization framework (that could be added to various meta-learning algorithms) to promote these properties in the learned feature representation. <sep>	I-abstract
2021-2045	Finally, the authors experimentally evaluate to what degree the properties are already observed by some meta learning methods, and whether the proposed additions will improve performance.	I-abstract
2021-2045	It is established that adding the regularization terms improves performance on most tasks.	I-abstract
2021-2045	The authors thus argue that incorporating insights obtained form recent theoretical frameworks of analysis, can lead to improved performance in practice.	I-abstract
2021-2045	Naturally, the purpose of the presented results is not to establish a new state of the art on a set of benchmark tasks, but to systematically study and compare the effect of adding regularization terms that will promote the properties that are desirable for a  feature representation based on statistical bounds. <sep>	I-abstract
2021-2045	I would argue that the research community should support this type of studies.	O
2021-2045	The work is well presented and conducted.	B-strength
2021-2045	Most importantly, the study has a clear and general message, that will be valuable for researchers and practitioners working in on meta-learning. <sep>	I-strength
2021-2045	However, the reviewers did not recommend publishing this type of study for ICLR.	B-rating_summary
2021-2045	The authors are encouraged to resubmit their work to a different venue. <sep>	B-decision

2021-2056	This paper evaluates the extent to which disentangled representations can be recovered from pre-trained GANs with style-based generators by finding an orthogonal basis in the space of style vectors, and then training an encoder to map images to coordinates in the resulting latent space.	B-abstract
2021-2056	To construct the orthogonal basis, the authors consider 3 recently proposed methods for controllable generation, along with a newly developed generalization of one of these methods.	I-abstract
2021-2056	The authors evaluate metrics for disentanglement for 4 datasets, consider an abstract visual reasoning task, and compute unfairness scores. <sep>	I-abstract
2021-2056	Reviewers expressed diverging opinions on this paper.	B-rating_summary
2021-2056	R2 is in support of acceptance,  R3 finds the paper borderline but is leaning towards acceptance, whereas R4 is critical.	I-rating_summary
2021-2056	R2 and R4 engaged in a relatively detailed discussion, but maintained their scores. <sep>	I-rating_summary
2021-2056	Having read the paper, the metareviewer feels this submission indeed has strengths and weaknesses.	O
2021-2056	On the one hand, the main results are notable; it is worth reporting that disentangled representations can be recovered from pretrained GANs is a relatively straightforward manner.	B-strength
2021-2056	In this context, the metareviewer feels that some comments by R4 are more critical than is warranted.	B-ac_disagreement
2021-2056	The authors do not necessarily have to show that GAN-based methods uniformly improve upon VAE-based methods, either in terms of disentanglement metrics or in terms of sensitivity to hyperparameters.	I-ac_disagreement
2021-2056	The main claim in this submission is that GAN-based methods are mostly comparable to VAE-based methods, and this claim is both sufficiently notable and sufficiently supported by experimental results. <sep>	I-ac_disagreement
2021-2056	At the same time, this submission is not without flaws.	B-weakness
2021-2056	The writing is on the rough side, and as R4 notes the authors have removed all white space between paragraphs.	I-weakness
2021-2056	The metareviewer also feels it is not satisfactory to show a box plot for GAN-based methods in Figure 2 and ask the reader to compare these plots to the violin plots for VAE-based methods in the Locatello paper.	I-weakness
2021-2056	The authors need to find a way to make a more direct comparison here.	B-suggestion
2021-2056	R4's comments about the comparison in the abstract-reasoning setting are also well-taken  here the baseline employs standard (entangled) models, so it is unclear what conclusions we should draw from this experiment.	B-weakness
2021-2056	Similarly the unfairness results once again appeal to an indirect comparison to results in the  Locatello paper on this topic. <sep>	I-weakness
2021-2056	On balance, the metareviewer is inclined to say that this submission, in its current form, falls just below the threshold for acceptance.	B-decision
2021-2056	These results are clearly of note to the community and worth reporting, but the presentation has enough flaws that another round of reviews is warranted based on a revised manuscript.	B-weakness
2021-2056	The metareviewer hopes to see this paper appear a conference in the (near) future. <sep>	B-decision

2021-2058	All four knowledgeable referees have indicated reject due to many concerns.	B-rating_summary
2021-2058	In particular, reviewers pointed out that the novelty of this paper is not clear because the difference from related work is very limited (ie, the difference from Z. Wang and S. Ji is not clear, other than using one additional layer),  and they were concerned that the results of the experiment are not convincing (For example, the results reported in this paper are significantly inferior to those reported in other papers, the GNN architecture used is limited, and the performance difference especially in the additional experiments in the revision, is very marginal).	B-weakness
2021-2058	No reviewers were convinced by the authors' claims even through the author's rebuttal and revision. <sep>	B-rebuttal_process
2021-2058	One important note: Reviewers have stated that they did not explicitly check the identity of the author and did not pose a problem on this, but if we follow the link specified in the original submission, we can see the identity of the author, which may be considered as a violation of the double-blind policy.	B-weakness
2021-2058	This is a small and regrettable mistake, but it can be a serious problem in the review process.	I-weakness
2021-2058	In this review process, reviewers unanimously suggested rejection even ignoring this issue, but it seems that you need to pay attention in your future submissions. <sep>	B-rating_summary

2021-2068	There was fairly detailed discussion among three of the four reviewers.	O
2021-2068	The fundamental concern of the reviewers is regarding the contribution of the paper.	B-weakness
2021-2068	During the rebuttal, the authors clarified the following: <sep> 	B-rebuttal_process
2021-2068	while the effects of varying uncertainty / horizon lengths is well-understood for Bayes-optimal policies, it is not understood for existing meta-RL approaches, which is the topic of this paper<sep>	I-rebuttal_process
2021-2068	That is, the contribution of the paper is to understand the effects of varying uncertainty/horizon lengths for meta-RL approaches.	I-rebuttal_process
2021-2068	However, it is known in prior work that meta-RL algorithms such as RL^2 can implement Bayes-optimal policies in principle.	I-rebuttal_process
2021-2068	As a result, it's not clear whether this contribution is significant relative to prior knowledge, and this paper does not seem to bring any new insights. <sep>	I-rebuttal_process
2021-2068	An alternative framing of the paper would be to consider the question of how meta-RL solutions compare to Bayes-adaptive optimal policies.	I-rebuttal_process
2021-2068	While this framing would be interesting and novel, the current version of the paper does not sufficiently answer this question, since the only experiments include RL^2 (and such a study would require experimenting with more sophisticated meta-RL algorithms beyond RL^2). <sep>	I-rebuttal_process
2021-2068	As such, this paper isn't suitable for publication at ICLR in its current form. <sep>	B-decision

2021-2080	The paper addresses learning with noisy labels, by detecting and correcting samples with noisy labels.	B-abstract
2021-2080	Reviewers had concerns about the empirical evaluations, specifically about comparing to additional methods, about hyperparameter tuning, and about the improvements being vey small.	B-weakness
2021-2080	There was also a concern that the analysis of the objective does not take into account explicitly the L2 regularization induced by weight decay.	I-weakness
2021-2080	Based on these concerns the paper is not ready yet for publication. <sep>	B-decision

2021-2081	The reviewers noted that this is an important, interesting but difficult topic.	B-strength
2021-2081	They appreciated that the authors clarified their assumptions in the theorem statements.	I-strength
2021-2081	Nevertheless, they recommend the authors to detail in depth when the method work better than the method where only the covariates are adjusted.	B-suggestion
2021-2081	They still think that the paper would require major modifications to be considered for publication hence the decision is rejection the paper. <sep>	B-decision

2021-2094	Reviewer #2 has written a nice summary of the paper which I quote below. <sep>	O
2021-2094	"The core idea is simple - which is a strength in my view - and does not require retraining the base language model, which could be important as language models become more expensive to train.	B-strength
2021-2094	However, the clarity and experiments in this paper fall short: the experimental setup has issues, the effect on perplexity is quite large but relegated to the Appendix, several claims are speculative and lacking corresponding experimental evidence, and it is unclear how the additional heuristics affect performance. <sep>	B-weakness
2021-2094	The method seems promising, but with the current experiments it is difficult to draw conclusions about how the method affects performance and which parts of it are necessary; given that this is an empirical paper, I would therefore not recommend acceptance in its current form." <sep>	B-decision
2021-2094	Key Strengths<sep> Well-motivated problem of considerable interest <sep>	B-strength
2021-2094	A relatively straightforward Bayesian solution <sep>	I-strength
2021-2094	Proposed solution is computationally efficient compared to other competing approaches.<sep>	I-strength
2021-2094	The paper has been thoroughly reviewed by the reviewers and as a result numerous questions has surfaced.	O
2021-2094	While the authors addressed most of the questions adequately, there are still many unanswered questions.	B-rebuttal_process
2021-2094	They include: <sep> Readability issues highlighted by Reviewer #1 <sep> 	B-weakness
2021-2094	Reviewer #1: "how did you measure model confidence about the toxicity label" <sep>	I-weakness
2021-2094	Reviewer #4: The perplexity gets much worse as the gedi training is introduced (ie  decreases), eg going from 25 to 45 on IMDb.	I-weakness
2021-2094	This result is in the Appendix, and perplexity is never evaluated/reported in the other experiments. <sep>	I-weakness
2021-2094	Reviewer #4: Crucially, the GEDI training does not appear to help over just re-weighting with the conditional LM ( vs. ).	I-weakness
2021-2094	Could the authors comment on this result?	I-weakness
2021-2094	How well does domain transfer work for less similar domains?	I-weakness
2021-2094	How is perplexity affected for the models reported in Table 2? <sep>	I-weakness
2021-2094	Reviewer #4: How small can the conditional LM be?	I-weakness
2021-2094	Why was medium used instead of small?	I-weakness
2021-2094	What if large was used?	I-weakness
2021-2094	Does the conditional LM need to be a large-scale pretrained model (it would be nice to see a baseline of a simpler conditional LM)? <sep>	I-weakness
2021-2094	Several heuristics are used:  weighting, nucleus filtering, keeping tokens over a threshold, repetition penalty, and rescaling the logits to positive (used in only one experiment). <sep>	I-weakness
2021-2094	How does each of these affect performance?	I-weakness
2021-2094	There are no ablations, and given the small differences in some of the experiments it is unclear whether performance would actually be worse if we changed one of the heuristics.	I-weakness
2021-2094	One outcome may be that the method only works for a careful balance of hyperparameters, which could be fine, but we don't have a sense of the variation. <sep>	I-weakness
2021-2094	Reviewer #2: The output in Table 6 makes me doubt how the experiments are badly controlled.	I-weakness
2021-2094	The outputs from positive and negative sentiment are totally different and almost random text, meaning that the content of the generators is not controlled properly.<sep>	I-weakness

2021-2110	This paper proposes a new method to combine non-autoregressive (NAT) and autoregressive (AT) NMT.	B-abstract
2021-2110	Compared with the original iterative refinement for non-autoregressive NMT, their method first generates a translation candidate using AT and then fill in the gap using NAT. <sep>	I-abstract
2021-2110	All of the reviewers think the idea is interesting and this research topic is not well-studied.	B-strength
2021-2110	However, the empirical part did not convince all the reviewers.	B-weakness
2021-2110	The revised version and response is good; however, it still does not solve some major concerns of reviewers. <sep>	B-rebuttal_process

2021-2113	The paper describes an RL technique to learn how to branch in discrete optimization.	B-abstract
2021-2113	This advances the state of the art in comparison to previous imitation learning techniques.	B-strength
2021-2113	However, the reviewers and a public reader raised concerns about the validity of the experiments due to several inconsistencies and differences with previous work that might suggest some cherry picking.	B-weakness
2021-2113	This is too bad since the reviewers really liked the work, but it is important to make sure that the experimental evaluation is done fairly.	I-weakness
2021-2113	I read the paper and I share the concerns regarding the experimental methodology.	I-weakness
2021-2113	Hence the experimental evaluation needs to be revised before publication. <sep>	B-decision

2021-2114	This paper presents a knowledge distillation method for face recognition, by inheriting the teacher's classifier as the student's classifier and optimizing the student model with advanced loss functions.	B-abstract
2021-2114	It received comments from three reviewers: 1 rated "Ok but not good enough - rejection", 1 rated "Marginally below" and 1 rated "Marginally above".	B-rating_summary
2021-2114	The reviewers appreciate the simple yet clear methodology illustration and the well written paper.	B-strength
2021-2114	However, a number of major concerns are raised by the reviewers, including limited novelty, lack of comparison with more advanced knowledge distillation methods and their special case in face recognition.	B-weakness
2021-2114	During the rebuttal, the authors made efforts to response to all reviewers' comments.	B-rebuttal_process
2021-2114	However, the rating were not changed.	I-rebuttal_process
2021-2114	The ACs concur these major concerns and more comprehensive comparisons with the state of the art KD methods are necessary to better illustrate the contribution of this work.	B-weakness
2021-2114	Therefore, this paper can not be accepted at its current state. <sep>	B-decision

2021-2119	The reviewers had some initial concerns about this submission.	B-rebuttal_process
2021-2119	While the authors' rebuttal does a good job to address these concerns, the reviewers still have some doubts about the contribution of this paper and potential impact.	I-rebuttal_process
2021-2119	In particular, it is not clear whether the performance improvements observed with the proposed algorithms is due to the ability to correct for noisy rewards or whether there are multiple other explanations for the improvement in performance.	I-rebuttal_process
2021-2119	This makes it hard to predict whether the proposed algorithms will actually be useful in settings where noisy rewards or demonstration data are present. <sep>	I-rebuttal_process

2021-2131	This paper received 4 reviews with mixed initial ratings: 7, 5, 4, 5.	B-rating_summary
2021-2131	The main concerns of R1, R2 and R4, who gave unfavorable scores, included: lack of methodological novelty (analysis-only paper), absence of experiments on real data (3 synthetic-only benchmarks), missing baselines and an overall inconclusive discussion.	B-weakness
2021-2131	At the same time R5 notes that the offered fair comparison between SOTA methods was indeed "much needed", and the paper can "serve an important role" in guiding future developments in the community.	B-strength
2021-2131	In response to that, the authors submitted a new revision and provided detailed answers to each of the reviews separately.	B-rebuttal_process
2021-2131	R1, R2 and R4 did not participate in the discussion, and R5 stayed with the positive rating. <sep>	B-rating_summary
2021-2131	AC agrees with R5 that the provided analysis is insightful, and the effort put into organizing the research community around a single set of benchmarks and metrics is indeed valuable.	B-strength
2021-2131	However, given a simplistic nature of the proposed datasets and lack of other methodological contributions, the submission is not meeting the acceptance bar for ICLR.	B-decision
2021-2131	After discussion with PCs, the final recommendation is to reject. <sep>	I-decision

2021-2143	The paper presents a personalized federated learning approach using a mixture of global and local models.	B-abstract
2021-2143	Four reviewers evaluated this paper; one of the reviewers is luke-warm (6) while the rest of the reviewers pretty negative to this work (3, 3, 3).	B-rating_summary
2021-2143	The reviewers pointed out many weaknesses, especially about novelty, motivation, contribution, presentation, etc.	B-weakness
2021-2143	Most importantly, although the idea of a "mixture of experts" makes sense, it is not clear what the real technical contribution of this paper is in terms of federated learning. <sep>	I-weakness
2021-2143	Considering all the comments by the reviewers, I believe that this paper is not ready yet for publication.	B-decision
2021-2143	The authors need to improve the novelty and technical soundness of the proposed direction to convince the readers including reviewers. <sep>	B-suggestion

2021-2161	The paper describes a framework for multi-agent reinforcement learning that uses Markov Random Fields.	B-abstract
2021-2161	Unfortunately, the paper is not clearly written and would benefit from significant revisions that improve its structure and make the model and approximations more explicit. <sep>	B-weakness
2021-2161	In particular, the paper says a graph says which agents i,j communicate.	I-weakness
2021-2161	This is typically called the "coordination graph" in this setting, see <sep>	I-weakness
2021-2161	"Collaborative Multiagent Reinforcement Learning by Payoff Propagation", Kok and Vlassis, 2006.	O
2021-2161	Note that within that paper they provide Q-function decomposition, which can only serve to approximate the optimal policy. <sep>	B-weakness
2021-2161	The authors of this submission claim that an MRF is sufficient for optimal policies.	I-weakness
2021-2161	I fail to see how this is true.	I-weakness
2021-2161	In particular, Proposition 1 has to be checked more carefully.	I-weakness
2021-2161	I tried to go through it, but it did not seem to make sense to me.	I-weakness
2021-2161	Why is there an exp() term in the definitoin of the optimal trajectory probability?	I-weakness
2021-2161	Why would minimising the KL divergence be enough to obtain an optimal policy?	I-weakness
2021-2161	Perhaps it gives an optimal policy within the class of MRF policies, but that's not the same thing as the globally optimal policy. <sep>	I-weakness
2021-2161	Overall, I find the lack of clarity and in depth discussion of early related work disturbing, particularly with respect to the theoretical claims in the paper. <sep>	I-weakness

2021-2167	This paper received mixed reviews, 3 positives (7, 6, 6) and 2 negatives (4, 4).	B-rating_summary
2021-2167	Due to the divergence of the reviews, I carefully read the paper and made my best efforts to understand the paper and the review comments.	O
2021-2167	This paper proposes to learn a quantization network using a small calibration set given a network trained with the full precision.	B-abstract
2021-2167	The combination of AdaQuant, integer programming, and batch-norm tuning makes sense although they do not have substantial novelty.	B-strength
2021-2167	The three components are reasonably tightly-coupled and comprise a complete algorithm.	I-strength
2021-2167	However, the sequential-AdaQuant distracts the main claim of this work significantly.	B-weakness
2021-2167	This is probably added during the review process but looks ad-hoc to me.	I-weakness
2021-2167	Sequential AdaQuant seems to be effective to improve accuracy, but cannot be applied before the bit allocation was set, which makes it require integer programming no more.	I-weakness
2021-2167	Because of this issue, the overall presentation becomes confusing and the argument sometimes sounds unfair (please refer to the last posting by R5. ). <sep>	I-weakness
2021-2167	In addition, the presentation of this paper could be improved, especially for the details of the integer programming formulation.	I-weakness
2021-2167	It is not clear how to define some variables mathematically.	I-weakness
2021-2167	The discussion about the size of the calibration set together with the overfitting issue is lacking, and rigorous discussion and analysis would make the paper much stronger.	I-weakness
2021-2167	The reviewers are not convinced of the novelty of this paper, and they rather believe that this is an engineering-oriented work.	I-weakness
2021-2167	Considering this fact,  the evaluation of this paper is not very comprehensive.	I-weakness
2021-2167	The ablation study with respect to the size of the calibration set should be conducted more intensively.	I-weakness
2021-2167	The experiment fails to show the benefit of mixed precision quantization effectively and it is limited to presenting the compression ratio in Figure 3.	I-weakness
2021-2167	The authors used a small calibration set taken from the training dataset, which looks weird because they claim that the post-training quantization requires only a small "unlabeled" calibration set at the beginning of the abstract; it is more desirable to use arbitrary examples in the same domain. <sep>	I-weakness
2021-2167	Despite the interesting aspects, I believe that this paper needs a focus and substantial improvement for publication, and, consequently, recommend rejection. <sep>	B-decision

2021-2186	The paper considers exploiting low-rank structure in Q-function and the Hamiltonian Monte-Carlo (HMC) to approximate the expectation in Q-learning to reduce the stochastic approxiamtion error, and thus, achieves "efficient RL".	B-abstract
2021-2186	The authors tested the algorithm empirically within some simple environments. <sep>	I-abstract
2021-2186	As reviewers (R1, R3, R4) mentioned, the major bottleneck of this algorithm is the assumption that the dynamics is known up to a constant, which is extremly strong, and thus, limits the application of the algorithm.	B-weakness
2021-2186	I suggest the authors to consider the common RL setting, without any knowledge about the transition models, and make fair empirical comparison with baselines in the same setting. <sep>	B-suggestion

2021-2188	The authors address the problem of self-supervised monocular depth estimation via training with only monocular videos.	B-abstract
2021-2188	They propose to use additional information extracted from semantic segmentation at training time to (i) provide additional "semantic context" supervision and (ii) to improve depth estimation at discontinuities through an edge guided point sampling based approach.	I-abstract
2021-2188	Results are presented on the KITTI and Cityscapes datasets. <sep>	I-abstract
2021-2188	One of the main concerns is related to the utility of the semantic supervision given the relative cost required to obtain semantic training data in the first place.	B-weakness
2021-2188	The authors state that "the pixel-wise local depth information can not be well represented by current depth network".	B-abstract
2021-2188	However, Guizilini 2020a can generate detailed depth edges and they do NOT require any semantic information during training.	B-weakness
2021-2188	The authors also state that "the required labeled semantic dataset only accounts for a very tiny proportion, which indicates a relatively lower cost."	B-abstract
2021-2188	This is a bit misleading.	B-weakness
2021-2188	The proposed method uses per pixel semantic ground truth from three datasets (Mapillary Vistas, Cityscapes, and KITTI).	B-abstract
2021-2188	It takes a lot of effort to provide this ground truth compared to self-supervised methods which do not require any ground truth depth.	B-weakness
2021-2188	It is encouraging that dataset specific semantic finetuning does not seem to have a large impact (Table 3), but this still requires access to a large enough initial semantic training set.	I-weakness
2021-2188	Finally, the quantitative results are not much better than methods that don't require any semantics eg Guizilini 2020a, Johnston and Carneiro.	I-weakness
2021-2188	Clearly, methods that do not require semantics are much more scalable, especially when adapting to new types of scenes. <sep>	I-weakness
2021-2188	Regarding the specific contributions of the paper, the SEEM module is the most novel component of the model.	B-strength
2021-2188	However, the addition of the SEEM module does not improve quantitative performance by much (see Table 2).	B-weakness
2021-2188	In addition, the qualitative improvement it provides is also very subtle.	I-weakness
2021-2188	This can be seen by comparing the last two rows of Fig 7 ie without and with.	I-weakness
2021-2188	The authors need to make a stronger case, either quantitatively or qualitatively, as to why this is valuable. <sep>	B-abstract
2021-2188	Finally, but only a minor concern, the following relevant reference is missing: Jiao et al Look Deeper into Depth: Monocular Depth Estimation with Semantic Booster and Attention-Driven Loss, ECCV 2018 <sep>	B-weakness
2021-2188	In conclusion, there were mixed views from the reviewers - with some supportive of the paper (R2&3) others not as enthusiastic (R1&4).	B-rating_summary
2021-2188	The authors should be commended for their detailed responses and changes already made based on reviewer comments and suggestions.	B-rebuttal_process
2021-2188	Unfortunately, this did not change the mind of the reviewers.	I-rebuttal_process
2021-2188	It is the opinion of this AC that there is still more work required to fully show the utility of the proposed approach, especially considering the non trivial effort that is required to obtain semantic supervision in novel domains. <sep>	B-weakness

2021-2213	I think we did learn something new from this paper, and I think the reviewers all seem to agree with this. <sep>	B-strength
2021-2213	The observation you make about the objective seems correct and interesting (though reviewers and ACs do sometime miss errors), but I have the following complaints that keep me from recommending acceptance: <sep> 	B-decision
2021-2213	The theory seems right, but in practice, all sorts of GANs with all sorts of objective functions experience "mode collapse", <sep>	B-weakness
2021-2213	so it doesn't seem like the issue you point out w/ the NS-GAN objective can be the whole story. <sep>	I-weakness
2021-2213	However, we generally don't ask of a paper that it tells the whole story all in one go...<sep>	I-weakness
2021-2213	I do think the experiments are somewhat poorly done (compared to those for say the median paper about GANs that gets accepted to one of these conferences).	I-weakness
2021-2213	Moreover, many people have made similar experimental claims to the ones that are in this paper that haven't held up on more complicated data sets, so I tend to apply more scrutiny to such claims when they're only evaluated on smaller tasks.<sep>	I-weakness
2021-2213	There have, as R3 points out, been a huge number of papers proposing tricks for training GANs, and some of them work really well. <sep>	I-weakness
2021-2213	What I'm missing from this paper is an exploration of the relationship between your observation and those (mostly ad-hoc) tricks. <sep>	I-weakness
2021-2213	Does your observation explain why those tricks are necessary? <sep>	I-weakness
2021-2213	Does it explain why some existing trick works as well as it does? <sep>	I-weakness
2021-2213	If your observation is totally orthogonal to existing tricks, can you get much better performance on a challenging data set by using it? <sep>	I-weakness
2021-2213	I don't feel like I got satisfactory answers to those questions.<sep>	I-weakness
2021-2213	All this being said, the paper was borderline, and I think if you dealt with some of the complaints above you would have a pretty good shot of getting a revised version accepted at another major machine learning conference. <sep>	B-decision

2021-2225	I thank the authors for their submission and very active participation in the author response period.	O
2021-2225	I want to start by stating that I rank the paper higher as is currently reflected in the average score of the reviewers.	O
2021-2225	The reasons for this are that a) R2 and R3, while responding to the author's rebuttal, do not seem to have updated their score or indicated that they want to keep their initial assessment of the paper -- in particular, R2 has acknowledged that additional experiments by the authors were useful and results on KeyCorridorS4/S5R3 are nice, and b) I disagree with R2's sentiment that MiniGrid is not a suitable testbed -- it is by now an established benchmark for evaluating RL exploration and representation learning methods (see list of publications on https://github.com/maximecb/gym-minigrid).	B-ac_disagreement
2021-2225	However, despite my more positive stance on the paper, I fully agree with R1 and R2 that a comparison to EC is needed in order to shed light into which factors of EC-SimCLR actually led to improvements in comparison to RIDE.	B-weakness
2021-2225	I therefore recommend rejection, but I strongly encourage the authors to take the feedback from the reviewers and work on a revised submission to the next venue. <sep>	B-decision

2021-2228	The paper analyzes connections between algorithmic fairness and domain generalization literatures.	B-abstract
2021-2228	The reviewers found the paper interesting but they also raised some important concerns about it. <sep>	O
2021-2228	The applicability of the method presented in the paper is not clear nor well-discussed in the paper. <sep>	B-weakness
2021-2228	The papers and the revised version do not not cite important related work. <sep>	B-rebuttal_process
2021-2228	The mathematical exposition in the paper is a bit hard to read.	I-rebuttal_process
2021-2228	Even after revision, the reviewers find part of the paper(Appendix F) very hard to read. <sep>	I-rebuttal_process
2021-2228	Overall, the paper in the current version is below the high acceptance bar of ICLR. <sep>	B-decision

2021-2251	This paper studies how to efficiently expose failures of "top-performing" segmentation models in the real world and how to leverage such counterexamples to rectify the models.	B-abstract
2021-2251	The key idea is to discover most "controversial" samples from massive online unlabeled images.	I-abstract
2021-2251	The approach is sound, well grounded, and quite logical.	B-strength
2021-2251	Results demonstrate the effectiveness. <sep>	I-strength
2021-2251	However, there exists some limitations coming from R2 and R3, for example, 1) Segmentation benchmarks may not require pixel-level dense annotation.	B-weakness
2021-2251	There are also examples of benchmarks where the groundtruth consists of computer segmentations corrected by humans.	I-weakness
2021-2251	2) It is much harder for segmentation data to be class-balanced in the pixel level, making highly skewed class distributions common for this particular task.	I-weakness
2021-2251	3) Citing the field of computer-assisted annotation as relevant work. <sep>	I-weakness
2021-2251	In the end, I think that this paper may not be ready for publication at ICLR, but the next version must be a strong paper if above limitations can be well addressed. <sep>	B-decision

2021-2264	The reviewers are in consensus that the manuscript is not ready for publication in its current form: more comprehensive evaluation, and careful analysis (either theoretical or empirical) of the simple-but-effective methodology would improve the quality further.	B-rating_summary
2021-2264	The discussion was constructive and helped the authors to reason about their work better. <sep>	O
2021-2264	The AC recommends Reject and encourages the authors to take the constructive feedback into consideration . <sep>	B-decision

2021-2276	The authors propose a particle-based entropy estimate for intrinsic motivation for pre-training an RL agent to then perform in an environment with rewards.	B-abstract
2021-2276	As the reviewers discussed, and also mentioned in their reviews, this paper bears stark similarity to work of 5 months ago, presented at the ICML 2020 Lifelong ML workshop, namely, "A Policy Gradient Method for Task-Agnostic Exploration", Mutti et al, 2020--MEPOL.	B-weakness
2021-2276	What is novel here is the adaptation of this entropy estimate to form an intrinsic reward via a contrastive representation and the subsequent demonstration on standardized RL environments.	B-strength
2021-2276	The authors have added a comparison to MEPOL, and in these experiments, APT outperforms this method, sometimes by some margin.	B-rebuttal_process
2021-2276	Unfortunately this work does not meet the bar for acceptance relative to other submissions. <sep>	B-decision

2021-2329	This paper presents a novel approach to grammar induction.	B-abstract
2021-2329	Like older work by Klein and Manning, the paper finds benefit in jointly inducing both constituency and dependency structure.	I-abstract
2021-2329	However, unlike most approaches to grammar induction, the model is not generative -- rather, it is a transformer-based architecture that is trained to optimize a masked language modeling objective.	I-abstract
2021-2329	The resulting parses appear to beat non-trivial baselines, but direct comparisons with several relevant state-of-the-art systems are not drawn.	B-weakness
2021-2329	Reviewers overall found the approach interesting and novel.	B-strength
2021-2329	However, nearly all reviewers raised serious concerns about experimental comparisons with related work and brought up several missing state-of-the-art baselines that, like the proposed system, do not require gold POS.	B-weakness
2021-2329	Reviewers also pointed out issues with clarity in several sections.	I-weakness
2021-2329	In rebuttal, authors provide a substantial update to the original draft.	B-rebuttal_process
2021-2329	So substantial that all reviewers mentioned in discussion that the new draft would effectively require an entirely new review.	I-rebuttal_process
2021-2329	While I applaud authors for the substantial revisions, and while ICLR guidelines do not explicitly limit the amount change to a draft allowed in rebuttal, in this case the revisions are sufficiently drastic that I agree with reviewers that a new review process is required.	I-rebuttal_process
2021-2329	Thus, I recommend rejection but strongly encourage authors to resubmit. <sep>	B-decision

2021-2330	The work focuses on a new method for sampling hyper-parameter based on an "Population-Based Training" schedule that tend to sample more often configurations that gave good performances in the past.	B-abstract
2021-2330	The authors have conducted experiments to verify the superior of their method, especially for the effectiveness and generalisability. <sep>	I-abstract
2021-2330	Pros: <sep> simple method that can be implemented without much effort, <sep> 	B-strength
2021-2330	good empirical performances on Imagenet, <sep>	I-strength
2021-2330	paper well organised and written.<sep>	I-strength
2021-2330	Cons: <sep> lack of explanation about the DensNet121 performance degradation [partially addressed in the rebuttal], <sep> 	B-weakness
2021-2330	additional simple experiments in Section 4.4 were recommended to evaluate the generality of the method [addressed in Table 5], <sep>	I-weakness
2021-2330	empirical validation seems not sufficient [partially addressed in the rebuttal], <sep>	I-weakness
2021-2330	similarity with respect to prior art, such as the focal loss [partially addressed in the rebuttal], <sep>	I-weakness
2021-2330	clarification of the randomisation strategy in experiments [addressed in the rebuttal].<sep>	I-weakness
2021-2330	Despite most of the issues being addressed, the reviewers decided that this paper would benefit more work to be accepted for the conference this year. <sep>	B-rating_summary

2021-2342	This paper proposes two mechanisms, SelfNorm (used during training and inference) leveraging an attention-based recalibration of mean and standard deviation for instance normalization, and CrossNorm which performs cross-channel swapping of mean/stdev.	B-abstract
2021-2342	Is is shown that the combination (often combined with AugMix) performs well across several datasets in terms of model robustness.	I-abstract
2021-2342	Overall the paper has strength in the fact that the method is interesting, simple to implement, and modular.	B-strength
2021-2342	However, reviewers brought up a number of issues including the overstated motivation/writing, lack of clarity, and most importantly need for clear experimental results (comparing to uniform/standard baselines) and identification of the separate mechanisms.	B-weakness
2021-2342	It is especially uncertain why it is necessary that they are used together (often with AuxMix as well) to obtain the strong performance.	I-weakness
2021-2342	As a result, the score for this paper is borderline, tending towards a weak acceptance. <sep>	B-rating_summary
2021-2342	It is appreciated that the authors provided a lengthy rebuttal, including new results in a different domain (NLP); however, the reviewers agreed that not all of the concerns were addressed.	B-rebuttal_process
2021-2342	After a lengthy discussion, all of the reviewers agree that while the method is simple, modular, and effective when combined (hence the positive scores from some reviewers), the authors fail to describe the underlying reason for the method's gains, especially with respect to the individual parts (SelfNorm vs. CrossNorm) and why the results only come when these rather independently derived modules are used together.	I-rebuttal_process
2021-2342	The exposition of the experimental results, with differing baselines/conditions that make it very hard to understand where the effect is coming from, exacerbates this issue. <sep>	I-rebuttal_process
2021-2342	As a result of these concerns, I recommend rejection of this paper.	B-decision
2021-2342	However, the method is interesting and results promising, so I hope that the authors can clarify the writing and improve the presentation of the results (specifically separating out the effects of SelfNorm and CrossNorm, as well as analyzing how they interact together to improve results) and submit to a future venue. <sep>	B-suggestion

2021-2347	The paper proposes a deep learning approach to blind image denoising based on deep unrolling.	B-abstract
2021-2347	In particular, the proposed network is derived from convolutional sparse coding algorithms, which are unrolled, untied across layers and learned from data.	I-abstract
2021-2347	The paper proposes a frequency domain regularization scheme in which the filters consist of a single analytically defined low-pass filter and a large collection of filters which are constrained to reside in the mid-to-high frequency ranges.	I-abstract
2021-2347	It also proposes to tie the thresholds in the soft-thresholding stages of the learned network to estimates of the noise variance, making the proposed scheme more robust to variations in the noise level. <sep>	I-abstract
2021-2347	Pros and Cons: <sep> [+] Having a single low-pass dictionary atom reduces redundancy (and potentially coherence) in the learned dictionary.	B-strength
2021-2347	This type of regularization may also reduce the time/data required to learn. <sep>	I-strength
2021-2347	[+/-] Using noise estimators and a noise adaptive threshold renders the model more robust to variations in the noise level.	I-strength
2021-2347	This is important, since in most denoising applications the noise level is not known a-priori.	I-strength
2021-2347	As the reviewers note, the idea of tuning thresholds in an unrolled sparse coding method based on the noise level is not a novelty of the paper; the novelty here is coupling this with a wavelet-based estimate of the noise level. <sep>	I-strength
2021-2347	[-] All three reviewers raise concerns regarding the novelty of the work compared to existing convolutional sparse coding-based neural networks.	B-weakness
2021-2347	The structure of the network is similar; the main difference is the frequency restriction for learned atoms, which is enforced by prefiltering the learned atoms with a high-pass filter. <sep>	I-weakness
2021-2347	[-] The paper is not entirely clear in its motivation and argumentation.	I-weakness
2021-2347	Reducing the coherence of the learned dictionary makes sense from the perspective of certain worst case results from sparse approximation.	I-weakness
2021-2347	However, the coherence is a worst case quantity; moreover, certain approaches to coherence control (eg, using large stride) control coherence at the expense of the expressiveness of the dictionary, and hence may not actually improve its ability to provide sparse reconstructions of natural signals.	I-weakness
2021-2347	The proposed frequency domain regularization is a sensible approach to controlling coherence, since low-frequency atoms will tend to be highly coherent, but would benefit from a crisper analytical motivation. <sep>	I-weakness
2021-2347	[-] Reviewers found the experiments lacking in some regards.	I-weakness
2021-2347	In particular, the paper only evaluates its proposals on synthetic experiments with Gaussian noise.	I-weakness
2021-2347	While this is in line with some previous work on deep learning based denoising, more extensive and realistic experiments would have bolstered the paper's argument. <sep>	I-weakness
2021-2347	Overall, the paper makes a sensible proposal regarding the adaptivity to unknown noise levels, and introduces a potentially useful frequency-domain restriction on the learned filters in a CSC network.	B-strength
2021-2347	However, the reviewers did not find that the paper made a clear argument for the significance of these proposals, and raised other concerns regarding the clarity and experiments.	B-weakness
2021-2347	The consensus of the reviewers is to recommend rejection. <sep>	B-rating_summary

2021-2353	For many problems such as ligand-protein binding, quantitative structure activity prediction (QSAR), predicting protein function from structure, etc., the 3D geometry of the molecules is of great importance.	B-abstract
2021-2353	One way to represent this is simply to assign locations to all atoms in 3-dimensional space.	I-abstract
2021-2353	If using graph convolutional kernels or other relational representations such that aligning molecules is not necessary, these approaches with 3D geometry can be efficient and far more effective than 1D or 2D representations.	I-abstract
2021-2353	The contribution of the paper is to make this point and to produce a resource with this kind of 3D data.	B-strength
2021-2353	Such a resource would be of high value.	I-strength
2021-2353	Nevertheless, reviewers feel provision of such a resource is perhaps not a major contribution to the ICLR and ML communities.	B-weakness
2021-2353	There is a sense that more innovative and substantial contribution would come from addressing also the challenge that 3D geometry can changes and that there may be multiple low-energy conformations of biomolecules that should be considered.	B-suggestion
2021-2353	The authors contend that unlike ligands which are small and may have many low-energy conformations, large biomolecules have a much more constrained conformational space. <sep>	B-rebuttal_process
2021-2353	This meta-reviewer is sympathetic to the authors' point and appreciates the importance of the resource.	I-rebuttal_process
2021-2353	Nevertheless, even large biomolecules often have some portions of flexible conformation and high 3D structure variation that should be considered.	I-rebuttal_process
2021-2353	And indeed addressing the kind of multiple instance problem that arises by considering multiple conformations of large molecules or of ligands binding to large molecules would certainly require and likely yield bigger ICLR/ML innovations.	I-rebuttal_process
2021-2353	In the end the paper contributes a useful resource but does not excite the reviewers substantially enough, without those extensions or others, for a recommendation of acceptance at this time. <sep>	B-decision

2021-2377	This paper proposed a new measure of effective gradient flow (EGF), and also compared sparse vs. dense networks on CIFAR-10 and CIFAR-100.	B-abstract
2021-2377	The notion of EGF would be interesting, but the paper did not present enough evidence to support this notion. <sep>	B-weakness

2021-2397	The main idea of the paper is to  use image data to guide radar data acquisition by focusing on the blocks where the object has appeared.	B-abstract
2021-2397	Four reviewers have relatively consistent rating: 3 of them rated "Ok but not good enough - rejection", while 1 rated "clear rejection".	B-rating_summary
2021-2397	The main concerns include ad-hoc choices of algorithm design, lack of algorithm novelty, not adequate experiments in illustrating the performance, etc.	B-weakness
2021-2397	During the rebuttal, the authors made efforts to response to all reviewers' comments.	B-rebuttal_process
2021-2397	However, the major concerns remain, and the rating were not changed.	I-rebuttal_process
2021-2397	While the motivation is clear and the work has merits, the ACs agree with the reviewers' concerns and this paper can not be accepted at its current state. <sep>	B-decision

2021-2399	Although the paper studies a relevant and important topic, which is about learning of hierarchy of concepts in an unsupervised manner, the reviewers raised several critical concerns.	B-strength
2021-2399	In particular, although the hierarchical structure of concepts is the key idea in this paper, the concept of hierarchy itself is not well explained.	B-weakness
2021-2399	How to define the hierarchical level of concepts should be carefully and mathematically discussed.	I-weakness
2021-2399	In addition, empirical evaluation is not thorough as reviewers pointed out.	I-weakness
2021-2399	Although we acknowledge that the authors addressed concerns by the author response, newly added results are still confusing and more careful treatment is needed before publication.	B-rebuttal_process
2021-2399	I will therefore reject the paper. <sep>	B-decision
2021-2399	This work reminds me the the topic called "formal concept analysis" (eg see [1]), which mathematically defines concepts as closed sets and constructs a hierarchy of concepts in an unsupervised manner.	O
2021-2399	This method can be viewed as co-clustering and also has a close relationship to closed itemset mining.	O
2021-2399	This approach is used in machine learning (eg [2]).	O
2021-2399	I think it is beneficial for the authors to refer such existing and well-established approaches to elaborate this work further. <sep>	B-suggestion
2021-2399	[1] Davey, B.A., Priestley, H.A.: Introduction to Lattices and Order, Cambridge Univ. Press (2002)	O
2021-2399	[2] Yoneda, et al, Learning Graph Representation via Formal Concept Analysis, arXiv:1812.03395 <sep>	O

2021-2409	The reviews were a bit mixed: on one hand, by combining and adapting existing techniques the authors obtained some interesting new results that seem to complement existing ones; on the other hand, there is some concern on the novelty and on the interpretation of the obtained results.	B-weakness
2021-2409	Upon independent reading, the AC agrees with the reviewers that this paper's presentation can use some polishing.	I-weakness
2021-2409	(The revision that the authors prepared has addressed some concerns and improved a lot compared to the original submission.)	B-rebuttal_process
2021-2409	Overall, the analysis is interesting but the significance and novelty of this work require further elaboration.	B-weakness
2021-2409	In the end, the PCs and AC agreed that this work is not ready for publication at ICLR yet.	B-decision
2021-2409	Please do not take this decision as an under-appreciation of your work.	O
2021-2409	Rather, please use this opportunity to consider further polishing your draft according to the reviews.	B-suggestion
2021-2409	It is our belief that with proper revision this work can certainly be a useful addition to the field. <sep>	I-suggestion
2021-2409	Some of the critical reviews are recalled below to assist the authors' revision: <sep>	I-suggestion
2021-2409	(a) The result in Theorem 4.1 needs to be contrasted with a single machine setting: do we improve the convergence rate in terms of T here?	B-weakness
2021-2409	do we improve the constants in terms of L and M here?	B-suggestion
2021-2409	What is the advantage one can read off from Theorem 4.1, compared to a single machine implementation?	I-suggestion
2021-2409	How should we interpret the dependence of (optimal) H on r and lambda_2? <sep>	I-suggestion
2021-2409	(b) The justification for Tn4 is a bit  weak and requires more thoughts: one applies distributed SGD because n is large.	B-weakness
2021-2409	What happens if T does not satisfy this condition in practice, as in the experiments? <sep>	B-suggestion
2021-2409	(c) Extension 1 perhaps should be more detailed as its setting is much more realistic than Theorem 1.	B-weakness
2021-2409	One could use Theorem 1 to motivate and explain some high level ideas but the focus should be on Extension 1-3.	B-suggestion
2021-2409	In extension 2, the final bound seems to be exactly the same as in Theorem 1, except a new condition on T. Any explanations?	I-suggestion
2021-2409	Why asynchronous updates only require a larger number of interactions but retain the same bound?	I-suggestion
2021-2409	These explanations would make the obtained theoretical results more accessible and easier to interpret. <sep>	I-suggestion

2021-2412	All reviewers agree that this paper is interesting, but needs improvement in order to be suitable for a highly competitive venue such as ICLR.	B-rating_summary
2021-2412	Reviewer 3 is especially incisive and detailed, but other reviewers make similar points. <sep>	O

2021-2430	Well, this paper has achieved something remarkable in this review process:  The initial scores came in at fairly low scores (4, 5, 3, 6).	B-rebuttal_process
2021-2430	However, as the discussions / rebuttals went back and forth, the reviewers were able understand and see the merits of the proposed methodology.	I-rebuttal_process
2021-2430	Namely, the setting of L2E (Learning to Exploit), which makes use of a novel method called Opponent Strategy Generation, to quickly generate very different types of opponents to play against.	I-rebuttal_process
2021-2430	One more pertinent component is the use of MMD (maximum mean discrepancy regularization) which can remove the necessity of dealing with task distributions, and does a better job in creating diverse opponents. <sep>	I-rebuttal_process
2021-2430	Having understood the technical approach, three of the reviewers decided to substantially increase their scores.	I-rebuttal_process
2021-2430	R4 increased 4->6, R5 increased 5->6, R3 increased 3->4, while R2 held steady with a score of 6.	I-rebuttal_process
2021-2430	It was also good to see empirical favorable results compared to other baseline methods: L2E had the best return against unclear opponents, such as Rocks opponent and Nash opponent. <sep>	I-rebuttal_process
2021-2430	Without any reviewer arguing strongly for acceptance, the program committee decided that the paper in its current form does not quite meet the bar, and also that it would benefit from another revision. <sep>	B-decision

2021-2443	The paper proposes a new meta-learning algorithm which promises greater robustness to adversarial examples.	B-abstract
2021-2443	I will be brief, as the fault with the paper is quite clear: the experimental results are not sufficient.	B-weakness
2021-2443	The attack used (FGSM) is particularly dated and weak, and the comparison to existing defences is insufficient.	I-weakness
2021-2443	Additionally, prior work (Adversarially Robust Few-Shot Learning: A Meta-Learning Approach) obtains better results, and is not compared against.	I-weakness
2021-2443	The reviewers provided further criticism regarding the motivation for and explanation of the method, but the empirical aspects of the paper are where it primarily falls short of the publishable standard for ICLR. <sep>	I-weakness
2021-2443	I recommend rejection, and invite the authors to consider demonstrating robustness to a wider range of attacks (including non-gradient based), and a more thorough comparison to defence methods, before resubmitting to another conference. <sep>	B-decision

2021-2444	The paper proposes a method for offline meta-RL, where we meta-train on pre-collected offline data for several RL tasks and adapt to a new task with a small amount of data.	B-abstract
2021-2444	The paper assumes that there is no interaction with the environment either during meta-train or meta-test.	I-abstract
2021-2444	In this setting, motivated by the ide of leveraging offline experience from multiple tasks to enable fast adaptation to new tasks, the paper introduces MACAW, which combines the consistent MAML and the popular offline AWR, improving upon them by adding capacity through parameterization and adding an extra objective in the policy update.	I-abstract
2021-2444	As a result, the MACAW proposed for the offline meta-RL has the desirable property of being consistent, ie, converging to a good policy if enough time and data for the meta-test task are given, regardless of meta-training. <sep>	I-abstract
2021-2444	Pros: <sep> Most of the experiments are well executed, using good baselines.	B-strength
2021-2444	Extensive ablations on the various modifications to MAML+AWR confirmed the utility of the approach for the fully offline meta-RL problem. <sep>	I-strength
2021-2444	MACAW is a simple algorithm with theoretical guarantees; the modifications to the policy functions are backed by theory.<sep>	I-strength
2021-2444	Cons: <sep> The reviewers have concerns on the formulation of offline meta-RL.	B-weakness
2021-2444	One major contribution of the paper is to introduce offline meta-RL.	B-abstract
2021-2444	However the paper largely borrows the meta-RL formulation from the online setting where task=MDP.	B-weakness
2021-2444	The reviewers think that directly borrowing from regular meta-RL as the formulation of offline meta-RL might be misleading.	I-weakness
2021-2444	The reviewers suggest including behavior policy as part of the task definition for offline meta-RL formulation.<sep>	B-suggestion
2021-2444	Several reviewers raised concerns that the fully offline setting might be unrealistic.	B-weakness
2021-2444	Although the author did add a motivation, the reviewers would be interested in seeing MACAW being adapted online at test time on in-distribution tasks.<sep>	B-rebuttal_process
2021-2444	Unfortunately, the authors accidentally revealed their names in one of the modified versions.<sep>	I-rebuttal_process

2021-2448	This paper investigates the topic of nondeterminism and instability in neural network optimization.	B-abstract
2021-2448	The reviewers found the results on different sources of nondeterminism particularly interesting and relevant.	B-strength
2021-2448	The experiments are carried on both language and also vision, which strengthens the findings.	I-strength
2021-2448	Concerns were raised about the use of smaller non-standard models, which were somewhat mitigated by the addition of Resnet-18 experiments on CIFAR.	B-weakness
2021-2448	The reviewers also noted that the measures used in the experimental protocol were already present in the literature, and that the proposed mitigation strategy is from another work.	I-weakness
2021-2448	Furthermore, R2 also found that the optimization instability section should be more developed.	I-weakness
2021-2448	The paper should be resubmitted with an improved discussion of related works and more developed section on instability as suggested by the reviewers. <sep>	B-decision

2021-2464	The new non linearity proposed in this paper present interesting observations and improvements on image and text datasets. <sep>	B-strength
2021-2464	However, reviewers point out that there should've been more comparisons to other efficient transformers and on more datasets. <sep>	B-weakness
2021-2464	The speed improvements are also not clear. <sep>	I-weakness
2021-2464	I'd encourage the authors to revise and submit in the future. <sep>	B-decision

2021-2470	This paper proposes an interesting collaborative multi-head attention (MHA) method to enable heads to share projections, which can reduce parameters and FLOPs of transformer-based models without hurting performance on En-De translation tasks.	B-abstract
2021-2470	For pre-trained language models, a tensor decomposition method is used to easily covert the original MHA to its collaborative version without retraining. <sep>	I-abstract
2021-2470	This paper receives 3 weak reject and 1 weak accept recommendations.	B-rating_summary
2021-2470	On one hand, all the reviewers agree that the paper is well motivated and the proposed idea is interesting.	B-strength
2021-2470	On the other hand, all the reviewers also commented that the current empirical results and comparisons are weak, which are not enough to support the paper's main claim.	B-weakness
2021-2470	From the current results, it is difficult to draw a conclusion that collaborative MHA is better. <sep>	I-weakness
2021-2470	Specifically, (i) From Table 2, it can be seen that the proposed method is not effective for pre-trained models, ie, even if the model size is not reduced much, the performance can be dropped significantly.	I-weakness
2021-2470	(ii) More experiments, such as QA, more translation/generated tasks will make this paper more convincing.	I-weakness
2021-2470	(iii) More rigorous experiments are needed to justify the practical value of the proposed method.	I-weakness
2021-2470	If the authors try to emphasize that they go beyond practical realm, then probably a careful re-positioning of the paper is needed, which may not be a trivial task. <sep>	B-suggestion
2021-2470	The rebuttal unfortunately did not fully address the reviewers' main concerns.	B-rebuttal_process
2021-2470	Therefore, the AC regrets that the paper cannot be recommended for acceptance at this time.	B-decision
2021-2470	The authors are encouraged to consider the reviewers' comments when revising the paper for submission elsewhere. <sep>	I-decision

2021-2503	Unfortunately some of the reviewers' reactions to the author feedback won't be visible to the authors. <sep>	B-rebuttal_process
2021-2503	The reviewers highly appreciated the replies and revision of the paper <sep>	I-rebuttal_process
2021-2503	Pros: <sep> The paper renders Generalized Exploration tractable for deep RL. <sep> 	B-strength
2021-2503	The idea is applicable to many DRL methods and is potentially very valuable to deal with the headaches associated to DRL.<sep>	I-strength
2021-2503	Cons: <sep> R2 and R4 are still concerned about whether 'smart' exploration will always be advantageous, and whether the added complexity is a good trade-off for the (potentially) better performance.	B-weakness
2021-2503	A comparison to 'pure' exploration would still be insightful. <sep>	I-weakness
2021-2503	the new 'SAC with Deep Coherent Exploration' only partially addresses the concerns of R2 and R4, especially in terms of performance<sep>	I-weakness
2021-2503	While the paper has improved drastically during the reviewing process, there are still a few too many doubts. <sep>	B-rebuttal_process

2021-2512	The paper provides an interesting set of theoretical ideas to improve the estimation of normalizing flows on datasets that fail to be fully dimensional.	B-abstract
2021-2512	Although the method is appealing, I believe the paper falls a bit short of acceptance at the conference.	B-decision
2021-2512	Too many practical issues are left out, as discussed by reviewers, and the method seems promising but not fully connected to the rest of the literature on estimating low-dimensional distributions living in high dimensional spaces.	B-weakness
2021-2512	We encourage the authors to use the feedback contained in this round of reviews to improve their work. <sep>	B-suggestion

2021-2515	Three reviewers have reviewed this manuscript, and they had severe reservations regarding the presentation quality and the lack of sufficient theoretical support behind empirical observations.	B-weakness
2021-2515	Even after rebuttal, the reviewers maintained that the above issues are not fully resolved.	B-rebuttal_process
2021-2515	Unfortunately, this paper cannot be accepted in its current form. <sep>	B-decision

2021-2533	This paper is certainly on the way to be a solid contribution: it's an interesting research question, and we need more understanding papers (rather than yet another algorithmic trick paper). <sep>	B-weakness
2021-2533	The reviewers thought the paper was not yet ready.	B-rating_summary
2021-2533	The reviewers suggested: (1) more motivation of why the proposed metrics were of interest, (2) clearer discussion and evidence of how the analysis better articulates the performance of PER, (3) missing empirical details like methodology for setting hyper-parameters, why these 9 Atari games, undefined errorbars, unspecified number of runs, and (4) conclusions not supported by evidence in Atari: with missing experiment details, likely too few runs, and overlapping errorbars in most games few scientific conclusions can be drawn. <sep>	B-suggestion
2021-2533	The work might be strengthen by developing the first part of the paper (and focussing on the reviewer's suggestions) and deemphasizing the novel algorithmic contribution part. <sep>	I-suggestion

2021-2545	The authors propose two linguistic verifiers for improving extractive question answering when the question is answerable.	B-abstract
2021-2545	The first replaces the interrogative in the question with candidate answers and evaluates the result both in isolation and in combination with the answer-containing sentence to do answer verification.	I-abstract
2021-2545	The second jointly encodes individual sentences and spans with questions in a hierarchical manner to improve use of context in answer prediction performance. <sep>	I-abstract
2021-2545	The reviews for this paper are roughly on the cusp: 2 reviewers rate the paper a bit below the acceptance threshold, 1 a bit above, and then 1 now rates the paper as a solid Accept. <sep>	B-rating_summary
2021-2545	Pros<sep> The main strength of the paper, certainly as emphasized by the most positive reviewer is the strong empirical results.	B-strength
2021-2545	Especially on SQuAD v2, the method here seems to roughly equal the current leading system on the leaderboard. <sep>	I-strength
2021-2545	The paper also proposes two methods for improving question answering that make sense, are relatively simple, and work<sep>	I-strength
2021-2545	Cons<sep> The writing and presentation of the paper is not that great.	B-weakness
2021-2545	Even at the level of the introduction, the writing just is not very focused: The first page has a lot of background and tutorial information on MRC that just doesn't get to the point of where this paper is situated and what it contributes. <sep>	I-weakness
2021-2545	Neither of the proposed systems are that novel (though it is interesting to see that they still have value even in the age of large contextual language models) <sep>	I-weakness
2021-2545	The paper lacks ML novelty <sep>	I-weakness
2021-2545	The methods appear to be significantly more expensive to run <sep>	I-weakness
2021-2545	Some empirical comparisons appear to be lacking<sep>	I-weakness
2021-2545	As well as the missing comparisons mentioned by some reviewers, I think that there are a number of other missing relevant datapoints.	I-weakness
2021-2545	While not denying that gathering the available results for NewsQA/TriviaQA is much less straightforward than with that nice leaderboard for SQuAD, aren't there are lot of systems with better results on TriviaQA that aren't mentioned in the paper.	I-weakness
2021-2545	These include: RoBERTa and SpanBERT (mandarjoshi); BigBird-ETC see https://proceedings.neurips.cc/paper/2020/file/c8512d142a2d849725f31a9a7a361ab9-Paper.pdf; Longformer; SLQA see https://www.aclweb.org/anthology/P18-1158. pdf . <sep>	O
2021-2545	But, overall, I think the decision on this paper comes down to focus and contributions.	O
2021-2545	Not withstanding the growing size of ICLR, I would like to think that it is not just another ML and ML applications conference, but it is a conference centered on representation learning.	O
2021-2545	The present paper, no matter its quality and strong results, just isn't a contribution to representation learning.	B-weakness
2021-2545	It is a much better fit to an NLP conference where it would be a strong contribution to question answering, showing the continuing value of linguistic methods like question rewriting in answer validation.	B-decision
2021-2545	But this just isn't a contribution within the focus of representation learning.	B-weakness
2021-2545	Just as R4 does, I encourage the authors to clean up the presentation of the paper a bit and to submit it to an NLP conference, where it would be a strong contribution, for the reasons that R3 emphasizes. <sep>	B-decision

2021-2548	This paper studies the tensor principal component analysis problem, where we observe a tensor T = \\beta v^{\\otimes k} + Z where v is a spike and Z is a Gaussian noise tensor.	B-abstract
2021-2548	The goal is to recover an accurate estimate to the spike for as small a signal-to-noise ratio \\beta as possible.	I-abstract
2021-2548	There has been considerable interest in this problem, mainly coming from the statistics and theoretical computer science communities, and the best known algorithms succeed when \\beta \\geq n^{k/4} where n is the dimension of v. The main contribution of this paper is to leverage ideas from theoretical physics and build a matrix whose top eigenvector is correlated with v for sufficiently large \\beta using trace invariants.	I-abstract
2021-2548	On synthetic data, the algorithms achieve better performance than existing methods. <sep>	I-abstract
2021-2548	The main negative of this paper is that it is not so clear how tensor PCA is relevant in machine learning applications.	B-weakness
2021-2548	The authors gave some references to applications of tensor methods, but I want to point out that all of those works are about using tensor decompositions, which despite the fact that they are both about tensors, are rather different sorts of tools.	I-weakness
2021-2548	Many of the reviewers also found the paper difficult to follow.	I-weakness
2021-2548	I do think exposition is particularly challenging when making connections between different communities, as this work needs to introduce several notions from theoretical physics.	I-weakness
2021-2548	I am also not sure how novel the methods are, since a somewhat recent paper Moitra and Wein, "Spectral Methods from Tensor Networks", STOC 2019 also uses tensor networks to build large matrices whose top eigenvalue is correlated with a planted signal, albeit for a different problem called orbit retrieval. <sep>	I-weakness

2021-2550	The paper studies the problem of satisfying group-based fairness constraints in the situation where some demographics are not available in the training dataset.	B-abstract
2021-2550	The paper proposes to disentangle the predictions from the demographic groups using adversarial distribution-matching on a "perfect batch" generated by a clustered context set. <sep>	I-abstract
2021-2550	Pros: <sep> The problem of satisfying statistical notions of fairness under "invisible demographics" is a new and well-motivated problem. <sep> 	B-strength
2021-2550	Creative use of recent works such as DeepSets and GANs applied to the fairness problem.<sep>	I-strength
2021-2550	Cons: <sep> Makes a strong assumption that the clustering of the context set will result in a partitioning that has information about the demographics.	B-weakness
2021-2550	This requires at the very least a well-behaved embedding of the data wrt the demographic groups, and a well-tuned clustering algorithm (where optimal tuning is difficult in practice on unsupervised problems) -- but at any rate, as presented, the requirements for a "perfect batch" is neither clear nor formalized. <sep>	I-weakness
2021-2550	Lack of theoretical guarantees. <sep>	I-weakness
2021-2550	Various concerns in the experimental results (ie proposed method does not clearly outperform other baselines, high variance in experimental results, and other clarifications).<sep>	I-weakness
2021-2550	Overall, the reviewers agreed the studied problem is new, interesting and relevant to algorithmic fairness; however, there were numerous concerns (see above) which were key reasons for rejection. <sep>	B-decision

2021-2553	This work proposes a novel reparameterization of batch normalization that is hypothesized to give a better inductive bias for learning several tasks, including neural architecture search, conditional image generation, adversarial robustness and neural style transfer.	B-abstract
2021-2553	The reviewers indicate that this is useful and is of interest to the ICLR audience, but they are not satisfied with the analysis offered in the paper.	B-weakness
2021-2553	Specifically, the reviewers request that the authors provide a more detailed analysis of why the proposed reparameterization improves results, given that it does not change the expressive power of the model class.	I-weakness
2021-2553	Additionally, the reviewers have some concerns about the structure of the paper.	I-weakness
2021-2553	I therefore recommend rejecting the paper at this time. <sep>	B-decision

2021-2567	This paper introduces an object perception and control method for RL, derived from a control-as-inference formulation within a POMDP.	B-abstract
2021-2567	The paper provides a theoretical derivation and experiments where the proposed joint-inference approach outperforms baselines. <sep>	I-abstract
2021-2567	The discussion focussed on understanding the paper's contribution relative to prior work.	B-rebuttal_process
2021-2567	The reviewers highlighted the similarities with earlier systems (R1, R2, R4), the unclear benefits of joint inference over independently trained modules in the experiments (R3), and the lack of clarity of the presentation (R1, R2, R3).	I-rebuttal_process
2021-2567	The authors responded to some of these criticisms, bolstering the paper with additional experiments to show the benefits of joint inference and increasing the discussion of related work.	I-rebuttal_process
2021-2567	The reviewers examined the revisions and rebuttal and found the paper still did not resolve all their original concerns.	I-rebuttal_process
2021-2567	Two limitations mentioned in the final phase of the discussion were the use of a single environment to evaluate the general framework, and continuing doubts on the contribution of joint inference mechanism to the measured performance. <sep>	B-weakness
2021-2567	Four knowledgeable reviewers indicate reject as their concerns were not adequately resolved.	B-rebuttal_process
2021-2567	The paper is therefore rejected. <sep>	B-decision

2021-2595	The paper proposes to address the out-of-distribution generalization problem by means of conditional computation in form of a feature modulating module. <sep>	B-abstract
2021-2595	While the approach is interesting and brings a new take on how to perform feature modulation (although initially felt too similar to Conditional Batch Normalization) some major concerns about the experiments and validation of the approach are raised by all reviewers.	B-weakness
2021-2595	Some of the hypothesis made are also challenged due to lack of proper validation. <sep>	I-weakness
2021-2595	Although the discussion clarified some points I am afraid many open questions are left unanswered and would require a more work to be fully addressed before acceptance. <sep>	B-decision

2021-2608	This paper performs visual odometry using variational information bottleneck.	B-abstract
2021-2608	It assumes video and pose observations and aims to find a latent state that is maximally predictive of the pose observations, while minimizing the mutual information between the image observations and the latent state.	I-abstract
2021-2608	It approximates this cost using variational inference.	I-abstract
2021-2608	The paper also makes use of deterministic+stochastic latent transition models, as in Hafner et al 2019, 2020.	I-abstract
2021-2608	The paper contributes generalization bounds that are based on recent work by Xu and Raginsky 2017 and Zhang et al 2018.	I-abstract
2021-2608	This is useful to include, but this contribution is not the main focus of the work in my opinion, and it is not clear how tight the bounds are, especially considering that the original cost function is approximated. <sep>	B-weakness
2021-2608	Pros: <sep> The idea of using the deterministic+stochastic transition models of Hafner et al and related works for visual odometry is very interesting and promising avenue for research. <sep>	B-strength
2021-2608	The fact that the experiments are done on some of the major camera and IMU datasets is great. <sep>	I-strength
2021-2608	Needs fixing: <sep> Major: The paper mentions "Extensive ablation studies were conducted to examine the effects of (1) the deterministic component, (2) sample size and (3) extra sensors.".	B-weakness
2021-2608	These are great, but I would also have expected to see a range of variations in terms of the weight gamma, including a value of zero.	I-weakness
2021-2608	The appendix is vague in this regard and says " and perform a non-intensive and small-range grid searching."	I-weakness
2021-2608	The utility of the information bottleneck idea depends heavily on the weight gamma, and I would have liked more results confirming that the method does well under a range of choices for gamma. <sep>	I-weakness
2021-2608	Major: The rotation results produced by this method on EuroC should be improved to be more competitive with okvis.	I-weakness
2021-2608	As it stands, it is unclear why the method does not perform as well and the explanations offered in the paper are speculative. <sep>	I-weakness
2021-2608	Medium: The paper also mentions in the appendix: "Though 3D von Mises-Fisher distribution and 4D-Bingham distribution can be arguably more appropriate to model Euler angles and quaternions respectively, it is non-trivial to evaluate and use them for training in practice."	I-weakness
2021-2608	So, the paper represents rotations using Euler angles.	I-weakness
2021-2608	The authors are encouraged to look at https://www.gilitschenski.org/igor/publication/202004-iclr-deep_orientation_uncertainty_learning/ <sep>	B-suggestion
2021-2608	Minor: MSCKF was originally coined by Mourikis and Roumeliotis https://ieeexplore.ieee.org/document/4209642 and even though the term is used by follow-up works,  it is worth adding the reference. <sep>	B-weakness
2021-2608	Finally, I would disagree with one of the reviewers that this paper needs to compare with ORBSLAM2.	B-ac_disagreement
2021-2608	I think comparing with OKVIS and an MSCKF variant is sufficient for "classic" SLAM and odometry methods. <sep>	I-ac_disagreement
2021-2608	I think the paper needs one more iteration to fix these issues, even though it is very promising work. <sep>	O

