I am eager to see a hardware realization for this method because of its promising results. In overall, this paper is an accept since it shows good performance on standard problems and invent some nice tricks to implement NN in hardware, for *both* training and inference.
The results show that the agents generate effective communication systems, and some analysis is given of the extent to which these communications systems develop compositional properties – a question that is currently being explored in the literature on language creation. - A number of novel analyses are presented to evaluate the learned languages and perceptual systems.
They apply this approach to multi-modal (several "intentions") imitation learning and demonstrate for a real visual robotics task that the proposed framework works better than deterministic neural networks and stochastic neural networks. on Value Iteration Networks is highly relevant to this work: the authors there learn similar tasks (i.e., similar modalities) using the same network. The authors argue that their contribution is 3-fold: (1) does not require robot  rollouts, (2) does not require label for a task, (3) work within raw image inputs. Since the variational bound has much better sampling properties (due to recognition network, reparameterization trick and bounding to get log likelihoods instead of likelihoods) it is hard to believe that it is harder to get to work than the proposed framework. The authors cover in Section 2 related topics, and indeed the relevant literature includes behavioral cloning, IRL , Imitation learning, GAIL, and VAEs. I find that recent paper by Tamar et al 2016. The authors propose a new sampling based approach for inference in latent variable models. 2. New latent variable model bound that might work better than classic approaches.
This paper proposes a method to build a CNN in the Winograd domain, where weight pruning and ReLU can be applied in this domain to improve sparsity and reduce the number of multiplication. Rather than strictly keeping the architecture of ordinary CNNs, the proposed method applied ReLU to the transform domain, which is interesting. Because this yields a network, which is not mathematically equivalent to a vanilla or Winograd CNN, the method goes through three stages: dense training, pruning and retraining. The paper is well-written. It provides a new way to combine the Winograd transformation and the threshold-based weight pruning strategy. 3123-3131. 2015. Lin, Zhouhan, Matthieu Courbariaux, Roland Memisevic, and Yoshua Bengio. References: Courbariaux, Matthieu, Yoshua Bengio, and Jean-Pierre David. The resulting Winograd-ReLU CNN shows strong performance in three scenarios (CIFAR10 with VGG, CIFAR100 with ConvPool-CNN-C, and ImageNEt with ResNet-18). In particular, the pruned model in the Winograd domain performs comparably to the state-of-the-art dense neural networks and shows significant theoretical speedup.
This paper studies problems that can be solved using a dynamic programming approach and proposes a neural network architecture called Divide and Conquer Networks (DCN) to solve such problems. Using an architecture to learn how to split the input, find solutions, then merge these is novel. In all three cases, the proposed solution outperforms the baselines on larger problem instances.
Only whether the design of transformation captures the latent representation of the input data, the pseudo-labelling might improve the performance of the unsupervised learning task. This paper presents a method for clustering based on latent representations learned from the classification of transformed data after pseudo-labellisation corresponding to applied transformation. Given that in many applications such parent-class supervised information is not available, the authors of this paper propose domain specific pseudo parent-class labels (for example transformed images of digits) to adapt ACOL for unsupervised learning. The novelty seems to be in the adaptation to GAR from the semi-supervised to the unsupervised setting with labels indicating if data have been transformed or not.
########## UPDATED AFTER AUTHOR RESPONSE ########## I am willing to increase my score for the paper if the authors can address my points. In particular I would like to see a clear comparison in terms of latent traversals on dSprites between beta-VAE and DIP-VAE models presented in Table 3. I would like to see latent traversals of the best DIP-VAE vs beta-VAE to demonstrate good disentangling and the improvements in reconstruction quality.
In a set of experiments it is demonstrated that a significant reduction in gradient variance is achieved, resulting in speedups for training time. In Section 4 they provide quite varied empirical analysis: they confirm their theoretical results on four architectures; they show its use it to regularise on language models; they apply it on large minibatch settings where high variance is a main problem; and on evolution strategies. They start in the Introduction and Section 2 by explaining the multiple uses of random connection weights in deep learning and how the computational cost often restricts their use to a single randomly sampled set of weights per minibatch, which results to higher-variance gradient estimatos than could be achieved otherwise. However, the painful part: while I am convinced by the idea and love its detailed exposure, and the gradient variance reduction is made very clear, the experimental impact in terms of accuracy (or perplexity) is, sadly,  not very convincing.
Specifically, they observe that: (1) The angle between continuous vectors sampled from a spherical symmetric distribution and their binarized version is relatively small in high dimensions (proven to be about 37 degrees when the dimension goes to infinity), and this demonstrated empirically to be true for the binarized weight matrices of a convenet. This paper presents three observations to understand binary network in Courbariaux, Hubara et al. (2016). It further explains why binarization is able to preserve the model performance by analyzing the weight-activation dot product with "Dot Product Proportionality Property." It also proposes "Generalized Binarization Transformation" for the first layer of a neural network. This paper tries to analyze the effectiveness of binary nets from a perspective originated from the angular perturbation that binarization process brings to the original weight vector. Pros: The authors lead a very nice exploration into the binary nets in the paper, from the most basic analysis on the converging angle between original and binarized weight vectors, to how this convergence could affect the weight-activation dot product, to pointing out that binarization affects differently on the first layer. b. Related to the previous issue, it is not clear to me if in figure 3 and 5, did the authors binarize the activations of that specific layer or all the layers? Many empirical and theoretical proofs are given, as well as some practical tricks that could be useful for diagnosing binary nets in the future. So, how can the given observations be used to explain more recent works?
I have some clarifying questions below: - Figure 4 is unclear: does "Confidence loss with original GAN" refer to the method where the classifier is pretrained and then "Joint confidence loss" is with joint training? The authors propose to train a generator network in combination with the classifier and an adversarial discriminator. Classifier is trained to not only maximize classification accuracy on the real training data but also to output a uniform distribution for the generated samples. This paper is clearly written, proposes a simple model and seems to outperform current methods. However, equation 4 and algorithm 1 were very helpful in clarifying the cost function. The problem setting is new and objective (1) is interesting and reasonable. One thing missing is a discussion of how this approach is related to semi-supervised learning approaches using GANS where a generative model produces extra data points for the classifier/discriminator. - Could the authors comment on whether they think the proposed approach may be more resilient to adversarial attacks? Algorithm 1. is called "minimization for detection and generating out of distribution (samples)", but this is only gradient descent, right?
The main significance of this paper is to propose the task of generating the lead section of Wikipedia articles by viewing it as a multi-document summarization problem. The main strength is in the task setup with the dataset and the proposed input sources for generating Wikipedia articles. In general, the paper is well-written and the main ideas are clear. Unfortunately it is hard to judge the effectiveness of the abstractive model due to the scale of the experiments, especially with regards to the quality of the generated output in comparison to the output of the extractive stage. More importantly, it was not clear from the paper if there was a constraint on the output length when each model generated the Wikipedia content. Authors claim that the proposed model can generate "fluent, coherent" output, however, no evaluation has been conducted to justify this claim. Although authors argue in Section 2.1 that existing neural approaches are applied to other kinds of datasets where the input/output size ratios are smaller,  experiments could have been performed to show their impact. --- The additional experiments and clarifications in the updated version give substantially more evidence in support of the claims made by the paper, and I would like to see the paper accepted.
The paper devises a sparse kernel for RNNs which is urgently needed because current GPU deep learning libraries (e.g., CuDNN) cannot exploit sparsity when it is presented and because a number of works have proposed to sparsify/prune RNNs so as to be able to run on devices with limited compute power (e.g., smartphones). Unfortunately, due to the low-level and GPU specific nature of the work, I would think that this work will be better critiqued in a more GPU-centric conference.
The paper extends softmax consistency by adding in a relative entropy term to the entropy regularization and applying trust region policy optimization instead of gradient descent. - Even though the paper claims Trust-PCL (on-policy) is close to TRPO, the initial performance of TRPO looks better in HalfCheetah, Hopper, Walker2d and Ant. - Some ablation studies (e.g., on entropy regularization and relative entropy) and sensitivity analysis on parameters (e.g. \\alpha and update frequency on \\phi) would be helpful.
The most interesting part is a joint training for both compression and image classification. Pros: + The idea of learning from a compressed representation is a very interesting and beneficial idea for large-scale image understanding tasks. In fact, I am glad someone asked this question and tried to answer it. I changed my given score from 3 to 6. Summary: This work explores the use of learned compressed image representation for solving 2 computer vision tasks without employing a decoding step.
Strong points: * To my knowledge, the proposed defense strategy is novel (even if the idea of transformation has been introduced at https://arxiv.org/abs/1612.01401). To increase robustness to adversarial attacks, the paper fundamentally proposes to transform an input image before feeding it to a convolutional network classifier. On the contrary, white-box means that the adversary knows everything about the classification method, including the transformation implemented to make it more robust to attacks. * The writing is reasonably clear (up to the terminology issues discussed among the weak points), and introduces properly the adversarial attacks considered in the work. * In a white-box scenario, the adversary knows about the transformation and the classification model. The discussion remarks are particularly interesting: the non differentiability of the total variation and image quilting methods seems to be the key to their best performance in practice. Overall, the works investigates an interesting idea, but lacks maturity to be accepted.
The paper proposes the Skip RNN model which allows a recurrent network to selectively skip updating its hidden state for some inputs, leading to reduced computation at test-time. Experiments are performed on a variety of tasks, demonstrating that the Skip-RNN compares as well or better than baselines even when skipping nearly half its state updates.
Overall the strength of this paper is that the main insight is quite interesting — though many people have informally thought of residual networks as having this interpretation — this paper is the first one to my knowledge to explain the intuition in a more precise way. This paper shows that residual networks can be viewed as doing a sort of iterative inference, where each layer is trained to use its "nonlinear part" to push its values in the negative direction of the loss gradient.
Gated Graph Sequence Neural Networks. However, I am a little worried that the proposed model may be hard to reproduce due to its complexity and therefore choose to give a 6. The rebuttal is convincing and I decided to increase my rating, because adding the proposed counting module achieve 5% increase in counting accuracy. I wonder whether the proposed method scales. The paper is revised and I saw NMS baseline is added. I would recommend the authors to use something more general, like graph convolutional neural networks (Kipf & Welling, 2017) or graph gated neural networks (Li et al., 2016).
The model proposed is a variant of the cycle GAN in which in addition embeddings helping the Generator are learned for all the values of the discrete variables. Unfortunately, my understanding is that the theory proposed in section 2 does not correspond to the scheme used in the experiments (contrarily to what the conclusion suggest and contrarily to what the discussion of the end of section 3, which says that using embedding is assumed to have an equivalent effect to using the methodology considered in the theoretical part).
The paper proposes a way to speed up the inference time of RNN via Skim mechanism where only a small part of hidden variable is updated once the model has decided a corresponding word token seems irrelevant w.r.t. a given task. Contribution: - The paper proposes to use a small RNN to read unimportant text. It would be good to use REINFORCE to do a fair comparison with (Yu et al., 2017 ) to see the benefit of using small RNN.
Apart from efficiency results, the paper also contributes a comparison of model convergence on a long-term dependency task due to (Hochreiter and Schmidhuber, 1997). A novel linearized version of the LSTM outperforms traditional LSTM on this long-term dependency task, and raises questions about whether RNNs and LSTMs truly need the nonlinear structure. A key problem here is that processing such sequences with ordinary RNNs requires a reduce operation, where the output of the net at time step t depends on the outputs of *all* its predecessor.
The idea to constraint the dimension reduction to fit a certain model, here a GMM, is relevant, and the paper provides a thorough comparison with recent state-of-the-art methods. I'm also not convinced of how well the Gaussian model fits the low-dimensional representation and how well can a neural network compute the GMM mixture memberships. Significance – Constraining the dimension reduction to fit a certain model is a relevant topic, but I'm not convinced of how well the Gaussian model fits the low-dimensional representation and how well can a neural network compute the GMM mixture memberships.
This is complemented by the SCAN model, which is a beta-VAE trained to reconstruct symbols (y; k-hot encoded concepts like {red, suitcase}) with a slightly modified objective. It would also be helpful to see a more extensive evaluation of the model's ability to learn logical recombination operators, since this is their main contribution. The main way that their model differs from other multimodal methods is that their latent representation is well-suited to applying symbolic operations, such as AND and IGNORE, to the text. This paper does have technical innovations: the SCAN architecture and the way they learn "recombination operators" are newly proposed.
In this paper, the authors studied the problem of semi-supervised few-shot classification, by extending the prototypical networks into the setting of semi-supervised learning with examples from distractor classes. Extensive experiments are performed to demonstrate the effectiveness of the proposed methods. Overall, I would like to vote for a weakly acceptance regarding this paper.
Authors present complex valued analogues of real-valued convolution, ReLU and batch normalization functions. An important argument in favour of using complex-valued networks is said to be the propagation of phase information. I'm on the fence about this work: I like the ideas and they are explained well, but I'm missing some insight into why and how all of this is actually helping to improve performance (especially w.r.t. how phase information is used). However, cases the authors address, which are training batch-norm ReLU networks on standard datasets, are already formulated in terms of real valued arithmetic. 2. Formulate two complex-valued alternatives to ReLU and compare them Some more insight into how phase information is used, what it represents and how it is propagated through the network would help to make sense of this. Empirical results show that the new complex-flavored neural networks achieve generally comparable performance to their real-valued counterparts, on a variety of different tasks.
Final comments: - I like the idea and it seems novel it may lead to some promising research directions related to lossy pooling methods/channel aggregation. While the currently tested datasets are a good indication of the performance of the proposed method, an evaluation on a large scale scenario, e.g. ILSVRC'12, could solidify the message sent by this manuscript. As such, I think it will be a nice addition to *CONF*, especially if the authors decide to run some of the experiments I was suggesting, namely: show what happens when larger pooling windows are used (say 4x4 instead of 2x2), and compare to other lossy techniques (such as Fourier or cosine-transforms).
In the actual performance, the paper presents both practical efficiency and better generalization error in different deep neural networks for image classification tasks, and the authors also show differences according to different settings, e.g., Batch Size, Regularization. Comments: I really appreciate the author(s) by providing experiments using real models on the ImageNet dataset. The algorithm seems to be easily used in practice. As you mention in the paper that the algorithm uses the same amount of computation and memory as Adam optimizer, but could you please provide the reason why you only compare Neumann Optimizer with Baseline RMSProp but not with Adam? I understand that you are trying to improve the existing results with their optimizer, but this paper also introduces new algorithm.
Summary of paper: The authors present a novel attack for generating adversarial examples, deemed OptMargin, in which the authors attack an ensemble of classifiers created by classifying at random L2 small perturbations. The paper presents a new approach to generate adversarial attacks to a neural network, and subsequently present a method to defend a neural network from those attacks. The provided analysis is insightful, though the authors mostly fail to explain how this analysis could provide further work with means to create new defenses or attacks. As a summary, the authors presented a method that successfully attacks other existing defense methods, and present a method that can successfully defend this attack. In addition to examining the effectiveness, authors also performed experiments to explain why OPTMARGIN is superior. This and / or the defense of training to adversarial examples is an important experiment to assessing the limitations of the attack.
Optimizing this formulation using gradient descent can be proven to yield only one optimal global Nash equilibrium, which the authors claim allows Coulomb GANs to overcome the "mode collapse" issue. The authors draw from electrical field dynamics and propose to formulate the GAN learning problem in a way such that generated samples are attracted to training set samples, but repel each other. Leave the stochastic gradient descent optimization algorithm apart (since most of the neural networks are trained in this way), the parametrization and the richness of discriminator family play a vital role in the model collapsing issue. In fact, even with KL-divergence in which log operation is involved, if one can select reasonable parametrization, e.g., directly handling in function space, the saddle point optimization is convex-concave, which means under the same assumption made in the paper, there is only one global Nash Equilibrium. Overall, it presents an interesting approach to overcome the mode collapse problem with GANs.
--- Quality: The experiments compare the three proposed neural network architectures with two syntax-based architectures. The proposed models make sense and the writing is for the most part clear, though there are a few places where ambiguity arises: - The variable "Evidence" in equation (4) is never defined. - Figure 5 seems to suggest that dependencies are only enforced at points in a program where assignment is performed for a variable, is this correct? However the application to program repair is novel (as far as I know). Clarity: The paper is clearly written. I noticed the following issues: 1) The learning task is based on error patterns, but it's not clear to me what exactly that means from a software development standpoint. (2017). Making Neural Programming Architectures Generalize via Recursion.
As a result, this would allow the evolutionary search algorithm to design modules which might be then reused in different edges of the DAG corresponding to the final architecture, which is located at the top level in the hierarchy. In addition, the authors present results that appear to be on par with the state-of-the-art with architecture search on CIFAR-10 and ImageNet benchmark datasets. The method proposed for an hierarchical representation for optimizing over neural network designs is well thought and sound. Compared to existing work, this approach should emphasise modularity, making it easier for the evolutionary search algorithm to discover architectures that extensively reuse simpler blocks as part of the model. Instead of merely defining an architecture as a Directed Acyclic Graph (DAG), with nodes corresponding to feature maps and edges to primitive operations, the approach in this paper introduces a hierarchy of architectures of this form. Finally, while the main motivation behind neural architecture search is to automatise the design of new models, the approach here presented introduces a non-negligible number of hyperparameters that could potentially have a considerable impact and need to be selected somehow. An important contribution is to show that a well-defined architecture representation could lead to efficient cells with a simple randomized search.
There is a big literature on learning from demonstrations that the authors could compare with, or explain why their work is different. Comments: - The paper is well-written and relevant literature is cited and discussed. Exploration is still a challenging problem for RL. The paper is clear, very well written, and well-motivated. However experimentally the paper performs better than the previous approach. Dagger and related methods like Aggrevate provide sample-efficient ways of exploring the environment near where the initial demonstrations were given. - My main concern is that while imitation learning and inverse reinforcement learning are mentioned and discussed in related work section as classes of algorithms for incorporating prior information there is no baseline experiment using either of these methods.
Two key ideas in the paper include the use of Gaussian noise for the aggregation mechanism in PATE instead of Laplace noise and selective answering strategy by teacher ensemble. The novelty of this work is a refined aggregation process, which is improved in three ways: a) Gaussian instead of Laplace noise is used to achieve differential privacy. The paper proposes novel techniques for private learning with PATE framework. Summary: In this work, PATE, an approach for learning with privacy,  is modified to scale its application to real-world data sets. It is demonstrated that sampling from a Gaussian distribution (instead from a Laplacian distribution) facilitates the aggregation of teacher votes in tasks with large number of output classes. I am not familiar with privacy learning but it is interesting to see that more concentrated distribution (Gaussian) and clever aggregators provide better utility-privacy tradeoff. I think this is a nice modular framework form private learning, with significant refinements relative to previous work that make the algorithm more practical. on the positive side: Having scalable models is important, especially models that can be applied to data with privacy concerns. The paper is well written, and the idea of the model is clear. The extension of an approach for learning with privacy to make it scalable is of merit. This is for sure an important topic. These works also involve a "student" being trained using sensitive data with queries being answered in a differentially private manner. minor comments: Figure 2, legend needs to be outside the Figure, in the current Figure a lot is covered by the legend This paper considers the problem of private learning and uses the PATE framework to achieve differential privacy. If the final privacy guarantee is data-dependent, then this is very different to the way differential privacy is usually applied.
The experimental results are convincing enough to show that it outperforms other active learning algorithms. Active learning for deep learning is an interesting topic and there is few useful tool available in the literature.
Typographical Issues - Page 1:  "floor-level accuracy" back ticks Update: Based on the discussions and the revisions, I have improved my rating.
In this paper, the expressive power of neural networks characterized by tensor train (TT) decomposition, a chain-type tensor decomposition, is investigated. The result of this paper is interesting and also important from a viewpoint on analysis for the tensor train decomposition. (b) Standard RNNs do not use the multilinear units shown in Figure 3, but use a simple addition of an input and the output from the previous layer (i.e., h_t = f(Wx_t + Vh_{t-1}), where h_t is the t-th hidden unit, x_t is the t-th input, W and V are weights, and f is an activation function.) In addition, I would like to see the performance of RNNs and MLPs with the same number of units/rank in order to validate the analogy between these networks. Finally the authors show that almost all tensor train networks (exluding a set of measure zero) require exponentially large width to represent in CP networks, which is analogous to shallow networks. The authors compare the complexity of TT-type networks with networks structured by CP decomposition, which corresponds to shallow networks.
Several of the cited papers use counters to determine which states are "known" and then solve an MDP to direct exploration past immediate outcomes. It would have also been nice to do some analysis on how the update rule in a function approximation case is affecting the bonus terms. What is crucially lacking from the paper is any reference to model-free Bayesian methods that have very similar intuition behind them: taking into account the long term exploratory benefits of actions (passed on through the Bayesian inference). The paper has minimal theoretical analysis of the proposed algorithm, essentially only showing convergence with infinite visiting. ": I think you should cite http://research.cs.rutgers.edu/~nouri/papers/nips08mre.pdf , which also combines a kind of counter idea with function approximation to improve exploration.
The comparison with relative gain of bootstrap wrt ensemble of policies still needs more thorough experimentation, but the approach is novel and as the authors point out, does improve continually with better Contextual Bandit algorithms. 3. The method claims to learn an internal representation of a denser reward function for the sparse reward problem, however the experimental analysis of this is pretty limited (Section 5.3). The authors provide a few regret theoretical results (that I did not check deeply) obtained by reduction to "value-aware" contextual bandits. The algorithm design and theoretical results in the appendix could be made substantially more rigorous. -- At the end of page 4, "Internally, ReslopePolicy takes a standard learning to search step."  Two issues: 1) ReslopePolicy is not defined or referred to anywhere else.
I now think the paper does just enough to warrant acceptance, although I remain a bit concerned that since the benefits are only achievable with customized hardware, the relevance/applicability of the work is somewhat limited. In the end, while do like the general idea of utilizing the gradient to identify how sensitive the model might be to quantization of various parameters, there are significant clarity issues in the paper, I am a bit uneasy about some of the compression results claimed without clearer description of the bookkeeping, and I don't believe an approach of this kind has any significant practical relevance for saving runtime memory or compute resources. It would be interesting to know an analogy, for instance, saying that this adaptive compression in memory would be equivalent to quantizing all weights with n bits. *l2 -> L_2 or l_2 in section 3.1 last paragraph. I am confused by the equality in Equation 8: What happens for values shared by many different quantization bit depths (e.g., representing 0 presumably requires 1 bit, but may be associated with a much finer tolerance)? *Delta -> \\Delta in last paragraph of section 2.2
I am still not convinced by the convolution case (which is the main point of this paper), as even though it does not require Gaussian input (a major plus), it still seems very far from "general distribution". A major strength of the result is that it can work for general continuous distributions and does not really rely on the input distribution being Gaussian; the main weakness is that some of the distribution dependent quantities are not very intuitive, and the alignment requirement might be very high.
[Reviewed on January 12th] This article applies the notion of "conceptors" -- a form of regulariser introduced by the same author a few years ago, exhibiting appealing boolean logic pseudo-operations -- to prevent forgetting in continual learning,more precisely in the training of neural networks on sequential tasks. This paper introduces a method for learning new tasks, without interfering previous tasks, using conceptors. I think that this work might interest the community since such methods might be part of the tools that practitioners have in order to cope with learning new tasks without destroying the previous ones. I realise not everyone has the computational or engineering resources to try extensively on multiple benchmarks from classification to reinforcement learning.
The paper by Arjovsky et al (2017) provided a framework based on the Wasserstein distance, a distance measure between probability distributions belonging to the class of so-called Integral Probability Metrics (IPMs).
However, as I pointed out above, there are several weaknesses in the paper. The proposed model can generate question with respect to different topic and pre-decode seems a useful trick. In this paper, the authors treat the descriptive text as answers, is this motivation still true if the question generation is conditioned on answers, not descriptive text? When the ground truth topic is provided, it's not fair to compare with the previous method, since knowing the similar word present in the answer will have great benefits to question generation. Authors claim to generate topic-specific questions, however, the dataset choice, experiments, and examples show that the generated questions are essentially keyword/key phrase-based. If we only consider the automatically generated topic, the performance of the proposed model is similar to the previous method (Du et al). I would also suggest, for your next experiments, that you try to generate questions leading to answers with list of values. Also, please explain the absence of the "why" type question. Taking all these into account, I think this paper still needs more works to make it solid and comprehensive before being accepted. Then, the author proposed a topic-specific question generation model by encoding the extracted topic using LSTM and a pre-decode technique that the second decoding is conditioned on the hidden representation of the first decoding result.
Summary: The paper considers the problem of a single hidden layer neural network, with 2 RELU units (this is what I got from the paper In this paper the authors studied the theoretical properties of manifold descent approaches in a standard regression problem, whose regressor is a simple neural network. Leveraged by two recent results in global optimization, they showed that with a simple two-layer ReLU network with two hidden units, the problem with a standard MSE population loss function does not have spurious local minimum points. These are crucial for understanding the contribution of the paper; while reading the paper, I assumed that the authors consider the case of a single hidden unit with K = 2 RELU activations (however, that complicated my understanding on how it compares with state of the art). Based on the results by Lee et al, which shows that first order methods converge to local minimum solution (instead of saddle points), it can be concluded that the global minima of this problem can be found by any manifold descent techniques, including standard gradient descent methods. in total?). Later on, in Section 3, the expression at the bottom of page 2 seems to consider a single-layer RELU network, with two units. The title is not clear whether the paper considers a two layer RELU network or a single layer with with two RELU units.
This paper studies the question: Why does SGD on deep network is often successful, despite the fact that the objective induces bad local minima? Secondly, as the authors aptly pointed out in the discussion section, this results doesn't mean neural networks will converge to good local minima because these bad local minimas can have a large basins of attraction. Pros: Authors ask the question of convergence of optimization (ignoring generalization error): how "likely" is that an over-parameterized (d1d0 > N) single hidden layer binary classifier "find" a good (possibly over-fitted) local minimum. ## Summary This paper aims to tackle the question: "why does standard SGD based algorithms on neural network converge to 'good' solutions?"
Summary: This paper studies the geometry of linear and neural networks and provides conditions under which the local minima of the loss are global minima for these non-convex problems. For example, Lemma 4 is not correct as written — an invertible mapping \\sigma is not necessarily locally open. Paper claims to remove assumption on Y, but they get much weaker results as they cannot differentiate between saddle points and global minima, for a critical point. It will attract some attention at the conference.
On the other hand, the whole model structure looks to be easily generalized to other tree-to-tree tasks and might have some potential to contribute this kind of problems. If authors were interested in the tendency of real program translation task, they should arrange the experiment by collecting parallel corpora between some unrelated programming languages using resources in the real world. There are two benefits of using the tree2tree model: i) use the grammar of the language, and ii) use the structure of the tree for locating relevant sub-trees
This paper is utilizing reinforcement learning to search new activation function. Based on the Figure 6 authors claim that the non-monotonic bump of Swish on the negative side is very important aspect. al. (2017). In terms of experimental validation, in most cases the increase is performance when using Swish as compared to other models are very small fractions. Overall, I think this paper is not meeting *CONF* novelty standard.
This paper presents a study of reinforcement learning methods applied to Erdos-Selfridge-Spencer games, a particular type of two-agent, zero-sum game. The authors describe the game and some of its properties, notably that there exists a tractable potential function that indicates optimal play for each player for every state of the board. I am also not aware of trying to use games with known potential functions/optimal moves as a way to study the performance of RL algorithms. In particular, I have the following concerns: • these games have optimal policies that are expressible as a linear model, meaning that if the architecture or updating of the learning algorithm is such that there is a bias towards exploring these parts of policy space, then they will perform better than more general algorithms. ◦ It is unclear whether 'incorrect actions' in the supervised learning evaluations, refer to non-optimal actions, or simply actions that do not preserve the dominance of the defender, e.g. both partitions may have potential >0.5 This relates to the "surprising" fact that "Reinforcement learning is better at playing the game, but does worse at predicting optimal moves.". There is value in this work, but in its current state I do not think it is ready for publicaiton. The defender network has to do this to the features of A and of B, and compare the values; the attacker (with the action space following theorem 3) has to do this for (at most) K progressive partitions. Various points: - The explanation of the Erdos-Selfridge-Spencer attacker-defender game is clear. All of this leads me to think that a linear baseline is a must-have in most of the plots, not just Figure 15 in the appendix on one task, moreso as the environment (game) is new. [p7 Fig 6 and text] Here the authors are comparing how well agents select the optimal actions as compared to how close they are to the end of the game. In fact, there is a level of non-determinism in how the attacker policies are encoded which means that an optimal policy cannot be (even up to soft-max) expressed by the agent (as I read things the number of pieces chosen in level l is always chosen uniformly randomly). Reinforcement learning >> is better at playing the game, but does worse at predicting optimal moves.
The authors have to include RL agent in all their experiments to be able to dissociate what is due to human priors and what is due to the noise introduced in the game. Overall: I really enjoyed reading this paper and think the question is super important. Both the premises assumed and the conclusions drawn are quite reasonable given the experimental paradigm and domain in which they are conducted. Given recent advances in RL and ML that eschew all manner of structured representations, I believe this is a well-timed reminder that being able to transfer know-how from human behaviour to artificially-intelligent ones. The paper is clearly written, and the experiments follow a clean and coherent narrative. The authors' Main Claim appears to be: "While common wisdom might suggest that prior knowledge about game semantics such as ladders are to be climbed, jumping on spikes is dangerous or the agent must fetch the key before reaching the door are crucial to human performance, we find that instead more general and high-level priors such as the world is composed of objects, object like entities are used as subgoals for exploration, and things that look the same, act the same are more critical." There are two ways to interpret the authors' main claim: the strong version would maintain that semantic priors aren't important at all. The problem here comes from an unclear definition of what the authors mean by an "object" so in revision I would like authors to clarify what precisely they mean by a prior about "the world is composed of objects" and how this particular experiment differentiates "object" from a more general prior about "video games have clearly defined goals, there are 4 clearly defined boxes here, let me try touching them." There were a couple of concerns I did have however: 1. Right at the beginning, and through the manuscript, there is something of an apples-to-oranges comparison when considering how quickly humans can complete the task (order of minutes) and how quickly the SOTA RL agents can complete the task (number of frames). The authors interpret the fact that performance falls so much between conditions b and c to mean that human priors about "objects are special" are very important. However, I believe that many of my criticisms can be assuaged during the rebuttal period. So it cannot be concluded that the change of performances is due to human priors. This paper investigates human priors for playing video games.
Summary: In this paper the authors offer a new algorithm to detect cancer mutations from sequencing cell free DNA (cfDNA). In this paper the author propose a CNN based solution for somatic mutation calling at ultra low allele frequencies. We also liked the thoughtful construction of the network and way the reference, the read, the CIGAR and the base quality were all combined as multi channels to make the network learn the discriminative features of from the context. Using matched samples of tumor and normal from the patients is also a nice idea to mimic cfDNA data. The authors suggest to overcome this problem by training an algorithm that will identify the sequence context that characterize sequencing errors from true mutations. The authors claim the reduced performance show they are learning lung cancer-specific context. Finally, performance itself did not seem to improve significantly compared to previous methods/simple filters, and the novelty in terms of ML and insights about learning representations seemed limited. The data is based on mutations in 4 patients with lung cancer for which they have a sample both directly from the tumor and from a healthy region. Since we know nothing about all these samples it may very well be that that are learning technical artifacts related to their specific batch of 4 patients. If the entire point is to classify mutations versus errors it would make sense to combine their read based calls from multiple reads per mutations (if more than a single read for that mutation is available) - but the authors do not discuss/try that. A more reasonable CS style of organization is to first introduce the methods/model and then the results, but somehow the authors flipped it and started with results first, lacking many definitions and experimental setup to make sense of those. Moreover, the authors filter the "normal" samples using those (p.7 top), which makes the entire exercise a possible circular argument. The issue is that the ctDNA are expected to be found with low abundance in such samples, and therefore are likely to be hit by few or even single reads. We believe the method and paper could potentially be improved and make a good fit for a future bioinformatics focused meeting such as ISMB/RECOMB.
The paper presents a combination of evolutionary computation (EC) and variational EM for models with binary latent variables represented via a particle-based approximation. Experiments on artificial bars data and natural image patch datasets compare several variants of the proposed method, while varying a few EA method substeps such as selecting parents by fitness or randomly, including crossover or not, or using generic or specialized mutation rates. After Mutation section: Remind readers that "N_g" is number of generations This paper proposes an evolutionary algorithm for solving the variational E step in expectation-maximization algorithm for probabilistic models with binary latent variables.
Although the author claim the novelty as adding noise to the discriminator, it seems to me that at least for the RBF case it just does the following: 1. write down MMD as an integral probability metric (IPM) This manuscript explores the idea of adding noise to the adversary's play in GAN dynamics over an RKHS. ==== original review === The paper describes a generative model that replaces the GAN loss in the adversarial auto-encoder with MMD loss.
This paper introduces a comparison between several approaches for evaluating GANs. The authors consider the setting of a pre-trained image models as generic representations of generated and real images to be compared. The paper describes an empirical evaluation of some of the most common metrics to evaluate GANs (inception score, mode score, kernel MMD, Wasserstein distance and LOO accuracy). In the paper, the authors discuss several GAN evaluation metrics. I think this paper tackles an interesting and important problem, what metrics are preferred for evaluating GANs. In particular, the authors showed that Inception Score, which is one of the most popular metric, is actually not preferred for several reasons. Appendix G in https://arxiv.org/pdf/1706.04987.pdf) Do you think it would be useful to compare other generative models (e.g. VAEs) using these evaluation metrics? - The authors implicitly contradict the argument of Theis et al against monolithic evaluation metrics for generative models, but this is not strongly supported. Cons -It is not clear why GANs are the only generative model considered -The evaluations rely on using a pre-trained imagenet model as a representation. The authors point out that different architectures yield similar results for their analysis, however it is not clear how the biases of the learned representations affect the results. The use of learned representations needs more rigorous justification -Unprecedented visual quality as compared to other generative models has brought the GAN to prominence and yet this is not really a big factor in this paper. -The evaluation for discriminative metric, increased score when mix of real and unreal increases, is interesting but it is not convincing as the sole evaluation for "discriminativeness" and seems like something that can be gamed. E.g. Danihelka et al proposed using an independent Wasserstein critic to evaluate GANs: Comparison of Maximum Likelihood and GAN-based training of Real NVPs https://arxiv.org/pdf/1705.05263.pdf Overall, I think this paper is worthy for acceptance as several GAN methods are proposed and good evaluation metrics are needed for further improvements of the research field.
The authors propose a method for graph classification by combining graph kernels and CNNs. In a first step patches are extracted via community detection algorithms. Moreover, although the authors claim that typical graph kernel methods are two-stage approached decoupling representation from learning, their proposal also folds into that respect, as representation is achieved in the preprocessing step of patching extractions and normalization, while learning is achieved by the CNN. * The originality is not high as the application of neural networks for graph classification has already been studied elsewhere and the proposed method is a direct combination of three existing methods, community detection, graph kernels, and CNNs. Have the authors  tried to test what is the performance of graph kernel representation in the complete graph as input to the CNN, instead of a set of patches? The paper is well written, proposes an interesting and original idea, provides experiments with real graph datasets from two domains, bioinformatics and social sciences, and a comparison with SoA algorithms both graph kernels and other deep learning architectures.
The proposed architecture could identify the 'key' states through assigning higher weights for important states, and applied reservoir sampling to control write and read on memory. Therefore, empirically, it is really hard to justify whether this proposed method could work better. Also, intuitively, this episodic memory method should work better on long-term dependencies task, while this article only shows the task with 10 timesteps. For the latter question, the authors propose using a "query network" that based on the current state, pulls out one state from the memory according to certain probability distribution.
Overall, I don't think this paper meet *CONF*'s novelty standard, although the authors present some good numbers, but they are not convincing. This paper proposes an activation function, called displaced ReLU (DReLU), to improve the performance of CNNs that use batch normalization. The proposed method shows encouraging results in a controlled setting (i.e., all other units, like dropout, are removed). The arguments for skipping this experiments are respectful, but not convincing enough. Although the BN paper suggests using BN before non-linearity many articles have been using BN after non-linearity which then gives normalized activations (https://github.com/ducha-aiki/caffenet-benchmark/blob/master/batchnorm.md) and also better overall performance. 2) I believe the control experiments are encouraging, but I do not agree that other techniques like Dropouts are not useful. I personally have tried ELU/LReLU/RReLU on Inception V3 with Batch Norm, and all are better than ReLU. 3. In all experiments, ELU/LReLU are worse than ReLU, which is suspicious.
In the paper "DEEP DENSITY NETWORKS AND UNCERTAINTY IN RECOMMENDER SYSTEMS", the authors propose a novel neural architecture for online recommendation. This paper presents a methodology to allow us to be able to measure uncertainty of the deep neural network predictions, and then apply explore-exploit algorithms such as UCB to obtain better performance in online content recommendation systems. Moreover the given strategy would achieve a linear regret if used as described in the paper which is not desirable for bandits algorithms (smallest counter example with two arms following a Bernouilli with different parameters if the best arms generates two zero in a row at the beginning, it is now stuck with a zero mean and zero variance estimate). The experiments only concern two slightly different versions of the proposed algorithm in order to show the importance of the deconvolution of both considered noises, but nothing indicates that the model performs fairly well compared to existing approaches. Also, it would have been useful to compare ot to other neural models dealing with uncertainty (some of them having been applied to bandit problems- e.g., Blundell et al. (2015)). * If the claim is about a better exploration,  I'd like to have an idea of the influence of the tuning parameters and possibly a discussion/comparison over alternatives strategies (including an epsilon-n greedy algorithm) My main doubt comes from Section 4.2.1, as I am not sure how exactly the two subnets fed into MDN to produce both mean and variance, through another gaussian mixture model. The only positioning argument that is given in that section is the final sentence "In this paper we model measurement noise using a Gaussian model and combine it with a MDN". The state of the art section is very confusing, with works given in a random order, without any clear explanation about the limits of the existing works in the context of the task addressed in the paper. One depend only on the number of observations of the given context and the average reward in this situation and the second term begin the noise.
The paper proves the separation by constructing a very specific function that cannot be approximated by 2-layer networks. The fact that sigmoidal neural networks with bounded weights can be expressed as "low" degree polynomials is not new. Moreover, although the result is robust to the choice of input distribution, the function used to get the lower bound is still rather artificial ( x -> sin(N||x||^2) for some large N).
There are a few ideas the paper discusses: (1) compared to pruning weight matrices and making them sparse, block diagonal matrices are more efficient since they utilize level 3 BLAS rather than sparse operations which have significant overhead and are not "worth it" until the matrix is extremely sparse. There are two half-papers here, one on parameter pruning and one on applying insights from random matrix theory to neural networks, but I don't see a strong connection between them. (3) to summarize points (1) and (2), block diagonal architectures are a nice alternative to pruned architectures, with similar accuracy, and more benefit to speed (mainly speed at run-time, or speed of a single iteration, not necessarily speed to train)
p5: There is related work for incorporating trace supervision into a neural abstract machine that is otherwise trained end-to-end from input-output examples [2]. Weaknesses As explained in the summary, it is not clear to me why the abstraction to NCM is useful if one still needs to define specific subtrace losses for different neural abstract machines.
Linda Smith and Eliana Colunga have published a series of papers that explore these questions in detail: http://www.iub.edu/~cogdev/labwork/kinds.pdf http://www.iub.edu/~cogdev/labwork/Ontology2003.pdf I was impressed by the range of phenomena they tackled and their analyses were informative in understanding the behavior of deep learning models One analysis that would have helped convince me is a comparison to an equivalent non-grounded deep learning model (e.g., a CNN trained to make equivalent classifications), and show how this would not help us understand human behavior. 4.1 Word learning biases This experiment shows that, when an agent is trained on shapes only, it will exhibit a shape bias when tested on new shapes and colors. The crucial question, here, would be whether, when an agent is trained in a naturalistic environment (i.e., where distributions of colors, shapes and other properties reflect those encountered by biological agents), it would show a human-like shape bias. I also found it hard to understand why colors were hard to learn given the bias towards colors shown earlier in the paper. Concerning the attention analysis, it seems to me that all it's saying is that lower layers of a CNN detect lower-level properties such as colors, higher layers detect more complex properties, such as shapes characterizing objects. 4. The section on learning speeds could include more information on the actual patterns that are found with human learners, for example the color words are typically acquired later. What would it mean if the equivalent non-situated model does not show the phenomena? Comments: 1. The results on word learning biases are not particularly surprising given previous work in this area, much of which has used similar neural network models. Also, I don't see what we should learn from Figure 5 (besides the fact that in the controlled condition shapes are easier than categories).
This submission claims that: [a] "[based on the critiqued paper] one might assume that DRL-based algorithms are able to 'learn to navigate' and are thus ready to replace classical mapping and path-planning algorithms", More worryingly, when observing that the method of (Mirowski et al, 2017) may not generalize to unseen environments in claim [c], the authors of this submission seem to confuse navigation, cartography and SLAM, and attribute to that work claims that were never made in the first place, using a straw man argument. It seems, as the authors also claim in [b], that the work of (Mirowski et al, 2017), which was about navigation in known environments, actually is repeatable. I therefore recommend not to accept this paper in its current form.
Based on experiments using CIFAR10, the authors show that adversarial training is effective in protecting against "shared" adversarial perturbation, in particular against universal perturbation. - Singular perturbations are easily detected by a detector model, as such perturbations don't change much when applying AT. The paper shows that adversarial training increases the destruction rate of adversarial examples so that it still has some value though it would be good to see if other adversarial resistance techniques show the same effect. Summary: This paper empirically studies adversarial perturbations dx and what the effects are of adversarial training (AT) with respect to shared (dx fools for many x) and singular (only for a single x) perturbations.
The paper proposes an understanding of the relation between inverse problems, CNNs and sparse representations. For example different network architecture figures (training/testing for CNNs) could be used to explain in a compact way instead of plain text. Given that much of the main result seems to already be known, I feel that this work is not novel enough at this time.
To reduce the memory required for training, the authors also propose a path-wise training procedure based on the independent convolution paths of CrescendoNet. The experimental results on CIFAR-10, CIFAR-100 and SVHN show that CrescendoNet outperforms most of the networks without residual connections. The authors claim that "Through our analysis and experiments, we note that the implicit ensemble behavior of CrescendoNet leads to high performance". While CrescendoNet seems to slightly outperform FractalNet in the experiments conducted, it is itself outperformed by DiracNet. Hence, CrescendoNet does not have the best performance among skip connection free networks. While this is not explicitly mentioned in the FractalNet paper, it clearly would not break the design principle of FractalNet which is to train a path of multiple layers by ensembling it with a path of fewer layers.
This paper consider a version of boosting where in each iteration only class weights are updated rather than sample weights and apply that to a series of CNNs for object recognition tasks. - While the motivation is that classes have different complexities to learn and hence you might want each base model to focus on different classes, it is not clear why this methods should be better than normal boosting: if a class is more difficult, it's expected that their samples will have higher weights and hence the next base model will focus more on them. -  "to replace the softmax error function (used in deep learning)": I don't think we have softmax error function In conventional boosting methods, one puts a weight on each sample.
Summary: The authors present a simple variation of vanilla recurrent neural networks, which use ReLU hiddens and a fixed identity matrix that is added to the hidden-to-hidden weight matrix. - While the LSTM baseline matches the results of Le et al., later work such as Recurrent Batch Normalization or Unitary Evolution RNN have demonstrated much better performance with a vanilla LSTM on those tasks (outperforming both IRNN and RIN).
The paper presents a number of interesting results: 1) Larger networks are typically more learnable than smaller ones (typically we think of larger networks as being MORE complicated than smaller networks -- this result suggests that in an important sense, large networks are simpler). These results are in line with several of the observations made by Zhang et al (2017), which showed that neural networks are able to both (a) fit random data, and (b) generalize well; Review Summary: The primary claim that there is "a strong correlation between small generalization errors and high learnability" is correct and supported by evidence, but it doesn't provide much insight for the questions posed at the beginning of the paper or for a general better understanding of theoretical deep learning. More importantly, this relationship between test accuracy and learnability doesn't answer the original question Q2 posed: "Do larger neural networks learn simpler patterns compared to neural networks when trained on real data". Other results presented in the paper are puzzling and require further experimentation and discussion, such as the trend that the learnability of shallow networks on random data is much higher than 10%, as discussed at the bottom of page 4. -As suggested in the final sentence of the discussion, it would be nice if conclusions drawn from the learnability experiments done in this paper were applied to the design new networks which better generalize Summary: This paper presents very nice experiments comparing the complexity of various different neural networks using the notion of "learnability"
This is unfortunate because I believe this method, which takes as input a large complex network and compresses it so the loss in accuracy is small, would be really appealing to companies who are resource constrained but want to use neural network models. - Your compression ratio is much higher for MNIST but your accuracy loss is somewhat dramatic, especially for MNIST (an increase of 0.53 in error nearly doubles your error and makes the network worse than many other competing methods: http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html#4d4e495354).
* Three important results are stated as "theorem", with a statement like "Deep feedforward networks learn by breaking symmetries" proven in 5 lines, with no formal mathematics. In this paper, an number of very strong (even extraordinary) claims are made: * The abstract promises "a framework to understand the unprecedented performance and robustness of deep neural networks using field theory."
Quick summary: This paper proposes an energy based formulation to the BEGAN model and modifies it to include an image quality assessment based term. * results are only partially motivated and analyzed This paper proposed some new energy function in the BEGAN (boundary equilibrium GAN framework), including l_1 score, Gradient magnitude similarity score, and chrominance score, which are motivated and borrowed from the image quality assessment techniques. Based on its incremental nature and weak experiments, I'm on the margin with regards to its acceptance. As a result I feel that the title and the claims in the paper are somewhat misleading and premature: that the proposed techniques improves the training and evaluation of energy based gans. Summary: The paper extends the the recently proposed Boundary Equilibrium Generative Adversarial Networks (BEGANs), with the hope of generating images which are more realistic.
This paper presents a set of regularizers which aims for manipulating the statistical properties like sparsity, variance and covariance. The paper falls short in explaining how DNNs and Shannons channel coding theory fit together theoretically and how they used it to derive the proposed regularizers. The paper does a good job of setting up and comparing empirical performance of various regularizers (penalties on weights and penalties on hidden unit representations) and compares results against a baseline.
The authors propose reducing the number of parameters learned by a deep network by setting up sparse connection weights in classification layers. Detailed comments and questions: The distribution of connections in "windows" are first described to correspond to a sort of semi-random spatial downsampling, to get different views distributed over the full image. Minor Second line of Section 2.1: "lesser" -> less or fewer This paper examines sparse connection patterns in upper layers of convolutional image classification networks. It was not clear to me why scatter (the way it is defined in the paper) would be a useful performance proxy anywhere but the first classification layer. But in the upper layers, the spatial extent can be very small compared to the image size, sometimes even 1x1 depending on the network downsampling structure.
The authors present a solid overview of unsupervised metrics for NLG, and perform a correlation analysis between these metrics and human evaluation scores on two task-oriented dialog generation datasets using three LSTM-based models. The authors here show that the performance of various NN models as measured by automatic metrics like BLEU and METEOR is correlated with human eval. This paper can be considered as an extension of Liu, et al, 2016 while the later one did an empirical study in non-task-oriented dialogue generation. However, there isn't enough novel contribution in this paper to warrant a publication.
Contributions: While there are architectural changes (e.g. the customer agent and client agent have different roles and parameters; the parameters of both agents are updated via self-play training), the information isolation claim is not clear.
The paper proposes a method for few-shot learning using a new image representation called visual concept embedding. - The paper uses a hard thresholding  in the visual concept embedding. The used Visual Concepts (VCs) were already introduced by other works (Wangt'15), and is not a novelty. Therefore, I rate the current manuscript as a reject. My main concern for this paper is that the description of the Visual Concepts is completely unclear for me.
either in terms of FLOPS or measured times This paper applies gated convolutional neural networks [1] to speech recognition, using the training criterion ASG [2]. Arguments in section 2.3 are weak because, again, all other grapheme-based end-to-end systems have the same benefit as CTC and ASG. The authors argue that ASG is better than CTC in section 2.3.1 because it does not use the blank symbol and can be faster during decoding. However, all of the other grapheme-based end-to-end systems enjoy the same benefit as CTC and ASG. There is no reason to believe that ASG can be faster than CTC in both training and decoding. This paper starts by bashing the complexity of conventional HMM systems, and states the benefits of their approach. That limits the value. If rejected from here, it could perhaps be submitted as an ICASSP or Interspeech paper.
Overall, the idea is interesting, as providing an end-to-end trainable technique for distributing the precision across layers of a network would indeed be quite useful. While the idea makes sense, the paper is not well executed, and I cannot understanding how gradient descend is performed based on the description of Section 4. But it's not at all clear that the gradient of either the loss or the quantization error w.r.t. the number of bits will in general suggest increasing the number of bit (thus requiring the bit regularization term).
However, the paper then goes on to treat data augmentation as distinct from other explicit regularisation techniques, so I guess this is not the intended meaning. The paper could potentially be made more interesting or solid if some of the followings could be investigated: - considering a wider range of different problems apart from image classification, and investigate the effectiveness of domain specific data augmentation and general data augmentation
This paper presents a model for visual question answering that can learn both parameters and structure predictors for a modular neural network, without supervised structures or assistance from a syntactic parser. We know that more traditional semantic parsing approaches with real logical forms are capable of getting excellent accuracy on structured QA tasks with a lot more complexity and less data than this one. dataset. If the current paper really wants to make denotational semantics part of the core claim, I think it would help to talk about the representational implications in more detail---what kinds of things can and can't you model once you've committed to set-like bottlenecks between modules? At the same time, one of these things that's really nice about the structure-selection part of this model is that it doesn't care what kind of messages the modules send to each other!
In overall, it is a nice idea to use DNN to represent all update operators in MCTS. The presentation is very clear, the design of the architecture is beautiful, and I was especially impressed with the related work discussion that went back to identify other game search and RL work that attempts to learn parts of the search algorithm. If I understand them correctly, the comparison is between a neural network that has been learned on 250,000 trajectories of 60 steps each where each step is decided by a ground truth close-to-optimal algorithm, say MCTS with 1000 rollouts (is this mentioned in the paper). I suspect that generating the training data and learning the model takes an enormous amount of CPU time, while 25 MCTS rollouts can probably be done in a second or two. This is different from standard planning when a backup is handled with more simulations, the Q value function will have better statistics, and then get smaller regrets (see 4(b) in Algorithm 1). The paper is thorough and well-explained. + is R_1 similar to R^1 Would it be fair to have a baseline that learns the MCTS coefficient on the training data? - It looks like after training MCTSnet with a massive amount of data from another MCTS, MTCSnet algorithm as in Algorithm 2 will not do very much more planning yet. Or one that uses the value function that was learned with classic search? This paper proposes a framework for learning to search, MCTSNet. The paper proposes an idea to integrate simulation-based planning into a neural network.
- provides fairly extensive experimental comparison of their method and 3 others (Reluplex, Planet, MIP) on 2 existing benchmarks and a new synthetic one or something similar. In summary, I feel that while there are some issues with the paper, it presents interesting results and can be accepted.
Summary: This paper proposes an approach to learn embeddings for structured datasets i.e. datasets which have heterogeneous set of features, as opposed to just words or just pixels. Finally, the most striking flaw of this paper is the lack of references to previous works on word embeddings and feature representation, I would suggest the author check and compare themselves with previous work on this topic. Comments: The paper is well written and addresses an important problem of learning word embeddings when there is inherent structure in the feature space. The paper compares against a Word2vec baseline that pools all the heterogeneous content learns just one set of embeddings. The structured deep-in factorization machines allow higher-level interactions in embedding learning which allows the authors to learn embeddings for heterogeneous set of features. This paper provides a clean way of learning embeddings for structured features that can be discrete -- indicating presence / absence of a certain quality. The authors introduced a new compatibility function between features and, as in the skipgram approach, they propose a variation of negative sampling to deal with structured features. The introduction of structured deep-in factorization machine should be more clear with examples that give the intuition on the rationale of the model.
This paper proposes a fast way to learn convolutional features that later can be used with any classifier. The acceleration of the training comes from a reduced number of training epocs and a specific schedule decay of the learning rate. Pros: The paper compares different classifiers on three datasets. Two proposals in the paper are: (1) Using a learning rate decay scheme that is fixed relative to the number of epochs used in training, and Essentially, if I understand correctly, this paper is proposing to prematurely stop training an use the intermediate feature to train a conventional classifier (which is not that away from the softmax classifier that CNNs usually use).
The paper also presents some visualizations the similarity structure of the learned representations and proposes a window-based method for processing the data. Main issues: No related works (such as those using RNN for time series analysis or clustering of time series data streams etc.) were described by the paper, no baselines were used in the comparison evaluations, and no settings/details were provided in the experiment section. For example, the authors claim that the proposed LSTM-based autoencoder networks can be natively scaled up to data with very high dimensionality. The only conclusions I can draw from the visual analysis is that the context vectors are more similar to each other when they are obtained from time steps in the data stream which are close to each other. The authors show that for their application, better performance is obtained when the network is only trained to reconstruct a subset of the data measurements.
Pros: - interesting self-supervised framework provided for highlighting relevant substructures for a given prediction task It would be unconvincing that the proposed neural nets approach fits to this hard combinatorial task rather than these existing (mostly exact) methods. Cons: - it would be a bit unconvincing that identifying 'hard selection' is better suited for neural nets, rather than many existing exact methods (without using neural networks). If the conv net performs the best when we use the entire structure, then learning might be forced to ignore the selection. Overall the method is interesting and has a clear impact for molecular prediction, however the paper has limited appeal to the broader audience. The model is experimented on two small datasets of few thousand of molecules, and compared to a state-of-the-art DeepTox, and also to some basic baselines (RF/SVM/logreg).
The authors propose a DNN, called subspace network, for nonlinear multi-task censored regression problem. They compare the proposed SN with other traditional approaches on a very small data  set with 670 samples and 138 features. The topic is important. Experiments on real data show improvements compared to several traditional approaches. Conclusion: Though with a quite novel idea on solving multi-task censored regression problem, the experiments conducted on synthetic data and real data are not convincing enough to ensure the contribution of the Subspace Network. This work proposes a multi task learning framework for the modeling of clinical data in neurodegenerative diseases. Differently from previous applications of machine learning in neurodegeneration modeling, the proposed approach models the clinical data accounting for the bounded nature of cognitive tests scores. 8. The performance on One-Layer Subspace Network (with only the input features) could be added. The authors are generating the synthetic data according to the model, and it is thus not surprising that they managed to obtain the best performance. The proposed algorithm is also generally compared with respect to linear methods, and the authors could have provided a more rigorous benchmark including standard non-linear prediction approaches (e.g. random forests, NN, GP, …).
They propose directly optimizing the time-dependent discrimination index using a siamese survival network. That being said, this particular use of deep learning in this context might be novel. While the idea of optimizing directly for the c-index directly is a good one (with an approximation and with useful complementary loss function terms), the paper leaves something to be desired in quality and clarity. It is unclear why the authors solution is able to solve such an issue, specially given the modest reported gains in comparison with several competitive baselines. - One of the main motivations of the authors is to propose a model that is specially design to avoid the nonidentifiability issue in an scenario with competing risks.
All in all this paper reads like a tech report but not a conference publication. The paper presents an approach to do task aware distillation, task-specific pruning and specialized cascades. Would it be the gain in speed much lower?
The fact that these different inverse maps arise under these conditions is interesting --- and Figure 5 is quite convincing in showing how each expert generalizes. Overall, it is nice to see the different inverse maps arise naturally in this setting. 2) The authors only run experiments on the MNIST data, where 1) the mechanisms are simulated and relatively simple, and 2) samples from the canonical distribution are also available. How D will handle an example far from fake or real ones ?
To me this greatly mutes the value of this result, and the contribution of the paper overall, because local minimum are *very* likely to occur on the boundary between activation regions at non-differentiable points (e.g. as in Figure 2). For example, if there is a differentiable valley in L_f that terminates on the boundary of an activation region, then this phenomena could occur, since a local-minima-creating boundary in L_f might just lead to a saddle point in L_gA. The main result is that every local minimum of the total surface is a global minimum of the region where the ReLU activations corresponding to each sample do not change.
On both of the synthetic tasks --- which involve predicting gaussians --- the proposed approach can fit the data reasonably using far fewer parameters than the baselines, although 3BE does achieve better overall performance. On a real world task that involves predicting a distribution of future stock market prices from multiple input stock marked distributions, the proposed approach significantly outperforms both baselines. By letting the conditional distribution between nodes be unnormalized, and using an energy function that incorporates child nodes independently, the approach admits efficient computation that does not need to model the interaction between the distributions output by nodes at a given level. It's not clear how much value there is adding yet another distribution to distribution regression approach, this time with neural networks, without some pretty strong motivation (which seems to be lacking), as well as experiments.
A few contributions are claimed in this paper: (1) differentiable decision tree which allows for gradient-based optimization; (2) supervised VAE where class-specific Gaussian prior is used for the probabilistic decoder in the VAE; (3) combination of these two models. Can we also interpret a VAE given labels by varying each dimension of the latent variables without jointly training a DTT? Lastly, I do not think the interpretability provided by the decision tree is as great as the authors seem to claim.
The authors propose a particular variance regularizer on activations and connect it to the conditional entropy of the activation given the class label. The data is then partitioned within a batch based on this Z value, and monte carlo sampling is used to estimate the variance of Y conditioned on Z, but it's really unclear as to how this behaves as a regularizer, how the z is sampled for each monte carlo run, and how this influences the gradient. It would have been nice if the authors more directly compared SHADE to BN.
The primary issues I have with this work are threefold:  (i) The paper is not suitably organized/condensed for an *CONF* submission, (ii) the presentation quality is quite low, to the extent that clarity and proper understanding are jeopardized, and (iii) the novelty is limited. The appropriateness of using additional pages over the recommended length will be judged by reviewers."  In the present submission, the first 8+ pages contain minimal new material, just various background topics and modified VAE update rules to account for learning noise parameters via basic EM algorithm techniques. In contrast, this submission suggests either treating sigma^2 as a trainable parameter, or else introducing a more flexible zero-mean mixture-of-Gaussians (MoG) model for the decoder noise.
b) In Section 2 the authors state "Imposing extra data hypothesis actually violates the ME principle and degrades the model to non-ME model." … Statements like this need to be made much clearer, since imposing feature expectation constraints (such as Eq. (3) in Berger et al. 1996) is a perfectly legitimate construct in ME principle.
This work proposed an interesting graph generator using a variational autoencoder. - some of the main issues with graph generation are acknowledged (e.g. the problem of invariance to node permutation) and a solution is proposed (the binary assignment  matrix) However, there are some significant weaknesses. The main challenges of generating graphs as opposed to text or images are said to be the following: (a) Graphs are discrete structures, and incrementally constructing them would lead to non-differentiability (I don't agree with this; see below) Overall, the paper is about an interesting subject, but in my opinion the execution isn't strong enough to warrant publication at this point. - the evaluation should include a measure of the capacity of the architecture to : a) reconstruct perfectly the input b) denoise perturbations over node labels and additional/missing  edges This paper studies the problem of learning to generate graphs using deep learning methods. A downside to the algorithm is that it has complexity O(k^4) for graphs with k nodes, but the authors argue that this is not a problem when generating small graphs. The main issue with training models in this formulation is the alignment of the generated graph to the ground truth graph. I would have at least liked to see a comparison to a method that generated SMILES format in an autoregressive manner (similar to previous work on chemical graph generation), and would ideally have liked to see an attempt at solving the alignment problem within an autoregressive formulation (e.g., by greedily constructing the alignment as the graph was generated). - notions for measuring the quality of the output graphs are of interest: here the authors propose some ways to use domain knowledge to check simple properties of molecular graphs Strengths: - Generating graphs is an interesting problem, and the proposed approach seems like an easy-to-implement, mostly reasonable way of approaching the problem. Pros: - the formulation of the problem as the modeling of a probabilistic graph is of interest To handle this, the paper proposes to use a simple graph  matching algorithm (Max Pooling Matching) to align nodes and edges. First, the motivation for one-shot graph construction is not very strong: - I don't understand why the non-differentiability argued in (a) above is an issue. Even after doing so, the authors need to solve a matching problem to resolve the alignment issue. Based on this motivation, the paper decides to generate a graph in "one shot", directly  outputting node and edge existence probabilities, and node attribute vectors.
Summary of the paper: This paper presents a method, called \\alpha-DM (the authors used this name because they are using \\alpha-Divergence to measure the distance between two distributions), that addresses three important problems simultaneously: (a) Objective score discrepancy: i.e., in ML we minimize a cost function but we measure performance using something else, e.g., minimizing cross entropy and then measuring performance using BLEU score in Machine Translation (MT). My comments / feedback: The paper is well written and the problem addressed by the paper is an important one. The model is trained on an empirical distribution whose points are sampled from the true distribution. Furthermore, in the same Section 2 the paper fails to mention that reinforcement learning training also does not completely correspond to the evaluation approach, at which stage greedy search or beam search is used. (b) Sampling distribution discrepancy: The model is trained using samples from true distribution but evaluated using samples from the learned distribution 1. The idea is a good one and is great incremental research building on the top of previous ideas. The objective is based on alpha-divergence between the true input-output distribution q and the model distribution p. The new objective generalizes  Reward-Augmented Maximum Likelihood (RAML) and entropy-regularized Reinforcement Learning (RL), to which it presumably degenerates when alpha goes to 1 or to 0 respectively. Then the authors present the results for machine translation task and also analysis of their proposed method. Crucially, there is no comparison to a trivial linear combination of ML and RL, which in one way or another was used in almost all prior work, including GNMT, Bahdanau et al, Ranzato et al. The paper does not argue why alpha divergence is better that the aforementioned combination method and also does not include it in the comparison. First, the model is *not* trained on the true distribution which is unknown. 2. In page 2, the line before the last line, "… resolbing problem" --> "… resolving problem" This paper considers a dichitomy between ML and RL based methods for sequence generation.
(2) simple MLP/CNN models to show the cross-layer relationships (e.g. sudden increase and decrease of the number of channels across layers will be penalized by c^l_{f^{l+1}, t}), etc. The authors then introduce a greedy algorithm that expands the different layers in a neural network until the metric indicates that additional features will end up not being used effectively.
5.3: so that results more comparable In this paper the authors studied the problem of off-policy learning, in the bandit setting when a batch log of data generated by the baseline policy is given. [Minor] The derived bounds depend on M, an a priori upper bound on the Renyi divergence between the logging policy and any new policy. The paper proposes an interesting alternative to recent approaches to learning from logged bandit feedback, and validates their contribution in a reasonable experimental comparison.
The improved upper bound given in Theorem 1 appeared in SampTA 2017 - Mathematics of deep learning ``Notes on the number of linear regions of deep neural networks'' by Montufar. [2] On the expressive power of deep neural networks, 2017, Raghu, Poole, Kleinberg, Ganguli, Sohl-Dickstein This is quite an interesting paper. (The improvement on Zaslavsky's theorem is interesting.) The idea of counting the number of regions exactly by solving a linear program is interesting, but is not going to scale well, and as a result the experiments are on extremely small networks (width 8), which only achieve 90% accuracy on MNIST. Overall, while the paper is well written and makes some interesting points, it presently isn't a significant enough contribution to warrant acceptance. The paper also discusses the exact computation of the number of linear regions in small trained networks.
A composite of transformations coupled with the LAM/RAM networks provides a highly expressive model for modelling arbitrary joint densities but retaining interpretable conditional structure. Specifically, the authors examine models involving linear maps from past states (LAM) and recurrence relationships (RAM). Given that, the most important part of the paper would be to demonstrate how it performs compared to Masked Autoregressive Flows.
Once we train and achieve a network with best performance under this constraint, we take the sign of each weight (and leave them intact), and use the remaining n-1 bits of each weight in order to add some new connections to the network. In my opinion, the authors should explain how to apply their algorithm to more general network architectures, and test it, in particular to convnets.
This paper addresses multiple issues arising from the fact that commonly reported best model performance numbers are a single sample from a performance distribution. The method of choosing the best model under 'internal' cross-validation to take through to 'external' cross-validation against a second hold-out set should be regarded as one possible stochastic solution to the optimisation problem of hyper-parameter selection.
The key contribution of the paper is facilitating optimization of these models by gradient based methods, which eventually leads to improved accuracy on relevant benchmark data (on par or beyond SOTA). That is, transforming the original model constructed from indicator functions (hence difficult to optimize by gradient based method) to a smooth differentiable function by diffusing the landscape. This paper proposes a soft relaxation of the box lattice (BL) model of Vilnis et al. 2018 and applies it to several graph prediction tasks. - The main thrust of section 5.2 is that smoothed box embeddings retain better performance with increasing numbers of negatives. (Why would we expect the smoothed box model to handle unseen captions better?) ------ The paper presents smoothing probabilistic box embeddings with softplus functions, which make the optimization landscape continuous, while also presenting the theoretical background of the proposed method well. As the authors find, the smoothed function leads to improved performance against SOTA on relevant benchmark data such as WordNet hypernymy, Flick caption entailment and MovieLnes market basket data. This is a great paper and should be accepted.
To explain the difficulty of training pruned networks from scratch or why training needs the overparameterized networks that make pruning necessary,  the authors propose a lottery ticket hypothesis: unpruned, randomly initialized NNs contain subnetworks that can be trained from scratch with similar generalization accuracy. The paper follows by proposing a method to find these winning tickets by pruning methods, which are typically used for compressing networks, and then proceed to test this hypothesis on several architectures and tasks. (Score raised from 8 to 9 after rebuttal) Given our lack of understanding of the optimization and generalization properties of neural networks, as well as how these two interact, then any insight into this process, like this paper suggests, could have a significant impact on both theory and practice. The author defines a winning lottery ticket as a sparse subnetwork that can reaching the same performance of the original network when trained from scratch with the "original initialization". Most importantly, the hypothesis and experiments presented in this paper gave me a new perspective on both the generalization and optimization problem, which as a theoretician gave me new ideas on how to approach analyzing them rigorously — and that is why I strongly vote for the acceptance of this paper.
This work introduces SNIP, a simple way to prune neural network weights before training according to a specific criterion. The experiments done are also extensive, as they cover a broad range of tasks: MNIST / CIFAR 10 classification with various architectures, ablation studies on the effects of different initialisations, visualisations of the pruning patterns and exploration of regularisation effects on a task involving fitting random labels. The fact that it works is very surprising and again suggests that the method identifies constant background pixels rather than important weights. This is important - on MNIST each digit has a constant zero border, all connections to the border are not needed and can be trivially removed (one can crop the images to remove them for similar results). - It is correct that the weights used to train the pruned model are possibly different from the ones used to compute the connection sensitivity. - SNIP seems to be a good candidate for applying it to randomly initialised networks; nevertheless, a lot of times we are also interested in pruning pre-trained networks. [2] A Simple Procedure for Pruning Back-Propagation Trained Neural Networks. Furthermore, the authors should also refer to [5] as they originally did the same experiment and showed that they can obtain the same behaviour without any hyper parameters. [3] Learning Sparse Neural Networks through L_0 Regularization. [4] Generalized Dropout. [5] Variational Dropout Sparsifies Deep Neural Networks. [1] Skeletonization: A Technique for Trimming the Fat from a Network via Relevance Assessment. [4] Generalized Dropout. [5] Variational Dropout Sparsifies Deep Neural Networks.
Based on the insight from the analysis in the supplementary materials, the authors propose a two-stage VAE which separate learning the a parsimonious representation of the low-dimensional (lower than the ambient dimension of the input space), and the training a second VAE to learn the unknown approximate posterior. Also, I think *the reported FID scores alone may be considered as a significant enough contribution*, because to my knowledge this is the first paper significantly closing the gap between generative quality of GAN-based models and non-adversarial AE-based methods. It has clarified a few key issues, and convinced me of the value to the community for publication in its present (slightly edited according to the reviwers' feedback) form.
As a result, I believe the article might be of interest to practitioners interested in solving related cross-modal matching tasks. * The image of the voice waveform in Figures 1 and 2 should be replaced by log Mel-spectrograms in order to illustrate the network's input. The key innovation of the article, compared to the aforementioned papers, lies on the idea of learning face/voice embeddings to maximise their ability to predict covariates, rather than by explicitly trying to optimise an objective related to cross-modal matching. Compared with similar work from Nagrani et al (2018) who generate paired inputs of voices and faces and train a network to classify if the pair is matched or not, the proposed method doesn't require paired inputs. Instead, the resulting embeddings are fed to a modality-agnostic, multiclass logistic regression classifier that aims to predict simple covariates such as gender, nationality or identity.
This paper discusses the effect of weight decay on the training of deep network models with and without batch normalization and when using first/second order optimization methods. It would have been interesting to carefully study the effect of weight decay on the gamma parameter of batch-norm which controls the complexity of the network along with the softmax layer weights as it was left for future work in van Laarhoven 2017.
The main contribution is the model the problem as a time-dependent context and then use a directed information flow loss instead of the mutual information loss. Authors present results both on continuous and discrete environments. One of the main difference of this work in comparison to unsupervised segmentation models GMM or BP-AR-HMM is the fact that the options learned are composable. 4) Have you tried training your model on the pixels on the continuous control tasks? The proposed approach uses a pre-training step, based on a variational auto-encoder (VAE), to estimate latent variable sequences. A reasonable confirmation that the model indeed learns composition is to generate a trajectory for a sequence of latent code not seen in data.
In terms of presentation quality, the paper is clearly written, the proposed methods are well explained, and the notation is consistent. The main advantage of the proposed method as illustrated in particular by the experimental results in the citation network domain is its ability to generalize well in the presence of a small  amount of training data, which the authors attribute to its efficient capturing of both short- and long-range interactions. The proposed method is novel and achieves good results on a set of experiments. Overall, the idea of using Lanczos algorithm to bypass the computation of the eigendecomposition, and thus simplify filtering operations in graph signal processing is not new [e.g., 35]. This paper proposes to use a Lanczos alogrithm, to get approximate decompositions of the graph Laplacian, which would facilitate the computation and learning of spectral features in graph convnets.
This paper proposes an approach for automatic robot design based on Neural graph evolution. [Strengths]: This paper shows some promise when graph network-based controllers augmented with evolutionary algorithms. Paper is quite easy to follow. The results in this paper are impressive, and the paper seems free of technical errors. The overall approach has a flavor of genetical algorithms, as it also performs evolutionary operations on the graph, but it also allows for a better mechanism for policy sharing across the different topologies, which is nice. Detailed comments: - in the abstract you say that "NGE is the first algorithm that can automatically discover complex robotic graph structures". The authors propose a scheme based on a graph representation of the robot structure, and a graph-neural-network as controllers. using ES. Given that, the novelty of the paper is fairly incremental as it uses NerveNet to evaluate fitness and ES for the main design search. The cost of evaluating the function is typically more pressing, and as a result it is important to have algorithms that can converge within a small number of iterations/generations. - Sec 4.1:  would argue that computational cost is rarely a concern among evolutionary algorithms. Please include further description of the ES cost function and algorithm in the main body of the paper. How would the given graph network compare to this? - in the introduction you mention that automatic robot design had limited success. This paper uses graph network to train each morphology using RL. This expedites the score function evaluation improving the time complexity of the evolutionary process. [Summary]: This paper tackles the problem of automatic robot design. This baseline can be thought of a shared parameter graph with no message passing.
The authors use their derived formula for VRR to predict the minimum mantissa precision needed for accumulators for three well known networks: AlexNet, ResNet 32 and ResNet 18. The analysis is based on Variance Retention Ratio (VRR), and authors show the theoretical impact of reducing the number of bits in the floating point accumulator. For tightness analysis they present convergence results while perturbing the mantissa bits to less than those predicted by their formula, and show that it leads to more than 0.5% loss in the final test error of the network. The authors address this with  an analytical method to predict the number of mantissa bits needed for partial summations during the forward, delta and gradient computation ops for convolutional and fully connected layers. The authors conduct a thorough analysis of the numeric precision required for the accumulation operations in neural network training.
Authors propose to augment a conditional GAN model with an unsupervised branch for spanning target manifold and show better performance than the conditional GAN in natural scene generation and face generation. The authors claim the generator of RoC-Gan will span the target manifold, even in the presence of large amounts of noise. Conclusion: The author(s) are thoughtful and they put lots of work on this paper. The methodological novelties seem more-or-less limited, but the theoretical analysis and the intuitive (and well-motivated) modification over CGANs add merits to the paper. 1.Similar idea of using an autoencoder as another branch to help image generation has been proposed in Ma et al.'s work. ------------------------------- After Rebuttal --------------------------------- I am very satisfied with the authors' response, so I will change my vote from rejection to acceptance. I will change decision from rejection to acceptance.
## Strengths - The paper is theoretically sound, the statement of the theorems are clear and the authors seem knowledgeable when bounding the generalization error via Rademacher complexity estimation. Then, {\\bf Theorem 1} provides an upper bound for the empirical Rademacher complexity of the class of 1-layer networks with hidden units of bounded \\textit{capacity} and \\textit{impact}. Finally {\\bf Theorem 3} is presented, which provides a lower bound for the Rademacher complexity of a class of neural networks, and such bound is compared with existing lower bounds. Next, {\\bf Theorem 2} which is the main result, presents a new upper bound for the generalization error of 1-layer networks. An empirical comparison with existing generalization bounds is made and the presented bound is the only one that in practice decreases when the number of hidden units grows. The main result is a bound on the empirical Rademacher complexity of F_{alpha,beta}.
The paper also shows the empirical behavior of the gradient coherence statistic during model training; I want to see the theoretical analysis of the relation between model complexity and staleness.
Based on looking at past *CONF* proceedings, this paper's topic and collection of techniques is not in the *CONF* mainstream (though it's not totally unrelated). I can accept that the model will be rather insensitive to hyper-parameters alpha and beta, but I've serious doubt about the number of clusters, especially as the evaluation is done here in the best possible setting.
Unlike Arora's original work, the assumptions they make on their subject material are not supported enough, as in their lack of explanation of why linear addition of two word embeddings should be a bad idea for composition of the embedding vectors of two syntactically related words, and why the corrective term produced by their method makes this a good idea. - the tensor model does deliver some improvement over linear composition on noun-adjective pairs when measured against human judgement The authors claim that the Arora's RAND-WALK model does not capture any syntactic information. Given that the main attraction of the paper is the potential for more performant word embeddings, I do not believe the work will have wide appeal to *CONF* attendees, because no evidence is provided that the features from the learned tensor, say [a, b, T*a*b], are more useful in downstream applications than [a,b] (one experiment in sentiment analysis is tried in the supplementary material with no compelling difference shown). The authors point out that others have observed that this form of compositionality does not leverage any information on the syntax of the pair (a,b), and the propose using a tensor contraction to model an additional multiplicative interaction between a and b, so they propose finding the word whose embedding is closest to a + b + T*a*b, where T is a tensor, and T*a*b denotes the vector obtained by contracting a and b with T. Some additional citations: - the above-mentioned *CONF* paper provides a performant alternative to unweighted linear composition
However, I have concerns about the presented data and the validity of rotation equivariance in modeling visual responses in general (below). Update after revisions: The authors performed extensive work to address my concerns.
I think that this paper makes a good contribution by establishing a benchmark and providing some preliminary results. There are challenging desiderata involved in building the training+tests sets, and the authors have an interesting and involved methodology to accomplish these. Strengths: I am happy to see the proposal of a very large dataset with a lot of different axes for measuring and examining the performance of models. Weaknesses: The dataset created here is entirely synthetic, and the paper only includes one single small real-world case; it seems like it would be easy to generate a larger and more varied real world dataset as well (possibly from the large literature of extant solved problems in workbooks). It would have been useful to compare the general models here with some specific math problem-focused ones as well. The results and discussions in the main part of the paper are too light in my opinion; the average model accuracy across modules is not an interesting metric at all, although it does show that the Transformer performs better than recurrent networks. I'd like to see more of a discussion of *prior data sets* rather than papers proposing models for problems. Whether to accept it or not depends on what standard *CONF* has towards such papers (ones that do not propose a new model/new theory). The authors try to do this with composition, which is good, but I am not sure whether that captures the real important thing - the ability to generalize, say learning to factorise single-variable polynomials and test it on factorising polynomials with multiple variables? The paper is relatively well-written, although the description of the neural models can be improved. The dataset is then used to evaluate a number of recurrent models (LSTM, LSTM+attention, transformer); these are very powerful models for general sequence-sequence tasks, but they are not explicitly tailored to math problems. In fact, assuming that such methods outperform general-purpose models, we could investigate why and where this is the case (in fact the proposed dataset is very useful for this). One suggestion is that it might be useful to also release the structured (parsed) form besides the freeform inputs and outputs, for analysis and for evaluating structured neural network models like the graph networks.
This paper is positioned in the context of Bayesian GANs (Saatsci & Wilson 2017) which, by placing a posterior distribution over the generative and discriminative parameters, can potentially learn all the modes. randomness is introduced by the distributions over the generator and discriminator parameters. Note the MGAN paper reports results on STL-10 and ImageNet as well.
For instance, the experiments seem to indicate that generalizing density estimation from CIFAR training set to CIFAR test set is likely challenging and thus the models underfit the true data distribution, resulting in the simpler dataset (SVHN) having higher likelihood. Minor nitpick: There seems to be some space crunching going on via Latex margin and spacing hacks that the authors should ideally avoid :) This paper displays an occurrence of density models assigning higher likelihood to out-of-distribution inputs compared to the training distribution. - Lack of an extensive exploration of datasets Pros: - The finding that SVHN has larger likelihood than CIFAR according to networks is interesting. The section concludes that if the second dataset has small variances, it will get higher likelihood. Questions for the authors: 1. (Also AREA CHAIR NOTE): Another parallel submission to *CONF* titled "Generative Ensembles for Robust Anomaly Detection" makes similar observations and seemed to suggest that ensembling can help counter the observed CIFAR/SVHN phenomena unlike what we see in Figure 10.
The main contribution is the use of a IHT-based strategy to update the coefficients, with a gradient-based update for the dictionary (NOODL algorithm). I think the paper is relevant and proposes an interesting contribution. The paper shows that there is an alternating optimization-based algorithm for this problem that under standard assumptions provably converges exactly to the true dictionary and the true coefficients x (up to some negligible bias). COLT 2015. The main contributions of this work are essentially on the theoretical aspects. Also, it is clear in the experiments the superiority with respect to Arora in terms of iterations (and error), but what about computational time?
The paper analyzes the loss landscape of a class of deep neural networks with skip connections added to the output layer. This paper presents a class of neural networks that does not have bad local valleys. This paper shows that a class of deep neural networks have no spurious local valleys –--implying no strict local-minima. Page 8 says "This effect can be directly related to our result of Theorem 3.3 that the loss landscape of skip-networks has no bad local valley and thus it is not difficult to reach a solution with zero training error". From what I read, the definition of "bad local valley" is implied by the abstract and in the proof of Theorem 3.3(2), but I did not find a formal definition anywhere else. The good property of loss surface for networks with skip connections is impressive and the authors present interesting experimental results pointing out that
Original review: Summary: they propose a differentiable learning algorithm that can output a brush stroke that can approximate a pixel image input, such as MNIST or Omniglot. I think the value in this method is that it can be converted to a full generative model with latent variables (like a VAE, GAN, sketch-rnn) where you can feed in a random vector (gaussian or uniform), and get a sketch as an output, and do things like interpolate between two sketches. As mentioned in my original review, the main advantage of this is the ability to train with very limited compute resources, due to the model-based learning inspired by model-based RL work (they cited some work on world models). From reading the paper, my guess is the authors came from a background that is not pure ML research (for instance, they are experts in Javascript, WebGL, and their writing style is easy to read), and it's great to see new ideas into our field. While research from big labs [1] have the advantage of having access to massive compute so that they can run large scale RL experiments to train an agent to "sketch" something that looks like MNIST or Omniglot, the authors probably had limited resources, and had to be more creative to come up with a solution to do the same thing that trains in a couple of hours using a single P40 GPU. If things don't work out this time, I recommend the authors asking some friends who have published (successfully) at good ML conferences to proof read this paper for content and style. 2) While I like this method and approach, to play devil's advocate, what if I simply use an off the shelf bmp-to-svg converter that is fast and efficient (like [6]), and just build a set of stroke data from a dataset of pixel data, and train a sketch-rnn type model described in [3] to convert from pixel to stroke? The idea presented in the paper is however interesting and timely and deserves to be shared with the wider generative models community, which makes me lean towards an accept. That being said, things are not all rosy, and I feel there are things that need to be done for this work to be ready for publication in a good venue like *CONF*.
The paper demonstrates the use of this framework to assess the role of compositionality in a hypothetical compression phase of representation learning, compares the correspondence of TRE with human judgments of compositionality of bigrams, provides an explanation of the relationship of the metric to topographic similarity, and uses the framework to draw conclusions about the role of compositionality in model generalization. The reported experiments cover reasonable ground in terms of questions relevant to compositionality (relationship to representation compression, generalization), and I appreciate the comparison to human judgments, which lends credibility to applicability of the framework. The authors find that this measure correlates with the mutual information between the input x and z, approximates the human judges of compositionality on a language dataset and finally presents a study on the relation between the proposed measure and generalizalization performance, concluding that their measure correlates with generalization error as well as absolute test accuracy. I think this is a reasonable paper to accept for publication. -- how do we know if the learned representations capture the compositional structure present in the inputs, and tries to come up with a systematic framework to answer that question. Given instances of data x annotated with semantic primitives, the authors learn a vector for each of the primitive such that the addition of the vectors of the primitives is very close (in terms of cosine) to the latent representation  z of the input x. Reviewer 2's comments also remind me that, from a perspective of learning composition-ready primitives, Fyshe et al. (2015) is a relevant reference here, as it similarly learns primitive (word) representations to be compatible with a chosen composition function.
The paper proposes a new way to construct adversarial examples: do not change the intensity of the input image directly, but deform the image plane (i.e. compose the image with Id + tau where tau is a small amplitude vector field). The paper introduces an iterative method to generate deformed images for adversarial attack. - The interpolation scheme (how is defined the intensity I(x,y) for a non-integer location (x,y) within the image I) is rather important (linear interpolation, etc.) and should be at least mentioned in the main paper, and at best studied (it might impact the gradient descent path and the results); The idea is quite simple, generate small displacement and resample (interpolate) image until the label flips. Pros: - The way of constructing deformation adversarial is interesting and novel The authors briefly discussed this point in the experiment section and provided a few numerical results in Table 2. - for instance, a study of the impact of the regularization would have been interesting (how does the sigma of the Gaussian smoothing affect the type of adversarial attacks obtained and their performance -- is it possible to fool the network with [very] smooth deformations?); These results, as acknowledged by the authors, do not well support the effectiveness of deformation adversarial attack and defense. However, the intuition behind the proposal does not make strong sense to the reviewer: since the main focus of this work is on model attack, why not directly (iteratively or not) adding random image deformations to fool the system? Minor: - The paper is a bit nationally convoluted for no good reason, the general idea is straightforward. - Numerical study shows some promise in adversarial attack, but is not supportive to the related defense capability. More clearly: the space of small deformations tau comes with an inner product (here L2, but one could choose another one), and the gradient \\nabla g obtained depends on this inner product choice M, even though the derivative Dg is the same (they are related by Dg(tau) = < \\nabla_M g | tau >_M for any tau). The idea of gradually adding deformations based on gradient information is somewhat interesting, and novel as far as the reviewer knows about.
Clarity: The paper is clearly written in the sense that the motivation of research is clear, the derivation of the proposed method is easy to understand. The problem author focused on is unique and the solution is simple, experiments show that proposed method seems promising. The whole paper is written in a clean way and the method is effective. Significance: I think this kind of research makes the variational inference more useful, so this work is significant. But I cannot tell the proposed method is really useful, so I gave this score. Question and minor comments: In the original paper of STL, the author pointed out that by freezing the gradient of variational parameters to drop the score function term, we can utilize the flexible variational families like the mixture of Gaussians. I checked all the derivations, and they seem to be correct. Overall, I think the idea is nice and the results are encouraging. About the motivation of the paper, I think it might be better to move the Fig.1 about the Bias to the introduction and clearly state that the author found that the STL is biased "experimentally".
The paper proposes a novel reward function - "partial zero sum", which only encourages the tracker-target competition when they are close and penalizes whey they are too far. This work aims to address the visual active tracking problem in which the tracker is automatically adjusted to follow the target. This paper presents a simple multi-agent Deep RL task where a moving tracker tries to follow a moving target. The reward function is not completely zero-sum, as the tracked agent's reward vanishes when it gets too far from a reference point in the maze. Both agents are standard convnet + LSTM neural architectures trained using A3C and are evaluated in 2D and 3D environments. A training mechanism in which tracker and the target serve as mutual opponents is derived to learning the active tracker. The work is very incremental over Luo et al (2018) "End-to-end Active Object Tracking and Its Real-world Deployment via Reinforcement Learning", as the only two additions are extra observations o_t^{alpha} for the target, and a reward function that has a fudge factor when the target gets too far away. Originality: Most of the components are pretty standard, however I value the part that seems pretty novel to me - which is the "partial zero-sum" idea. Further multi-agent tasks could also have been considered, such as capture the flag tasks as in "Human-level performance in first-person multiplayer games with population-based deep reinforcement learning".
The authors perform a careful study of mixed integer linear programming approaches for verifying robustness of neural networks to adversarial perturbations. - Results: the efficiency of the MIP on the tightened model, and the improvements in the bounds on the adversarial error as compared to very recent methods from the literature are both very strong points in favor of the paper.
This paper proposes a novel method of solving ill-posed inverse problems and specifically focuses on geophysical imaging and remote sensing where high-res samples are rare and expensive. At the heart of this paper is the idea that for an L-Lipschitz function f : R^k → R the sample complexity is O(L^k), so the authors want to use the random projections to essentially reduce L. Pros: - The proposed approach is interesting and novel - I've not previously seen the idea of predicting different picewise constant projections instead of directly predicting the desired output (although using random projections has been explored) To  alleviate these problems, this paper proposes a novel idea: instead of fully reconstructing in the original space, the authors create reconstructions in projected spaces. More specifically, for each projection the authors start with a random set of points in the image domain and compute a Delaunay triangulation.
The method of the authors assumes that a goal-conditioned policy is already learned, and they use a Kullback-Leibler-based distance between policies conditioned by these two states as the loss that the representation learning algorithm should minimize. The paper presents a method to learn representations where proximity in euclidean distance represents states that are achieved by similar policies. A positive side of the experimental study is that the 6 simulated environments are well-chosen, as they illustrate various aspects of what it means to learn an adequate representation. But learning the goal-conditioned policy from the raw input representation in the first place might be the most difficult task. The first weakness of the approach is that it assumes that a learned goal-conditioned policy is already available, and that the representation extracted from it can only be useful for learning "downstream tasks" in a second step. This is partly suggested when the authors mention that the representation could be learned from only a partial goal-conditioned policy, but this idea definitely needs to be investigated further. But when looking at the framework, this is close to what the authors do in practice: they use a distance between two *goal*-conditioned policies, not *state*-conditioned policies. In that respect, wouldn't it be possible to *simultaneously* learn a goal-conditioned policy and the representation it is based on? - Still in Section 6.4, the authors insist that ARC outperforms VIME, but from Fig.7, VIME is not among the best performing methods. - As the goal-conditional policy is quite similar to the original task of navigation, it is important to know for how long it was trained and taken into account. Major remarks: - The author state they add experimental details and videos via a link to a website. To me, the last paragraph of Section 4 should be a subsection 4.4 with a title such as "state abstraction (or clustering?) from actionable representation". The authors admit that having one is "a significant assumption" and state that they will discuss why it is reasonable assumption but I didn't find any such discussion  (only a sentence in 6.4). - The authors refer to Pathak et al. (2017), but not to the more recent Burda et al. (2018) (Large-scale study of curiosity-driven learning) which insists on the idea of inverse dynamical features which is exactly the approach the authors may want to contrast theirs with. The authors should describe the oracle in more details and discuss why it does not provide a "perfect" representation. Some more detailed points or questions about the experimental section: - not so important, Section 6.2 could be grouped with Section 6.1, or the various competing methods could be described directly in the sections where they are used.
The paper presents a coupled deep learning approach for generating realistic liquid simulation data that can be useful for real-time decision support applications. Given densely registered 4D implicit surfaces (volumes over time) for a structured scene, a neural-network based model is used to interpolate simulations for novel scene conditions (e.g. position and size of dropped water ball). 1. The primary novelty here is in the problem formulation (e.g., defining cost function etc.) where two networks are used, one for learning appropriate deformation parameters and the other to generate the actual liquid shapes. The results are impressive from the perspective of the current abilities of deep neural networks. The experimental results are sufficient for simulating liquids/smoke, except I would like to also see a comparison to using deformation field network only, without its predecessor. Right now the implicit surface deformation model is only tested on liquids examples, which limits the impact to that specialist domain -- it's a bit more of a SIGGRAPH type of paper than *CONF*. So, the authors need to provide accuracy and computation cost/time comparisons with such methods to establish the benefits of using a deep learning based surrogate model. While this is a good applied paper with a large variety of experimental results, there is a significant lack of novelty from a machine learning perspective. 2. But based on my understanding, this does not really explicitly incorporate the physical laws within the learning model and can't guarantee that the generated data would obey the physical laws and invariances. So, this is closer to a graphics approach and deep learning has been used before extensively in a similar manner for shape generation, shape transformation etc. The specific way deformations are composed -- using v_inv to backwards correct basis deformations, following up the mixing of those with a correction model -- is intuitive and is also something I see for the first time. 3. In terms of practical applications, to the best of my knowledge there are sophisticated physics-based and graphics based approaches that perform very fast fluid simulations. Experimental results are sufficient. However, it is also necessarily to add more intuitions to the current approach. I found the paper hard to read at first, since the paper is heavy on terminology, only really understood what is going on when I went through the examples in the appendix, which are helpful and then on a second read the content was clear and appears technically correct. Experimental results are sufficient. However, it is also necessarily to add more intuitions to the current approach. This was done for Fig 6, but would be nice to also see it numerically in ablation in Fig. 4. Overall, it is good paper to see at *CONF*.
The authors propose different methods to train the input dependent baseline function: o) a multi-value network based approach o) a meta-learning approach Learning an input-dependent baseline function helps clear out the variance created by such perturbations in a way that does not bias the policy gradient estimate (the authors provide a theoretical proof of that fact). 1312-1320). [POST-rebuttal] I've read the author's response and it clarified some of the concerns. Detailed comments: 1) On learning the input-dependent baselines: Generalising over context via a parametric functional approximation, like UVFAs [1] seems like a more natural first choice.
The authors then address the problem of data drift in BMI and describe a number of domain adaptation algorithms from simple (CCA to more complex ADAN) to help ameliorate it. Here the authors define a BMI that uses an autoencoder -> LSTM -> EMG. A number of different approaches are described from creating a pre-execution calibration routine whereby trials on the given day are used to calibrate to an already trained BMI (e.g. required for CCA) to putting data into an adversarial network trained on data from earlier days. Please clarify. Page 8, How do the AE results and architecture fit into the EMG reconstruction "BMI" results?
Summary: Train a multilingual NMT system using the technique of Johnson et al (2017), but augment the standard cross-entropy loss with a distillation component based on individual (single-language-pair) teacher models. +++++++++++++++++++ I have updated my rating after reading author's responses The authors apply knowledge distillation for many-to-one multilingual neural machine translation, first training separate models for each language pair.
The novel part of the approach (using subsplit Bayesian networks as a variational distribution) is intelligently combined with recent ideas from the approximate-inference literature (reweighted wake-sleep, VIMCO, reparameterization gradients, and multiple-sample ELBO estimators) to yield what seems to be an effective approach to a very hard inference problem. Its leverages recently proposed subsplit Bayesian networks (SBNs) as a variational approximation over tree space and combines this with modern gradient estimators for VI. The paper would be stronger if it discussed these in more detail - how close can they come to approximating the models usually used in phylogenetic analyses?
The main goal of the paper is to show that SOS converges locally to SFP, and to fixed points only, while avoiding strict saddles. Casting the recently proposed LOLA gradient adjustment into a general matrix form, they diagnose an example where the shaping term in LOLA prevents convergence to SFP. Minor Comment: First paragraph in Section 2.2, "It is highly undesirable to converge to Nash in this game" -> Nash equilibria This paper introduces a new algorithm for differential game, where the goal is to find a optimize several objective functions simultaneously in a game of n players. In Section 2.2, the authors make a broad statement that ''Nash equilibria cannot be the right solution concept for multi-agent learning.'' They provide one example where Nash is undesirable (L^1 = L^2 = xy). As Theorem 2 is the crux for all the theoretical advancement presented in the paper, clarifications on above correctness questions is very important for clear acceptance of this work.
In the submitted manuscript, the authors compare the performance of sign-symmetry and feedback alignment on ImageNet and MS COCO datasets using different network architectures, with the aim of testing biologically-plausible learning algorithms alternative to the more artificial backpropagation. They extend results of Bartunov et al 2018 (which found that feedback alignment fails on particular architectures on ImageNet), demonstrating that sign-symmetry performs much better, and that preserving error signal in the final layer (but using FA or SS for the rest) also improves performance. Summary: The authors are interested in whether particular biologically plausible learning algorithms scale to large problems (object recognition and detection using ImageNet and MS COCO, respectively). The obtained results are promising and quite different to those in (Bartunov , 2018) and lead to the conclusion that biologically plausible learning algorithms in general and sign- symmetry in particular are effective alternatives for ANN training. On the other hand, I think the conclusions regarding the first question -- whether sign-symmetry can be useful in artificial settings -- are fine given the experiments. In this work, on the other hand, there seems to be two scientific questions: first, to assess whether BP algorithms can be useful in artificial settings, and second, to determine whether they can say anything about how the brain learns, as in Bartunov (indeed, the author's conclusions highlight precisely these two points). Instead, I would suggest keeping black (or gray) for backpropagation (the baseline), and then using two hues of one color (e.g. light blue and dark blue) for the two sign-symmetry models. This is fine under the pretense of answering the first question, but to seriously engage with the results of Bartunov et al. and assess sign-symmetry's merit as a BP algorithm for learning in the brain, the work requires the authors the algorithms to be tested under similar conditions before claiming that there is a "direct conflict". A couple of remarks: I would be interested in understanding the robustness of the sign-symmetry algorithm w.r.t.
This paper proposes learning a latent variable deep generative model over every randomly sampled subset of observed features. The paper appears to be technically sound, and the experiments are thoughtfully designed. While not the first model to try to handle modeling data with missing features, it is still a fairly original and elegant formulation. The method is compared against 1) classical approaches in missing data imputation on UCI benchmarks; 2) image inpainting against recently proposed GANS for the similar task, as well as; 3) against universal marginalizer, which learns conditional densities using a feedforward / autoregressive architecture. The paper presents a model for learning conditional distribution when arbitrary partitioning the input to observed and masked parts. Inference in this latent variable model is achieved through the use of an inference network which conditions on the set of "missing" (to the generative model) features. "Missing Value Imputation Based on Deep Generative Models." arXiv preprint arXiv:1808.01684 (2018). The complexity of data considered is simplistic (and may not make use of the expressivity of the deep generative model). My concern about the experimental results on missing data imputation is that strong competition such as Gondra et al'17 and Yoon et al'18 that report better results on UCI than classical approaches are not included. While the derivation of the method is principled, it assumes that either the mask is known during the training OR one could efficiently sample a distribution of masks to learn arbitrary conditional densities. Experimental Results The model is evaluated against MICE and MissForest on UCI datasets.
The authors propose an approximation of the so-called best-response function, that maps the hyperparameters to the corresponding optimal parameters (w.r.t the minimization of the training loss), allowing a formulate as a single-level optimization problem and the use gradient descent algorithm. Can cross-validation be adapted to this approach?
The experimental results show that ENorm performs better than baseline methods on CIFAR-10 and ImageNet datasets. The authors show that the proposed method preserve functionally equivalent property in respect of the output of the functions (Linear, Conv, and Max-Pool) and show also that ENorm converges to the global optimum through the optimization. The authors propose a new weight re-parameterization technique called Equi-normalization (ENorm) inspired by the Sinkhorn-Knopp algorithm. The authors propose a new regularization method for neural networks. Their method explicitly use this property to balance the weights of the network, without changing the function computed by the network. (+) The computational overhead reduced by the proposed method compared with BN and GN looks good. (+) The theoretical analysis of the convergence of the proposed algorithm is well provided. ================== After rebuttal ================== The authors provided new experiments supporting the proposed method. While the authors present the method as an alternative to batch normalization, most of the reported results show a better performance for BN.
After showing theoretical guarantees of these methods (linear convergence) the authors propose to combine them with existing techniques, and show in fact this leads to better results. This paper looks at solving optimization problems that arise in GANs, via a variational inequality perspective (VIP). Summary: The authors take a variational inequality perspective to the study of the saddle point problem that defines a GAN. The authors show in a simple example (a bilinear function) these exhibit better performance than Adam and a basic gradient method. The authors look at a simple GAN setup where both the generator and the discriminator are linear models. Two techniques that have been widely used to solve VIP problems are averaging and extragradient methods. For strongly-monotone operators (a generalization of strongly-convex functions) extrapolation updates are shown to have linear convergence. For clarity it should be mentioned that GANs parametrized with neural nets lead to non-monotone VIs. A provably convergent algorithm for that setting is still an open problem, no?
This paper is about issues that arise when applying Information Bottleneck (IB) concepts to machine learning, more precisely in deterministic supervised learning such as classification (deterministic in the sense that the target function to estimate is deterministic: it associates each example to one true label only, and not to a distribution over labels). SUMMARY: This paper is about potential problems of the information bottleneck principle in cases where the output variable Y is a deterministic function of the inputs X. Such a deterministic relationship between outputs and inputs induces the problem that the the IB "information curve" (i.e. I(T;Y) as a function of I(X;T)) is piece-wise linear and, thus, no longer strictly concave, which is crucial for non-degenerate ("interesting") solutions. This work analyses the information bottleneck (IB) method applied to the supervised learning of a deterministic rule Y=f(X). The authors argue that most real classification problems indeed show such a deterministic relation between the class labels and the inputs X, and they explore several issues that result from such pathologies. - the two first problems described ((1) and (2)) are original, interesting contributions to the field, of particular interest for people interested in applying information bottleneck concepts to supervised learning; In the experiments of the present paper, the results seem to suggest that the interesting intermediate representations (separation in 10 compact clusters of the MNIST classes) is actually easier to obtain (large range of \\beta) optimizing the IB Lagrangian rather than the proposed squared IB Lagrangian. This is not an image classification task though, as predictions are made for each pixel; still, given an input image X, there is only one correct output Y, so, still in the deterministic supervised classification problem. Analyzing situations in which Y = f(X) (with f being a deterministic function) is certainly interesting from a theoretic point of view, but I am not convinced that this analysis is truly relevant for practical problems. 6) They show that multiple successive representations (like in DNNs), have identical predicting power (mutual information with output Y) when they allow for perfect prediction. Cons: - The fact that multiple successive representations have identical predicting power when the prediction error is zero, was already observed for example in Shwartz-Ziv et al. 2017. I feel it would be appropriate, either in the general literature section, either for discussing how to compute in practice the mutual informations (exact values vs. Namely: (1) the "Information Bottleneck curve" cannot be computed with the Information Bottleneck Lagrangian approach (because of optimization landscape issues: optimization of such a piecewise-linear function with a linear penalty will always yield the same optimum whatever the slope of the penalty is [same story as L1 vs. estimates or lower bounds as here).
This paper proposes training multiple generative models that share a common latent variable, which is learned in a weakly supervised fashion, to achieve high level coordination between multiple agents. The approach extends VRNN to a hierarchical setup with high level coordination via a shared learned latent variable. The generative models are hierarchical, and these latent variables correspond to higher level goals in agent behavior.
The authors conduct experiments on both static and time series data and validate that the method perform better than related methods in terms of clustering results as well as interpretability. The authors also suggest augmenting their setup with a model of cluster transition dynamics for time-series data, which seems to improve the clustering further, as well as providing an interpretable 2D visualisation of the system's dynamics. This paper proposes a deep learning method for representation learning in time series data. This paper deals with an interesting problem as learning an interpretable representation in time series data is important in areas such as health care and business. The authors may want to better discuss the performance of this algorithm, especially compared to its much lower modeling complexity with respect to the proposed method. The goal is to learn a discrete two-dimensional representation of the time series data in an interpretable manner. However this aspect is not discussed in detail, while it would be beneficial to provide experiment about the sensitivity and accuracy with respect to the choice if this parameters.
Indeed, the method performs better than several competitors plus a single human expert. 4. Compared to the existing models (DenseNet, Multi-scale CNN etc.), the performance improvement of the proposed model is limited. Compared to the claimed baselines (Liimatainen et al. and human experts), the proposed architecture shows a much higher performance.
Novelty: (1) The error bound of value iteration with the Boltzmann softmax operator and convergence & convergence rate results in this setting seem novel. In that work, the weighting is state-dependent, so the main algorithmic novelty in this paper is removing the dependence on state visitation for the beta parameter by making it solely dependent on time. (2) The proof of Theorem 1 uses the fact that |L(Q) - max(Q)| <= log(|A|) / beta, which is not immediately clear from the result cited in McKay (2003). Another recent paper that actually does prove a regret bound for a Boltzmann policy for RL is 'Variational Bayesian Reinforcement Learning with Regret Bounds', which also anneals the temperature, this should be mentioned. Clarity: In the DBS Q-learning algorithm, it is unclear under which policy actions are selected, e.g. using epsilon-greedy/epsilon-Boltzmann versus using the Boltzmann distribution applied to the Q(s, a) values. Summary: This work demonstrates that, although the Boltzmann softmax operator is not a non-expansion, a proposed dynamic Boltzmann operator (DBS) can be used in conjunction with value iteration and Q-learning to achieve convergence to V* and Q*, respectively.
The authors show that the proposed approach leads to good results on three translation datasets. However, the core idea of the proposed method, that is, combining the word representation, sub-graph state, incoming and outgoing representations seems to be novel. The paper is generally written fairly clearly, though I think the clarity of section 3.3 could be improved; it took me several reads to understand the architectural difference between this second variant and the original one. Comparing on the small datasets, the proposed method seems to significantly improve the performance over current best results of NPMT+LM. Therefore, it is unclear whether the proposed method has essential effectiveness to improve the performance on the top-line NMT baselines. This paper proposes a method for combining the Graph2Seq and Seq2Seq models into a unified model that captures the benefits of both.
But as pointed out above, it is possible to convert a fixed perturbation attack to a zero confidence attack via a binary search. || from the beginning. This paper proposes an efficient zero-confidence attack algorithm, MARGINATTACK, which uses the modified Rosen's algorithm to optimize the same objective as CW attack. While one would indeed expect an overhead due to the binary search, it is not clear a priori how large this overhead needs to be to achieve a competitive zero confidence attack with PGD (especially with a tuned step size for PGD, see above). - In the second paragraph of the introduction, the authors claim that fixed perturbation attacks and zero confidence attacks differ significantly. I am still a bit confused about the difference between "zero-confidence attacks" and those that don't fall into that category such as PGD. - In the introduction, the authors emphasize the distinction between fixed perturbation attacks and zero confidence attacks. However, from an optimization point of view, these two notions are clearly related and a fixed perturbation attack can be converted to a small perturbation / zero confidence attack via a binary search over the perturbation size. i have change my rating from 5 to 6 after reading the numerous and thorough rebuttals from the authors. Method such as projected gradient descent fall into the "fixed perturbation" category, while MarginAttack and CW belong to the "zero confidence" category. As a result, it is not clear whether the running time benchmarks are a fair comparison since MarginAttack does not automatically tune its parameters.
Authors present a novel regularizer to impose graph structure upon hidden layers of a neural Network. [2] Koutnik et al, Evolving neural networks in compressed weight space. Pros: Interesting idea for bringing some benefits of graphical models into Neural Networks using a regularizer. Experiments verify that one can successfully improve the intrepretability of hidden representations. Authors propose a class of graph spectral regularizers and their performance is different in different tasks. It is not surprising that adding such regularizers to the training process of neural networks can help to get more structural activations. Also, in none of the experiments authors mention how the added regularizer affects the model performance. Regarding the capsule network example, when you write that without regularization each digit responds differently to perturbation of the same dimension, isn't it possibly true only up to a, unknown, permutation of the neurons?
This paper argues that a random orthogonal output vector encoding is more robust to adversarial attacks than the ubiquitous softmax. Basically, authors notice that gradients of a deep neural network when one hot encoding is used can be highly correlated and hence can be used in the design on an attack. One could train a classification model where the final fully connected layer (C inputs K output logits) were a frozen matrix (updates disabled) of K orthogonal basis vector (ie, the same as the C_{RO}) codebook they propose. This work proposes an alternative loss function to train models robust to adversarial attacks.
This paper considers parameterizing Dirichlet, Dirichlet-multinomial, and Beta distributions with the outputs of a neural network. The authors briefly argue that the proposed methods are superior because they provide uncertainty estimates for the output distributions. As the authors note, parameterizing an exponential family distribution with the outputs of a neural network is not a novel contribution (e.g. Rudolph et al. (2016) and David Belanger's PhD thesis (2017)) and though I have never personally seen the Dirichlet, Dirichlet-multinomial, and Beta distributions used, the conceptual leap required is small. 2. In section 3, the authors consider the unique challenges of using the proposed networks. Their transformation from real-valued network output to, say, strictly positive concentration parameters in a Dirichlet are worth studying; but they don't analyze this in any detail. If the main benefit of the proposed networks is proper uncertainty quantification, then the evaluations (even if they are qualitative) should reflect that. In summary, I do not think the models proposed in section 2 are sufficiently novel to justify publication alone which means that the authors need to either: (1) evaluate novel methods that are critical for use of these models or
The authors propose to estimate and minimize the empirical Wasserstein distance between batches of samples of real and fake data, then calculate a (sub) gradient of it with respect to the generator's parameters and use it to train generative models. [6]: https://www.sciencedirect.com/science/article/pii/0377042794900337 The paper 'Generative model based on minimizing exact empirical Wasserstein distance' proposes a variant of Wasserstein GAN based on a primal version of the Wasserstein loss rather than the relying on the classical Kantorovich-Rubinstein duality as first proposed by Arjovsky in the GAN context. Typos: Eq (1) and (2): when taken over the set of all Lipschitz-1 functions, the max should be a sup The paper proposed to use the exact empirical Wasserstein distance to supervise the training of generative model. My concerns come from both theoretical and experimental aspects: The linear-programming problem Eq.(4)-Eq.(7) has been studied in existing literature. The contribution is about combining this existing method to supervise a standard neural network parametrized generator, so I am not quite sure if this contribution is sufficient for the *CONF* submission.
This paper discusses applications of variants of RNNs and Gated CNN to acoustic modeling in embedded speech recognition systems, and the main focus of the paper is computational (memory) efficiency when we deploy the system. One of the biggest issues of this paper is that they use CTC as an acoustic model, while still many real speech recognition applications and major open source (Kaldi) use hybrid HMM/DNN(TDNN, LSTM, CNN, etc.) systems. - Section 3.2: I actually don't fully understand the claims of this experiment based on TIMIT, as it is phoneme recognition, and not directly related to the real application, which is the main target of this paper I think. The main issue of this paper is the lack of novelty: the three evaluated approaches (Diag LSTM, QRNN and Gated ConvNet) are not novel, the only novelty is the addition of a 1D convolution, which is not enough for a conference like *CONF*. This analysis is actually valuable, and this suggested change about the position of this TIMIT experiment can avoid some confusion of the main target of this paper.) This paper investigates a number of techniques and neural network architectures for embedded acoustic modeling. Hence I argue for rejection, and suggest that the authors consider submitting the paper to a speech conference like ICASSP.
The contributions of this paper allow to use this model on real-word datasets by reducing the time and space complexity of the NTP model. This paper propose an extension of the Neural Theorem Provers (NTP) system that addresses the main issues of this method. [Summary] This paper scales NTPs by using approximate nearest neighbour search over facts and rules during unification. 3) Section 2 on the NTP framework is not very helpful for a reader that has not read the previous paper on NTP (in particular, the part on training and rule learning). The increments presented are reasonable and justified, but the experimental results, specifically on the larger datasets, warrant further investigation. In particular, it would be useful for the authors to focus on providing more insights on how the proposed techniques improve the results, and in what ways. The attention mechanism (essentially reducing the model capacity) is also well-known but its effect in this particular framework is not properly elaborated. - No ablation study is performed so the effect of incorporating mentions and attention are unclear. NTP systems by combining the advantages of neural models and symbolic reasoning are a promising research direction. This is the most elaborated section out of the three, yet seems like the most trivial -- unless the authors can provide an analytical bound on the loss in ntp score w.r.t the neighborhood size. However, I feel that the paper in its current form is not yet ready for publication in *CONF*, for the following reasons: 1) The authors propose three improvements.
This paper proposes a new open-domain QA system which gives state-of-the-art performance on multiple datasets, with a large improvement on QuasarT and TriviaQA. It will make more sense if the authors apply the Re-Ranker or S-Norm to the proposed base open-domain QA model, and compare the improvement from different methods (Re-Ranker, S-Norm, SSL, CSL).
Overall, the paper is well-written with clear definitions/explanations plus  comprehensive ablation-analyses throughout, and thus constitutes a nice addition to the recent literature on leveraging neural networks for IIG. The problem of learning cumulative quantities in a neural net is that we need two types of samples: - the positive examples: samples from which we train our network to predict its own value plus the new quantity, By using these networks within a CFR framework, the authors manage to avoid huge memory requirements traditionally needed to save cumulative regret and average strategy values for all information sets across many iterations. As a result, the NN is not providing any compression or generalization, and I would expect that the network can memorize the training set data exactly, i.e. predict the exact mean counterfactual value for each infoset over the data. Here, maybe a way to alleviate this problem would be to generate negative samples (where the network would be trained to predict low cumulative values) by following a different (possibly more exploratory) policy.
Contribution: - Using a known parameters crystallography simulator (X-ray beam, structure being analyzed, environment (crystalline or not)) built a dataset (called DiffraNet) of 25,000 512x512 grayscale labeled images of resulting diffraction images of various materials/structures (crystalline or not) . o This is to assess the generalization level of the DiffraNet dataset patterns' fine-tuned classification algorithms to real-life obtained patterns (relates to the previously stated representability of the samples). • Were any real-life setting obtained pattern samples classified using DiffraNet dataset patterns' fine-tuned classification algorithms? Since the contribution is mainly on the dataset level and not on the methodological level, I suggest submitting such an article in venues more focused on the application domain.
2. The authors claim monotonic propagation in the constant forget gates is more interpretable than those of the vanilla-LSTM, as no abrupt shrinkage and sudden growth are observed.
This submission proposes a method for learning to follow instructions by splitting the policy into two stages: human instructions to robot-interpretable goals and goals to actions. This paper contains an important core insight---much of what's hard about instruction following is generic planning behavior that doesn't depend on the semantics of instructions, and pre-learning this behavior makes it possible to use natural language supervision more effectively. It is claimed that the natural language instruction following approaches described in the first paragraph "require a large amount of human supervision" in the form of action sequences. It would be better to evaluate on one of the few common benchmarks for robot language understanding, e.g., the SAIL corpus, which considers trajectory-oriented instructions. Even if we set aside the distinctions between human-generated instructions and synthetic command languages like used in Hermann Hill & al., the goal -> policy module is defined by a buffer of cached trajectories and goal representations.
This paper presents a thorough and systematic study of the effect of pre-training over various NLP tasks on the GLUE multi-task learning evaluation suite, including an examination of the effect of language model-based pre-training using ELMo. The work presented in this paper relates to the impact of the dataset on the performance of contextual embedding (namely ELMO in this paper) on many downstream tasks, including GLUE tasks, but also alternative NLP tasks. Table 2 is very interesting, the results suggesting that we are indeed very far from fully robust sentence representation method. The paper seems to suggest that it consists of pre-training a model on the same task on which it is later evaluated. The main conclusion is that both single-task and LM-based pre-training helps in most situations, but the gain is often not large, and not consistent across all GLUE tasks. Contextualized word representations have gained a lot of interest in recent years and the NLP and ML community could benefit from such detailed comparison of such methods. One of the issue is that the authors if seems to believe that ELMO is the best contextual language model. The field is moving so quickly that the experiments might become invalid pretty soon (e.g. see BERT model referenced below). The study and the experimental results will be useful and interesting to the community. Only a handful of NLP tasks have an ample amount of labeled data to get state-of-the-art results without using any form of transfer learning. I'm confused by what this means, and how this is different from just training on that task.
The authors propose a method to improve sample diversity of GANs. They introduce multiple discriminators, each aims to not only compare real and fake examples but also compare different "micro-batch" of examples. The submission proposes to increase the variety of generated samples from GANs by a) using an ensemble of discriminators, and b) tasking them with distinguishing not only fake from real samples, but also their fake samples from the fake samples given to the respective other discriminators. Various methods have been proposed, as curtly summarized in Section 1.1 of the submission. As a sanity check - given a fixed generator - if you continue to train the discriminators on randomly drawn samples from this generator distribution does the microbatch discrimination objective continue to make progress and converge to a minimum? It also misses PACGan (Lin 2017) which also augments a discriminator to look at multiple samples to improve diversity.
This paper presents a continual learning method that aims to overcome the catastrophic forgetting problem by holding out small number of samples for each task to be used in training for new tasks. The authors consider the last layer and the softmax function as SVM, and obtain support vectors, which are used as important samples of the old dataset. Pros: (1) The authors propose a sensible approach, which is also novel to be best of our knowledge, using SVM to select support data from old data to be fed to the network along with the new data in the incremental learning framework to avoid catastrophic forgetting. Cons: Major Points: (1) To show that the method proposed in the paper addresses catastrophic forgetting, in addition to the overall accuracy shown in Figure 3, it is also necessary to show the accuracy of different models on old classes when new classes are added to the network. (2) The authors claim that iCaRL suffers from overfitting on real training data (section 4.1) however Table 2 shows iCaRL only on the enzyme function prediction which is also the dataset where the difference in performance between iCaRL and SupportNet is the largest. As the authors referred, the features of support vector data continuously change as the learning goes on.
This work aims to use formal languages to add a reward shaping signal in the form of a penalty on the system when constraints are violated. Why was this? If these are really constraints on the action sequence, isn't this showing that the algorithm does not work for the problem you are trying to solve? In most work on DQN in Atari, the game rewards are clipped to be between -1 and 1 to improve stability of the learning algorithm. As of this revision, however, I'm not sure I would recommend it for publication.
This work proposes an ensemble method for convolutional neural networks wherein each convolutional layer is replicated m times and the resulting activations are averaged layerwise. The CIFAR-10 baseline numbers are not ideal, and since IEA is basically "plug and play" in existing architectures, starting from one of these settings instead (such as Wide ResNet https://arxiv.org/abs/1605.07146) and showing a boost would be a stronger indication that this method actually improves results. I would also significantly reduce the claims of novelty, such as "We introduce the usage of such methods, specifically ensemble average inside Convolutional Neural Networks (CNNs) architectures." in the abstract, given that this is the exact idea explored in other work including followups to Maxout. My key concerns here are on relation to past work, greater comparison to closely related methods, and improvement of baselines results. IEA proposes to use multiple "parallel" convolution groups, which are then averaged to improve performance. Is the performance boost greater than simply using an ensemble of m networks directly (resulting in the equivalent number of parameters overall)? Given the close similarity of this work to Maxout and others, a much stronger indication of the benefits and improvements of IEA seems necessary to prove out the concepts here. Given the above issues of clarity and that this simple method seems to not make a favorable comparison to the comparable ensemble baseline (significance), I can't recommend acceptance at this time.
By jointly training all these networks, the authors are now able to compress a given network by mapping it's discrete architecture into the latent space, then performing gradient descent towards higher accuracy and lower parameter count (according to the learned regressors). This paper deals with Architecture Compression, where the authors seem to learn a mapping from a discrete architecture space which includes various 1D convnets. Two further regressors are trained to map from the continuous latent space to accuracy, and parameter count. In particular, it first extracts some characteristics for the neural network architecture and then learns two mapping functions, one from the encoded architecture characteristics to the expected accuracy and the other from the same encoded architecture characteristics to the number of parameters. Pros: 1. The idea of converting the architecture characteristics, which is discrete in nature, to continuous variables is interesting. Specifically, compared to those methods, the search space for the proposed paper is larger because although the number of layers is fixed, the connections between layers give more freedom to the compression algorithm. Overall I really like the idea in this paper, the latent space is well justified, but I cannot recommend acceptance of the current manuscript.
Summary: Proposes a framework for performing adversarial attacks on an NMT system in which perturbations to a source sentence aim to preserve its meaning, on the theory that an existing reference translation will remain valid if this is done. Next, standard gradient-based adversarial attacks are carried out, replacing the three tokens that result in the biggest drop in (approximate) reference probability, either 1) with no constraints, 2) constrained to character swaps of the original token, or 3) constrained be among the 10 closest embeddings to the original token. Coming to positives, the two real contributions for me are: (a) the result that chrF correlates better with human judgement, and (b) the measurement of adversarial perturbation's success measured via a sum that includes relative decrease in target score and the similarity of source sentence with the perturbed version. Limiting source perturbations to character swaps and neighbors in embedding space, then using automatic metrics to measure semantic distance seems both unnecessary and unlikely to succeed. The authors further study a series of automatic metrics for determining whether semantic meaning in the input space has changed, and find that the chrF method produces scores most correlated with human judgement of semantic meaning.
The proposed method instead constructs a target distribution which places probability mass not only on leaf category nodes but also on their neighbors in a known semantic hierarchy of labels, then penalizes the KL-divergence between a model's predicted distribution and this target distribution. Instead of directly optimizing the standard cross-entropy loss, the paper considers some soft probability scores that consider some class graph taxonomy. I suspect that the authors chose not to perform this comparison since unlike DeViSE and ConSE their method cannot predict category labels not seen during training; instead it is constrained to predicting a known supercategory when presented with an image of a novel leaf category. The main contribution is that soft probability scores are used to perform classification instead of using only class membership information. However due to the omission of key references and incomplete comparison to prior work, the paper is not suitable for publication in its current form.
Instead, the paper proposes learning a conditional GAN, which can potentially generate large amounts of realistic synthetic data, and use this data (in addition to original training data) for model compression. The experimental results look good, GAN generated data help train a better performed student in knowledge distillation. One way to test this is to use some unseen real data (e.g. validation or a held-out part of training data) for model compression, and showing that it can indeed help in improving student performance. I like the authors' explanation on why GAN is particularly good in a student-teacher setting. The paper claims that the best compression score is 1 (training student model on real data), while the paper shows that in fact, good synthetic data should produce *better* accuracy than using real data.
This paper presents an end to end rl approach for hierarchical text classification. 1. "we optimize the holistic metrics over the hierarchy by providing the policy network with holistic rewards" For the scale of datasets discussed, where SVM based methods seem to be working well, it is possible that approaches [1,2] which can exploit label correlations can do even better. The approach offers clever and promising techniques to force the inference process in structured classification to converge, but experiments seem to lack apple-to-apple comparisons. However, I think the authors should rather present this work as structured classification, as labels dependencies not modeled by the hierarchy are exploited, and as other graph structure could be exploited to drive the RL search. The results the authors quote as representative of other approaches seem in fact entirely reproduced on datasets that were not used on the original papers, and the authors do not try an apple-to-apple comparison to determine if this 'reproduction' is fair.
To improve the robustness of neural networks under various conditions, this paper proposes a new regularizer defined on the graph of the training examples, which penalizes the large similarities between representations belonging to different classes, thus increase the stability of the transformations defined by each layer of the network. This paper proposes the interesting addition of a graph-based regularisers, in NNs architectures, for improving their robustness to different perturbations or noise. Experiments on CIFAR-10 show that the method improves the robustness of the neural networks to different types of perturbations (perturbations of the input, aka adversarial examples, and quantization of the network weights/dropout0. 1. It is still not clear why would this regularization help robustness especially when considering adversarial examples. Compared to the previous version, this paper made a good improvement in its experimental results, by adding two different robustness settings in section 4.1 and section 4.3, and also include DeepFool as a strong attack method for testing adversarial robustness. While the overall concept of graph regularization is appealing, the exact relationship between the proposed regularization and robustness to adversarial examples is unclear.
The paper investigates the problem of universal replies plaguing the Seq2Seq neural generation models. - it would be interesting to know for the trivial questions if the performance was impacted by the deemphasizing (one that do result in universal replies) This paper presents a framework for understanding why seq2seq neural response generators prefer "universal"/generic replies. On top of this wrong factorization, section 2.2 & 2.3 derives a bunch of meaningless lemmas with extremely crude assumptions. Overall, given the problems this work is not technically sound to be accepted.
I suggest rewriting especially the abstract and the introduction and then submitting to a different venue as the approach itself seems promising. Strengths --------- The framework is expressive enough that many interesting use cases are clear, from specifying background knowledge during training to model inspection. By replacing labeled examples with domain knowledge about the relationships among classes in CIFAR-100, the paper demonstrates a compelling use case for DL2. PSL by construction produces convex loss functions, and so the constraint that all outputs for a group of classes is either high OR low would probably not work well. Additionally, as only very limited comparison experiments can be performed the method itself should be more thoroughly inspected by performing, for example, edge-case or time/number of constraints inspections. It would furthermore be interesting to inspect the corner cases of the proposed method such as what happens if two constraints are nearly opposing each other and so on.
One result that stands out is that RMGD achieves better results than the best performing batch size. I don't think the performance improvements are totally conclusive, but one of the most appealing properties of their proposal is that it shouldn't be much more computationally expensive than using a fixed minibatch size. The idea of viewing the choice of hyperparameters in a learning algorithm as a bandit problem is known and has been explored in different contexts, although the specific application to minibatch size is new as far as I know. This paper considers a resizable mini-batch gradient descent (RMGD) algorithm based on a multi-armed bandit for achieving best performance in grid search by selecting an appropriate batch size at each epoch with a probability defined as a function of its previous success/failure. I tend towards accepting the paper. The paper could have gained strength if bandits had been considered in wider context of parameter/model selection in deep learning.
In this paper, the author proposed a better control variate formula for second-order Monte Carlo gradient estimators, based on a special version of DiCE (Foerster et al, 2018). Overview: This nicely written paper contributes a useful variance reduction baseline to make the recent formalism of the DiCE estimator more practical in application. Such issues reduce the value of the contribution in its current form and may contribute to ongoing misunderstandings of the control variate framework and action-dependent baselines in RL, to the detriment of variance reduction techniques in machine learning. Since this work only focuses on second-order gradient estimations, I think it would be better to verify its advantages in various scenarios such as meta-learning or sparse reward RL  as the author suggested in the paper. I recommend the authors look into adaptively estimating an optimal scale for the baseline using a rolling estimator of the covariance and variance to fix this issue.
Figure 3 shows the results on four different maps for which expert demonstrations are generated from a Finite-Machine Tree-search method (a competitive method in this environment). The strength of the paper is the ability to learn from even 10 sub-optimal demonstrator trajectory thereby achieving optimality in reaching the goal. Although, the authors comment about the strong assumptions being made to aid the analysis. Also, it is unclear to me whether the curves shown is comparable as the starting point of the agent, at least in the beginning of training, is close to the goal with higher success rate for the Backplay method compared to baselines. The paper has also acknowledges these previous works. The method in the paper is as follows: assuming access to expert demonstration and a resettable simulator, the start state of the agent in the beginning of training is sampled from end of demonstration (close to the rewarding state) where the task of achieving the goal is easy. I believe this paper requires substantial improvements for publication and is not up to the *CONF* standards in its current form.
The community might possibly start to focus more on cooperative games because of this paper. This paper develops a reinforcement learning approach for negotiating coalitions in cooperative game theory settings. The authors propose a new setup in which self-interested agents must cooperatively form teams to achieve a reward. Thus I think some of the concerns above should be addressed before publication, but I would not be very disappointed if it were published as is. The paper shows 4 results: 1) It considers a hand-designed bot similar to models from the game theory literature. It uses simple cooperative weighted voting games 1) to study the efficacy of deep RL in theoretically hard environments and 2) to compare solutions found by deep RL to a fair solution concept known in the literature on cooperative game theory.
The importance of proving the main statement under more general conditions on activation functions is doubtful and the authors do not comment on that. * it would be beneficial to define the main objects, wide-network limits, in a more formal way (Section 2.3)
It shows different pooling strategies reach to similar levels of deformation stability after sufficient training. It shows different pooling strategies reach to similar levels of deformation stability after sufficient training. Fair experiments with conclusion of similar stability of different pool layers after training is very evident. iii) Although, the results presented on smoother filter initialization are interesting, but these results are not compared in a one to one setting to different pooling methods, convolutions or residual networks. iii) Although, the results presented on smoother filter initialization are interesting, but these results are not compared in a one to one setting to different pooling methods, convolutions or residual networks. This paper tries to argue that pooling is unnecessary for deformation invariance, as title suggests, and proposes initialization based on smooth filters as an alternative.
This work proposes a hybrid VAE-based model (combined with an adversarial or maximum mean discrepancy (MMD) based loss) to perform timbre transfer on recordings of musical instruments. I infer that this was necessary to achieve good performance, but it would be instructive to see the results without this additional input, since it does in a sense constitute a form of supervision, and therefore limits the types of training data which can be used. I appreciated that the one-to-one transfer experiments are incremental comparisons, which provides valuable information about how much each idea contributes to the final performance. Some detailed comments are listed as follow, * MMD losses in the context of GANs have also been studied in the following papers: - "Training generative neural networks via Maximum Mean Discrepancy optimization", Dziugaite et al. (2015) - "Generative Models and Model Criticism via Optimized Maximum Mean Discrepancy", Sutherland et al. (2016) - "MMD GAN: Towards Deeper Understanding of Moment Matching Network", Li et al. (2017) The many-to-many results are clearly better than the pairwise results in this regard, but in the context of musical timbre transfer, I don't feel that this model successfully achieves its goal -- the results of Mor et al. (2018), although not perfect either, were better in this regard. * Mor et al. (2018) do actually make use of an adversarial training criterion (referred to as a "domain confusion loss"), contrary to what is claimed in the introduction. The proposed method is designed to be many-to-many, and uses a single pair of encoders and decoders with additional conditioning inputs to select the source and target domains (timbres). By further conditioning our system on several different instruments, the proposed method can generalize to many-to-many transfer within a single variational architecture able to perform multi-domain transfers. Overall, I feel that this paper falls short of what it promises, so I cannot recommend acceptance at this time.
[3] Exemplar Guided Unsupervised Image-to-Image Translation Since the attribute information has been modeled by the network parameters, will different exemplar image lead to different translation outputs? Some of these works also applied the mask technique or adaptive instance normalization to the image-to-image translation problem. 1) The paper says that "For example, in a person's facial image translation, if the exemplar image has two attributes, Cons: * There seems a conflict in the introduction (page 1): the authors clarify that "previous methods [1,2,3] have a drawback of ...." and then clarify that "[1,2,3] have taken a user-selected exemplar image as additional input ...". [4] StarGAN: Unified Generative Adversarial Networks for Multi-Domain Image-to-Image Translation 2) As co-segmentation is proposed to "capture the regions of a common object existing in multiple input images", why does the co-segmentation network only capture the eye and mouth part in Figure 2 and 3, why does it capture the mouth of different shape and style in the third macro column in Figure 4 instead of eyes?
This paper presents a GAN approach adapted for multi-modal distributions of single class data. The proposed idea is to train a GAN generator to simulate anomalies in the data in order to provide the one-class classifier with more negative examples. 3) the general idea (train GAN to generate samples in low-density regions of the original probability density; since a disrciminator is trained to classify such samples from real data, mainly belongning to high-density regions of the original probability density, then the discriminator can be used to detect anomalies) is nice. Overall I find that the paper is not clear and reproducible enough for me to recommend its acceptance: - Results are presented on a single private dataset, and I don't see any indication that the dataset will be shared with the community.
1) adam models keeps in memory x_t (the model parameter), g_t (the model gradient), m_t (momentum) \\hat v_t and v_{t-1} (the monotone and non monotone version of the second order moment estimation. Second, even when disregarding the slightly abusive assumptions over the iterate sequences, that are common in the adaptive stochastic optimizers community, I think that the bound proposed in theorem 1 is non informative, as the second term behaves like T sqrt(T) assymptotically, due to the presence of 1 / \\alpha_t. Regarding the experiment section, I am afraid that testing a new optimizer over MNIST and CIFAR is not enough to show the relevance of the method for the whole deep learning community. In contrast, the proposed model do not track v_{t-1}: this amounts to a memory saving of 20% considering all model related parameters.
The MDP state proposed by this paper is the current performance score on each sample in a hold-out set. On many of the datasets, the performance difference between the proposed method and uncertainty sampling is quite small in table 1. In the experiments, there needs to be discussion of how much variety there is in the different datasets in terms of their statistical properties that are relevant to active learning, such as how well the data cluster? It would be good to evaluate this featurisation with a supervised active learner (like LAL), in order to disambiguate whether the good performance comes from these feature choices, or from the recent RL algorithms used to optimise.
The paper also presents issues with the standard "learning to learn" optimizers, one being the short-horizon bias and as credited by the authors has been observed before in the literature, and the second one is what is termed the "exponential explosion of gradients" which I think lacks enough justification as currently presented (see below for details). 3. Figure 6 - the results here seem to indicate that the learned optimizer transfers reasonably well, achieving similar performance to first-order methods (slightly faster validation reduction).
Specifically, the proposed solution is to repeatedly apply weight compression and fine-tuning over the entire training process. Pros: - The proposed method is shown to work with existing methods like weight pruning, low-rank compression and quantization. This paper proposed a general framework, DeepTwist, for model compression. Therefore, it is not clear how the proposed framework is helping the model compression techniques. Overall, I think the novelty of the paper is very limited, as all the weight distortion algorithms in the paper can be formulated as the proximal function in proximal gradient descent. PS: After discussion, I think the motivation of the method is not clear to understand why the proposed method works. The paper does not really propose a new way of compressing the model weights, but rather a way of applying existing weight compression techniques. Then proximal function can be applied directly after Distortion Step to project the solutions.
arXiv preprint arXiv:1611.03530. This paper proposes Permutation Phase Defense (PPD), a novel image hiding method to resist adversarial attacks. In summary the authors propose a  simple and intuitive method to improve the defense on adversarial attacks by combining random permutations and using a 2d DFT. The permuted phase component does not admit weight sharing and invariances exploited by convolutional networks, which results in severely hindered clean accuracy -- only 96% on MNIST and 45% on CIFAR-10 for a single model. While the security of a model against adversarial attacks is important, a defense should not sacrifice clean accuracy to such an extent. The experiments with regards to robustness to adversarial attacks I find convincing, however the overall performance is not very good (such as the accuracy on Cifar10). - Insufficient baselines. While the permutation is kept as a secret, it is plausible that the adversary may attempt to learn the transformation when given enough input-output pairs. The paper demonstrated the method on MNIST and CIFAR10, and evaluates it against a number of adversarial attacks. The idea is drawn from cryptography, where the random permutation is treated as a secret key that the adversarial does not have access to. The authors should introduce an appropriate threat model and evaluate this defense against plausible attacks under that threat model.
More importantly, the performance boost is significant when DQN's parameters which are used to initialize the model for the new environment were trained using dropout and L2 regularization in the default flavour/mode. Specifically, they showed that when features (parameters of DQN) are trained in one environment (default flavour/mode) and then used as an initialization for the same model but for a slightly different environment ( i.e. still captures key concepts of the original environment ) can boost the performance of the model in the new flavour/environment. Strengths: + This paper is interesting in the sense that it empirically shows that using regularization in training deep RL can be helpful when the goal is the generalization from one flavour of an environment to another one but very similar to the original. During training, if I do not see car accelerating, I think it makes no sense to expect to generalize to a new game that has this property as it is out-of-distribution. - According to the paper (at least my understanding) DQN's hyper-parameters are tuned based on default mode/flavour environment.
The paper proposes a sampling-based method that aims at accelerating Batch Normalization (BN) during training of a neural network. This paper proposes a new technique that can reduce the computational complexity of batch normalization. (+) The way of reducing the computational cost looks good. Clarity: I have not been able to fully understand why the proposed (uniform) sampling variant of BN is better than previous effort at making BN less computationally expensive in a GPU-based training environment by reading the paper: 1. The authors argue that the "summation" operation is the one that makes BN expensive; however, the authors have not demonstrated enough evidence of this argument (-) How to determine the sampling ratio for each normalization method is not provided, and it would be better if the authors can show some studies about sampling ratio versus the speed gain. Furthermore, the proposed sampling strategy looks heuristic without any studies.
This paper presents a way use using FSA-augmented MDPs to perform AND and OR of learned policies. The experiments results show that the composition method does better than soft Q-learning on composing learned policies, but how it performed compared to earlier hierarchical reinforcement learning algorithms? In section,  -> In this section are it has -> and it has This paper mainly focuses on combining RL tasks with linear temporal logic formulas and proposed a method that helps to construct policy from learned subtasks.
This paper proposes Deep Overlapping Community detection model (DOC), a graph convolutional network (GCN) based community detection algorithm for network data. Most of the shallow/deep graph embedding methods can also be used for link prediction task (many of the recent paper report such results). The paper is very well aligned with recent literature on ML for graphs, which is focused on combining different ideas of deep learning, tailoring them to particular graph problem and reporting results on some datasets.
2. In section 3 — Interpretable Multi-Variable LSTM, by stacking exogenous time series and target series, the authors implicitly formulate their algorithms in a way to consider auto-regression. 1. In the related work the authors state that "In time series analysis, prediction with exogenous variables is formulated as an auto-regressive exogenous model " . Summary: The authors propose IMV-LSTM, which can handle multi-variate time series data in a manner that enables accurate forecasting, and interpretation (importance of variables across time, and importance of each variable).
This is largely an experimental paper, proposing and evaluating various modifications of variational recurrent models towards obtaining sequence data representations that are effective in downstream tasks. General questions: - "dependence of observations at each time step on all latent variables": Unfortunately, this means that the complexity of evaluating the model during training is O(n^2), where n is the sequence size, rather than linear in the standard case. - regarding section 2.1.: Multi-modal marginal probabilities are also used due their increased modeling power, and this again seems like a potential limitation of the proposed approach w.r.t. the baseline, and is not discussed.
The paper studies a particular task (the XOR detection problem) in a particular setup (see below), and proves mathematically that in that case, the training performs better when the number of features grows. The paper proves that, under this setup, training with a number of features k > 120 will perform better than with k = 2 only (while k = 2 is theoretically sufficient to solve the problem). Note also that the probabilities in some theorems are not really probabilities of convergence/performance of the training algorithm per se (as one would expect in such PAC-looking bounds), but actually probabilities of the batch of examples to all satisfy some property (the diversity). I suggest also adding discussion on the optimization to Sec.4 as well.
In section 4, Empathetic dialog generator, you state that the dialog model has access to the situation description given by the speaker (also later called the situational prompt) but not the emotion word prompt. You state in section 2, Related Work "we train models for emotion detection on conversation data that has been explicitly labeled by annotators" – please describe how this was done. The paper in particular is contributing its collected set of 25k empathetic dialogs, short semi-staged conversations around a particular seeded emotion and the results of various ways of incorporating this training set into a generative chatbot. I also have some doubts about the two claimed contributions of the paper (the authors actually list 3 contributions in the introduction, but for convenience I lump the 2 non-data ones together): (1) Dataset: The dataset was crowdsourced by giving workers an emotion label (e.g., afraid) and asking them to define a situation in which that emotion might occur and inviting them to have a conversation on that situation.
This paper proposes to use n-ary representations for convolutional neural network model quantization. ECCV 2018. Summary: This paper proposes a technique for quantizing the weights and activations of a CNN. - In section 4, it is not made clear whether the activations are quantized according to the same scheme as the weights (apart from the issue of selecting a good clipping interval, which is addressed). - Last paragraph of section 4: "(Cai et al., 2017) experimentally showed that the pre-activation distribution after batch normalization are all close to a Gaussian with zero mean and unit variance. I have several questions for this paper: 1) the main algorithm is mainly based on the hypothesis that the weights are with Gaussian distribution. Strengths: 1. The idea of nested-means clustering is interesting, which somehow shows its effectiveness. The main idea is to use 'nest' clustering for weight quantization, more specifically, it partitions the weight values by recurring partitioning the weights by arithmetic means and negative of that of that weight clustering.
This paper proposed a new method for image restoration based a task-discriminator in addition to the GAN network. It shows superior performance than the baseline methods without such task-discriminator on medical image restoration and image super-resolution. For medical image reconstruction and image super-resolution, the proposed method was not compared with any of the state-of-the-art methods, but only with the same method without a task-discriminator as a baseline. Authors propose to augment GAN-based image restoration with another task-specific branch such as classification tasks for further improvement. Please see the following comments: 1. Adding an task-discriminator in a GAN network seems straightforward to improve the specific task. and Mu Lee, K., Accurate image super-resolution using very deep convolutional networks. 2. Actually, as the authors mentioned, GAN is not an appropriate model for image restoration when  accurate image completion is required. The authors are expected to make comparison with methods not based on GAN framework. While the results are better, the idea seems straightforward and has limited novelty.
This paper analyzes the surface of the complete orthogonal dictionary learning problem, and provides converge guarantees for randomly initialized gradient descent to the neighborhood of a global optimizer. Boumal et al. Global rates of convergence for nonconvex optimization on manifolds.
Summary: This work tackles few-shot (or meta) learning, providing an extension of the gradient-based MAML method to using a mixture over global hyperparameters. Stochastic EM is used for end-to-end learning, an algorithm that is L times more expensive than MAML, where L is the number of mixture components. They further propose a non-parametric approach to dynamically increase the capacity of the meta learner in continual learning problems. + The extension to continual learning is particularly interesting, as current methods for avoiding catastrophic forgetting. State of the art results on miniImageNet 5-way, 1-shot, the only experiments here which compare to others, show accuracies better than 53: - Versa: https://arxiv.org/abs/1805.09921. While the idea of task clustering is potentially useful, and may be important in practical use cases, I feel the proposed method is simply just too expensive to run in order to justify mild gains. Please just be exact in the main paper about what you do, and what main competitors do, in particular about the number of points to use in each task update. This type of effort is needed to motivate an extension of MAML which makes everything quite a bit more expensive, and lacks behind the state-of-art, which uses amortized inference networks (Versa, neural processes) rather than gradient-based. There is also a nonparametric version, based on Dirichlet process mixtures, but a large number of approximations render this somewhat heuristic. 1. The performance of few-shot classification on MiniImageNet is not comparable to the state of the art (Table 2, Table 1). If expensive gradient-based meta-learning methods are to be consider in the future, the authors have to provide compelling arguments why the additional computations pay off. Especially, by Table, the proposed model performs much worse than existing methods (50% vs 60%).
+ On the experiments run, the HVM method appears to be an improvement over the two previous approaches of softmax weighting and straightforward averaging for multiple discriminators. As surveyed in the paper, the idea of training of Generative Adversarial Networks employing a set of discriminators has been explored by several previous work, and showed some performance improvement. 2. We propose a new method for training multiple-discriminator GANs: Hypervolume maximization, which weighs the gradient contributions of each discriminator by its loss. I guess that the reason may be that: the significant computational cost (both in FLOPS and memory consumption) increase due to multiple discriminators destroys the benefit from the small performance improvement. Training multiple discriminators (in this paper up to 24) significantly increases compute cost and effective model size.
The paper proposes a class of Evolutionary-Neural hybrid agents (Evo-NAS) to take advantage of both evolutionary algorithms and reinforcement learning algorithms for efficient neural architecture search. It would be better to add comparative results on the CIFAR and Imagenet data for convenient comparisons with state-of-the-art.
However, the paper is not in a good shape for publication in its current form. Similar question arises in the first paragraph of Section 4.1: if the connector network is constructed as a piecewise linear function (as stated earlier in the paper in abstract and introduction), then how can it be written as a matrix?
The major problem is that this paper does not correctly state the difference between their analysis and Kolher et al who already derived similar results for OLS. However, this paper has a significant number of problems that need to be addressed before publication, perhaps the most important one being the overlap with prior work.
This latent space representation is used as the reinforcement learning signal for the learner (probing) agent similar to the curiosity driven techniques where larger changes in the representation of mind are sought out since they should lead to larger differences in demonstrator agent behavior. The authors consider the scenario of two agents, a demonstrator acting in an environment to achieve a goal, and a learner, which can also interact with the environment, but whose goal is to learn the demonstrator's policy by carrying out actions eliciting strong changes in the demonstrator's trajectory. Particularly for the claim of generalization to different environments, the details are all in the engineering of the particular grid world tasks, how they relate to each other and the sate representation used for the demonstrator s_d. Overall, while the agent behaviour modelling focused on a type of inner state (based on past trajectories) provides benefits in the evaluated examples, it is unsure how well the approach scales to more complex domains based on strong similarity and simplicity of the tested toy scenarios (evaluation on sorting problems is an interesting step towards to address this shortcoming). 3. The core premise behind training the learner agent with RL is using a curiosity driven approach to train a probing policy to incite new demonstrator behaviors by maximizing the differences between the latent vectors of the behavior trackers at different time steps. - Same scale for the y-axes across figures This paper presents a method for interactive agent modeling that involves learning to model a demonstrator agent not only through passively viewing the demonstrator agent, but also through interactions from a learner agent that learns to probe the environment of the demonstrator agent so as to maximally change the behavior of the demonstrator agent. When the probing agent is testing the expert, it is essentially showing the imitator many different configurations of the environment. The method builds on imitation learning (behavioural cloning) to model the agent's behaviour and reinforcement learning to learn a probing policy to more broadly explore different target agent behaviours.
This paper describes an approach for training conditional future frame prediction models, where the conditioning is with respect to the current frame and additional inputs - specifically actions performed in a reinforcement learning (RL) setting. If this method is useful for model-based RL, I would expect to see experimental results for RL, at least for Frostbite (rather than just the training loss in Table 1). While I think this work has potential, this paper is clearly not ready for publication, and below are a few suggestions on what I think the authors need to do to improve the work: (1) The authors emphasize novelty, and being "first" a few times in the paper, but fail to mention the large existing work done on video prediction (i.e. [1]), many of which also used these triplet loss or adversarial losses.
Inspired by these networks, the authors propose weight initialization schemes for finite width networks. Furthermore, the only application of these infinitely wide networks proposed in this paper is for initialization of the weights of finite width networks. For practical implementation, the authors use this scheme with random Fourier features to construct finite-width network. Response to rebuttal: The authors have addressed my question about the weights being still in the same RKHS.
The interesting part of this paper appears in Section 4, where the author makes a connection between the SGD training process and \\alpha-SMLC(strong Markov learning chain). There is no actual connection to SGD left, therefore it is even hard to argue that the predicted shape will be observed, independent of dataset or model(one could think about a model which can not model a bias and the inputs are mean-free thus it is hard to learn the marginal distribution, which might change the trajectory) (the \\alpha SMLC) - through experiments, the paper shows that the trajectories of SGD and \\alpha-SMLC have  similar conditional entropy. The paper tries to describe SGD from the point of view of the distribution p(y',y) where y is (a possibly corrupted) true class-label and y' a model prediction. I think the paper is acceptable if the author can provide more insights (against Cons 2).
This paper proposes an architecture search technique in which the hyperparameters are modeled as categorical distribution and learned jointly with the NN. Cons: -I speculate that there is a trade-off between the number of different parameters and whether one training is good enough to learn the architecture distribution. I very much enjoyed the simplicity of the approach, but the question of innovation is making wonder whether this paper makes the *CONF* bar of acceptance.
The author proposed an Imagination Reconstruction Network for the recommendation task, which implements an imagination-augmented policy via three components: (1)  the imagination core (IC) that predicts the next time steps conditioned on actions sampled from an imagination policy; The paper proposed a new framework for session-based recommendation system that can optimize for sparse and delayed signal like purchase. Summary: The paper presents a session-based recommendation approach by focusing on user purchases instead of clicks. The paper aimed at improving the performance of recommendation systems via reinforcement learning. Regardless the sketchy description of the algorithm, the empirical results look good, with comprehensive baseline methods for comparison. Comments: The proposed architecture is an interesting inspiration from Neuroscience which fits into the sequential recommendation problem. The proposed algorithm with an innovative IRN architecture was intriguing. Empirically, the authors compare their method to several recent baselines. Minor comments: (1) It would be better if the authors can test the proposed model on more datasets. (2) State-of-the-art reinforcement learning algorithms were not taken into account for baselines in the experiments. Even though the author has promised to release their implementation upon acceptance, I still think the paper needs a major change to make the proposed algorithm more accessible and easier for reproduce. (3) Some details are missing, resulting in the fact that it is hard for other researchers to fully capture the mechanism of the proposed algorithm. Weaknesses of the paper: (1) The motivations of applying reinforcement learning techniques are not convinced to me. With this level of clarity, I don't think it's easy for other people to reproduce the results in this paper, especially in section 4, where I expect more details about the description of the proposed new architecture. This seems one of the most important components of the proposed algorithm, but I found it's very hard to understanding what is done here exactly. "where Tj,τ is the τ-th imagined item, φ(·) is the input encoder shared by π (for joint feature learning), AE is the autoencoder that reconstructs the input feature, and the discounting factor γ is used to mimic Bellman type operations. The motivation of creating imagined trajectories instead of actual user trajectories is unclear. As the proposed method is built based on reinforcement learning, it would be better if the authors could include state-of-the-art reinforcement learning algorithms as their baselines. Is it because reinforcement learning based methods work better than traditional machine learning based ones? It looks like a loss from a previous paper, but it's kind hard to track what it is exactly.
This paper proposes a new approach to enforcing disentanglement in VAEs using a term that penalizes the synergistic mutual information between the latent variables, encouraging representations where any given piece of information about a datapoint can be garnered from a single latent. The paper proposes a new objective function for learning disentangled representations in a variational framework, building on the beta-VAE work by Higgins et al, 2017. I commend the authors for taking a multi-disciplinary perspective and bringing the information synergy ideas to the area of unsupervised disentangled representation learning. Unfortunately, the authors do not properly evaluate their newly proposed Non-Syn VAE, only providing a single experiment on a toy dataset and no quantitative metric results. Also why one should use the authors' suggested penalization term instead of total correlation is not discussed, nor demonstrated as they perform similarly on both disentanglement and synergy loss. If the authors want to continue with the synergy minimisation approach, I would recommend that they attempt to use it as a novel interpretation of the existing disentangling techniques, and maybe try to develop a more robust disentanglement metric by following this line of reasoning. Though I appreciate this is a somewhat subjective opinion, for me, penalizing the synergistic information is probably actually a bad thing to do when taking a more long-term view on disentanglement.
This paper proposed a new way of learning point-wise alignment of two images, and based on this idea, one-shot classification and open-set recognition can be further improved. On the other hand, authors may argue that the hyper-column matching is not just about performance, whereas it also adds interpretability to why two images are categorized the same. The tacit assumption of the authors is that a classifier driven by a point-wise alignment may improve the interpretation. Adding the residual connection and just comparing the final layer embeddings is a cleaner method than ABM which  provides a richer embedding than baseline and could potentially close the performance gap between ABM and final layer matching. Authors argue that using average (independent) greedy matching of pixel embedding (based on 4-6 layer cnn hypercolumns) is a better metric for one-shot learning than just using final layer embedding of a 4-6 layer cnn for the whole image. It would have been nice if authors used this added interpretability in some manner. The authors propose a deep learning method based on image alignment to perform one-shot classification and open-set recognition. One motivation for proposing an alignment-based matching is a better explanation of results.
The authors compare the differentiable function against using prediction error via REINFORCE and DQN, showing that their intrinsic curiosity method results in more interactions with unseen objects than the other two methods. The authors motivate this work with arguments about the sample-efficiency required by real robot learning, and demonstrate basic results using a real robot. There are several things to like about this paper: - The problem of making "real-world" practical algorithms for exploration is clearly one of the biggest outstanding problems in reinforcement learning. - There is very little *science* in this paper, beyond the experiments pitting "improved algorithm" vs DQN/REINFORCE, which nobody ever claimed would be a good approach to exploration! However, the contribution of the paper is based on a more serious error: - sec3.1, "If the policy could be optimized using direct gradients, the rewarder could ...
This paper proposes a new framework for topic modeling, which consists of two main steps: generating bag of words for topics and then using RNN to decode a sequence text. In addition to the above framework, to make the model work better, several add-ons are also proposed, combining autoencoder, loss clipping, and a generative model to generate text sequences based on the bag-of-words. This paper proposes TopicGAN, a generative adversarial approach to topic modeling and text generation. As a result, this paper achieved impressive outcome for topic modeling tasks. For the first task, classification is not the main purpose of topic models, and while text classification _is_ used in many topic modeling papers, it is almost always accompanied by other evaluation metrics such as held-out perplexity and topic coherence. Table 4 shows slightly better results for "Preference" for TopicGAN with joint training, but "Accuracy" is measured only for the proposed model and not the baseline model. (2) Another major one is why the word sequence generator is introduced in the proposed model. (3) I did not see a major improvement of the proposed model over others, given that the only numerical result reported is classification accuracy and the state-of-the-art conventional topic models are not compared. I did not see the contribution of this part to the whole model as a topic model, although the joint training shows the marginal performance gain on text generation. Thus I feel it is not a fair evaluation to just compare the models using text classification tasks. 4. As you mentioned in this paper "your model can be easily combined with any current text generation models", have you done any experiments for demonstrating the original text generation model will get better performance after applying your framework? (3) Some of the experiment settings are not provided, for example, the number of topics, the value of \\alpha and \\lambda in the proposed model, the hyperparameters of LDA, which are crucial for the results.
Particularly, they adopt the gradient-based meta learning algorithm, MAML, and the maximal entropy (MaxEnt) IRL framework, and derive the required meta gradient expression for parameter update. Pros: + The solution proposed here in novel - combining meta-learning on tasks to alleviate a key problem with IRL based approaches. b) While the experimental results suggest the algorithm can recover the similar performance to the optimal policy of the true reward function, whether this observation can generalize outside the current synthetic environment is unclear to me. Can the authors comment on this and the sensitivity of the results to section of meta-learning tasks and rapid adaption?
It builds on a recent approach by Achiam et al on Constrained Policy Optimization (oft- mentioned "CPO") and an accepted NIPS paper by Chow which introduces Lyapunov constraints as an alternative method.
The paper presents a pool-based active learning method that achieves sub-linear runtime complexity while generating high-entropy samples, as opposed to linear complexity of more traditional uncertainty sampling (i.e., max-entropy) methods. (2) In terms of accuracy comparison, on Cifar-10-ten classes experiments, all ASAL variants have similar accuracies as random sampling, while traditional pool-based max-entropy clearly works much better. The main difference is sample matching that searches the nearest neighbor from pool and add the real unlabeled data for AL. The approach is interesting. However, the results presented in the paper are not strong and I do not see whether or not I should be using this method over uncertainty sampling. I believe that it would be better for your paper if you work a bit more on the experimental evaluation and submit a revised version at a later deadline.
Wrt the experiments, I appreciate that the authors took the time to investigate the poor performance of MIXER. In summary, the contribution over RAML and SPG in combining them is quite incremental, and the practical importance of combining them is questionable, as is the integrity of the presented experiments, given how poorly MIXER is reported perform, and the omission of stronger baselines like SCST and AC methods. Significance    3/5: RAML and SPG have not been established as important methods in practice, so combining them is less interesting. - Existing baselines in the paper (i.e. MIXER) do not perform as expected (i.e barely better than ML, worse than RAML) In particular, if this generalization can significantly outpeform existing methods it generalizes with non-degenerate settings, this would overcome the more incremental contribution of combining SPG and RAML. While the authors have shown that they can outperform SPG and RAML with a scheduled objective, it is not currently clear how sensitive/robust the results are to the term weight scheduling, or even what most appropriate general weights/scheduling approach actually is. Considering this, I feel that the importance of the paper largely rests on investigating and establishing the utility of the approach experimentally.
This work attempts to study the degree to which a layer by layer information bottleneck inspired objective can improve performance, as well as generally attempt to clarify some of the discussion surrounding Shwartz-Ziv & Tishby 2017. By using the method, the authors 1) validate the IB theory of deep nets using weight decay, and 2) provides a layer-wise explicit IB functional training for DNN which is shown to have better prediction accuracy. This paper provides a method to do explicit IB functional estimation for deep neural networks inspired from the recent mutual information estimation method (MINE). This work is about layer-wise training of networks by way of optimizing the IB cost function, which basically measures the compression of the inputs under the constraint that some degree of information with respect to the targets must be preserved. Here, the authors study a deterministic neural network, for which the mutual information estimation is difficult (I(X,L)) and error prone. I believe the authors instead meant to say that the cited works deviate from the information bottleneck theory of learning suggested in (Shwartz-Ziv & Tishby 2017). The title, abstract and especially the conclusion ("This provides, for the first time, strong and direct emperical evidence for the validity of the IB theory of deep learning") seem to present the paper as somehow offering some clarity and further support for the assertions of the Shwartz-Ziv & Tishby 2017 paper, but that paper hoped to establish that information bottleneck can explain the workings of ordinary networks. On one hand, I find this paper interesting, because it aims at carefully studying the proposed link between DNN training and IB optimization, thereby showing that layer-wise IB training indeed seems to work very well in practice. Despite a recurring focus of the text that this paper applies and information theoretic objective at each layer of the network, and hence is novel, the final sentence of the paper suggests it might not actually be needed and single layer IB objectives can work as well. There is some discussion of the text suggesting they believe their method acts like an approximate weight decay, but there are no results showing the effect of weight decay just on the baseline classification accuracies they compare against.
It seems to me that (except the minor small section of streaming data), the paper is more like a proper verification of how tree-based learning algorithms work very well in tabular data--which is far from the basis of the paper and does not make the paper novel enough for *CONF*. This paper proposes a hybrid machine learning algorithm using Gradient Boosted Decision Trees (GBDT) and Deep Neural Networks (DNN).
This paper discusses the problem of evaluating and diagnosing the representations learnt using a generative model. They suggest analysing performance of generative models based on these tools. Authors present a set of criteria to categorize MNISt digists (e.g. slant, stroke length, ..) and a set of interesting perturbations (swelling, fractures, ...) to modify MNIST dataset. This is a very important and necessary problem. However, when the entire image is subject to the generative model, it learns multiple properties from the image apart from shape too - such as texture and color. 3. Now assuming that my GAN model has learnt good representation in Morpho-MNIST dataset, is it guaranteed to learn good representations in other datasets as well? I'm not convinced that ability of a model in disentangling thickness correlates to their ability in natural image generation. Since their method is manually designed for MNIST, the manuscript would benefit from a justification or discussion on the  common pitfalls and the correlation between MNIST generation and more complex natural image generation tasks. However, here the authors have assumed that the latent space of the generative models are influenced only by the morphological properties of the image - which is wrong. Since the presented metrics do not show a significant difference between the VAE and Vanilla GAN model, the question remains whether evaluating on MNIST is a good proxy for the performance of the model on colored images with backgrounds or not. Studying the properties of a generative model on such datasets is very challenging and the authors have not added a discussion around that. * Providing benchmark data for tasks such disentanglement is important but I am not sure generating data is sufficient contribution for a paper. 2. Extracting morphological properties of the image is straight-foward for MNIST kind of objects. 1. Morphological properties deals with only the "shape" properties of the image object.
In particular it shows the stationary point of an l1 regularized layer has bounded non-zero elements. page 4, the concept of stationary point and general position can be introduced before presenting Theorem 1 to improve readability. 3. the notations of subgradient and gradient are used without claim The paper theoretically analyzes the sparsity property of the stationary point of layerwise l1-regularized network trimming. Specific comments follow. 1. While the paper analyzes the properties of the stationary point of the layerwise objective (5), the experiments seem to be conducted based on the different joint objective (8).
The paper proposes a multi-layer pruning method called MLPrune for neural networks, which can automatically decide appropriate compression ratios for all the layers. The connections from all layers with the smallest loss increments are pruned and the network is re-trained to the final model. Summary: I do appreciate the fact that the proposed method does not require hyper-parameters and that it seems to yield higher compression rates than other pruning strategies that act on individual parameters. Experiments: - While the reported compression rates are good, it is not clear to me what they mean in practice, because the proposed algorithm zeroes out individual parameters in the matrix W_l of each layer. Weaknesses: Novelty: - In essence, this method relies on the work of Marten & Grosse to approximate the Hessian matrix used in the Optimal Brain Surgeon strategy. Is that guaranteed? This paper introduces an approach to pruning the parameters of a trained neural network.
The paper is addressing the problem of a specific multi-task learning setup such that there are two tasks namely main task and auxiliary task. Authors suggest to further scale loss functions using the cosine similarity but it only experiments with the simpler case of binary decision of using both gradients or only the main one. So a more general question would be: rather than define the similarity measure to measure the gradient similarity of the target and auxiliary loss, it would be more useful to try to learn or define whether the auxiliary task is good for the target task beforehand. Example of these methods are: [GradNorm: Gradient Normalization for Adaptive Loss Balancing in Deep Multitask Networks, ICML 2018] and [Multi-Task Learning Using Uncertainty to Weigh Losses for Scene Geometry and Semantics, CVPR 2018]. There have been many interesting developments in adaptive scaling of multiple loss functions in the literature. Although the method limits the negative effect of the auxiliary task on the optimization of the main loss function, it can still slow down optimization if the auxiliary task is not well chosen. There are still major issues with the experimental study. Here is the summary: Although the response addresses some of my concerns. Method should be experimented with some of those setups. For example, one can simply try (g(target task)-g(auxiliary task)) in the equation. For Breakout experiment, single task actually outperforms all baselines and this means the proposed method results in a harm similar to ImageNet case. 1) The proposed method is based on the intuition: if the gradients of the target and auxiliary loss are in the same direction, the auxiliary loss will help the main/target task. Paper needs to be improved with a stronger experimental study and need to be re-submitted.
The paper proposes a method for training generative models with general f-divergences between the model and empirical distribution (the VAE and GAN objectives are captured as specific instantiations). This paper proposed a novel variational upper bound for f-divergence, one example of which is the famous evidence lower bound for max-loglikelihood learning. As far as I understood from the paper, changing the objective function to the upper bound of f-divergence have two merits compared to the existing methods. 3) Important analysis/experiments on several key points are missing, for example, (i) how to specify the variance of the spread divergence in practice? Moreover, the subjective nature of the results in the real dataset experiments makes it difficult to judge what the increased flexibility in training really provides - although I do note that the authors make this same point in the paper. Regarding the methodology, it would have been nice to see the method of Nowozin et al. (2016) applied in all experiments since this is a direct competitor to the proposed method.
2. The major novelty of this approach is the use of annotations supporting images and textual (pretrained) embedding spaces, but no related work regarding Wes was neither introduced in the Related Work section nor was it clearly explained in the text. 6. Table 2 presents the impact of the use of pretrained embeddings (word2vec) instead of one-hot vectors for labels.
A sufficient condition for Manifold Mixup to avoid this underfitting phenomenon is that the dimension of the hidden layer exceeds the number of classes. Major remarks: 1. The authors suggest that Mixup can suffer from interpolations intersecting with a real sample, but how Manifold Mixup can avoid this issue is not very clear to me. - I find the manifold mixup idea to be closely related to several lines of work for generalization abilities in machine learning (not just for deep neural networks). 4. It would be useful to also present the results for SVHN for supervised learning since the Cifar10 and Cifar100 datasets are similar, and the authors have already used SVHN for other task in the paper.
Under several assumptions (input is Gaussian, non-linear activation is strictly increasing, stable system) it is shown that SGD converges linearly to the ground truth system with near-optimal sample complexity. However, understanding non-linear dynamical systems is extremely challenging and this paper provides strong convergence guarantees. The main weakness of the paper is the assumption that the state equals the output. I agree that the paper has nice convergence results that could possibly be building steps towards the harder problem of unobserved hidden states however, there is more work that could be done for unstable systems and possible extension to ReLU and other activations to take it a notch higher. The proof idea is to reduce this problem to the problem of learning a single non-linear neuron in the case that the covariance matrix of the data is well-conditioned. The authors should update prior work on generalized linear models as well as neural networks.
For denoising, the authors propose to learn the latent variable that generates the clean test image by solving a ridge regularized non-convex inverse problem (Eq. 3). Pros: the authors develop a novel GAN-based approach to denoising, demixing, and in the process train generators for the various components (not just inference). As authors mention in the paper, it is indeed surprising that the proposed GANs-based model with two generators is able to produce samples from the distribution of each signal component by observing only additive mixtures of these signals. Actually in the appendix, comparisons are provided for a basic compressive sensing problem, but their only comparator is "LASSO" with a "fixed regularization parameter", and vanilla GAN. The proposed method is evaluated on both tasks (i.e., denoising and demixing) by conducting toy experiments on handwritten digits (MNIST). ********************* Update after author response: I think the Fashion-MNIST experiments and comparisons with ICA are many times more compelling than the original experiments. This paper proposed two new GAN structures for learning a generative modeling using the superposition of two structured components. The authors need to provide (R)MSE  results that show how well the method can reconstruct mixture components on average over the dataset.
This paper studies the problem of making predictions with a model trained using dropout. My problem here is that authors employ convoluted arguments to introduce this geometric mean prediction and the average prediction, without making the connection discussed above. I think it would be clearer to directly introduce the weighted power mean instead of the standard power mean in Section 3.2.
The authors seek to make it practical to use the full-matrix version of Adagrad's adaptive preconditioner (usually one uses the diagonal version), by storing the r most recently-seen gradient vectors in a matrix G, and then showing that (GG^T)^(-½) can be calculated fairly efficiently (at the cost of one r*r matrix inversion, and two matrix multiplications by an r*d matrix). Rather than adapting diagonal elements of the adaptivity matrix, the paper proposes to consider a low-rank approximation to the Gram/correlation matrix. The paper considers adaptive regularization, which has been popular in neural network learning. When you say that full-matrix computation "requires taking the inverse square root", I assume you know that is not really correct? I'm glad to see that the authors considered adding momentum (to adapt ADAM to this setting), and their experiments show a convincing benefit in terms of performance *per iteration*. The authors also discuss how to adapt the proposed algorithm with Adam style updates that incorporate momentum. In particular, I think that the proposed algorithm has a more-than-superficial resemblance to stochastic LBFGS: the main difference is that LBFGS approximates the inverse Hessian, instead of (GG^T)^(-½). (It's fair to modularize the proof, but as it is it's hard to know what it's saying, except that your method comes with some guarantee that isn't stated.) adaptive versions of sgd are commonly used in machine learning.
The paper proposes a method for converting a non-differentiable machine learning pipeline into a stochastic, differentiable, pipeline that can be trained end-to-end with gradient descent approaches. * Significance: The concept of converting a non-differentiable pipeline to a differentiable version is indeed very useful and widely applicable, but the experimental section did not convince me that this particular method indeed works: the results show a very small improvement (0.7-2%) on a single system (Faster R-CNN), that has already been pretrained (so not clear if this method can learn from scratch). Clearly c(y + z) is not generally equal to c(y) + z, even if E z = 0.
This paper proposes a generalization of variational auto-encoders to account for meta-data (attributes), learning new ones, in a way that these can be controlled to generate new samples. This paper proposes to automatically discover these attribute and thus work to produce variations even in the absence of known attribute information. Given that the ground truth attribute decomposition for MNIST is not known, even the qualitative results are impossible to evaluate. I recommend that the authors present quantitive results in the updated version of their paper (i.e. disentanglement metric scores, the log-likelihood of the reconstructions), including new experiments on a dataset like dSprites or CelebA, where the ground truth attributes are known. While the idea is interesting, the paper is lacking an experimental section, so the methodology is impossible to evaluate.
The paper proposes a new algorithm for implicit maximum likelihood estimation based on a fast Nearest Neighbor search. Evaluation:  This paper presents an interesting contribution: an implicit likelihood estimation algorithm amenable to theoretical analysis. Algorithm - While significant advancements have indeed been made for nearest neighbor evaluation as the authors highlight, it's hard to believe without any empirical evidence that nearest neighbor evaluation is indeed efficient in comparison to other methods of likelihood evaluation. The prior work trained the same normalizing flow model via maximum likelihood and adversarial training, and observed vastly different results on likelihood and sample quality metrics. The authors are clearly aware of the current research in generative modeling but the current work provides almost no strong evidence to consider this work as an alternative to other approaches. Some of them I've highlighted earlier in my review (e.g., some of the theorem assumptions being typically true, comparison of likelihood and sample quality based on model capacity etc.). The authors acknowledge this, but then say "this is no longer the case due to recent advances in nearest neighbor search algorithms (Li & Malik 2016; 2017)" (p 3-4).
The new thing is that the authors found that quantization of activation function improves robustness, and the approach can be naturally combined with FGSM adversarial training. I would in the current form reject the paper.
This paper proposes a method to train a machine translation system using weakly paired bilingual documents from Wikipedia. Positives - Large improvement over previous attempts at unsupervised MT for the En-De language pair. c) training the usual unsup MT pipeline with two additional losses, one that encourages good translation of the extracted parallel sentences and another one forcing the distribution of words to match at the document level. - Are the Supervised results in Table 2 actually a fair reflection of a reasonable NMT model trained with sub-word representations and back translated data? It would be useful to know the proportion of sentences you managed to extract to train your models. The model also uses the denoising autoencoding and reconstruction objectives of Lample et al. (2017). The results show improvements over the Lample et al. (2017) and that performance is heavily dependent on the number of sentences extracted from the weakly aligned documents.
The paper proposes a new graph pooling method by learning the node assignments from a CRF based structure prediction formulation. Strength: -- An interesting idea to use CRF idea to cluster the nodes on a graph for pooling purpose --  On a few datasets and task, the proposed method works pretty well
Summary: the authors introduce a method to learn a deep-learning model whose loss function is augmented with a DPP-like regularization term to enforce diversity within the feature embeddings. I strongly encourage the authors to carefully edit the paper before publication so that their nice work will be properly presented. At a high level, this paper is experimentally focused, but I am not convinced that the experiments are sufficient for acceptance.
The paper presents a case study of training a video classifier using convolutional networks, and on how the learned features related to previous, hand-designed ones. the paper uses model interpretation techniques to understand blackbox CNN fit of zebrafish videos. In order to identify which particular features the neural networks are paying attention to, the paper used Deep Taylor Decomposition, which allowed the authors to identify "clever-hans"-type phenomena (network attending to meaningless experimental setup differences that actually gave a way the ground truth classes). they show that, relying on the technique of deep taylor decomposition, their CNN relies its prediction on a different part of zebra fish than existing understanding. This allowed for the authors to mask out such features and make the network attend to more meaningful ones. it is also able to detect the use of experimental artifacts, whose removal improves predictive performance. while the experimental studies rely on our belief that the interpretation technique indeed interprets, the result that removing experimental features and improving predictive performance is convincing and interesting. This is evident in this paper with the authors calling CNNs "black box" and the learnings of a neural network "cheating". Perhaps the authors are also not aware that the fallacies that causes CNNs to overfit on some characteristics in the input data are also present in other machine learning tools such as SVMs. Perhaps the authors are not aware that CNNs are hardly black boxes, their inner workings quite transparent in mathematical terms, which the submitted paper itself explores.
This paper shows the non-acceleration of Nesterov SGD theoretically with a component decoupled model. MaSS is both theoretically and empirically proved to outperform Nesterov SGD as well as SGD. The authors present a new first order optimization method that adds a corrective term to Nesterov SGD.
Summary This paper studied the expressive power of graph NNs, specifically, their universality and limitations under the non-anonymous setting, via the theory of distributed computations. Decision This paper gave us a new approach to analyzing the expressive power of graph NNs. Not only does this paper give new theoretical results, but also it opens the door to a new research direction by bridging the theories of graph NNs and distributed computations. Overall, I tend to accept it and would like to increase the score based on authors' feedback.
This work proposes to leverage a pre-trained semantic segmentation network to learn semantically adaptive filters for self-supervised monocular depth estimation. The authors have addressed many of my initial concerns and provided valuable additional experimental evaluations. === Post rebuttal update === Hence I think which the idea advanced in the paper has merits, the manuscript is not really ready for publication.
This paper proposed a neural iterated learning algorithm to encourage the dominance of high compositional language in the multi-agent communication game. The main contribution of this paper is really: "studying how neural networks behave when trained in an iterated learning setting in a simple referential game". Overall, I like the paper, but due to the concerns mentioned above I think it's borderline, with a slight lean towards rejection.
### Summary: This paper propose a new model for learning generic feature representations for visual-linguistic tasks by pretraining on large-scale vision and language datasets like Conceptual Captions and language-only datasets like BookCorpus and English Wikipedia. This paper proposed a pre-trainable generic representation for visual-linguistic tasks call VL-BERT. While ViLBERT designs for easier extendable for other modalities, VLBERT is more focus on the representation learning on the vision and language, since the caption input also combines with the visual feature embedding. ### Other questions: - When training on text-only datasets, what is the input on Visual Feature Embedding since there are no associated images.
The paper proposes a metric for unsupervised model (and hyperparameter) selection for VAE-based models. The UDR score can be used for unsupervised hyperparameter tuning and model selection for variational disentangled method. -- To validate the fundamental assumption of UDR, the authors might consider to quantitatively validate that, disentangled representations learned by those approaches you used in the paper are almost the same (up to permutation and sign inverse). Overall, I think a reliable metric for model selection/evaluation is needed for the VAE-based disentangled representation learning. The problem this paper focuses on is essential because we usually apply unsupervised disentangled methods to analyze the data when the labels are unavailable.
This paper proposes two modifications for the MixMatch method [1] and achieves improved accuracy on a range of semi-supervised benchmarks. Mix-Match is already an elaborate method, and ReMixMatch additionally introduces learned data augmentation, an additional loss term for matching label distributions between labeled and unlabeled data, consistency-loss, and a self-supervised loss (section 3.3). For the reasons above, I think the paper is borderline, but I am currently voting for acceptance based on the strong empirical performance.
Overview: This paper proposes to use semi-supervised learning to enforce interpretability on latent variables corresponding to properties like affect and speaking rate for text-to-speech synthesis. I think it differs from standard semi-supervised training in that at test time we aren't explicitly interested in predicting labels from the semi-supervised labelled classes; rather, we feed in these labels as input to affect the generated model output.
The primary claim is that simple components which compute elementwise sum/average/max over the activations seen over time are highly robust to noisy observations (as encountered with many RL environments), as detailed with various empirical and theoretical analyses. This paper studies reinforcement learning for settings where the observations contain noise and where observations have long-range dependencies with the past. # UPDATE after rebuttal I have changed my score to 8 to reflect the clarifications in the new draft.
Equipped with the proposed techniques, the authors obtain promising results when training deep models with various batch sizes. The authors discuss four techniques to improve Batch Normalization, including inference example weighing, medium batch size, weight decay, the combination of batch and group normalization. 3. By combining all the techniques, the proposed method yields promising performance when training deep models with different batch sizes. (3) In Section 3.1, "we need only figure out …" should be "we only need to figure out …" The paper introduces four techniques to improve the deep network model through modifying Batch Normalization (BN). The CVPR paper reports that training \\gamma and \\beta without weight decay on ResNet-50 yields significant performance improvement.
This paper introduces Precision Gating, a novel mechanism to quantize neural network activations to reduce the average bitwidth, resulting in networks with fewer bitwise operations. The idea is to have a learnable threshold Delta that determines if an output activation should be computed in high or low precision, determined by the most significant bits of the value. To show that the resulting lower average bitwidth gained with PG leads to increased performance, the authors implement it in Python (running on CPU) and measure the wall clock time to execute the ResNet-18 model. Even though at the moment it is unclear to me how statistically significant the results are, and I strongly recommend commenting on this in the paper, I think the idea of PG and the demonstrated benefits make the paper interesting enough to be accepted at *CONF*.
This paper proposed VHE-GAN for the text-to-image generation task. Their proposed VHE-GAN model encodes an image to decode its associated text and feeds the variational posterior as the source of randomness into the GAN image generator. attnGAN (CVPR18), b. TA-GAN (NIPS18), c. Object-GAN (CVPR19). Weaknesses: - The experimental comparison only included old baselines and the authors should compare to some more recent work such as TA-GAN (NIPS18), and Object-GAN (CVPR19). - while it is difficult to dilute such a complex model to 8 pages, and the included appendix clarifies many questions in the text body, it would be worth further passes through the main paper with a specific focus on clarity and brevity, to aid in the accessibility of this work. Therefore, the authors should claim how the proposed VHE variational posterior can help the task. The authors have done a commendable job adding detail, further analysis, and experiments in the appendix of the paper. The proposed method utilizes the off-the-shell modules and feeds the VHE variational posterior into the generator.
Under the general framework of A2C, the core contribution of the paper is to apply a graph attention network on the knowledge graph to help learn better representation of the game state and reduce the action space. Pros: 1, I like the idea of constructing the knowledge graph as the agent roll out. The paper is very well written, especially the introduction section, demonstrating novelty in the context of fictional games literature, and showing good empirical results. The authors propose an agent that builds a dynamic knowledge graph of each state from the textual observation provided by the games, while choosing actions from a template-based action space. One of the main contribution in the paper is to represent the partial observations of the world as a knowledge graph, and so as to efficiently infer appropriate actions.
This work proposes a regularization strategy for learning optimal policy for a dynamic control problem in a latent low-dimensional domain. I hope the authors can clarify this and do not oversell the proposed approach.
Next to some experiments on data sets with a manifold structure, the main contribution of the paper is a tandem of theorems that state the conditions under which models can recover the topology---or the number of connected components---of a data set correctly, This paper studies robustness to adversarial examples from the perspective of having 'topology-aware' generative models. However, the main theoretical result on the number of connected components only applies to mixtures of Gaussian distributions, but the purported scope of the paper is the analysis of manifold-based defences in general. If the number of connected components does not match, based on theorem 2, Corollary 1 argues that a generative model can generate an adversarial example that does not exist in the data-generating manifold. I appreciate novel research that employs topology-based methods, but at present, I cannot fully endorse accepting the paper.
"efficiency searching scheme" This paper proposes a method, termed as Filter Summary (FS), for weight sharing across filters of each convolution layer. Some promising results demonstrate the effectiveness in CNN compression and the acceleration on the tasks of image classification, object detection and neural architecture search. arXiv: 1910.01271 In this paper authors propose a novel idea (called Filter Summary, FS)  how to compress convolutional neural networks with 2D convolutions (kernels are 3D tensors, it is also applicable to the 1D convolutions). - Via experiments it is demonstrated that proposed approach provides compression of the model while having close to the baseline quality for image classification and object detection tasks and for small and large datasets.
The paper suggest the method to train neural networks using 8 bit floating point precision values. The paper focuses on training neural networks using 8-bit floating-point numbers (FP8). On the downside, the first sections give the impression that FP32 is not used at all: "S2FP8 does not require keeping the first and last layer in FP32 precision, which is needed for other approaches (Mellempudi et al., 2019).". [1] https://en.wikipedia.org/wiki/Bfloat16_floating-point_format Good ideas Important area Impressive results. The evaluation is very convincing - the approach is demonstrated for image classification, Transformer-based translation, and neural collaborative filtering. S2FP8 outperforms previous FP8 approaches and reaches the accuracy of FP32 out-of-the-box. Thus, I recommend accepting this paper.
The authors present a biologically inspired sleep algorithm for artificial neural networks (ANNs) that aims to improve their generalization and robustness in the face of noisy or malicious inputs. They present a detailed comparative study spanning three datasets, four types of adversarial attacks and distortions, and two other baseline defense mechanisms, in which they demonstrate significant improvements (in some cases) of the sleep algorithm over the baselines. The paper proposes an ANN training method for improving adversarial robustness and generalization, inspired by biological sleep. Although the results are (I would argue) somewhat mixed, they are nonetheless positive enough to encourage more work in applying "sleep" and other relevant ideas from neuroscience to the problem of robustness in deep neural networks. Section 1: Introduction "We report positive results for four types of adversarial attacks tested on three different datasets (MNIST, CUB200, and a toy dataset) ..." I have some questions and concerns which I will detail per-section below, but overall, I believe that this paper is a valuable contribution to the literature and should be accepted once the authors have made a few necessary revisions.
The main tool introduced was the path angle visualization and the primary empirical result was that standard GAN methods may reach non-Nash stable attractors and perform well. The path angle visualization provides a novel tool to evaluate the empirical ability of any dynamics proposed for GANs to cancel out rotational components.
The main result is that the use of upsampling via a fixed interpolation filter provides an inductive bias towards "natural" images. Please proofread. The paper provides a theoretical study of regularization capabilities of over-parameterized convolutional generators trained via gradient descent, in the context of denoising with an approach similar to the "deep image prior". After discussion: The authors have addressed my concerns, so I am changing my decision from Weak Reject to Weak Accept.
Experiments are performed to evaluate the proposed method against baselines on 3D object detection (both in the semi-supervised setting and unsupervised moving object detection), 3D motion estimation, as well as sim-to-real transfer results (training in CARLA and testing on the KITTI dataset). Results demonstrate the strengths of the proposed view-contrastive framework in feature learning, 3D moving object detection, and 3D motion estimation. The results show that the proposed method: 1) has higher 3D object detection mAP than a view regression-based baseline from prior work (Tung 2019) for settings with little available 3D boundign box supervision; To facilitate the 3D view-contrast learning, this paper proposed a novel 2D-3D inverse graphics networks with a 2D-to-3D un-projection encoder, a 2D encoder, a 3D bottlenecked RNNs, an ego-motion stabilization module, and a 3D-to-2D projection module. Again, the authors also release the code, making the paper a valuable baseline for the following work. - A natural follow-up to this paper is Contrastive Predictive Losses (which had several successes in pure vision setting[1]). (4) As the latent map update module uses an RNN, it would be good to consider consistency beyond 2 frames (given mask is applied to view-contrastive objective). I am positive with respect to accepting this work, but find that there are a few unclear points in the evaluation that should be clarified to strengthen the empirical results.
This paper tackles the problem of learning with noisy labels and proposes a novel cost function that integrates the idea of curriculum learning with the robustness of 0/1 loss. Apart from the motivation raised by the authors, as we can see from this curriculum loss paper, NPCL nicely outperformed generalized cross entropy loss in [7], which is impressive. I would like to vote accept for this paper but the following point highly concerns me and I am not sure about the correctness (see the concern below).
------------------------------------------------------------------------------------------------------------------------------------------------------------------------- Summary: This paper presents a detailed empirical study of the recent bonus based exploration method on the Atari game suite. The authors combine Rainbow with different exploration methods, such as count-based bonus methods, curiosity-driven methods, and noisy networks. This also leads to the conclusion that recent results on the game Montezuma's revenge can be attributed to architectural changes instead of the exploration method. Throughout the paper, the experiments and results raise questions on the robustness and generalization of existing exploration methods across various ATARI games, but the paper puts absolutely zero effort into investigating if there is a quick fix to the questions it poses. I think the main contribution of the paper is that it raises some questions over existing methods/trends in solving exploration problems in reinforcement learning by comparing the performance of multiple methods across various games in ATARI suite. I can appreciate the contributions of the paper and I am happy to recommend accept.
The authors present a quantum algorithm for approximating the forward pass and gradient computation of a classical convolutional neural network layer with pooling and a bounded rectifier activation. - There's a clear separation of background (which is concise and well explained) and contributions, but maybe it would be worth connecting the introduced algorithm more closely to existing work in non-convolutional quantum neural networks? it would be valuable to convert those to estimates of which values of eta would be enough (given quantum networks of the size used in the classical simulation experiment, or given larger networks). Remarks: The authors should point out how this is specific to convolutional neural networks.
This paper provides a new technique to adapt a source neural network performed well on classification task to image segmentation and objective detection tasks via the author called parameter-remapping trick. Concrete comments 1. The paper's overall method is a novel one, unifying NAS on det/seg tasks, while prior works mostly only focus on one task. 4. Though the improvement over prior methods is good, the experiments lack an apple-to-apple comparison. But no one ever pretrain every classification network for searching on det/seg tasks right? On segmentation, the adapted model achieves ~1% mIOU improvement using  similar or less iterations and similar size of model with the methods it compared to, and GPU hours' saving is more significant. But it is also interesting to see that this naive method works, and actually beat some of the more advanced alternatives in Section 4. To the best of my knowledge, the experiments setting is sensible and the results are good. I like the direction this paper takes, NAS is too expensive and we need faster methods through meta learning/transfer learning. Overall I find the method is effective and experiments convincing and I recommend weak accept in my rating.
# Summary This paper proposes a new way to learn the optimal transport (OT) cost between two datasets that can utilize subset correspondence information. # Significance - This paper seems to present a neat and sensible idea for learning OT with neural networks given subset correspondence information. # Originality - This paper considers a new problem which is to learn OT given "subset correspondence" information. I recommend that the paper be accepted.
Recommendation: Accept. Minor comments and questions for the author: - I am slightly confused by the introduction of the rtop operator. If the author(s) would like to show good performance, I would suggest to compare the algorithms with the state-of-the-art algorithms in Deep Learning such as Adam, SGD-Momentum.
In this paper, the authors proposed two methods of Nesterov Iterative Fast Gradient Sign Method (NI-FGSM) and Scale-Invariant attack Method (SIM) to improve the transferability of adversarial examples. Experiments are carried out to verify the scale-invariant property and the Nesterov Accelerated Gradient method on both single and ensemble of models. Two methods have been proposed,  namely Nesterov Iterative Fast Gradient Sign Method (NI-FGSM) and Scale-Invariant attack Method (SIM). In this paper, the authors apply the Nesterov Accelerated Gradient method to the adversarial attack task and achieve better transferability of the adversarial examples.
The authors proposed a new method called "DropEdge", where they randomly drop out the edges of the input graphs and demonstrate in experiments that this technique can indeed boost up the testing accuracy of deep GCN compared to other baselines. I also like the discussion in sec 4.3 where the authors explicitly clarify what are the difference between DropEdge, Dropout and DropNode, as the other two are the methods that will pop up during reading this paper. I vote weak-accept in light of convincing empirical results, some theoretical exploration of the method's properties, but limited novelty.
Particularly, the authors propose a new benchmark PG-19 for long-term sequence modeling. This paper proposes a way to compress past hidden states for modeling long sequences. A variety of compression techniques and training strategies have been investigated in the paper and verified using tasks from multiple domains including language modeling, speech synthesis and reinforcement learning. The paper also introduces a new benchmark for long-range dependencies modelling composed of thousands of books. For testing and evaluating the modeling of really long context sequence modeling, the authors introduce PG-19, a new benchmark based on Project Gutenberg narratives. The outcome is a versatile model that enables long-range sequence modeling, achieving strong results on not only language model tasks but also RL and speech. The paper finally presents an analysis of the compressed memory and provide some insights, including the fact that the attention model uses the compressed memory. I am happy with the efforts made by the authors and I am raising my score to 8 (accept). The latest version of the paper adressed all my concerns, hence I change my rating to Accept. ## Updated review I have read the rebuttal. - The presented approach is significant, as modelling long-range dependencies is an important milestone in sequence modelling. The probably more interesting part of this paper is the training schemes designed to train the memory compression network. While this paper is more incremental and novelty may be slightly lacking, I think the breadth of experiments and competitive results warrants an acceptance.
The authors present an algorithm CHOCO-SGD to make use of communication compression in a decentralized setting. The authors consider CHOCO-SGD for non-convex decentralized optimization and establish the convergence result based on the compression ratio. Extensive empirical results are presented in this paper and the two use cases highlight some potential usage of the algorithm. This paper studies non-convex decentralized optimization with arbitrary communication compression. Overall, this could be a great paper if fixing the issues above. First, the authors only provide analysis on CHOCO-SGD but the comparison with baselines are based on their momemtum versions.
The authors consider several of these choices and then run a variety of experiments on small latent-dimension cases for VAEs. These reveal that sometimes non-Euclidean and in particular product spaces improve performance. Summary: This paper devised a framework towards modeling probability distributions in products of spaces with constant curvature and showed how to generalize the VAE to learn latent representations on such product spaces using Gaussian-like priors generalized for this case. Strengths, Weakness, Recommendation I like what the authors are trying to do here; embeddings and discriminative models on non-Euclidean spaces have been developed, offer credible benefits, and generative models are the next step. --Past works have considered VAEs on single constant curvature spaces and hence it is well-motivated to consider a more flexible model that enables usage of products of such spaces. I believe this work should be accepted, as while the numerical results are not particularly impressive, it provides some clear foundational work for further exploration of the use of non-euclidean latent spaces in VAEs.
The second method is to learn a re-scale factor for binary activations using real-valued activations from the previous block. The authors observed in Section 4.2 that it is very important for the teacher and student to have similar architectures, but did not explain the more important question that why a real-valued network would be able to teach a binary network, since they have quite different information flow. Therefore I am looking forward to the real-timing results. Some specific questions: - Why the real-valued teacher can help train the binary network while they have different information flow?
https://openreview.net/forum?id=rJl-b3RcF7 This paper studies the tasks of pruning filters, a provable, sampling-based approach for generating compact Convolutional Neural Networks (CNNs). *CONF* 2019. Summary: In this paper, the author propose a provable pruning method, and also provide a bound for the final pruning error. Unlike the other deterministic method, sampling skill suffer variance propagation problem, the pre-layer variance will affect the sampling probability of next layer, how this pruning work if we change status of the pre-layer,  I didn't find any theoretical guarantee and only find a proof of single layer reconstruction bound. Besides, Author should also consider an experiment in modern lightweight network, vgg and resnet like model are out of fashion and so big that any one can make a sound result on it. Author should at least provide a imagenet model and make this work more convincing. But ever since the "lottery" papers, I think it makes sense in locating the sparse and representative pruned structure, which can achieve better performance than full overparameterized model. This paper gives rise to a fully-automated procedure for identifying and preserving the filters in layers that are essential to the network's performance.
Alike other HRL agents, their method has two types of policies (manager and subpolicies), but different from other works they do not keep parameters fixed in post training for new tasks. The authors clearly position their work in the HRL paradigm and explain current limitations/challenges within that field. The motivation of this paper is "most methods still decouple the lower-level skill acquisition process and the training of a higher level that controls the skills in a new task." The paper proposes a method to learn higher-level skill selection and lower-level skill improvement jointly. It seems that the author is not aware of this point as the paper claims a particular way of achieving HRL is the HRL itself (in section 4.1 "In the context of HRL, a hierarchical policy with a manager πθh(zt|st) selects every p time-steps one of n sub-policies to execute."). But I don't think most works are limited in this aspect, as claimed by the paper in the abstract. This paper is under the research area of "hierarchical reinforcement learning." However, just like temporal abstraction, the HRL is a general idea instead of an existing problem formulation or a particular algorithm. 2. I think the author didn't justify his key design choices well.
This paper describes an approach to applying attention in equivariant image classification CNNs so that the same transformation (rotation+mirroring) is selected for each kernel. [Original review] The authors propose a self-attention mechanism for rotation-equivariant neural nets. This is not defined. That said, I feel that the content and conclusions of the paper are technically sound, having followed the maths, because the text was too confusing. There are three main issues detailed below that I'd like to see addressed in the authors' response and/or a revised version of the paper.
I appreciate the feedback of the Authors and I have decided to increase the rating. Since the proposed method is using RSB as it's core part, and claims to be a non-linear extension of it, it would be crucial to have a comparison with RSB, at least on those experimental setups, where high-level features are used (Tiny Imagenet with ResNET features, Reuters-21578, and 20 Newsgroups).
[4] Danilo Jimenez Rezende, Shakir Mohamed, Daan Wierstra, Stochastic Backpropagation and Approximate Inference in Deep Generative Models, 2014 Overview: This paper introduces a new method for uncertainty estimation which utilizes randomly initialized networks. <update> I would like to thank authors for verbose response and the revised version: it is a bit more clear. Additional comments / questions: (somewhat minor) p1: "While deep ensembles …, where the individual ensembles are trained on different data" - here and related text, it should probably be "individual models" / "individual networks".
In this paper, the authors outline a method for system control utilizing an "agent" formed by two neural networks and utilizing a differentiable grid-based PDE solver (assuming the PDE describing the system is known). [Summary] This paper proposes to combine deep learning and a differentiable PDE solver for understanding and controlling complex nonlinear physical systems over a long time horizon. ## Summary The authors propose a method for training physical systems whose behavior is governed by partial differential equations. [Major Comments] Predicting the middle point between two states for modeling the dynamics via deep neural networks is not new, but I did not know any other works that use this idea for controlling PDEs. The paper presents an interesting mix of neural networks and traditional PDE solvers for system control, and I vote for acceptance.
Summary: This paper proposes the use of two intrinsic rewards for exploration in MARL settings. Although the method uses a series of approximations and assumptions, I believe most of them are clearly stated and fairly well-motivated (plus they are not very far from those of other recent work in the deep MARL literature). My main reservation is a lack of comparisons to single agent exploration methods. -------------------UPDATE AFTER AUTHOR RESPONSE--------------------- The authors have done a great job address my two concerns (similarity to prior work and empirical comparisons with single-agent exploration).
This paper discusses an extended DSL language for answering complex questions from text and adding data augmentation as well as weak supervision for training an encoder/decoder model where the encoder is a language model and decoder a program synthesis machine generating instructions using the DSL. This paper presents a semantic parser that operates over passages of text instead of a structured data source. They show interesting results on two datasets requiring symbolic reasoning for answering the questions. This is excellent work, and it should definitely be accepted. I found myself immediately looking for the numbers/results when you introduce the experiment. The three claimed contributions are (1) better numbers, (2) better compositionality / domain applicability, and (3) better interpretability. But the fact that you have this predicate lets the model do these filters and greater-than comparisons inside the network in an opaque way, while also getting interpretable operations for some questions (table 5 is further confirmation of this, and of the fact that you probably are not capturing many of more the complex, compositional questions in DROP). The rest of this review focuses on things that I thought could be more clear, or that raise new questions, and might sound negative. Another question raised by the passage-span predicate: the more you use bare passage-span programs for training, the more the network learns to put all of its compositional reasoning inside, in an opaque way, instead of giving you interpretable compositionality. Interpretability: The use of passage-span as a predicate is really interesting, and it raises a lot of questions.
It's important to be able to estimate natural gradient in a practical way, and there have been a few papers looking at this problem but mostly for the case with a Fisher-Rao metric. This paper takes a different and general approach to approximate natural gradient by leveraging the dual formulation for the metric restricted to the Reproducing Kernel Hilbert Space. This a  strong submission with insightful angle on natural gradients and with provable guarantees. Overall, I lean to the acceptance side.
After highlighting the said properties in context of related work, the authors propose an approach to calculate the context-independent importance of a phrase by computing the difference in scores with and without masking out the phrase marginalized over all possible surrounding word contexts (approximated by sampling surrounding context for a fixed radius under a language model). This paper proposes a hierarchical decomposition method to encode the natural language as mathematical formulation such that the properties of the words and phrases can encoded properly and their importance be preserved independent of the context. For example, consider the input: "The movie is the best that I have ever seen.
The authors demonstrate that learnable K-matrices achieve similar metrics compared to hand-crafted features on speech processing and computer vision tasks, can learn from permuted images, achieve performance close to a CNN trained on unpermuted images and demonstrate the improvement of inference speed of a transformer-based architecture for a machine translation task. Even though in the worst-case complexity is not optimal, the authors argue that for matrices that are commonly used in machine learning architectures (e.g. circulant matrix in a convolution layer) the characterization is optimal.
The construction of the dataset focuses on demonstrating that compositional action classification and long-term temporal reasoning for action understanding and localization in videos are largely unsolved problems, and that frame aggregation-based methods on real video data in prior work datasets, have found relative success not because the tasks are easy but because of dataset bias issues. The paper introduces CATER: a synthetically generated dataset for video understanding tasks. This paper proposed a new synthetic dataset (CATER) for video understanding. The authors recognize that since the dataset is synthetically generated it is not necessarily predictive of how methods would perform with real-world data, but still it can serve a useful and complementary role similar to the one CLEVR has served in image understanding. The new dataset is the first to account for all of the following fundamental aspect of videos: temporal ordering, short- and long term reasoning, and control for scene biases. Due to the inherent biases in available action recognition datasets, models that simply averages video frames do nearly as well as models that take temporal dependencies into account. - p9 phenomenon -> phenomena; the the videos -> the videos; these observation -> these observations; of next -> of the next; in real world -> in the real world This paper introduces a new synthetic video understanding dataset, borrowing many ideas from the visual question answering dataset CLEVR.
The score network is a deep learning model with transformer and convolution layers, and the post-process network is solving a constrained optimization problem with an T-step unrolled algorithm. The authors decompose this problem into two part: (i) predicting an affinity score between each base pair in the input sequence, using a combination of a transformer sequence encoder network and a convolutional decoder, and (ii) a post-processing step that ensures that structural local and global constraints are enforced. The approach of unrolling structural constraints as shown in the paper is interesting and applicable to much wider domains than secondary structure prediction. I advocate for acceptance. *Comments* The actual specification of the output constraints doesn't occur until late in the paper.
This paper introduces another new attribution method that measures the amount of information (in bits!) each input region contains, calibrating this score by providing a reference point at 0 bits. ii) Proposal of a novel quantitative measure to compare quality of pixel-level attribution maps in image classification, and extension of a previously reported method. II) Perturbation-based approaches that inject noise (into the input image directly) have been proposed previously. My suggestions below are aimed at helping improve the paper even further.
This paper proposes a new option discovery method for multi-task RL to reuse the option learned in previous tasks for better generalization. [3],[5], [6], [7]. Some of these approaches might also be better baselines than OptionCritic which doesn't explicitly take into account learning from demonstrations or multi-task transfer. - While the paper cites a number of option learning approaches, it could do a better job of situating the research within the literature. Moreover, the framework requires optimal policies to generate trajectories for offline option learning, which seems to add more supervision signals than prior work such as option-critic. - The authors indicate that their learning objective needs the transition function P for the MDP. In particular, the visualization in 4b showing options learned in Amidar does not show much improvement from what was observed before in Harb, 2018. I currently recommend rejection. The evaluation of the proposed method is rather weak and does not clearly demonstrate that the author's goals have been achieved.
The proposed techniques use both the graph structure, and the current classifier performance/accuracy into account while (actively) selecting the next node to be labeled. The authors present an algorithm for actively learning the nodes in a graph that should be sampled/labeled in order to improve classifier performance the most. To decide which nodes in a graph to label during the active labeling, the paper proposes two approaches. Second, it proposes to adapt the page rank algorithm (APR) to determine which nodes are far away from labeled nodes. 1.2. "We have here shown that the accuracy of AL when uncertainty is computed regionally is much higher than when either local uncertainty or representative nodes are used", this is not the case on CiteSeer in Table 1 Overall it remains unclear *how* to select the right strategy (before seeing the results for a dataset) i.e. which of the proposed approaches or variants should one select for a new dataset. Weaknesses: 1. The paper contains several confusing and contradicting statements or claims which are not supported by the experimental results: For example: 1.1. 2.3. Page 6 mentions twice the "ratio between APR and PR", is this is this used/evaluated in the results? There seem to be two main contributions in the paper.
The aim of this work is to address this issue by learning how to select representative samples from a dataset for training local linear models to reproduce predictions of black-boxes. This work is closely related to Ren et al. 2018 [1], which proposes to meta-learn how to weight samples in a batch so as to maximize performance on a validation set. A key challenge is that in order for local models to be interpretable, they need to be simple in form and therefore lower in capacity (i.e. linear); thus, if they are trained on entire datasets they will underfit. * Pros: * Considers an interesting dataset subsampling variant of the sample weighting meta-learning problem. I've given a weak accept, conditioned on being provided more evidence regarding 1) comparisons to simple differentiable alternatives, 2) sample efficiency of the RL method, and 3) basic analysis of the weighting function. In particular, they propose to use RL to learn to weight instances from datasets; RL is required as they make hard (i.e. non-differentiable) decisions to select a subset of the dataset. A few ideas: * Randomly sample a (possibly large batch) and learn to weight it (closely related to the straight through estimator) * This would help elucidate whether RL is optimal in this setting: fitting a linear model on more data might be cheaper learning to subsample with REINFORCE. [1] Learning to Reweight Examples for Robust Deep Learning.
The paper makes an empirical claim that CNNs for object recognition do not contain hidden neuron which is highly selective to each class, mainly based on three aspects: (a) metrics related to the maximum informedness, (b) jitterplots of activation data, and (c) a user study assessing whether generated images maximizing a given unit is perceptible to the user. The paper empirically studies the category selectivity of individual cells in hidden units of CNNs. It is a sort of "meta-study" and comparison of different metrics proposed to identify cells with a preference for a specific target category. Overall, I think that the authors have presented a strong meta-analysis and compelling argument for further study in rigorously identifying the presence (or lack thereof) of selective units in neural networks and the degree to which they may be considered "object detectors. This work investigates the collection of methods that have been proposed to find units in neural networks that are selective for certain object classes. - Do the overall results mean that the "maximum informedness"-based metrics are superior to the others for assessing selectivity of a unit? According to that study, almost 60% 0f all fc8 units are "object detectors", with very high conherence between humans and selectivity metrics. In this context terminology matters: the study effectively tries to disprove that the network learns "near-perfect single output-category detectors", but who claims that it would do that? - In Section 2 - Network and Dataset - "... - e.g., there could be a highly selective "bird" unit which nevertheless has high false positive rate for any of the more specific bird species categories in the imageNet nomenclature. In the words of the paper, the "selective units are sensitive to some feature that is frequently, but not exclusively associated with the class" - I thought this is the standard majority view, not a surprising finding. This research area is important for understanding deep networks because claims have been made as to the relative importance (or lack thereof) of these individual units as identified by different measures vis-a-vis distributed representations -- the identification of such units would be interesting for understanding the predictions of classification networks.
I noticed the comment of the authors that gives a correction for the introduction. More datasets are needed to thoroughly verify the performance of the proposed ballistic graph neural network. While most existing works rely on classical random walks on the graph, the paper proposes to cope with the "speed of diffusion" problem by introducing ballistic walk. From my point of view, without a full re-writting of the paper, this work cannot be published in a conference like *CONF*.
The authors discuss here the problem of isomorphism bias in graph dataset, i.e. the overfitting effect in learning networks whenever graph isomorphism features are incorporated within the model. The authors fairly discuss the problem in the introduction, with a good coverage of the related literature; the background theory is reasonably discussed, although is not very deep. Testing on isomorphic graphs evaluates the ability of a model to infer these equivalence classes from data which is an important property. - I am not sure if *CONF* is the right venue for this work The paper presents three contributions: (a) the observation that there's train-to-test leakage in many graph classification datasets (under isomorphism equivalence), (b) what appears to be a theoretically motivated way of improving scores on such datasets, by focusing on solving the examples that are isomorphic with training instances, and (c) a recommendation to remove such leakage from test sets.
PAPER SUMMARY: This paper proposes a fast inference method for Gaussian processes (GPs) that imposes a sparse decomposition on the VI approximation of the posterior GP (for computational efficiency) using the KNN set of each data point. This is, however, a somewhat strange direction which, to me, seems to raise extra issues that could have been avoided if one follows the conventional VI approximation: (1) As the posterior surrogate is now directly over f instead of f_I, the number of variational parameters is now proportional to the data size which requires several (redundant) extra approximations including armortized inference & the lower-bound on the entropy term that admits a sparse decomposition. I also find the experiment lacking as comparison with fast approximation method such as [*] that incorporate local information is not included. See Questions. 4) Originality The use of a localized (in the KNN sense) set of inducing inputs to improve GP inference but the impact needs to be better quantified empirically. (1) - (4) in my original review
In this paper, the authors propose the Homotopy Training Algorithm (HTA) for neural network optimization problems. Regarding (2): The authors evaluated their procedure on CIFAR10, a relatively simple image classification task that modern neural networks can solve easily and is not representative of the types of nonlinear optimization problems prevalent in deep learning. Decision and reasoning This paper should be rejected because
This paper presents a new discriminator metric for adversarial attack's detection by deriving the different properties of l-th neuron network layer on different adv/benign samples. However, I have several concerns: Major: 1. It seems that the whole process assumes that there is difference for the parameters in the environment of GGD with adv/benign samples, and the goal is to search for the major components of it and use a classifier to detect.
The paper then follows this framework by constructing deep neural network mapping from "observations so-far" to "sufficient statistics" and by universal approximation theorem this is possible. They consider a general state space model, then use FNNs to approximate the so-called transition equation and observation equation of that SS model. Overall the paper seems to be a straightforward application of universal approximation theorem of deep neural network. Then, the resulting RNN-based filter is shown to be able to approximate well the optimal filter of the original SS model. Based on the well-known universal approximation property of FNNs, the paper shows that their RNN-based filter can approximate arbitrarily well the optimal filter.
Unfortunately, their tasks seem quite easy, and it is hard to assess the impact of their method when working with more real-world data-sets, where the number of instances of every class is more loosely defined (we could always describe more objects in a real image from the COCO dataset for example). Given my concerns on how this unsupervised approach can scale to real-life datasets, I suggest a weak reject, but I think the  proposed method has some interest for the community and I strongly encourage the authors to provide further evidence of performance of their method on more complex vision tasks. The main contribution is the introduction of a differentiable top-K region proposal that allows to train the whole model with only a supervision of the total number of instances (and their class) in the image.
2) Page 6 third line below Theorem 16.
A generic method is proposed with O(1/T) convergence rate, which is also empirically demonstrated with good performance on often-used MNIST and CIFAR-10 benchmarks. This should be clearly surfaced in the introduction and presentation of contributions if DPAR is required for the proposed generalized min max formulation to improve robustness.
With proposed GE bound connecting 'margin' with AR radius via 'singular values of weight matrices of NNs', they present 3 key results with empirical experiments on CIFAR10/100 and Tiny-ImageNet. 1) AR reduces the variance of outputs at most layers given perturbations. Contributions: 1. Derive a generalization bound on the performance of adversarially robust networks  that depends on the margin between training examples and the decision boundary. - After presenting the main result of the paper — Theorem 4.1 — very little intuition is provided about each of the terms in the bounds. ===== Review ===== The problem that the paper addresses is very significant to the robust optimization field and the study of adversarial robustness in neural networks. The paper studies this hypothesis by deriving a bound on the generalization error based on the margin between the samples in the training set and the decision boundary. I an inclined to increase my rating and would suggest to weak accept this paper.
ICML 2019. This paper presents DIMCO, a meta-learner that is trained by maximizing mutual information between a discrete data representation and class labels across tasks. While the authors address the generalization of meta-learning methods for small values of M, they do not address how their model behaves viz-a-viz others when the number of tasks N available for training is small.
Discussion would be helpful. The paper introduces a new concept-based interpretability method that lies in the family of self-interpretable models (i.e. it's not a post-hoc method). The difference comes on how easy it is to interpret the methods, as these other rationale-based text processing methods would make use of captured words, while EDUCE would make use of detected "concept" clusters. The paper evaluates on this measure, which is included in the appendix, and the results are pretty disappointing compared to the existing models such as Lei et al's initial baseline or Bastings et al. While the paper argues this method isn't necessarily designed for this task unlike the other methods, I'm not sure this is necessarily the case. The idea of unsupervised extraction of concepts for interpretability was introduced before (https://arxiv.org/pdf/1902.03129.pdf) and is not discussed by the authors (although the utilized terminology is very similar). The introduced method is well-justified and the use of concept-based self-interpretable models is very useful to the field. It is also interesting to see that the performance is comparable to non-self-nterpretable baselines which would make a case for the use of self-interpretable models. Authors should make a much more comprehensive discussion of what already exists in the concept-based interpretability literature and make the contribution of this work more clear (unsupervised concept extraction for a self-interpretable model instead of post-hoc interpretations).
I have increased my rating.) In this paper, the authors study the generalization bound for GANs based on a new definition of generalization error where the distribution corresponding to the generator is assumed to be known for each generator (i.e., there is no empirical distribution for generators). "Norm-based capacity control in neural networks." Conference on Learning Theory. ---------------------- After rebuttal: I have read the authors' response. I suggest authors to change multiple hyper-parameters and train more networks to improve the evaluation. However, since the generalization bound in Arora et al (2017) is based on a different definition of generalization error where empirical distributions are considered for both discriminators and generators, the comparison seems not fair. This paper proves generalization bounds for GANs. I think the paper can be improved significantly in several ways: 1- Writing: The first two sections are relatively well-written. b)Theorem 2.3 is a general statement but it is followed by Corollary 3.3 which is a very specific generalization bound.
Summary: This paper proposes a modification to the VAE-GAN model where mode coverage is encouraged by passing samples G(Z) through the encoder and minimizing the forward KL between E(G(Z)) and the prior over Z. From the figure, it seemed that LDMGAN improved the sample quality and diversity compared to GANs, but it is still prone to characterizing only a single or a few modes of the data distribution. My Take: This paper's only point of novelty over a vanilla VAE-GAN implementation is the inclusion of the KL(E(G(Z)) || p) term in the generator loss, which is very similar to the idea behind VEEGAN. I argue strongly in favor of rejection.
ICCV 2019. In this paper, the authors tackle the problem of multi-modal image-to-image translation by pre-training a style-based encoder. Compared to the direct baseline BicycleGAN, the training procedure proposed in this paper replaces the simultaneous training of the encoder E and the generator G with a staged training that alternatively trains on E and G and then finetune them together. I think the idea of pre-training a style-based encoder is straightforward. My overall rating is borderline. The current presentation of the paper mostly consists of detailed descriptions of the proposal training procedure, without some interesting discussions about why this pretraining makes the problem easier.
This paper produces a new method call TVmax and presents that the selective visual attention could improve the score in the Image Captioning task. The authors then compare their TVMax approach with softmax and Sparsemax attention for image captioning and show improvements on the MSCOCO and Flickr datasets. The main idea is to augment the Sparsemax projection loss with a Lasso like penalty which penalizes assigning different attention probabilities to contiguous regions in the image. Some questions: 1.From the experiments, the proposed method achieved only a little higher performance than the baseline(softmax). Moreover, for generalization one can use attention dropout which is simpler instead.] This paper proposes two sparsifying methods of computing attention weights, dubbed sparsemax and TVmax, which appear to slightly improve objective and subjective image captioning scores. One problem in this paper is that the author applies their proposed TVmax on Image Captioning  task, however it only achieves a little improvement on the automated metrics compared with the baseline(softmax). Therefore, My decision leans to a weak accept. 2.Could you show some results of TVmax on the other task in order to show the effectiveness of the proposed method?
How do you think? This paper proposes Surprise Minimizing RL (SMiRL), a conceptual framework for training a reinforcement learning agent to seek out states with high likelihood under a density model trained on visited states. However, the statement that SMiRL agents seek to visit states that will change the parametric state distribution to obtain higher intrinsic reward in the future is controversial (see e.g. at the end of Section 2.1), because optimizing a non-stationary signal is outside the scope of the problem formulation. The reinforcement learning problem of maximizing intrinsic rewards does not know how the intrinsic reward signal is altered in the course of the future, i.e. how the parametric state distribution is updated.
The paper proposes an approach to learning domain invariant representations using the adaptive decomposition of the convolutional filters. The methods appear to be a useful addition to tools available for domain invariant learning. * Please do not use the Office dataset, it is commonly used in unsupervised domain adaptation papers, especially older ones, but it's hard to tell anything about proposed methods from this dataset as there is label pollution and not enough samples per class to be used with neural nets. Clearly state your decision (accept or reject) with one or two key reasons for this choice. Weak Accept. * I think the paper was very well written, the explanations were clear and the technical contributions seem sound.
The experiments on the Atari games shows that by using tensor regression to replace the dense layer of the neural nets and using K-FAC for the optimization, one can reduce around 10 times of parameters without losing too much of performance. The second method the authors have attempted is to swap the convolution layer of the deep RL architecture with wavelet scattering. Overall it is a good attempt to reduce the number of weights in the deep RL architecture, but I do think the novelty of this work is a bit thin and the three contributions were not tied together with the main theme of the paper. The performances of wavelet scattering for the reported tasks are weak (better in only one game) and the space saving is not clear.
This work discusses how to set the projection size for each head (head size) in multi-head attention module, especially Transformer. In response to such observations, the paper proposes Fixed Multihead Attention, where the constraint that `head_size * number_of_heads = embedding_size` in standard multihead attention is lifted; and it allows for using more attention heads without making each head smaller. Details: - Theorem 1 presents a rank-based view of each attention head's capacity, which is nice. While I did not have the luxury of time to parse the Appendix to validate the legitimacy of the proof, I think the overall shortcomings of the paper (highly non-readable, bad presentation and perhaps a fair attempt at masking the lack of contribution) warrants a clear reject from me.
Summary. The paper improves the existing feature attribution method by adding regularizers to enforce (human) expectations about a model's behavior. More impressively, the model does outperform all other controls with a good margin in the anti-cancer drug prediction experiment, which is a nice demonstration of that domain knowledge could be incorporated in a neural network training to achieve better performance. Is it just smoothing? The authors proposed the attribution priors framework to incorporate human domain knowledge as constraints when training deep neural networks. It would be nice to see how integrated gradient method perform in the three experiments (image, drug data, mortality prediction), does the expected gradient method always outperform? In section 2.2. I think a few papers to have a look at are a survey article about graph based biasing http://www.nature.com/articles/s41698-017-0029-7 as well as methods for using graph convolutions with biases based on graphs: https://arxiv.org/abs/1711.05859 and https://arxiv.org/abs/1806.06975 . as measured by R^2 (Figure 2 Left). It is not clear if the paper is presenting "expected gradients" or existing attribution priors. Attribution priors as you formalize it in section 2 (which seems like the core contribution of the paper) was introduced in 2017 https://arxiv.org/abs/1703.03717 where they use a mask on a saliency map to regularize the representation learned. A user study would be needed to support that the proposed method can really provide a way to incorporate humans into the modeling process. 2. When the authors refer to Figure 2 and Figure 3 multiple times in the main text, they are referring to either left or right panel. In section 2.2. I think a few papers to have a look at are a survey article about graph based biasing http://www.nature.com/articles/s41698-017-0029-7 as well as methods for using graph convolutions with biases based on graphs: https://arxiv.org/abs/1711.05859 and https://arxiv.org/abs/1806.06975 . The expected gradient method does indeed also performed better than the integrated gradient method in the benchmark (see Table 1.) The results in all three experiments are impressive. Three different datasets (i.e. image, gene expression, health-care) are chosen to evaluate the proposed model's effectiveness, while different regularizers (i.e. image prior, graph prior, and sparsity prior) are explored for the respective task. For example, Figure 1 (left) shows an attribution map that highlights multiple intermittent regions from which I cannot understand its behavior. For the image and the graph data, the prior is basically to have smoother attributions for nearby features; while for the clinical medical data, the authors used the Gini coefficient formula to encourage sparsity, which is of several practical benefits clinical practice. This is a general framework that the users can define different attribution priors for different tasks.
The paper's contributions can be summarized as follows: 1) A data representation that converts the 3D atom locations to a 3D voxel density map, so that they can be encoded by a standard 3D convolutional network. 2) A decoder network that first estimates a 3D density map from the latent vector using upsampling and convolutions and then classifies the atom type (atomic number) per voxel using a 3D segmentation. The paper deals with accurately encoding and decoding 3D atomic positions and the crystal's species using 2 sets of neural networks 2. VV-NET: Voxel VAE Net with Group Convolutions for Point Cloud Segmentation I appreciate that the paper addresses an interesting problem that is not sufficiently explored and can motivate the development of novel methods that generate 3D molecules with particular structure and multiple types of atoms, however the current work combines existing methods, without any architectural modifications that exploit the new domain. Weaknesses ------------------------ 1. Limited methodological novelty: The methods used in the paper, i.e. the data representation (see detailed comments), the encoder network, the decoder network and the segmentation network are all existing methods without any (or with only minor) modifications. Also, it is not clear whether the 3D representation is better than 1D or 2D representations especially since there have been many new 1D models that perform very well for tasks like molecular property prediction (For example All SMILES VAE https://arxiv.org/abs/1905.13343 ). On the other hand, if the authors can show that the errors in atomic numbers suggest they correspond to similar atoms (may be along the same column in the periodic table), then one can have better confidence that the network has learned a meaningful representation. It would be more appropriate to submit this paper to a domain-specific venue rather than to *CONF*.
Based on this observation, they propose a new normalization method not only achieves significantly better results under a variety of attack methods but ensures a comparable test accuracy to that of BatchNorm on unperturbed datasets. Experiments on more datasets and the sensitivity of the proposed method to \\rho would have validated the claims of the authors.
The proposal is inspired from Graph convolution Networks with the idea of overcoming the major drawback of these models that lies of their behavior in case of limited coverage of the labeled nodes, which implies using deeper versions of the model leading at the price of what the authors call the over-smoothing problem. The authors do not change the GCN but extend the self-training portion as per the prior GCN paper by introducing Dynamic Self-Training that keeps a confidence score of labels predicted for unlabelled nodes. 4. If we had soft-labelling or uncertainty on which label each node has, how would the dynamic self-training be changed? Using self training is not new in GCN but the way it is used here, computing adaptively a threshold for incorporating pseudo labels and using weights according to the confidence off predictions is new. Should "three" be "four". Cons: 1. The proposed framework makes modification on the existing work, which is a good extension but the novelty is limited. You may include other additional sections here This paper propose to modify the existing work [1] of self-training framework for graph convolutional networks. Evaluation of the proposed framework is performed on four networks for semi-supervised node classification task with varying label rates.
Similar to the robust MDP work, they also show that this robust RL problem has a robust optimal Bellman operator, and the optimal value function can be computed using  robust policy iteration, which can be extended to model-free actor critic algorithm when state/action spaces are large/continuous. Similar policy gradient and actor-critic algorithms have been derived in risk-sensitive MDPs for mean-variance, mean-VaR, and mean-CVaR optimization (see the references below), but such algorithms for RMDPs is relatively new. I will keep my score, but do believe that further experiments and small adjustments to the writing will see a future version of this accepted.
(Though I am less positive due to the concern of novelty raised by other reviewers.) Summary: This paper presents an efficient stochastic neural network architecture by directly modeling activation uncertainty and adding a regularization term to encourage high activation variability by maximizing the entropy of stochastic neurons. The idea of producing distributions in each layer (i.e., using stochastic layers) is not new and is closely related to the work on local reparameterization trick and variational dropout [1] (predecessor of the cited sparse variational dropout), and various works that directly model neurons as distribution [2, 3].
2) I find it quite interesting that the top-down attention in selective RIM activation is corresponding to the states of these recurrent cells. The reviewer feels that the paper stands at a high level in general, but lacks concrete examples/applications for general readers to appreciate the significance. The related work section helped a bit, but still unclear.
The authors found that in a setting with low budget for hyperparameter tuning, tuning only Adam optimizer's learning rate is likely to be a very good choice; it doesn't guarantee the best possible performance, but it is evidently the easiest to find well-performing hyperparameter configurations. as the main contribution. However, I do not think the metric they introduce is good enough to be recommended in future work, when comparing tunability of optimizers (or other algorithms with hyperparameters). In addition, the proposed stability metric seems not quite related with the above intuitions, as the illustrations (1.a and 1b) define the tunability to be the flatness of hyperparameter space around the best configurations, but the proposed definition is a weighted sum of the incumbents in terms of the HPO budgets. A good prior of one optimizer could significantly affect the HPO cost or increase the tunability, i.e., the better understanding the optimizer, the less tuning cost. The motivation of defining tunability of optimizer is a very interesting question, however, the study seems to preliminary and the conclusion is not quite convencing due to several reasons: In section 3.2, to characterize its difficulties of finding best hyperparameters or tunability, the authors seem to try to connect the concept of "sharpness" of a minima in loss surface to the tunability of an optimizer, which is similar to comparing the loss landscape of minimums. Other comments/notes: * One aspects that is mostly left out of the discussion (except from one side comment) is the wallclock time, as some optimizers might be on average quicker to train (for example due to quicker convergence), this can easily lead it to be quicker to tune even though it requires a higher budget of trials.
>>>  wrong adjective. Summary: The authors extended the regular convolution and proposed spatially shuffled convolution to use the information outside of its RF, which is inspired by the idea that horizontal connections are believed to be important for visual processing in the visual cortex in biological brain. One minor thing, in the main paper, the abbreviation for spatial shuffled convolution (ss convolution) is mentioned multiple times.
Caption of Table 1: (Moreno et al., 2015) > Moreno et al. (2015) I am also wondering why evaluating portions of the dataset where annotations were made by 'sparse' users would not work to highlight the effectiveness of the proposed method for sparse users. If the knowledge of communities is useful, why did the advantage of the proposed method disappear in Figures 1, 2, and 3 when we used all data? It considers the underlying communities of different workers, which may lead to better annotations when the data is scarce. The comparisons are meaningful. The work in interesting and much needed if SOTA indeed is from 2015, but the CommunityMPA results are not strong enough for me to recommend full accept. The experimental results show that, when the data is sparse, the proposed method (CommunityMPA) worked better than MPA on Phrase Detectives 2 corpus in terms of mention-pair accuracy, silver-chain quality, and the performance of the state-of-the-art method trained on the aggregated mention pairs.
The authors then show that the preconditioning matrix of RMSProp and Adam can be used as norm inducing matrices for second order trust region methods. My understanding is that the paper does not claim to deliver some great results here and now but instead suggest a promising direction ("that ellipsoidal constraints prove to be a very effective modification of the trust region method in the sense that they constantly outperform the spherical TR One can show that the empirical Fisher is not an accurate curvature matrix in general, and so there is no reason to believe this would in fact enforce the proper ellipsoidal trust region for the method? The method overall doesn't seem to be able to match first order gradient methods, and it is not clear whether this is because of using the RMSProp/Adam preconditioner as a curvature matrix. I decide to raise my rating to 3.
Their evaluation setup largely follows the style of Liu et al., but they construct a different subset of ILSVRC validation set, and some of the model architectures in their ensemble are different from Liu et al. Their results show that by including the distilled logits when computing the gradient, the generated adversarial examples can transfer better among different models using both single-model and ensemble-based attacks. In overall, I liked its novel motivation and simplicity of the method, but it seems to me the manuscript should be improved to meet the *CONF* standard. This paper proposes distillation attacks to generate transferable targeted adversarial examples.
The authors use a regularized reward function that minimizes the divergence between the policy of the expert and the one followed by the agent. Given a set of expert demonstrations, this work provides a policy-dependent reward shaping objective that can utilize demonstration information and preserves policy optimality, policy improvement, They use demonstrations obtained from a trained agent and experiment their method on several mujoco tasks. The end of the related work section is not very clear, you say these methods are problematic because "the adopted shaping reward yields no direct dependence on the current policy" but there's no explanation or motivation for why that would be a problem. Finally, they only test on mujoco tasks which are very specific tasks with deterministic dynamics and very dense rewards  around states visited by the optimal strategy so initializing with an expert policy that is learned from demonstrations of a similar network of course helps. The related work section should also discuss and compare/contrast to GAIL, I was surprised that wasn't in there, especially since you also use a discriminator to differentiate expert and agent actions. It is already studied in several works and more generally it comes with some assumptions on the policy update. The revised version of the paper addresses many of my concerns about the motivation, related works, and comparisons with GAIL, so I'm updating my score to Weak Accept. They actually propose exactly the same framework as a special case in the appendix of that paper. Converting the tasks to sparse reward in this way makes them partially observable, and then potentially the expert demonstrations are required to overcome that partial observability. It is not clear that in what type of MDPs the optimal stochastic policy exists and it can satisfy Asm. 1. The provided theorems are not compatible with the MDP where only deterministic optimal policy exists. I would be more impressed by experiments on stochastic environments and sparse rewards.
In this paper, the authors propose a new adaptive gradient algorithm AdaX, which the authors claim results in better convergence and generalization properties compared to previous adaptive gradient methods. This paper points out that existing adaptive methods (especially for the methods designing second-order momentum estimates in a exponentially moving average fashion) do not consider gradient decrease information and this might lead to suboptimal convergences via simple non-cvx toy example. 3. In the experiment, it is not clear that the authors use the same strategy for constructing first-order momentum for Adam with a newly introduced parameter β3. I am sticking to my score.
This paper proposes to provide a novel gradient-based meta-learning framework (Meta-Graph) for a few shot link prediction task. Moreover, it would be nice if the authors could also provide some ideas for future research directions, such as the prospects of using their approach for improving link prediction models and incorporating Meta-Graph in other domains like molecules structure. My concerns are as follows: •    I am wondering if you can adopt R-GCN [1] instead of the GCN model for extending the Meta-Graph to multi-relational graphs? •    I suggest providing a comparison of computational complexity between Meta-Graph and the baselines.
The key idea is to create the smaller possible subset with the same or similar coverage (in the paper the neurons coverage is considered) and output distribution, maintaining the difference below a (small) threshold. In this paper, the work is done by adding a test-sample search algorithm on top of the HGS algorithm to balance the output distribution. In this way, an estimate of the output distribution is considered during the extraction of the representative subset of the testing data. More explanation is needed. 2. In Table 2, the authors try to compare the output distribution. Finally, it would be interesting to know the runtime to obtain the subsets of the test data of Table 2 required by the considered systems. Furthermore, the result does not present a strong success: the error of output distribution is much worse than the compared work. The paper develops methods to reduce the test data size while maintaining the coverage and the effectiveness on large test data. To better demonstrate the change between raw testing set and proposed subset, I think that it could be better to present the metrics of distribution or the accuracy of each class instead.
Additionally, they also notice that fine-tuning a _linear model_ using the learned features for the wider networks provide better accuracy for new (but related) tasks over the shallower counterparts. Given that there are many levers in a neural net (batch norm, architectural choices, hyper-params etc.) one could fiddle with, to make the claim made in the introduction one needs a more extensive set of experiments. However, I tend to reject this paper since it doesn't show very compelling evidence through experiments.
This paper proposes a novel way to increase efficiency for self-attention based sequence modeling neural networks. With a content-based "routing" technique ("group/cluster attention" may be a more accurate term to differentiate with the dynamics of Capsule Networks), the time steps of each layer form different clusters, and the self-attention mechanism is only performed within each such cluster. However, since the cluster centroids are learned parameters, does this mean the trained routing Transformer cannot easily generalize to different sequence lengths easily? - What is the *actual* running time/memory for local/routing/full attention layers? Are we then, in language modeling tasks, performing attentions among words only around these "anchors"? I find the discussions around NMF to be somewhat orthogonal, especially considering the paper does not use NMF techniques for their clustering algorithm in section 4.2. How does the model do if it only uses routing attention? In particular, although the routing mechanism avoids computing A, having to deal with each of the n clusters means that one will need to process the cluster-based attentions sequentially (and, as mentioned, you have 8 heads for local attention and 8 for routing attention, which I think are also processed in two steps? 7. The paper proposes to use n clustering centers, which makes sense from the perspective of minimizing total computations. It seems like the routing layer requires additional operations (i.e. online k-means) which could increase running time. The routing Transformer seems to be able to do very well on the WikiText-103 word-level language modeling task. Picking the initial centers for k-means clustering has been long known as an interesting/challenging problem, especially in a high-dimensional space (which tasks like language modeling deal with). + The paper addresses a very important (efficiency) question in Transformers, and gave a proper review to prior related works (such as Child et al.). Summary: This paper proposes a new mechanism for computing the attention scores efficiently in Transformers to avoid the quadratic computational cost in conventional Transformers (i.e., when computing the QK^T for self-attention).
This paper proposes a very interesting idea of loss function optimization. If these are two different goals, then you should have two sets of experiments analyzing how GLO can find interesting (and perhaps different) loss functions for each. They find that applying their EC method to mnist yields an interesting loss function that they name the 'Baikal loss.' Much of the paper is devoted to analyzing the properties and performance of the Baikal loss. It would be good to show that loss functions meta-learned on mnist generalize to larger-scale problems than cifar.
The paper introduces a general framework for behavior regularized actor-critic methods, and empirically evaluates recent offline RL algorithms and different design choices. This paper proposes a unifying framework, BRAC, which summarizes the idea and evaluates the effectiveness of recently proposed offline reinforcement learning algorithms, specifically BEAR, BCQ, and KL control. Results from a  thorough series of experiments are presented which suggest that certain details of recently proposed RL methods are not necessary for achieving strong performance. This paper presents a framework for evaluating offline reinforcement learning (RL) algorithms. And I think the authors did a good job addressing a difficult problem. Overall, this paper could be an interesting summary of prior works in offline RL and provide some empirical insights on the effectiveness of each building block in the previous approaches, though it neither offers theoretical explanations nor proposes a new offline RL algorithm that outperforms the existing methods under the BRAC framework. I commend the authors for performing a valuable test and comparison of existing offline RL methodology. The authors provide extensive results; but it wasn't clear whether these were "apples-to-apples" comparisons with the previous results in the papers that proposed the "unnecessary" technical complexities. This paper could be improved by providing more clear insight and intuition about the deeper meaning of these results regarding the "unnecessary" technical complexities. Does the BRAC framework reproduce the results for previous papers? While the experimental results demonstrate some interesting phenomenons such as combining vp and the primal form of KL divergence achieving the best performance and taking minimum over Q functions outperforming using a mix of maximum and minimum over the Q functions, I believe the paper would be greatly improved if the authors can provide a new offline RL method based on the BRAC that can achieve better performance than current approaches and is less incremental than simply combining vp and KL divergence. There are some comments for the experiments. For example, I didn't see the authors say that they reproduced the results of previous works, only that they tested previous methods in certain tests. I am leaning to accept the paper because (1) the experimental design is rigorous and the results provide several insights into how to design a behavior regularized algorithm for offline RL. If so, this should be made more clear and stated prominently in the paper so that the reader knows that BRAC is, in this reproduction of previous results sense, reliable. Could the authors suggest why certain complexities are unnecessary? Minor issue:  In the Conclusion Section, the authors say, "Unfortunately, off-policy ... If not, how if the reader to know that the "unnecessary" technical complexities, are truly unneccessary?
+ The proposed model adopts the multi-task learning idea to represent each sequence in the training set. The experiments on the synthetic data and the ocap dataset show the customization is beneficial for prediction tasks and enables for style transfer and morphing within generated sequences. - Separate parameterization of the latent variable z for different tasks seems to be a key idea in this paper.
This paper tackles the out of distribution detection problem and utilizes the property that the calculation of batch-normalization is different between training and testing for detecting out-of-distribution data with generative models. The paper first empirically demonstrates that the likelihood of out-of-distribution data has a larger difference between training mode and testing mode, then provides a possible theoretical explanation for the phenomenon. One experiment that is missing from your paper (and would prove my hypothesis wrong) would be if you adapted the OoD criteria to compare the *mean* likelihoods in the evaluation mode, and show that for OoD datasets, the difference between batches still remains small. This is consistent with your batch normalization experiments: in training mode, the likelihood is computed from mean activations over a batch of OoD samples, several of which probably contribute to the low likelihoods. We recommend a weak accept. If you take a likelihood model and evaluate on 64 samples from SVHN, you are all but guaranteed to sample a sample with *exceedingly* low likelihood, which dominates the mean statistic, making it possible to separate SVHN batch from CIFAR10 batches.
To model functions s and t In this paper, a GraphNVP framework for molecular graph generation is proposed. Overall, I think the method proposed in this paper sacrifices some more import aspects in graph generation such as novelty and uniqueness by introducing an invertible flow architecture. I shall improve my rating if GraphNVP is applied to general graph structures - synthetic / real.
Summary: this work uses tensor methods to improve graph convolution for dynamic graph, where the nodes are fixed and the edges are changing. + Improved prediction results on 3/4 real-world dynamic graph datasets Cons & Questions: 1, My first concern is that M-product formulation does not bring any new insights as people have already used some of the key elements in practice for a long time. Given the current status, I could not accept the paper. However, if in practice, you do not need the inverse transform, then do those theoretical properties still hold and what is the meaning of introducing such M-product formulation?
The paper proposed ``TriMap''---a novel dimensionality reduction technique that learns to preserve relative distances among points in a triplet. The illustrated S-shape example in Figure 1 somehow demonstrate the difference of the proposed method with PCA, t-SNE and UMAP, but the usage of the embedding is not clear since Figure 1(d) looks like a 2-d visualizing of the original 3-d data visualized from certain angle. However, the experimental section can be stronger if the authors can show the learned embeddings are better at helping some downstream tasks than other baselines (by preserving non-local information?).
The notion of feature robustness, which is a notion the paper proposes, connects the flatness measure to generalization error. **************************** After author rebuttals: Author have added discussion of related work which was missing in the original submission (thanks!). Atleast on CIFAR10 and MNIST it is possible to achieve training loss <1e-4 so am not sure if the networks that the authors are testing are minima at all (It is important for them to be minima since the flatness measure is only defined at minima). This seems to be a missing step in relating flatness/feature robustness of a layer to the generalization of the whole network. Otherwise, it is hard to claim that this paper connected the modified flatness measure to generalization error. Additionally and importantly, the Fisher-Rao norm has a direct connection with the size of input gradients, which has a strong relationship with the feature robustness. However, the other two issues are still present. They did address some of my concerns.
[Summary] This paper proposes an interactive agent that tries to infer the underlying causal structure by interacting with the environment; the authors called it "causal induction."  The inferred graph will later help the agent complete goal-directed tasks referred to as a "causal inference" stage. I was a bit disappointed to see that training is mostly supervised (both providing the ground truth causal graph and an oracle policy as target) but on the other hand it is impressive to obtain these results with raw images as input and the comparative results are good. The authors can directly read this information from the synthetic environments used in this paper, yet, in more complex real-world situations, we might not know the underlying causal structure for supervised training the induction model. For learning the goal-conditioned policies, the authors also assume that they have access to the ground truth causal graph. * On page 1, the authors claim that empirical evidence suggests the lack of correct causal modeling is an important factor for lack of generalization, generation of unrealistic captions, and difficulties in transfer learning. My rating is weak reject but I am ready to upgrade with appropriate explanations answering the above questions.
The authors claim that the proposed method for semi-supervised graph learning is based on data augmentation which is efficient and can be applied to different graph neural architectures, that is an architecture-agnostic regularization technique is considered in the paper. • The performance gain is from mostly the use of a semi-supervised learning approach based on entropy minimization, which has been developed without consideration of graph-structured data. The authors may want to further evaluate their framework on those GNNs such as GMNN and Graph U-Net to make the experiments more comprehensive, or justify on why they cannot be implemented using the proposed framework.
The authors analyze possible causes, and for the first time present two simple yet effective "black-box" methods to calibrate the GAN learned distribution, without accessing either model parameters or the original training data. It proposed a new metric to measure the diversity of the generative model's "worst" outputs based on the sample clustering patterns. This paper presents a set of statistical tools, that are applicable to quantitatively measuring the mode collapse of GANs. The authors consistently observe strong mode collapse on several state-of-the-art GANs using the proposed toolset. Unlike most existing works that address the model collapse problem, a blackbox approach does not make assumptions about having access to model weights or the artifacts produced during model training, making it more widely applicable than the white-box approaches. 1) I wonder if the proposed method work for most GAN models, more experiments evaluated on more recent GAN-based  models should be added to verify the superiority claimed in this paper, e.g., TP-GAN [Huang et al., ICCV 2017], PIM [Zhao et al., CVPR 2018], DR-GAN [Tran et al., CVPR 2017], DA-GAN [Zhao et al., NIPS 2017], MH-Parser [Li et al., 2017], 3D-PIM [Zhao et al., IJCAI 2018], SimGAN [Shrivastava et al., CVPR 2016], AIM [Zhao et al., AAAI 2019]. Rather than using the labels, the authors use an off-the-shelf model (on faces) to provide a space on which to measure distances between generated images. For those reasons, I propose to REJECT this paper.
On the sequence generation task (translation), the authors showed that the proposed KA strategy achieved better performance compared with KD based methods when distilling the knowledge from a teacher model to a student model. [Cons] The main concern about the proposed method is whether it can be used as a generic strategy for transferring the knowledge from the teacher model to student model. [Pros] 1. The authors proposed a new strategy to perform the knowledge distillation from a teacher model to student model for sequence generator.
This paper proposes a novel model of recurrent unit for RNNs which is inspired from tensor product representation (TPR) introduced by Smolensky et al. in 1990. The main reasons are: 1. The advantage in terms of accuracy of the proposed approach seems marginal in the experiment, and the analysis of the interpretability of the learned representations could be improved: loosely speaking, particular examples of interpretability are given but sometimes without contexts or baselines to compare to (see the two last comments below). For example, tensor product representation (TPR) is introduced in the second paragraph of Introduction. More specifically, the paper claims TPRU's interpretability by Table 5. I think this paper is not yet ready for publication: the proposed model is interesting and relevant but its validity could be better assessed and the paper needs some thorough proof-reading.
The paper proposes a new way of stabilizing Wasserstein GANs by using Sinkhorn distance to upper-bound the objective of WGAN's critic's loss during the training. [p.8] Discussions -> Discussion This paper proposes bounding the Wasserstein term in WGAN with an aim to stabilize training. The paper thus requires further work in terms of design, theory and experiments; for instance the choice of Sinkhorn distance as a boundary-heuristic might be too limiting. Although the main bounding strategy seems to be a promising idea, this work does not meet the quality requirements of *CONF*.
The main contribution of this paper is to apply the state-of-the-art Transformer model and other techniques in NLP to address the specific issues in retrosynthesis. The main contributions are data augmentation techniques, pre-training and a mixture model that seems to improve performance on the USPTO-50K dataset. This is a well written paper with good baselines.They use multiple techniques that are both domain specific (data augmentation) as well as methods from NLP adapted for this task.
The idea of the paper is good and Algorithm 1 sets out to learn the exploration policy when the expert policies do not agree.
The paper presents an idea on making adversarial attack on deep learning model. This paper proposed a BO-based black-box attack generation method. The authors in [1] consider using Bayesian optimization to make adversarial attack for model testing. ############ Post-feedback ########## Thanks for the clarification and the additional experiments. That is, one can solve problem min_{\\delta} attack_loss( x, y, g(\\delta) ) by estimating gradients via finite-difference of function values, where g(\\cdot) is the dimension-reduction operator, and \\delta is the low-dimensional perturbation.
This paper investigates the impact of using a reduced precision (i.e., quantization) in different deep reinforcement learning (DRL) algorithms. I wonder if the quantization during training has a regularization effect, which is known to improve agent's performance in reinforcement learning (e.g., Cobbe et al., 2018, Farebrother et al., 2018). There's also an important distinction in the results that is not discussed in the paper: DQN estimates a value function while methods such as PPO directly estimate a policy. It shows that overall, reducing the precision of the neural network in DRL algorithms from 32 bits to 16 or 8 bits doesn't have much effect on the quality of the learned policy. - I'd introduce/explain quantization in the beginning of the second paragraph of the Introduction for those not familiar with the term.
*CONF* 2019 This paper proposes to use a 'slot-based' (factored) representation of a 'scene' s.t. a forward model learned over some observed transitions only requires sparse updates to the current representation. Specifically: 1) It is unclear what this paper is claiming to improve over prior work: is the goal to a) learn a good forward model, or b) show that emergent entities allow better downstream tasks. 3) While the experiments in Sec 4.2 clearly demonstrate the benefits of the approach, the ones in Sec 4.1 and 4.3 are less convincing: 3a) Sec 4.1 shows that the slot based transition model generalizes better, but this is only in comparison to a naive fully-connected baseline. The results show that jointly learning the forward model and the scene representation encourages meaningful 'entities' to emerge in each slot. Given some improvements, this work might become quite promising, but for the time being I am leaning against publication.
As a result, the paper demonstrates that this set of entity embeddings are highly useful, and they were evaluated on 1) entity-level typing 2) entity linking 3) few-shot category reconstruction 4) answering trivia questions (TriviaQA). Few examples are, even though in entity linking results (Table 1) the model achieves 83.0 with other papers achieving 90.9.
This paper considers how we can train image classification models so that they can ignore task irrelevant features. (approach) Actdiff: 1) The Actdiff loss requires a mask that highlights areas of the input image which have signal and not distractor regions. The authors then proposed using Actdiff loss, Reconstruction loss, and Gradmask loss that are designed to suppress the effect of irrelevant features. By designing actdiff loss and reconstruction loss, the authors demonstrate that classifiers are likely to predict using features unrelated to the task and their losses can mitigate this problem. (evaluation - Medical Segmentation Decathalon) The actdiff loss makes sense and masking + activation mapping have not been tried together before to my knowledge. Quality: Quality is mixed. Lots of relevant experiments are reported but they don't support clear conclusions and I'm not sure how well the models were tuned. [1]: Gatys, Leon A. et al. "A Neural Algorithm of Artistic Style." ArXiv abs/1508.06576 (2015): n. Suggestions --- * This paper would be a bit more convincing if it started with a concrete example of the problem illustrated on some dataset (e.g., maybe an example from Gradmask). I expect the authors to design the more effective experiments in the future version.
The paper presents an unsupervised approach for learning landmarks in images or videos with single objects by separating the representation of the image into foreground and background and factorizing the representation of the foreground into pose and appearance. In this paper, authors propose to design an unsupervised learning framework, which can capture pose representation by reconstructing images or videos. Overall the paper is well written, easy to follow, presents a straightforward extension of previous work and appears to show an improvement. The paper presents an unsupervised method to get disentanglement of pose, appearance, background from both images domain and video domain. Their methods let the network focus more on the foreground to regress the landmark and improve state-of-the-art performance on landmark regression (unsupervised.), video prediction and image reconstruction.
- Finally, I suggest revising the very vague title to the paper This paper tries to analyze the similarities and transferring abilities of learned visual representations for embodied navigation tasks. Each experiment compares two highly similar tasks - as the authors themselves acknowledge - in ways that do not obviously connect to realistic transfer scenarios, such as transferring to a new environment. I hope the comments are useful for preparing a future version of this work.
The authors propose using influence functions to efficiently estimate pointwise confidence intervals for regression models. The authors propose an algorithm, "discriminative jackknife", based on the standard jackknife confidence interval estimate which they augment by a "local uncertainty estimate" based on the variability of the n leave-one-out fitted versions of the underlying algorithm (n = # data points). https://arxiv.org/abs/1705.08292 The authors provide an interesting study on uncertainty estimation for deep learning for regression problems. This paper studies how to construct confidence intervals for deep neural networks with guaranteed coverage. The paper also elides the fact that the global minimizer theta^ cannot in general be computed in non-convex models like neural networks. Next, they explain and then develop the concept of higher order influence functions. (ii) The claims of guaranteed frequentist coverage are not backed up as, according to thm.2, they only hold when n >> 0 and the number of influence functions used goes to infinity (ideally, the authors would provide non-asymptotic bounds as in [1], but at the very least, these limitations should have been clearly pointed out and their practical implications discussed). It seems that [1] already proposes use of the higher order expansions, provides an efficient implementation based on forward mode autodiff (do you plan to release code?), and moreover provides non-asymptotic bounds which are not present in your work?! I suggest that this paper is weak accepted. Note that I am not suggesting the above method is perfect either (it completely ignores the actual sizes of the intervals), but I'm currently having trouble interpreting the results you report, so it would be very helpful to understand how you selected this particular measure of "discriminative power" and why alternatives like the example above were discarded please.
This paper reports the BLEU score of WMT17 Chinese-English dataset for 32.3, which significantly outperformed the best score, and improved the performance of existing state-of-the-art results. The paper proposes an approach to train NMT models on extremely large parallel corpora. What would be the performance of an ensemble of 10 models trained with the regular 20M parallel sentence pairs? In Section 5.1 the paper mentions "the single-model achieves a BLEU score of 29.7, already outperforming the current best system", but in Tables 1 and 2 it seems that the best score with single model is 28.7, not 29.7. Note that back translation only needs monolingual data, while the pretraining in this work needs bilingual sentence pairs. This work conducts a large scale study on pretraining for neural machine translation.
The authors provide a theoretical discussion that both DAT and UMixUp converges to be equivalent to each other when the number of training samples becomes infinity. This paper introduces directional adversarial training (DAT) and UMixUP, which are extension methods of MixUp. DAT and UMixUp use the same method of MixUp for generating samples but use different label mixing ratios where DAT retains the sample's original label. arXiv 1801.02929 This paper proposes a novel data augmentation method, untied MixUp (UMixUp), which is a general case of both MixUp and Directional Adversarial Traning (DAT). Since UMixUp is also focusing on the mixing ratio between two training samples, Between-Class Learning should have been compared to the proposed method.
This paper proposes an extension of the conditional GAN objective, where the generator conditions on an attention map produced by the discriminator in addition to the input image. pg. 4: "like random noisy" -> "like random noise" I imagine the generator could rely too much on the attention map as a result - how this is alleviated/prevented should be explained. I was struggling to understand precisely how the StarGAN results were obtained on CityScapes: As a multi-modal image-to-image translation model, StarGan takes as input, an image and a binary vector pointing to which modality to transform the image into.
The manuscript discusses a metric-learning formulation as a quotient for reconstruction-error terms, how to optimize the quotient based on results from Wang et 2014, an iterated reweighted approach to circumvent the non-smooth part of the l1 loss in zero, and experiments on brain images of Alzeimer's disease. Brand et al., 2018; Lu et al., 2018) as example approaches which, despite their effectiveness, present an added complexity to the problem." is better written as "This problem is excessively studied by (Wang et al., 2012; This paper proposed an unsupervised method to make better usage of the inconsistent longitudinal data by minimizing the ratio of Principal Components Analysis and Locality Preserving Projections.
The authors present experiments on two tasks showing that keeping overall model size fixed (i..e, number of parameters), transformers with shorter (but denser) binary codes achieve better performances than standard transformers using one-hot encodings. The paper proposes to use codes based on multiple hashing functions to reduce the input and output dimensions of transformer networks. While the technical contribution is limited, because most of the principles are already known or straightforward, the main contribution of the paper is to show that random hash functions are sufficient to create significantly shorter codes and maintain good performances. - following the comment above, and assuming I understood correctly: There is an originality on the paper compared to other works that use binary codes/bloom filters: In the current paper, the authors actually predict the result of individual hashing functions. For instance, if there are m hash functions taking values in {1, ..., P}, an approach based on Bloom filters would predict a binary output of dimension P, while here there are m multiclass problems with P classes (IIUC). Since the method is applicable to any problem involving natural language data (and more generally categorical values, such as knowledge base completion), I would have expected experiments on tasks with a well-defined state-of-the-art. The authors borrow from Bloom filter the way to create the codes using random hash functions, but the analogy stops here.
[Original reviews] This paper proposed to modeling image as the combination of a GAN with a Deep Decoder, to remove the representation error of a GAN when used as a prior in inverse problems. Authors devote themselves to remove the representation error of the GAN image prior. Summary: This paper proposes to use a combination of a pretrained GAN and an untrained deep decoder as the image prior for image restoration problem. The method is evaluated on compressive sensing and super-resolution, where a better performance than the isolated use of Deep Decoders and GAN priors. Therefore, I recommend weak reject.
The paper presents a new approach, SMOE scale, to extract saliency maps from a neural network. Edit: The authors have answered most of my concerns and I am happy to re-evaluate my score to weak accept. In summary, this paper presents a nice way of generating saliency maps from activations inside a network. I Summary The authors present a method that computes a saliency map after each scale block of a CNN and combines them according to the weights of the prior layers in a final saliency map. The paper gives two main contributions: SMOE, which captures the informativeness of the corresponding layers of the scale block and LOVI, a heatmap based on HSV, giving information of the region of interest according to the corresponding scale blocks. This paper is interesting in that it provides a different way to extract saliency maps from a network.
A very related work to this paper: L_{DMI}: A Novel Information-theoretic Loss Function for Training Deep Nets Robust to Label Noise, where no restrictions have been made on the class-dependent transition matrix and the proposed method does not need to estimate the transition matrix. See also the public comment posted by Nontawat when a special symmetric condition is assumed on the surrogate loss function. Novelty: The paper introduced peer prediction, an area in computational economics and algorithmic game theory, to learning with noisy labels. Though I'm not that familiar with learning from noisy labels, I think it is a good paper and I suggest to accept. I tried several times to go through the details but failed.
- Du et al. Learning to ask: Neural question generation for reading comprehension. The proposed pretrained model is then used to finetune on a standard question generation task, which is then used to generate synthetic QA pairs for data augmentation in QA. (By the way, I am giving 3 as a rating although my actual rating is closer to 4 or 5, because 4 or 5 is blocked in the review system. Regarding Section 2.1 1) Did you attempt to predict 'number of answer candidates' by regression or classification? -------------------------------------------------------------------------------------------------------------------------------------------- Now, here are clarification questions. **** Update on Nov 10 **** Increasing the score to 6. I am happy to increase the score after rebuttals.)
It first formulates the problem as a MDP, where the agent takes actions to explore the environment and has two special actions (Answer_True, and Answer_False) to indicate that the agent has made a prediction about the validity of the hypothesis. The paper looks into the problem of training agents that can interact with their environments to verify hypotheses about it. Overall, I enjoyed reading this paper and thought that it provided an interesting take on the question of how to train agents that can appropriately gather information about their environments. However, (1) the paper lacks any discussion of related work in terms of causal reasoning and partial observability, and (2) the experiments and analysis seem weak. I was though encouraged to see that one of the environment seemed to require slightly different setting in the pre-training reward setup, however the authors didn't follow up with some analysis on why there was such a difference. Similarly, the setup of the MDP in the paper is actually a POMDP, where the state includes the truth value of the hypothesis but where observations do not include this information. It would have been better to also present problems with fairly different settings (e.g. much different - sparser and/or denser - types of reward function), rather than evaluating multiple times on effectively the same grid-world. In particular, I still feel the paper lacks sufficient discussion of the literature on causal reasoning. ================== #Post Rebuttal Remark I have gone through the authors' response and I thank them for it, particularly for making some of the suggested enhancements. At this point, I cannot recommend the article for acceptance, but I'd be willing to change my rating if the authors were to address some of the above points. - The difference in performance on each environment with different pre-training reward function (only one in show in the paper right now)
Motivated by the observation that powerful deep autoregressive models such as PixelCNNs lack the ability to produce semantically meaningful latent embeddings and generate visually appealing interpolated images by latent representation manipulations, this paper proposes using Fisher scores projected to a reasonably low-dimensional space as latent embeddings for image manipulations. As mentioned in 1), it's obvious that Fisher scores contain more information than latent activations for deep autoregressive models and are better suited for manipulations. When the α is small, the learned decoder will function similarly to the original pixelCNN, therefore, latent activations produce smaller FID scores than projected Fisher scores for small α's. This separate powerful decoder has nothing to do with PixelCNN, which is the major reason that I vote reject.
Summary: This paper proposes a clustering attention-based approach to handle the problem of unsmoothness while modeling spatio-temporal data, which may be divided into several regions with unsmooth boundaries. However, three of them - DCRNN, GeoMAN, and ASTGCN - use important elements of the authors' own design, namely attention-based  models and encoder-decoder architectures (GeoMan uses both).
This work proposes an outlier detection method based on WAE framework. This paper proposes a novel outlier detection approach, based on Wasserstein auto encoders. 2.I agree with authors point that WAE is a better choice than VAE for outlier detection because, former "encourages the latent representations as a whole to match the prior ". Eg: I. Chong You, Rene Vidal, Provable Self-Representation Based Outlier Detection in a Union of Subspaces, CVPR 17 Post Rebuttal: Authors have partially addressed my concerns. Overall, I am hesitant to recommend the paper before cross-checking the issue with contamination proportion and learning more about how a VAE framework is indeed important for anomaly detection.
1. Correctness: Classifiers have higher accuracy on explanation-masked images than on images they were least confident on (the ones used to fill in the background). ------------------------- BEFORE rebuttal The paper proposed different metrics for comparing explainers based on their correctness (ability to find most relevant features in an input, used in prediction), consistency (ability to capture the relevant components while input is transformed), and the confidence of the generated explanations. The quality of the paper is my reason for the low rating. This paper proposes 3 such explanation evaluation metrics, correctness, consistency, and confidence.
[2] MuJoCo: A physics engine for model-based control, Emanuel Todorov, Tom Erez Yuval Tassa The paper proposes a weak supervision method to obtain labels from functions that are easily programmable, and propose to use this for learning policies that can be "calibrated" for specific style. My main concern here is the technical novelty of the proposed method: it seems that once we have the labels (which are limited to programmable functions), all we need to do is to learn a policy that conditions on the labels. - Again Table 4, if the "-" is indeed a typo, then CTVAE-style is actually performing better than the baselines, especially for Cheetah (normally one wants negative log-likelihood to be as close to 0 as possible). For Cheetah, it would be easy to report p(y) as a function of the target forward speed, that would give readers a sense of diversity for each label.
The submission proposes to reduce the memory bandwidth (and energy consumption) in CNNs by applying PCA transforms on feature vectors at all spatial locations followed by uniform quantization and variable-length coding. A lossy transform coding approach was proposed to reduce the memory bandwidth of edge devices deploying CNNs. For this purpose, the proposed method compresses highly correlated feature maps using variable length coding. However, the paper and the work should be improved for a clear acceptance: - Some parts of the method need to be explained more clearly: – In the statement "Due to the choice of 1 × 1 × C blocks, the PCA transform essentially becomes a 1 × 1 tensor convolution kernel", what do you mean by "the PCA transform becomes a convolution kernel."? My concern with the paper is two-fold: 1) The major technique of transform-domain coding is borrowed from previous work (e.g., Goyal 2001), hence the novelty of the proposed method is in doubt.
A new framework for certification is proposed, which allows to use different distributions compared to previous work based on Gaussian noise. From this framework, a trade-off between accuracy and robustness is identified and new distributions are proposed to obtain a better trade-off than with Gaussian noise. Also, the reported certified accuracy for Salman et al.'s model for L_inf on CIFAR-10 reported in the original paper is 68.2 at 2/255, which is very far from the 58 in Table 3. Is it a typo? In any case, the values reported for the proposed model in Table 3 are only a marginal improvement over Figure 1 (left) in Salman et al. (2019), just going by the trivial \\ell_2 to \\ell_\\infty certificate. Typos in Table 2.: the columns 2.0 to 3.5 are mislabeled The paper introduces an improvement to the randomized smoothing analysis in Cohen et al. (2019), using Lagrangian relaxation to achieve a more general lower bound. Salman et al.'s have achieved better certified accuracy under the L_2 norm so it would only seem natural to use their model. 3. On p.5, why was the toy classifier sphere-based? I think the paper should be rejected because (1) For \\ell_2 perturbations, there is no major difference between this new family of distributions (d-k \\chi^2) and a Gaussian with different variance.
General: The paper proposed to use a causal fairness metric, then tries to identify the Pareto optimal front for the vectorized output, [accuracy, fairness]. This paper proposes a method to approximate the "fairness-accuracy landscape" of classifiers based on neural networks. As main contributions, the paper provides: * A Pareto objective formulation of the accuracy fairness trade-off There is nothing specific in the method or analysis that relates to neural networks, except for the use of the causal estimand in a 'hidden layer'. However, there may be several problems with this approach: 1) For a causal estimate to be valid we need several assumptions. However, I found the writing / notation imprecise at times and the experimental section too small (lacking an extensive set of baselines, and only on two datasets). For these reasons, I give it a Weak Accept.
The proposed learning objective in this work optimizes not only the cross entropy loss, but also the difference between the CD score of a given feature and its explanation target value.
If the original space is not structured or doesn't naturally have a good distance measure, then the proposed method cannot work. ###### Overall Recommendation I vote for the "Weak Accept" decision for this paper. It would be interesting also to check how stable is the method (i.e. the losses) for different underlying network architectures, and also how RDP compares to very basic approaches employing dimensionality reduction algorithms such as t-SNE or UMAP.
Conventionally, the sensor placement strategy is tasked to gather the most informative observations (given a limited sensing budget) for maximimally improving the model(s) of choice (in the context of this paper, the neural networks) so as to maximize the information gain. What the authors have done differently is to consider the use of neural nets (as opposed to the widely-used Gaussian process) as the learning models in this sensor placement problem, specifically to (a) approximate the expectation using a set of samples generated from a generator neural net and to (b) estimate the probability term in the entropy by a deterministic/inspector neural net. This paper describes a sensor placement strategy based on information gain on an unknown quantity of interest, which already exists in the active learning literature. Consequently, it is not clear to me whether their proposed strategy would be general enough for use in sensor placement for a wide variety of environmental monitoring applications. The authors have performed some simple synthetic experiments to elucidate the behavior and performance of their proposed strategy. (b) What do the authors do with the new observations obtained from placing the sensors in the last experiment?
The paper presents a visually-guided interpretation of activations of the convolution layers in the generator of StyleGAN on four semantic abstractions (Layout, Scene Category, Scene Attributes and Color), which are referred to as the "Variation Factors" and validates/corroborates these interpretations quantitatively using a re-scoring function. A scoring function is obtained to quantify (Equation 1) how the corresponding images vary in a particular semantic aspect when the latent code is moved from the separation boundary. This is repeated at every layer of the GAN generator and the same lamda is used to perturb the resulting output code from the separation boundary. Some of my primary concerns were regarding the presentation, and I feel they have been mostly addressed with the changes to the introduction and abstract (I'd still recommend using 'layerwise latent code' instead of 'layerwise representation' everywhere in the text). Are the four semantic abstractions decided based on the desired output? The paper analyzes the relation of various scene properties w.r.t the latent variables across layers, and does convincingly show that aspects like layout, category, attribute etc, are related to different layers. leads the reader to believe that the findings here are generally applicable e.g. the sentence "the generative representations learned by GAN are specialized to synthesize different hierarchical semantics" should actually be something like "the per-layer latent variables for StyleGAN affect different levels of scene semantics". The results showing scene property manipulation e.g. in Fig 4 are obtained by varying a certain y_l, and it'd help to also show the results if the initial latent code w was modified directly (therefore affecting all layers!). Then, 2000 top positive examples and 2000 top negative examples identified by the image classifiers are used to train a linear SVM, i.e., a binary-SVM all the four scene abstractions ("Varying Factors"), and the separation boundary is obtained. 2. The visual results depicting manipulation of specific properties of scenes by changing specific variables in the latent space, and the ones in Sec 3.2 studying transitions across scene types, are also impressive and interesting. 3) In Sec 4, this paper only shows some sample results other models e.g. BIGGAN, but no 'semantic hierarchy in deep generative representation' is shown (not surprising given only a global latent code). Finally, I agree that given the popularity of StyleGAN like models, the investigation methodology proposed, and the insights presented might be useful to a broad audience. Overall, while the results are interesting, they are only in context of a specific GAN, and using an approach that is applicable to generative models having a multi-layer code. Despite these positives, I am not sure about accepting the paper because I feel the investigation methods and the results are both very specific to a particular sort of GAN, and the writing (introduction, abstract, related work etc.)
==== [Summary] To detect out-of-distribution (OOD) samples, the authors proposed to add an explicit "reject" class instead of producing a uniform distribution and OOD sample generation method. The proposed method is based on the idea from theoretical analysis, and is reasonable and valid. Overall the paper is well-written and well-organized. However, I like the method and could be accepted as an *CONF* paper. 2. Experimental results are not convincing: in the paper, only grayscale datasets, such as MNIST and FMNIST, are considered to evaluate the proposed method and I think it is not enough. Comments on rebuttal I don't think that the authors made a valid argument to address my concerns about theoretical justification and experiments.
Mann, and Shie Mannor. "Adaptive skills adaptive partitions (ASAP)." Advances in Neural Information Processing Systems. Summary: The authors propose a method for learning hierarchical policies in a multi-task setting built on options and casts the concepts into the Planning as Inference framework. 2017. [5] Ammar, Haitham Bou, et al. "Online multi-task learning for policy gradient methods." International Conference on Machine Learning. I liked the flow and the organization of this paper. Detailed Comments: A primary weakness of this approach is that it seems like there is one network that learns the options and is shared across all task (that would be the prior) and then there is a task-specific network for all options (posterior), wouldn't this be very difficult to scale if we want to learn reusable options over the lifetime of an agent? Term 2 controls how the option posterior deviates from the prior. The results in moving bandits alone are very convincing. It does not seem clear why "term 1 of the regularization is only nonzero whenever we terminate an option and the master policy needs to select another to execute." Some parts of the experiments section does not seem clear to me, Does the proposed approach use a network per task? 2016. This paper is about learning hierarchical multitask policies over options. In directional Taxi (2c) Distral(+action) manages to reach the same final performance (if we care about that), can you please comment on this.
This paper proposes a layer on top of BERT which is motivated by a desire to disentangle content (meaning of the tokens) and form (structural roles of the tokens). This paper proposes a fine-tune technique to help BERT models to learn & capture form and content information on textual data (without any form of structural parsing needed). One has to wait for the figure. The empirical gains in transfer learning can be simply attributed to: - More params it seems adding an LSTM over bert embeddings already does some improvement, I would have loved to see this more exploited but it wasn't. In evaluation authors design several experiments to show that: * Does transferring disentangled role & symbol embeddings improve transfer learning For each bert embedded token, the proposed method aims at disentangling semantic information of the word from its structural role.
Experiments on LeNet + MNIST show (a) different methods can achieve similar accuracy, (b) pruned sub-networks may differ significantly despite identical initialization, (c) weight reinitialization between pruning iterations yields more structured convolutional layer pruning than not reinitializing, and (d) pruning methods may differ in the stability of weights over pruning iterations. (5) *Replications*: The paper presents results only a single set of experiments using the MNIST dataset with the LeNet architecture. (1) *Overlap in pruned sub-networks*: In the middle of Sec. 4, Fig 3-5 examine the similarity of pruning masks between methods. Second, the observations are only presented for LeNet and MNIST and it is non-trivial whether they extend to large scale models. I have to give this paper a reject as the experiments conducted are far too weak, and there is little evidence anything found here will, say, generalise to a ResNet/DenseNet on ImageNet. This paper study the lottery ticket hypothesis by observing the properties of lottery tickets. Perhaps a mathematical formulation for stability (perhaps based on average standard deviation of each weight's values over training) with a table of values for each method/layer would help to clarify.
In addition to that, it analyzed a mixture of linear and non-linear activation functions, and show that mixture is better than single nonlinearity in terms of expected training error for ridge regression estimators. They analyze the performance of a simple regression model trained on the random features and revealed several interesting and important observations. Pros: - This paper investigates an interesting problem and it successfully extends the existing work. In practice, we may consider an input with additional constant feature, X <- [X,1], to deal with both models in a unified manner.
The main contribution of this paper is that the authors have proved the convergence to the iterated dominance solution for two RL algorithms: REINFORCE (Section 3.1, binary action case only) and Importance Weighted Monte-Carlo Policy Improvement (IW-MCPI, Section 3.2). [1] Michael Bowling, "Convergence Problems of General-Sum Multiagent Reinforcement Learning", Sec. 5.2 The authors examined slightly different learning rules (REINFORCE and MCPI), but I would expect that almost any reasonable learning rule would converge in iterated-dominance-solvable games; if anything, it would be surprising if this were *not* the case. Question to the authors: - How does this work differ from the known results about convergence of naive learners in iterated-dominance-solvable games? In other words, the authors have not proved how fast the agents converge to the iterated dominance solution. I'll wait to hear the author response to this). I did not check the proofs thoroughly. To the current status of the paper, I have a few concerns below. The main idea of this paper is to solve multi-agent reinforcement learning problem in dominance solvable games.
There is no qualitative comparison to other algorithms for two of the problems considered (colorization, super-resolution) and the comparison with other algorithms for unconditional image generation is limited to CIFAR-10; thus, the impact of this contribution is not clear. Overall, although the paper explain clearly the intuition and the motivation of the proposed technique, I think that the paper in its present state have low novelty, weak related work analysis review and insufficient experiments to support a publication at *CONF*.
The experimental results show that (1) the first modification, i.e., moving the layer normalization layer to the input stream significantly stabilizes the training; (2) Gated Recurrent Unit (GRU) gating seems to be most effective gating mechanism. First, the paper was based on the hypothesis of the authors that the Transformer is not stable, however, there is no comprehensive study on the unstability, and deep understanding on the root cause of it.
The authors conduct extensive experiments on image classification and segmentation and show that dynamic convolutional kernels with reduced number of channels lead to significant reduction in FLOPS and increase in inference speed (for batch size 1) compared to their static counterparts with higher number of channels. - The paper proposes a dynamic convolution selection method can be applied to arbitrary classification networks based on the global average pooled (GAP) feature map info. - Testing the ImageNet trained network of the proposed method into an object detection task (as the pre-trained backbone). The method also proposes the attention-based scaling of channels, where the attention comes from GAP, so the reviewer thinks that it is possible to explain this work as some variation of SEnet. - However, the reviewer cannot convince the novelty of the proposed approach and usefulness of the pre-trained backbone network from the proposed method when applying it to the other tasks (Object detection). The reviewer agrees that some recent works focus more on Flops, but the number of parameters is also discussed in general, when telling about the 'model size'.
The paper introduces a regulatory ratio: the probability of using the averaged advantage estimate vs using the order statistic, for computing the policy gradients. The paper conducts experiments on  different domains (sparse and dense rewards, discrete and continuous actions, fully observable and partially observable environments) which show the effect of choosing different order statistics and regulatory ratio on the policy performance. Further, the fact that using a smaller number of advantage estimates worked better (point #2 on pg 5, Effect of Ensemble Size in Appendix A) suggests that the ensemble size is an important hyperparameter, and that risk-seeking / risk-aversion (i.e., regulatory vs promotion focus) cannot alone explain why the proposed method works. This paper could be accepted as it presents an interesting idea with extensive experiments showing where it works and where it fails, along with some justification for the hyperparameter choices.
###  Summary The paper proposes a method for regularizing neural networks to mitigate certain known biases from the representations learned by CNNs. The authors look at the setting in which the distribution of biases in the train and test set remains the same, but the distribution of targets given biases changes. I vote for accepting the paper as a poster. Moreover, even in these two examples, I'm not convinced that the proposed biased models only use B for making predictions.
The paper proposes a framework for learning with rejection using ideas from adversarial examples. There is still no universal method to deal with adversarial examples, and introducing a reject option to flag potential attacks seems a sensitive choice for many applications. [1] Adversarial Examples For Improving End-to-End Attention-based Small-Footprint Keyword Spotting, ICASSP 2019 The paper fails to realize that the motivated application is actually called "cost-sensitive learning" and has been studied long time before. The Foundations of Cost-Sensitive Learning. If the definition has no relationship, it is classical learning with rejection. ------------------------------------------ Thank you for the rebuttal.
The authors propose a framework to incorporate additional semantic prior knowledge into the traditional training of deep learning models such that the additional knowledge acts as both soft and hard constraints to regularize the embedding space instead of the parameter space. The domain that the method is applied to is VQA, with various relations on the questions translated into hard constraints on the embedding space. 2. In Section 2, the authors say "constraints on the parameter space of a model are often non-intuitive". How are they "non-intuitive" and why the proposed method is more intuitive in terms of theory? in fact, so frustrating that it is hard for me to recommend acceptance in its current form.
This paper proposed to use VAE to learn a sampling strategy in neural architecture search. It is still possible that the current search space is putting the proposed method at advantage, especially if having dense connections is a useful prior. Given the above, I would like to increase my score from 1 to 3 (weak reject). I'm therefore unable to recommend acceptance for the paper at the moment, but am willing to raise my score if the authors can properly address those issues in the rebuttal.
The authors propose DTSIL to learn a trajectory-conditioned policy to imitate diverse trajectories from the agent's own past experience. This paper proposes an approach for diverse self-imitation for hard exploration problems. The authors identify and address the problem of sub-optimal and myopic behaviors of self-imitation learning in environments with sparse rewards. Unlike other self-imitation learning methods, the proposed method not only leverages sub-trajectories with high rewards, but lower-reward trajectories to encourage agent exploration diversity. The approach taken is to apply self-imitation to a diverse selection of trajectories from past experience -- practice re-doing the strangest things you've ever done. This is claimed to drive more efficient exploration in sparse-reward problems, leading to SOTA results for Montezuma's Revenge without certain common aides. The results on stochastic environments (in the Appendix) seem pretty weak (but please correct me if I'm mistaken here). However, I have a few concerns below, that prevent me from giving a direct acceptance.
*Summary* This paper considers the effect of partial models in RL, authors claim that these models can be causally wrong and hence result in a wrong policy (sub optimal set of actions). Then authors suggest a simple solution using backdoors (Pear et al) to learn causally correct models.They also conduct experiments to support their claims. The authors then reformulate the model-learning problem using a causal learning framework and propose a solution to the above-mentioned problem using the concept of "backdoors." They perform experiments to demonstrate the advantages of doing so. POST-RESPONSE COMMENTS: In my opinion, the authors adequately addressed both my own concerns, and also several valid concerns from the other reviewers. Therefore, I'm raising my score to "accept. Authors demonstrate this issue with a simple MDP model, and emphasize the importance of behavior policy and data generation process. The paper considers the problem of predicting a variable y given x where x may suffer from a policy change, e.g., x may follow a different distribution than the original data or suffer from a confounding variable. The flow of the paper proceeds in learning a causally correct model in the sense that the model is robust to any intervention changes. 4. For sentence, "Fundamentally, the problem is due to causally incorrect reasoning: the model learns the observational condi- tional p(r|s0, a0, a1) instead of the interventional conditional given by p(r|s0, do(a0), do(a1)) = s1 p(s1|s0, a0)p(r|s1, a1)." *Decision* I vote for rejection of this paper, based on the following argument: To my understanding authors are basically solving the "off-policy policy evaluation" problem, without relating to this literature.
The authors provide a general application on sequential data for continual learning, and show their proposed model outperforms baseline. Summary: In this paper, the authors propose a new method to apply continual learning on sequential data. [1] Kirkpatrick, James, et al. "Overcoming catastrophic forgetting in neural networks." Proceedings of the national academy of sciences 114.13 (2017): 3521-3526. - In the experiments, the authors only compare the proposed model with simple LSTM or LMN. Weakness: - In this paper, the model size linearly increases since the number of LSTM/LMN and AE increases when a new task comes in. In traditional continual learning settings, researchers may not always increase the model size for overcoming catastrophic forgetting. The experiments on several datasets show the proposed model outperforms basic LSTM/LMN. It is natural that their naive baseline shows poor performance since they do not consider any continual learning issues like the catastrophic forgetting problem.
This paper proposed a new query efficient black-box attack algorithm using better evolution strategies. In terms of better evolution strategies, the authors show that (1+1) and CMA-EA can achieve better attack result but it lacks intuition/explanations why these helps, what is the difference. ====================== after the rebuttal I thank the authors for their response but I still feel that there is a lot more to improve for this paper in terms of intuition and experiments. This paper proposes a black box adversarial attacks to deep neural networks.
In this paper, the authors proposed a training method called global momentum compression (GMC) for distributed momentum SGD with sparse gradient. But I did not find convincing support of this advantage in the paper: Empirically, in the experiment results, I don't think GMC demonstrate better performance than DGC in a statistical meaningful way; instead they are basically demonstrating matching performance. Although they theoretically prove a new version of DGC, it's just a minor modification and no significant performance improvement as shown from their empirical results. If it is for demonstrating the benefits of global gradient for gradient memory, I think it is more proper to also include the results of DGC without factor masking? The primary questions and concerns (critical to the rating) are: 1. One important claimed advantage of GMC over existing method is that it uses global gradient for memory gradient, while existing methods such as DGC uses local-work gradient to do so.
The algorithm selects a subset of datapoints to approximate the training loss at the beginning of each epoch in order to reduce the total amount of time necessary to solve the empirical risk minimization problem. "Stochastic optimization with importance sampling for regularized loss minimization." international conference on machine learning. Based on the experiments provided in the paper, it does appear to yield a significant speedup in training time. The empirical evaluation of the method shows large speedups in training time without degradation in performance for reasonably large subsets (e.g. 20% of the data). Strengths: The proposed idea is novel and intriguing, utilizing tools from combinatorial optimization to select an appropriate subset for approximating the training loss. "On the ineffectiveness of variance reduced optimization for deep learning." arXiv preprint arXiv:1812.04529 (2018). But if the baselines weren't thoroughly tuned, it could be the case that IG on the CRAIG subset performs similarly to IG on the full training data, but that neither is actually reaching satisfactory performance in a given domain. chosen? One of the main claims of the paper is that using the subset selected by CRAIG doesn't significantly effect the optimization performance. Clarifying questions: - In results reporting speedups, does the reported training time for CRAIG This paper is a clear accept.
has been -> have without explicit defines -> defining This paper proposes a training objective that combines three terms: * A Stein discrepancy for learning a energy model with intractable normalizing constant This is my main concern about the paper if this term appears simply because that energy model has an unnormalized density while from GANs we can sample, and Stein discrepancy is best applicable to such pair of distributions.
Summary: This paper list several limitations of translational-based Knowledge Graph embedding methods, TransE which have been identified by prior works and have theoretically/empirically shown that all limitations can be addressed by altering the loss function and shifting to Complex domain. Furthermore, the paper proposes TransComplEx -- an adaption of ideas from ComplEx/HolE  to TransE -- to mitigate issues that can not be overcome by a simply chosing a different loss. pag. The paper revisits limitations of relation-embedding models by taking losses into account in the derivation of these limitations. Regarding the experimental evaluation: The paper compares the results of TransComplEx and the different loss functions to results that have previously been published in this field (directly, without retraining). Even more serious: Following again Section 5 (Dataset), it seems that the paper imputes all missing triples in the training set for symmetric and transitive relations ("grounding"). The results show that the proper selection of the loss function can mitigate the limitations of TransX (X=H, D, R, etc) models. They propose and evaluate a new relation encoding (TransComplEx) and show that this encoding can address the limitations previously underlined in the literature when using the right loss. 2. In Section 2, the authors have mentioned that RotatE obtains SOTA results using a very large embedding dimension (1000).
This paper aims to propose a speeding-up strategy to reduce the training time for existing GNN models by reducing the redundant neighbor pairs. Through experiments, the authors demonstrate that the proposed method can get faster computation than vanilla algorithms. However, the equal-contribution (in Comment 1) is still a big one that the authors should pay attention. Regards the comments above, I prefer a grade around the borderline. However, I still have some concerns and comments.
This paper studies the connection between sensitivity and generalization where sensitivity is roughly defined as the variance of the output of the network when gaussian noise is added to the input data (generated from the same distribution as the training error). All told, if taken in isolation from prior work, I think the insights and empirical results presented in this paper are quite interesting and certainly sufficient for acceptance to *CONF*.
The authors also propose a new dataset for theorems and proofs for a simple equational theory of arithmetic, which is again suitable for improving (via learning) and testing the ability of the prover for finding long proofs. Another thing that demotivated me is that I couldn't find the discussion about the subtleties in using curriculum learning and PPO for the theorem-proving task in the paper. Also, if the dataset includes variables and other propositional logic formulas, such as disjunction, negation and conjunction, so that the prover can be applied to any formulas from Peano arithmetic via Skolemization, I would be much more supportive for the paper. I suggest you to add some further explanation so that a reader can share your sentiment and excitement on the improvement brought by your technique. OVERALL: I don't work on ATP and am not particularly well suited to review this paper, but I am slightly inclined to accept for the following reasons.
#Summary: The paper proposed a method that utilizes the model's explainability to detect adversarial images whose explanations that are not consistent with the predicted class. - I would also like to see how these results hold good for a complicated dataset like ImageNet A Simple method to detect adversarial examples, but needs more work. The method works by looking at the prediction made by the classifier as well as the output of the explainability method, and labelling the input as an adversarial example if the predicted class is inconsistent with the model explanation.
Very good paper that studies the error rate of low-bit quantized networks and uses Pareto condition for optimization to find the best allocation of weights over all layers of a network. The authors show that given the constrained optimization, layers that have a large number of weights receive lower bitrates and vice-versa. My main concern is regarding the related work and experimental validation being incomplete, as they don't mention a very recent and similar work published in ICIP19 https://ieeexplore.ieee.org/document/8803498: "Optimizing the bit allocation for compression of weights and activations of deep neural networks".
First, the authors calculate the influence score for the models with/without pretraining, and then propose some implementation details (i.e., use CG to estimate the inversed Hessian). To calculate the influence function of a model with pretraining, the authors use an approximation f(w)+||w-w*||, where w* is pretrained. These seem to be non-standard pretraining settings. I believe that these are useful technical contributions that will help to broaden the applicability of influence functions beyond the standard supervised setting. 1. The idea of converting a pre-trained model with f(w)+||w-w*|| is interesting. The authors derive the influence function of models that are first pre-trained and then fine-tuned. To do so, the authors make two methodological contributions: 1) working through the calculus for the pre-training setting and deriving a corresponding efficient algorithm, and 2) adding L2 regularization to approximate the effect of fine-tuning for a limited number of gradient steps. I have some questions and reservations about the current paper: 1) Does pretraining actually help in the MNIST/CIFAR settings considered? This extends influence functions beyond the standard supervised setting that they have been primarily considered in. For that reason, I recommend a weak accept.
From my perspective, the whole story revolves around how to compute persistence barcodes from the sub-levelset filtration of the loss surface, obtained from function values taken on a grid over the parameters. The authors state that "it is possible to apply it to large-scale modern neural networks", but it's not clear to me how that would work or what additional algorithmic improvements (if any) would need to be made in order to do so. Given the problems in the writing of the paper, my assessment is that this idea boils down to computing persistent homology of the sub-levelset filtration of the loss surface sampled at fixed parameter realizations. (4) The author's talk about the "minima's barcode" - I have no idea what is meant by that either; the barcode is the result of sub-levelset persistent homology of a function -> it's not associated to a minima. I don't think that the results on tiny neural networks have much relevance to practice, so I think the empirical data presented in this paper will have very limited impact.
This work provides  theoretical analysis for the NAS using weight sharing in two aspects: 1) The authors give non-asymptotic stationary-point convergence guarantees (based on stochastic block mirror descent (SBMD) from Dang and Lan (2015)) for the empirical risk minimization (ERM) objective associated with weight-sharing. The author proposed ASCA, as an alternative method to SBMD. The reviewer has several concerns: 1) the SBMD and ASCA algorithms are existing generic algorithms. For example, what is the beta parameter when training a NAS problem? Based on this analysis, the authors proposed to use  exponentiated gradient to update architecture parameter, which enjoys faster convergence rate than the original results in Dang and Lan (2015).
My major concerns are as follows. The authors study a combinatorial multi-robot scheduling problem (in fact the robot part is a bit inflated, since the experiments only involve agents in a simulated discrete state-space maze) using a method that builds upon recent advances from [Dai et al. (2017)]. The main contribution is to consider each of the steps taken by Dai et al. to solve combinatorial problems on graphs, and adapt them to the considered scheduling problem.
To study this problem, this paper proposes two models, Graph Feature Network (GFN) and Graph Linear Network (GLN). Suggestions for improvement: 1) Considering the experimental results in this paper, it is possible that the existing graph classification tasks are not that difficult so that the simplified GNN variant can also achieve comparable or even better performance (easier to learn). See: https://arxiv.org/abs/1809.02670 https://arxiv.org/abs/1905.13192 The paper dissects the importance of two parts in GCN: 1) nonlinear neighborhood aggregation; 2) nonlinear set function by linearizing the two parts and resulting in Graph Feature Network (GFN) and Graph Linear Network (GLN). However, I cannot accept the paper in the current form because of the following reasons.
For evaluation authors report their average accuracy on Permuted MNIST, Split-MNIST, and CIFAR10-100 and achieve superior performance over EWC, DLP, SI, VCL-Coreset, and FRCL. 4- Ambiguous claims about prior work: (a) On page 1, paragraph 3, when authors mention that methods such as GEM or iCaRL use random selection to pick previous samples, I think the line of follow-up work on these methods should be mentioned as well that have explored different techniques for sample selection and have provided benchmark comparisons (ex. Particularly I suggest that the authors elaborate more on their claimed differences stated on page 4, paragraph 5 such as "tractability of the objective function only when we assume independence across tasks". 5- Claim on the state of the art should be double-checked: Although the results shown for the experiments are superior to the provided baselines, there is an important baseline missing which has achieved higher performance than the reported ones.
I Summary The paper directly answers two sanity checks for saliency maps proposed by Adebayo et al (2018): 1. While the mechanism for competition is very simple, the resulting activation maps subject to randomization tests are reasonably convincing. My main concern is that the method seems to be designed only to answer the sanity checks: the resulting saliency maps can hardly be seen as more informative as other existing methods (eg figure 1). II Comments 1. Content The paper can be hard to read, due to multiple writing mistakes, abrupt phrasing, not well-articulated sentences. 3. The introduced approach makes Grad.Input pass the sanity checks introduced by Adebayo et al. Weaknesses: 1. For any interpretability technique, passing the sanity check is a must, but just because a saliency technique passes the sanity checks, it doesn't mean that these maps explain the network's decision well. It addresses a problem posed for existing methods for characterizing saliency in activation subject to sanity checks which measure the degree to which the activation (saliency) map changes subject to different randomization tests.
The paper introduces a method to self-supervised train a model for object detection/segmentation. Many previous unsupervised video object segmentation methods make use of optical flow and boundary detection, which I thought are OK cues to be used, especially when both can be learned in a self-supervised manner. I think it should be accepted to promote such future work.
When networks trained with SGD reliably find solutions which can be linearly interpolated without loss in test accuracy despite different data ordering,  the paper refers to these networks as 'stable.' Thus I give weak reject. This paper empirically examines an interesting relationship between mode connectivity and matching sparse subnetworks (lottery ticket hypothesis).
This paper proposes a new method for geometric matrix completion based on functional maps. So, it is expected that the proposed method shows a better result when the given geometric model is not accurate. (4) As a followup question, without such implicit regularization, it is unclear why the proposed approach does not suffer from overfitting. Though the authors include the connection between [Arora et al. (2019)], this is not convincing enough since as explained above, the implicit regularization there depends on the smallness of the initialization. I vote for a weak reject of the paper at the current pace and would like to increase my score if the following questions can be clearly answered.
This paper explores how tools from perturbative field theory can be used to shed light on properties of the generalization error of Gaussian process/kernel regression, particularly on how the error depends on the number of samples N. This theoretical paper exploits a recent rigorous correspondence between very wide DNNs (trained in a certain way) and Neural Tangent Kernel (a case of Gaussian Process-based noiseless Bayesian Inference). I encourage the authors to submit this paper again after they make more efforts to improve its accessibility. I suspect that only a small fraction of the community will have the adequate background to get much out of this paper. I am OK with the physics jargon the authors used in the paper, as well as the non-rigorous of the result. as do for – > as we do for uniformally - > uniformly This paper used the field-theory formalism to derive two approximation formulas to the expected generalization error of kernel methods with n samples. Some ways to achieve this might include reorganizing the technical points into bite-size chunks, laying out a roadmap for the main calculations and results, highlighting the important takeaways, including more figures, and concretely emphasizing the connections to practice and prior work.
The proposed approach is validated with experiments on three synthetic datasets and one real-world dataset (Bitcoin is same dataset from two different resources with little difference in characteristics) and compared against random graph models. This paper should be rejected due to following reasons: (1) The authors do not justify/discuss the motivation and importance of the task and corresponding applications that would require to predict topology of complete graph in the next step. A better model should be proposed to address this challenge. The authors need to provide concrete justification for the problem they address, instances where such a task would be useful and discussion on other techniques that can do similar tasks but lack in aspects that such a method can capture. This paper proposes a framework to model the evolution of dynamic graphs for the task of predicting the topology of next graph given a sequence of graphs. Why not use various graph datasets available in papers that learn representations (e.g. cited by authors themselves) What is special about bitcoin dataset that makes it suitable for this task? In fact, I appreciate the authors for reporting negative results as it provides a transparent insights into the effectiveness of model in different settings. It seems that the proposed model is the first deep learning model for graph sequence prediction. The learned vector is then used as input to a GraphRNN decoder to generate a graph that would serve as a predicted next graph in the sequence.
Summary: The paper proposes an autoencoder-based initialization for RNNs with linear memory. Cons. 1. The authors claimed the proposed method could help with exploding gradient  in training the linear memories. Minor: 1. There are some confusions, on P2 "we can construct a simple linear recurrent model which uses the autoencoder to encode the input sequences within a single vector", I think the authors meant encode the input sequences into a sequence of vectors? Second, the autoencoder-based init scheme (Pasa&Sperduti, 2014) is not new while the only technical contribution of this paper is a minor change of this scheme so that it works for the LMN. Long short-term memory. Neural computation, 9(8): 1735–1780, 1997. Summary: This paper proposes a new initialization method for recurrent neural networks. [2] Linear Memory Networks This paper proposes an initialization scheme for the recently introduced linear memory network (LMN) (Bacciu et al., 2019) and the authors claim that this initialization scheme can help improving the model performance of long-term sequential learning problems. Strength: The method of initializing LMN using a linear RNN is natural and simple. e.g. Penn Treebank. Reference: [1] Pre-training of Recurrent Neural Networks via Linear Autoencoders
The paper explores multi-task learning in embodied environments and proposes a Dual-Attention Model that disentangles the knowledge of words and visual attributes in the intermediate representations. *Additional feedback In conclusion, "interpretablew" -> "interpretable I thank the authors for their detailed response and appreciate their hard work in bringing us this paper. I would recommend for acceptance, as the experimental results show that the proposed approach successfully transfers knowledge across tasks.
This paper proposes a method to summarize a given graph based on the algebraic multigrid and optimal transport, which can be further used for the downstream ML tasks such as graph classification. The paper proposes a differentiable coarsening approach for graph neural network (GNNs). Although the problem of graph summarization is a relevant task, there are a number of unclear points in this paper listed below: - In Section 3.1, the coarsening method has been proposed, which is said to be achieved by finding S such that A_C = S^T A S. It looks like the main point is  that this architecture is trying to emulate iterative coarsened residual optimization of the Wasserstein metric between a graph and its representation. Moreover, if you check the paper above, they report much better results for PatchySan on MUTAG, better results on Protein for graph kernels, better results on IMDB-B using a hierarchical GNN approach, based on ideas of higher-order WL.
The reviewer votes for rejection as the method has limited novelty. The authors stated that "F-pooling remarkably increases accuracy and robustness w.r.t. shifts of moderns CNNs"; however, in Table 1-3, the winning margin of accuracy is actually quite small (<2%), and the consistency (<3.5% compared to the second best baseline except resnet-18 on CIFAR 100 has large improvement ~7-8%). At the same time, this work can be further enhanced at the following aspects: 1. This work can make it clearer in principle how anti-aliasing contributes to improving the classification performance and robustness. 2. Compared to AA-pooling, it seems that F-pooling has a better theoretical guarantee (i.e. the optimal anti-aliasing down sampling operation given U). Also, from the three Tables in the experimental part, the improvement of F-pooling over AA-pooling (developed by the main reference of this work) does not seem to be significant or consistent. The authors first derived the theory of F-pooling to be optimal anti-aliasing down sampling and is shift-equivalent in sec 2, and then demonstrated the experimental results of 1D signals and image classification tasks. But in the current form, the paper has less value to be published in *CONF*.
This paper proposed a new realistic setting for few-shot learning that we can obtain representations from a pre-trained model trained on a large-scale dataset, but cannot access its training details. 2) The algorithm does not have any important contributions comparing to existing ones: they define a prototype per class based on the pre-trained model and apply the nearest neighbor classification. Is the attention way used in the paper a good way to exploit the pre-trained model for few-shot classification problems? ========================================================= After Rebuttal: I thank the author for the response. Back to the standard few-shot classification problem, they will first adapt the model with base class samples and then adapt to novel classes. A new task is suggested, similarly to FSL the test is done in an episodic manner of k-shot 5-way, but the number of samples for base classes is also limited.
Experiments illustrate that the multi-headed architecture approximates the ensemble marginally better than approaches that use a network with a single head. The paper proposes to distill the predictions of an ensemble with a multi-headed network, with as many heads as members in the original ensemble. Distillation proceeds by minimizing the KL divergence between the predictions of each ensemble member with the corresponding head in the student network. - This paper experimentally shows that multi-head architecture performs well on MNIST and CIFAR-10 in terms of accuracy and uncertainty. </update> Summary & Pros - This paper proposes a simple yet effective distillation scheme from an ensemble of independent models to a multi-head architecture for preserving the diversity of the ensemble. To verify the effectiveness of the proposed distillation method, other large-sized datasets should be tested, e.g., CIFAR-100, ImageNet. - The proposed scheme provides the same advantages of the ensemble in terms of uncertainty estimation and predictive performance, but it is computationally efficient compared to the ensemble. The paper would have been more interesting if the authors had managed to demonstrated significant improvements over competitors on not toy (MNIST / CIFAR) problems.
The authors propose a method based on principal components projection to tackle this issue. The authors conduct experiments on image classification tasks to show the performance of the proposed method and compare it with two other baselines EWC and OWM. Experiments show improved results over OWM (the method that this paper builds on) and EWC. 4. It is not clear why the proposed method can solve the issue that OWM faces with (bad accuracy when tasks are not quite related). I am therefore giving the paper a weak reject. 2. It might strengthen the paper if the authors can show the comparison results on more other datasets, e.g., other image classification tasks.
for MINIST, etc. The paper also show ACN uses much less numbers of parameters and achieves similar accuracy when comparing with a large optima FC network on a set of datasets. Otherwise, I think the work makes sense, the idea is nice, and the results show promise! (5) The claims that "ACNs achieve compression rates of up to three orders of magnitudes compared to fine-tuned fully-connected neural networks with only a fractional deterioration of classification accuracy" is quite misleading. Given fully-connected neural networks achieve up to 528 times with also a fractional deterioration (Sec. 4.3), by presumably having a shallower architecture.
The authors proposed a series of improvements, including alternative optimization, dynamic scheduling, detach and batch normalization to help boosting the performance to SOTA under 4-bit quantization. I addresses the existing issues in the common paradigm, where a floating-point network is trained first, followed by a second-phase training step for the quantized version. Comments: I consider this a well-written paper with great clarity and good empirical performance. However, it seems to me that this drastic scheduling strategy sounds like very similar to the traditional approach that trains the floating point network first and then finetune the quantized one, except for the fact that this proposed algorithm repeats this process a few times. For example, the alternative optimization of W and \\theta is similar to alternative re-training in network pruning, although a unified loss/optimization framework is applicable in this case. However, a major weakness of this work is that most of the performance improvement comes from a combination of add-on improvements, except that the authors put them together into a unified framework and explained elegantly. This raises the question whether the boost in performance is due to the several additional steps employed (which in general can be applied to other quantization techniques as well), and not due to the main idea itself. The results on ImageNet under 4-bit quantization are strong and convincing, but the paper could benefit from conducting additional experiments on different datasets and bitwidth configurations.
The paper proposes to use Contrastive Predictive Coding (CPC), an unsupervised learning approach, to learn representations for further image classification. The authors augment contrastive predictive coding (CPC), a recent representation learning technique organized around making local representations maximally useful for predicting other nearby representations, and evaluates their augmented architecture in several image classification problems. They then use this improved model to obtain impressive performance in classification within semi-supervised and transfer learning settings, giving strong support for the use of such methods within image processing applications. "Representation learning with contrastive predictive coding." arXiv preprint arXiv:1807.03748 (2018). This paper improves Contrastive Predictive Coding method and reaches a good performance in several downstream tasks. I am sure that some of the methods used here could lead to improvements in the use of CPC for other types of data, but the authors currently don't provide any insight on this issue. This paper only proposes some minor improvements based on the original CPC method and use a deeper network to get better performance. It would be great if you provide more evidence that the improvement in low-data classification results from the increased 'predictability'. I highly appreciate new results and new architectures, but it is not enough for a full conference paper.
[ref2] Conditional image-to-image translation, CVPR'18 This paper presents a technique for encoding the high level "style" of pieces of symbolic music. ## summary In this paper, the author extends the standard music Transformer into a conditional version: two encoders are evolved, one for encoding the performance and the other is used for encoding the melody. The following two papers are about conditional unsupervised image-to-image translation, which build a cycle-consistency loss during the feedback and might help improve the performances. The authors conduct experiments on the MAESTRO dataset and an internal, 10,000+ hour dataset of piano performances to verify the proposed algorithm. I am not working on music generation but I list two CV related papers about conditional image translation, which mathematically describes "an image with specific style". Finally, it would be useful if the authors comment on existing methods for measuring music similarity in symbolic music and how their proposed feature fits into existing work. The performance conditioning vector is generated by an additional encoding transformer, compared to the Music Transformer paper (Huang et. Is it computational efficiency? Why not compare the conditioning melody with the generated performance similar to query-by-humming? Why use this feature compared to existing techniques for measuring similarity between symbolic music pieces? I also don't see the connection between this proposed feature vector and using the IMQ kernel for measuring similarity. 5. In section 5.2, a conditioning sample, a generated sequence and an unconditional sample are used to compute the similarity measure. It took me a couple of passes and reading the Music Transformer paper to realise that in the melody and performance conditioning case, the aim is to generate the full score (melody and accompaniment) while conditioning on the performance style and melody (which is represented using a different vocabulary). Where does the performance feature come from? I find the description of the "performance feature" to be lacking in necessary background and detail. Measuring music similarity is a difficult problem and the topic has been the subject of at least 2 decades of research. 4. Considering that this is an unsupervised setting that two styles are transformed, can cycle-consistency be implemented as a baseline? In terms of technical presentation, I think the authors should clarify how the model is trained. Although I understand the need for anonymity and constraints while referring to unreleased datasets, it would still be useful for the reader/reviewer to have some details of how the melody was extracted and represented.
Typos: Page 7: with with roughly 200M ->  with roughly 200M In this paper, the authors propose a method to train deep neural networks using 8-bit floating point representation for weights, activations, errors, and gradients. For example, how much improvement can this work achieve when just using enhanced loss scaling method or a stochastic rounding technique?
While potentially offering a faster convergence with respect to epochs, the nonlinear updates have two major drawbacks: 1) While there are preliminary theoretical results (fixed points of the method are critical points), it remains unclear whether the computed update is still a descent direction on the original energy. The work is based on the recent paper 'Proximal Backpropagation', Frerix et al., *CONF* 2018, which views error back propagation as block coordinate gradient descent steps on a penalty functional comprised of activations and parameters.
Since these OCHs are differentiable, the proposed DBNN model can be used for streaming input data with time-variant distributions. The paper proposes to use an online code vector histogram (OCH) method attached to the input and output of a classical DNN. This paper introduces the vector quantization but doesn't mention the use of it in streaming data in related work, which kind of blurs the contribution a bit. The paper is missing a related work section describing state of the art methods to address stream data. In the introduction, the authors motivate the proposed DBNN by saying that BNN needs dozens of samples from weight distributions and therefore is rather inefficient. I believe that this paper needs for work and is not yet suitable for acceptance.
This paper proposed a new method for knowledge distillation, which transfers knowledge from a large teacher network to a small student network in training to help network compression and acceleration. I updated my rate to weak accept. 1. To reduce model size, there are several different ways including efficient architecture design, parameter pruning, quantization, tensor decomposition and knowledge distillation. It looks to me the proposed method use an ad-hoc selected layer to transfer knowledge from teacher to student, and the transfer is indirect because it has to go through the pre-trained subnetwork in teacher. In conclusion, I will give a weak reject currently, unless the authors improve their literature survey and modify their claims. Possible improvement of the paper is the instruction on how to choose the intermediate layer from where to teach the representation, i.e. where the student sub-network ends and teacher sub-network begins.
The paper has provided consistency guarantee and several synthetic and real data experiments as support. This paper appears to be technically sound, but it should be rejected based on 1) the relatively limited applicability of the model and 2) a lack of thorough experimentation indicating that this is an appropriate method under more general circumstances. ############## After reading the author's feedback and the comments from other reviewers, I keep the current rating but tend to a borderline score and it is ok if it must be rejected because of the concerns of limited applicability and the experimental.
- Summary: This paper proposes an out-of-distribution detection (OOD) method under constraints that 1) no OOD is available for validation and 2) model parameters should be unchanged. The motivation of the proposed approach is clear, and the method seems novel. After reading the other reviews and comments, I appreciate the effort by the Authors, but it looks like the paper still needs some work before being ready. As the authors addressed, Mahalanobis detector proposed by Lee et al. (2018b) requires validation to determine weights for feature ensembling, but the validation can be done without OOD data by generating adversarial samples as proposed in the same paper. - Comments: 1. As addressed by the authors, feature concatenation ("assemble") is not effective for the Mahalanobis method but the proposed method. Again, weights can be validated by adversarial samples to satisfy the constraints. Their main difference would be, while adversarial attack is very close to the clean data in the data space, OOD is relatively far from the in-distribution in the data space. To me, if the work could be changed to compare against works which are not so tightly constrained, not for the purposes of holding it to the same standard but to understand it's relative standing, or to better justify the very strict constraints which somehow, despite out-of-distribution detection being a popular upcoming topic, apparently only has one other paper that matches it. There is also  theoretical guarantees showing exhaustiveness of the proposed methods in detecting all possible out-of distribution examples. In worst case, the performance gain from the compared method would be from their incorrect implementation on the prior works. As the authors addressed, Mahalanobis detector proposed by Lee et al. (2018b) requires validation to determine weights for feature ensembling, but the validation can be done without OOD data by generating adversarial samples as proposed in the same paper. Obviously, not having to retrain is more efficient than having to and not having to validate on out-of-distribution samples is helpful in times when we don't know them ahead of time, but it is unclear to me if it is the case that we will have a situation in which both of the above are, for instance, not possible at all. So, it would be interesting to see the performance of both baselines and proposed approach in those settings where inputs are similar in nature but very different in some aspects. For this reason, I am borderline unless that caveat is addressed as described below, in which case I would be happy to accept.
The paper presents an approach - Delayed and Temporally Sparse Update (DTS) - to do distributed training on nodes that are very distant from each other in terms of latency. (plus, orthogonal gradient compression techniques do exist, e.g. as in "Federated Learning: Strategies for Improving Communication Efficiency").
This paper investigates benefit of over-parameterization for latent variable generative model while existing researches typically focus on supervised learning settings. This paper performs empirical study on the influence of overparameterization to generalization performance of noisy-or networks and sparse coding, and points out overparameterization is indeed beneficial. As the authors point out (and I agree), the paper constitutes a compelling reason for theoretical research on the interplay between overparameterization and parameter recovery in latent variable neural networks trained with gradient descent methods. The paper "aims to be a controlled empirical study making precise the benefits of overparameterization in unsupervised learning settings. I am expecting some theoretical analysis for tasks simple as noisy-or and sparse coding, or some experiments for more complicated (deep) models need to be done, to make the paper more solid. " The author's empirical study is comprehensive, and to my knowledge the most detailed published work on this to date. If there were thorough investigations on more modern deep generative models, then the paper would be stronger. A small gripe: the authors promise " a controlled empirical study making precise the benefits of overparameterization in unsupervised learning settings". The generative models to obtain disentanglement representation could be investigated in the frame-work of this paper. Decision: weak accept. The paper contains some new insights, but its contributions are not quite as substantial (e.g. lack of precise mathematical statements) or surprising as those in stronger *CONF* papers. Decision: weak accept. The paper contains some new insights, but its contributions are not quite as substantial (e.g. lack of precise mathematical statements) or surprising as those in stronger *CONF* papers. I find the paper has some drawbacks.
Although I really liked section 3 where authors establish the different ways in which `posterior collapse' can be defined, overall I am not sure if I can extract a useful insight or solution out of this paper. The paper first discusses various potential causes for posterior collapse before diving deeper into a particular cause: local optima. After categorizing difference causes of posterior collapse, the authors present a theoretical analysis of one such cause extending beyond the linear case covered in existing work. I've been unable to resolve this discrepancy myself and would appreciate comments from the authors (or others). In particular, I believe it would be more accurate to say that IF the autoencoder has bad local minima then the VAE is also likely to have category (v) posterior collapse. 2) Section 4 provides a brief overview of existing results in the affine case and introduces a non-linear counter-example showing that local minima may exist which encourage complete posterior collapse. c. I think it would be good to think about the intuition of this as well: "unavoidably high reconstruction errors, this implicitly constrains the corresponding VAE model to have a large optimal gamma value": isn't this intuitive to improve the likelihood of the hyperparameter gamma given the data?
This paper aims to identify the primary source of transfer error in vision&language navigation tasks in unseen environments. The second contribution is to use semantic information, compact statistics derived from (1) detected objects and (2) semantic segmentation, to replace the RGB image and provide input to the system in a way that maintains state-of-the-art performance but shrinks the performance gap between the seen and unseen data. The authors tease apart the contributions of the out-of-distribution severity of language instructions, navigation graph (environmental structure), and visual features, and conclude that visual differences are the primary form in which unseen environments are out of distribution. This paper has two main contributions. The authors proposal methods perform nominally better on the tasks being investigated, but much of the latter portion of the paper continues to focus on the 'improvement' in the metric they use to diagnose the 'bias'. * another nit: "suggest a surprising conclusion: the environment bias is attributed to low-level visual information carried by the ResNet features." --> idk that this is that surprising, it was kind of natural given the result that removing visual features entirely doesn't hurt performance and helps generalization. -------- After discussing with the reviewers about the methodological issue of the validation set, I have lowered my score to a weak accept, but I think this paper should still be published. I recommend this paper for acceptance; my decision is based on the thorough analysis of the ultimate cause of a recurring problem in this field.
I conjecture the paper would like to bring the evolution in genetics and perhap brain cirecuits as well to define a novel neural net model, called NNE by the authors. This is my main concern about this work. Based on the above comments, I think the work will benefit from further developments before being ready for publication. Based on this, I give my rating.
They also use a human judgement dataset based on odd-one-out classification for triplets of inputs as comparison to evaluate whether the CNNs are able to capture the linguistic structure in the label categories as determined by the relation of the superordinate labels to the basic labels. - The authors claim "Surprisingly, the kind of supervised input that proved most effective in matching human performance on the triplet odd-one-out task was training with superordinate labels". would all likely give different ratings.
SUMMARY OF REVIEW This paper discusses an interesting problem of BO in the cases of robustness and antifragility to aleatoric noise/uncertainty. The y-axis in the experiment, the paper has considered the objective function value + noise. Can the authors provide supporting evidence (in the form of references) that such a dataset is due to heteroscedastic aleatoric uncertainty? Since no convergence guarantee is given, a more extensive empirical analysis with real datasets needs to be provided to better understand the performance and behavior of the proposed BO algorithms.
This paper proposes a fractional graph convolutional networks for semi-supervised learning. The key approach of the proposed method is to apply a classification function (equation (3)) obtained by solving a GSSL problem to graph convolutional networks. ------------------------------------------------- The response from authors addressed many of my concerns. The rating has been updated. The paper provides a new model for semi-supervised node classification in directed and undirected graphs. I believe the experimental results could justify an accept, but I would not claim I am an expert in semi-supervised learning on graphs.
They introduce a method that learns a latent dynamics model exclusively from multi-step reward prediction, then use MPC to plan directly in the latent space. This is in contrast to existing work which generally use a combination of reward prediction and state reconstruction to learn the latent model. Summary: This paper proposes a novel algorithm for planning on specific domains through latent reward prediction. I tend to reject this work, because although I support the premise and believe it is very important, and like the style of experiments run with the use of distractors, I believe it is not impactful if only looking at the dense reward setting. (2) given that the proposed method is a minor modification to the PlaNet paper, it seems that PlaNet should be included as a comparison (especially because it has been shown to work on high dimensional states), and Questions and minor comments: - What objective is used to learn the latent model of the state-prediction model algorithm?
This paper performs a regret analysis for a new hierarchical reinforcement learning (HRL) algorithm that claims an exponential improvement over applying a naive RL approach to the same problem. My understanding is that the transitions in hMDPs work _like_ a clockwork (more on this in Mis6), the algorithm interacts with the sub-MDPs at each layer in turns according to their fixed horizons H_l's. For instance, the paper begins by contrasting HRL approaches with a number of standard RL algorithms, saying that approaches such as AlphaGo do not require high-level planning.
The main idea follows the evidential deep learning work proposed in (Sensoy et al., 2018) extending it from the classification regime to the regression regime, by placing evidential priors over the Gaussian likelihood function and performing the type-II maximum likelihood estimation similar to the empirical Bayes method [1,2]. The approach starts from the standard modelling assuming iid samples from a Gaussian distribution with unknown mean and variances and places evidential priors (relying on the Dempster-Shafer Theory of Evidence [1] /subjective logic [2]) on those quantities to model uncertainty in a deterministic fashion, i.e. without relying on sampling as most previous approaches. 2. The experimental results show consistent improvement in performance over a wide base of benchmarks, scales to large vision problems and behaves robustly against adversarial examples. Even though the presentation largely follows (Sensoy et al., 2018) and uses terms from theory of evidence, the derivation actually is more aligned with the prior network [3] under the Bayesian framework which is missing from the references. Predictive uncertainty estimation via prior networks. This is a very relevant topic in deep learning, as deep learning methods are increasingly deployed in safety-critical domains, and I think that this works deserves its place at *CONF*. I think that the authors should consider adding a section similar to Section 3 of Sensoy et al. [3] should be considered.
This paper purposes to cluster data in an unsupervised manner that estimates the distribution with GMM in latent space instead of original data space. The author(s) posit a Mixture of Gaussian's prior for a compressed latent space representation of high-dimensional data (e.g. images and documents). I much prefer this article's method, but for comparison purposes, the author(s) should similarly use a KNN classifier on their latent space to compute accuracy in the same manner. Being the first to pair an existing model with an existing method, in my eyes, does not necessarily meet the *CONF* bar.
*Synopsis*: This paper proposes using the features learned through Contrastive Predictive Coding as a means for reward shaping. Moreover, the reward shaping method assumes we know the goal state but using CPC feature does not. 6. I quite like the idea of predictive coding (albeit the original scheme presented by Rao and Ballard 1999) as an unsupervised representation learning scheme, but am unsure this is critical for your method and the current approach is not really predictive coding in a sense (or at least the ideas I'm familiar with from cognitive computational neuroscience). Overall, I am leaning to reject this paper because (1) the main contribution of the paper is not clear (2) the experiments are missing some details and does not seem to support the claim that the proposed methods can tackle the sparse reward problem.
The authors provide a new universal approximation theorem on real-valued functions that doesn't require the assumption of a fixed cardinality of the input set. While the theorems proved in the paper are original and novel, they are a refinement of the already known results regarding approximation theorems for PointNet and DeepSets, respectively, hence only a marginal improvement in understanding these function classes. However, the rebuttal does not alleviate my concerns about additional impact beyond the UATs in the original paper. It would be interesting to investigate empirically the limitations of these architectures, for instance by playing with the diameter and center of mass functions as suggested in 3.3. The presentation in this paper does remove the assumption of a fixed cardinality, but since this seems to be a mild assumption, it is not clear what is gained by this (beyond mathematical elegance). === Post rebuttal update === I'd be grateful if the authors could comment on this. This paper brings a valuable theoretical contribution to the existing state of the art of their approximation abilities. This work examines the fundamental properties of two popular architectures -- PointNet and DeepSets -- for processing point clouds (and other unordered sets). To this end, the authors prove a series of theoretical results and establish limitations of these architectures for learning from point clouds. I thus maintain my recommendation of weak reject PointNet (Qi et al, 2017) and Deep sets (Zaheer et al, 2017) have allowed to use deep architectures that deal with point clouds as inputs, taking into account the invariance in the ordering of points. This paper removes the cardinality limitation and gives two kinds of results: 1. PointNet (resp.
The work starts by describing the construction of interesting crowdsourced data sets that include people's estimates of typical quantities, what they would consider to be low or high values for a given object in given units e.g. the temperature of a hot spring or the height of a giraffe. This paper attempts to study if learned word embeddings for common objects contain information about "numerical common sense". If the NCS dataset does not represent "numerical common sense", it invalidates all experimental results from the paper. This paper should be rejected because (1) the NCS datasets are too small to represent "numerical common sense" (2) the NCS datasets contain faulty data points and (3) the results from the experiments conducted are not sufficient to accept or refute the hypotheses.
Summary - The paper first makes the observation that training algorithms and architectures for meta-learning have become increasingly specific to the few-shot set of tasks. In the end, the paper is essentially arguing that it's better to use different learning rates (for the inner loop) during meta-training and meta-testing. This paper proposes to apply MAML-style meta-learning to few-shot semantic segmentation in images.
The justification for their overall experimental design (i.e., evaluating per choice clusters) is reasonable. The authors compare various choices of configurations obtained from the Cartesian product of 8 factors which they call thematic groups: Policy Losses (Sec. Overall, this is a strong paper and I recommend it for publication.
The experiments show the advantages of training RBM with weights initialised from BN projection weights in generation and classification. The experiments show advantages of BN initialisation, pointing to new directions of improving RBM training. I congratulate the authors on their spirit of maintaining a high standard on the theory, experiments and descriptions, and therefore significantly raise my score. All my concerns are addressed and reflected in the revision (though some are much better done than the rest). Recommendation I'm in favour of rejection, but some concerns can be addressed fairly easily (with experiments) so I'm open to raising my score if questions are well-addressed.
This paper proposes a novel NAS method that searches the model architectures by grows the networks. Pros: This paper nicely unifies two different classes of approaches (NAS + sparsity) for determining the topology of neural networks.
The proposed method subsumes previous SBGM techniques of score matching with Langevin dynamics (SMLD aka NCSN) and denoising diffusion probabilistic modeling (DDPM) and shows how they correspond to different discretizations of Stochastic Differential Equations (SDEs).
Experimental results show that the proposed method outperforms pure EBM defined on image space and also pure VAE models by large margins. This paper proposes a model that corrects VAE by an energy-based model defined on image space. Strengths: The paper provides a thorough overview of recent work towards training EBMs.The approach generates high quality image samples by combining EBMs and VAE based models.
In summary, I believe this paper will foster more productive research by establishing the strong baseline on both decision-tree based method (although it has been known) and neural method (on which authors make good technical contributions). Thanks to the authors for their hard work and the nice paper. Pros: This paper discusses potential reasons why neural LTR models are worse than gradient boosted decision tree-based LTR models, and uses empirical results to show the effectiveness of the proposed solutions. Concerns: Regarding the proposed solutions, the authors use data augmentation to improve neural LTR models. It would've been very interesting to see how these techniques improve the performance of previous neural LTR models; log1p transformation, data augmentation, and model ensembling would straightforwardly apply to other neural models as well. Then, it presents a few tweaks related to feature transformation and data augmentation to improve the performance of neural models. I think this paper establishes an unfair comparison between GBDT and neural-based models. Summary In this paper, the authors study the problem of neural LTR models. Comparing to traditional gradient boosted tree-based LTR models, is it really worth putting efforts into studying neural LTR models? When I was reviewing other LTR papers, I often had to point out that the proposed method significantly underperforms LightGBM.
The authors propose a novel Adversarial Sparse Convex Combination (ASCC) method in which they model the word substitution attack space as a convex hull and leverages a regularization term to enforce perturbation towards an actual substitution. Reason for score: Overall, I vote for accepting this paper.
Summary: This work focuses on the study of (global) convergence and gradient dynamics of a recently proposed family of models, the deep equilibrium  (linear) models (DELM) under common classes of loss functions.
---- Summary ---- The paper proposes a method for modifying an experience replay when learning in communication environments, by relabelling messages using the latest policy. The paper proposes a communication correction mechanism where, during the centralized training, messages there were received in the past from other agents are reevaluated according to the updated policy. Summary: This paper considers communication games when agents use experience replay. Negatives: I feel like there's a pretty obvious question about "What happens in richer domains?" that (unless I missed it) isn't addressed in the paper - I'll expand on that in my 'Questions to clarify recommendation' section below. Questions to clarify recommendation: The three environments presented in the paper, if I've understood them correctly, are pretty straightforward in that Recommendation and Justification: Overall, I feel like this was a strong paper and should be accepted.
They train a neural network consisting of a "core" and a "readout" in an end-to-end fashion to learn stimulus (visual inputs) -- response (single neuron activity) pairs. There is closely related work in the literature, but this paper achieves very good performance, partly through a new way of accounting for neurons' receptive-field positions. Though it needs more work to fully justify this claim, their demonstration that transferred representations seem to be more effective than direct training is surprising and interesting. (iv) The authors report that transfered "core" representations work better than direct-training in their generalization experiments. The paper advances a few contributions: Confirm that task-driven models based on object recognition, are outperformed by data-driven models for predicting single neuron responses.
The authors also study the rank correlation of the architectures from the  NAS-Bench-201 space regarding different hardware metrics (including both measured ones and theoretic ones like FLOPs) and find several pairs with low rank correlation. For this, the authors adopt two popular search spaces (NAS-Bench-201 and FBNet) and measure/estimate hardware performance metrics such as energy costs and latency for six hardware devices (spanning commercial edge devices, FPGA, and ASIC) for all architectures in this search spaces. In addition, the authors present results from running three architecture searches using ProxylessNAS with different target hardware devices. The authors convincingly argue that properly performing on-device inference time/energy benchmarks properly is challenging for practitioners because it "requires various hardware domain knowledge including machine learning development frameworks, device compilation, embedded systems, and device measurements." Notes on Rating: I've given the paper a borderline score (5) in my initial review, due to the open questions mentioned in the "cons" section above. I hope that the author's statement "All the codes and data will be released publicly upon acceptance" also includes the code for conducting the measurements. However, I believe proposed dataset could be a valuable contributions to the ML research community, and would lean toward accepting the paper if the concerns are suitably addressed.
The paper presents a linear time and space attention mechanism based on random features to approximate the softmax. Proposes an extension of RFA with gating that improves accuracy on language modeling, relative to softmax attention. The experiments support the claims of improved accuracy via choice of kernel and gating, and preservation of speedups inherited from the linear attention formulation. Strengths The RFA-gated formulation is both more accurate and potentially faster (at least for decoding) than softmax attention, as demonstrated on language modeling. Recommendation: Weak Accept Well-written and timely exploration of linear attention. Empirical results demonstrate accuracy improvement over softmax attention, while preserving the linear time complexity. The authors should clarify that the time complexity in Table 3 is based on the assumption that we have infinite number of threads or GPT/TPU cores that can be scaled up when M is increased.
Then, it also provides an instantiation of the lemma applicable to residual networks, an explicit compression analysis via pruning with a corresponding generalization bound, and empirical supports. The paper provides generalization bounds for seemingly complex neural networks on the basis of much simpler ones. This paper provides an upper boundary of the generalization error of networks: the sum of its training error, the distillation error, and the complexity of the distilled network. The topic of this work is of great significance given that understanding the generalization error of neural nets is relevant to the community. Overall, I think this is an interesting paper and should be accepted.
This paper presents Gauge Equivariant Mesh CNNs. The method is motivated by the fact that graph convolutions can be modified for meshes to take into account the angular arrangement of local neighborhoods. The work presents a novel message passing GNN operator for meshes that is equivariant under gauge transformations. The architecture is an elegant way to incorporate gauge symmetry on meshes and RegularNonlinearity addresses an important issue for equivariant neural nets. Page 2: One strength of the paper is the argument for why gauge symmetry is necessary in the first place. I think a toy experiment verifying it would be necessary, especially since equivariance of the non-linearity is only approximated (maybe by showing appropriate feature histograms after the non-linearity for changing reference points). While it is plausible and reasonable to model such data using vector features of type ρi in the hidden layers, the argument for the necessity of gauge equivariance would be even stronger if the input and/or output signal was itself vector valued, for example a velocity or gradient on the mesh. The paper would benefit from other experiments on manifold like performing mesh convolutions for human or object reconstruction from images. My recommendation is positive because the community needs principled ways of convolution on non-Euclidean domains, and this paper seems to make an incremental contribution towards that direction. It would then be nice to mention clearly that one would like to omit certain local reference frame choices and directly convolve as there is an ambiguity in the tangent plane. Since the experiments are scarse, I wonder if the operator is hard to apply to larger tasks or if the kernel restrictions weaken the approach on other tasks.
A practical consideration remains. Though theorem 4.1 reduces construction of a basis of steerable kernels to 1) finding Clebsch-Gordon decomposition of tensor products, 2) describing endomorphisms of irreps, and 3) describing harmonic functions, none of these problems is trivial (or even necessarily solved) for a general compact group G. However, in that case, we are still back to solving the problem on a group by group basis. theory and so it is not necessary to use physics here to describe steerable CNN. If I had a student who needed this material I would instead give him or her the original Cohen Welling paper and a representation theory textbook such as Hall 2015 or one of the several textbooks they cite. The application section is in particular way too sketchy to convince the novice that this impressive work will be useful to the machine learning community and an effort in this direction should be made to clarify the expected impact. 3.Page 5, the fact that input and output representations Vin and Vout decompose into irreps does not immediately explain how to construct a steerable kernel basis for Vin→Vout given ones between irreps Vi→Vj.
Summary To work with data that is not sampled on a grid, this work represents the activations of a neural network using a Gaussian process with RBF covariance. In Section 2 para 3, the authors mention that Gaussian process adapter (Li and Marlin, 2016) operates on a deterministic signal on a regular grid and cannot reason probabilistically about discretization errors. The authors repeatedly discuss that their approach does not require any discretization unlike some the previous approaches based on GP(mentioned in Section 2). Comments The paper references related work, makes a meaningful contribution, and I think the empirical methodology is sound. This work is most useful in the setting of irregularly sampled data.
The authors studies an episodic MDP learning problem, where they propose to study an Optimistic Closure assumption which allows the Q function to be expressed as a generalized linear function plus a positive semi-definite quadratic form. The paper then proves that LSVI-UCB still enjoys sub-linear regret in the generalized linear setting with strictly weaker assumptions. This paper analyses an existing algorithm (LSVI-UCB) with generalized linear function approximation instead of conventional linear function approximation. I am in favor of acceptance, given that it provides a non-trivial extension to what is known and the Optimistic Closure assumption seems to me to be closer to the reality than the linear MDP assumption.
The paper demonstrates the applicability of disentanglement promoting VAEs for achieving adversarial robustness and further enhancing such VAEs by considering their hierarchical counterparts. They demonstrate that the proposed method is more robust to other VAE baselines for the attacks. The experiments are sound and well documented (results are reported across latent space dimensions, and adversarial attack parameters) (Update): The score has been updated after a rebuttal from the authors.
Based on these, the authors show that it is possible to upper bound the likelihood of reaching an unsafe state at every training step, thereby guaranteeing that all safety constraints are satisfied with high probability. The authors formally show that it is possible to upper bound the expected probability of failure during every policy update iteration (Thm1), which is a non-trivial result. The main idea is to formulate the safe RL problem as  a CMDP problem, but with worst-case bounds to ensure that the safety constrained is guaranteed throughout the learning. A summarize of point 4 & 5: to me I don't feel the authors well-defined the `''safe exploration''  and give a rigorous analysis on the ``the safety guarantee. The authors answers one of the most important concerns, I have raised score to 6.
The paper indeed tackles an important problem that can affect the overall performance and efficiency of the hardware. This would be useful to get an idea of how EGRL fairs against [3] and [4] which also trained on real hardware and took many hours to finish training the policy. Questions and Clarifications: I believe that the related work section should add a clarification - [1], [2], [3] and [4] primarily deal with device placement, i.e., placing components of computation graph on different CPUs/GPUs to optimize run time via better parallelization. Device placement optimization with reinforcement learning.
They then determined parameters for the modified ReLU that would minimize the deviation between these activation functions, and computed the minimum conversion error (for converting ANN -> SNN). Nice performance was obtained in all cases: better than using a normal ReLU, or other comparison activation functions, in the "target" ANN. Summary The authors suggest a relationship between a leaky relu and a spiking integrate and firing neuron model. To achieve their goal, they described the spiking neuron non-linearity by a "staircase" function of the input (spiking output increases by 1 each time the input gets big enough to reach the next stair), and related that to the ReLu function used in the non-spiking neural net. The authors seek a mechanism to train a spiking neural net to duplicate the function of a non-spiking one. This relationship suggests a mapping between the two models which is imperfect, a loss seems to be derived to reduce this mismatch along the network training. With the new equation (1) the paper is hopefully more understandable now. Strength: (1) This paper proposes a layer-wise optimization method for ANN-SNN conversion.
While the paper's idea on applying optimal transport tools for training generative models seems interesting, the discussed theoretical and numerical results are not supportive enough to show that the proposed approach indeed improves upon WGANs. The paper should either remove these sentences or precisely explain why the proposed non-convex problem enjoys better convergence properties than the minimax problems.
The limitation/assumption mentioned in weaknesses 1 must be sufficiently disclosed in section 3.2. [Strengths] The proposed framework is sounded and can well handle a variety of cases when class labels or OOD data are available. It also presents two ways to leverage labeled outlier data if available, including an improved mahalanobis distance method and the application of supervised contrastive learning methods proposed recently. The way that the presented outlier detection methods utilizes the labeled outlier data may less effective than previous work, because the cluster-based anomaly scoring here is separated from the representation learning. There are a number of studies on self-supervised outlier detection approaches as well as what is called few-shot outlier detection approaches, but I cannot find any discussion of those work and the empirical comparison to these methods. Some closely related methods are: self-supervised methods such as GT and E3Outlier that learns feature representations using a pre-text task in a self-supervised way; unsupervised outlier detection methods such as RDA, REPEN, ALOCC, OCGAN, etc.; methods that use a few labeled outlier data such as Deep SAD, DevNet, REPEN, etc.
The authors also provide numerical results showing the empirical rate matches the theoretical asymptotic rate of the Maclaurin coefficients of the Laplace kernel and NTKs. Overall, I think the paper is well-written, and the proof is solid. In addition to proving concrete results comparing the Laplace, NTK, and exponential power kernels, it is serves as a proof-of-concept for potentially using the tools of singularity analysis to understand neural networks. I didn't go much deeper into the Appendix,
This paper studies how to improve the worst-case subgroup error in overparameterized models using two simple post-hoc processing techniques: (1) learning a new linear classification layer of a network, or (2) learning new per-group threshold on the logits. I also have a few minor notes/suggestions: The findings presented in Figure 4 is interesting (mainly the fact that the overparametrization seem to be improving the worst-group performance with threshold tuning). What's going on there? ============== Update after rebuttal: Thank you for clarifying the numbers in Table 1 should match the main text. Recommendation:I recommend acceptance. While I remain concerned about the limited scope of the experiments, I believe the paper adds valuable insights to the overall important topic of robustness / worst-case generalization.
Summary The paper proposes a defense against recent flavours of model stealing attacks by exploiting the insight that the recent effective attack query out of distribution examples to the victim model. Pros: The idea of using ensemble of diverse models to create discontinuous predictions on OOD datasets is interesting. 2. Diversity Objective If I understand the training objective correctly (Fig. 2b, Eq. 3), each model fi in the ensemble is encouraged to generate a random prediction when queried with out-of-distribution (OOD) inputs. However, my first concern is that is also naturally true for benign users.
The model produces impressive high-resolution and long-horizon results, and is extensively evaluated on Kitti, Cityscapes, and dancing data, outperforming some previously proposed methods. This paper proposes a VAE based hierarchical model for video prediction. The real authors of this paper are Nevan Wichers and Ruben Villegas.
This work proposes the first collective robustness certificate that considers the structure of the graph by modeling locality in order to derive stronger guarantees  that the predictions remain stable under perturbations. This paper addresses the limitation of the existing adversarial robustness certificates that ignores that a single shared input is present, and thus assumes an adversary can use different perturbed inputs to attack different predictions. The improvements of the collective robust certificate over the existing ones is sufficiently high in terms of certified ratios. The arguments are valid on the limitations of independent based certificates for collective tasks. Pros: This is the first effort that considers collective robustness certificate. In summary, I like the novelty of this method and the through experiments that were conducted that illustrate the efficacy of the proposed collective certificate, thus I recommend an accept.
The paper discusses, at length, two previously proposed algorithms named Entropy-SGD and Replicated-SGD and demonstrates, using (i) controlled experiments where Belief Propagation (BP) can be used to estimate the local entropy integral precisely, and (ii) empirical results on deep networks that flatter minima generalize better. This paper studies local entropy measures for characterizing flat regions in the energy landscape of deep networks. The paper revisits the line of work that initiated this debate and shows that flatness, as measured by local entropy instead, indeed correlates with good generalization. Clearly, the contributions of this work are not on either new algorithms or new measures, but rather it is a comprehensive empirical study that nicely brings seemingly different things together. (4)" and "Eq. (4)". Update after response: I appreciate the authors making their contributions clearer, and adding details about the training loss and error. I am recommending a weak accept but I am willing to increase my score if the authors make a convincing case against this concern.
[1] Yang Liu and Hongyi Guo. Peer loss functions: Learning from noisy labels without knowing noise rates. Generalized cross entropy loss for training deep neural networks with noisy labels. We can get the same β by very rough estimation(their approach) in instance-independent noise settings. As such, I am raising my score to a 6, and would like to recommend accepting this paper. Unfortunately, I think the theoretical analysis for the noise-robust loss is orthogonal to their sampling sieving approach. I would appreciate if the authors of the paper could provide further insights and intuition on why the introduced confidence regularization improves noise robustness.
Summary This paper presents a new method for structure pruning called ChipNet. The ChipNet employs continuous Heaviside function with commonly used logistic curve and crispness loss to estimate sparsity masks. == Cons == Many previous works propose a new sparsity penalty function, and associated optimization, for neural network training. Therefore, I recommend accepting this paper.
The paper under review introduces a number of geometric measures (isoperimetric, isocapacitory ratios that relate to Brownian motion or heat diffusion probabilities) that are applied to study neural network decision boundaries locally. Title: HEATING UP DECISION BOUNDARIES: ISOCAPACITORY SATURATION, ADVERSARIAL SCENARIOS AND GENERALIZATION BOUNDS
Summary: This paper introduces DARC, an RL approach that aims to transfer from a source environment to a target environment with different dynamics. Theorem 4.1 provides a theoretical guarantee on the performance of a policy trained on such a modified reward in the source domain by giving a bound on the performance in the target domain, under a very mild assumption that the optimal policy on the target domain achieves similar rewards when put in the source domain. Summary This paper proposes a method for domain adaptation in RL where the source and target domains differ only in the transition distriubtions. The empirical evidence shows that the proposed method has very similar performance to the RL on target baselines while improving over other domain adaptation baselines in four continuous control environments. Post-rebuttal Update The authors have shown new experiments on icy environments that show good results for the proposed method (DARC). Overall Recommendation Although I believe the method to be quite restricted by the assumptions that it makes, overall, I recommend acceptance, but it could be stronger after clearing up a few things.
Based on their findings, they propose a new supervised pretraining method, which has a good trade-off for transfer learning applications, and validates with other contexts such as few-shot classification and landmark localization. Authors analyse in detail the difference in performance between self-supervised and supervised pretraining and propose a new method to train model which improves over standard supervised models when used as pretraining. Conclusion: I believe the paper is strong enough for publication.
They propose two energy functionals (first one based on maximizing the norm of the parameters and the second one making the adaptation in closed form, based on the NTK) that approximate the MAML's infeasible learning objective. -Meta-RKHS-II Here, the authors propose an adaptation function based on the NTK and the gradient flow. I am particularly impressed by the second algorithm, Meta-RKHS-II, which derives a closed form-solution to gradient-based adaptation in RKHS that they then map back into parameter space via NTK. -Section 4.3 and 4.4: The Meta-RKHS methods significantly outperform other approaches in the case of adversarial attacks and out-of-the-distribution tasks. The authors perform extensive experiments on regression and classification datasets and compare their results with other MAML-type algorithms. The work is interesting and  supported by theory inspired from the NTK theory, and adds to the newly expanding literature in the use of kernels in meta-learning (unlike the authors' claim in the introduction, theirs is not the first meta-learning paradigm in the RKHS cf (Wang et al 2020, Cerviño et al 2019)). Thus, I recommend acceptance. Strengths This paper is generally well written and proceeds to develop insights into gradient-based few-shot adaptation on first principles from NTK theory.
Paper summary The authors empirically investigate the calibration performance of NN-GPs in CIFAR10 and several UCI data sets, in three forms: Bayesian inference for the NN-GP function-space prior, through a softmax link function In particular, through a series of experiments, the paper investigates their uncertainty properties and answers the question "how calibrated are the predictive uncertainties for in-distribution/out-of-distribution inputs?": i) a comparison between GP classification with the infinite-width neural network kernels and finite width neural network classification was provided to test the calibration, The stated goal of this paper is very ambitious: how NNGPs provide better confidence prediction, in terms of calibration, OOD data  and distributional shift. This is an interesting paper which evaluates the calibration of NNGPs, including around OOD detection. Next the authors "describe" their NNGP and try an impressive number of architectures over several datasets, however, assembling the paper seems to have been an hasty business where many clarity issues have been left unresolved, and these are severe, especially for reader such as myself who are not an expert in GP. Thus, I will keep my score as is -- 7: good paper, accept. It could be of high significance for a practitioner like myself interested in added methods improving the calibration of an existing model, but the authors set aside computational complexity issues and do not give an implementation. Figure 2 is not readable or comparable to Ovadia et al and should be made larger (maybe remove the Brier Score?): Like in table 1, accuracy is in the 0.6 range when it should be 0.9
Summary The paper proposes a framework built on DVBF-LM, extended to dense 3D mapping. Currently, the related work is too focussed on traditional SLAM approaches (like VinsMono) and 2D/2.5D methods like VAST/DVBF-LM. The probabilistic graphical model considers the observations, dynamics and latent states of the agent (i.e. the pose and dense map). The paper also reports two outlier data points in the experiments where the learning-based dynamics model leads to significantly higher localization errors. Specifically, the paper seems to marginalise most of the recent methods that try to combine deep learning with dense SLAM. Strengths The work builds on a fundamentally new and interesting line of generative variational approaches to SLAM
The authors introduce Contextual Transformation Networks (CTNs), a replay-based method for continual learning based on a dual-memory design and a controller that modulates the output of a shared based network to task-specific features. This paper tackles online continual neural network learning (following Lopez-Paz & Ranzato, 2017) with a combination of techniques: (1) a controller (or base parameter modulator, or hypernetwork) is introduced which produces task-specific scale and shift parameters, which modulate the feature maps of a base model (Perez et al., 2017); In particular, the authors introduce a dual memory framework that contains an episodic memory for base networks and semantic memory for task controllers. Summary of paper This paper introduces a continual learning method called Contextual Transformation Networks (CTNs). The authors provide different metrics for continual learning (not just average accuracy), and compare against some strong baselines. Pros: Results are generally strong in comparison to other memory based CL techniques on accepted benchmarks. Cons of paper There are related works that I think the authors can mention, which have some similar ideas as in CTN (although all the ideas are never all put together as in CTN): (a) FiLM layers for meta-learning / continual learning / multi-task learning: [1] Requeima et al., 2019, "Fast and Flexible Multi-Task Classification using Conditional Neural Adaptive Processes" Also, the description of HAT (Serra et al.,2018) and task-conditioned hypernetworks (von Oswald et al., 2020) is not very accurate, as neither method requires "unbounded growth of the network"; in fact, both are quite related to the authors' approach, and both deserve more attention.
Questions from my understanding the issue of prior work that constrains the domain-related MI to 0 (described in Appendix, section A) appears for any two domains (since, particularly in the beginning of training, there will always be differences in the goal-reaching distributions of expert and policy data). The method employs an adversarial imitation learning objective function that incorporates proposed mutual information constraints that are intended to force the representation space to be invariant to the domain of the data sources, and instead only encode goal-completion information. "Domain-Adversarial and-Conditional State Space Model for Imitation Learning." arXiv preprint arXiv:2001.11628 (2020). Post-rebuttal updates: After reading the author response and other reviews, I agree that the difference between the paper and prior works is now much more clear and the empirical evidence shows that the new method works well, though I'm still concerned about the part where the authors add many components and make the algorithm much more complex and potentially hard to work in practice. --> an additional baseline "DisentanGAIL w/ domain confusion loss & prior data" is needed, which additionally trains the domain confusion objective on the prior data collected for the DisentanGAIL prior regularization objective, to allow for fair comparison of both regularization approaches with access to the same data missing baseline results on harder tasks: on the harder tasks shown in Fig 3 there is no evaluation of the baseline methods, which makes it hard to judge how hard these tasks actually are for prior third-person visual imitation approaches Weaknesses not fully fair comparison to baselines: since the main novelty lies in the introduction of novel regularization objectives, the "DisentanGAIL w/ domain confusion loss" is the main comparison method since the only difference to the proposed method is the representation regularization function. what differences result in the substantial performance difference between TPIL and DisentanGAIL w/ domain confusion loss? --> how do you think an approach like the proposed one would scale to visually substantially different environments, eg from one kitchen to another? For pros, I think the experimental setup is reasonable and the authors conduct relatively thorough ablation studies to show the usefulness of various components such as the prior data constraint, double statistics network, and spectral normalization regularization, which is helpful for understanding the method. [3] Kim, Kuno, et al. "Domain adaptive imitation learning." arXiv preprint arXiv:1910.00105 (2019). Suggestions to improve the paper add an additional baseline "DisentanGAIL w/ domain confusion loss & prior data", as discussed in the "weaknesses" section, particularly for the transfer tasks on the bottom right of Fig 2 in which the discrepancy between DisentanGAIL and the baselines is the largest add evaluation of baselines (particularly DisentanGAIL w/ domain confusion loss (w/ and w/o prior data)) to the harder manipulation environments in Fig 3 to show the benefits of the introduced regularizations since the proposed method addresses a concrete problem of prior work (as explained in appendix Section A) with a clear intuition, it could be nice to add a toy experiment early in the paper that demonstrates this effect empirically for an easy-to-analyze imitation problem, showing that for MI=0 the agent cannot properly learn to imitate since it is unable to capture the relevant information --> since this is a different assumption from prior work on cross-domain imitation it would be good to mention this earlier, maybe in a dedicated "Problem Statement" section for qualitative matching results in Fig 4 in the appendix it would be nice to show the corresponding matches found when using the domain confusion loss instead of the proposed regularizations to see whether some of the failure cases are interpretable --> such an experiment could help to further motivate the need for the new regularizations the additional assumption of a "prior dataset" collected in both domains for additionally constraining the latent representation is first mentioned in section 4.2
Summary: In this paper, the authors propose a multimodal transformer network for audio-visual video representation learning. Empirical results on both audio and video understanding tasks demonstrate that the proposed method does indeed learn useful representations, and that multimodal training provides the expected boost to results. It builds on a line of research on multi-modal video understanding that utilises transformers where these works: 1) fix one of the transformer models (e.g. BERT) and 2) utilise tokens and thus do not train the approach in an end-to-end fashion. To allow for end-to-end learning, the paper argues for shared parameters (across the network), primarily: a) sharing weights in CNNs of the same model [understandable] Concerns The authors imply that using a partially fixed model (as has been done with multimodal vision/language tasks) is inferior to end-to-end training, hence the motivation for the proposed parameter-reducing technique, which is the major technical contribution of this work, as presented by the authors. This makes the readability of the experimental section below acceptable bar IMO.
Summary: The paper presents HICTL which enables models to learn sentence level representations and uses contrastive learning to force better language agnostic representations for large multilingual encoders. To take care of these gaps, the authors propose using HCTL as an approach that can learn more universal representations for sentences across different languages. Contrastive losses are promising and the paper shows positive results when adding them to the previously proposed XLM-R model. At the end of reading this paper, I am not sure if implementing what the authors proposed, versus other variations of existing models, would have given the same improvements: While these improvements can be seen across many data sets, they are often modest. Why did you not make an official submission to the XTREME leaderboard?
The paper includes ablation experiments that clearly show that current works that use the body morphology structure to constrain the graph structure of graph neural network based approaches do not actually improve the performance. This paper instead proposes to use Transformers as a simpler mechanism to be able to train and discover the helpful morphological distinctions between agents in order to better solve multitask reinforcement learning problems. Overall an interesting approach that is expected to improve performance in MTRL and needs further exploring. The paper is well written, and methods and analysis approach used are clear. While I do agree that training a graphical neural network to be able to produce a quality policy for a number of control tasks from the opening item environment is difficult the author of the paper might be missing at least one of the key points from the previous work in that you can learn a stronger modularization of policy. There needs to be an ablation with respect to the residual connections added to the Transformer based network to make sure the improvement for amorphous is not working well just because of these residual connections. As the paper explains, "transformers can be seen as GNNs operating on fully connected graphs". Similarly the cyclical structure noticed in Fig. 6 definitely points towards the powerful nature of transformer architectures at learning this relations. In other places, the paper contrasts transformers with GNN-based methods ("substantially outperforms GNN-based methods"), as if transformers were not GNNs. To avoid confusing readers, it would help to explain that GNNs are a broad class that includes both transformers and SMP, which differ in their message passing schedules, etc.
(4) Based on an educated guess of the problem setting (per my understanding), an intuitive and perhaps simpler algorithm would be to learn a goal classifier using the provided {s_0} data, i.e. P(s=goal), as well as a forward dynamics model P(st+1|st,at). This paper introduces an algorithm, called deep reward learning by simulating the past (deep RLSP), that seeks to infer a reward function by looking at states in demonstration data. To achieve this, the paper assumes a Boltzmann distribution on the demonstration policy and a reward function that is linear in some pre-trained state features. Worryingly, appendix D states that learning a dynamics model was attempted by the authors but failed to yield good results. I think environments more aligned with the eventual application areas for a method such as Deep RLSP would make the paper much more compelling. I find the idea of the paper very interesting and the results showing meaningful behavior emerge from a single demonstration are quite nice. Methodologically, the paper stays close to the ideas of RLSP, but instead of computing relevant quantities (optimal policy, forward and inverse dynamics) through explicit derivations, it suggests most exact steps can be replaced by leveraging deep learning, reinforcement learning and self-supervised learning. While the paper does not have strong methodological novelty, it is well written, the approach is sensible and combines well with state of the art deep RL, and the results are certainly interesting. Overall I think the paper has the potential to be a good paper but could still be substantially improved and I'm leaning towards rejection.
In contrast to the Kanerva Machine, the authors simplify the process of memory writing by treating it as a fully feed forward deterministic process, relying on the stochasticity of the read key distribution to distribute information within the memory. Specifically, this paper proposed a novel memory allocation scheme,  replacing the stochastic memory writing process in prior works KM[1]  and  DKM[2] with a set of deterministic operations. 1508-1518. 2018. This paper proposes a new memory mechanism based on the Kanerva Machine inspired by computer heap allocation. The machine stores the knowledge more efficiently through the sharable part-based system and the authors simplified the writing mechanism by designing the memory deterministic. Basically, I think the idea of this paper (simplified write mechanism and sharable part-based memory) is good enough and you evaluated it well. Pros: The authors combine the idea of differentiable indexing in Spatial Transformer (Jaderberg et al., 2015) into the memory of Kanerva Machine (Wu et al, 2018a;b) and prove by experiments that this allocation scheme on the memory helps improve the test negative likelihood. To be specific, the K++ model is Kanerva Machine + Spatial Transformer + a powerful encoder (namely, Temporal Shift Module).
This paper proposes a novel framework called VA-RED2 to reduce spatial and temporal features to be computed for video understanding, which can reduce FLOPs when inferencing the video but remains the performance. Experiments are thorough. This submission has done experiments on both action recognition and action localization task. For example, AR-Net already has shown that a policy network can decide the video input resolution, i.e., spatial dimension, adaptively, leading to improve both efficiency and accuracy. For the video action recognition task, experiments are carried out using Mini-Kinetics-200, Kinetics-400, and Moments-In-time datasets. Summary The paper presents a framework to reduce internal redundancy in the video recognition model. Directional Temporal Modeling for Action Recognition, ECCV2020
-> "If local signals are not representative of the global goal" The paper proposes a method for decoupled training of neural networks called SEDONA. The approach achieves better performance than using backprop on small datasets (CIFAR10 and TinyImageNet), and comparable or slightly improved performance on ImageNet with 2x claimed training speedup. Summary This paper proposes a differentiable architecture search approach for splitting a deep network into locally-trained blocks to achieve training speedup. Do they correlate positively? To what extent can such decoupled feedback implement credit assignment in a neural network? S3: On ImageNet, the method achieves slightly improved performance with claimed 2x training speedup. -> "to flatten the learning landscape", Section 4.1: "to let auxiliary networks in the pool computationally lightweight"
This paper proposes a teacher-student training scheme to incorporate the useful information of trajectory to improve the predictive performance of model-free methods. It uses a teacher model to learn to interpret a trajectory of the dynamic system, and distills target activations for a student model to learn to predict the system label based only on the current observation. Typos: using only using model-free methods This paper presents a student-teacher framework, where the teacher network can be used to select and prioritize the relevant properties of the given dynamical system that should be learned by the student. Summary: This work tries to find a compromise of model-based and model-free methods, using a teacher and student network . It interprets the trajectories and provides activations for a student network that is supervised for a given task using the current state. Overall, I have a mixed feeling about this paper and I currently stand between scores 5 and 6. UPDATE: My major concerns were addressed in the revised version of the paper. The proposed model is interesting and may lead to a series of follow-up studies that leverage the strengths of both model-free and model-based methods using knowledge distillation techniques. The argument should then be if the proposed method outperform model-based methods given the same complexity (e.g. the number of parameters) or same amount of data.
The authors use a general construction for proving universality of equivariant networks for the point cloud group, rather than being specific to certain architectures. The authors accomplish this by writing the network as a composition of an equivariant function from a class F_feat and followed by a linear pooling layer. pros: provide sufficient conditions for equivariance of shape-preserving architectures to satisfy the universal approximation property prove two methods based of Tensor Field Networks that satisfy the universal approximation property raise two simple models based on the TFN Recommendation: The authors proof the useful statement of universality of a prominent class of neural networks, which is why I recommend the acceptance of this paper. Post rebuttal With consideration of the authors' responses to reviewer questions and revisions to the submitted work I have changed my rating to clear accept.
This paper presents a new CNN module to learn video feature representations for action recognition, with a particular focus on increasing channel interactions for spatio-temporal modeling. The paper proposes a novel Channel Tensorized Module (CT-Module) to construct an efficient tensor separable convolution and learn the discriminative video representation. Strengths: The paper's novelty is the first method for exploring the spatial/temporal tensor separable convolution along each sub-dimension. To achieve that, the authors propose to divide feature channels into several sub-dimensions (called channel tensorization) and then perform group convolutions at each sub-dimension sequentially to improve channel interactions. Paper strengths I think the ideas proposed in this paper are interesting and I would not be aware of any architecture that would use a similar arrangement of tensorization and attention in the mid-layer part to allow for a lightweight 3D convolution architecture. Addressed Concerns: corrected the typos. Majority of the weaknesses are addressed. Addressed Concerns: corrected the typos. Majority of the weaknesses are addressed.
Inspired by the observations of feedforward inhibition in the brain, the authors propose a novel ANN architecture that respects Dale's rule (DANN). Summary: It is shown that Dale's principle can be observed in feedfoward ANNs if one uses inhibitory neurons in the form of feedforward inhibition, while the other neurons are purely excitatory. Although, I find the contribution interesting, my enthusiasm is tempered by the following two issues: Although feedforward inhibition has its place in the brain, most connections of inhibitory interneurons with excitatory neurons are reciprocal, resulting in feedback inhibition. Pros:1.To my knowledge, this is the first E/I network that could achieve comparable performance with the standard ANN model on MNIST task (although at the same time, I have to say that not too many papers have studied and reported this issue). 2. The ingredients in the proposed model is well motivated in neuroscience, such as the feedforward inhibition, and E/I balance, as well as no connections between I neurons across the different layers. Drawing from that experience, the mutual inhibition within layer may provide a natural mechanism to keep balance in the output distribution as shown for example in mean field models that investigate the regulation of activity in a dynamical neural layer (see for example https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1003133).
In particular, it focuses on the expected loss reduction (ELR) strategy, analyzes its problem, and modifies the original ELR method to make sure the active learner converges to the optimal classifier along learning iterations. The proposed algorithm is simple and addresses the problem caused by only mitigating the mean difference. The proposed algorithm can diminish the mean difference and ensure that Mw(π∗(θ)) converges to 0. Strong point: The paper's finding on the existing ELR method is interesting and novel. In the synthetic experiment, it should be possible to simulate a case with ground truth optimal classifier and verify whether the proposed method actually converges to the optimal. Additional questions and suggestions: I think the paper would be improved if there is a discussion on how the proposed method can be extended to deal with high-dimensional data and/or using deep learning models.
The paper proposes to use graph-based networks for evaluations of PDEs with continuous time formulations. In contrast to existing works on continuous time ODE formulations with graph structures, the proposed networks incorporate relative spatial information in order for the network to evaluate spatial derivatives in addition to the temporal dynamics. As mentioned above, previous methods had already proposed the usage of graph neural networks with continuous time for the learning of differential equations, and I am not sure that the addition of spatial mesh information to such a graph neural network constitutes a significant enough modification at this point. I am not an expert on the experimental side of this area, but my sense was that the performance of the approach is relatively good, especially compared to other methods when the time step is large. I hope the authors can shed light on this aspect during the rebuttal, as apart from this relatively central open question I like the paper. Overall, given the "cons" described above, notably the potential lack of strong novelty in the proposed method, and the lacking experimental description and results, I am for now classifying this paper as marginally below the acceptance threshold.
--The visualization of the key indicators may be helpful for understanding how batch normalization works and how to remove batch normalization.--The experiment results seem that the proposed method achieves good performance in large-scale ResNets. --update--- I am upgrading to 7: Good paper accept; as my concerns have been addressed by the additional experiments. UPDATE: The author has addressed most of my concerns, but regarding the motivations and the benefits for the community, I still keep my score. (3)    It's better to add some accuracy comparisons with other removing batch normalization works. Can you give the visualization of the same indicators of other initialization methods like Fixup initialization and make some comparison? The proposed method is not yet a drop in replacement for BatchNorm in general, but it can be useful in specific circumstances, i.e. small batch-size training.
This paper propose utilizing the existing "lottery ticket" result for constructing binary neural networks. The paper has many simulation results to support the theoretical guarantees, and the proposed approach on binary-weight networks has advantages over existing methods. This work has some novelty, in the sense that I haven't seen any other papers on untrained binary neural networks. Post rebuttal Thanks the authors for clarifying and revising the paper.
Summary The authors present the idea of adaptive stochastic search as a building block for neural networks, as an alternative to other "inner loop" optimization methods like gradient descent. Pros : Using adaptive stochastic search allows the inner optimization module to take multiple iterations without unrolling the computation graph (unlike meta-learning methods), since the initial value used by the module is arbitrary, and not provided by the network. They propose to perform the estimation of the gradient of the optimum of the embedded problem with respect to its parameterization by the differentiation of one step of a stochastic search algorithm. Cons There are some cases (although not in general) of meta-learning for adaptation, for example where the update can be described in an implicit fixed point way or when the method used in FirstOrderMAML , which allows not needing to unroll (when backpropagating) the function to be optimized. I do not recommend to accept the paper. Perhaps the authors could add at least some analysis of the variance of their gradient estimator compared to other gradient estimators for embedded optimization problems for different examples and show how the variance behaves depending on the difficulty of the optimization task (dimensionality, curvature) taking the above perspective into account. More analysis about the quality of the gradients obtained by the suggested method compared to other methods would improve the paper: How many unrolling steps do what to the gradient variance? For non-global optimizers (such as gradient descent) starting in such a way that we do not end up in the wrong local optimum is crucial. Another small suggestions: On Page 4 "desirable properties of optimization" seems rather vague, perhaps it should be stated more specifically (most likely what is meant is smoothness of the resulting objective function).
The paper also provides a hypothesis why does it happen -- that the graidents of difference examples are orthogonal in the "bad" mode and proposes a measure called "alignment" to predict the generalization reghime of the network. Overview The paper studies how the generalization of the neural network trained with SGD is affected by the scale of the random initialization. It then proposes an alignment measure which correlates with generalization for different initial scale. The authors also propose a measure of gradient alignment which they show correlates with generalization performance Reasons for score: Overall, I find the paper to be a bit borderline. I strongly recommend the paper to be accepted. The combination of things is too much for me to recommend acceptance out of the box, but the things are relatively small and I think easy to address, and I'd be happy to increase my score.
This paper proposes ANT to solve the problem of learning Sparse embeddings instead of dense counterparts for tasks like Text Classification, Language Modeling and Recommendation Systems. It also proposes a probabilistic interpretation of their method as a non-parametric Bayesian dictionary learning model, which can be inferred by optimizing the small-variance asymptotic objective. In this paper, the authors proposed a method to learn efficient representations of discrete tokens. In step 2, they learn a sparse matrix that is used to relate all tokens to the set of chosen anchors. Hence the paper proposes to only store a few anchor/latent vectors (the matrix is A with |A|<<|V|). It's not clear how much gain (especially the experiment section, for fair comparison purposes where other methods do know use domain knowledge in particular) is from incorporating domain knowledge; an ablation study might help. For language tasks and word embeddings, anchoring method has been shown to be effective in several tasks already (e.g. [1] http://papers.nips.cc/paper/8152-the-global-anchor-method-for-quantifying-linguistic-shifts-and-domain-adaptation). i.2) Bayesian non-parametric is useful because it can automatically learn the model size. What I agree with the authors are: i) Using properly chosen basis vectors may greatly reduce the memory cost for embeddings, especially for huge vocabulary sizes (e.g. over 100 million).
Among others, the authors also consider early-stopping in a non-parametric RKHS setup, showing that an appropriate interpolation between NGD and GD achieves optimal rates with a much smaller number of steps compared to GD, a difference which becomes larger for "difficult" problems (which require more weight on the Fisher preconditioner). A5) For parametric least squares with a mis-aligned ground truth parameter, it is shown that early stopping with NGD achieves lower Bias than any other pre-conditioned gradient descent (Proposition 6). A3) For non-parametric regression, gradient descent pre-conditioned with the inverse regularised population covariates covariance is considered. Rosasco "Asymptotics of Ridge (less) Regression under General Source Condition", arXiv:2006.06386 (2020) This makes the paper a strong contribution, and I am in favor of acceptance.
The benefit of using a biased compressor is its low variance; this is what improves the performance. -->The paper has solid theoretical reasoning via a convergence analysis that does demonstrate that for certain (common) parameter choices the induced compressor is expected to have better convergence than the biased counter part Weaknesses Neither the theoretical analysis (comparison of upper bounds) nor the experiments (comparisons for some specific biased and unbiased compressors) in this paper convincingly show that the proposed approach will always be theoretically and practically better than Error Feedback. How does the approach work with methods combining sparsity and communication delay, e.g., Sparse Ternary Compression (Sattler et al. 2020).
Using DDEs is a novel technique in machine learning and can complement and build on the framework of Neural ODEs and help model systems with time delay dependencies and overcome some limitations of ODEs. However, in fine, the average performance is nearly of the same order, a significant difference only on MNIST but with a very bad score considering this is an easy problem. -- The motivation for resorting to DDEs over ODEs is laid out reasonably well, although I think the intuition behind why NODEs cannot learn certain functions is lacking (specifically, that trajectories cannot cross due to the lack of "memory" provided by the delay). Some of them could be easily improved I think, others call for further  work. This is a promising direction for the community to go to push past current ODE modeling limitations, but I recommend for rejection in the current form due to improper characterization and evaluation with respect to prior work (more details below).
The paper proposes a new Information Bottleneck objective, which compresses the latent by learning to drop features similar to DropOut. Unlike DropOut, a different probability is learnt for each latent feature/dimension using Concrete Relaxation. Summary This paper proposes the Drop-Bottleneck (DB) method that performs feature selection during the training with the mutual information. Originality and significance aspect This paper combines mainly two ideas 1) classic feature selection (choose Xis to drop) with respect to the mutual information (between X and Y) 2) Information Bottleneck (IB) formulation that maximizes the prediction-term mutual information term (between Z and Y) and minimizes the compression information (between X and Z) simultaneously. While the approach is limited to dropping input features, which does not make it a general IB objective, it seems to work very well in the presented RL experiments as well as show robustness that is better than DVIB's. The paper does not discuss connections of the presented approach to prior works for (discrete) feature selection. The paper does not perform experiments on datasets with meaningful features where a feature selection makes more sense than for specific pixels in images. Weak Points: I found the reinforcement learning experiments not convincing since only a fixed region of the input is modified by noise (ie. Furthermore, the experiments do not fit to the focus of this paper on reinforcement learning. I would like to recommend this paper to be accepted.
Using comprehensive experiments on synthetic datasets and real-world datasets, the authors verify that the proposed method can improve the robustness of the classifiers against noisy labels. This paper tackles the problem of learning with noisy labels and proposes a novel method CDR which is inspired by the lottery ticket hypothesis. I recommend to accept this paper, and hope that the authors can address the above issues carefully. The proposed method makes use of the memorization effects of deep models. Questions: I don't think the ablation is convincing – why does the proportion of non-critical parameters is assumed to be the same as the noise rate? The proposed method implicitly exploits the memorization effects of deep models, and can reduce the side effect of noisy labels before early stopping. The major comments and issues are as follows: ------Major comments------
The paper proposes a framework to generate good architectures according to the datasets. In particular, a framework, MetaD2A, is proposed, which yields a neural architecture for a new dataset. 2019. The authors address neural architecture search (NAS) scenarios. In a nutshell, the framework learns a "dataset-to-neural-network-architecture" transformation using a database of datasets and architectures. No comparing to other methods on fast adaptation by NAS such as [2]. I think there are some other perspectives to perform ablation study. In my view, I think MetaD2A pays more emphasis on meta-learning in NAS field. I think the main contribution of this paper is their intuition that performing neural architecture search rapidly from the datasets, while the components are all proposed before in different NAS scenarios.
The paper builds upon previous lines of research on multi-task learning problem, such as conditional latent variable models including the Neural Process. To summarize, in the context of neural processes I feel the paper makes good methodological contributions in presenting a much cleaner and more natural (from a Bayesian perspective) version of the model that has more of the flavor of standard amortized inference for latent variable models. To start with, the authors clearly demonstrate the value of both Bayesian context aggregation and a MC based likelihood approximation scheme on precisely the same types of problems that existing neural processes papers (e.g., Garnelo et al., 2018) have considered (with the notable exception that the 2D image completion task considers only MNIST as a target dataset). ================= Score raised to 6 after inclusion of MA + SA results in rebuttal. The authors show that this improves predictive performance (in terms of likelihood) compared to mean aggregation MA that it replaces on various regression tasks with varying input-output dimensionality. I believe that this is a relevant paper for the conference.
Summary The paper points out a limitation of the implicit option version of the Variational Intrinsic Control (VIC) [Gregor et al., 2016] algorithm in the form of a bias in stochastic environments. The differences between the proposed algorithm and VIC shows that the intrinsic reward now has an added term which depends on an approximate model of the transition probability distribution. The new experiments and visualizations have been helpful (I am happy with the author's responses to R3), but the overall clarity of the paper is still lacking due to the dense mathematical notation. Feedback to authors The paper introduces extremely dense notation. Overall, I give this paper a score of 5 / 10, primarily because of (1) a lack of clarity and (2) the limited experiments. Considering the experiments on partially observed environments presented in the VIC paper, this paper chooses a much simpler set of discrete environments for empirical analysis instead of stepping up to more complicated environments which would have strengthened both the motivation for fixing the bias of VIC and the empirical evidence of the GMM algorithm (Algorithm They point out that, in stochastic environments, the implicit VIC formulation in the original paper is missing a term in the mutual information (involving log likelihood ratios of state transitions). Strengths The paper provides a sound theoretical analysis of the limitation of the VIC implicit-option algorithm, the proposed fix and a practical algorithm (Algorithm 2). Pros: The missing term highlighted, and the derivations of solutions generally looked correct (at least at the level I followed them). Would be a clear accept if you could show that, but as is the paper's contribution is bordering on acceptance.
Edit after Rebuttal I thank the authors for their engagement with my review. Summary Using backward error analysis, the paper argues that SGD with small but finite step sizes stays on the path of a gradient flow ODE of a modified loss, which penalizes the squared norms of the mini-batch gradients. However—as the authors themselves note in their critique of SDE approximations to SGD—the devil is in the details with continuous time approximations. Many of my comments and questions have been resolved and, consequently, I have increase my score and recommend accepting this paper.
Summary The paper provides an extensive empirical analysis of Pruning-at-Initialization (PaI) techniques and compares it against two pruning methods after (or during) training. Further, the ablation studies performed on these techniques yield surprising results, both in isolation (inverting GraSP improves accuracy!) and when compared to magnitude pruning after training (these three are invariant to shuffling and re-initialization). In fact, an intuitive ablation setting is to test for the effect of training with the same saliency measure, as done in Section 6, but unfortunately the result out of this seems to be unhelpful for figuring out the potential cause of poor performances for pruning early methods.
The paper presents an interesting approach to solving the on-screen vs off-screen sound problem in audio-visual source separation. This paper proposed an unsupervised method for open-domain, audio-visual separation system. -----Experiments----- I understand that it is hard to obtain single source on-screen clips, but it could have been better if the authors had collected some small samples and test the single mixture separation performance. The proposed method extends the recent unsupervised source separation framework MixIT by conditioning video input. I would expect the authors to run an ablation study / analysis to better understand their contribution to the final model performance. Pros Audio-visual sound source separation is an impotant task.
In the first stage modeling, the authors proposed a new phoneme-level acoustic condition modeling in addition to the speaker and utterance-level approaches. In this paper, the authors present AdaSpeech, a TTS system that can adapt to a custom voice with a high quality output and a low number of additional parameters. A global acoustic embedding conditions the decoder in addition to speaker embeddings, in the hopes of accounting for recording conditions, and, I suppose, timbre, which should then be disentangled from the linguistic information from the text in the decoder during pretraining and adaptable to new recordings at fine-tuning/inference. Its multi-phonetic-level acoustic condition modeling approach seem technically new and interesting, Pros This paper takes one important issue of current speech synthesis area: TTS adaptation to new voice. On the contrary, the phoneme hiddens used in phoneme-level acoustic predictor do not seem to contain any personal voice information because they are resulted from the phoneme encoder that uses text information only as its input in Fig 1. Similarly, I highly doubt the utterance-level acoustic condition modelling does not also capture speaker information. Please clarify this issue. The overall structure of acoustic condition modeling in Figure 2 (a) is not so clear. (Both the normalization parameters and the speaker embedding itself are fine-tuned.) I am not sure what the speaker-embedding is left to do with all this acoustic-level input, but OK. There is little discussion on the theoretical side of the acoustic condition modelling, such as how the authors are able to determine that the utterance-level and phoneme-level vectors are modelling things like room condition. However notice that, if I'm not mistaken, these acoustic embeddings are used zero-shot; it is only the speaker embedding that is the input to fine-tuning, and this only via the normalization parameters. There is also a phoneme-level acoustic embedding which is used in the same way, which at inference is taken from random sentences (why not in training?), and, I guess, is supposed to cover phoneme-level idiosyncrasies of the speaker, although this isn't clear to me.
This approach dramatically benefits clients with limited computation capability and fully exploits their computation power. "The results show that HeteroFL can boost clients' performance with low computation and communication capabilities by allowing the training of heterogeneous models with larger computation complexities." -> This sentence is unclear and leads to a wrong statement. However, it is worth noting that the core idea of this paper: HeteroFL, is a very simple and elegant way to deploy FL on an heterogeneous client set.
Summary Under the context of learning from demonstrations, the paper studies the problem of leaning interpretable low dimensional representations from high dimensional multimodal inputs using weak supervision. This paper proposes and interesting and novel way to handle weak labels from human demonstrators. Towards this end, the paper proposes to learn probabilistic generative models capturing high-level notions from demonstrations using variational inference. (semi)supervised learning [see Locatello et al. and papers citing this work) Authors motivate the paper using the idea that learning interpretable low dimensional concepts called "common sense" will enable generalization and adaptation. The strength of the paper is in demonstrating that conditional latent variable models can learn disentangled low dimensional represented using weak supervision; which authors effectively demonstrated using real-world experiments. The paper needs to present real robot results and cover related work in more detail. From a robotics point of view, I would have accepted it as a very good application paper, if the authors had also presented real-robot results that show the learned model in action (generating actual trajectories for the robot). Learning this manifold effectively and in an interpretable way, especially using weak supervision, can significantly change how robots can acquire skills from demonstrations and generalize them to new unseen scenarios. Further experiments should be done with other, more standard tasks and potentially user-provided labels to better determine the performance of the weak-label models as compared to the baselines.
This paper compares R-GAP with the DLG algorithm. The authors have improved the paper and addressed my concerns.
Motivated by the sensitivity of RL algorithms to the choice of hyperparameters and the data efficiency issue in training RL agents, the authors propose a population-based automated RL framework which can be applied to any off-policy RL algorithms. Summary: This paper propose a population-based AutoRL framework for hyperparameter optimization of off-policy RL algorithms. Pros: The authors propose an important novel component to PBT-like framework, which is sharing experience among agents in the population. To show that the neural architecture adaptation is a crucial part of the framework (which is a major difference between the proposed method and PBT), the authors might have to move to a complex domain of environments to demonstrate that. They reduced the number of environment interactions significantly compared to baselines like random search and a modified population-based training algorithm.
This paper studies the asymptotic convergence properties of (population-level) policy gradient methods with two-layer neural networks, softmax parametrization, and entropic regularization, in the mean-field regime. Under certain regularity conditions, the paper shows that if the training dynamics converge to a stationary point, this limiting point is a globally optimal policy. The optimization landscape and convergence properties of policy gradient methods have drawn attention in RL theory for a long time, and it is nice to see a work that studies this problem from the perspectives of mean-field limit of neural networks, albeit being completely asymptotic. It would be nicer to spend more space to discuss the major differences of the theoretical analysis in this paper compared to earlier results on the mean-field limit in the supervised learning setting.
This paper proposes an easy-to-implement algorithm for the efficient exploration, which is a temporally-extended version of \\eps-greedy. The paper lists three desiderata for an exploration strategy: (1) that it is simple, (2) that it is stationary, and (3) that it promotes full coverage of the state-action space. Strong points This paper analyze theoretical properties of temporally extended e-greedy exploration in Theorem 1. I think the paper provides a clear argument for simple and general exploration strategies and that ϵz-greedy seems to be an algorithm that achieves these goals. The idea is simple (a generalization of e-greedy) and the discussions nicely illustrate the main properties of an ideal generally-applicable exploration method.
In particular, it proposes to introduce high dimensional and high entropy label representations for group truth, to improve image classification performance from two practical matters --- Robustness and data efficiency, while achieving comparable accuracy to text labels as the standard representation. The results show that high dimensional and high entropy label representations are more useful, which is observed in the experiments related to robustness and a limited amount of training data. I am also interested in that if the audio signal is replaced by pre-trained embeddings, like glove or BERT, as label representation, how the effectiveness of labels is compared with the audio signals? Also, the author argues that audio labels is special, but in the paper, other type of high-dimensional representation of labels that uses external information are not explored, such as word2vec. Ratings: Although the paper shows some interesting research direction, the way the authors show the effectiveness of the proposed method is mostly built on empirical results, which is not enough to claim such a bold argument. -- achieving comparable accuracy with less data in training, I would still suggest the authors to conduct the following studies to enhance the quality of the paper: It could be valuable to future investigate the inherent property that contributes to the improvement, besides high dimensionality and high entropy.
Summary This paper considers the problem of adapting a pre-trained model for few-shot learning in case there is a shift of distribution from the meta-training set. In this setting, the authors argued further that one cannot run the meta training, so the focus of the paper is to improve stage (2) (gradient update process at the test time). The paper also proposes to add task adversarial examples to the training set to help the meta fine-tuning process. Towards this end, the paper proposes to run ensembling fine-tuning with the support set of the test task, and use the weight variances from this process to guide the training. I do like the use of ensemble and also adversarial examples, and intuitively they can improve the test-time adaptation. Following two intuitions/hypotheses: 1) if the model uncertainty of specific parameters is high, then the step size should be small and 2) high uncertainty on input gradients require more adversarial training to improve robustness. The paper provides extensive and convincing experiment results over evaluating the proposed model's robustness to the choice of base stepsizes.
This paper provides sample complexities analysis, showing that wide two-layer neural networks with standard activation functions and SGD optimization is able to capture the data regularity between input and output, modulated by the three kinds of task code considered. Pros: This paper is quite novel in many aspects, including modularity v.s. monolithic, constructing task codes by SQL-style aggregation queries, "inverse counterpart" of multitask learning, connections to cognitive science. The main contributions of the authors are the following: showing that "the two layer neural network can jointly learn the task coding scheme and the task specific functions without special engineering of the architecture" The authors show how complex tasks can be modularly formulated thus yielding a joint monolithic learning possibility. This paper posits a very interesting question about provable multi-task learning by neural nets. The authors clarify and discuss the topic with respect to two landmark papers very well. If we view it as one complex task, with the input-output data relationship modulated by one of the three task codes, then the questions of "learnability" by neural networks in handling this case are well addressed by the proposed theoretical analysis and I can see the empirical study is supportive of this claim. I recommend reject based on the clarity and structure of the paper. I would strongly suggest resubmitting to a future venue a self-contained paper with all the proofs.
I look forward to the author response so that I can refine both my understanding and assessment of this work. Originality: Some key ideas presented in the paper (e.g, eigendecomposition of the transition matrix, successor representation) is fairly standard in previous literature, although the authors show that there is a way to unify some of the previous models with these notions. The paper shows that this model can generate several experimentally observed properties of grid cells, and can be used in navigation of novel/mutable environments. If instead, the fourier component is meant to be distributed among the input population, it the authors should make this clear This paper proposed a model of navigation based on grid cells and the successor representation (SR). It is unclear how it is implemented in the network model, and how this information can get to the grid cells. I have worked at the intersection of theoretical neuro and ML for 8 years and am familiar with some recent literature on grid cell modeling, but still couldn't make sense of many aspects of the paper because of the many assumptions of prior knowledge. it is unclear to me how the successive exploration of possible directions could be implemented in practice by the grid cell network (see below for detailed comments). It is unclear how the "sense of direction" can be calculated in practice by a grid cell network. It is stated that  "a computational role for the neural grid codes: generating a "sense of direction" (eq. Given that in the previous section, the authors propose grid cells as being weighted Fourier modes (eq 10), I worry that there might be explicit band cells  in the proposed model. Concerns: It is claimed that the proposed model generalizes across  different environments. Section 3 suggests the transition structure can only derive from a 2D grid space, and that the method requires a periodic boundary condition hold. Can they demonstrate that there are sufficiently rich problems in the gridworld space where this method provides significant utility? It would be helpful to see a concrete description or simulation of a neural network implementing action-directed SR.
In this work, the authors provide a method for a posteriori calibration of DNN uncertainty with emphasis on constructing a classifier that has PAC uncertainty guarantees. They demonstrate and explore two use cases: applying this technique to get faster inference in deep neural networks, and using the PAC predictor to do safe planning. Strong points: The proposed method provides a provable guarantee on the reliability of a pre-trained methods prediction, which is a very nice property to have in the reliability/safety problem. Pros: Paper is well written Important and timely problem, motivating arguments are well constructed This approach is a simple but good idea, seems grounded in a good motivation and the explored use cases are informative and interesting. Paper appears to be mathematically sound though I did not check all the proofs in the appendix. The authors propose constructing calibrated outputs that have provable correctness guarantees, using PAC-style arguments. The authors define a 'calibrated' probability prediction to be one such that given (for example) an image labeled as a cat, that the probability assigned to the class label 'cat' by the predictor is equivalent to the probability that the classifier correctly predicts images from the class 'cat.' Take for example a predictor \\hat{f} which assigns \\hat{p}(x) = 0.9 to every input 'x' regardless of its ultimate accuracy on the class, then given a new input with unknown label, it is not clear to me exactly how the framework would use the intervals to improve the uncertainty of this classifier, especially given that the class of this new point is unknown. In particular, it seems to me like the proposed intervals only hold their PAC guarantee when the test-time distribution matches the training distribution.
The paper proposes an ego-centric representation that stores depth values and features at each pixel in a panorama. My main problem is that I don't understand what the different method that are evaluated are: Given the abbreviation ESMN introduced in the abstract, I assume that ESMN is the proposed approach. After rebuttal phase The answers provided by the authors and the revised version of the paper sufficiently address my concerns. Still, I feel that in the current form, the paper can be accepted.
(iii) How large should I expect the rank-restricted condition number to be in Theorem 2.1 and 2.2?
The method seems to be quite generic and can be applied to a large range of PDEs. The inverse problem experiment is interesting and highlights a potentially useful application of the proposed method. I liked the paper a lot, and it's definitely a big step-forward in neural operators. I like the idea of learning a mapping for a class of PDEs, rather than optimizing per instance However, our Fourier neural operator does not have this limitation." Paper Summary: The authors proposed a novel neural Fourier operator that generalizes between different function discretization schemes, and achieves superior performance in terms of speed and accuracy compared to learned baselines. The theoretical and experimental sections are, for the most part, clear and concise, although some important details remain unclear/lacking. The subsequent experimentation was extremely thorough (e.g. demonstrating that activation functions help in recovering high frequency modes) and, of course, the results were very impressive. To summarize, I believe that this work should be published. Pros. The authors address an important and practical problem
Summary: This paper found that combining ensembles and data augmentation can harm model calibration. This work analyses the interaction between data-augmentation strategies such as MixUp and model ensembles with regards to calibration performance. The authors note how strategies such as mixup and label smoothing, which reduce a single model's over-confidence, lead to degradation in calibration performance when such models are combined as an ensemble. ########################################################################## Reasons for score: Overall, I vote for accepting.
=== Summary This paper proposes a framework, HyperDynamics, that takes in observations of how the environment changes when applying rounds of interactions, and then, generates parameters to help a learning-based dynamics model quickly adapt to new environments. == Original Review == The paper proposes a model for predicting the dynamics of a physical system based on hypernetworks: given some observed interactions and some visual input, the hypernetwork outputs the parameters of a dynamics model, which then predicts the evolution of the system's state over time. === Strengths This paper targets an important question of building a more generalizable dynamics model that can perform online adaptation to environments with different physical properties and scenarios that are not seen during training. Strengths: The paper addresses an important question, namely, how a dynamics model may adapt to environments that don't fully match its training distribution. Weaknesses: The main claim of the paper is that hyperdynamics network offers better prediction accuracy and generalization than a standard dynamics model. Overall, while the paper presents an interesting idea, the experimental evaluation is not convincing in its current state: Baseline architectures are not fully specified, many of them did not receive the same input, and no benchmark task with previously reported results has been used. The authors have evaluated the method in several object pushing and robot locomotion tasks and shown superior performance over baselines that uses recurrent state representations or gradient-based meta-optimization. Table 1: Motion rediction error -> Motion prediction error
[Summary] Paper proposed to generate the communication message in MARL with the predicted trajectories of all the agents (include the agent itself). Specifically, compared to existing works, this paper proposes to generate messages based on not only current information but also future information (referred to as imagined trajectory) (Section 4.1). After reading this paper, indeed I find the authors failed to capture some important research in this narrow area and it's still not clear how does the proposed method really works and whether it is sensitive to some specific implementing factors. Machine Theory of Mind. ICML-18 I have read over the rebuttal and discussion and will keep my evaluation score as it was since the concerns about the weak performance result still remain. The method suggested - basing messages on predicted future trajectories - is sensible and well motivated.
Summary This paper extends neural compression approaches by fine-tuning the decoder on individual instances and including (an update to) the decoder in the bit-stream for each image/video. This paper considers the problem of per-instance model adaptation for neural data compression, and proposes a new method for end-to-end finetuning the model that is quantization-aware, by introducing an additional term that measures the compression cost of model update to the typical rate-distortion loss. Summary The paper describes an instance specific finetuning method for image and video compression including finetuning the decoder. This paper investigates how to improve the test time performance of learned image compression models through finetuning of the full model. Strength = Method which also considers to finetune/adapt the decoder side of image compression network, for improved performance. Evaluation on the UVG dataset shows encouraging performance, with an average distortion improvement of approximately 1 dB for the same bit rate compared to the naive baseline (without fine-tuning). If the experiment did perform per-instance model adaptation, then it would be much more convincing to evaluate on standard datasets like Kodak and Tecnick from the image compression literature, instead of frames of UVG videos. How sensitive is the compression performance to their choice, e.g., is it possible to discretize so finely that no amount of RD improvement can overcome the model update cost?
1) examples. This effect is measured by the authors by trying out the existing Siamese few-shot detector on 4 datasets: PASCAL, COCO, Objects365, and LVIS showing that the gap in performance on the seen training and the unseen (novel) testing categories is reduced when the base dataset has more classes (e.g. on LVIS where there are more than 1K classes, this "generalization" gap is shown to be minimal). Weaknesses: The paper studies the importance of number of object categories in training dataset and claims that the gap in one-shot detection can be closed by increasing the number of categories. -> either The paper suggests that a major factor for increasing few-shot performance in the few-shot object detection task is the number of categories in the base training set used to pre-train the few-shot model on a large set of data before it is adapted to novel categories using only a few (or even Pros: number of base classes is indeed an important factor in few-shot methods performance (not just in detection) Although the paper focuses on the data used for training, it gave great insights for understanding the generalization of object detector and provides practical guidelines for future large scale data collection.
(p.1, Abstract) we generalize the IRL problem to a well-posed expectation optimization problem stochastic inverse reinforcement learning (SIRL) to recover the probability distribution over reward functions. (p.2, Introduction) The solution of SIRL is succinct and robust for the learning task in the meaning that it can generate more than one weight over feature basis functions which compose alternative solutions to the IRL problem (p.3, Problem Statement) more likely generates weights to compose reward functions as the ones derived from expert demonstrations Intuitively, the main benefit is to allow for k reward-function archetypes that represent the set of expert trajectories well; however, there are no examples nor any evaluation to show in which case this is beneficial. If I understand the algorithm correctly, it is significantly more expensive than standard MaxEnt-IRL which already doesn't scale to such problem settings as it requires iteratively solving the reinforcement learning problem. For example "[...] suffer from the problem that the true reward shaped the changing environment." should probably mean something like "suffer from the problem that the learned reward function is shaped---that is, it is entangled with the dynamics---and, thus, does not transfer to different environments". Also, for the linear reward function, the maximum likelihood objective is convex, so I think that the different weights should even converge to the same solution. The robustness experiment seems to subsample the weights based on the EVD (which requires knowledge of the true reward) before evaluation.
---- Summary This paper proposes FLAG (Free Large-scale Adversarial Augmentation on Graphs), an adversarial data augmentation technique that can be applied to different GNN models in order to improve their generalization. ogbg-code: +2: GCN+virtual node+FLAG, +4: GIN+virtual node+FLAG, +4: GIN+FLAG, +2: GCN+FLAG This paper investigates adversarial feature augmentation for improving the generalizability of graph neural networks. Nevertheless, there are a few drawbacks: Even though the method is completely adopted from prior work and there are no novelties, the authors claim they propose a new solution for graph data augmentation. This paper presented an adversarial augmentation technique for graph neural networks. Strengths: There are various best practices which are commonly applied in common task framework competitions such as data augmentation, adversarial training, and ensembles that improve results by fractions or a small number of percentage points. Hence, my takeaway from the paper (which is in fact a valuable takeaway) is that adversarial augmentation is not considerably effective for graph neural nets, no matter what dataset and network architecture is used. The authors show that their method can easily be added to standard GNN models like GCN, GAT, GraphSAGE and DeeperGCN with minimal changes, and that it improves these models on different tasks. It is true that adversarial feature augmentation has not been studied for graph neural nets, but it is a straightforward idea to apply an existing feature augmentation method on graph nodes, which are represented using feature vectors just like other types of data. The authors adopt an existing augmentation algorithm and apply on the nodes of each training graph, and use the perturbed graphs for training. ---- Justification for score The paper is well written and the method interesting and well explained, but it lacks comparisons with other graph data augmentation approaches.
Cons: Though this paper presents a really focused contribution on training with small dataset, one can see that the paper lacks of in-depth analysis on either the target task or the proposed algorithm. [2]. https://github.com/NVIDIA/DALI/blob/1e9196702d991d3342ad7a5a7d57c2893abad832/docs/examples/use_cases/pytorch/resnet50/main.py#L116 [3]. http://cs231n.stanford.edu/reports/2015/pdfs/ondieki_final_paper.pdf After rebuttal update.
Summary This paper presents a new type of brain-inspired dual-pathway DNN model where the coarse (faster, less accurate) and fine (slower, more accurate) visual pathways augment each other during training and inference (via imitation and feedback) to boost the network's robustness to various noises. [8] Brain-Like Object Recognition with High-Performing Shallow Recurrent ANNs, NeurIPS, 2019 This paper proposed a two-pathway neural network to mimic the interplay between the parvocellular (slow and fine-grained) and magnocellular (fast and course) pathways in neural systems. The proposed architecture is novel and the results support the main claims regarding the improvements in robust object recognition behavior and replication of the psychological experiment. (Backward masking) Visual results alone (Fig 5 and 12) don't properly support the claim that this model "can explain visual cognitive behaviors that involve the interplay between two pathways". In sectino 3.3, in the FineNet-only models, it is unclear how the feedback loop is resolved in the absence of CoarseNet. In section 3.3 it is stated that "this highlights an important goal for the brain employing two-pathway processing". (Related work) Although the authors argued that existing models are conceptually different, it doesn't mean that architecturally similar models [3, Hou et al. ], SOTA in adversarial defense [4, 5], and most importantly other brain-inspired models, targeting robustness [6, 7] or not [8, Tang et al. ], shouldn't be compared against to properly prove the value of this work. [6] Brain-inspired Robust Vision using Convolutional Neural Networks with Feedback, NuerIPS-W, 2019 Recommendation I recommend rejection of the paper given the following two major cons (see details above).
This paper proposes a general aggregation function which summarizes sum, max, and softmax operations etc. Since the OGB benchmark is new and the reported GNN models on the OGB leaderboard were only run for 3 layers, it is crucial to analyze all the variants discussed here in detail to appreciate the performance gain achieved by the proposed generalized aggregation function. Update after Rebuttal: I have read the authors' response but do no change my scores. --- Post rebuttal: I've read the author's response and there is no change in my scores. #####Pros##### (1) The proposed generalized aggregation functions are intuitively and empirically effective. This paper studies how to train deeper graph convolutional networks by using different aggregation function. Hence, I do not vote for acceptance. It would be great if authors can state the novelty of this paper compared to DeepGCNs. Also, what makes difference between the proposed aggregation function and softmax.
However, I still have some concerns: In the experiments of unconditional setting, the authors included Classic, Classic+, and Classical++ as their comparisons. ** Weaknesses (1) It seems a strong limitation that the proposed approach is not able to generalize to different videos or has to be video-specific (i.e., train a model on each input video). First, although there may not exist any resampling method that can directly perform the video texture synthesis, I believe many related graph-based methods could be used to model the transition probabilities of frames. (i) a new pipeline for modeling and calculating probabilities of transitioning between frames of the same videos. The audio conditioned video synthesis part also seems like an extra module which does not influence the completeness of the whole model if not included. Summary of this paper: In this work, the authors propose a method to learn to generate long-range video sequences. (2) It is not true that existing methods fail to generate more than a short sequence of frames, e.g., (Lee et al. 2019) in theory can generate videos with arbitrary lengths. (4) Lacks of analysis or comparisons to justify some important hyper-parameters, e.g., (1) how the softmax temperature term impacts the synthesis results, (2) how t% is chosen to threshold the transition probabilities. (8) What are the results if the classic methods also use the interpolation network? Second, although the authors pointed out that the video resampling (textures) strategy is different from the recent generation-based strategy, they should provide visual results/comparisons to support their claims. (3) The pre-trained interpolation network (Jiang et al. 2018) seems a quite important component for the proposed algorithm to generate smooth videos. It is highly recommended that the authors could present more comparison with the previous baselines in both general idea and model details. Second, this method seems to be example-specific, which needs retraining if fed a new video sequence. In addition, the authors may further discuss the limitations of the proposed method. (2) Extend the proposed approach to audio conditioned video synthesis I am interested in if the proposed method can perform these tasks. In this paper, the authors proposed a non-parametric approach for video generation, i.e., video frame (un)conditional resampling. Specifically, during training a model is used to learn the transition probability between different video segments.
ArXiv, abs/2007.03730. In this paper, the authors propose a certifiable watermarking method for neural networks. The authors did not rebut many of my negative concerns. The method can be used as a black-box watermark (does not require model parameters to verify),  however, the certification bounds only apply to a white-box use case in which the verification can perform inference and test accuracy for a set of trigger images for multiple smoothed versions of the parameters. Did the authors experimented with other NN verification/certification methods such as the one proposed in [4]? The paper does not provide any direct comparisons to other watermarking methods. 2020. The proposed method exploits the randomized smoothing techniques for a certified watermark of neural networks. Can we say that models without watermark cannot attain this trigger set accuracy?
It shows performance improvements on ImageNet compared to previous hardware-aware NAS methods which only optimize the neural network architectures. [1] Neural-Hardware Architecture Search, NeurIPS 2019 Workshop on Machine Learning for Systems. The main difference between this paper and previous hardware-aware NAS papers is that this paper has an additional hardware search space beside the neural network architecture search space. In summary, co-designing neural network architecture and hardware architecture is not new. Co-designing neural network architecture and hardware architecture is not new. It would be good to understand better how the author's NAS approach improves on these previous works? Therefore, I recommend rejecting this submission.
The paper presents a simple addition to the Balanced Accuracy approach - which the authors refer to as 'importance'. This paper proposes a simple and general-purpose evaluation framework for imbalanced data classification that is sensitive to arbitrary skews in class cardinalities and importances. This paper presents a weighted balanced accuracy to evaulate the performance of multi-class classification. Pros The paper deals with practically important topic, and presents a simple, easy and intuitive solution. Some major comments on the paper: The proposed evaluation metric appears to be to show whether machine learning approach A is actually better than machine learning approach B. However, in order to judge if a particular approach is better or worse than another you need some way of showing that your metric is correct. Since the weighted terms e.g., wi play an important role in the proposed framework, more examples should be given to explain how to choose these parameters in different application domains.
Based on the observations and discussions, the paper then proposes a novel lower bound to regularize the neural networks and alleviate the problems of MINE. Some interesting experimental results are firstly provided on a synthetic dataset: the constant term in the statistical network is drifting after MI estimate converges. Summary The paper introduces a generalized version of the mutual information neural estimation (MINE), termed regularized MINE (ReMINE). Empirically, the proposed regularized term works well along with the original MINE estimator and ReMINE  has better performance in the continuous domain 2015. The work studies a neural-network based estimator, referred to as MINE, for approximating the mutual information between two variables. Yes, I agree with the author the drifting phenomenon is not the only problem that the proposed method solves. [R1] Moon et al."Ensemble estimation of mutual information," ISIT, 2017. Overall, I lean toward rejection given current concerns.
The authors propose an algorithm to enlarge the training set for image classification problems in certain medical applications where training data of the target modality is scarce. Authors claim that part of the novelty is that in their method translation is guided by the predictive task, however in [2] the translator is also jointly trained with the segmentation network. Using unpaired image-to-image translation networks for cross-modal medical image synthesis has been studied in [1, 2] (as the authors pointed out in the paper).
This paper studies an important problem: that differentially private algorithms can have disparate impact on model accuracy for different sub communities. If the claim isn't that Algorithm 1 is DP, then the privacy guarantee is restricted to the results in Figure 3, that attack algorithms perform similarly well on DPSGD, FairDP, and significantly better on SGD (non-private). I am not even entirely sure that the proposed algorithm is actually differentially private because the step that finds the optimal clipping thresholds seems to use the non-noisy mini-batch gradients without any privatization (please clarify if my understanding is not correct). -I'm not sure if I understand the optimization problem given in 6a, 6b and how the algorithm is solving it. In order to make the experiment more informative, I suggest authors compared FairDP with other algorithms that have been designed for addressing unbalanced datasets.
The paper presents a subgraph approximation method to reduce communication in distributed GCN training. Towards addressing this issue, the paper proposes a distributed GCN training scheme based on subgraph approximation, and demonstrates its empirical effectiveness on medium-to-large datasets. (Quality) The paper can be significantly improved by positioning with and/or exploring many other (more recent) large-scale single-machine graph neural networks (GNNs). iii) Layer-Dependent Importance Sampling for Training Deep and Large Graph Convolutional Networks, In NeurIPS'19.
The authors propose to adapt the model-agnostic meta-learning algorithm (MAML) of [1] to reflect this hierarchical structure by either observing (Section 4.1, FixedTree MAML) or inferring (Section 4.2, LearnedTree MAML) an assignment of tasks to clusters at each step of the inner loop (task-specific adaptation phase) of MAML; 6430-6439. 2019. Summary In the context of gradient-based meta-learning for few-shot learning, the authors propose TreeMAML, an algorithm that leverages the existence of a tree structure in a task distribution in order to pool inner-loop gradients between tasks. As claimed in this paper, the proposed model lacks scalability since it works well only in the tree-structured tasks. Significance: Results on the hierarchically structured synthetic regression task datasets demonstrate that {Fixed|Learned}Tree MAML: is at least as good as MAML, and often outperforms MAML; Recommendation I currently recommend a clear reject (3).
The authors experiment both with pre-training and fine-tuning of contextual models (BERT-{base,large}) and claim large reduction in training time, with reasonable loss in performance. Experiments for both pre-training (the first of its kind) and fine-tuning show that performance does not drop all that much for GLUE and Squad which are the main set of tasks BERT is typically evaluated on. The main contribution of this work is to use Early Bird Lottery Tickets to reduce pre-training and fine tuning time for BERT. Despite very encouraging results, several important methodological questions about the source of the efficiency gains and other aspects of the paper are left unanswered. (More of a question/nit) Is it possible to also show what happens when a winning ticket for BERT fine-tuning is selected based on the pre-training objective? The authors pitch it as a technique for reducing the training time of BERT and use LayerDrop as a baseline technique that also removes network components. Here are some thoughts and questions that could help improve the paper: Experiments in Section 4.2 are only during the fine tuning stage. This would involve computing the pre-training time (time to learn BERT parameters on Wikipedia) + total fine-tuning time across all datasets (QQP/CoLA/MNLI etc) considered. While a central argument is that most model distillation techniques still require expensive pre-training, it would still be useful to include some of those results in Table-2 since EarlyBERT is comparable to those techniques for the purpose of Table-2. References: https://arxiv.org/abs/2003.03033 I cannot recommend accepting this paper in its current form, but am looking forward to reading the authors' response which might clarify things.
This paper presents a deeply supervised few-shot learning model via ensemble achieving state-of-the-art performance on mini-ImageNet and tiredImageNet. The authors first studied the classification accuracy on mini-Image across convolutional layers and found the network could perform well even in the middle layer. The biggest concern is the contribution of this paper, to be more specific, the proposed method might not be useful and might need to be tuned in other few-shot settings. [1] - Dvornik et.al. "Diversity with cooperation: Ensemble methods for few-shot classification" [2] - Dvornik et.al. "Selecting Relevant Features from a Multi-domain Representation for Few-shot Classification" "Few-shot learning with localization in realistic settings." Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition.
It is empirically shown that, for a specific type of initialization, for less over-parameterized neural networks, the gradient dynamics follows two phases: a phase that follows the random features model where all the neurons are "quenched", and another phase in which there are a few "activated" neurons. In a controlled setting, a list of experiments investigates the dynamics and compares them for different regimes where the relation of number of hidden neurons to number of training samples is changed and for several specific target functions. Since it considers largely simplified settings (specific target functions, data sampled uniformly from the unit sphere, in most experiments the true solution can be found with a sub-network consisting of only one hidden neuron) and since the networks and optimization method (gradient descent) are rather simple, one could expect that at least some theoretical contribution or explanation could be given, which the paper lacks entirely. One can actually observe from Fig. 1 that most neurons are still not quenched when the network departs from the random-feature behavior. The paper may consider having a discussion on recent works (e.g. [1, 2]) that consider linearized behaviors in the regime where m depends mildly on n/d or is almost independent of n, and whether they may conflict with the result in the paper.
=== Summary This paper proposes a benchmark that aims to systematically evaluate models' ability in learning representations of high-level variables as well as causal structures among them. (2019). Phyre: A new benchmark for physical reasoning. To sum up, my reservations towards the degree of novelty in this paper have not changed (I agree with the authors' summary of their contributions, but disagree as to whether proposing a new benchmark constitutes novelty at *CONF*) and I recommend a rejection of the paper in its current form. The authors state: "The main goal of our paper is NOT to introduce novel models, but rather to introduce a NOVEL benchmark and insights/ingredients to study causal induction in model-based RL" and "It is true that the models we use do not learn an explicit structure for causal learning" which corresponds to my original reservations to the novelty of this paper. Though these models do not learn an explicit causal graph, they do learn structure that could allow them to discover causal relationships under certain assumptions" confirms my point that no causal formalism (e.g. connection to the data generating process) is accounted for. Post Rebuttal I would like to thank the authors for the detailed rebuttal.
2017. [2] Li, Yandong, et al. "Nattack: Learning the distributions of adversarial examples for an improved black-box attack on deep neural networks." arXiv preprint arXiv:1905.00441 (2019). There are two main concerns about this paper: The authors claim that their detection method is attack-agnostic, but their motivation in Fig 1 highly depends on the assumption of the attacking mechanism. Especially, NATTACK learns adversarial examples' distributions, and AttackDist is based on adversarial samples' distribution.
The paper applies it to three popular data augmentation methods: AugMix, Mixup, and adversarial training. One main disadvantage of the proposed method is that it seems to be sensitive to the distance function chosen for the label assignment and this function has to be adjusted for different data augmentation schema. The proposed method is general, applicable to different types of augmentations: AugMix, Mixup, and adversarial training. Overall, I currently hold my score as marginally below the acceptance threshold.
Hendrycks et al propose encouraging high entropy predictions on the outlier exposure set, instead of classifying them into a reject class. Training with a large OOD dataset like [Hendrycks et al.] is not common, and the observation in this paper is limited to this setting. Instead, this paper seems to show stronger results, with a more intuitive and simpler method (just classify the outlier exposure set into a separate class) that Hendrycks et al suggest doesn't work as well. Optimization method). It's great that the overall method does better, but I'd like to see at experiments on some datasets investigating what happens if you use the same procedure as OE except K+1-th class instead of entropy. More recent papers studied the use of self-supervised learning (e.g. [4, 5] ) and contrastive learning (e.g., [6, 7,8]) to improve ood detection. Reasons for score: The proposed setting with a large OOD dataset has already been proposed by [Hendrycks et al.], and the proposed method has been experimented in [Lee et al. (a)] and [Dhamija et al.], so the technical novelty of this work is limited. Summary: This paper shows that introducing an abstention class for out-of-distribution (OOD) works well for detecting it when the in-distribution dataset is CIFAR and TinyImageNet is available during training as an OOD dataset. The comparison is unfair. Performances of prior methods are borrowed from original works, but they are mostly experimented in settings different from this paper. Contrastive training for improved out-of-distribution detection. [Lee et al. (a)] Training confidence-calibrated classifiers for detecting out-of-distribution samples.
Pros: The proposed method can predict how a continuous-time time series will evolve under a sequence of interventions. It seems more accurate to refer to the proposed approach as one for counterfactual inference with confounders in the irregularly-spaced time series rather than continuous-time setting.
In addition to that, the authors conduct a lot of experiments involving non-linear networks and real-world datasets, that show the inverse dependence of the variance of gradient and batch size throughout the training. The paper shows that the variance of the gradient has an inverse dependence on the batch size in linear networks, subject to the knowledge of the initial weights. The result is more interesting than in the linear regression case because the 2 layers linear network is non convex, and therefore, one can imagine having multiple local minima and 2 different trajectory starting from the same point to diverge at some point, which could lead to drastically different gradients. My major concern is that the authors don't provide an application of their theorems, i.e. a setting where the exact knowledge of the variance of the gradient is useful. Conclusion Overall, I would say that this paper is just above the acceptance bar, because the theory holds up well and could be of interest for finer analysis of the dynamics of SGD, and in particular of different trajectory starting from the same point (how quickly will they diverge?
2017. Summary: The paper proposes a new Graph Transformer (UniMP) based model with the motive of combining two powerful semi-supervised node classification techniques, GNN and LPA. Proposed Graph Transformer unifies feature and label propagation in conjunction to provide a better performance in semi-supervised node property classification task. Minor comments: Abstract: we adopt a Graph Transformer jointly [using] label embedding? Compared with other methods such as APPNP,  TPN and GCN-LPA, the major differences is another effective/unified network structure to merge feature and label information together. The UniMP first employs graph Transformer networks to jointly propagate both feature and label information. Although the proposed UniMP achieves state-of-the-art performance, the experiments are not convincing enough. When compared with GAT, the main differences are (1) different implementations of self-attention mechanisms and (2) whether to adopt gated residual connections. This paper shows empirical discussion compared with GCN-LPA, while it could be better to show some insight/theoretical discussion/analysis. Overall, I vote for accepting the paper. Section 4.4: model still be uncertain → model still remains uncertain The paper presents a novel unified model that jointly harnesses the power of graph convolutional networks and label propagation algorithms based on the unified message passing framework.
The authors in the paper describe a deep learning approach to detect copy number variants (CNVs) from DNA sequencing data, CNV-Net. ########################################################################## Summary: The authors proposed CNV-Net, a deep learning-based approach for copy number variation identification.
Based on this, they propose a light and scalable GNN learning framework called LCGNN, which first adopts the local clustering method PPR-Nibble to partition full graph into subgraphs, then use GNN modules on subgraphs for training and inference. ########################################################################## Summary: This paper proposes to utilize local clustering to efficiently search for small but compact subgraphs for Graph Neural Networks (GNN) training and inference. Minor comments: As the authors mentioned, the local clustering methodology is only reasonable for graphs that has low conductance with respect to all "cluster" (nodes with same label). Notably, they show that using K≥10 gives significant improvement on Cora and PubMed dataset for node classification problem compare to using K≤5. The idea is to form local graph for each node using PPR-Nibble, a local clustering method proposed before, and then use transformer on top of the local graph as encoder for node classification and link prediction. The authors claim that 'the locality nature of LCGNN allows it to scale to graphs with 100M nodes and 1B edges on a single GPU' but use 8 NVIDIA Tesla V100 in their experiments. These works show that using merely short random walk is insufficient to fully extract the topological information from graphs and thus the claim by the authors seems questionable to me. Minor issues: In the paper, it is claimed that "Compared to full-batch GNNs, sampling-based GNNs and graph partition-based GNNs, LCGNN performs comparably or even better...", while from the experiments, we can see that it can be worse for some tasks.
The authors propose a method to train deep generative models on quotient manifolds and show improved performance on some simple standard test sets. Weaknesses: For me, the main problems are the motivation of encoder compatibility, the clarity of the paper and limited evaluation: Motivation of encoder compatibility: I don't understand why the inverse of all generators needs to represented by one (!) single encoder? Post-Rebuttal: Unfortunately, the authors neither did update their paper nor addresses my comments.
The authors develop methods based on these two factorization techniques for discrete versions of PPO and SAC (called FPPO and FSAC) and evaluate them on Gym Platform, Google Football, and discretized MuJoCo tasks. ########################################################################## Summary: The paper studies policy optimization in multidimensional action spaces. Also, the paper does not set a clear agenda for what would be interesting to see in the results; Is scalability to large action spaces being investigated (in which case, comparison with the non-factored baselines of PPO and SAC should be included)? Other methods for learning the independent critic in FSAC are also possible but are not discussed in the paper (see e.g. Ref. Update: After reading the other reviews and the responses, I have changed my score to 5: marginally below acceptance, due to the framing and related work issues, as discussed by R2 and R4. The paper nicely outlines how PPO and SAC should be configured to work with independent and autoregressive policies under the atomic factorization of the action space. (iii) IF-SAC vs IF-PPO. Overall, the experiments show some of the possible potential of factored action spaces with PPO and SAC. Exploring and understanding factored action spaces is important, and the paper presents a reasonable step in this direction, and will therefore provide a foundation for further work. However, at this moment, I will keep the paper below acceptance.
This paper considers the problem of private sign recovery for sparse mean estimation and sparse linear regression in a distributed setting. Furthermore, the paper states that this is the first deterministic algorithm with a provable high-probability privacy guarantee. Using robust estimators (which is deterministic), which gives privacy guarantees with high probability, is exactly the start point of these algorithms. As discussed in the concerns, the modification of the differential privacy notion makes the problem very different and this modification is not well justified in the paper. The paper gives "almost private" algorithms for problem of sign recovery of mean vector and of linear regression.
Firstly, the analysis is extended to certain ResNet and Convolutional architectures, showing that in both of these cases we can relate the neural tangent kernel matrix to the neural path kernel matrix using a result analogous to Theorem 5.1 in (Lakshminarayanan and Singh, NeurIPS 2020). A recent paper (Lakshminarayanan and Singh, NeurIPS 2020) provided a new perspective on Neural Tangent Kernels for Gated Neural Networks, by decomposing the network into independent paths. The discussion mentions performance when we fixed the input gram matrix to be a constant in the definition of the neural path kernel (and hence define the neural path kernel in terms of the gating structure only), but does not include numerical results for this case. For this, I think the authors need to make concrete comparisons with methods that are deeply rooted in kernels such as GPs or BNNs. For instance, does using a particular composite kernel structure give you the same predictive performance as when using a GP? (2) Lakshminarayanan and Singh (2020) has developed a neural path framework in the NTK regime. Specifically, they show that the value of the neural tangent kernel matrix tends to a constant multiple of the neural path kernel matrix as the width of the network goes to infinity. Lakshminarayanan and Singh showed that under certain assumptions, a kernel defined in terms of the neural path feature is approximately equal to the neural tangent kernel (up to a constant).
In this work, authors provide an analysis of mutual information for MSDA and the develop a new variant of mixup. Table 1 shows that in terms of mutual information, MixUp < Baseline < CutMix (and < FMix with a very small gap). The paper presents an interesting analysis of CutMix and MixUp data augmentation techniques.
Results on more datasets with different spectral distributions would make the study more conclusive.
The paper shows that neural networks in such a class allow for more efficient approximation of certain functions than (a) was previously known and (b) is possible using a related class of networks that can be implemented using an entirely classical circuit. If I understood the work properly, the role of quantum computers is to evaluate on a quantum computer the "binary" part of a Binary Polynomial Neural Network (this is what the authors call the acceleration phase), after in a prologue part the data is loaded as initial quantum state with a log-depth circuit.Then, in the epilogue phase, a nonlinearity, like a a ReLU function is applied classically,
Pros: The paper utilizes the self-attention mechanism to improves the computational complexity of permutation decoding. Reasons for score: The main idea of using self-attention for decoding linear error correction codes is interesting. For me, it is not clear why the top k (k>1) performance matters to measure the quality of the decoding algorithm of error correction codes for communication. Despite the previously described issues, I believe the paper presents an interesting method to advance the current state of permutation decoding. The paper empirically demonstrates the utility of their proposed method on BCH codes. Besides, the paper is nicely and clearly written, with all the important details explained and all the required background information, which allows perfectly understanding the methodology. The paper performs an ablation study to showcase the importance of different components of their solutions. Minor comments: It seems that WBP is not defined (is it weighted BP?) The paper focuses on improving the computational complexity of permutation decoding. IN BPL, although it is applied to Polar codes, the authors finally make use of only 5 different permutations, which is not that ineffective, and enable them to report a great performance. Still, there are some important aspects, concerning the results, that may require, from my point of view, some further comparison/study (more details in the section ''Questions and additional evidence''): First, I do consider the comparison of GPS + (W)BP vs rand quite unfair. Concerns: My main concern is that there is an insufficient amount of reasoning to explain why we use the proposed method over the existing decoding schemes. From my point of view, the aforementioned problems hinder the comparison of results, therefore impeding a full assessment of the scheme used, the decisions taken, and its performance compared to other similar approaches. ================================================================= [Main Weaknesses] The paper's main weakness is that it seems the motivations for choosing four different BCH codes (in Section 5) are not justified clearly. Decision, and key reasons Accept, after discussing and further elaborating some of the previous concerns. Instead of plotting the top k (>1) performance, it would be better to include the comparison of performance over the best existing decoding schemes (in terms of top 1 performance given a reasonable complexity). Finally, I am curious about the Block error rate, as I can imagine that sometimes, when a permutation is quite wrongly chosen, it might push y to a different c, hence leading to s=0, but still resulting in a wrong decoding. Obviously, we could just pass all the possible permutations through the decoder, but this is clearly extremely inefficient and hinders the usage of these methods on real-world scenarios.
Paper proposes Hybrid Discriminative Generative training of Energy based models (HDGE) which combines supervised and generative modeling by using a contrastive approximation of the energy based loss Novelty Paper proposes a simple but unified view of contrastive training, generative and discriminative modeling - a nice, novel contribution with empirically strong results Significance Results are compelling across a wide range of tasks over existing (EBM) baselines including calibration, robustness, OOD detection, generative modeling and classification accuracy Error bars are missing for the classification accuracy experiments in Table 1 which makes it hard to verify improvements especially wrt supervised contrastive loss method It could be that HDGE is just succeeding in learning the support of the dataset.In order for this paper to reach the standard of *CONF*, I believe that the authors must revise the strength of their work and redesign their experiments as such.
Minor points (not related to the manuscript's rating) The proposed approach relies on pre-training the DNN model on a separate dataset, since the authors have made no attempt to account for the impact of training itself on the null distribution. However, I believe the current version of the manuscript falls short in certain areas, and should be improved prior to publication. To this end, I believe the authors should: (1) discuss in much greater clarity how the pre-training of the DNN models interacts with their proposed approach;
Summary of Contributions The paper explores adversarial perturbations in deep RL, providing a new thread model where the perturbation is computed based on a single state. The authors studied how perturbations on states would affect the performance of deep reinforcement learning. I'm willing to adjust my score should my concerns be addressed. — idea: A framework composed of 6 different adversaries is proposed using which it is claimed that the transferability properties among Atari environments can be studied. In the experiments section, authors have used a pre-trained DDQN agent which originally does not show significant generalizability compared to methods like A3C. I do think the paper falls a little short in that there wasn't a representative sample of deep RL methods, as well as not commenting on design choices made and how/whether they might interplay with the transferability measured. In this case, the only insights we might possibly learn is the input similarity between different Atari games, which is hardly a contribution to any field.
In particular, the paper proposes a robust loss function and an algorithm for learning from complimentary labels. "can be summary as" I am unaware of this definition of robustness of a loss function as it seems very specific to the complementary label learning problem. However, in this paper it means if the loss function with ordinary and complementary labels has the same minimizer. Many others! This paper studied a new problem, that is, learning from complementary labels. The paper presents (two) simple yet insightful sufficient conditions for a usual loss to work well as a complementary label loss.
This paper proposes an approach that adaptively decides when to update the simulation policy, based on the difference between it and the current learned policy. Performance : It seems to me that FIX_10^3 has always lower switching cost, and also learns a good policy. Questions : I think that maybe another good criteria is to measure the distance between the deployed and the online policy, rather than feature representation. At the end, I would like to say that, the paper is a good a step, but for publication the analysis/ experiments should be more thorough and possibly give insights about how to "formalize" the problem.
This paper studied a simplified image classification task with orthogonal non-overlapping patches and is learned by a 3-layer CNN. The authors observed pattern statics inductive bias (PSI) in experiments. In this manuscript the authors derive theoretical analysis for the generalization guarantees of a naïve CNN (3-layers) where the task is a simplified binary classification task, under the assumption that the images contain orthogonal patches (a naïve assumption). The authors also verified PSI in some task based on MNIST that has non-orthogonal patches. My major concern about this paper is that the theory seems to only work under orthogonal patterns and non-overlapping filters, unfortunately, neither of which is true in practice.
Summary: The paper proposes a new algorithm for multi-agent reinforcement learning (MARL) that adaptively picks learning rates for actor and critic. It proposes AdaMa, an algorithm which balances the learning rates of actors and the critic, and can also make use of the second-order information. Pros: The topic and idea of using adaptive learning rates to avoid hand-tuning are quite interesting and important. However, the proposed method is not convincing in terms of the lack of larger-scale experiments or theoretical results. I have several questions: How is the adaptive learning rate related to MARL?
The proposed approach follows the same recipe in PVI where local agents learn their own model posteriors from private data, and communicate their posterior representations to a server, which aggregate local posterior representations into a universal representation. On the practical aspect of this paper (i.e. communication load & trustworthy prediction), while the demonstration is sufficient against point-estimate method such as FedAvg and DSGLD, there is no comparison against other non-parametric probabilistic methods such as PVI (Bui, 2018) and/or PNFM (Yurochkin, 2019). --- post-rebuttal feedback --- The authors have addressed most of my concerns. Following the above summary, I will give my opinions regarding several aspects of this paper below. I can not recommend the acceptation of this work for the following reasons: The originality of method is low because it directly builds on top of two well-established approaches PVI and SVGD.
Of course opponent observations should not be used at execution time, and training a VAE to recover the missing information seems like a good approach. Overall, I find this to be a well-executed study of learning opponent models with a VAE. This paper considers an algorithm for learning and using an opponent model that is only conditioned on an agent's local information (history of actions, observations, and rewards). so long as that is addressed (or convincingly rebutted, if the authors disagree with the prior works I've cited), then I feel the paper is over the bar for acceptance. I'm still in favour of accepting the paper. However, the authors appear unfamiliar with other opponent modeling work (I'm particularly familiar with the computer poker domain, although they cite two Ganzfried and Sandholm papers from that area) where there is a rich literature of opponent modelling being successfully used under these conditions (opponent observations needed only at training time, and never at execution). +++ Section 4.2 "In Sections 1 and 2, it was noted that most agent modelling methods assume access to the opponent's observations and actions both during training and execution. - some minor clarity points on the environments, a larger (but easily fixable) point about consistently describing when opponent observations are and aren't needed, suggestions for experiments I would have loved to see to verify basic robustness, and some notes about missed related work that also occupies this setting (use opponent info for training, never at execution). These are just some of the works that I'm most familiar with; the computer poker community has been studying this topic for a while, and under the conditions described in this paper (opponent observations available during training, but never at execution time). In general, the authors should be more careful about specifying "at execution time" whenever they claim their technique does not need opponent observations, and should probably be aware of more related work in this setting. The ongoing challenge in that community (with success against human professionals!) is to go further and perform opponent modelling without needing opponent obervations at any time, by using only the agent's observations at training time as well as execution time.
Response to the author feedback: We appreciate the authors for the extra effort to demonstrate the abiltiy of FLAP by adding more experimental results and discussions. This is not a criticism, revisiting and modifying existing ideas and demonstrating that they are able to obtain state of the art performance is useful. The proposed method trains the policy using soft actor-critic on training tasks and trains an adapter network to predict task-specific final linear layer from a single timestep transition (s, a, r, s'). I found that using an adapter network trained to predict linear layer parameters during training and used to infer parameters during testing is a setting that was not explored. For example, it would appear that the adapter network should not work on sparse rewards tasks (where the transition function is unchanged between tasks) or other situations where most experience tuples do not provide information regarding the specific task. Recommendation I recommend rejecting this paper because it is not clear why "linear representation meta RL" is better in general. In Figure 1, the proposed method showed about -200 reward in Cheetah-Vel (hard) tasks.
This paper presents a theoretical scenario where point-wise measure of adversarial robustness falls short in comparing model robustness, then conduct experiments to show that robustness curve is a more meaningful evaluation metric from a global perspective. The authors argue that robustness curves allow to compare "global robustness properties and their dependence on a given classifier, distribution and distance function". Other Questions or Comments: Most of the existing defenses against adversarial examples are typically trained using a specifically-chosen perturbation strength. If adopting robustness curves (or global robustness) as the evaluation criteria instead of point-wise robustness, how will this affect the existing adversarial training procedure? I mainly agree with the authors on the argument that point-wise measurement of robustness may be insufficient in explaining model robustness. At the end of the introduction, the author say: "It is our belief that the continued use of single perturbation thresholds in the adversarial robustness literature is due to a lack of awareness of the shortcomings of these measures". Summary: The authors advocate for the use of Robustness curves, plotting the adversarial accuracy as a function of the size of the neighbourhood region of allowed perturbation. Gopfert et al. 2020). it seems this paper's contribution is highly based on  the proposal of robustness curve, and providing more explanations and discussions. One thing that I would recommend the authors is to make clearer the distinction between robustness curves as they described them (based on finding the closest adversarial example) vs.
The main theoretical result comprises the population case, where infinite amount of data is sampled from the described generative model under an additional anchor word assumption, and proves that in such case the outcome of the proposed contrastive learning procedure is linearly related to all moments of the topic posterior up to a cerain degree. Evaluation: Overall, the paper provides a thorough theoretical analysis to prove that the approach allows to recover topic posterior information. The main strength of this paper is suggesting a sound and straightforward learning algorithm to construct document representation. I like the idea of  using topic models as a way to represent the document level information, but it is disappointed to see that the proposed method doesn't provide as good performance as the simply averaging word embeddings. And the authors explain the relatedness of the representation function and topics in a document. Overall, I admit that the suggested algorithm is a sound method to construct the document representation. In the experiments section, the paper proposes a Direct-NCE method that has best empirical performance , which can be confusing for readers. The authors compare the suggested representation with models that do not explicitly make document representation (maybe except LDA). But I think we can run the same experiments in Arora et al., 2019 with the suggested algorithm representation.
This paper proposed to learn a regression model using "skewed data", which is defined as the subset of training samples with true target above certain threshold. This paper proposed a semi-supervised learning approach to improve the regression model trained on output-skewed data. (1)     The paper assumes that the training data are often highly skewed (intentionally) but the true distribution of the output can be easily estimated or obtained. It's not clear how p(y) was estimated from the labeled dataset, where only samples with true target above certain threshold are available. However, defining the "skewed data" as the subset of training samples whose target values are above certain threshold is an overly strong assumption. Updates: I thank the authors for their response. For example, is it possible to provide any theoretical background to support the claim that the proposed approach is not sensitive to error in the assumed true distribution? For example (Kim et al. 2020)? Issues: Section 3.2: "To be a useful feature for Rpost, latent vectors should be arranged in a similar way to p(y), which possesses information about how the labeled dataset is skewed." - p(y) is a distribution of labels right? Current decision: 5 - until I read other reviews and understand some of the moving parts better. I will keep my original score.
ii) Graph Neural Networks with Generated Parameters for Relation Extraction, In ACL'19 The authors propose a method for simultaneously learning the graph structure (or a graph generative model) and the parameters of a GNN for node classification. The paper proposes a way to use self-supervision with denoising autoencoders to improve learning of the graph structure for GNNs. The approach is compared with a number of recent approaches from the literature. [Quality] Regarding experiments, the dataset domains (such as citation networks) considered in the paper are those in which the graph structure is known. If that is true, I feel that the self-supervision may actually work like a regularizer to the MLP subsequent to KNN. There are a few prior works covering self-supervision with GNNs that are all quite new [1-3]. [Relevance] The topic of GNNs has gained increasing attention recently such that a significant portion of the *CONF* community should be interested.
GOAL demonstrates superior performance compared to SoTAs. Weakness: -First, the method is quite similar to NAO, where an encoder and decoder approach the maps neural architectures into a continuous space and builds a predictor based on the latent representation. The only difference is that NAO is use a decoder to decode the optimized latent representation back to architecture representation while here GOAL applies gradient descent on a graph neural network. My main concerns are as follows: In section 2.1 the authors claim that "In contrast, our method directly optimizes the discrete architectures, avoids the drawbacks of continuous relaxation".
The proposed approach is a two-stage method involving candidate selection (learning a function ρ to determine a Bernoulli probability for each input) and AutoRegressive subset selection (learning a function f to generate probabilities for sampling elements from a reduced set); both stages use the Concrete distribution to ensure differentiability. This paper proposes a stochastic subset selection method for reducing the storage / transmission cost of datasets. The paper includes a broad set of experiments, but I have some questions and concerns regarding the baselines and the proposed SSS method. As a further baseline for SSS, could the authors simply have implemented an algorithm that learns separate Bernoulli probabilities for each example (without the model ρ)?
Authors propose a novel method to approximate a given directed graph with a´nother one (the sparsifier) which has fewer edges. Comments Section 4: I am not sure what is the downstream justification for the notion of spectral approximation studied here (relative condition number) in the context of directed graphs, and it would help if the authors could elaborate on this. The authors also evaluate the embedding given by the proposed symmetrized Laplacian in the context of clustering directed graphs, where the majority of existing spectral methods tend to underperform. In general I have doubts about fit to the venue; while *CONF* scope is broad and inclusive and spectral sparsification has certain potential connections to ML, this paper does not highlight any of them, and it is not entirely clear what it is attempting to achieve.
This submission proposes a training strategy that leverages background/noise data to learn robust representations. To improve multi-class classification problem using DNNs, authors propose to do multi-task learning of solving another auxiliary tasks which tells if data points are just noise/distractors or not. Part of the proposed method is building an extra class of training data that are background or noise. This paper proposes a training method for classification, with the goal of training with less data. A critical point, that is missing from this paper, is how would this data augmentation work by itself without the auxiliary classifier. In other words, neither training nor inference of the proposed method utilizes the content of Section 3. You abstract states that there has been not much work done on using background/noise data whilst estimating model parameters.
[+] Propose a method of applying TRUST-TECH to find local optimal solutions (LOS) of DNNs[+] Introduce DSP for exploration in high-dimensional parameter space[+] High ensemble performance through DSP-TT The proposed method is somewhat novel in the aspect of suggesting TRUST-TECH for an ensemble of DNNs. However, it is difficult to give a high score because the experimental results do not support The authors propose a method to obtain multiple local optimal solutions around the existing one. [-] Claims not sufficiently proven or supportedThe paper repeatedly claims that the solution found by the proposed method is high quality and diversity. However, I am still hesitant to give this paper a higher rating, in part because I find Section 5.4 to be poorly written and somewhat confusing (it lacks any sort of conclusion or insight and I cannot read Figure 4 at all, the text is too small) and partly because the paper does not clearly put its results in the context of the recent model ensemble progress (and thus makes me doubt the impact of this result on the field;
To address this task, the paper proposes a method consisting of three steps: (1) detecting out-of-class samples in the unlabeled set, (2) assigning soft-labels to the detected out-of-class samples using class-conditional likelihoods from labeled data, and (3) using auxiliary batch normalization layers  to help mitigate the class distribution mismatch problem. ########################################################################## Summary: The paper proposes a new approach for open set semi-supervised learning, where there are unlabeled data from classes not in the labeled data. It also explores the idea of auxiliary batch normalization (from Xie et al. 2020) in the open-set SSL setting but the results of the ablation study suggest the level of improvement achieved by this normalization is negligible and the most of the improvement comes from more accurate detection of out-of-class samples through using the projection header function introduced in SimCLR paper. This paper considers the problem of semi-supervised learning, where the unlabeled data may include out-of-class samples. Detailed Comments: The paper uses contrastive learning idea proposed in SimCLR (Chen et al. 2020) to detect out-of-class samples and treat them in a different way than in-class unlabeled samples during semi-supervised learning. ########################################################################## Summary: The paper proposes a new approach for open set semi-supervised learning, where there are unlabeled data from classes not in the labeled data. I like the idea of learning representation in an unsupervised way for both labeled and unlabeled data. Clarify: The preliminaries section clearly describes the setting of semi-supervised learning concerned in this paper and also clearly describes the contrastive representation learning. While the setting considered in [a] (for metric learning problems) is a bit different from that concerned in this submission (for image classification tasks), the high-level idea (learning representations that can be used to describe unlabeled images with labels different from those in the training set) is very similar. Class conditional likelihoods has been shown to be not very useful in detecting out-of-distribution samples in cross-entropy loss learning.
It seems to me that the analysis for general function class is different to those for kernel function and NTK, so I'm wondering that are there any upper/lower bounds for the Eluder dimension of the two non-parametric classes described in this paper?
Theory is provided for the case of synchronous symmetric averaging methods, and the paper is complemented with detailed experiments on CIFAR and tiny-ImageNet. This is a nice contribution to the growing literature on decentralized training for deep neural networks. The authors identify the consensus distance as the key factor that affects the generalization performance of decentralized training. It focuses on the so-called "critical consensus distance" and how disagreement during different stages of training ultimately effects optimization (training loss) and learning (generalization error). The main focus is to better understand the role of consensus, or lack there of, into the generalization abilities of decentralized training. Yin, Pananjady, Lam, Papailiopoulos, Ramchandran, and Bartlett, "Gradient diversity: A key ingredient for scalable distributed learning," AISTATS 2018 and arxiv: 1706.05699 This work investigated a very interesting topic about generalization in decentralized deep learning. (Th1 is based on previous work.) The other results, e.g., remark 2, proposition 3, and lemma 4 cannot claim how the consensus distance affects the generalization error. In general, the paper is well written and there are several interesting observations and discoveries involved regarding the generalization performance of decentralized learning. As also pointed out by reviewers 1 and 3, the gap between the convergence rate/consensus distance and the generalization capability still exists, causing the mismatch between the theory and the simulations. I expect the results to be useful to those working on decentralized training and am supportive of accepting it. Now that we need to potentially perform multiple rounds of gossip between each optimizer update, are decentralized methods still attractive for reducing overall training time?
Summary: This paper proposes a self-supervised learning framework for 3D object classification and retrieval based on multi-view representation, where a sub-task of transformation estimation is adopted as a regularizer. This paper proposed a self-supervised learning method of 3D shape descriptors for 3D recognition through multi-view 2D image representation learning. The Unsupervised Learning of Transformation Equivariant 2D Representations by Autoencoding Variational Transformations is used for 3D shape descriptor learning, which the authors claimed as "self-supervised" learning. -Secondly, since it has been suggested by many previous work that the joint-training of multi-task is helpful for the network, it will be appreciated that the authors provide more analysis and discussion on how the MV-TER loss helps the network learn transformation equivariant representation than simply using rotation as data-augmentation or using pose-estimation as sub-task. Areas for improvement: -Firstly, the representation of the transformation that the author used in the paper is not well specified. (AVT) Another concern was critical but not yet addressed neither: The authors could propose a method that can be developed based on the "3D Transformation Equivariant" to 3D objects directly instead of its 2D projections. As the rebuttal points out there are differences between the proposed method and AET but the core idea is very similar. In my opinion there is enough difference to still recommend acceptance. For instance, the authors could propose a method that can be developed based on the "3D Transformation Equivariant" to 3D objects directly instead of its 2D projections.
*CONF* 2020 [2] Lai et al. Contextual Grounding of Natural Language Entities in Images. The paper presents nice studies throughout section 6, looking into learning in a more challenging evaluation set up such as generalization and supplementary material is also thorough. In several places in the paper it is mentioned that the model learns "mapping between entity IDs in observation space and their symbols in text entirely through interaction with the environment." I am not sure what interaction means here. The general idea is to learn a parameterized policy model that inputs the pair (entity [this paper] or visual representation [1] and text) and outputs the action of the agent in the game (correct me if I am wrong). The paper presents a model for entity grounding from its textual description for a text-based language game. NeurIPS 2019 workshop Summary This is a significantly improved extension of prior work in this area.
Summary: The authors propose a training-free way of estimating the performance of a deep net architecture after training using correlations between linearizations of the network at initialization for different augmentations of the same image. This paper provides a reasonable start for a new potential direction in NAS research and so may be worth presenting at the conference, but the justification and applicability of the method is somewhat limited. What would be more useful is if around 200 networks were sampled from 80 to 100 accuracy bucket, for each of the 20 images (instead of 10), we compute the Kendall Tau of the ranking based on S and validation accuracy. This might be an unfair comparison, but will be important for people to know whether the proposed method/metric can generalize to different datasets. For example, uniform initialization [0, 1] seems not work at all but no further explanations, which may indicate the proposed method may work in a different way (e.g. taking advantage of some search space's bias).
In this paper, the authors propose a variant of FedAvg that not only produce the global training, but also a mixture of the local model and the global model, which is called personalized model. In overall, I think the paper is technically sound, making reasonable contributions. ==================================================== This paper considers the problem of personalization in federated learning. The authors defined the average distribution D¯=1/n∑i=1nDi. Why the global model can be obtained by minimizing the joint empirical distribution D¯ in the federated learning? One of the reasons that a centrally trained model on EMNIST performs worse than personalized models is the shift in the distribution of characters and digits across clients. First of all, the manuscript proposed an adaptive personalized federated learning algorithm, where each client will train their local models while contributing to the global model.
Summary: This paper presents a method to train a neural network to predict the time-dependent costs, and start and goal states needed to run time-dependent shortest-path planning in a dynamic 2-D environment. Overall, the paper could use more detail on the architecture, the hyperparameters, the baselines, the domains, and anything else needed to get this to work. Given the number of changes required, I encourage the authors to resubmit elsewhere with the updated paper, ideally with additional experimental comparisons as discussed. [6] Scalable Planning with Deep Neural Network Learned Transition Models, Wu et al. JAIR. Path Planning using Neural A* Search (2020). [2] Learning Neural-Symbolic Descriptive Planning Models via Cube-Space Priors: The Voyage Home (to STRIPS), Asai and Muise IJCAI-20. [3] Learning latent dynamics for planning from pixels, Hafner et al. ICML-19. [4] A benchmark and evaluation for multi-task and meta reinforcement learning, You et al. CoRL 2019.
The proposed "Causal Attribution of A Super-edge" is very useful for large-scale graphs, which can reduce the computational cost. Does table 1 contain the results of the standard causal screening or the cluster-based one? However, the mere improvement over the baselines is not surprising since the proposed explanation method directly optimizes an objective to mimic the full graph's function. The proposed method is very straightforward, which is a simple application of the individual causal effect. The GNN-explainer can obtain much better sanity check scores with the model randomization test, which means the proposed method is less dependent (less faithful) to the model compared with GNN-explainer. Furthermore, to better show the efficacy of the proposed method, in the experiments, the authors should vary the graph size and graph density. Overall The paper proposes a different approach compared to the currently-existing explanations for graph networks. If so, what are the results of the cluster-based method for w.r.t. contrastivity and sanity check? All in all, if the paper is improved on clarity during the revision period, I would lean towards acceptance. Cons: However, I have the following two concerns.
This paper proposes an approach to reducing the sample complexity in multi-task reinforcement learning using permutation invariant policies. The authors identify a key property in the targeted resource allocation problems -- the permutation invariance -- which intrinsically implies the independency of samples at different time steps. In particular, they present an algorithm that exploits permutation invariance, study its theoretical properties, and propose examples where this property holds and their algorithm can be leveraged. is an extension to your PI setting. The main assumption the paper makes is permutation invariance (PI). I didi not understand how a network trained using gradient descent alone would satisfy permutation invariance. This would mean that the rearrangement of state or actions do not change the output of the policy network. It seems the paper presents an interesting attempt at resource allocation using RL but I found the writing highly confusing. Overall, I think that the general direction is good, and significant progress has been made, however, the current state of the paper does not present a convincing story. Some comments: The paper gives a good discussion of existing works and where the paper lies in the line; it would be better if the authors can briefly discuss meta-RL which is close to the problem being studied in this paper How is permutation invariance property used? I feel the paper could have been better presented by starting with a motivating example where the permutation invariance property holds - for example the portfolio optimization example studied in the experiments.
Summary This work proposes an approach to update feedback weights in DFA using modification of kolen-pollack method, which helps in training deep CNN network. I enjoyed reading it. This paper introduces a new method for computing the backward updates of a neural network called Direct Kolen-Pollack learning (DKP). First Review Citation missing for key work on assessing the scalability of bio-inspired approaches and highlighting key limitations [Bartunov 18], variants of DFA [ Moskovitz 18, Frenkel 19]  and recently an approach similar to DFA with target projection known as LRA (also has similarity with Direct Kolen-Pollack) showing promising performance on deep CNNs [ Ororbia & Mali 2020]. For LRA Delta_b(update for error weights)  = learning rate *( teaching signal(error_k) * post-activation from layer below (a_(l-1))
The authors use group theoretical constructs such as shift and rotation operators to show that a latent space representation should be equivariant such transformations. An alternative definition of disentanglement is given, where instead of confining the effect of each transformation to a subspace, an operator is used that acts on the whole latent space (this operator is chosen as a shift operator, which works for cyclic groups). Second, I believe that the idea that is better stated in the introduction  on how disentanglement can be framed is valuable: " In this framework, the factors of variation are different subgroups  acting on the dataset, and the goal is to learn representations where separated (of the data) subspaces are equivariant  to distinct subgroups." Even if one can question whether Def 1 is a good formalization of disentangling, the paper does show empirically that it is easier to learn an equivariant encoder/decoder when the latent operator is a shift operator or a diagonalized complex version of it, rather than a disentangled operator (with one 2x2 rotation matrix block and an identity block; Instead the authors put forth the idea of transformations of data that are equivariant to the latent space representation as a formulation of disentangled factors.
The authors present conditions that need to be satisfied by the encoder and decoder parameters, and show empirically that the regularization terms that they propose ensure that the resulting autoencoder has an isometric decoder. The authors propose a new version of the regularized autoencoder where they explicitly regularizes its decoder to be locally isometric and its encoder to be the decoder's pseudo inverse. The paper suggests a novel auto-encoder based method for manifold learning, by encouraging the decoder to be an isometry and the encoder to locally be a pseudo-inverse of the decoder. Regarding the experiments, indeed the authors successfully show the IAE converges its decoder to be an isometry and the proposed regularizer promotes more favoured manifold. p. 2 Manifold learning generalizeS p. 4 Is there any possibility that the author can provide one more toy example for the global isometry when the data lie on some manifold shape? In case there is a setting where isometric AEs can be shown to model the data manifold better than regular AEs, that is not highlighted in the current draft. The numerical results on reconstruction error that the authors present in the appendix do not indicate any reason to prefer isometric AEs over other baselines that are considered. The authors claim that isometric autoencoders would "evenly sample the manifold" which is a little confusing, since the sampling of the data manifold is separate from the technique used to model the data (regular AEs vs isometric AEs). How can we practically make use of the isometry property in applications other than data visualization? Can the author provide some comments on this about why we need the "even" in the visualization?
In the paper, the authors proposed a novel privacy-preserving defense approach BAFFLE for federated learning which could simultaneously impede backdoor and inference attacks. To impede backdoor attacks, the Model Filtering layer (i.e., by dynamic clustering) and Poison Elimination layer (i.e., by noising and clipping) were presented respectively for the malicious updates and the weak manipulations of the model. Weaknesses: This paper does not provide any theoretical guarantee and only applies the existing methods to mitigate the backdoor attacks. To impede backdoor attacks, many models are marked as outliers and discarded, clipped, and noised, generally speaking, which could lead to performance degradation. After reading the response, I still think that the work is promising and would like to keep my recommendation. Using this clustering algorithm combined with adaptive clipping and noising, the proposed method can mitigate the backdoor attack. The author(s) have created many splendid terms to describe the modules used in this work, however, their implementation uses both clustering and median, which is very engineering and may not reliable with a different clustering algorithm or data set is severely unbalanced (just like the non-iid data sets among clients). Therefore, we cannot make sure that the proposed method works in any problem or on any dataset. In FL, clients locally train model updates using private data and provide these to a central aggregator. An Embarrassingly Simple Approach for Trojan Attack in Deep Neural Networks. The experiments could use a few popular trojan attack methods, the baselines are not comprehensive. minor: please attach your main context pdf in the submission and submit the appendix in the supplementary material The paper proposes a backdoor-resilient federated learning method to defend the backdoor attack of poisoning the models. This paper provides an interesting research direction for the cross-domain of federating learning and backdoor attacks.
General statements This paper introduces an interesting parallel between SDEs and GANs, and pushes the analogy to its practical implications as a way to learn neural SDEs. Globally, I found the paper a very good read, although it sometimes lack the details that could be useful for a non-specialist. I feel the Section 3 is more related to computational issue rather than the GAN formulation of neural SDE and can be individual interest? EDIT: after seeing all reviews, and most importantly thinking about it and pondering the answers given by the authors, I am sorry that I must lower my score.
Short summary: The paper introduces a promising new method, hierarchical nonnegative CP decomposition (HNCPD), as well as a training method for the HNCPD, neural NCPD, for topic modeling problems. SUMMARY: This paper presents a hierarchical nonnegative CP tensor decomposition method. Summary: In this paper, an extension of nonnegative CP decomposition called hierarchical nonnegative CP decomposition (HNCPD) is proposed. Potential Improvements: Equation (4) introduces the forward propagation for a NNMF, which is crucial for the HNCPD and Neural NCPD. Finally, results of experiments on three data sets (synthetic data, image data and text data) are utilized to evaluate the suggested methods. It seems like the authors combine existing ideas (hierarchical and neural NMF, NCPD) into a new method. In particular, in the discussion on approximation in Section 2.1, it is not clear if this idea is used in the implementation, which might make it difficult to replicate the results. In the experiments, do you use this approximation, or do you use the NCPD combined with HNMF for each factor matrix as discussed in the beginning of Section 2.1? In Section 2.1, I think the second paragraph is clear until Equation (7), but the rest of the paragraph is difficult to follow. For example, it's not clear to me how the columns of the hierarchical NMF factors are used to form NCPDs of ranks r(0), r(1), ..., r(L−2). The last sentence in Section 3.3 help explain the benefit of a hierarchical method. It is not clear how the Standard NCPD in Section 3 is computed.
The paper proves a universal approximation theorem for equivariant maps by group convolutional networks in an extremely general setting. Summary: The paper studies the approximation power of equivariant neural networks in a very general setup: the input space is assumed to be a function space (compared to a finite-dimensional space in standard setups) and the group is assumed to be any locally compact group. Having spent about a day with this paper, I think that at least the general idea of the proof is sound, and the authors have sidestepped several technical problems I could think of using well chosen technical assumptions. The main issues (that are detailed below) are: the paper does not sufficiently relate the discussed model or the universality results to previous or concrete models (set and graph NN, equivariant group NN, other unused but potentially useful variations), it does not provide sufficient explanation and justification to the different conditions in Theorem 11, there are some details in the proof and the description of Theorem 11 which are missing/unclear, Theorem 16 has some unclarity. As a result, the paper has improved and I increased my score. Since FNNs are known to be universal this implies universality of CNNs. I think the general CNN formulation and the Conversion theorem are of merit but i think the paper should undergo a rather serious revision before ready for publication. Universal approximation theorems are considered to be an important kind of result, and this paper proves a very general one for equivariant maps. If I understand correctly, the idea of the proof is basically using the extension mechanism of Theorem 3 to extend mapping to functions over base domains to equivariant mappings. Specifically, these works show that in some cases high order tensors are needed for universal approximation by invariant neural networks - can this result be recovered from your results? Is that correct? Can you provide the proof for a simple example of equivariant networks such as Deepsets? In summary, how the results in this paper relates to known and unknown results of CNN generalizations and what universality proof does it generalize? Related to that, I couldn't exactly understand the claim of the second to last layers in the proof of Theorem 11: the first layer outputs a function in C(G/HT×B), but the second layer of the FNN maps functions from some different domain C(B2). If this is true then how the FNN found from the universality result of FNN (e.g., Theorem 12 and 14) are guaranteed to satisfy this condition?
The stable weight decay property can be defined in dimension 1 as follow: the effective learning rate represents an amount of time ellapsed between two iteration. The authors propose to adjust for this by having the effective learning rate multiplying the weight decay term, ie Delta theta= -eta_{eff} lambda theta-eta_{eff} F(gradient). Statement 4, in my opinion, is one of key results in the draft. Overall I think the idea introduced by the author is interesting, although the theory is not completely coherant. The main conceptual point of the paper is simply that the weight decay should have the same effective learning rate as the gradients, it would be nice if the authors could make this more clear and more central. The paper main point then is to equate the learning rate in the weight decay coefficient by the effective learning rate and they show that this might be enough to bridge the gap between Adam and SGD. Review The reformulation introduced in equation (3) is an interesting alterntive view to look at weight decay, but I find it is not really used by the authors. Additionally, I am a little confused about the statement that the effect of weight decay can be interpreted as flattening the loss landscape of θ by a factor of (1−ηλ) per iteration and increase the learning rate by a factor of (1−ηλ)−2 per iteration. A minor point, in Eq 3, how did the authors arrive at −2t−1 in the superscript of weight decay rate, not −2t+1? They also make comments about reinterpreting weight decay as flattening the loss and increasing the learning rate which they don't pursue further nor connect with the main point and it seems a little out of context. In particular, the authors should strive to provide experiments on different training sets (ImageNet) with learning rate cross validation. Comments and questions: I think the concept of weight decay rate and total weight decay should be explained better, and earlier in the paper. Stable doesn't just mean constant, though I can completely understand what the authors really meant in the paper. In the draft, the authors can directly say constant or time-varying weight decay. The authors trick is to average the value of the moving average of the squared gradients along all dimensions before using it to rescale the weight decay. The concept of "weight decay rate" and "total weight decay" seems to be the most critical part of the paper, but the explanation surrounding them was messy and hard to understand. For example, Statement 1 that says "Equation-1-based weight decay is unstable weight decay in the presence of learning rate scheduler." can be quickly summarized even from an intuitive sense without any derivation, which makes it trivial. Verifying the stable weight decay property is actually not optimal, because it is not isotropic. The authors do not systematically compare across learning rates which make it hard to interpret the results as being conclusive. I increased my score to a 6 because I think the paper in its current form is enough to get accepted, but there are still improvements that could be done to make it much stronger.
This paper presents an estimator that predict higher-order structure in time-varying graphs. Weaknesses: Section 4, which is a critical part of the paper as it presents the theoretical guarantees with respect to the proposed estimator, is rather complicated and lacks explanations of theorems, definitions and assumptions. For weaknesses, first, the paper ought to make further clarification on the essential differences (and maybe even better, the advantages) between the defined GSC with the traditional high-order structures, such as simplex, hyperedges, or just small graphs. Perhaps the experiment should show its advantage when dealing with much higher-order structure predictions rather than these simple cases.
Originality: High. The proposed tensor network (TN) based text classification model looks new and interesting. The authors claim that when combined with BERT for word embedding, the proposed model can outperform the SOTA methods.
The authors investigate different tokenization methods for the translation between French and Fon (an African low-resource language). That being said, I strongly agree with the authors that neural machine translation of African low-resourced language is important. I agree with all of your points about what is lacking, but in my mind, the novelty was enough to still give a 7. I think this paper can reasonably be rejected, but I'd like to give actionable of constructive criticism, since I do think the work on this low resource language is important for the NLP community.
The proposed method improves ASR performance over model using the baseline GMM attention which does not take source-content into account,  generalizes better to input sequences much longer than those seen during training, while also obtaining competitive performance to other streaming seq2seq ASR models on "matched" test sets. This paper proposes a novel Gaussian mixture-based attention mechanism by incorporating the source (key) information, enabling a flexible representation of the Gaussian attention pattern with the well-described formulation. For example, in the experiments, CTC is used as an additional loss to guide the learning of monotonic attention. Please clarify. I would even group the Transformer, GMM, and SAGMM-tr rows together, since they provide the control experiments to support the usefulness of the proposed method. Experiments are somewhat incomplete/missing important comparisons, e.g. comparing baseline GMM attention to the proposed "source-aware" variant in Tables 1,2,5, and comparing to other location-based attention mechanisms, even if non-monotinic, e.g. from http://papers.nips.cc/paper/5847-attention-based-models-for-speech-recognition.pdf Detailed comments: In the introduction, "The GMM attention is a pure location-aware algorithm in which the model selects attention windows cumulatively without considering source contents.": This is a little bit hard to understand.
Center-wise Local Image Mixture For Contrastive Representation Learning Positives: interesting proposed method for expanding the neighborhood space of considered positive matches for contrastive learning ablation study provided to show the improvement from each proposed component (sample selection, cutmix, multi-resolution) While the improvements that the CLIM augmentation brings seem to be quite good in the linear evaluation on ImageNet by boosting the results of MoCo v2 (which is used as a baseline if I understood correctly) by 4 points, the improvement of this representation on other downstream tasks (ie. Pros The paper proposes a simple, yet effective, data augmentation method that seems to help learning stronger representations for some tasks. hyper-parameter selection: there are several different parameters to be set in the proposed work: multi-resolution scales, number of clusters and neighbors for CLIM, alpha in cut-mix, with the differences in accuracy between settings approaching the difference between say knn+cutmix and center-wise+cutmix. This is an overstatement. It sounds, like there's a serious problem with previous contrastive representation learning approaches and the proposed methods solves them. Neutral: overall novelty is moderate; I would consider the main novelty to be in the selection of positive matches, as the cutmix and multi-resolution augmentations are largely leveraging existing ideas. After rebuttal After reading the authors comments' and other reviews, I think that this is a borderline paper that could benefit from more rigorous experimental validation. p.4 "Cluster-based method regards all samples that belong to the same center as positive pairs, which breaks the local similarity among samples especially when the anchor is around the boundary." It is not very clear how the proposed method differs from the clustering-based methods in this sense. It seems like the proposed sampling method can also break local similarity around the cluster boundary because the anchor will be attracted to the cluster center increasing the distance to the samples from other clusters, which will result in very pronounced hubs in the representation space (as in Fig. 3). Overall summary: Given the overall improvements from the proposed method, I'd be inclined toward accept, if the concerns I raised regarding the empirical evaluation were addressed. The paper presents an improved positive sample selection and data augmentation method for unsupervised, contrastive representation learning.
This paper proposes a method to alleviate the over-smoothing problem of GNNs. The key idea is to generate a latent graph structure via leveraging stochastic block model to approximate the observed graph structure and label information. The main idea is to optimize the graph topology by removing the inter-class edges as well as adding the intra-class edges, and then the noise information would not pass between nodes with different categories. It clearly demonstrates that the proposed method reduces over-smoothing relative to a vanilla GCN as expected, especially as the depth of the model increases, but I wonder what a similar comparison to the other models in Table 1 would show. From table 2, the accuracy improvement is slightly better than table 1 in the scarce-label setting. But table 3 tells if adding a small amount of labels, the difference between GCN and the proposed method becomes small again, which draws a similar question as toward the results in table 1. However, when one performs semi-supervised learning, edges going across communities over smooth the labels, and especially in the absence of many labeled points this causes big issues in node classification. The method is designed to only handle semi-supervised node classification problem, which may not be flexible enough to be associated with various GNNs for different downstream tasks. From experimental results table 1, the performance gain of the proposed method is around 1-2% accuracy compared to vanilla GCN.
Specifically, the paper proposes replacing the encoder with a truncated butterfly network followed by a dense linear layer. The paper's aim is to establish that linear layers can be replaced by butterfly networks and uses three different experiments to show this. In that regard, the paper is very appealing, as it shows that replacing linear layers with the butterfly networks does not result in any loss in performance. I have the following comments and questions that should be clarified to evaluate the relevance of the results: When comparing with other architectures on image classification tasks (CIFAR10 and CIFAR 100), what is the dense layer that is replaced by the butterfly structure?
Specifically, this paper design three synthetic tasks to teach the transformer model to first learn three primitive reasoning steps: deduction, abduction, and induction respectively. The structure of the approach is to first pretrain the model on synthetic tasks that are designed around three principles of mathematical reasoning: deduction, induction, and abduction. To sum up, the synthetic tasks proposed by the authors might indeed help in learning an inductive bias capable of improving theorem provers, but there is a discrepancy between the logical notions of deduction, abduction, and induction defined by Pierce (and more generally in the Knowledge Representation literature), and the reasoning primitives (essentially some forms of pattern matching) presented in the synthetic tasks.
** END OF UPDATE*** The paper unifies two regularisation-based continual learning methods from the literature (Synaptic Intelligence and Memory Aware Synapses) by arguing that they both approximate the "Absolute Fisher", a variant of the Fisher Information that averages absolute instead of squared gradients, to determine the regularisation weights. This works aims at unifying 3 popular regularisation type continual learning methods, namely EWC, SI and MAS, by showing that under some assumptions they all relate to the Fisher Information Matrix. Pros of paper (mostly already written in the "Review summary") Pros Connecting the path integral of SI to the Absolute Fisher is interesting. The paper is written well, with good detail and very good experiments. However, I have an issue with the proposed quicker/cheaper update for calculating the (diagonal empirical) Fisher Information Matrix for Online EWC ("Batch-EF"), as I detail later. While the paper is well-written and -structured, and SI and MAS are popular methods in the literature, I'm not convinced by the link through the "Absolute Fisher". In more recent versions, Pytorch also provides autograd functions for computing jacobians natively -- I'm not sure how efficiently they are implemented, but in any case the empirical comparison here needs to use an efficient, batched implementation for calculating the Fisher in order for it to be meaningful. Cons of paper I am not convinced that the minibatching that the authors suggest (both for SI as an approximation to the AF and for EWC in the last paragraph of Section 5) is correct ("Batch-EF"). The authors show that the importance weights of MAS and SI are similar to the diagonal absolute Fisher. I think the authors should justify that the Absolute Fisher is an optimal regularization under certain assumptions. Although I am not aware of previous works using the absolute value of gradients (as in AF), this paper provides evidence that such an approximation might be worth considering. However, since the connection mainly hinges on the empirical correlation between the two, I still don't think that the paper quite establishes the theoretical connection between the three continual learning methods that it aims for. The authors merely show that MAS and SI are similar to using Absolute Fisher as importance weights. It appears to me that by minibatching instead of squaring each gradient element, one should obtain much worse approximations to the empirical Fisher information matrix ("EF"). So simply taking absolute instead of squared gradients and stating that this is "a natural variant" is not a sufficient basis for a paper, especially considering that the term has not appeared anywhere in the literature before as far as I could tell. Therefore, I cannot agree with the claim that this paper presents a unified framework, just because it fits several methods into distinct concepts that have "Fisher" common in their names. Similarly, I think other methods, such as MAS and SI, can also be optimal under different assumptions. Finally, I'm not quite convinced by the potential impact of the insights, which seem fairly specific to choosing the batch size of SI, so overall I think the paper still is below the acceptance threshold. Review summary I really like the majority of this paper. I however found several limitations while reading your paper: Limitations regarding MAS
The result is improved on multiple unsupervised machine translation, and this paper claims that more diversity is brought to the synthetic data, so a better translation model can be trained. In Appendix Summary: The paper proposes an additional stage of training for unsupervised NMT models utilizing synthetic data generated from multiple independently trained models. The authors show substantial (1-2) BLEU improvements with 3 different UMT systems in 5 low-data scenarios (En-Fr, Fr-En, En-De, En-Ro, Ro-En), all subsampled to 5M monolingual sentences for each language. Experimental results in several translation tasks show that the proposed approach improves the translation accuracy of the standard unsupervised machine translation models, outperforming the cross-lingual masked language model. The authors add this additional stage of training to unsupervised NMT models using different pipelines (PB unsupervised MT, Neural Unsupervised MT, XLM) and show that their approach improves all of these approaches by 1.5-2 Bleu on WMT En-Fr, De-En and En-Ro. Strengths: The paper is well written, the approach is simple and seems to improve quality by significant amounts in a variety of experimental settings. Unfair comparison with the enable distillation: authors need to compare CBD with the model trained on the synthetic data (y_s, x_t) of the ensemble of agent_1 and agent_ 2. In general, the CBD method in this paper is a simple and effective data enhancement method to improve the performance of the model. They can directly add the synthetic data decoded by cross model to continually train the original XLM model with a supervised translation objective (which is naturally supported in XLM from my experience), and report the effect comparison between them. Weaknesses / Questions for authors: As with any NMT model trained with synthetic data, it would be better to report results on source and target original splits of the test data to provide a clearer evaluation [2,3]. Source of promotion: the second stage of CBD method adopts (x_s, y_t), (z_s, y_t), (y_s, x_t), (y_s, z_t) synthetic translation pairs, it is not clear how much performance growth comes from increased data and how much growth comes from the new model implementation (ott et al., 2018). Did the authors try any experiments with unsupervised models utilizing parallel data in unrelated languages, similar to [4,5] or in real low resource settings [6]? Recommendation: Overall, this is a good paper and I would recommend acceptance.
About the second contribution, the paper claimed that SGD with BN behaves like a variant of Adam, AdamG*. The stated contribution that "in the presence of BN layers, standard SGD behaves like Adam without momentum" is also slightly over-selling the results of section 3. So I think is the contributions in this work are slightly below the acceptance bar of *CONF* before these concerns are solved appropriately.
This paper begins with the empirical observation that adversarially trained models often exhibit a large different in clean (and robust) accuracies across different classes. The paper then proceeds to use a theoretical example (adapted from the model in Tsipras et al 2018) where adversarial training increases the accuracy difference across classes. EDIT: Score changed from 6 to 5 during discussion, see comments below. Should the increase in error be multiplicative (which is the most likely scenario) then it would potentially explain the main observation of the paper without taking into account adversarially training at all. I give the paper a 6 overall, though could adjust my rating in either direction depending on the author feedback.
This paper proposed a powerful online sequential test which can efficiently detect qualitative treatment effects (QTE). === Contributions === This paper proposes a new framework for A/B testing in the frame of randomized online experiments. The authors propose a scalable online algorithm for Type 1 error control. The test algorithm involves adaptive randomization, sequential monitoring and online updating. Finally, the method is accompanied with experiments on the finite sample performance of the test procedure with simulated and real data from Yahoo!. === Weak points === Minor: Authors claim that Figure 3 reports experiment results regarding QTE.
This paper proposes PABI (PAC-Bayesian Informativeness?), a way of measuring and predicting the usefulness of "incidental supervision signal" for a downstream classification task. Summary This paper proposes a unified measure for the informativeness of incidental signals (ie, not standard ground truth supervised labels) derived from the PAC-Bayesian theoretical framework. ########################################################################## Summary: This paper proposes a unified PAC-Bayesian-based informativeness measure (PABI) to quantify the value of incidental signals. Mathematical developments of PABI are given for these cases, and experiments show that PABI is nicely positively correlated with the relative improvement that comes with various methods for integrating incidental supervision signal (including one which is developed as a side note by the authors). In particular, when labeled data is only available in noisy or partial form, or over a different domain than the target test domain, this data may still be used to improve a classifier, but it's unclear how to tell which forms of incidental supervision will be most useful. ########################################################################## Summary: This paper proposes a unified PAC-Bayesian-based informativeness measure (PABI) to quantify the value of incidental signals. PABI can measure various types of incidental signals such as partial labels, noisy labels, constraints, auxiliary signals, cross-domain signals, and their combinations. It also is not clear to me from the paper's text whether something close to the PABI framework can apply in broader settings like language modeling style pretraining, where the input-output format of the incidental supervision signal is different than that of the target task. Weaknesses While the generality of the proposed PABI framework is great and improves over existing work, I think this paper could be scoped more carefully and the scope could be clarified better. I would recommend to accept, the work represents an advance across both theory and practice on an important problem.
Then, the authors design SSWR metric evaluate the efficiency and quality of the search process. The second reason is the lack of positioning of the proposed scheme/objective/data structure to a long line of research on the use of machine learning (with novel metrics/objectives and data structures) for search, including the learning of space partitioning trees [A, B, C], locality sensitive hashes [E] and representations [D] However, I am currently leaning towards a reject because of two main reasons: The first being the clarity of the presentation in the paper that makes it hard to identify
[Paper Weakness] The self-supervision between segmentation masks and detection bounding boxes is the main contribution. I am willing to increase my rating. In all, I do not think it meets the *CONF* bar. Although the self-supervised loss is intuitive, incorporating it into MONet is non-trivial and it does outperform MONet. Despite that such self-supervised works are hard to work on real scenes, this paper does have some merits. The self-supervised idea is interesting that uses the segmentation mask to get the pseudo bounding box label for object detection, which could ensure the consistency of object mask and bounding box. The author should compare their R-MONet(UNet) with the baseline of R-MONet(UNet) w/o the self-supervised loss, i.e. removing the object detection branch. In general - I feel the performance on these datasets is quite saturated and I hope to see results on more challenging data in the future - the proposed method included Post rebuttal comments: Thank you authors for the detailed response - I think some of of my concerns have been answered - the paper may be a valid contribution to the community and I am raising my score. Update: In general, I am happy with the authors' responses. This paper presents a variation of the MONet model where an additional Region Proposal Network generates bounding boxes for various objects in the scene.
The theoretical analysis shows that neural networks trained with smaller inverse temperatures (beta) exit the linear regime faster, which implies better performance. Strong and weak points of the paper Strong points Provided the novel time scaling analysis for the cross-entropy loss with many empirical validations.Weak points To improve the theory, I hope the paper can provide more insights into the instability caused by small beta. As the CIFAR-10 experiment points out, using small beta can still lead to good performance, so the main problem for small beta seems to be instability (rather than slow training), which the current theory couldn't explain. Ideally, I hope the theory can be extended to explain why small beta causes instability (the conclusion section mentions this as future work), and/or how neural network architecture affects optimal beta, but these extensions do not seem obvious. The paper deserves an accept because it is fundamentally correct and it is one of the "secrets-of-the-trade", and I am glad that it is written.
This paper proposes a generalized additive model to learn joint intensity functions for multiple Poisson processes. In this paper, the authors seem to assume that every dimension is a different process, and somehow events in each dimension are grouped together based on their arrival order ("Each ti is the time of occurrence for the i-th event across D processes and T is the observation duration"). So my first question is: i) Can you define the likelihood in a single Poisson process model in order to consider all the interactions you mentioned in section 2.2? For example, on page 2: Given a realization of timestamps t1,t2,…,tN with ti∈[0,T]D from an inhomogeneous (multi-dimensional) Poisson process with the intensity λ.
The paper proposes an efficient long-range convolution method for point clouds by using the non-uniform Fourier transform. The paper is clearly written, and presents an approach to efficiently utilize long range convolutions through a nonuniform FFT in for coulomb particle configurations. In terms of results, the paper clearly shows that NUFFT scales essentially like O(N) (with N the points number) whereas naive direct space convolution scales as O(N^2). However, the authors ignore pointing out the existing issues. I think such a discussion (and possibly a couple of experiments) would improve the paper a lot, showing the limits of the method and letting the reader understand the reasons for its strengths (which are very real, I do believe it !) In conclusion, I think the paper is marginally above acceptance level.
The paper makes an observation that datasets used for training many deep neural nets exhibit a strong factor structure, i.e. have a small number of dominant principal components explaining most of the variance. Experiments using MNIST and CIFAR10 show that the proposed method accelerates the speed to reduce the training loss. Sec. 2.1. Using lower-case kappa for a matrix is strange,  I initially assumed it's a scalar. 2 is very dense and difficult to follow, and only seems to motivate the method in the special cases of a very shallow linear regression model, where the gradient is easily related to eigenvalues of the input features X (and assuming a "strong factor structure", which has been shown to be reasonable for natural images.) In terms of criticisms,  there is very limited scholarship of related ideas that have been used both for linear models and for DNNs, in particular (a) various factor-based models that already exist,  (b) preconditioning of linear systems,  (c) neural nets trained on some other sort of residuals Additional details: Prior work -- that should be cited / contrasted with your approach:(a) Since a substantial part of the paper analyzes linear models,  it's important to mention that factor structure has been long exploited in various ML / stats works. Weak points Analysis in section 2 is based on linear regression, but the proposed method is based on deep models. Sec 3.3: "a standard SGD algorithm cannot be used to train a DNN model". Missing obvious baselines of training separate models on either the factor features or residual alone. Another criticism -- is that there is no discussion of how to do large-scale PCA / factor analysis for high-dimensional image data arising in say modern DNN image classification pipelines, and its computational cost,  as simple numpy.linalg.svd won't work. Overall I feel that the paper is not ready for publication at this time since the experimental validation is incomplete and does not fully explore the benefits and potential downsides of the proposed method.
This paper proposes a contrastive autoencoder approach that only requires small data to perform a multi-label classification on the long-tail problem. The technique proposed in this paper revolves around learning the label embeddings that would match the input embeddings, which is quite limited to multi-classification and might not carry much value to other tasks. I think the paper requires another round of revision before it is ready for publication.
The paper proposed a learned variant of the well-known iterative Hessian sketch (IHS) method of Pilanci and Wainwright, for efficiently solving least-squares regression. This paper leverages learned sketches to improve the convergence rate of second-order optimization methods. Summary: Previous work has shown that sketching the Hessian can improve the running time of second-order optimization methods. The present paper considers a particular kind of sketch for which the sketch matrix is learned from data. Overall, the idea of combining IHS with learned sparse sketch is interesting, but the reviewer believes that the current version needs significant amount of rework to be publishable in top conferences. While getting a learned variant for IHS is an interesting direction, the current theoretical contribution of this paper is only incremental, and most importantly, the reviewer is unconvinced for the practicality of the current approach. For example in Figures 1, 2, 6, 7, that the learned IHS has the same linear convergence rate as the unlearn IHS, and actually only slightly faster overall (only 1 iteration faster if we view the plots horizontally), but the authors' claims are like "We can observe that the classical sketches have approximately the same order of error and the learned sketches reduce the error by a 5/6 to 7/8 factor in all iterations", "We observe that the classical sketches yield approximately the same order of error, while the learned sketches improve the error by at least 30% for the synthetic dataset and surprisingly by at least 95% for the Tunnel dataset. I like the paper, but there are a few issues that must be addressed before it can be published.
References Moradi, M., Madani, A., Gur, Y., Guo, Y., & Syeda-Mahmood, T. Considering multi-modal multi-task settings in the clinical domain is useful for the development in this area. Either alone might not be grounds for rejection, but together they likely put this work below the acceptance threshold.
Specifically, they propose two regularization terms to 1) capture the diversity of the tasks and 2) control the norm of the prediction layer, thereby satisfying the assumptions in meta-learning theory. To improve the practical performance of meta-learning algorithms, this paper proposes two regularization terms that are motivated by two common assumptions in some recent theoretical work on meta-learning, namely To ensure the assumptions of the theories, the authors propose a novel regularizer, which improves the generalization ability of the model. [2] HOW TO TRAIN YOUR MAML, *CONF* 2019 The main motivation of this paper is based on the theoretical results of meta-learning. ########################################################################## Summary: The paper reviews common assumptions made by recent theoretical analysis of meta-learning and applies them to meta-learning methods as regularization. Results show that these regularization terms improve over vanilla meta-learning. Summary: In this paper, the authors aim at bridging the gap between the practice and theory in meta-learning approaches. Some results on few-shot learning benchmarks show the proposed method improves w.r.t. those baselines. Ref: [1] Andrei A. Rusu, Dushyant Rao, Jakub Sygnowski, Oriol Vinyals, Razvan Pascanu, Simon Osindero, Raia Hadsell: Meta-Learning with Latent Embedding Optimization. This work serves as a nice attempt to instruct the practice of meta-learning with theoretical insights. Strength: The motivation of this paper is interesting, before proposing the methodology. Here are the main concerns of this paper: The proposed method in this paper is based on the meta-learning theory as stated in Section 2. *CONF* 2019 Above all, since the contribution and the technical details to calculate the subgradients are not clear to me, I have to currently recommend a weak reject.
compared to existing graph pooling methods, the authors think their methods are able to capture information from all nodes, collect second-order statistics, and leverage the ability of neural networks to learn relationships among node representations, making them more powerful. Weaknesses: My biggest concern is that the proposed approach lacks originality and novelty because it is a simplification and variant of SOPOOL from Second-Order Pooling for Graph Neural Networks (Ji and Wang, 2020) Experimental results on four datasets (PTC, PROTEINS, IMDB-BINARY, IMDB-MULTI) of two tasks (bioinformatics, social networks) show that the proposed graph pooling method can improve the performance by 0.5%-1.2% accuracy while decreasing the std. Based on the author's writing, it is unclear what is the second-order statistics for graph pooling, why it is important to have second-order pooling, and how the proposed method can capture the second-order statistics. The writing can be improved, In the abstract and introduction, the author should describe the approach briefly and explain its characteristics including why it can handle variable number of nodes, invariant to isomorphic graph structures, capture information of all nodes, and especially why it can collect second-order statistics. In this manuscript, the authors propose two novel methods using fully connected neural network layers for graph pooling, namely Neural Pooling Method 1 and 2.
March 2019. https://arxiv.org/abs/1903.08254 [4] Explore then Execute: Adapting without Rewards via Factorized Meta-Reinforcement Learning. This is an interesting observation, but prior work [2] already shows that meta-RL is equivalent to learning the Bayes-adaptive optimal policy. Overall, I found this paper to be a well-written, well-executed, illustration of when it is optimal to learn task-specific adaptive behaviors. Summary: This paper explores the effect of time horizon on meta-reinforcement learning agents. The bandit example in Section 3 clearly illustrates the two regimes that this paper studies: when it is necessary to learn adaptive behaviors vs. Strengths: Novelty. Most meta-RL papers study the case where adapting to a new task requires learning new behaviors. Summary. The authors investigate the question of when the optimal behavior for an agent is to learn from experience versus when the optimal behavior is to apply the same (memorized) policy in every scenario. I enjoyed reading the paper and although the results are intuitive and unsurprising, they nicely emphasize the importance of environment and task design choices in strategies learned via memory-based meta-learning. If the research question is "How does the optimal policy depend on task parameters such as uncertainty and horizon?" I believe Bayes-adaptive work answers that question. There are a number of relatively minor weaknesses (as described above) but this is overall a nice paper and would be a good contribution to *CONF* 2021.
The paper proposes an approach for handling noisy labels in predictive models without removing them. The authors claimed that, by adjusting α through training, the trained model m(⋅;θ^) with an optimal parameter θ^ is asymptotically consistent with the model trained on a dataset with clean labels, i.e.,  the trained model without α performs well on clean test data P(i|x;θ)=exp⁡(mi(x;θ))∑jexp⁡(mj(x;θ)) In the proposed method, for the training set D={xn,yn}, we first train the model by minimizing the following loss function: θ^,α^=arg⁡minθ∈Θ,α∈RN×K−1N∑n=1N∑i=1K1[yn=i]log⁡exp⁡(mi(xn;θ)+αni)∑jexp⁡(mj(xn;θ)+αnj) Adding experiments with synthetic datasets with different levels of noise can be helpful in understanding the advantages of AC1/AC2 over other methods for handling noisy labels.
I think that the authors explore some interesting connections to recent work on identifiability in latent variable models, and understanding what these results imply for causal inference is important. The work seems to provide a strong theoretical background for VAE-based models to be used for identification purposes with latent confounding when proxy variables are present, allowing to explicitly specify conditions under which approaches modelling confounders for adjustment can work. al. was made very clear, I needed to go to the original paper for a clear specification of the precise conditions required for the method to work (exponential family, outcome is function of latent with small additive noise, etc). Despite some clarifications from the authors I still vote for rejection as I believe the paper requires a major revision.
Although previous papers have reported that the NAT models can achieve the same performance level with auto-regressive translation models while the decoding speed is much faster, like two to five times, this paper points out that it deeply relies on the batch size and computation environment. Experiments on several MT benchmarks show that the proposed approach achieves speedup over the full AT baseline with comparable translation quality. This is a proper investigation for the community since some researchers might believe that NAT is always faster than standard auto-regressive models and became an excellent alternative to them. Besides, one thing should also be noticed: the speedup yielded by HRT seems less charming compared to recent NAT models, such as Levenshetain Transformer 3x-4x (Gu et al, NeurIPS 2019), and JM-NAT (k = 10) 5.73x (Guo et al, ACL 2020) where both models achieve similar translation quality to the AR baseline. Early on the paper claims that HRT can outperform AT in terms of translation quality, but this doesn't seem to be the case if one compares HRT against AT both trained with mixed distillation.
############################################################################### Summary The paper presents a novel method for learning branching strategies within branch-and-bound solvers, which consists in a graph-convolutional network (GCN) combined with a novelty-search evolutionary strategy (NS-ES) for training, and a new representation of B&B trees for computing novelty scores. The authors present their model as a primal-dual iteration policy, for what looks like a simplified GCN architecture for the one from Gasse et al. The same structure (bipartite graph) and the same features are used. But in the same table, the GCN model trained to imitate SB results in trees much smaller than the expert (418 vs 1304 on independent set), which again is suspicious, and most importantly contradicts the original claim of the authors. Also, Figure 4 indicates that a pre-trained GCN model results in a tree size of 350 on independent set problems, which does not coincide with the number 418 reported in Table 1. Cons: the experimental setup followed in the paper is questionable the presented experimental results are inconsistent with the literature, which I find suspicious the overall method description comprises several blind spots and fallacious arguments It is always easy to raise questions about whether testing could be on a larger set of problems, but those presented here seem suitable for a conference paper.
The paper proposes a knowledge distillation method for face recognition, which inherits the teacher's classifier as the student's classifier and then optimizes the student model with advanced loss functions. The paper demonstrates using an ensemble of teacher models can boost the performance of knowledge distillation. The idea of using teacher model's classifier to directly reshape the student model's feature representation is somewhat novel. The experiment lacks comparison with the general knowledge distillation methods (Ref.2) in image classification and the specific used methods (Ref.3) in face recognition. This paper proposes ProxylessKD method from a novel perspective of knowledge distillation. 2019. [3] Karlekar, Jayashree, et al. "Deep face recognition model compression via knowledge transfer and distillation." arXiv preprint arXiv:1906.00619 (2019). For example, when ProxylessKD is combined with the proxy task, i.e., feature distillation loss, how would it perform compared to only ProxylessKD? It would be interesting to see how performance changes with more layers of student model inherited from teacher model.
Before Author Response Of course, if the authors address my concerns then I'll increase my rating. Specifically, it proposes PeerPL to perform efficient policy learning from the available weak supervisions, which covers PeerRL (for RL with noisy rewards), PeerBC (for imitation learning from imperfect demonstration) and PeerCT (for hybrid setting). They also show that PeerBC outperforms standard behavioral cloning in learning from synthetic demonstrations on several Atari games, as well as the cart-pole and Acrobot tasks, all with noisy versions of the base reward signals. This paper formulates a framework for reinforcement learning and behavior cloning from weak supervisions (i.e., noisy rewards or imperfect expert demonstration).
As part of the  paper, they also evaluate and discuss the performances of four object-centric representation models, one of them (Video MONet) being an extension of an existing approach, proposed as part of this paper, and the remaining being state of the art approaches for the task. Instead of focusing on such a comprehensive set of things - 3 datasets, five models and multiple metrics it would have been better if authors did some minor extensions of the models and showcase novel directions. Paper Strengths Although I am not familiar with unsupervised learning of object-centric representation, I like the idea of proposing a common protocol for evaluation. The paper proposes a benchmark for the evaluation of unsupervised learning of object-centric representation. The benchmark consists of three datasets, multi-object tracking metrics and of the evaluation of four methods. Overall, this paper is interesting in setting up a benchmark for unsupervised object representations which is a very important problem in computer vision, reinforcement learning, etc. Given the cons and specifically on not a clear actionable suggestion on how to improve models and no analysis beyond synthetic datasets I am leaning towards a rating of below acceptance threshold. Overall, I think the paper could be a nice contribution to the literature on unsupervised learning of object-centric representation, but it lacks sufficient contribution for *CONF*, in my view.
The paper proposed a novel personalized federated learning method using a mixture of global and local models. The paper is an integration of the mixture-of-experts method with existing personalized federated learning. Authors claim the proposed method can protect user privacy since a client can select which data need to be excluded from the federation. The mixed-use of global and local models (equation 6) is not a novel way of federated learning. https://arxiv.org/abs/2007.14513 Overall Score Given the above concerns, I recommend reject this paper in the current stage. The authors should add a discussion to clarify how the old method (mixture-of-experts) can fit into the new environment: federated learning setting and deep learning.
The paper proposes a scalable approach via intention propagation to learn a multi-agent RL algorithm using communication in a structured environment. I'm not an expert in the area and wouldn't expect to follow all of the reasoning in constructing the method, but I would be expect to be able to follow some clear statements of the algorithm, or its theoretical properties (guarantees of some solution quality given certain assumptions, the parameters affecting this, etc. For the assumptions on rewards, Proposition 1 assumes that each agent's reward depends on its neighbors, while the derivation of Equation (3) (and thus the following algorithm) further assumes that the reward depends on pairwise actions. For partial observable environments, the proposed methods needs to reply on DGN. =-= comments after author discussion Although the authors emphasize that they are communicating the intentions of agents, I think their method is quite similar to those communicating local observations, like NDQ (https://arxiv.org/abs/1910.05366), DGN, or CollaQ (https://arxiv.org/abs/2010.08531). For the baselines used in the experiments, it seems that only IP and DGN allow communication/message passing during execution, which makes it unsurprising that the two methods outperform other baselines. What makes this principled? This would seem to need a clear statement: what are the exact assumptions, and what precisely is the quality of the output? However, the exact assumptions are not clear, and the chain of issues discussed throughout section 4 seems to include discussion of approximation. However, the exact assumptions are not clear, and the chain of issues discussed throughout section 4 seems to include discussion of approximation. However, I don't think the resulting method has much algorithmic novelty. (2) Some parts in the method section are hard to follow. It is a little bit unclear what assumptions are required for all the theoretical and experimental claims of this paper. Major Comments/Questions Although the motivation has an interpretation of intention propagation, the resulting architecture (Figure 1b) and loss functions (Section 4.2) seems to be a standard messaging passing architecture with SAC loss functions that loses the intention semantics.
The methods include AdaQuant (which jointly optimizes quantization steps for weight and activation per output activation of each layer), Integer Programming (which determines bit-precision for all the layers), and the batchnorm tuning. Tuning batch norm weights by re-computing statistics. In particular, the authors focus on sub-8 bit quantization and propose a novel integer linear programming formulation to find the optimal bit width for a given model size. Page 8: For instance, on the extensively studies -> For instance, on the extensively studied This work presents a quite comprehensive multi-step scheme for post-training neural quantization that does not rely on large datasets or large computational resources. Pros: The empirical results are relatively strong in this method; 4-bit quantization is a good achievement in the models considered here. In more detail, the authors claim four proposed components: AdaQuant, Integer programming, Batch-norm tuning, and two pipelines for neural quantization. This is a good result but please note that other work in the literature (arxiv:2001.00281) reports 72.91% for INT8 quantization of MobileNetV2 (this comparison is actually missing from the paper). Page 4: "Depending on the needs, our performance metrics P would be either the execution time of the network or its power consumption." This is good but no result on either latency or power consumption is provided in the paper. Overall the paper is strong however, please note the following: Page 3 last paragraph: it seems there are errors in the results for calibration data. Also, it seems that the "per-channel" quantization method is utilized in this work, but the formulation in (2) seems to be for "per-layer" optimization. The authors presented promising experimental results on various neural networks to support the proposed methods. There is no clear explanation of how AdaQuant increases the generality of the quantized model, and the discussion about the sample size (B) is hard to understand (why there's infinite solution when B << N? For instance, the authors omit Nagel et al 2020, which seems to do better at similar quantization levels, but I am not sure the results are directly comparable. BOPS proposed by (https://arxiv.org/pdf/2005.07093.pdf) is a good metric to measure the total reduction in computations for mixed precision quantization. Currently, the proposed methods only utilized "per-channel" quantization. Baselines are barely discussed. Also, I am not very familiar with quantization papers, so I might have missed relevant baselines, but they seem hard to compare. It is straightforward to think that the joint optimization of quantization step size for weight and activation would result in better quantization results. Overall: An engineering oriented paper with some lack of testable hypotheses and analysis of some parts of the methods, but the 4-bit results justify publication.
It proposes to use Hamiltonian Monte-Carlo (HMC) to sample the next states (instead of IID samples) and matrix completion to learn a low-rank Q matrix. It shows theoretical convergence. Experiments on discretized problems (CartPole and an ocean sampling problem) show that HMC and low-rank learning can behave more benignly compared to IID samples. The limitations of Q learning (equation (1)) should directly motivate use of Hamiltonian Monte Carlo (HMC) in Section 2.2, but instead the manner in which HMC is presented is as additional preliminary material. I strongly suggest the authors consider better explaining the links between step (4) of Algorithm 1 and proximal methods in any revision of this work.
I found this to be an interesting work that provided a non-trivial insight, backed it up with clear and easy-to-follow theory and demonstrated their observations held on some medium-scale experiments. If the experimental setup was more in line with prior work and the same trend in results held, then I would be more likely to recommend acceptance of this paper. In the next paragraph, the paper mentioned that the generator gradient "is only locally informative" - again this is true for all GANs. How is this relevant for the argument made in the paper? I am not an expert in the GAN field, but from inspecting the Conv-4 model with spectral normalization on CIFAR10, it appears like the best performing model achieves an FID of approx 42 and the worst model gets around 48 (figure 8, bottom right). The baseline NS-GAN with spectral normalization presented in this work greatly underperforms previously published methods that use the same objective. -- especially since the standard CNN model proposed in the original spectral norm paper can be trained with reasonable compute and the authors have released code. Also, for eq. (7), technically the optimal discriminator is only uniquely defined at the real data points (see Sinn & Rawat, AISTATS 2018) because the GAN is trained on a finite sample. Since a near 20-point increase in FID can be achieved with a few tweaks to the NS-GAN objective (SN-GAN), I am left wondering if the presented improvement from the MM-NSAT objective will vanish once those improvements are applied or if it will still hold. State of the art results are by no means required for publication in this venue, but the baseline models presented here perform much worse than they have been shown to in prior work. In the next paragraph, the paper mentioned "NS-GAN struggles to discover new modes: g(x) ≈ 0 ⇒ 1 − D_p(x) ≈ 0" - the connection between the claim about the difficulty of discovering new modes and the equations should be explained more clearly. The original paper on spectral normalization for GANs reports an FID of 29.3 for standard CNNs. So technically the paper's claim on the discovery of new modes cannot be justified by "g(x) ≈ 0 ⇒ 1 − D_p(x) ≈ 0", because D_p(x) could take on any value at locations other than the data points. It is unclear how the number of samples from O in the minibatch could cause a difference between MM-GAN and NS-GAN - this observation is true for both MM-GAN and NS-GAN! For these reasons, I am advocating against acceptance of this work but making it clear that I think this paper is borderline.
Ride: Rewarding impact-driven exploration for procedurally-generated environments. Unifying count-based exploration and intrinsic motivation. I do not agree with the statement: "We note that MiniGrid provides a sufficiently challenging suite of tasks for RL agents despite its apparent simplicity, as ICM and RND fail to learn any effective policies for some tasks due to the difficulty posed by procedurally-generated environments." The authors study exploration in procedurally-generated environments with sparse reward, proposing a new method, termed RIDE-SimCLR, for addressing this problem. I recommend rejection, though I am certainly open to changing my mind, especially if the case can be made that the benchmarks are exhaustive enough, or that it's unreasonable to expect more benchmarking within a single publication.
The paper develops its own algorithm EIIL which extends the Invariant Risk Minimization (IRM) of domain generalization to work in the situation when the prior knowledge of environments is not available. I have several points for clarification that I detail below. The authors are definitely correct in crediting a prior fairness paper for the idea of  the idea of adversarially re-weighting examples with a soft groups model, but as they themselves point out this idea has existed in different forms in the domain generalization literature (e.g. DRO). Overall, I'm able to see what the authors are trying to get at with this example, but unfortunately the revisions aren't sufficient to address all of my concerns regarding the theoretical results. ---post rebuttal--- After reading the authors' response, the other reviews, and the revision to the paper, I find that my comments are not sufficiently addressed. The author did not even acknowledge the existence of the prior work, REPAIR, in the revised paper. The high-level idea is that similarly to the way that fair algorithm are able to improve the worst-case accuracy of predictors across different groups without knowing the sensitive attributes, perhaps we can use these ideas to domain generalization when environment partitions are not known to the algorithm.
The paper therefore proposes a measure based on the discrepancy of a group of segmentation models to identify more valuable images to annotate and add to the training data in a iterative fashion. Annotating images for training of segmentation models is time consuming and it can be difficult to annotate enough examples to ensure good performance on the rare difficult examples that often occur when methods are applied to real world data. Weakly-supervised labeling is more practical for segmentation; and (2) extending to active training/tuning, leveraging the selected hard examples to improve the segmentation model for multiple rounds. Leveraging those counterexamples to improve the segmentation models' generalization performance on unseen images seems to be novel in this field. It is my understanding, but I am not actually sure so it would be good to have the approach clarified, that the test datasets (T(1), T(2), and T(3)) of iteration 1, 2, and 3 are hard examples, and are thus biased towards the methods involved. If the methods tend to disagree on a limit number of typical cases, then these cases will be added to the training set and it is not so surprising that improvements in the target model is seen. "This also provides direct evidence that existing segmentation models could be particularly weak at certain real-world generalization, which is not surprising because the 1,464 training images are deemed to be extremely sparsely distributed in the space of natural images." I am not sure I agree with the evidence part. I noticed in the Appendix, the authors also compared their method with entropy-based active learning – a vanilla baseline needing no competing model. Recommendation: Given the lack of a competitive baseline, opaqueness around the impact of the algorithm's hyperparameters, and what's likely to be noisy estimates of improvement, I cannot recommend this paper for acceptance as is.
Based on these factors I will raise my score from 'clear rejection' (3) to 'okay, but not good enough' (4). Author comments on step (ii) I'm open to adjust my recommendation, assuming these concerns are sufficiently discussed in the paper and additional ablation is conducted.
arXiv preprint arXiv:1906.05274 (2019). The paper presents a pre-training scheme (APT) for RL with two components: contrastive representation learning and particle based entropy maximization. Can authors present state entropy plots, and possibly compare the performance of APT with other methods seeking a similar objective, such as MaxEnt (with representation learning), SMM [2] or MEPOL [1]. The authors propose to overcome these limitations by using a particle based entropy estimate in the learned representation space. Authors claim that the main benefit of APT over a prior work method (MEPOL, Mutti er al., 2020) is a lower variance of the gradient estimation, thanks to the choice of avoiding importance weights corrections. EVALUATION Unfortunately, over some concerns regarding the novelty of the presented method and its experimental validation, which I find somewhat weak for an essentially empirical work, I would lean towards rejecting the paper.
The experimental results show that the method is effective to train a model in an unsupervised manner using just the raw data to predict dependency and constituency parses and performs better than the relevant baselines. This paper proposes a neural network optimized by MLM loss that has inductive bias to be useful for unsupervised constituency and dependency parsing. This paper has an interesting idea at its core: stemming from the success of models like ordered neurons, can we define a neural architecture that uses both constituency and dependency structure? However, there are a number of unclarities in the paper that make it difficult to determine whether the results on unsupervised parsing are actually comparable to prior work. The model outperforms ON-LSTM in constituency parsing and is competitive with classical NLP methods that use gold POS tags in dependency parsing. These examples of inaccessible writing (Algorithm and section 4.2) make the core part of the paper difficult to understand and hence I think this paper can be better with another round of thorough revision. The experimental results are difficult to evaluate, as there are different datasets and assumptions made by different work, so there does not appear to be a baseline that the new model can be compared to fairly. The results are better than trivial baselines for unsupervised constituency and dependency parsing but not as strong as related recent work. The model is trained for masked language modeling (MLM) and evaluated via MLM on held-out data and its ability to induce constituency and dependency trees. This paper describes a neural architecture that resembles the transformer but includes explicit representations of constituency and dependency structure. If I'm correct about this, it constitutes clear grounds for rejection of the paper.
This work presents an interesting exploration of learning optimal data sampling probability. To this end, authors propose to record the data frequencies through all previous steps and thereby generate the sampling policy for the next step. Advantages: l    The exploitation step and exploration step in AutoSampling is interesting, it is straightforward that this method can work well as the sampling strategy is updated dynamically according to the current state of model. First I will comment on the listed contributions: • To our best knowledge, we are the first to propose to directly learn a robust sampling schedule from the data themselves without any human prior or condition on the dataset. • We propose the AutoSampling method to handle the optimization difficulty due to the high dimension of sampling schedules, and efficiently learn a robust sampling schedule through shortened reward collection cycle and online update of the sampling schedule. To address the issue of optimizing high-dimensional sampling hyper-parameter in data sampling and release the requirement of prior knowledge from current methods, the authors introduce a searching-based method named AutoSampling. If so this does not seem sufficient to confirm that this method works because the data you sample from is always the same. The exploitation step train multi child models with current sampling strategy and save the best model for next iteration. Typo: "Algorithm 2: Search based AuoSampling" The authors mainly concentrate on data sampling. This method is comprised of exploration step and exploitation step which are conducted alternatively. Authors should compare with a few state-of-the-art data-sampling or data-reweighting methods, such as Focal Loss proposed by He et al. In addition, Tables 2 and 3 should be clarified in the rebuttal.
Summary The paper introduces two simple modules, SelfNorm (SN) and CrossNorm (CN), that are highly modular and can theoretically be attached to different parts of the CNNs to control the balance between style and content cues for their recognition. In particular, this paper proposes to recalibrate style using SelfNorm motivated by the fact that attention help emphasize essential styles and suppress trivial ones and reduce texture bias using CrossNorm by swapping feature maps within one instance. - Extensive experiments under different settings and tasks show the effectiveness of the proposed method. I find it difficult to follow the rationale behind key assumptions (e.g. that shape = content and texture = style) and the experimental section is, I have to admit, a bit chaotic. Cons: - In Sec. 3 Unity of Opposites, the authors explains that SelfNorm works during testing while CrossNorm functions only in training. The rating reflects this disappointment. I do observe a few improvements introduced by the two modules here and there, but I can't forgo the impression that these are only selected highlights that comply with the authors' arguments. .  Section  . .  Data  . .  Arch  . .  Evaluation  . .  Baselines  . .  Authors' methods  . .  Tab1  . .  CIFAR  . .  4 archs  . .  mCE,CleanAcc  . .  Cutout,Mixup,Cutmix,AA,Advtr,AugMix  . .  SNCN, SNCN+AugMix .  Fig2  . .  CIFAR  . .  28-2WideResNet  . .  mCE,CleanAcc  . .  WA,RA  . .  CN .  Fig4  . .  CIFAR  . .  40-2WideResNet  . .  mCE  . .  VanillaModel  . .  SN,CN .  Tab4  . .  CIFAR  . .  40-2WideResNet  . .  mCE  . .  VanillaModel  . . CN .  Tab5  . .  CIFAR  . .  40-2WideResNet  . .  mCE  . .  VanillaModel  . . SN,CN,SNCN,SNCN+Crop,SNCN+Crop+CR .  Tab2  . .  ImageNet  . .  ResNet50 . .  mCE,CleanAcc  . .  PU,AA,MaxBlur,SIN,AugMix  . . CN,SN,SNCN+AugMix .  Section  . .  Data  . .  Arch  . .  Evaluation  . .  Baselines  . .  Authors' methods  . .  Tab1  . .  CIFAR  . .  4 archs  . .  mCE,CleanAcc  . .  Cutout,Mixup,Cutmix,AA,Advtr,AugMix  . .  SNCN, SNCN+AugMix .  Fig2  . .  CIFAR  . .  28-2WideResNet  . .  mCE,CleanAcc  . .  WA,RA  . .  CN .  Fig4  . .  CIFAR  . .  40-2WideResNet  . .  mCE  . .  VanillaModel  . .  SN,CN .  Tab4  . .  CIFAR  . .  40-2WideResNet  . .  mCE  . .  VanillaModel  . . CN .  Tab5  . .  CIFAR  . .  40-2WideResNet  . .  mCE  . .  VanillaModel  . . SN,CN,SNCN,SNCN+Crop,SNCN+Crop+CR .  Tab2  . .  ImageNet  . .  ResNet50 . .  mCE,CleanAcc  . .  PU,AA,MaxBlur,SIN,AugMix  . . CN,SN,SNCN+AugMix Recommendation As it is right now I think the paper has to be rejected because the write up is just too chaotic and vague. - The authors explains SelfNorm recalibrate feature style while  CrossNorm performs style augmentation.
The authors present a systematic evaluation of atomistic learning across multiple tasks and show that 3D data consistently yields better performance than 1D and 2D methods. [3] Karimi et al: DeepAffinity: Interpretable Deep Learning of Compound-Protein Affinity through Unified Recurrent and Convolutional Neural Networks In this paper, the authors introduce a repository of datasets for several atomistic learning tasks. Bioinformatics, 35(3), 470-477. This paper presents a large benchmark of machine learning tasks for molecules represented by the 3D coordinates of their atoms. Some tasks like Ligand Binding Affinity (LBA) and Ligand Efficacy Prediction (LEP) requires modeling representations of both proteins and small molecules. By creating a standardized set of prediction tasks and associated data sets, the authors have presented a resource that may help the community to compare 3D atomistic methods quickly and fairly. Determining the multiple possible conformations of drug-like molecules is still an ongoing research topic (see for example the review of Hawkins (2017)), not to mention the determination of the 3D structure of proteins, which is indeed the topic of one of the data sets provided. I feel that the selection of multiple tasks is limiting the authors in the amount of information that they can fit in the actual paper. A systematic benchmark with atomistic learning methods is presented, showcasing the value of using 3D atom-level data instead of 1D or 2D features. The idea of using atomistic learning or at least 3D derived features have already been implemented or at least contemplated in many of the presented tasks (Gilmer et al [1], Wu et al [2], Townshend et al. [3]). Mahé, P., et al. "Graph kernels for molecular structure− activity relationship analysis with support vector machines." Journal of chemical information and modeling 45.4 (2005): 939-951. Hawkins, Paul C. D. "Conformation generation: the state of the art." Journal of Chemical Information and Modeling 57.8 (2017): 1747-1756. This is obviously essential to the paper and I would feel more comfortable accepting a version of the paper that includes this information (possibly with URLs withdrawn if there is a concern about maintaining the review process blind).
At the core of this paper, the authors argue that the effective gradient flow (grad norm from only activate model weight dimensions) is an effective indicator on the model accuracy attained by sparse training. Equipped with the proposed methods EGF and SC-SDF, the paper then analyzes several (standard) approaches like batch normalization etc as to determine which of these approaches are useful for training sparse models.
The paper proposes a sensor fusion approach combining radar and camera to improve the detection of object in an automotive sensing scenario. However, since object detection on radar data is already proposed and studied by some researchers (e.g., [1][2][3]), the reason why this problem needs to be proposed separately is not clear to me. Cons: Magic numbers?: in paragraph 2 of section 4, the author(s) list the way to split the regions and sampling rates for different regions, but does not explain the reason to do so. Post-rebuttal review: I carefully read through the rebuttal and other reviews and  I would stick to my current rating. ########################################################################## Summary: The paper develops a method to select a radar return region to be sampled at a higher rate based on a previous camera image and radar recording. Overall, I think the paper is not good enough to be accepted by *CONF*.
Strengths: The paper focuses on the important problem of exploiting weakly labeled video data, by exploiting its structure, for example by recovering temporal structure in an autoencoder fashion. The cons include (In my opinion, the main weakness is the experiments): Some design of model (e.g., Traversing across the concept hierarchy, Observation, and Instruction Regeneration, etc) are not fully estimated in this section: are they really useful? Experiment results are weak. The proposed model's performance does not exceed the simple equal division baseline a lot when 64 frames are sampled for each video. Introducing a two-level hierarchy into concept learning is also not new. Specifically, this paper considers 1) unsupervised setting; 2) the hierarchy of concepts, and conducts experiments in two datasets. Final recommendation Overall, I believe the paper as it stands is not ready to be presented to *CONF* and I recommend a rejection. The paper introduces the solution of an important task: hierarchical concept learning(or temporal abstractions) from demonstration data. This paper addresses the problem of extracting a hierarchy of concepts in an unsupervised way from demonstration data. This paper addresses a relatively new topic to learn the hierarchical concepts in videos and commentary in an unsupervised manner. The authors may need to look upon those for a better variety of baselines and also evaluation metrics. The authors present a Transformer-based concept abstraction architecture called UNHCLE and show how it discovers meaningful hierarchies using datasets from Chess and Cooking domains. This paper presents a method to unsupervisedly discover structure in unlabeled videos, by finding events at different temporal resolutions.
(6) On page 2, the authors said "SwarmSGD has a Θ(n) speedup in the non-convex case, matching results from previous work which considered decentralized dynamics but which synchronize upon every SGD step." What is the measure, is it the number of communications, local SGD iterations or gradient evaluations? (3) In Theorem 4.1, the assumption that T>=n^4 (n^4 can be very large) is the disadvantage of this algorithm because the same convergence rate O(1/sqrt(T)) has been achieved without such assumption in some distributed settings, including plain distributed SGD, federated average, etc. UPDATE I thank the authors for addressing my concerns and confirm my initial rating. Even nodes don't communicate, the algorithm should still converge because the global sampling. However, if the analysis can not explain why more local updates can reduce communications, I would not recommend to accept. Update Thanks for the authors to address my questions. Still, it would help to clarify the following points from the beginning: what the authors mean by decentralized, later explained as decentralized model updates, but centralized/distributed data for the experiments; Further questions: Is it possible to merge Section I with Theorem 4.1 or show the proof? where and when quantization is applied and why it helps in reducing communication complexity in the main text. Please clarify this. (7) In the contribution part, the authors mention that their new algorithm has lower average synchronization cost per iteration but more iterations in the experiments, how about the total synchronization cost? It would be also interesting to report communication complexities, with and without quantization, and compare them to state-of-the-art methods. define T (global number of edge updates) and H (number of local updates in between edge communication); For example, For Theorem 4.1, use 1≤r2λ22 can get rid of the constant 1. (8) The authors use multiple variables to denote the number of nodes, including n, P and m. Optional improvements: It may be better to remove some small terms to make rate more clearer. In the proposed algorithm, each time an edge is activated and the two nodes connected through the edge are updated. For (14) and (19), use rλ2≤r2λ22 to get rid of the first order term. This bound however stands for the average of all models obtained at each global step t, meaning that it is not necessarily a tight bound for the second moment of the last obtained model, which is the bound we are ultimately interested in. However, I believe the authors can improve in the next version. The 3rd equation in Section D, h~is also depends on g~i, which is not reflected.
Prior work used uniform samples, but this paper proposes an approach based on locality-sensitive hashing. [Edit: thanks for clarifying this.]
Thanks for the discussions. The primary reason for my score increase is discovering that the power of the framework is finding a representation that is quick at exploiting new opponents. Re-reading after the paper update, I am worried that a significant portion of readers may fall into the same trap despite the authors' additional edits. After seeing the authors' responses to my concerns, I am open to raising my score. Science, 2019. After rebuttal: The responses address most of my main concerns, and I have increased the rating from 5 to 6. Questions during rebuttal period I think several questions have already been raised in the rest of my review. Science, 2019. After rebuttal: The responses address most of my main concerns, and I have increased the rating from 5 to 6. Thanks for the discussions. The primary reason for my score increase is discovering that the power of the framework is finding a representation that is quick at exploiting new opponents. Score I am recommending reject [UPDATE - see above] - although I will maintain an open mind due to my middling confidence in some of the background literature around the paper.
The paper propose a new meta-learning algorithm ADML that uses adversarial and clean examples during meta-training (both for train-train and train-test). The authors demonstrate that a simple approach of mixing adversarial and clean data in the usual MAML update doesn't work very well, but don't go in to any details as to why this failure motivates their method. Recommendation: rejection Motivation: While the idea of adversarial meta-training is well motivated and generally sound, the specific method in this paper is primarily proposed and not really explored in any depth. Finally, minor points: there is a typo on P1 ("decent") and the works on adversarial attacks on related problems, such zero-shot learning, can be included into the discussion for completeness. Actually, a recent work "Adversarially Robust Few-Shot Learning: A Meta-Learning Approach, NeurIPS 2020" adopted nearly the same setting and datasets and achieved superior results. Recommendation Because of the lack of motivation for adversarial meta-learning, lacking intuition for ADML, and particularly the robustness evaluation against a very weak attack, the paper is a clear reject to me.
The authors propose a gradient-based meta learning method (MACAW) to approach this problem, which uses an actor-critic method combined with advantage weighting which is an offline RL method. Introducing MACAW: an algorithm for offline meta-RL that has the desirable property of being consistent (i.e. converges to a good policy if enough time and data for the meta-test task are given, regardless of meta-training). The paper also proposes a method for the fully offline meta-RL problem based on the MAML method. The paper proposes the problem of fully offline meta-RL. The authors also explore settings of good/bad adaptation data showing the robustness of the proposed method to quality of offline adaptation data as compared with MAML+AWR Overall I like the paper and think the problem setting is very interesting, and I like the proposed method. A solution is proposed for the fully offline meta-RL problem. Per paper definition, if an algorithm can find a good solution to test tasks regardless of the meta-training task distribution, is called consistent. To do so they rely on MAML (which provides consistency) and AWR (a simple, popular offline RL algorithm) and add a couple of changes: some hyper-network like parameterization to add capacity and adding an extra objective in the policy update to enrich the inner loop. My comments: While this paper touches on a very interesting and practical problem in meta-rl and batch-rl, I didn't find their setting is very realistic with respect to batch and offline RL setup. A main motivation for the offline RL setting is that you can use real-world data and train a metaRL agent using this. It would still be great to add an experiment where MACAW is adapted online at test time (entirely without offline data) like in C.2 but on in-distribution tasks. However, in the offline setting the policy that generated the batch of data is of critical importance and should be part of the task definition. This is one of the key differences, IMO, between meta-RL and offline meta-RL and given that this paper's main contribution is introducing offline meta-RL, I feel it really should be very clear about this point. It may be fine to first introduce the correct general version and then say something like "it may be useful to assume each RL task is given by an expert of roughly the same characteristics", i.e. we can assume behavior policy is constant across tasks.
The authors use statistical measures such correlation between model predictions, change in performance with and without ensemble game models, and a state of the art method to characterize the functional behavior. Positives: The paper offers some interesting revelations such as : all sources of uncertainty have similar effects, which is surprising as the authors note, and hence a valuable insight. Linking all the nondeterminism to a change of one bit in model weights is interesting and successfully highlights the instability and sensitivity of neural network optimization. Ensembling methods in general will help variability but there is no change in the optimization and training process and ensembling gains are due to other reasons. Some aspects about the protocol metrics are intuitive as the authors explain, despite this, a theoretical analysis to back the experimental findings would have made the paper stronger. Is there anything more fundamental about the particular value of variability that all the different sources of nondeterminism concentrate around? More novelty is needed for this work to be published in *CONF*.
The evaluation performed on multiple tasks shows that the proposed approach can reach the quality of a vanilla transformer and be more memory-efficient. For AFT models, on the other hand, this difference is negative which might suggest that they have already overfit at 70k iterations and they will never reach the resulting performance achievable by the baseline. While the motivation of the authors is to replace MHA with more cost-efficient operation, it is not clear whether the proposed method is the better alternative. This paper proposes an efficient transformer variant by replacing softmax in the self-attention layer with a RELU activation and arranging the computation using element-wise products and global/local pooling.
This paper presents an interesting collaborative MHA to enable heads to share projections, which can be easily applied to most existing transformer-based models, including NMT and pre-training models. The authors show that query-key projections are redundant because trained concatenated heads tend to compute their attention patterns on common features. The authors also propose a Tensor Decomposition based method to easily convert MHA to its collaborative version without retraining. Overall, the paper is well motivated and provides a deep analysis of redundancy of the multi-head attention. Namely, while the original definition of attention layers does not have biases in linear projections for q/k/v in attention, the authors claim that implementations contain the bias terms, and spend some time showing how to model the biases in key and query layers properly. Since the method operated only within attention layers (reduces only dimensions of queries and keys), in terms of efficiency/quality trade-off it should be compared to other methods, e.g. simple head pruning. While the paper does not provide such comparison, it is clear from the results that the simple head pruning is likely to be superior (and is simpler implementation-wise). While there are some parts of the paper that I like, the main claim is not supported empirically: both in terms of baselines and the overall decrease in the number of parameters. -- From Figure 3 and Table 2, even reducing the Dk^ from 768 to 128, the total number of parameters of pretrained models (e.g. BERT-base) only reduces from 108.3M to 96.6M. In the Figure 3, the training time of the standard MHA with D_k = 256 is 0.0? Overall, I think the paper has improved during the discussion period. In the current state, I think it is ok :) Overall recommendation Overall, I can not recommend accepting this paper.
Summary of the paper The basis of this work is van Hoof et al., 2017; there, "Generalized Exploration" views policy parameters as being drawn from a per-trajectory Markov chain. Following the prior work, this paper proposes an exploration method unifying the step-based and trajectory-based exploration. Pros: For combining the proposed method with on-policy learning, this paper derives the log-likelihood of whole trajectory recursively. For on-policy methods (A2C, PPO), the proposed method has large performance gain on Mujoco tasks. But for the three bullet points in section 1, the first point of "Generalizing Step-based and Trajectory-based Exploration" should not be one of the main contributions of this paper, because this paper follows the formulation of policy in van Hoof et al. (2017) and the latter proposed the generalized exploration connecting step-based and trajectory-based exploration. But the proposed method does not obviously improve the performance of SAC while inducing much more complexity in policy learning. The paper focuses on exploration, but the experiments only focus on the return performance of simple Mujoco tasks. Summary: This paper proposes Deep Coherent Exploration that unifies step-based exploration and trajectory-based exploration on continuous control.
Motivated by the fact that NF are diffeomorphic  transformations from a simple space, ideally Euclidean, the author address the problem of modeling data which distribution is defined on more complex and unknow manifolds.The idea consists in inflating the data manifold with suitable noise (normal noise) in order to make it diffeomorphic to a simpler space where a NF can be estimated. The proposal of this study is to make NF applicable by inflating low-dimensional manifolds with Gaussian noise. Page 12: Please correct the number of table in "Table ??" This paper proposes a method for estimating the probability deinsity distribution on a low-dimensional manifold embedded in a high-dimensional space using Normalizing Flow (NF). Since the dimension is not full, it is suggested to add a gaussian noise to the data points (this is equivalent to a certain convolution of the initial distribution function, ie equation 3).
In Section 3 "In FTSO, this problem is almost nonexistent because, the skip connection has no kernel weights to tune", this is not very clear how the problem exists in the DARTS formulation and how the formulation in the proposed approach can mitigate this issue. After reading the response and the comments of peer reviewers, the rating is altered as follows. Considering the significance of the improvement on computational efficiency, it will be more valuable to discuss whether such a kind of decoupled search can be generally applied, or it can only work for certain kinds of network architecture, data, or tasks.
Agreed that PER is a reasonable choice, and it can upper bound the EIB and EVB metrics (i have issues with this too, more on this next), it just seems to me that the paper doesn't make any convincing claim for why this helps us understand why PER works. Summary: The authors of this paper make a connection between the TD-error from a single unit of experience and various metrics of improvement for agents trained with prioritized experience replay and Q-learning or soft Q-learning. Notably, that when we know it upper bounds the three metrics, we want to continue prioritizing sampling experiences when training our agent, because this will yield faster learning, since we will have larger improvements to our agent. Could sequential replay be the result of using an underlying metric that is less myopic than EVB, which seems to shows better promise over PER?
The authors claim that: 1) they "build tractable algorithms with polynomial complexity", "a detection algorithm linear in time"; 2) the algorithms are very suitable for parallel architectures; 3) an improvement of the state of the art for the symmetric tensor PCA experimentally. The paper presents a pair of interesting algorithms using trace invariants to detect the signal in the signal-plus-noise tensor PCA framework. Page 8: "eg" should be "e.g." Summary: The paper provides an interesting algorithm for tensor PCA, which is based on trace invariants. The authors claim that they propose a new framework to solve the problem, by looking at the trace invariants of tensors. I am not able to follow the proofs in this paper due to missing definitions of terms and notations. At the current stage, filled with undefined or inconsistent notations and terms, this paper is not self-contained and hard to follow. It is claimed in the paper that the algorithm improves the state-of-the-art (signal to noise ratio requirements) in several cases, while a brief survey/table of the recent results is missing. This becomes worse considering the fact that this paper studies tensor problems -- many tensor-related terms have multiple definitions (e.g., eigenvalues, ranks). It will be very hard to follow the proofs if the definitions are unclear.
This paper introduces the problem of enforcing group-based fairness for "invisible demographics," which they define to be demographic categories that are not present in the training dataset. This paper tackles a fair classification problem with an invisible demographic, a situation where the records who have some specific target labels and sensitive attributes are missing. The paper states an interesting and practically relevant problem of enforcing fairness with "invisible demographics." The methodology is overall well documented, and the experimental baselines make sense. #Pros Zero-shot fairness is a very important topic under many practical settings, where the demographic information can be (partially) missing due to sampling bias or privacy reasons. #Over recommendation I think this paper studies a very interesting problem but some further analysis, e.g., how the distribution over the context data affects the results, and how to make the algorithm work reliably better in practice, is needed. #Minor comments and questions In the experiments, for colored-MNIST, a comparable portion for each quadrant is retained for the context dataset, have you tried different retained portions and how does that affect clustering quality? In the case where a dataset has zero labeled examples for some demographic groups, this is such an extreme situation that it is a clear red flag that there is a large bias in the collection process and/or the data collection design was poorly done---what is the justification for continuing to use this dataset as is? Avrim Blum∗, Kevin Stangl† I'm concerned that the paper calls the missing demographic groups "the invisibles" and then proceeds to still champion the use of the clearly flawed dataset. My recommendation is rejection. The main reason is that I have concerns about suspicious behavior in the experimental results.
The extensive experiments demonstrate the effectiveness of such methods in neural architecture search (NAS), image generation, adversarial training, and style transfer. the contribution. If I understand correctly, the only difference between previous work (Eqn 2) and the proposed method (Eqn 3) is the additional parameter gamma-sa/beta-sa. (2)What is the difference between the searched architectures by using BN, CCBN and SaBN? Could the authors give some analysis of the impact on the searched architecture when using SaBN? (2) I think it will be better to add the SaBN in Fig3, and point out the correspondence between the candidate operations and the conditional affine layer.
This paper proposes an extension of the RL as Inference framework, and demonstrates how to use it to express an object-centric RL model and train it on simple environments. (2020). Sophisticated Inference. arXiv preprint arXiv:2006.04120. This paper features two complementary contributions: The perception and control as inference (PCI) framework, which describes the graphical model of a POMDP with auxiliary optimality variables, allowing for the derivation of an objective for optimizing both a perception model and policy jointly to maximize log⁡p(O,x∣a). In my opinion, it should be stated more clearly in the paper that the main contribution is the application of the object-based perception model within the control-as-inference framework, and not an introduction of a novel framework. Overall: I consider this paper interesting and agree with the authors that the proposal of joint models is appealing and worthwhile investigating, but I would require much stronger evaluation to show that the joint aspect of OPC is driving the performance here, instead of just the object-based featurization. Comments/questions: I do not feel like the way the abstract and introduction argue for a general framework "Joint Perception and Control as Inference" is productive. Incorporating a perception model into the the control-as-inference derivation to yield a single unified objective for multiple components in a pipeline is an interesting idea and complements other work on incorporating reward structure into model-learning nicely. As mentioned during the introduction, this isn't a particularly novel framework (considering RL as inference, all the work done in temporal generative model for RL, or the rather new formulation by Hafner 2020 [4]) and it doesn't appear like it provides a very interesting fit to the object-centric RL model proposed. The overall idea of the model is extremely close to what has been done in OP3 [3], where they combine sequential IODINE with policy heads to solve tasks while simultaneously learning an iterative inference generative model. Summary This generalization of the control as inference formulation could be influential given a more didactic presentation of the PCI derivation and an evaluation better suited to its claimed strengths, but it does not seem quite ready for publication yet.
This conditional network approach is illustrated for two standard convolutional neural network (CNN) architectures, U-Net and VGG, on two benchmark datasets suitable for OOD detection, the Inria Aerial Image Labeling Dataset and the Tumor Infiltrating Lymphocytes classification dataset. The paper demonstrates the experiments on two tasks (i.e. semantic segmentation, image classification), but the proposed conditional network is different and does not have a unified architecture. In the paper by Huang et al. on AMLL U-Net higher IoU scores and accuracies on the transfer (test) set are reported (table 2, page 5 in their paper) compared to table 1, which in fact exceed the performance results of the proposed method. However, there are some concerns and questions outlined above which I believe need to be addressed / adapted in order to accept this paper.
