2018-12	This paper presents impressive results on scaling GANs to ILSVRC2012 dataset containing a large number of classes.	abstract
2018-12	To achieve this, the authors propose "spectral normalization" to normalize weights and stabilize training which turns out to help in overcoming mode collapse issues.	abstract
2018-12	The presented methodology is principled and well written.	strength
2018-12	The authors did a good job in addressing reviewer's comments and added more comparative results on related approaches to demonstrate the superiority of the proposed methodology.	rebuttal_process
2018-12	The reviewers agree that this is a great step towards improving the training of GANs.	strength
2018-12	I recommend acceptance.	decision

2018-32	This paper aims to improve on the intrinsically motivated goal exploration framework by additionally incorporating representation learning for the space of goals.	abstract
2018-32	The paper is well motivated and follows a significant direction of research, as agreed by all reviewers.	strength
2018-32	In particular, it provides a means for learning in complex environments, where manually designed goal spaces would not be available in practice.	strength
2018-32	There had been significant concerns over the presentation of the paper, but the authors put great effort in improving the manuscript according to the reviewers' suggestions, raising the average rating by 2 points after the rebuttal.	rebuttal_process

2018-78	Though the approach is not terribly novel, it is quite effective (as confirmed on a wide range of evaluation tasks).	strength
2018-78	The approach is simple and likely to be useful in applications.	strength
2018-78	The paper is well written. <sep>	strength
2018-78	+ simple and efficient <sep>	strength
2018-78	+ high quality evaluation <sep>	strength
2018-78	+ strong results <sep>	strength
2018-78	- novelty is somewhat limited	weakness

2018-79	An interesting application of graph neural networks to robotics.	strength
2018-79	The body of a robot is represented as a graph, and the agent's policy is defined using a graph neural network (GNNs/GCNs) over the graph structure. <sep>	abstract
2018-79	The GNN-based policy network perform on par with best methods on traditional benchmarks, but shown to be very effective for transfer scenarios: changing robot size or disabling its components.	abstract
2018-79	I believe that the reviewers' concern that the original experiments focused solely on centepedes and snakes were (at least partially) addressed in the author response: they showed that their GNN-based model outperforms MLPs on a dataset of 2D walkers. <sep>	rebuttal_process
2018-79	Overall: <sep> -- an interesting application <sep>	strength
2018-79	-- modeling robot morphology is an under-explored direction <sep>	strength
2018-79	-- the paper is  well written <sep>	strength
2018-79	-- experiments are sufficiently convincing (esp.	strength
2018-79	after addressing the concerns re diversity and robustness).	strength

2018-85	The submission modifies the SPEN framework for structured prediction by adding an inference network in place of the usual combinatorial optimization based inference.	abstract
2018-85	The resulting architecture has some similarity to a GAN, and significantly increases the speed of inference. <sep>	abstract
2018-85	The submission provides links between two seemingly different frameworks: SPENs and GANs.	abstract
2018-85	By replacing inference with a network output, the connection is made, but importantly, this massively speeds up inference and may mark an important step forward in structured prediction with deep learning.	strength

2018-88	Thank you for submitting you paper to *CONF*.	misc
2018-88	The paper presents a general approach for handling inference in probabilistic graphical models that employ deep neural networks.	abstract
2018-88	The framework extends Jonhson et al (2016) and Khan & Lin (2017).	abstract
2018-88	The reviewers are all in agreement that the paper is suitable for publication.	rating_summary
2018-88	The paper is well written and the use of examples to illustrate the applicability of the methods brings great clarity.	strength
2018-88	The experiments are not the strongest suit of the paper and, although the revision has improved this aspect, I would encourage a more comprehensive evaluation of the proposed methods.	rebuttal_process
2018-88	Nevertheless, this is a strong paper.	strength

2018-89	Thank you for submitting you paper to *CONF*.	misc
2018-89	The reviewers agree that the paper's development of action-dependent baselines for reducing variance in policy gradient is a strong contribution and that the use of Stein's identity to provide a principled way to think about control variates is sensible.	strength
2018-89	The revision clarified an number of the reviewers' questions and the resulting paper is suitable for publication in *CONF*.	decision

2018-111	I recommend acceptance based on the reviews.	decision
2018-111	The paper makes novel contributions to learning one-hidden layer neural networks and designing new objective function with no bad local optima. <sep>	strength
2018-111	There is one point that the paper is missing.	weakness
2018-111	It only mentions Janzamin et al in the passing.	weakness
2018-111	Janzamin et al propose using score function framework for designing alternative objective function.	weakness
2018-111	For the case of Gaussian input that this paper considers, the score function reduces to Hermite polynomials.	weakness
2018-111	Lack of discussion about this connection is weird.	weakness
2018-111	There should be proper acknowledgement of prior work.	weakness
2018-111	Also missing are some of the key papers on tensor decomposition and its analysis <sep>	weakness
2018-111	I think there are enough contributions in the paper for acceptance irrespective of the above aspect.	decision

2018-117	The reviewers all agree this is a well written and interesting paper describing a novel black box adversarial attack.	strength
2018-117	There were missing relevant references in the original submission, but these have been added.	rebuttal_process
2018-117	I would suggest the authors follow the reviewer suggestions on claims of generality beyond CNN; although there may not be anything obvious stopping this method from working more generally, it hasn't been tested in this work.	suggestion
2018-117	Even if you keep the title you might be more careful to frame the body in the context of CNN's.	suggestion

2018-129	With scores of 7-7-6  and the justification below the AC recommends acceptance. <sep>	decision
2018-129	One of the reviewers summarizes why this is a good paper as follows: <sep> "This paper discusses several gradient based attribution methods, which have been popular for the fast computation of saliency maps for interpreting deep neural networks.	strength
2018-129	The paper provides several advances: <sep> - This gives a more unified way of understanding, and implementing the methods. <sep>	strength
2018-129	- The paper points out situations when the methods are equivalent <sep>	strength
2018-129	- The paper analyses the methods' sensitivity to identifying single and joint regions of sensitivity <sep>	strength
2018-129	- The paper proposes a new objective function to measure joint sensitivity"	strength

2018-143	This paper proposes a new metric to evaluate the robustness of neural networks to adversarial attacks.	abstract
2018-143	This metric comes with theoretical guarantees and can be efficiently computed on large-scale neural networks. <sep>	abstract
2018-143	Reviewers were generally positive about the strengths of the paper, especially after major revisions during the rebuttal process.	rebuttal_process
2018-143	The AC believes this paper will contribute to the growing body of literature in robust training of neural networks.	strength

2018-156	The paper studies the use of PixelCNN density models for the detection of adversarial images, which tend to lie in low-probability parts of image space.	abstract
2018-156	The work is novel, relevant to the *CONF* community, and appears to be technically sound. <sep>	strength
2018-156	A downside of the paper is its limited empirical evaluation: there evidence suggesting that defenses against adversarial examples that work well on MNIST/CIFAR do not necessarily transfer well to much higher-dimensional datasets, for instance, ImageNet.	weakness
2018-156	The paper could, therefore, would benefit from empirical evaluations of the defense on a dataset like ImageNet.	suggestion

2018-161	All the reviews like the theoretical result presented in the paper which relates the gating mechanism of LSTMS (and GRUs) to time invariance / warping.	strength
2018-161	The theoretical result is great and is used to propose a heuristic for setting biases when time invariance scales are known.	strength
2018-161	The experiments are not mind-boggling, but none of the reviewers seem to think that's a show stopper.	weakness

2018-191	Well motivated and well written, with extensive results.	strength
2018-191	The paper also received positive comments from all reviewers.	rating_summary
2018-191	The AC recommends that the paper be accepted.	decision

2018-205	This clearly written paper describes a simple extension to hard monotonic attention -- the addition of a soft attention mechanism that operates over a fixed length window of inputs that ends at the point selected by the hard attention mechanism.	abstract
2018-205	Experiments on speech recognition (WSJ) and on a document summarization task demonstrate that the new attention mechanism improves significantly over the hard monotonic mechanism.	abstract
2018-205	About the only "con" the reviewers noted is that the paper is a minor extension over Raffel et al, 2017, but the authors successfully argue that the strong empirical results render this simplicity a "pro."	rebuttal_process

2018-207	Pros: <sep> + Nice demonstration of the equivalence between scaling the learning rate and increasing the batch size in SGD optimization. <sep>	strength
2018-207	Cons: <sep> - While reporting convergence as a function of number of parameter updates is consistent, the paper would be more compelling if wall-clock times were given in some cases, as that will help to illustrate the utility of the approach. <sep>	weakness
2018-207	- The paper would be stronger if additional experimental results, which the authors appear to have at hand (based on their comments in the discussion) were included as supplemental material. <sep>	weakness
2018-207	- The results are not all that surprising in light of other recent papers on the subject.	weakness

2018-220	This paper presents a simple yet effective method for weight dropping for an LSTM that requires no modification of an RNN cell's formulation.	abstract
2018-220	Experimental results shows good perplexity results on benchmarks compared to many baselines.	abstract
2018-220	All reviewers agree that the paper will bring good contribution to the conference.	strength

2018-228	I fully agree with strong positive statements in the reviews.	misc
2018-228	All reviewers agree that the paper introduces a novel and elegant twist on standard RL, wherein one agent proposes a sequence of diverse tasks to a second agent so as to accelerate the second agent's learning models of the environment.	strength
2018-228	I also concur that the empirical testing of this method is quite good.	strength
2018-228	There are strong and/or promising results in five different domains (Hallway, LightKey, MountainCar, Swimmer Gather and TrainingMarines in StartCraft).	strength
2018-228	This paper would make for a strong poster at *CONF*.	decision

2018-275	Observing that in contrast to classical information bottleneck, the deep variational information bottleneck (DVIB) model is not invariant to monotonic transformations of input and output marginals, the authors show how to incorporate this invariance along with sparsity in DVIB using the copula transform.	abstract
2018-275	The revised version of the paper addressed some of the reviewer concerns about clarity as well as the strength of the experimental section, but the authors are encouraged to improve these aspects of the paper further.	rebuttal_process

2018-308	All reviewers agreed that, despite the lack of novelty, the proposed method is sound and correctly linked to existing work.	strength
2018-308	As the topic of automatically learning the stepsize is of great practical interest, I am glad to have this paper presented as a poster at *CONF*.	decision

2018-309	Dear authors, <sep>	misc
2018-309	Based on the comments and your rebuttal, I am glad to accept your paper at *CONF*.	decision

2018-318	This is a meta-learning approach to model compression which trains 2 policies using RL to reduce the capacity (computational cost) of a trained network while maintaining performance, such that it can be effectively transferred to a smaller student network.	abstract
2018-318	The approach has similarities to recently proposed methods for architecture search, but is significantly different.	abstract
2018-318	The paper is well written and the experiments are clear and convincing.	strength
2018-318	One of the reviews was unacceptable; I am not considering it (R1).	ac_disagreement

2018-323	This paper proposes an interesting new idea which creates an interesting discussion.	strength

2018-335	This paper provides a comparison of different types of a memory augmented models and extends some of them to beyond their simple form.	abstract
2018-335	Reviewers found the paper to be clearly written, saying it "nice introduction to the topic" and noting that they "enjoyed reading this paper".	strength
2018-335	In general though there was a feeling that the "substance of the work is limited".	weakness
2018-335	One reviewer complained that experiments were limited to small English datasets PTB and Wikitext-2 and asked why they didn't try "machine translation or speech recognition".	weakness
2018-335	(The author's note that they did try the Linzen dataset, and while the reviewers found the experiments impressive, the task itself felt artificial) .	rebuttal_process
2018-335	Another felt that the "multipop model" alone was not too large a contribution.	weakness
2018-335	The actual experiments in the work are well done, although given the fact that the models are known there was expectation of "more "in-depth" analysis of the different models".	weakness
2018-335	Overall this is a good empirical study, which shows the limited gains achieved by these models, a nevertheless useful piece of information for those working in this area.	strength

2018-346	R1 thought the proposed method was novel and the idea interesting.	strength
2018-346	However, he/she raised concerns with consistency in the experimental validation, the trade-off between accuracy and running time, and the positioning/motivation, specifically the claim about interpretability.	weakness
2018-346	The authors responded to these concerns, and R1 upgraded their score.	rebuttal_process
2018-346	R2 didn't raise major concerns or strengths.	misc
2018-346	R3 questioned the novelty of the work and the experimental validations.	weakness
2018-346	All reviewers raised concerns with the writing.	weakness
2018-346	Though I think the work is interesting, issues raised about experiments and writing make me hesitant to go against the overall recommendation of the reviewers, which is just below the bar.	rating_summary
2018-346	I think this is a paper that could make a good workshop contribution.	decision

2018-362	This paper studies an important problem (visual relationship detection and generalization capabilities existing networks for this task).	abstract
2018-362	Unfortunately, all reviewers raise concerns (eg limited relations studied) and are largely on the fence about this paper.	rating_summary
2018-362	While this paper does not propose solutions, it does present interesting "negative results" that should get some visibility in the workshop track.	decision

2018-382	This work proposes an improved visualisation techniques for ReLU networks that compensates for filter scale symmetries/invariances, thus allowing a more meaningful comparison of low-dimensional projected optimization landscapes between different network architectures. <sep>	abstract
2018-382	- the visualisation techniques are a small variation over previous works <sep>	weakness
2018-382	+ extensive experiments provide nice visualisations and yield a clearer visual picture of some properties of the optimization landscape of various architectural variants <sep>	strength
2018-382	A promising research direction, which could be further improved by providing more extensive and convincing support for the significance of its contribution in comparison to prior techniques, and to its empirically derived observations, findings and claims.	suggestion

2018-385	The paper sets out to analyze the problem of exploding gradients in deep nets which is of fundamental importance.	abstract
2018-385	Reviewers largely acknowledge the novelty of the main ideas in the paper towards this goal, however it is also strongly felt that the writing/presentation of the paper needs significant improvement to make it into a coherent and clean story before it can be published.	weakness
2018-385	There are also some concerns on networks used in the experiments not being close to practice. <sep>	weakness
2018-385	I recommend invitation to the workshop track as it has novel ideas and will likely generate interesting discussion.	decision

2018-391	The paper based on cGAN developed a data augmentation GAN to deal with unseen classes of data.	abstract
2018-391	The paper developed new modifications to each component and designed network structure using ideas from state-of-the-art nets.	abstract
2018-391	As pointed out by reviewer 1 & 2, the technical contribution is not sufficient.	weakness
2018-391	We hence recommend it to workshop publication.	decision

2018-419	The reviewers think that the theoretical contribution is not significant on its own.	weakness
2018-419	The reviewers find the empirical aspect of the paper interesting, but more analysis of the empirical behavior is required, especially for large datasets.	weakness
2018-419	Even for small datasets with input augmentation (eg random crops in CIFAR-10) the pre-processing can become prohibitive.	weakness
2018-419	I recommend improving the manuscript for a re-submission to another venue and an *CONF* workshop presentation.	decision

2018-420	Dear authors, <sep>	misc
2018-420	While I appreciate the result that a convolutional layer can have full rank output, this allowing a dataset to be classified perfectly under mild conditions, the fact that all reviewers expressed concern about the statement is an indication that the presentation sill needs quite a bit of work. <sep>	weakness
2018-420	Thus, I recommend it as an *CONF* workshop paper.	decision

2018-423	The authors addressed the reviewers concerns but the scores remain somewhat low. <sep>	rebuttal_process
2018-423	The method is not super novel, but it is an incremental improvement over existing approaches.	strength

2018-425	The authors present an environment for semantic navigation that is based on an existing dataset, SUNCG.	abstract
2018-425	Datasets/environments are important for deep RL research, and the contribution of this paper is welcome.	strength
2018-425	However, this paper does not offer enough novelty in terms of approach/method and its claims are somewhat misleading, so it would probably be a better fit to publish it at a workshop.	decision

2018-426	Pros: <sep> - The paper proposes to use a hierarchical structure to address reconstruction issues with ALI model. <sep>	strength
2018-426	- Obtaining multiple latent representations that individually achieve a different level of reconstructions is interesting. <sep>	strength
2018-426	- Paper is well written and the authors made a reasonable attempt to improve the paper during the rebuttal period. <sep>	strength
2018-426	Cons: <sep> - Reviewers agree that the approach lacks novelty as similar hierarchical approaches have been proposed before. <sep>	weakness
2018-426	- The main goal of the paper to achieve better reconstruction in comparison to ALI without changing the latter's objective seems narrow.	weakness
2018-426	More analysis is needed to demonstrate that the approach out-performs other approaches that directly tackle this problem in ALI. <sep>	weakness
2018-426	- The paper does not provide strong arguments as to why hierarchy works (limited to 2 levels in the empirical analysis presented in the paper). <sep>	weakness
2018-426	- Semi-supervised learning as a down-stream task is impressive but limited to MNSIT.	weakness

2018-439	This paper proposes a way to automatically weight different tasks in a multi-task setting.	abstract
2018-439	The problem is a bit niche, and the paper had a lot of problems with clarity, as well as the motivation for the experimental setup and evaluation.	weakness

2018-445	Pros and cons of the paper can be summarized as follows: <sep>	misc
2018-445	Pros: <sep> * The underlying idea may be interesting <sep>	strength
2018-445	* Results are reasonably strong on the test set used <sep>	strength
2018-445	Cons: <sep> * Testing on the single dataset indicates that the model may be of limited applicability <sep>	weakness
2018-445	* As noted by reviewer 2, core parts of the paper are extremely difficult to understand, and the author response did little to assuage these concerns <sep>	weakness
2018-445	* There is little mathematical notation, which compounds the problems of clarity <sep>	weakness
2018-445	After reading the method section of the paper, I agree with reviewer 2: there are serious clarity issues here.	weakness
2018-445	As a result, I do cannot recommend that this paper be accepted to *CONF* in its current form.	decision
2018-445	I would suggest the authors define their method precisely in mathematical notation in a future submission.	suggestion

2018-446	The pros and cons of the paper can be summarized below: <sep>	misc
2018-446	Pro: <sep> * The improvements afforded by the method are significant over baselines, although these baselines are very preliminary baselines on a new dataset. <sep>	strength
2018-446	Con <sep> * There is already a significant amount of work in using grammars to guide semantic parsing or code generation, as rightfully noted by the authors, and thus the approach in the paper is not extremely novel. <sep>	weakness
2018-446	* Because there is no empirical comparison with these methods, the relative utility of the proposed method is not clear. <sep>	weakness
2018-446	As a result, I recommend that the paper not be accepted at this time.	decision

2018-485	The proposed relational reasoning algorithm is basically a fairly standard graph neural network, with a few modifications (eg, the prediction loss at each layer - also not a new idea per se). <sep>	abstract
2018-485	The claim that previously reasoning has not been considered in previous applications of graph neural networks (see discussion) is questionable.	weakness
2018-485	It is not even clear what is meant here by 'reasoning' as many applications of graph neural networks may be regarded as performing some kind of inference on graphs (eg, matrix completion tasks by Berg, Kipf and Welling; statistical relational learning by  Schlichtkrull et al). <sep>	weakness
2018-485	So the contribution seems a bit over-stated.	weakness
2018-485	Rather than introduces a new model, the work basically proposes an application of largely known model to two (not-so-hard) tasks which have not been studied in the context of GNNs.	weakness
2018-485	The claim that the approach is a general framework for dealing with complex reasoning problems is not well supported as both problems are (arguably) not complex reasoning problems (see R2). <sep>	weakness
2018-485	There is a general consensus between reviewers that the paper, in its current form, does not quite meet acceptance criteria. <sep>	rating_summary
2018-485	Pros: <sep> -- an interesting direction <sep>	strength
2018-485	-- clarity <sep>	strength
2018-485	Cons: <sep> -- the claim of generality is not well supported <sep>	weakness
2018-485	-- the approach is not so novel <sep>	weakness
2018-485	-- the approach should be better grounded in previous work	weakness

2018-487	The proposed neural tree transduction framework is basically a combination of tree encoding and tree decoding.	abstract
2018-487	The tree encoding component is simply reused from previous work (TreeLSTM) whereas the decoding components is somewhat different from the previous work.	abstract
2018-487	They key problems (acknowledge also by at least 2 reviewers): <sep>	misc
2018-487	Pros: <sep> -- generating trees input under-explored direction (note that it is more general than parsing as nodes may not directly correspond to input symbols) <sep>	strength
2018-487	Cons: <sep> -- no comparison with previous tree-decoding work <sep>	weakness
2018-487	-- only artificial experiments <sep>	weakness
2018-487	-- the paper is hard too read (confusing) / mathematical notation and terminology is confusing and seems sometimes inaccurate (see R3)	weakness

2018-492	The reviewers tend to agree that the empirical results in this paper are good compared to the baselines.	strength
2018-492	However, the paper in its current form is considered a bit too incremental.	weakness
2018-492	Some reviewers also suggested additional theory could help strengthen the paper.	suggestion

2018-498	The reviewers agree that the problem being addressed is interesting, however there are concerns with novelty and with the experimental results.	weakness
2018-498	An experiment beyond dealing with class imbalance would help strengthen this paper, as would experiments with other kinds of GANs.	suggestion

2018-510	Thank you for submitting your paper to *CONF*.	misc
2018-510	The reviewers agree that the idea of sharing the approximating distribution across sets of variables is an interesting one and that the Omniglot experiments are thorough.	strength
2018-510	However, although the authors make the nice addition of some simple examples during the revision period and a new table of quantitative results on Omniglot, the consensus is that the experimental results are not quite persuasive enough for publication.	rating_summary
2018-510	Adding a second dataset, such as mini-imagenet or the youtube faces dataset, would make the paper very strong.	suggestion

2018-520	Thank you for submitting you paper to *CONF*.	misc
2018-520	*CONF*.	misc
2018-520	Although there revision has improved the paper, the consensus from the reviewers is that this is not quite ready for publication.	decision

2018-523	This paper presents a method for learning more flexible prior distributions for GANs by learning another distribution on top of the latent codes for training examples.	abstract
2018-523	It's reminiscent of layerwise training of deep generative models.	abstract
2018-523	This seems like a reasonable thing to do, but it's probably not a substantial enough contribution given that similar things have been done for various other generative models.	weakness
2018-523	Experiments show improvement in samples compared with a regular GAN, but don't compare against various other techniques that have been proposed for fixing mode dropping.	weakness
2018-523	For these reasons, as well as various issues pointed out by the reviewers, I don't recommend acceptance.	decision

2018-563	The reviewers feel that the novelties in the model are not significant.	weakness
2018-563	Furthermore, they suggest that empirical results could be improved by <sep> 1: analyses showing how the significance network functions and directly measuring its impact <sep>	suggestion
2018-563	2: More reproducible experiments.	suggestion
2018-563	In particular, this is really an applications paper, and the experiments on the main application are not reproducible because the data is proprietary. <sep>	weakness
2018-563	3: baselines that make assumptions more in line with the authors' problem setup	suggestion

2018-610	The paper appears unfinished in many ways: the experiments are preliminary, the paper completely ignored a large body of prior work on the subject, and the presentation needs substantial improvements.	weakness
2018-610	The authors did not provide a rebuttal. <sep>	rebuttal_process
2018-610	I encourage the authors to refrain from submitting unfinished papers such as this one in the future, as it unnecessarily increases the load on a review system that is already strained.	decision

2018-617	The paper identifies an interesting problem in sigmoid deep nets, addressed diffferently by batchnorm, and proposes a different simple fix.	abstract
2018-617	It shows empirically that constraining neuron's weights to sum to zero improves training of a 100 layers sigmoid MLP. <sep>	abstract
2018-617	The work is currenlty limited in its theoretical contribution, and regarding the showcased practical interest of the method compared to batchnorm (it's not appplicable to RELUs and shows positive effect on optimization but not generalization).	weakness

2018-624	The paper received mixed reviews with scores of 5 (R1), 5 (R2),  7 (R3).	rating_summary
2018-624	All three reviewers raise concerns about the lack of comparisons to other methods.	weakness
2018-624	The rebuttal is not compelling on this point.	rebuttal_process
2018-624	There are quite a few methods that could be used for this application available (often with source code) and should be compared to, eg DenseNets (Huang et al).	weakness
2018-624	Given that the proposed method isn't in of itself hugely novel, a thorough experimental evaluation is crucial to the justifying the approach.	weakness
2018-624	The AC has closely looked at the rebuttal and the paper and feels that it cannot be accepted for this reason at this time.	decision

2018-637	Two reviewers recommended rejection, and one is slightly more positive.	rating_summary
2018-637	The main concern is that the experiments are not convincing (ie, the number of base and added classes is very small).	weakness
2018-637	Furthermore, while the paper introduces several interesting ideas, the AC agrees with the second reviewer that each of these could be explored in more detail.	weakness
2018-637	This work seems preliminary.	weakness
2018-637	The authors are encouraged to resubmit to a future conference.	decision

2018-638	Three reviewers recommended rejection and there was no rebuttal to overturn their recommendation.	rating_summary

2018-639	The reviewers agree that the idea of incorporating humans in the training of generative adversarial networks is interesting and worthwhile exploring.	strength
2018-639	However, they felt that the paper fell short in providing strong support for their approach.	weakness
2018-639	The AC agrees.	misc
2018-639	The authors are encouraged to strengthen their work and resubmit to a future venue.	decision

2018-644	This work takes dialogue acts into account to generate responses in a human-machine conversation.	abstract
2018-644	However, incorporating dialogue acts into open-domain dialogue was already the focus of Zhao et al's ACL 2017 paper, Learning Discourse-level Diversity for Neural Dialog Models using Conditional Variational Autoencoders, and using dialogue acts in a policy for human-machine conversation was also an idea that already appeared in Serban et al 2017, A Deep Reinforcement Learning Chatbot.	weakness
2018-644	Despite the authors' response that tries to adjust their claims and incorporate a more thorough overview, I encourage the authors to re-work their research with a much more careful and reliable examination of previous work and how their effort should be understood in that more comprehensive context.	rebuttal_process

2018-650	This work attempts to incorporate affect information from additional resources into word embeddings.	abstract
2018-650	This is a valuable goal, but the methods used are very similar to existing ones, and the experimental results are not quite convincing enough to make a strong enough case for accepting the paper.	decision

2018-685	Pros: <sep> + The paper is very clearly written. <sep>	strength
2018-685	+ The proposed re-embedding approach is easily implemented and can be integrated into fancier architectures. <sep>	strength
2018-685	Cons: <sep> - A lot of the gains reported come from lemmatization, and the gains from background knowledge become marginal when used on a stronger baseline (eg, ESIM with full training data and full word vectors). <sep>	weakness
2018-685	This paper is rather close to the decision boundary.	rating_summary
2018-685	The authors had reasonable answers for some of the reviewers' concerns, but in the end the reviewers were not completely convinced.	rebuttal_process

2018-691	The reviewers generally thought the proposed algorithm was a straightforward extension of Yin et al, 2017, and not enough for a new paper.	weakness
2018-691	They also objected to a lack of test results (to show generalization), but the authors did provide these in their revision. <sep>	rebuttal_process
2018-691	Pros: <sep> + Adaptive batch sizing is useful, especially if the larger batches license parallelization. <sep>	strength
2018-691	Cons: <sep> - Small, incremental change to the algorithm from Yin et al, 2017 <sep>	weakness
2018-691	- Test performance did not improve over well-tuned momentum optimization, which limits the appeal of the method.	weakness

2018-696	The author's revisions addressed clarity issues and some experimental issues (eg, including MAML results in the comparison).	rebuttal_process
2018-696	The work takes an original path to an important problem (transfer learning, essentially).	abstract
2018-696	There is a question of significance, and this is due to the fact that the empirical comparisons are still very limited.	weakness
2018-696	The task is an artificial one derived from MNIST.	abstract
2018-696	I would call this "toy" as well.	misc
2018-696	On this toy task, the approach isn't that much different from MAML, which is not in of itself a problem, but it would be interested to have a less superficial discussion of the differences. <sep>	suggestion
2018-696	The authors mention that they didn't have time for a larger empirical study.	rebuttal_process
2018-696	I think one is necessary in this case because the work is purposing a new learning algorithm/framework, and the question of its potential impact/significance is an empirical one.	rebuttal_process

2018-705	The paper presents an adversarial learning framework for reading comprehension.	abstract
2018-705	Although the idea is interesting and presents an approach that ideally would make reading comprehension approaches more robust, the results are not substantially solid (see reviewer 3's comments) compared to other baselines to warrant acceptance.	weakness
2018-705	Comments from reviewer 2 are also noteworthy where they mention that adversarial perturbations to a context around an answer can alter the facts in the context, thus destroying the actual information present there, and the rebuttal does not seem to satisfy the concern.	rebuttal_process
2018-705	Addressing these issues will strengthen the paper for a potential future venue.	decision

2018-707	This paper tries to establish that LSTMs are suitable for modeling neural signals from the brain.	abstract
2018-707	However, the committee and most reviewers find that results are inconclusive.	weakness
2018-707	Results are mixed across subjects.	weakness
2018-707	We think it would have been far more interesting to compare other types of sequence models for this task other than the few simple baselines implemented here.	suggestion
2018-707	It is also unclear what is the LSTM learning extra in contrast with the other models presented in the paper.	weakness

2018-710	None of the reviewers are enthusiastic about the paper, primarily due to lack of proper evaluation.	weakness
2018-710	The response of the authors towards this criticism is also not sufficient.	rebuttal_process
2018-710	The final results are mixed which does not show very clearly that the presented associative model performs better than the sole seq2seq baseline that the authors use for comparison.	weakness
2018-710	We think that addressing these immediate concerns would improve the quality of this paper.	suggestion

2018-711	The reviewers generally agree that the DDRprog method is both novel and interesting, while also seeing merit in outperformance of related methods in the empirical results.	strength
2018-711	However, There were a lot of complaints about the writing quality, the clarity of the exposition, and unclear motivation of some of the work.	weakness
2018-711	The reviewers also noted insufficient comparisons and discussions regarding relevant prior art, including recursive NNs, Tree RNNs, IEP, etc.	weakness
2018-711	While the authors have made substantial revisions to the manuscript, with several additional pages of exposition, reviewers have not raised their scores or confidence in response.	rebuttal_process

2018-725	This paper proposes an approach for learning a sparsifying transform via a set of nonlinear transforms at learning time.	abstract
2018-725	The presentation needs a lot of work.	weakness
2018-725	The original paper was 17 pages long and very difficult to understand.	weakness
2018-725	The revised paper is 12 pages long, which is still too long for the content.	weakness
2018-725	The paper needs to better distinguish between the major and minor points.	weakness
2018-725	It is still too difficult to judge the contribution.	weakness

2018-729	This paper has been withdrawn by the authors.	misc

2018-748	The paper aims to address the mode collapse issue in GANs by training multiple generators and forcing them to be diverse. <sep>	abstract
2018-748	Reviewers agree that the proposed solution is not novel and has disadvantages such as increased parameters due to multiple generator models.	weakness
2018-748	The authors do not provide convincing arguments as to why the proposed approach should work well.	weakness
2018-748	The experiments presented also fail to demonstrate this.	weakness
2018-748	The results are limited to smaller MNIST and CIFAR10 datasets.	weakness
2018-748	Comparisons with approaches that directly address the mode collapse problem are missing.	weakness

2018-752	The paper aims to combine Wasserstein GAN with Improved GAN framework for semi-supervised learning. <sep>	abstract
2018-752	The reviewers unanimously agree that: <sep> - the paper lacks novelty and such approaches have been tried before. <sep>	weakness
2018-752	- the approach does not make sufficient gains over the baselines and stronger baselines are missing. <sep>	weakness
2018-752	- the paper is not well written and experimental results are not satisfactory.	weakness

2018-762	The reviewers were largely agreed that the paper presented an interesting idea and has potential but needs a better empirical evaluation.	weakness
2018-762	It seems that the authors largely agree and are working to improve it. <sep>	rebuttal_process
2018-762	PROS: <sep> 1. Improving the speed of program synthesis is a useful problem <sep>	strength
2018-762	2. Good treatment of related work, eg CEGIS <sep>	strength
2018-762	CONS: <sep> 1. The approach likely does not scale <sep>	weakness
2018-762	2. The architecture is underspecified making it hard to reproduce <sep>	weakness
2018-762	3. Only 1 domain for evaluation	weakness

2018-775	The reviewers have unanimously expressed strong concerns about the technical correctness of the theoretical results in the paper.	weakness
2018-775	The paper should be carefully revised and checked for technical errors.	weakness
2018-775	In its current form, the paper is not suitable for acceptance at *CONF* 2018.	decision

2018-791	Reviews are marginal. <sep>	rating_summary
2018-791	I concur with the two less-favorable reviews that the metrics  for privacy protection are not sufficiently strong for preserving privacy.	weakness

2018-794	This paper describes active vision for object recognition learned in an RL framework. <sep>	abstract
2018-794	Reviewers think the paper is not of sufficient quality: Insufficient detail, and insufficient evaluation. <sep>	weakness
2018-794	While the authors have provided a lengthy rebuttal, the shortcomings have not yet been addressed in the paper.	rebuttal_process

2018-798	All 3 reviewers consider the paper insufficiently good, including a post-rebuttal updated score. <sep>	rating_summary
2018-798	All reviewers + anonymous comment find that the paper isn't well-enough situated with the appropriate literature. <sep>	weakness
2018-798	Two reviewers cite poor presentation - spelling /grammar errors making hte paper hard to read. <sep>	weakness
2018-798	Authors have revised the paper and promise further revisions for final version.	rebuttal_process

2018-810	The authors propose a hierarchical VAE model with a discrete latent variable in the top-most layer for unsupervised learning of discriminative representations.	abstract
2018-810	While the reported results on the two flow cytometry datasets are encouraging, they are insufficient to draw strong conclusions about the general effectiveness of the proposed architecture.	weakness
2018-810	Also, as two of the reviewers stated the proposed model is very similar to several VAE models in the literature.	weakness
2018-810	This paper seems better suited for a more applied venue than *CONF*.	decision

2018-813	The submission provides an interesting way to tackle the so-called distributional shift problem in machine learning.	abstract
2018-813	One familiar example is unsupervised domain adaptation.	abstract
2018-813	The main contribution of this work is deriving a bound on the generalization error/risk for a target domain as a combo of re-weighted empirical risk on the source domain and some discrepancy between the re-weighted source domain and the target domain.	abstract
2018-813	The authors then use this to formulate an objective function. <sep>	abstract
2018-813	The reviewers generally liked the paper for its theoretical results, but found the empirical evaluation somewhat lacking, as do I.	weakness
2018-813	Especially the unsupervised domain adaptation results are very toy-ish in nature (synthetic data), whereas the literature in this field, cited by the authors, does significantly larger scale experiments.	weakness
2018-813	I am unsure as to how much I value I can place in the IHDP results since I am not familiar with the benchmark (and hence my lower confidence in the recommendation). <sep>	misc
2018-813	Finally, I am not very convinced that this is the appropriate venue for this work, despite containing some interesting results.	decision

2018-816	In principle, the idea behind the submission is sound: use a generative model (GANs in this case) to learn to generate desirable "goals" (subsets of the state space) and use that instead of uniform sampling for goals.	abstract
2018-816	Overall I tend to agree with Reviewer 3 in that the current set of results is not convincing in terms of it being able to generate goals in a high-dimensional state space, which seems to be be whole raison d'etre of GANs in this proposed method.	weakness
2018-816	The coverage experiment in Figure 5 seems like a good *illustration* of the method, but for this work to be convincing, I think we would need a more diverse set of experiments  (a la Figure 2) showing how this method performs on complicated tasks. <sep>	suggestion
2018-816	I encourage the authors to sharpen the definitions, as suggested by reviewers, and, if possible, provide experiments where the Assumptions being made in Section 3.3 are *violated* somehow (to actually test how the method fails in those cases).	suggestion

2018-824	This paper proposes a tree-structured tensor factorisation method for parameter reduction.	abstract
2018-824	The reviewers felt the paper was somewhat interesting, but agreed that more detail was needed in the method description, and that the experiments were on the whole uninformative.	weakness
2018-824	This seems like a promising research direction which needs more empirical work, but is not ready for publication as is.	decision

2018-834	This paper combines multiple existing ideas in Bayesian optimization (continuous-fidelity, use of gradient information and knowledge gradient) to develop their proposed cfKG method.	abstract
2018-834	While the methodology seems neat and effective, the reviewers (and AC) found that the presented approach was not quite novel enough in light of existing work to justify acceptance to *CONF*.	weakness
2018-834	Continuous fidelity Bayesian optimization is well studied and knowledge gradient + derivative information was presented at NIPS.	strength
2018-834	The combination of these things seems quite sensible but not sufficiently novel (unless the empirical results were *really* compelling). <sep>	weakness
2018-834	Pros: <sep> - The paper is clear and writing is of high quality <sep>	strength
2018-834	- Bayesian optimization is interesting to the community and compelling methods are potentially practically impactful <sep>	strength
2018-834	- Outperforms existing methods on the chosen benchmarks <sep>	strength
2018-834	Cons: <sep> - Is an incremental combination of existing methods <sep>	weakness
2018-834	- The paper claims too much	weakness

2018-840	This paper proposes a regularizer for recurrent neural networks, based on injecting random noise into the hidden unit activations.	abstract
2018-840	In general the reviewers thought that the paper was well written and easy to understand.	strength
2018-840	However, the major concern among the reviewers was a lack of empirical evidence that the method works consistently.	weakness
2018-840	Essentially, the reviewers were not compelled by the presented experiments and demanded more rigorous empirical validation of the approach. <sep>	weakness
2018-840	Pros: <sep> - Well written and easy to follow <sep>	strength
2018-840	- An interesting idea <sep>	strength
2018-840	- Regularizing RNNs is an interesting and active area of research in the community <sep>	strength
2018-840	Cons: <sep> - The experiments are not compelling and are questioned by all the reviewers <sep>	weakness
2018-840	- The writing does not cite relevant related work <sep>	weakness
2018-840	- The work seems underexplored (empirically and methodologically)	weakness

2018-844	The paper performs an ablation analysis on LSTM, showing that the gating component is the most important.	abstract
2018-844	There is little novelty in the analysis, and in its current form, its impact is rather limited.	weakness

2018-852	The idea studied here is interesting, if incremental.	strength
2018-852	The empirical results are not particularly stellar, but it's clear that the authors have done their best to provide reproducible and defensible results.	weakness
2018-852	A few sticking points: a) The use of the term 'UCB', as mentioned in an anonymous comment, is somewhat misleading.	weakness
2018-852	"Approximate Confidence Interval" might be less controversial; b) there are a number of recent research results on exploration that are worth paying attention to (Plappert et al, O'Donoghue et al) and worth comparing to, and c) the theoretical results are not always justified or useful (eg Equation 9: the bound is trivial, posterior >= 0 or 1).	weakness

2018-857	The reviewers agree the proposed idea is relatively incremental, and the paper itself does not do an exemplary job in other areas to make up for this.	weakness

2018-882	Dear authors, <sep>	misc
2018-882	The reviewers agreed that the theoretical part lacked novelty and that the paper should focus on its experimental part which at the moment is not strong enough to warrant publication. <sep>	weakness
2018-882	Regarding the theoretical part, here are the main concerns: <sep> - Even though it is used in previous works, the continuous time approximation of stochastic gradient overlooks its practical behaviour, especially since a good rule of thumb is to use as large as stepsize as possible (without reaching divergence), as for instance mentioned in The Marginal Value of Adaptive Gradient Methods in Machine Learning by Wilson et al <sep>	weakness
2018-882	- The isotropic approximation is very strong and I don't know settings where this would hold.	weakness
2018-882	Since it seems central to your statements, I wonder what can be deduced from the obtained results. <sep>	weakness
2018-882	- I do not think the Gaussian assumption is unreasonable and I am fine with it.	ac_disagreement
2018-882	Though there are clearly cases where this will not be true, it will probably be OK most of the time. <sep>	weakness
2018-882	I encourage the authors to focus on the experimental part in a resubmission.	suggestion

2018-885	Dear authors, <sep>	misc
2018-885	Thank you for your submission to *CONF*.	misc
2018-885	Sadly, the reviewers were not convinced by the novelty of your approach nor by its experimental results.	weakness
2018-885	Thus, your paper cannot be accepted to *CONF*.	decision

2018-888	The authors present a toy stacking task where the goal is to stack blocks to match a given configuration, and a method that is a slightly modified DQN algorithm where the target configuration is observed by the network as well as the current state.	abstract
2018-888	There are a few problems with this paper.	misc
2018-888	First, the method lacks novelty - it is very similar to DQN.	weakness
2018-888	Second, the claims of learning physical intuitions is not borne out by the method or experimental results.	weakness
2018-888	Third, the tasks are very simple and there is no held-out test set of target configurations.	weakness

2018-904	The paper is interesting, and the update to the paper and additional experiments has already improved it in many ways, but the paper still does still not have as much impact as it could, by further strengthening the comparisons and usefulness in many of situations of current practice.	rebuttal_process

2018-909	The paper addresses the important question of determining the intrinsic dimensionality, but there remain several issue, which make the paper not ready at this point: unclear exposition, lack of contextualisation of existing work and seemingly limited insights.	decision
2018-909	The reviewers have provided many suggestions to improve the paper which we hope will be useful to improve the paper.	misc

2019-29	The paper presents some reasonable experiments and approaches for unsupervised time series.	abstract
2019-29	However as mentioned by R2 there is several issues.	misc
2019-29	The paper also overclaims  a bit some of the novelty.	weakness
2019-29	As noted by  R1 and the metareviewer there is other works using triplet loss for timeseries, a relatively common approach in temporal dataset (eg video, audio), the popular causal convolution structure from wavenet is also quite well known, contributions should be more clear.	weakness

2019-47	The paper has some relatively minor issue but an overall interesting concept	strength

2019-121	All reviewers recommend acceptance, with two reviewers in agreement that the results represent a significant advance for autoregressive generative models.	rating_summary
2019-121	The AC concurs.	decision

2019-126	Well-written paper that motivates through theoretical analysis new memory writing methods in memory augmented neural networks.	abstract
2019-126	Extensive experimental analysis support and demonstrate the advantages of the new solutions over other recurrent architectures. <sep>	abstract
2019-126	Reviewers suggested extension and clarification of the analysis presented in the paper, for example, for different memory sizes.	suggestion
2019-126	The paper was revised accordingly.	rebuttal_process
2019-126	Another important suggestion was considering ACT as a baseline.	suggestion
2019-126	Authors explained clearly why it wasn't considered as a baseline, and updated the paper to include references and explanations in the paper as well.	rebuttal_process

2019-134	The authors propose a method for low-resource domain adaptation where the number of examples available in the target domain are limited.	abstract
2019-134	The proposed method modifies the basic approach in a CycleGAN by augmenting it with a "content" (task-specific) loss, instead of the standard reconstruction error.	abstract
2019-134	The authors also demonstrate experimentally that it is important to enforce the loss in both directions (target → source and source --> target).	abstract
2019-134	Experiments are conducted on both supervised as well as unsupervised settings. <sep>	abstract
2019-134	The main concern expressed by the reviewers relates to the novelty of the approach since it is a relatively straightforward extension of CycleGAN/CyCADA, but in the view of a majority of reviewers the work serves a useful contribution as a practical method for developing systems in low-resource conditions where it is feasible to label a few new instances.	weakness
2019-134	Although the reviewers were not unanimous in their recommendations, on balance in the view of the AC the work is a useful contribution with clear and detailed experiments in the revised version.	strength

2019-135	The authors describe a very counterintuitive type of layer: one with mean zero Gaussian weights.	abstract
2019-135	They show that various Bayesian deep learning algorithms tend to converge to layers of this variety.	abstract
2019-135	This work represents a step forward in our understanding of bayesian deep learning methods and potentially may shine light on how to improve those methods.	strength

2019-136	The paper investigates a novel initialisation method to improve Equilibrium Propagation.	abstract
2019-136	In particular, the results are convincing, but the reviewers remain with small issues here and there. <sep>	strength
2019-136	An issue with the paper is the biological plausibility of the approach.	weakness
2019-136	Nonetheless publication is recommended.	decision

2019-146	Strengths: Strong results on future frame video prediction using a 3D convolutional network.	strength
2019-146	Use of future video prediction to jointly learn auxiliary tasks shown to to increase performance.	strength
2019-146	Good ablation study. <sep>	strength
2019-146	Weaknesses: Comparisons with older action recognition methods.	weakness
2019-146	Some concerns about novelty, the main contribution is the E3D-LSTM architecture, which R1 characterized as an LSTM with an extra gate and attention mechanism. <sep>	weakness
2019-146	Contention: Authors point to novelty in 3D convolutions inside the RNN. <sep>	rebuttal_process
2019-146	Consensus: All reviewers give a final score of 7- well done experiments helped address concerns around novelty.	rating_summary
2019-146	Easy to recommend acceptance given the agreement.	decision

2019-155	see my comment to the authors below	misc

2019-174	This paper investigates learning to represent edit operations for two domains: text and source code.	abstract
2019-174	The primary contributions of the paper are in the specific task formulation and the new dataset (for source code edits).	strength
2019-174	The technical novelty is relatively weak. <sep>	weakness
2019-174	Pros: <sep> The paper introduces a new dataset for source code edits. <sep>	strength
2019-174	Cons: <sep> Reviewers raised various concerns about human evaluation and many other experimental details, most of which the rebuttal have successfully addressed.	rebuttal_process
2019-174	As a result, R3 updated their score from 4 to 6. <sep>	rebuttal_process
2019-174	Verdict: <sep> Possible weak accept.	decision
2019-174	None of the remaining issues after the rebuttal is a serious deal breaker (eg, task simplification by assuming the knowledge of when and where the edit must be applied, simplifying the real-world application of the automatic edits).	rebuttal_process
2019-174	However, the overall impact and novelty of the paper is relatively weak.	weakness

2019-176	This paper proposes a method for unsupervised learning that uses a latent variable generative model for semi-supervised dependency parsing.	abstract
2019-176	The key learning method consists of making perturbations to the logits going into a parsing algorithm, to make it possible to sample within the variational auto-encoder framework.	abstract
2019-176	Significant gains are found through semi-supervised learning. <sep>	abstract
2019-176	The largest reviewer concern was that the baselines were potentially not strong enough, as significantly better numbers have been reported in previous work, which may have a result of over-stating the perceived utility. <sep>	weakness
2019-176	Overall though it seems that the reviewers appreciated the novel solution to an important problem, and in general would like to see the paper accepted.	rating_summary

2019-212	All reviewers recommend accept. <sep>	rating_summary
2019-212	Discussion can be consulted below.	misc

2019-214	Two of the reviewers raised their scores during the discussion phase noting that the revised version was clearer and addressed some of their concerns.	rebuttal_process
2019-214	As a result, all the reviewers ultimately recommended acceptance.	rating_summary
2019-214	They particularly enjoyed the insights that the authors shared from their experiments and appreciated that the experiments were quite thorough.	strength
2019-214	All the reviewers mentioned that the work seemed somewhat incremental, but given the results, insights and empirical evaluation decided that it would still be a valuable contribution to the conference.	strength
2019-214	One reviewer added feedback about how to improve the writing and clarity of the paper for the camera ready version.	suggestion

2019-248	There is a clear consensus among the reviews to accept this submission thus I am recommending acceptance.	decision
2019-248	The paper makes a clear, if modest, contribution to language modeling that is likely to be valuable to many other researchers.	strength

2019-252	The paper makes progress on a problem that is still largely unexplored, presents promising results, and builds bridges with prior work on optimal control.	abstract
2019-252	It designs input convex recurrent neural networks to capture temporal behavior of dynamical systems; this then allows optimal controllers to be computed by solving a convex model predictive control problem. <sep>	abstract
2019-252	There were initial critiques regarding some of the claims.	rebuttal_process
2019-252	These have now been clarified. <sep>	rebuttal_process
2019-252	Also, there is in the end a compromise between the (necessary) approximations of the input-convex model and the true dynamics, and being able to compute an optimal result. <sep>	strength
2019-252	Overall, all reviewers and the AC are in agreement to see this paper accepted. <sep>	decision
2019-252	There was extensive and productive interaction between the reviewers and authors. <sep>	rebuttal_process
2019-252	It makes contributions that will be of interest to many, and builds interesting bridges with known control methods.	strength

2019-271	This paper presents a system which exploits semantic information of partial programs during program synthesis, and ensembling of synthesisers.	abstract
2019-271	The idea is general, and admirably simple.	strength
2019-271	The explanation is clear, and the results are impressive.	strength
2019-271	The reviewers, some after significant discussion, agree that this paper makes an import contribution and is one of the stronger papers in the conference.	strength
2019-271	While some possible improvements to the method and experiment were discussed with the reviewers, it seems these are more suitable for future research, and that the paper is clearly publishable in its current form.	decision

2019-273	The paper explores the effect of normalization and initialization in residual networks, motivated by the need to avoid exploding and vanishing activations and gradients.	abstract
2019-273	Based on some theoretical analysis of stepsizes in SGD, the authors propose a sensible but effective way of initializing a network that greatly increases training stability.	abstract
2019-273	In a nutshell, the method comes down to initializing the residual layers such that a single step of SGD results in a change in activations that is invariant to the depth of the network.	abstract
2019-273	The experiments in the paper provide supporting evidence for the benefits; the authors were able to train networks of up to 10,000 layers deep.	abstract
2019-273	The experiments have sufficient depth to support the claims.	strength
2019-273	Overall, the method seems to be a simple but effective technique for learning very deep residual networks. <sep>	strength
2019-273	While some aspects of the network have been used in earlier work, such as initializing residual branches to output zeros, these earlier methods lacked the rescaling aspect, which seems crucial to the performance of this network. <sep>	strength
2019-273	The reviewers agree that the papers provides interesting ideas and significant theoretical and empirical contributions.	strength
2019-273	The main concerns by the reviewers were addressed by the author responses.	rebuttal_process
2019-273	The AC finds that the remaining concerns raised by the reviewers are minor and insufficient for rejection of the paper.	decision

2019-289	The paper describes a method which, given a music waveform, generates another recording of the same music which should sound as if it was performed by different instruments.	abstract
2019-289	The model is an auto-encoder with a WaveNet-like domain-specific decoder and a shared encoder, trained with an adversarial "domain confusion loss".	abstract
2019-289	Even though the method is constructed mostly from existing components, the reviewers found the results interesting and convincing, and recommended the paper for acceptance.	rating_summary

2019-291	The submission proposes a machine learning approach to directly train a prediction system for whether a boolean sentence is satisfiable.	abstract
2019-291	The strengths of the paper seem to be largely in proposing an architecture for SAT problems and the analysis of the generalization performance of the resulting classifier on classes of problems not directly seen during training. <sep>	strength
2019-291	Although the resulting system cannot be claimed to be a state of the art system, and it does not have a correctness guarantee like DPLL based approaches, the paper is a nice re-introduction of SAT in a machine learning context using deep networks.	strength
2019-291	It may be nice to mention eg (W. Ruml.	suggestion
2019-291	Adaptive Tree Search.	suggestion
2019-291	PhD thesis, Harvard University, 2002) which applied reinforcement learning techniques to SAT problems.	suggestion
2019-291	The empirical validation on variable sized problems, etc.	strength
2019-291	is a nice contribution showing interesting generalization properties of the proposed approach. <sep>	strength
2019-291	The reviewers were unanimous in their recommendation that the paper be accepted, and the review process attracted a number of additional comments showing the broader interest of the setting.	rating_summary

2019-299	This paper studies the behavior of weight parameters for linear networks when trained on separable data with strictly decreasing loss functions.	abstract
2019-299	For this setting the paper shows that the gradient descent solution converges to max margin solution and each layer converges to a rank 1 matrix with consequent layers aligned.	abstract
2019-299	All reviewers agree that the paper provides novel results for understanding implicit regularization effects of gradient descent for linear networks.	strength
2019-299	Despite the limitations of this paper such as studying networks with linear activation, studying gradient descent not with practical step sizes, assuming data is linearly separable, reviewers find the results useful and a good addition to existing literature.	strength

2019-300	The reviewers and AC note that the strength of the paper includes a) an interesting compression algorithm of neural networks with provable guarantees (under some assumptions), b) solid experimental comparison with the existing *matrix sparsification* algorithms.	strength
2019-300	The AC's main concern of the experimental part of the paper is that it doesn't outperform or match the performance of the "vanilla" neural network compression algorithms such as Han et al'15.	weakness
2019-300	The AC decided to suggest acceptance for the paper but also strongly encourage the paper to clarify the algorithms in comparison don't include state-of-the-art compression algorithms.	decision

2019-322	This paper proposes a solution for the well-known problem of posterior collapse in VAEs: a phenomenon where the posteriors fail to diverge from the prior, which tends to happen in situations where the decoder is overly flexible. <sep>	abstract
2019-322	A downside of the proposed method is the introduction of hyper-parameters controlling the degree of regularization.	abstract
2019-322	The empirical results show improvements on various baselines. <sep>	abstract
2019-322	The paper proposes the addition of a regularization term that penalizes pairwise similarity of posteriors in latent space.	abstract
2019-322	The reviewers agree that the paper is clearly written and that the method is reasonably motivated.	strength
2019-322	The experiments are also sufficiently convincing.	strength

2019-339	The paper proposes a new attentional pooling mechanism that potentially addresses the issues of simple attention-based weighted averaging (where discriminative parts/frames might get disportionately high attentions).	abstract
2019-339	A nice contribution of the paper is to propose an alternative mechanism with theoretical proofs, and it also presents a method for fast recurrent computation.	abstract
2019-339	The experimental results show that the proposed attention mechanism improves over prior methods (eg, STPN) on THUMOS14 and ActivityNet1.3 datasets.	abstract
2019-339	In terms of weaknesses: (1) the computational cost may be quite significant.	weakness
2019-339	(2) the proposed method should be evaluated over several tasks beyond activity recognition, but it's unclear how it would work. <sep>	weakness
2019-339	The authors provided positive proof-of-concept results on weakly supervised object localization task, improving over CAM-based methods.	weakness
2019-339	However, CAM baseline is a reasonable but not the strongest method and the weakly-supervised object recognition/segmentation domains are much more competitive domains, so it's unclear if the proposed method would achieve the state-of-the-art by simply replacing the weighted-averaging-attentional-pooling with the proposed attention mechanism.	weakness
2019-339	In addition, the description on how to perform attentional pooling over images is not clearly described (it's not clear how the 1D sequence-based recurrent attention method can be extended to 2-D cases).	weakness
2019-339	However, this would not be a reason to reject the paper. <sep>	misc
2019-339	Finally, the paper's presentation would need improvement.	weakness
2019-339	I would suggest that the authors give more intuitive explanations and rationale before going into technical details.	suggestion
2019-339	The paper starts with Figure 1 which is not really well motivated/explained, so it could be moved to a later part.	suggestion
2019-339	Overall, there are interesting technical contributions with positive results, but there are issues to be addressed.	suggestion

2019-346	The paper presents a generative model of sequences based on the VAE framework, where the generative model is given by CNN with causal and dilated connections. <sep>	abstract
2019-346	Novelty of the method is limited; it mainly consists of bringing together the idea of causal and dilated convolutions and the VAE framework.	weakness
2019-346	However, knowing how well this performs is valuable the community. <sep>	strength
2019-346	The proposed method appears to have significant benefits, as shown in experiments.	strength
2019-346	The result on MNIST is, however, so strong that it seems incorrect; more digging into this result, or sourcecode, would have been better.	weakness

2019-353	This paper proposes adding a second objective to the training of neural network classifiers that aims to make the distribution over incorrect labels as flat as possible for each training sample.	abstract
2019-353	The authors describe this as "maximizing the complement entropy."	abstract
2019-353	Rather than adding the cross-entropy objective and the (negative) complement entropy term (since the complement entropy should be maximized while the cross-entropy is minimized), this paper proposes an alternating optimization framework in which first a step is taken to reduce the cross-entropy, then a step is taken to maximize the complement entropy.	abstract
2019-353	Extensive experiments on image classification (CIFAR-10, CIFAR-100, SVHN, Tiny Imagenet, and Imagenet), neural machine translation (IWSLT 2015 English-Vietnamese task), and small-vocabulary isolated-word recognition (Google Commands), show that the proposed two-objective approach outperforms training only to minimize cross-entropy.	abstract
2019-353	Experiments on CIFAR-10 also show that models trained in this framework have somewhat better resistance to single-step adversarial attacks.	abstract
2019-353	Concerns about the presentation of the adversarial attack experiments were raised by anonymous commenters and one of the reviewers, but these concerns were addressed in the revision and discussion.	rebuttal_process
2019-353	The primary remaining concern is a lack of any theoretical guarantees that the alternating optimization converges, but the strong empirical results compensate for this problem.	rebuttal_process

2019-355	This work examines how to craft adversarial examples that will lead trained seq2seq models to generate undesired outputs (here defined as, assigning higher-than-average probability to undesired outputs).	abstract
2019-355	Making a model safe for deployment is an important unsolved problem and this work is looking at it from an interesting angle, and all reviewers agree that the paper is clear, well-presented, and offering useful observations.	strength
2019-355	While the paper does not provide ways to fix the problem of egregious outputs being probable, as pointed out by reviewers, it is still a valuable study of the behavior of trained models and an interesting way to "probe" them, that would likely be of high interest to many people at *CONF*.	strength

2019-372	This paper integrates a bunch of existing approaches for neural architecture search, including OneShot/DARTS, BinaryConnect, REINFORCE, etc.	abstract
2019-372	Although the novelty of the paper may be limited, empirical performance seems impressive.	strength
2019-372	The source code is not available.	weakness
2019-372	I think this is a borderline paper but maybe good enough for acceptance.	decision

2019-380	The paper proposes a simple and general technique based on the information bottleneck to constrain the information flow in the discriminator of adversarial models.	abstract
2019-380	It helps to train by maintaining informative gradients.	abstract
2019-380	While the information bottleneck is not novel, its application in adversarial learning to my knowledge is, and the empirical evaluation demonstrates impressive performance on a broad range of applications.	strength
2019-380	Therefore, the paper should clearly be accepted.	decision

2019-385	The authors consider the use of MAML with model based RL and applied this to robotics tasks with very encouraging results.	abstract
2019-385	There was definite interest in the paper, but also some concerns over how the results were situated, particularly with respect to the related research in the robotics community.	weakness
2019-385	The authors are strongly encouraged to carefully consider this feedback, as they have been doing in their responses, and address this as well as possible in the final version.	suggestion

2019-388	The paper presents a technique of training robust classification models that uses the input distribution within each class to achieve high accuracy and robustness against adversarial perturbations. <sep>	abstract
2019-388	Strengths: <sep> - The resulting model offers good robustness guarantees for a wide range of norm-bounded perturbations <sep>	strength
2019-388	- The authors put a lot of care into the robustness evaluation <sep>	strength
2019-388	Weaknesses: <sep> - Some of the "shortcomings" attributed to the previous work seem confusing, as the reported vulnerability corresponds to threat models that the previous work did not made claims about <sep>	weakness
2019-388	Overall, this looks like a valuable and interesting contribution.	strength

2019-400	The paper addresses the problem of interpreting recurrent neural networks by quantizing their states an mapping them onto a Moore Machine.	abstract
2019-400	The paper presents some interesting results on reinforcement learning and other tasks.	strength
2019-400	I believe the experiments could have been more informative if the proposed technique was compared against a simple quantization baseline (eg based on k-means) so that one can get a better understanding of the difficulty of these task. <sep>	suggestion
2019-400	This paper is clearly above the acceptance threshold at *CONF*.	decision

2019-403	The paper proposed a GAN approach to robust learning against adversarial examples, where a generator produces adversarial examples as perturbations and a discriminator is used to distinguish between adversarial and raw images.	abstract
2019-403	The performance on MNIST, SVHN, and CIFAR10 demonstrate the effectiveness of the approach, and in general, the performance is on par with carefully crafted algorithms for such task. <sep>	abstract
2019-403	The architecture of GANs used in the paper is standard, yet the defensive performance seems good.	strength
2019-403	The reviewers wonder the reason behind this good mechanism and the novelty compared with other works in similar spirits.	weakness
2019-403	In response, the authors add some insights on discussing the mechanism as well as comparisons with other works mentioned by the reviewers. <sep>	rebuttal_process
2019-403	The reviewers all think that the paper presents a simple scheme for robust deep learning based on GANs, which shows its effectiveness in experiments.	strength
2019-403	The understanding on why it works may need further explorations.	suggestion
2019-403	Thus the paper is proposed to be borderline lean accept.	decision

2019-415	The paper proposes the idea of using optimal transport to evaluate the semantic correspondence between two sets of words predicted by the model and ground truth sequences.	abstract
2019-415	Strong empirical results are presented which support the use of optimal transport in conjunction with log-likelihood for training sequence models.	abstract
2019-415	I appreciate the improvements to the manuscript during the review process, and I encourage the authors to address the rest of the comments in the final version.	rebuttal_process

2019-423	All reviewers gave an accept rating: 9, 7 &6. <sep>	rating_summary
2019-423	A clear accept -- just not strong enough reviewer support for an oral.	decision

2019-437	Strengths <sep> The paper presents a method of training two-level hierarchies that is based on relatively intuitive ideas and that performs well. <sep>	strength
2019-437	The challenges of hierarchical RL makes this an important problem.	strength
2019-437	The benefits of periodicity and the separation of internal state from external state is a clean principle that can potentially be broadly employed. <sep>	strength
2019-437	The method does well in outperforming the alternative baselines. <sep>	strength
2019-437	Weaknesses <sep> There is no video of the results.	weakness
2019-437	There is related work, ie, [Peng et al 2016] (rev 4) uses a policy ensemble; phase info is used in DeepLoco/DeepMimic; methods such as "Virtual Windup Toys for Animation" <sep>	weakness
2019-437	exploited periodicity (25y ago);  More comparisons with prior work such as Florensa et al would help. <sep>	weakness
2019-437	The separation of internal and external state is an assumption that may not hold in many cases. <sep>	weakness
2019-437	The results are locomotion focussed.	weakness
2019-437	There are only two timescales. <sep>	weakness
2019-437	Decision <sep> The reviewers are largely in agreement to accept the paper. <sep>	rating_summary
2019-437	There are fairly-simple-but-useful lessons to be found in the paper for those working on HRL problems, particularly those for movement and locomotion. <sep>	strength
2019-437	The AC sees the novely with respect to different pieces of related work is the weakest point of the paper. <sep>	weakness
2019-437	The reviews contain good suggestions for revisions and improvements; the latest version may take care of these (uploaded after the last reviewer comments).	suggestion
2019-437	Overall, the paper will make a good contribution to *CONF* 2019.	decision

2019-449	The paper presents interesting idea, but the reviewers ask for improving further paper clarity - that includes, but is not limited to, providing in-depth explanation of assumptions and also improving the writing that is too heavy and difficult to understand.	weakness

2019-461	This paper proposes distributional concavity regularization for GANs which encourages producing generator distributions with higher entropy. <sep>	abstract
2019-461	The reviewers found the contribution interesting for the *CONF* community.	strength
2019-461	R3 initially found the paper lacked clarity, but the authors took the feedback in consideration and made significant improvements in their revision.	rebuttal_process
2019-461	The reviewers all agreed that the updated paper should be accepted.	rating_summary

2019-472	The paper considers the problem of incorporating human physiological feedback into an autonomous driving system, where minimization of a predicted arousal response is used as an additional source of reward signal, with the intuition that this could be used as a proxy for training a policy that is risk-averse.<sep>	abstract
2019-472	Reviewers were generally positive about the novelty and relevance of the approach but had methodological concerns.	strength
2019-472	In particular, concerns about the weighting of the intrinsic vs. extrinsic reward (why under different settings the optimal tradeoff parameter was different, how this affects the optimal policy if the influence of the intrinsic reward is not decreased with time).	weakness
2019-472	Additional baseline experiments were requested and performed, and the paper was modified to significantly incorporate other feedback such as drawing connections to imitation learning.	rebuttal_process
2019-472	A title change was proposed and accepted to reflect the focus on the application of risk aversion (I'd ask that the authors update the paper OpenReview metadata to reflect this). <sep>	rebuttal_process
2019-472	At a high level, I believe this is an original and interesting contribution to the literature.	strength
2019-472	I have not heard from two of three reviewers regarding whether their concerns were addressed, but given that their concerns appear to me to have been addressed (and their initial scores indicated that the work met the bar for acceptance, if only marginally), I am inclined to recommend acceptance.	decision

2019-473	The paper was judged by the reviewers as providing interesting ideas, well-written and potentially having impact on future research on NN optimization.	strength
2019-473	The authors are asked to make sure they addressed reviewers comments clearly in the paper.	suggestion

2019-478	as r1 and r2 have pointed out, this work presents an interesting and potentially more generalizable extension of the earlier work on introducing noise as regularization in autoregressive language modelling.	strength
2019-478	although it would have been better with more extensive evaluation that goes beyond unsupervised language modelling and toward conditional language modelling, but i believe this is all fine for this further work to be left as follow-up. <sep>	suggestion
2019-478	r3's concern is definitely valid, but i believe the existing evaluation set as well as exposition merit presentation and discussion at the conference, which was shared by the other reviewers as well as a programme chair.	decision

2019-487	This paper develops a stagewise optimization framework for solving non smooth and non convex problems.	abstract
2019-487	The idea is to use  standard convex solvers to iteratively optimize a regularized objective with penalty centered at previous iterates - which is standard in many proximal methods.	abstract
2019-487	The paper combines this with the analysis for non-smooth functions giving a more general convergence results.	abstract
2019-487	Reviewers agree on the usefulness and novelty of the contribution.	strength
2019-487	Initially there were concerns about lack of comparison with current results, but updated version have addressed this issue.	rebuttal_process
2019-487	The main weakness is that the results only holds for \\mu weekly convex functions and the algorithm depends on the knowledge of \\mu.	weakness
2019-487	Despite this limitations, reviewers believe that the paper has enough new material and I suggest for publication.	decision
2019-487	I suggest authors to address these issues in the final version.	suggestion

2019-490	The paper proposes an interesting idea (using "reliable" samples to guide the learning of "less reliable" samples).	strength
2019-490	The experimental results and detailed analysis show clear improvement in object detection, especially small objects. <sep>	strength
2019-490	On the weak side, the paper seems to focus quite heavily on the object detection problem, and how to divide the data into reliable/less-reliable samples is domain-specific (it makes sense for object detection tasks, but it's unclear how to do this for general scenarios).	weakness
2019-490	As the authors promise, it will make more sense to change the title to "Feature Intertwiner for Object Detection" to alleviate such criticisms. <sep>	rebuttal_process
2019-490	Given this said, I think this paper is over the acceptance threshold and would be of interest to many researchers.	decision

2019-491	Reviewers mostly recommended to accept after engaging with the authors.	rating_summary
2019-491	I have decided to reduce the weight of AnonReviewer3 because of the short review.	misc
2019-491	Please take reviewers' comments into consideration to improve your submission for the camera ready.	suggestion

2019-493	Dear authors, <sep>	misc
2019-493	All reviewers liked your work.	misc
2019-493	However, they also noted that the paper was hard to read, whether because of the notation or the lack of visualization. <sep>	weakness
2019-493	I strongly encourage you to spend the extra effort making your work more accessible for the final version.	suggestion

2019-499	This paper proposes a new kernel learning framework for change point detection by using a generative model.	abstract
2019-499	The reviewers agree that the paper is interesting and useful for the community.	strength
2019-499	One of the reviewer had some issues with the paper but those were resolved after the rebuttal.	rebuttal_process
2019-499	The other two reviewers have short reviews and somewhat low confidence, so it is difficult to tell how this paper stands among other that exist in the literature.	rating_summary
2019-499	Overall, given the consistent ratings from all the reviewers, I believe this paper can be accepted.	decision

2019-504	This paper introduces an unsupervised algorithm to learn a goal-conditioned policy and the reward function by formulating a mutual information maximization problem.	abstract
2019-504	The idea is interesting, but the experimental studies seem not rigorous enough.	weakness
2019-504	In the final version, I would like to see some more detailed analysis of the results obtained by the baselines (pixel approaches), as well as careful discussion on the relationship with other related work, such as Variational Intrinsic Control.	suggestion

2019-515	This paper generated a lot of discussion (not all of it visible to the authors or the public). <sep>	misc
2019-515	R1 initially requested reasonable comparisons, but after the authors provided a response (and new results), R1 continued to recommend rejecting the paper simply because they personally did not find the manuscript insightful.	rebuttal_process
2019-515	Despite several requests for clarification, we could not converge on a specific problem with the manuscript.	rebuttal_process
2019-515	Ungrounded gut feelings are not grounds for rejection. <sep>	misc
2019-515	After an extensive discussion, R2 and R3 both recommend accepting the paper and the AC agrees.	decision
2019-515	Paper makes interesting contributions and will be a welcome addition to the literature.	strength

2019-516	The authors consider the problem of active plasticity in the mammalian brain, seen as being a means to enable lifelong learning.	abstract
2019-516	Building on the recent paper on differentiable plasticity, the authors propose a learnt, neuro-modulated differentiable plasticity that can be trained with gradient descent but is more flexible than fixed plasticity.	abstract
2019-516	The paper is clearly motivated and written, and the tasks are constructed to validate the method by demonstrating clear cases where non-modulated plasticity fails completely but where the proposed approach succeeds.	strength
2019-516	On a large, general language modeling task (PTB) there is a small but consistent improvement over LSTMS.	strength
2019-516	The reviewers were very split on this submission, with two reviewers focusing on the lack of large improvements on large benchmarks, and the other reviewer focusing on the novelty and success of the method on simple tasks.	strength
2019-516	The AC tends to side with the positive review because of the following observations: the method is novel and potentially will have long term impact on the field, the language modeling task seems like a poor fit to demonstrate the advantages of the dynamic plasticity, so focusing on that benchmark overly much is misleading, and the paper is high-quality and interesting to the community.	strength

2019-522	This paper provides an RL environment defined over Coq, allowing for RL agents and other such systems to to be trained to propose tactics during the running of an ITP.	abstract
2019-522	I really like this general line of work, and the reviewers broadly speaking did as well.	misc
2019-522	The one holdout is reviewer 3, who raises important concerns about the need for further evaluation.	weakness
2019-522	I understand and appreciate their points, and I think the authors should be careful to incorporate their feedback not only in final revisions to the paper, but in deciding what follow-on work to focus on.	suggestion
2019-522	Nonetheless, and with all due respect to reviewer 3, who provided a review of acceptable quality, I am unsure the substance of their review merits a score as low as they have given.	ac_disagreement
2019-522	Considering the support the other reviews offer for the paper, I recommend acceptance for what the majority of reviewers believes is a good first step towards one day proving substantial new theorems using ITP-ML hybrids.	decision

2019-534	+ An interesting and original idea of embedding words into the (very low dimensional) Wasserstein space, ie clouds of points in a low-dimensional space <sep>	strength
2019-534	+ As the space is low-dimensional (2D), it can be directly visualized. <sep>	strength
2019-534	+ I could imagine the technique to be useful in social / human science for data visualization, the visualization is more faithful to what the model is doing than t-SNE plots of high-dimensional embeddings <sep>	strength
2019-534	+ Though not the first method to embed words as densities but seemingly the first one which shows that multi-modality  / multiple senses are captured (except for models which capture discrete senses) <sep>	strength
2019-534	+ The paper is very well written <sep>	strength
2019-534	-  The results are not very convincing but show that embeddings do capture word similarity (even when training the model on a small dataset) <sep>	weakness
2019-534	-  The approach is not very scalable (hence evaluation on 17M corpus) <sep>	weakness
2019-534	-  The method cannot be used to deal with data sparsity, though (very) interesting for visualization <sep>	weakness
2019-534	-  This is mostly an empirical paper (ie an interesting application of an existing method) <sep>	weakness
2019-534	The reviewers are split.	rating_summary
2019-534	One reviewer is negative as they are unclear what the technical contribution is (but seems a bit biased against empirical papers).	rating_summary
2019-534	Another two find the paper very interesting.	rating_summary

2019-558	This paper proposes a method to encourage diversity of Bayesian dropout method.	abstract
2019-558	A discriminator is used to facilitate diversity, which the method deal with multi-modality.	abstract
2019-558	Empirical results show good improvement over existing methods.	strength
2019-558	This is a good paper and should be accepted.	decision

2019-581	The paper is proposed as probable accept based on current ratings with a majority accept (7,7,5).	decision

2019-625	The paper investigates an incremental form of Sliced Inverse Regression (SIR) for supervised dimensionality reduction.	abstract
2019-625	Unfortunately, the experimental evaluation is insufficient as a serious evaluation of the proposed techniques.	weakness
2019-625	More importantly, the paper does not appear to contribute a significant advance over the extensive literature on fast generalized eigenvalue decompositions in machine learning.	weakness
2019-625	No responses were offered to counter such an opinion.	rebuttal_process

2019-664	The reviewers highlighted that the application in the paper is interesting, but note a lack of new methodology, and also highlight serious flaws in the testing methodology.	weakness
2019-664	Specifically, the reviewers are discouraged by the straightforward reuse of Siamese networks without clear modifications.	weakness
2019-664	Further, the testing setup might be unfairly easy, since chemical families are represented in both training and test sets, while in true application of the method would be exposed to previously unseen chemical families. <sep>	weakness
2019-664	The authors did not participate in the discussion, and address concerns.	rebuttal_process
2019-664	The reviewer consensus is a rejection.	rating_summary

2019-671	The reviewers reached a consensus that the paper is not fit for publication for the moment because a) the paper lacks thorough experiments and b) the criteria provided by the paper are relatively evague (see more details in reviewer 3's comments.）	rating_summary

2019-682	The paper presents a novel problem formulation, that of generating 3D object shapes based on their functionality.	abstract
2019-682	They use a dataset of 3d shapes annotated with functionalities to learn a voxel generative network that conditions on the desired functionality to generate a voxel occupancy grid.	abstract
2019-682	However, the fact that the results are not very convincing -resulting 3D shapes are very coarse- raises questions regarding the usefulness of the proposed problem formulation. <sep>	weakness
2019-682	Thus, the problem formulation novelty alone is not enough for acceptance.	decision
2019-682	Combined with a motivating application to demonstrate the usefulness of the problem formulation and results, would make this paper a much stronger submission.	suggestion
2019-682	Furthermore, the authors have greatly improved the writing of the manuscript during the discussion phase.	rebuttal_process

2019-691	Strengths <sep> The paper proposes to include exploration for the PETS (probabilistic ensembles with trajectory sampling) <sep>	abstract
2019-691	approach to learning the state transition function.	abstract
2019-691	The paper is clearly written. <sep>	strength
2019-691	Weaknesses <sep> All reviewers are in agreement regarding a number of key weaknesses: limited novelty, limited evaluation, <sep>	weakness
2019-691	and aspects of the paper are difficult to follow or are sparse on details. <sep>	weakness
2019-691	No revisions have been posted. <sep>	rebuttal_process
2019-691	Summary <sep> All reviewers are in agreement that the paper requires significant work and that it is not ready for *CONF* publication.	rating_summary

2019-692	The work presents a method to back propagate and visualize bias distribution in network as a form of explainability of network decisions.	abstract
2019-692	Reviewers unanimous reject, no rebuttal from authors.	rating_summary

2019-696	The current version of the paper receives a unanimous rejection from reviewers, as the final proposal.	rating_summary

2019-708	The paper addresses the problem semantic segmentation using a sequential patch-based model.	abstract
2019-708	I agree with the reviewers that the contributions of the paper are not enough for a machine learning venue: (1) there has been prior work on using sequence models for segmentation and (2) the complexity of the proposed approach is not fully justified.	weakness
2019-708	The authors did not submit a rebuttal.	rebuttal_process
2019-708	I encourage the authors to take the feedback into account and improve the paper.	suggestion

2019-711	This paper proposes an approach for learning to generate 3D views, using a surfel-based representation, trained entirely from 2D images.	abstract
2019-711	After the discussion phase, reviewers rate the paper close to the acceptance threshold. <sep>	rating_summary
2019-711	AnonReviewer3, who initially stated "My second concern is the results are all on synthetic data, and most shapes are very simple", remains concerned after the rebuttal, stating "all results are on synthetic, simple scenes.	weakness
2019-711	In particular, these synthetic scenes don't have lighting, material, and texture variations, making them considerably easier than any types of real images." <sep>	weakness
2019-711	The AC agrees with the concerns raised by AnonReviewer3, and believes that more extensive experimentation, either on more complex synthetic scenes or on real images, is needed to back the claims of the paper.	weakness
2019-711	Particularly relevant is the criticism that "While the paper is called 'pix2scene', it's really about 'pix2object' or 'pix2shape'."	weakness

2019-716	The paper addresses an important problem of detecting biases in classifiers (eg in face detection), using simulation tools with Bayesian parameter search.	abstract
2019-716	While the direction of research and the presented approach seem to be practically useful, there were several concerns raised by the reviewers regarding strengthening the results (eg, beyond single avatar, etc), and suggestions on possibly a more applied conference as a better venue.	weakness
2019-716	While thourough rebuttals by the authors addressed some of these concerns, which increased some ratings, overall, the paper was still in the borderline range.	rating_summary
2019-716	We hope the suggestions and comments of the reviewers can help to improve the paper.	misc

2019-737	There is a clear reviewer consensus to reject this paper so I am also recommending rejecting it.	decision
2019-737	The paper is about an interesting and underused technique.	abstract
2019-737	However, ultimately the issue here is that the paper does not do a good enough job of explaining the contribution.	weakness
2019-737	I hope the reviews have given the authors some ideas on how to frame and sell this work better in the future. <sep>	misc
2019-737	For instance, from my own reading of the abstract, I do not understand what this paper is trying to do and why it is valuable.	weakness
2019-737	Phrases such as "we exploit the sparsity" do not tell me why the paper is important to read or what it accomplishes, only how it accomplishes the seemingly elided contribution.	weakness
2019-737	I am forced to make assumptions that might not be correct about the goals and motivation.	weakness
2019-737	It is certainly true that the implicit one-hot representation of words most common in neural language models is not the only possibility and that random sparse vectors for words will also work reasonably well.	weakness
2019-737	I have even tried techniques like this myself, personally, in language modeling experiments and I believe others have as well, although I do not have a nice reference close to hand (some of the various Mikolov models use random hashing of n-grams and I believe related ideas are common in the maxent LM literature and elsewhere).	weakness
2019-737	So when the abstract says things like "We show that guaranteeing approximately equidistant vector representations for unique discrete inputs is enough to provide the neural network model with enough information to learn" my immediate reaction is to ask why this would be surprising or why it would matter.	weakness
2019-737	Based on the reviews, I believe these sorts of issues affect other parts of the manuscript as well.	weakness
2019-737	There needs to be a sharper argument that either presents a problem and its solution or presents a scientific question and its answer.	suggestion
2019-737	In the first case, the problem should be well motivated and in the second case the question should not yet have been adequately answered by previous work and should be non-obvious.	suggestion
2019-737	I should not have to read beyond the abstract to understand the accomplishments of this work. <sep>	weakness
2019-737	Moving to the conclusion and future work section, I can see the appeal of the future work in the second paragraph, but this work has not been done.	weakness
2019-737	The first paragraph is about how it is possible to use random projections to represent words, which is not something I think most researchers would question.	weakness
2019-737	Missing is a clear demonstration of the potential advantages of doing so.	weakness

2019-744	The reviewers have agreed this work is not ready for publication at *CONF*.	rating_summary

2019-749	The paper proposes to define the GAN discriminator as an explicit function of a invertible generator density and a structured Gibbs distribution to tackle the problems of spurious modes and mode collapse.	abstract
2019-749	The resulting model is similar to R2P2, ie it can be seen as adding an adversarial component to R2P2, and shows competitive (but no better) performance.	abstract
2019-749	Reviewers agree, that these limits the novelty of the contribution, and that the paper would be improved by a more extensive empirical evaluation.	weakness

2019-751	The paper presents a novel architecture, reminescent of mixtures-of-experts, <sep>	abstract
2019-751	composed of a set of advocates networks providing an attention map to a separate "judge" network.	abstract
2019-751	Reviewers have several concerns, including lack of theoretical justification, potential scaling limitations, and weak experimental results.	weakness
2019-751	Authors answered to several of the concerns, which did not convinced reviewers.	rebuttal_process
2019-751	The reviewer with the highest score was also the least confident, so overall I will recommend to reject the paper.	decision

2019-780	This paper proposes the NonLinearity Coefficient (NLC), a metric which aims to predicts test-time performance of neural networks at initialization.	abstract
2019-780	The idea is interesting and novel, and has clear practical implications.	strength
2019-780	Reviewers unanimously agreed that the direction is a worthwhile one to pursue.	strength
2019-780	However, several reviewers also raised concerns about how well-justified the method is: in particular, Reviewer 3 believes that a quantitative comparison to the related work is necessary, and takes issue with the motivation for being ad-hoc.	weakness
2019-780	Reviewer 2 also is concerned about the soundness of the coefficient in truly measuring nonlinearity. <sep>	weakness
2019-780	These concerns make it clear that the paper needs more work before it can be published.	decision
2019-780	And, in particular, addressing the reviewers' concerns and providing proper comparison to related works will go a long way in that direction.	suggestion

2019-788	The paper describes an architecture search method which optimises multiple objectives using a genetic algorithm.	abstract
2019-788	All reviewers agree on rejection due to limited novelty compared to the prior art; while the results are solid, they are not ground-breaking to justify acceptance based on results alone.	rating_summary

2019-789	This paper proposes a combination of three techniques to improve the learning performance of Atari games.	abstract
2019-789	Good performance was shown in the paper with all three techniques together applied to DQN.	strength
2019-789	However, it is hard to justify the integration of these techniques.	weakness
2019-789	It is also not clear why the specific decisions were made when combining them.	weakness
2019-789	More comprehensive experiments, such as a more systematic ablation study, are required to convince the benefits of individual components.	weakness
2019-789	Furthermore, it seems very hard to tell whether the improvement of existing approaches, such as Ape-X DQN, was from using the proposed techniques or a deeper architecture (Tables 1&2&4&5).	weakness
2019-789	Overall, this paper is not ready for publication.	decision

2019-795	This paper targets improving the computation efficiency of super resolution task.	abstract
2019-795	Reviewers have a consensus that this paper lacks technical contribution, therefore not recommend acceptance.	rating_summary

2019-799	This paper is concerned with combining past approximation methods to obtain a variant of Deep Recurrent GPs.	abstract
2019-799	While this variant is new, 2/3 reviewers make very overlapping points about this extension being obtained from a straightforward combination of previous ideas.	weakness
2019-799	Furthermore, R3 is not convinced that the approach is well motivated, beyond "filling the gap" in the literature. <sep>	weakness
2019-799	All reviewers also pointed out that the paper is very hard to read.	weakness
2019-799	The authors have improved the manuscript during the rebuttal, but the AC believes that the paper is still written in an unnecessarily complicated way. <sep>	rebuttal_process
2019-799	Overall the AC believes that this paper needs some more work, specifically in (a) improving its presentation (b) providing more technical insights about the methods (as suggested by R2 and R3), which could be a means of boosting the novelty.	weakness

2019-808	While the proposed method is novel, the evaluation is not convincing.	weakness
2019-808	In particular, the datasets and models used are small.	weakness
2019-808	Susceptibility to adversarial examples is tightly related to dimensionality.	weakness
2019-808	The study could benefit from more massive datasets (eg, Imagenet).	suggestion

2019-827	Both R3 and R1 argue for rejection, while R2 argues for a weak accept.	rating_summary
2019-827	Given that we have to reject borderline paper, the AC concludes with "revise and resubmit".	decision

2019-828	This paper introduces a novel idea, and demonstrates its utility in several simulated domains.	abstract
2019-828	The key parts of the algorithm are (a) to prefer keeping and using samples in the ER buffer where the corresponding rho_t, using the current policy pi_t, are not too big or small and (b) preventing the policy from changing too quickly, so that samples in the ER buffer are more on-policy. <sep>	abstract
2019-828	They key weakness is not better investigating the idea of making the ER buffer more on-policy, and the effect of doing so.	weakness
2019-828	The experiments compare to other algorithms, but do not sufficiently investigate the use of both Point 1 and Point 3.	weakness
2019-828	Further, the appendix contains an investigation into parameter sensitivity and gives some confidence intervals.	abstract
2019-828	However, the presentation of this is difficult to follow, and so it is difficult to gauge the sensitivity of Ref-ER.	weakness
2019-828	With a more thorough experimental section, better demonstrating the results (not necessarily running more things), the paper would be much stronger. <sep>	suggestion
2019-828	For more context, the authors rightly mention "It is commonly believed that off-policy methods (eg Q-learning) can handle the dissimilarity between off-policy and on-policy outcomes.	weakness
2019-828	We provide ample evidence that training from highly similar-policy experiences is essential to the success of off-policy continuous-action deep RL."	weakness
2019-828	Q-learning can significantly suffer from changing the state-sampling distribution.	weakness
2019-828	However, adjusting sampling in the ER buffer using rho_t does not change the state-sampling distribution, and so that mismatch remains a problem.	weakness
2019-828	Changing the policy more slowly (Point 3) could help with this more.	suggestion
2019-828	In general, however, these play two different roles that need to be better understood.	weakness
2019-828	The introduction more strongly focuses on classifying samples as more on or off-policy, to solve this problem, rather than the strategy used in Point 3.	weakness
2019-828	So, from the current pitch, its not clear which component is solving the issues claimed with off-policy updates. <sep>	weakness
2019-828	Overall, this paper has some interesting results and is well-written.	strength
2019-828	With more clarity on the roles of the two components of Ref-ER and what they mean for making the ER buffer more on-policy, in terms of both action selection and state distribution, this paper would be a very useful contribution to stable control.	suggestion

2019-845	The reviewers appreciated the contribution of combining Wasserstein Autoencoders with the Sinkhorn algorithm. <sep>	strength
2019-845	Yet R4 as well as the author of the WAE paper (Ilya Tolstikhin) both expressed concerns about the empirical evaluation. <sep>	weakness
2019-845	While R1-R3 were all somewhat positive in their recommendation after the rebuttal, they all have somewhat lower confidence reviews, as is also clear by their comments. <sep>	rating_summary
2019-845	The AC decided to follow the recommendation of R4 as they were the most expert reviewer.	misc
2019-845	The AC thus recommends to "revise and resubmit" the paper.	decision

2019-846	The aim of this paper is to interpret various optimizers such as RMSprop, Adam, and NAG, as approximate Kalman filtering of the optimal parameters.	abstract
2019-846	These algorithms are derived as inference procedures in various dynamical systems.	abstract
2019-846	The main empirical result is the algorithms achieve slightly better test accuracy on MNIST compared to an unregularized network trained with Adam or RMSprop. <sep>	abstract
2019-846	This was a controversial paper, and each of the reviewers had a significant back-and-forth with the authors.	rebuttal_process
2019-846	The controversy reflects that this is a pretty interesting and relevant topic: a proper Bayesian framework could provide significant guidance for developing better optimizers and regularizers.	strength
2019-846	Unfortunately, I don't think this paper delivers on its promise of a unifying Bayesian framework for these various methods, and I don't think it's quite ready for publication at *CONF*. <sep>	decision
2019-846	There was some controversy about relationships to various recently published papers giving Bayesian interpretations of optimizers.	misc
2019-846	The authors believe the added value of this submission is that it recovers features such as momentum and root-mean-square normalization.	rebuttal_process
2019-846	This would be a very interesting contribution beyond those works.	strength
2019-846	But R2 and R3 feel like these particular features were derived using fairly ad-hoc assumptions or approximations almost designed to obtain existing algorithms, and from reading the paper I have to say I agree with the reviewers. <sep>	weakness
2019-846	There was a lot of back-and-forth about the correctness of various theoretical claims.	rebuttal_process
2019-846	But overall, my impression is that the theoretical arguments in this paper exceed the bar for a primarily practical/empirical paper, but aren't rigorous enough for the paper to stand purely based on the theoretical contributions. <sep>	weakness
2019-846	Unfortunately, the empirical part of the paper is rather lacking.	weakness
2019-846	The only experiment reported is on MNIST, and the only result is improved test error.	weakness
2019-846	The baseline gets below 99% test accuracy, below the level achieved by the original LeNet, suggesting the baseline may be somehow broken.	weakness
2019-846	Simply measuring test error doesn't really get at the benefits of Bayesian approaches, as it doesn't distinguish it from the many other regularizers that have been proposed.	weakness
2019-846	Since the proposed method is nearly identical to things like Adam or NAG, I don't see any reason it can't be evaluated on more challenging problems (as reviewers have asked for). <sep>	weakness
2019-846	Overall, while I find the ideas promising, I think the paper needs considerable work before it is ready for publication at *CONF*.	decision

2019-849	As the reviewers point out, the paper seems to be below the *CONF* publication bar due to low novelty and limited significance.	rating_summary

2019-854	This paper shows experiments in favor of learning and using heteroscedastic noise models for differentiable Bayes filter.	abstract
2019-854	Reviewers agree that this is interesting and also very useful for the community.	strength
2019-854	However, they have also found plenty of issues with the presentation, execution and evaluations shown in the paper.	weakness
2019-854	Post rebuttal, one of the reviewer increased their score, but the other has reduced the score.	rebuttal_process
2019-854	Overall, the reviewers are in agreement that more work is required before this work can be accepted. <sep>	rating_summary
2019-854	Some of existing work on variational inference has not been included which, I agree, is problematic.	weakness
2019-854	Simple methods have been compared but then why these methods were chosen and not the other ones, is not completely clear.	weakness
2019-854	The paper definitely can improve on this aspect, clearly discussing relationships to many existing methods and then picking important methods to clearly bring some useful insights about learning heteroscedastic noise.	weakness
2019-854	Such insights are currently missing in the paper. <sep>	weakness
2019-854	Reviewers have given many useful feedback in their review, and I believe this can be helpful for the authors to improve their work.	misc
2019-854	In its current form, the paper is not ready to be accepted and I recommend rejection.	decision
2019-854	I encourage the authors to resubmit this work.	decision

2019-871	The paper presents "deep deducing", which means learning the state-action value function of 2 player games from a payoff table, and using the value function by maximizing over the (actionable) inputs at test time. <sep>	abstract
2019-871	The paper lacks clarity overall.	weakness
2019-871	The method does not contain any new model nor algorithm.	weakness
2019-871	The experiments are too weak (easy environments, few/no comparisons) to support the claims. <sep>	weakness
2019-871	The paper is not ready for publication at this time.	decision

2019-896	The paper proposes a method to escape saddle points by adding and removing units during training.	abstract
2019-896	The method does so by preserving the function when the unit is added while increasing the gradient norm to move away from the critical point.	abstract
2019-896	The experimental evaluation shows that the proposed method does escape when positioned at a saddle point - as found by the Newton method.	abstract
2019-896	The reviewers find the theoretical ideas interesting and novel, but they raised concerns about the method's applicability for typical initializations, the experimental setup, as well as the terminology used in the paper.	weakness
2019-896	The title and terminology were improved with the revision, but the other issues were not sufficiently addressed.	rebuttal_process

2019-928	The reviewers raised a number of concerns including insufficiently demonstrated benefits of the proposed methodology, lack of explanations, and the lack of thorough and convincing experimental evaluation.	weakness
2019-928	The authors' rebuttal failed to alleviate these concerns fully.	rebuttal_process
2019-928	I agree with the main concerns raised and, although I also believe that the work can result eventually in a very interesting paper, I cannot suggest it at this stage for presentation at *CONF*.	decision

2019-939	The paper analyzes the performance of CNN models when data is mislabelled in different manners. <sep>	abstract
2019-939	The reviewers and AC note the critical limitation of novelty of this paper to meet the high standard of *CONF*. <sep>	weakness
2019-939	AC thinks the proposed method has potential and is interesting, but decided that the authors need more works to publish.	decision

2019-951	This paper presents an heuristic method to detect periodicity in a time-series such that it can handle noise and multiple periods. <sep>	abstract
2019-951	All reviewers agreed that this paper falls off the scope of *CONF* since it does not discuss any learning-related question.	rating_summary
2019-951	Moreover, the authors did not provide any response nor updated manuscript addressing the reviewers remarks.	rebuttal_process
2019-951	The AC thus recommends rejection.	decision

2019-965	Significant spread of scores across the reviewers and unfortunately not much discussion despite prompts from the area chair and the authors.	rating_summary
2019-965	The most positive reviewer is the least confident one.	rating_summary
2019-965	Very close to the decision boundary but after careful consideration by the senior PCs just below the acceptance threshold.	decision
2019-965	There is significant literature already on this topic.	weakness
2019-965	The "thought delta" created by this paper and the empirical results are also not sufficient for acceptance.	decision

2019-966	Reviewers are in full agreement for rejection.	rating_summary

2019-979	There is no author response for this paper.	rebuttal_process
2019-979	The paper formulates a definition of easy and hard examples for training a neural network (NN) in terms of their frequency of being classified correctly over several repeats.	abstract
2019-979	One repeat corresponds to training the NN from scratch.	abstract
2019-979	Top 10% and bottom 10%  of the samples with the highest and the lowest frequency define easy and hard instances for training.	abstract
2019-979	The authors also compare easy and hard examples across different architectures of NNs. <sep>	abstract
2019-979	On the positive side, all the reviewers acknowledge the potential usefulness of quantifying easy and hard examples in training NNs, and R1 was ready to improve his/her initial rating if the authors revisited the paper. <sep>	strength
2019-979	On the other hand, all the reviewers and AC agreed that the paper requires (1) major improvement in presentation clarity -- see detailed comments of R1 on how to improve as well as comments/questions from R3 and R2; try to avoid  confusing terminology such as 'contradicted patterns'. <sep>	weakness
2019-979	R1 raised important concerns that the proposed notion of easiness is drawn from the experiment in Fig.	weakness
2019-979	1 of Arpit et al (2017) which is not properly attributed.	weakness
2019-979	R3 and R2 agreed that in its current state the experimental results are not conclusive and often non informative.	weakness
2019-979	To strengthen the paper the reviewers suggested to include more experiments in terms of different datasets, to propose a better metric for defining easy and hard samples (see R3's suggestions). <sep>	suggestion
2019-979	We hope the reviews are useful for improving the paper.	misc

2019-995	The paper proposes an interesting idea for efficient exploration of on-policy learning in sparse reward RL problems.	abstract
2019-995	The empirical results are promising, which is the main strength of the paper.	strength
2019-995	On the other hand, reviewers generally feel that the proposed algorithm is rather ad hoc, sometimes with not-so-transparent algorithmic choices.	weakness
2019-995	As a result, it is really unclear whether the idea works only on the test problems, or applies to a broader set of problems.	weakness
2019-995	The author responses and new results are helpful and appreciated by all reviewers, but do not change the reviewers' concerns.	rebuttal_process

2019-999	This paper develops an active variable selection framework that couples a partial variational autoencoder capable of handling missing data with an information acquisition criteria derived from Bayesian experimental design.	abstract
2019-999	The paper is generally well written and the formulation appears to be natural, with a compelling real world healthcare application.	strength
2019-999	The topic is relatively under-explored in deep learning  and the paper appears to attempt to set a valuable baseline.	strength
2019-999	However, the AC cannot recommend acceptance based on the fact that reviewer 2 has brought up concerns about the competitiveness of the approach relative to alternative methods reported in the experimental section, and all reviewers have found various parts of the paper to have room for improvement with regards to technical clarity.	decision
2019-999	As such the paper would benefit from a revision and a stronger resubmission.	suggestion

2019-1050	Both authors and reviewers agree that the ideas in the paper were not presented clearly enough.	weakness

2019-1061	The paper presents a method to learn inference mapping for GANs by reusing the learned discriminator's features and fitting a model over these features to reconstruct the original latent code z. R1 pointed out the connection to InfoGAN which the authors have addressed.	abstract
2019-1061	R2 is concerned about limited novelty of the proposed method, which the AC agrees with, and lack of comparison to a related iGAN work by Zhu et al (2016).	weakness
2019-1061	The authors have provided the comparison in the revised version but the proposed method seems to be worse than iGAN in terms of the metrics used (PSNR and SSIM), though more efficient.	rebuttal_process
2019-1061	The benefits of using the proposed metrics for evaluating GAN quality are also not established well, particularly in the context of other recent metrics such as FID and GILBO.	rebuttal_process

2019-1062	The paper studies the problem of uncertainty estimation of neural networks and proposes to use Bayesian approach with noice contrastive prior. <sep>	abstract
2019-1062	The reviewers and AC note the potential weaknesses of experimental results: (1) lack of sufficient datasets with moderate-to-high dimensional inputs, (2) arguable choices of hyperparameters and (3) lack of direct evaluations, eg, measuring network calibration is better than active learning. <sep>	weakness
2019-1062	The paper is well written and potentially interesting.	strength
2019-1062	However, AC decided that the paper might not be ready to publish in the current form due to the weakness.	decision

2019-1074	Strengths of the paper: <sep> Based on previous work suggesting that radial basis features can help defend against adversarial attacks, the paper proposes a concrete method for incorporating them in deep networks.	strength
2019-1074	The paper evaluates the method on multiple datasets, including MNIST and  ISBI International Skin Imaging Collaboration (ISIC) Challenge. <sep>	strength
2019-1074	Weaknesses: <sep> Reviewers 2 and 3 felt that the paper was not clearly written, and cited several concrete questions about the method that could not be understood from the paper.	weakness
2019-1074	There were additional concerns of lacking comparison to existing methods, and Reviewer 1 pointed out that a competing method gave higher performance, although this was not reported in the present submission. <sep>	weakness
2019-1074	Points of contention: <sep> The authors did not provide a response to the reviewer concerns. <sep>	rebuttal_process
2019-1074	Consensus: <sep> All reviewers recommended that the paper be rejected, and the authors did not provide a rebuttal.	rating_summary

2019-1090	This paper presents an interesting approach to image compression, as recognized by all reviewers.	strength
2019-1090	However, important concerns about evaluating the contribution remains: as noted by reviewers, evaluating the contribution requires disentangling what part of the improvement is due to the proposed approach and what part is due to the loss chosen and evaluation methods.	weakness
2019-1090	While authors have done a valuable effort adding experiments to incorporate reviewers suggestions with ablation studies, it does not convincingly show that the proposed approach truly improves over existing ones like Balle et al Authors are encouraged to strengthen their work for future submission by putting particular emphasis on those questions.	decision

2019-1095	This paper shows that combining GAN and VAE for video prediction allows to trade off diversity and realism.	abstract
2019-1095	The paper is well-written and the experimentation is careful, as noted by reviewers.	strength
2019-1095	However, reviewers agree that this combination is of limited novelty (having been used for images before).	weakness
2019-1095	Reviewers also note that the empirical performance is not very much stronger than baselines.	weakness
2019-1095	Overall, the novelty is too slight and the empirical results are not strong enough compared to baselines to justify acceptance based solely on empirical results.	weakness

2019-1097	The paper aims to clean data samples with label noise in the training procedure. <sep>	abstract
2019-1097	The reviewers and AC note the following potential weaknesses: (1) the assumption of uniform noise, which is not the case in practice, (2) marginal gains under real-world datasets and (3) highly empirical and ad-hoc approach. <sep>	weakness
2019-1097	AC thinks the proposed method has potential and is interesting, but decided that the authors need more significant works to publish the work.	decision

2019-1108	This paper proposes a document classification algorithm based on partitioned word vector averaging. <sep>	abstract
2019-1108	I agree with even the most positive reviewer.	misc
2019-1108	More experiments would be good.	weakness
2019-1108	This is a very developed old area.	weakness

2019-1117	All reviewers agree to reject.	rating_summary
2019-1117	While there were many positive points to this work, reviewers believed that it was not yet ready for acceptance.	rating_summary

2019-1144	The reviewers are unanonymous in their assessment that the paper is not *CONF* quality in its current form.	rating_summary

2019-1199	This work studies the performance of several end-to-end CNN architectures for the prediction of biomedical assays in microscopy images.	abstract
2019-1199	One of the architectures, GAPnet, is a minor modification of existing global average pooling (GAP) networks, involving skip connections and concatenations.	abstract
2019-1199	The technical novelties are low, as outlined by several reviewers and confirmed by the authors, as most of the value of the work lies in the empirical evaluation of existing methods, or minor variants thereof. <sep>	weakness
2019-1199	Given the low technical novelty and reviewer consensus, recommend reject, however area chair recognizes that the discovered utility may be of value for the biomedical community.	decision
2019-1199	Authors are encouraged to use reviewer feedback to improve the work, and submit to a biomedical imaging venue for dissemination to the appropriate communities.	suggestion

2019-1223	Reviewers are in a consensus and recommended to reject after engaging with the authors.	rating_summary
2019-1223	Please take reviewers' comments into consideration to improve your submission should you decide to resubmit.	decision

2019-1228	Multiple reviewers had concerns about the clarity of the presentation and the significance of the results.	weakness

2019-1241	This paper proposes a new method for speeding up convolutional neural networks.	abstract
2019-1241	It uses the idea of early terminating the computation of convolutional layers.	abstract
2019-1241	It saves FLOPs, but the reviewers raised a critical concern that it doesn't save wall-clock time.	weakness
2019-1241	The time overhead is about 4 or 5 times of the original model.	weakness
2019-1241	There is not any reduced execution time but much longer.	weakness
2019-1241	The authors agreed that "the overhead on the inference time is certainly an issue of our method".	rebuttal_process
2019-1241	The work is not mature and practical.	weakness
2019-1241	recommend for rejection.	decision

2019-1248	The paper presents a conformal prediction approach to supervised classification, with the goal of reducing the overconfidence of standard soft-max learning techniques.	abstract
2019-1248	The proposal is based on previously published methods, which are extended for use with deep learning predictors.	abstract
2019-1248	Empirical evaluation suggests the proposal results in competitive performance.	abstract
2019-1248	This work seems to be timely, and the topic is of interest to the community. <sep>	strength
2019-1248	The reviewers and AC opinions were mixed, with reviewers either being unconvinced about the novelty of the proposed work or expressing issues about the strength of the empirical evidence supporting the claims.	weakness
2019-1248	Additional experiments would significantly strengthen this submission.	suggestion

2019-1249	This paper extends the transformer model of Vashwani et al by replacing the sine/cosine positional encodings with information reflecting the tree stucture of appropriately parsed data.	abstract
2019-1249	According to the reviews, the paper, while interesting, does not make the cut.	rating_summary
2019-1249	My concern here is that the quality of the reviews, in particular those of reviewers 2 and 3, is very sub par.	ac_disagreement
2019-1249	They lack detail (or, in the case of R2, did so until 05 Dec(!!)), and the reviewers did not engage much (or at all) in the subsequent discussion period despite repeated reminders.	ac_disagreement
2019-1249	Infuriatingly, this puts a lot of work squarely in the lap of the AC: if the review process fails the authors, I cannot make a decision on the basis of shoddy reviews and inexistent discussion!	misc
2019-1249	Clearly, as this is not the fault of the authors, the best I can offer is to properly read through the paper and reviews, and attempt to make a fair assessment. <sep>	misc
2019-1249	Having done so, I conclude that while interesting, I agree with the sentiment expressed in the reviews that the paper is very incremental.	weakness
2019-1249	In particular, the points of comparison are quite limited and it would have been good to see a more thorough comparison across a wider range of tasks with some more contemporary baselines.	weakness
2019-1249	Papers like Melis et al 2017 have shown us that an endemic issue throughout language modelling (and certainly also other evaluation areas) is that complex model improvements are offered without comparison against properly tuned baselines and benchmarks, failing to offer assurances that the baselines would not match performance of the proposed model with proper regularisation.	weakness
2019-1249	As some of the reviewers, the scope of comparison to prior art in this paper is extremely limited, as is the bibliography, which opens up this concern I've just outlined that it's difficult to take the results with the confidence they require.	weakness
2019-1249	In short, my assessment, on the basis of reading the paper and reviews, is that the main failing of this paper is the lack of breadth and depth of evaluation, not that it is incremental (as many good ideas are).	weakness
2019-1249	I'm afraid this paper is not ready for publication at this time, and am sorry the authors will have had a sub-par review process, but I believe it's in the best interest of this work to encourage the authors to further evaluate their approach before publishing it in conference proceedings.	decision

2019-1270	With positive unlabeled learning the paper targets an interesting problem and proposes a new GAN based method to tackle it.	abstract
2019-1270	All reviewers however agree that the write-up and the motivation behind the method could be made more clear and that novelty compared to other GAN based methods is limited.	weakness
2019-1270	Also the experimental analysis does not show a strong clear performance advantage over existing models.	weakness

2019-1311	Dear authors, <sep>	misc
2019-1311	Thank you for submitting your work to *CONF*.	misc
2019-1311	The original goal of using smaller models to train a bigger one is definitely interesting and has been the topic of a lot of works. <sep>	strength
2019-1311	However, the reviewers had two major complaints: the first one is about the clarity of the paper and the second one is about the significance of the tasks on which the algorith is tested.	weakness
2019-1311	For the latter point, your rebuttal uses arguments which are little known in the ML community and so should be expanded in a future submission.	decision

2019-1327	Two out of three reviews for this paper were provided in detail, but all three reviewers agreed unanimously that this paper is below the acceptance bar for *CONF*.	rating_summary
2019-1327	The reviewers admired the clarity of writing, and appreciated the importance of the application, but none recommended the paper for acceptance due largely to concerns on the experimental setup.	rating_summary

2019-1328	This paper suggests a method for defending against adversarial examples and out-of-distribution samples via projection onto the data manifold.	abstract
2019-1328	The paper suggests a new method for detecting when hidden layers are off of the manifold, and uses auto encoders to map them back onto the manifold. <sep>	abstract
2019-1328	The paper is well-written and the method is novel and interesting.	strength
2019-1328	However, most of the reviewers agree that the original robustness evaluations were not sufficient due to restricting the evaluation to using FGSM baseline and comparison with thermometer encoding (which both are known to not be fully effective baselines). <sep>	weakness
2019-1328	After rebuttal, Reviewer 4 points out that the method offers very little robustness over adversarial training alone, even though it is combined with adversarial training, which suggests that the method itself provides very little robustness.	rebuttal_process

2019-1331	The reviewers agree the paper is not ready for publication at *CONF*.	rating_summary

2019-1333	This paper proposes search-guided training for structured prediction energy networks (SPENs). <sep>	abstract
2019-1333	The reviewers found some interest in this approach, though were somewhat underwhelmed by the experimental comparison and the details provided about the method. <sep>	strength
2019-1333	R1 was positive and recommends acceptance; R2 and R3 thought the paper was on the incremental side and recommend rejection.	rating_summary
2019-1333	Given the space restriction to this year's conference, we have to reject some borderline papers.	misc
2019-1333	The AC thus recommends the authors to take the reviewers comments in consideration for a "revise and resubmit".	decision

2019-1340	The paper proposes a filtering technique to use less training examples in order to train faster; the filtering step is done with an autoencoder. <sep>	abstract
2019-1340	Experiments are done on CIFAR-10.	abstract
2019-1340	Reviewers point to a lack of convincing experiments, weak evidence, lack of experimental details. <sep>	weakness
2019-1340	Overall, all reviewers converge to reject this paper, and I agree with them.	decision

2019-1355	This paper offers a new angle through which to study the development of comparison functions for sentence pair classification tasks by drawing on the literature on statistical relational learning.	abstract
2019-1355	All three reviewers seemed happy to see an attempt to unify these two closely related relation-learning problems.	strength
2019-1355	However, none of the reviewers were fully convinced that this attempt has yielded any substantial new knowledge: Many of the ideas that come out of this synthesis have already appeared in the sentence-pair modeling literature (in work cited in the paper under review), and the proposed new methods do not yield substantial improvements for the tasks they're tested on. <sep>	weakness
2019-1355	I'm happy to accept the authors' arguments that sentence-to-vector models have practical value, and I'm not placing too much weight on the reviewer's comments about the choice to use that modeling framework.	rebuttal_process
2019-1355	I am slightly concerned that the reviewers (especially R2) observed some overly broad statements in the paper, and I urge the authors to take those comments very seriously. <sep>	weakness
2019-1355	I'm mostly concerned, though, about the lack of an impactful positive contribution: I'd have hoped for a paper of this kind to offer a  a method with clear empirical advantages over prior work, or else a formal result which is more clearly new, and the reviewers are not convinced that this paper makes a contribution of either kind.	weakness

2019-1361	The paper introduces a modification of batch normalization technique.	abstract
2019-1361	In contrast to the original batch normalization that normalizes minibatch examples using their mean and standard deviation, this modification uses weighted average of mean and standard deviation from the current and all previous minibatches.	abstract
2019-1361	The authors then provide some theoretical justification for the superiority of their variant of BatchNorm. <sep>	abstract
2019-1361	Unfortunately, the empirical demonstration of the improved performance seems not sufficient and thus fairly unconvincing.	weakness

2019-1381	This paper proposes to combine RL and imitation learning, and the proposed approach seems convincing. <sep>	abstract
2019-1381	As is typical in RL work, the evaluation of the method is not strong enough to convince the reviewers.	weakness
2019-1381	Increasing community criticism on RL methods not scaling must be taken seriously here, despite the authors' disagreement.	rebuttal_process

2019-1438	1. Describe the strengths of the paper.	misc
2019-1438	As pointed out by the reviewers and based on your expert opinion. <sep>	misc
2019-1438	- The problem and approach, steganography via GANs, is interesting. <sep>	strength
2019-1438	- The results seem promising.<sep>	strength
2019-1438	2. Describe the weaknesses of the paper.	misc
2019-1438	As pointed out by the reviewers and based on your expert opinion.	misc
2019-1438	Be sure to indicate which weaknesses are seen as salient for the decision (ie, potential critical flaws), as opposed to weaknesses that the authors can likely fix in a revision. <sep>	misc
2019-1438	The original submission was imprecise and difficult to follow and, while the AC acknowledges that the authors made significant improvements, the current version still needs some work before it's clear enough to be acceptable for publication. <sep>	decision
2019-1438	3. Discuss any major points of contention.	misc
2019-1438	As raised by the authors or reviewers in the discussion, and how these might have influenced the decision.	misc
2019-1438	If the authors provide a rebuttal to a potential reviewer concern, it's a good idea to acknowledge this and note whether it influenced the final decision or not.	misc
2019-1438	This makes sure that author responses are addressed adequately. <sep>	misc
2019-1438	Concerns varied by reviewer and there was no main point of contention. <sep>	misc
2019-1438	4. If consensus was reached, say so.	misc
2019-1438	Otherwise, explain what the source of reviewer disagreement was and why the decision on the paper aligns with one set of reviewers or another. <sep>	misc
2019-1438	The reviewers did not reach a consensus.	misc
2019-1438	The final decision is aligned with the less positive reviewers, one of whom was very confident in his/her review.	decision
2019-1438	The AC agrees that the paper should be made clearer and more precise.	weakness

2019-1441	perhaps the biggest issue with the proposed approach is that the proposed approach, which supposedly addresses the issue of capturing long-term dependency with a faster convergence, was only tested on problems with largely fixed length.	weakness
2019-1441	with the proposed k_n gate being defined as a gaussian with a single mean (per unit?)	weakness
2019-1441	and variance, it is important and interesting to know how this network would cope with examples of vastly varying lengths.	weakness
2019-1441	in addition, r3 made good points about comparison against conventional LSTM and how it should be done with careful hyperparameter tuning and based on conventional known setups. <sep>	suggestion
2019-1441	this submission will be greatly strengthened with more experiments using a better set of benchmarks and by more carefully placing its contribution wrt other recent advances.	suggestion

2019-1456	The paper proposes a nice approach to massively multi-label problems with rare labels which may only have a limited number of positive examples; the approach uses Bayes nets to exploit the relationships among the labels in the output layer of a neural nets.	abstract
2019-1456	The paper is clearly written and the approach seems promising, however, the reviewers would like to see even more convincing empirical results.	weakness

2019-1461	This work examines how to deal with multiple classes.	abstract
2019-1461	Unfortunately, as reviewers note, it fails to adequately ground its approach in previous work and show how the architecture relates to the considerable research that has examined the question beforehand.	weakness

2019-1478	This paper proposes an anomaly-detection approach by augmenting VAE encoder with a network multiple hypothesis network and then using a discriminator in the decoder to select one of the hypothesis.	abstract
2019-1478	The idea is interesting although the reviewers found the paper to be poorly written and the approach to be a bit confusing and complicated. <sep>	weakness
2019-1478	Revisions and rebuttal have certainly helped to improve the quality of the work.	rebuttal_process
2019-1478	However, the reviewers believe that the paper require more work before it can be accepted at *CONF*.	rating_summary
2019-1478	For this reason, I recommend to reject this paper in its current state.	decision

2019-1480	The reviewers agree the paper is not ready for publication.	rating_summary

2019-1493	This paper considers "prototypes" in machine learning, in which a small subset of a dataset is selected as representative of the behavior of the models.	abstract
2019-1493	The authors propose a number of desiderata, and outline the connections to existing approaches.	abstract
2019-1493	Further, they carry out evaluation with user studies to compare them with human intuition, and empirical experiments to compare them to each other.	abstract
2019-1493	The reviewers agreed that the search for more concrete definitions of prototypes is a worthy one, and they appreciated the user studies. <sep>	strength
2019-1493	The reviewers and AC note the following potential weaknesses: (1) the specific description of prototypes that the authors are using is not provided precisely, (2) the desiderata was found to be informal, leading to considerable confusion regarding the choices that are made and their compatibility with each other, (3) concerns in the evaluation regarding the practicality and the appropriateness of the user study for the goals of the paper. <sep>	weakness
2019-1493	Although the authors provided detailed responses to these concerns, most of them still remained.	rebuttal_process
2019-1493	Both reviewer 1 and reviewer 2 encourage the authors to define the prototypes defined more precisely, providing motivation for the various choices therein.	suggestion
2019-1493	Even though some of the concerns raised by reviewer 3 were addressed, it still remains to be seen how scalable the approach is for real-world applications. <sep>	rebuttal_process
2019-1493	For these reasons, the reviewers and the AC feel that the authors would need to make substantial improvements for the paper to be accepted.	decision

2019-1537	The paper proposes to improve VAE by using a prior distribution that has been previously proposed for independent subspace analysis (ISA).	abstract
2019-1537	The clarity of the paper could be improved by more clearly describing the proposed method and its implementation details.	weakness
2019-1537	The originality is not that high, as the main change to VAE is replacing the usual isotropic Gaussian prior with an ISA prior.	weakness
2019-1537	Moreover, the paper does not provide comparison to VAEs with other more sophisticated priors, such as the VampPrior, and it is unclear whether using the ISA prior makes it difficult to scale to high-dimensional observations.	weakness
2019-1537	Therefore, it is difficult to evaluate the significance of ISA-VAE.	weakness
2019-1537	The authors are encouraged to carefully revise their paper to address these concerns.	suggestion

2019-1569	This work presents extensions of dialogue systems to simultaneously capture speakers' "personas" (in the framing of Li et al's work) and adapt to them.	abstract
2019-1569	While the ideas are interesting, reviewers note that the incremental contribution compared to previous work is a bit too limited for *CONF*'s expectation, without being offset by strongly convincing experimental results.	weakness
2019-1569	Authors are encouraged to incorporated their ideas into future submissions after having combined them with other insights to provide a stronger overall contribution.	suggestion

2019-1572	The paper studies an narrowly focused but interesting problem -- if the Visual Question answering model "FILM" from Perez et al (2018) is able to decide if "most" of the objects have a certain attribute or color.	abstract
2019-1572	While the work itself is appreciate by the reviewers, concerns remain about the conclusion being limited in scope due to the synthetic nature of the data, and the analysis fairly narrow (a single model with a single very specific task).	weakness
2019-1572	We encourage the authors to use reviewer feedback to make the manuscript stronger for a future deadline.	suggestion

2019-1581	The authors study an inverse reinforcement learning problem where the goal is to infer an underlying reward function from demonstration with bias.	abstract
2019-1581	To achieve this, the authors learn the planners and the reward functions from demonstrations.	abstract
2019-1581	As this is in general impossible, the authors consider two special cases in which either the reward function is observed on a subset of tasks or in which the observations are assumed to be close to optimal.	abstract
2019-1581	They propose algorithms for both cases and evaluate these in basic experiments.	abstract
2019-1581	The problem considered is important and challenging.	strength
2019-1581	One issue is that in order to make progress the authors need to make strong and restrictive assumptions (eg, assumption 3, the well-suited inductive bias).	weakness
2019-1581	It is not clear if the assumptions made are reasonable.	weakness
2019-1581	Experimentally, it would be important to see how results change if the model for the planner changes and to evaluate what the inferred biases would be.	suggestion
2019-1581	Overall, there is consensus among the reviewers that the paper is interesting but not ready for publication.	rating_summary

2019-1587	The authors propose a technique for compressing neural networks by examining the correlations between filter responses, by removing filters which are highly correlated.	abstract
2019-1587	This differentiates the authors' work from many other works which compress the weights independent of the task/domain. <sep>	abstract
2019-1587	Strengths: <sep> Clearly written paper <sep>	strength
2019-1587	PFA-KL does not require additional hyperparameter tuning (apart from those implicit in choosing \\psi) <sep>	strength
2019-1587	Experiments demonstrate that the number of filters determined by the algorithm scale with complexity of the task <sep>	strength
2019-1587	Weaknesses: <sep> Results on large-scale tasks such as Imagenet (subsequently added by the authors during the rebuttal period) <sep>	weakness
2019-1587	Compression after the fact may not be as good as training with a modified loss function that does compression jointly <sep>	weakness
2019-1587	Insufficient comparisons on ResNet architectures which make comparisons against previous works harder <sep>	weakness
2019-1587	Overall, the reviewers were in agreement that this work (particularly, the revised version) was close to the acceptance threshold.	rating_summary
2019-1587	In the ACs view, the authors addressed many of the concerns raised by the reviewers in the revisions.	rebuttal_process
2019-1587	However, after much deliberation, the AC decided that the weaknesses 2, and 3 above were significant, and that these should be addressed in a subsequent submission.	decision

2019-1609	This paper proposes an approach for incremental learning of new classes using meta-learning. <sep>	abstract
2019-1609	Strengths: The framework is interesting.	strength
2019-1609	The reviewers agree that the paper is well-written and clear.	strength
2019-1609	The experiments include comparisons to prior work, and the ablation studies are useful for judging the performance of the method. <sep>	strength
2019-1609	Weaknesses: The paper does not provide significant insights over Gidaris & Komodakis '18.	weakness
2019-1609	Reviewer 1 was also concerned that the motivation for RBP is not entirely clear. <sep>	weakness
2019-1609	Overall, the reviewers found that the strengths did not outweigh the weaknesses.	rating_summary
2019-1609	Hence, I recommend reject.	decision

2019-1611	The reviewers found the paper to be well written, the work novel and they appreciated the breadth of the empirical evaluation.	strength
2019-1611	However, they did not seem entirely convinced that the improvements over the baseline are statistically significant.	weakness
2019-1611	Reviewer 1 has lingering concerns about the experimental conditions and whether propensity-score matching within a minibatch would provide a substantial improvement over propensity-score matching across the dataset.	weakness
2019-1611	Overall the reviewers found this to be a good paper and noted that the discussion was illuminating and demonstrated the merits of this work and interest to the community.	rating_summary
2019-1611	However, no reviewers were prepared to champion the paper and thus it falls just below borderline for acceptance.	decision

2019-1612	This work examines the AlphaGo Zero algorithm, a self-play reinforcement learning algorithm that has been shown to learn policies with superhuman performance on 2 player perfect information games.	abstract
2019-1612	The main result of the paper is that the policy learned by AGZ corresponds to a Nash equilibrium, that and that the cross-entropy minimization in the supervised learning-inspired part of the algorithm converges to this Nash equillibrium, proves a bound on the expected returns of two policies under the and introduces a "robust MDP" view of a 2 player zero-sum game played between the agent and nature. <sep>	abstract
2019-1612	R3 found the paper well-structured and the results presented therein interesting.	strength
2019-1612	R2 complained of overly heavy notation and questioned the applicability of the results, as well as the utility of the robust MDP perspective (though did raise their score following revisions). <sep>	weakness
2019-1612	The most detailed critique came from R1, who suggested that the bound on the convergence of returns of two policies as the KL divergence between their induced distributions decreases is unsurprising, that using it to argue for AGZ's convergence to the optimal policy ignores the effects introduced by the suboptimality of the MCTS policy (while really interesting part being understanding how AGZ deals with, and whether or not it closes, this gap), and that the "robust MDP" view is less novel than the authors claim based on the known relationships between 2 player zero-sum games and minimax robust control. <sep>	weakness
2019-1612	I find R1's complaints, in particular with respect to "robust MDPs" (a criticism which went completely unaddressed by the authors in their rebuttal), convincing enough that I would narrowly recommend rejection at this time, while also agreeing with R3 that this is an interesting subject and that the results within could serve as the bedrock for a stronger future paper.	decision

2019-1619	The paper proposes an attention mechanism to focus on robust features in the context of adversarial attacks.	abstract
2019-1619	Reviewers asked for more intuition, more results, and more experiments with different attack/defense models.	weakness
2019-1619	Authors have added experimental results and provided some intuition of their proposed approach.	rebuttal_process
2019-1619	Overall, reviewers still think the novelty is too thin and recommend rejection.	rating_summary
2019-1619	I concur with them.	decision

2019-1620	Reviewers are in a consensus and recommended to reject after engaging with the authors.	rating_summary
2019-1620	Please take reviewers' comments into consideration to improve your submission should you decide to resubmit.	suggestion

2019-1628	Strengths: <sep> -- Solid experiments <sep>	strength
2019-1628	-- The paper is well written <sep>	strength
2019-1628	Weaknesses: <sep> -- The findings are not entirely novel and not so surprising, previous papers (eg, Brevlins et al (ACL 2018)) have already suggested that LM objectives are preferable and also using LM objective for pretraining is already the standard practice (see details in R1 and R3). <sep>	weakness
2019-1628	There is a consensus between the two reviewers who provided detailed comments and engaged in discussion with the authors.	misc

2019-1654	The paper studies the convergence of a primal-dual algorithm on a special min-max problem in WGAN where the maximization is with respect to linear variables (linear discriminator) and minimization is over non-convex generators.	abstract
2019-1654	Experiments with both simulated and real world data are conducted to show that the algorithm works for WGANs and multi-task learning. <sep>	abstract
2019-1654	The major concern of reviewers lies in that the linear discriminator assumption in WGAN is too restrictive to general non-convex mini-max saddle point problem in GANs.	weakness
2019-1654	Linear discriminator implies that the maximization part in min-max problem is concave, and it is thus not surprise that under this assumption the paper converts the original problem to a non-convex optimization instance and proves its first order convergence with descent lemma.	weakness
2019-1654	This technique however can't be applied to general non-convex saddle point problem in GANs.	weakness
2019-1654	Also the experimental studies are also not strong enough.	weakness
2019-1654	Therefore, current version of the paper is proposed as borderline lean reject.	decision

2019-1659	The paper adds a new level of complexity to neural networks, by modulating activation functions of a layer as a function of the previous layer activations.	abstract
2019-1659	The method is evaluated on relatively simple vision and language tasks. <sep>	abstract
2019-1659	The idea is nice, but seems to be a special case of previously published work; and the results are not convincing.	weakness
2019-1659	Four of five reviewers agree that the work would benefit from: improving comparisons with existing approaches, but also improving its theoretical framework, in light of competing approaches.	suggestion

2019-1666	The paper presents a GAN for learning a target distribution that is defined as the difference between two other distributions. <sep>	abstract
2019-1666	The reviewers and AC note the critical limitation of novelty and appealing results of this paper to meet the high standard of *CONF*. <sep>	weakness
2019-1666	AC thinks the proposed method has potential and is interesting, but decided that the authors need more works to publish.	decision

2019-1668	This paper presents an interesting strategy of curriculum learning for training neural networks, where mini-batches of samples are formed with a gradually increasing level of difficulty. <sep>	abstract
2019-1668	While reviewers acknowledge the importance of studying the curriculum learning and the potential usefulness of the proposed approach for training neural networks, they raised several important concerns that place this paper bellow the acceptance bar: (1) empirical results are not convincing (R2, R3); comparisons on other datasets (large-scale) and with state-of-the-art methods would substantially strengthen the evaluation (R3); see also R2's concerns regarding the comprehensive study; (2) important references and baseline methods are missing – see R2's suggestions how to improve; (3) limited technical novelty -- R1 has provided a very detailed review questioning novelty of the proposed approach wrt Weinshall et al, 2018. <sep>	weakness
2019-1668	Another suggestions to further strengthen and extend the manuscript is to consider curriculum and anti-curriculum learning for increasing performance (R1). <sep>	suggestion
2019-1668	The authors provided additional experiment on a subset of 7 classes from the  ImageNet dataset, but this does not show the advantage of the proposed model in a large-scale learning setting. <sep>	rebuttal_process
2019-1668	The AC decided that addressing (1)-(3) is indeed important for understanding the contribution in this work, and it is difficult to assess the scope of the contribution without addressing them.	suggestion

2020-2	The authors addressed the issues raised by the reviewers; I suggest to accept this paper.	decision

2020-14	This paper studies an interesting new problem, federated domain adaptation, and proposes an approach based on dynamic attention, federated adversarial alignment, and representation disentanglement. <sep>	abstract
2020-14	Reviewers generally agree that the paper contributes a novel approach to an interesting problem with theoretical guarantees and empirical justification.	strength
2020-14	While many professional concerns were raised by the reviewers, the authors managed to perform an effective rebuttal with a major revision, which addressed the concerns convincingly.	rebuttal_process
2020-14	AC believes that the updated version is acceptable. <sep>	decision
2020-14	Hence I recommend acceptance.	decision

2020-16	The authors propose a scale-invariant sparsity measure for deep networks.	abstract
2020-16	The experiments are extensive and convincing, according to reviewers.	strength
2020-16	I recommend acceptance.	decision

2020-19	This paper proposes a new gradient-based stochastic optimization algorithm by adapting theory for proximal algorithms to the non-convex setting. <sep>	abstract
2020-19	The majority of reviewers voted for accept.	rating_summary
2020-19	The authors are encouraged to revise with respect to reviewer comments.	suggestion

2020-27	The paper proposes a black box algorithm for MRF training, utilizing a novel approach based on variational approximations of both the positive and negative phase terms of the log likelihood gradient (as R2 puts it, "a fairly creative combination of existing approaches"). <sep>	abstract
2020-27	Several technical and rhetorical points were raised by the reviewers, most of which seem to have been satisfactorily addressed, but all reviewers agreed that this was a good direction.	rebuttal_process
2020-27	The main weakness of the work is that the empirical work is very small scale, mainly due to the bottleneck imposed by an inner loop optimization of the variational distribution q(v, h).	weakness
2020-27	I believe it's important to note that most truly large scale results in the literature revolve around purely feedforward models that don't require expensive to compute approximations; that said, MNIST experiments would have been nice. <sep>	weakness
2020-27	Nevertheless, this work seems like a promising step on a difficult problem, and it seems that the ideas herein are worth disseminating, hopefully stimulating future work on rendering this procedure less expensive and more scalable.	strength

2020-37	The paper proposes a novel model-free solution to POMDPs, which proposes a unified graphical model for hidden state inference and max entropy RL.	abstract
2020-37	The method is principled and provides good empirical results on a set of experiments that relatively comprehensive.	strength
2020-37	I would have liked to see more POMDP tasks instead of Atari, but the results are good.	strength
2020-37	Overall this is good work.	misc

2020-74	The reviewers are unanimous in their opinion that this paper offers a novel approach to secure edge learning.	strength
2020-74	I concur.	misc
2020-74	Reviewers mention clarity, but I find the latest paper clear enough.	ac_disagreement

2020-76	This paper studies the effectiveness of self-supervised approaches by characterising how much information they can extract from a given dataset of images on a per-layer basis.	abstract
2020-76	Based on an empirical evaluation of RotNet, BiGAN, and DeepCluster, the authors argue that the early layers of CNNs can be effectively learned from a single image coupled with strong data augmentation.	abstract
2020-76	Secondly, the authors also provide some empirical evidence that supervision might still necessary to learn the deeper layers (even in the presence of millions of images for self-supervision). <sep>	abstract
2020-76	Overall, the reviews agree that the paper is well written and timely given the growing popularity of self-supervised methods.	strength
2020-76	Given that most of the issues raised by the reviewers were adequately addressed in the rebuttal, I will recommend acceptance.	decision
2020-76	We ask the authors to include additional experiments requested by the reviewers (they are valuable even if the conclusions are not perfectly aligned with the main message).	suggestion

2020-80	This paper proposes RTFM, a new model in the field of language-conditioned policy learning.	abstract
2020-80	This approach is promising and important in reinforcement learning because of the difficulty to learn policies in new environments. <sep>	abstract
2020-80	Reviewers appreciate the importance of the problem and the effective approach.	strength
2020-80	After the author response which addressed some of the major concerns, reviewers feel more positive about the paper.	rebuttal_process
2020-80	They comment, though, that presentation could be clearer, and the limitations of using synthetic data should be discussed in depth. <sep>	rebuttal_process
2020-80	I thank the authors for submitting this paper.	misc

2020-90	This paper demonstrates that for deep RL problems one can construct adversarial examples where the examples don't really need to be even better than the best opponent.	abstract
2020-90	Surprisingly, sometimes, the adversarial opponent is less capable than normal opponents which the victim plays successfully against, yet they can disrupt the policies.	abstract
2020-90	The authors present a physically realistic threat model and demonstrate that adversarial policies can exist in this threat model. <sep>	abstract
2020-90	The reviewers agree with this paper presents results (proof of concept) that is "timely" and the RL community will benefit from this result.	strength
2020-90	Based on reviewers comment, I recommend to accept this paper.	decision

2020-102	The submission proposes a robustness certification technique for smoothed classifiers for a given l_2 attack radius. <sep>	abstract
2020-102	Strengths: <sep> -The majority opinion is that this work is a non-trivial extension of prior work to provide radius certification. <sep>	strength
2020-102	-The work is more efficient that strong recent baselines and provides better performance. <sep>	strength
2020-102	-It successfully achieves this while avoiding adversarial training, which is another novel aspect. <sep>	strength
2020-102	Weaknesses: <sep> -There were some initial concerns about missing experiments and unfair comparisons but these were sufficiently addressed in the discussion. <sep>	rebuttal_process
2020-102	AC shares the majority opinion and recommends acceptance.	decision

2020-122	This paper proposes an ensemble method to identify noisy labels in the training data of supervised learning.	abstract
2020-122	The underlying hypothesis is that examples with label noise require memorization.	abstract
2020-122	The paper proposes methods to identify and remove bad training examples by retaining only the training data that maintains low losses after perturbations to the model parameters.	abstract
2020-122	This idea is developed in several candidate ensemble algorithms.	abstract
2020-122	One of the proposed ensemble methods exceeds the performance of state-of-the-art methods on MNIST, CIFAR-10 and CIFAR-100. <sep>	abstract
2020-122	The reviewers found several strengths and a few weaknesses in the paper.	misc
2020-122	The paper was well motivated and clear.	strength
2020-122	The proposed solution was novel and plausible.	strength
2020-122	The experiments were comprehensive.	strength
2020-122	The reviewers identified several parts of the paper that could be more clear or where more detail could be provided, including a complexity analysis and extended experiments.	weakness
2020-122	The author response addressed the reviewer questions directly and also in a revised document.	rebuttal_process
2020-122	In the discussion phase, the reviewers were largely satisfied that their concerns were addressed. <sep>	rebuttal_process
2020-122	This paper should be accepted for publication as the paper presents a clear problem and solution method along with convincing evidence of method's merits.	decision

2020-124	The paper extends the work on randomized smoothing for certifiably robust classifiers developed in prior work to a weaker specification requiring that the set of top-k predictions remain unchanged under adversarial perturbations of the input (rather than just the top-1).	abstract
2020-124	This enables the authors to achieve stronger results on robustness of classifiers on CIFAR10 and ImageNet (where the authors report the top-5 accuracy). <sep>	abstract
2020-124	This is an interesting extension of certified defenses that is likely to be relevant for complex prediction tasks with several classes (ImageNet and beyond), where top-1 robustness may be difficult and unrealistic to achieve. <sep>	strength
2020-124	The reviewers were in consensus on acceptance and minor concerns were alleviated during the rebuttal phase. <sep>	rating_summary
2020-124	I therefore recommend acceptance.	decision

2020-142	This paper proposes an extension to deterministic autoencoders, namely instead of noise injection in the encoders of VAEs to use deterministic autoencoders with an explicit regularization term on the latent representations.	abstract
2020-142	While the reviewers agree that the paper studies an important question for the generative modeling community, the paper has been limited in terms of theoretical analysis and experimental validation.	weakness
2020-142	The authors, however, provided further experimental results to support the claims empirically during the discussion period and the reviewers agree that the paper is now acceptable for publication in *CONF*-2020.	rating_summary

2020-148	This paper introduces an algorithm for online Bayesian learning of both streaming and non-stationary data.	abstract
2020-148	The algorithmic choices are heuristic but motivated by sensible principles.	strength
2020-148	The reviewers' main concerns were with novelty, but because the paper was well-written and addressing an important problem they all agreed it should be accepted.	rating_summary

2020-172	Main content: BasiGAN, a novel method for  introducing stochasticity in conditional GANs <sep>	abstract
2020-172	Summary of discussion: <sep> reviewer1: interesting work and results on GANs.	strength
2020-172	Reviewer had a question on pre-defned basis but i think it was answered by the authors. <sep>	rebuttal_process
2020-172	reviewer3: interesting and novel work on GANS, wel-written paper and improves on SOTA.	strength
2020-172	The main uestion is around bases again like reviewer 1, but it seems the authors have addressed this. <sep>	rebuttal_process
2020-172	reviewer4: Novel interesting work.	strength
2020-172	Main comments are around making Theorem 1 more theoretically correct, which it sounds like the authors addressed. <sep>	rebuttal_process
2020-172	Recommendation: Poster.	decision
2020-172	Well written and novel paper and authors addressed a lot of concerns.	strength

2020-187	This paper is consistently supported by all three reviewers during initial review and discussions.	rating_summary
2020-187	Thus an accept is recommended.	decision

2020-188	All three reviewers gave scores of Weak Accept.	rating_summary
2020-188	AC has read the reviews and rebuttal and agrees that the paper makes a solid contribution and should be accepted.	decision

2020-189	This paper proposes a new method to learning heuristics for quantified boolean formulas through RL.	abstract
2020-189	The focus is on a method called backtracking search algorithm.	abstract
2020-189	The paper proposes a new representation of formulas to scale the predictions of this method. <sep>	abstract
2020-189	The reviewers have an overall positive response to this paper.	rating_summary
2020-189	R1 and R2 both agree that the paper should be accepted, and have given some minor feedback to improve the paper.	rating_summary
2020-189	R3 initially was critical of the paper, but the rebuttal helped to clarify their doubt.	rebuttal_process
2020-189	They still have one more comment and I encourage the authors to address this in the final version of the paper. <sep>	rebuttal_process
2020-189	R3 meant to increase their score but somehow this is not reflected in the current score.	rebuttal_process
2020-189	Based on their comments though, I am assuming the scores to be 6,8,6 which makes the cut for *CONF*.	rating_summary
2020-189	Therefore, I recommend to accept this paper.	decision

2020-197	This paper presents an unsupervised method for completing point clouds obtained from real 3D scans based on GAN.	abstract
2020-197	Generally, the paper is well-organized, and its contributions and experimental supports are clearly presented, from which all reviewers got positive impressions. <sep>	strength
2020-197	Although the technical contribution of the method seems marginal as it is essentially a combination of established methods, it well fits in a novel and practical application scenario, and its useful is convincingly demonstrated in intensive experiments.	strength
2020-197	We conclude that the paper provides favorable insights covering the weakness in technical novelty, so I'd like to recommend acceptance.	decision

2020-204	All reviewers come to agreement that this is a solid paper worth publishing at *CONF*; the authors are encouraged to incorporate additional comments suggested by reviewers.	rating_summary

2020-206	This paper introduces the problem of overlearning, which can be thought of as unintended transfer learning from a (victim) source model to a target task that the source model's creator had not intended its model to be used for.	abstract
2020-206	The paper raises good points about privacy legislation limitations due to the fact that overlearning makes it impossible to foresee future uses of a given dataset. <sep>	abstract
2020-206	Please incorporate the revisions suggested in the reviews to add clarity to the overlearning versus censoring confusion addressed by the reviewers.	suggestion

2020-215	This paper focuses on studying neural network-based denoising methods.	abstract
2020-215	The paper makes the interesting observation that most existing denoising approaches have a tendency to overfit to knowledge of the noise level.	abstract
2020-215	The authors claim that simply removing the bias on the network parameters enables a variety of improvements in this regard and provide some theoretical justification for their results.	abstract
2020-215	The reviewers were mostly postive but raised some concerns about generalization beyond Gaussian noise and not "being very well theoretically motivated".	rating_summary
2020-215	These concerns seem to have at least partially been alleviated during the discussion period.	rebuttal_process
2020-215	I agree with the reviewers.	misc
2020-215	I think the paper looks at an important phenomena for denoising (role of variance parameter) and is well suited to *CONF*.	strength
2020-215	I recommend acceptance.	decision
2020-215	I suggest that the authors continue to further improve the paper based on the reviewers' comments.	suggestion

2020-228	This paper provides a surprising result: that randomization and FGSM can produce robust models faster than previous methods given the right mix of cyclic learning rate, mixed precision, etc.	abstract
2020-228	This paper produced a fair bit of controversy among both the community and the reviewers to the point where there were suggestions of bugs, evaluation problems, and other issues leading to the results.	misc
2020-228	In the end, the authors released the code (and made significant updates to the paper based on all the feedback).	rebuttal_process
2020-228	Multiple reviewers checked the code and were happy.	misc
2020-228	There was an extensive author response, and all the reviewers indicated that their primary concerns were address, save concerns about the sensitivity of step-size and the impact of early stopping. <sep>	rebuttal_process
2020-228	Overall, the paper is well written and clear.	strength
2020-228	The proposed approach is simple and well explained.	strength
2020-228	The result is certainly interesting, and this paper will continue to generate fruitful debate.	strength
2020-228	There are still things to address to improve the paper, listed above.	misc
2020-228	I strongly encourage the authors to continue to improve the work and make a more concerted effort to carefully discuss the impacts of early stopping.	suggestion

2020-230	The authors design a deep model architecture for learning word embeddings with better performance and/or more efficient use of parameters.	abstract
2020-230	Results on language modeling and machine translation are promising.	strength
2020-230	Pros:  Interesting idea and nice results.	strength
2020-230	New model may have some independent value beyond NLP.	strength
2020-230	Cons:  Empirical comparisons could be more thorough.	weakness
2020-230	For example, it is not clear (to me at least) what would be the benefits of this approach applied to whole words versus a competitor using subword units.	weakness

2020-239	This paper describes a method for bounding the confidence around predictions made by deep networks.	abstract
2020-239	Reviewers agree that this result is of technical interest to the community, and with the added reorganization and revisions described by the authors, they and the AC agree the paper should be accepted.	decision

2020-242	The authors propose a way to recover latent factors implicitly constructed by a neural net with black box access to the nets output.	abstract
2020-242	This can be useful for identifying possible adversarial attacks.	abstract
2020-242	The majority of reviewers agrees that this is a solid technical and experimental contribution.	strength

2020-247	The submission presents an approach to single-view 3D reconstruction.	abstract
2020-247	The approach is quite creative and involves predicting the weights of a network that is then applied to a point set.	abstract
2020-247	The presentation is good.	strength
2020-247	The experimental protocol is well-informed and the results are convincing.	strength
2020-247	The reviewers' concerns have largely been addressed by the authors' responses and the revision.	rebuttal_process
2020-247	In particular, R2, who gave a "3", posted "I would now advise to raise my score (3 previously) to a be in line with the 6: Weak Accept given by the other reviewers."	rebuttal_process
2020-247	This means that all three reviewers recommend accepting the paper.	rating_summary
2020-247	The AC agrees.	decision

2020-253	Thanks for an interesting discussion.	misc
2020-253	The authors present a supposedly task-independent evaluation metric for generation tasks with references that relies on BERT or similar pretrained language models and a BERT-internal alignment.	abstract
2020-253	Reviewers are moderately positive.	rating_summary
2020-253	I encourage the authors to think about a) whether their approach scales to language pairs where wordpieces are less comparable; b) whether second order similarly, eg, using RSA, would be better than alignment-based similarity; c) whether this metric works in the extremes, eg, can it distinguish between bad output and super-bad output (where in both cases alignment may be impossible), and can it distinguish between good output and super-good output (where BERT scores may be too biased by BERT's training objective).	suggestion

2020-261	This paper considers deep reinforcement learning skill transfer and composition, through an attention model that weighs the contributions of several base policies conditioned on the task and state, and uses this to output an action.	abstract
2020-261	The method is evaluated on several Mujoco tasks. <sep>	abstract
2020-261	There were two main areas of concern.	misc
2020-261	The first was around issues with using equivalent primitives and training times for comparison methods.	weakness
2020-261	The second was around the general motivation of the paper, and also the motivation for using a BiRNN.	weakness
2020-261	These issues were resolved in a comprehensive discussion, leaving this as an interesting paper that should be accepted.	decision

2020-262	All reviewers found the work interesting but worried about the extension to non-bilinear games.	weakness
2020-262	This is a point the authors should explicitly address in their work before publication.	decision

2020-274	This paper studies numerous ways in which the statistics of network weights evolve during network training.	abstract
2020-274	Reviewers are not entirely sure what conclusions to make from these studies, and training dynamics can be strongly impacted by arbitrary choices made in the training process.	weakness
2020-274	Despite these issues, the reviewers think the observed results are interesting enough to clear the bar for publication.	rating_summary

2020-285	This paper tackles the problem of regret minimization in a multi-agent bandit problem, where distributed learning bandit algorithms collaborate in order to minimize their total regret.	abstract
2020-285	More specifically, the work focuses on efficient communication protocols and the regret corresponds to the communication cost.	abstract
2020-285	The goal is therefore to design protocols with little communication cost.	abstract
2020-285	The authors first establish lower bounds on the communication cost, and then introduce an algorithm with provable near-optimal regret. <sep>	abstract
2020-285	The only concern with the paper is that *CONF* may not be the appropriate venue given that this work lacks representation learning contributions.	weakness
2020-285	However, all reviewers being otherwise positive about the quality and contributions of this work, I would recommend acceptance.	decision

2020-293	The authors propose a novel model-based reinforcement learning algorithm.	abstract
2020-293	The key difference with previous approaches is that the authors use gradients through the learned model.	abstract
2020-293	They present theoretical results on error bounds for their approach and a monotonic improvement theorem.	abstract
2020-293	In the small sample regime, they show improved performance over previous approaches. <sep>	abstract
2020-293	After the revisions, reviewers raised a few concerns: <sep> The results are only for 100,000 steps, which does not support the claim that the models achieves the same asymptotic performance as model – free algorithms would. <sep>	rebuttal_process
2020-293	The results would be stronger as the experiments were run with more than 3 random seats. <sep>	rebuttal_process
2020-293	In the revised version of the text, it's unclear if the authors are using target networks. <sep>	rebuttal_process
2020-293	Overall, I think the paper introduces some interesting ideas and shows improved performance over existing approaches.	strength
2020-293	I recommend acceptance on the condition that the authors tone down their claims or back them up with empirical evidence.	decision
2020-293	Currently, I don't see evidence for the claim that the method achieves similar asymptotic performance to model free algorithms or the claim that their approach allows for longer horizons than previous approaches.	weakness

2020-305	The major contribution of this paper is the use of random Fourier features as temporal (positional) encoding for dynamic graphs.	abstract
2020-305	The reviewers all find the proposed method interesting, and believes that this is a paper with reasonable contributions.	strength
2020-305	One comment pointed out that the connection between Time2Vec and harmonic analysis has been discussed in the previous work, and we suggest the authors to include this discussion/comparison in the paper.	suggestion

2020-306	This paper proposed an improved ensemble method called BatchEnsemble, where the weight matrix is decomposed as the element-wise product of a shared weigth metrix and a rank-one matrix for each member.	abstract
2020-306	The effectiveness of the proposed methods has been verified by experiments on a list of various tasks including image classification, machine translation, lifelong learning and uncertainty modeling.	abstract
2020-306	The idea is simple and easy to follow.	strength
2020-306	Although some reviewers thought it lacks of in-deep analysis, I would like to see it being accepted so the community can benefit from it.	decision

2020-319	This paper proposes a method, SNOW, for improving the speed of training and inference for transfer and lifelong learning by subscribing the target delta model to the knowledge of source pretrained model via channel pooling. <sep>	abstract
2020-319	Reviewers and AC agree that this paper is well written, with simple but sound technique towards an important problem and with promising empirical performance.	strength
2020-319	The main critique is that the approach can only tackle transfer learning while failing in the lifelong setting.	weakness
2020-319	Authors provided convincing feedbacks on this key point.	rebuttal_process
2020-319	Details requested by the reviewers were all well addressed in the revision. <sep>	rebuttal_process
2020-319	Hence I recommend acceptance.	decision

2020-332	The paper lies on the borderline.	misc
2020-332	An accept is suggested based on majority reviews and authors' response.	decision

2020-341	Four knowledgable reviewers recommend accept.	rating_summary
2020-341	Good job!	misc

2020-346	The authors introduce an approach to learn a random forest model and a representation simultaneously.	abstract
2020-346	The basic idea is to modify the representation so that subsequent trees in the random forest are less correlated.	abstract
2020-346	The authors evaluate the technique empirically and show some modest gains.	abstract
2020-346	While the reviews were mixed, the approach is quite different from the usual approaches published at *CONF*  and so I think it's worth highlighting this work.	decision

2020-356	This paper deals with the under-sensitivity problem in natural language inference tasks.	abstract
2020-356	An interval bound propagation (IBP) approach is applied to predict the confidence of the model when a subsets of words from the input text are deleted.	abstract
2020-356	The paper is well written and easy to follow.	strength
2020-356	The authors give detailed rebuttal and 3 of the 4 reviewers lean to accept the paper.	rating_summary

2020-361	The paper studies out-of-sample generalisation that require an agent to respond to never-seen-before instructions by manipulating and positioning objects in a 3D Unity simulated room, and analyzes factors which promote combinatorial generalization in such environment. <sep>	abstract
2020-361	The paper is a very thought provoking work, and would make a valuable contribution to the line of works on systematic generalization in embodied agents.	strength
2020-361	The draft has been improved significantly after the rebuttal.	rebuttal_process
2020-361	After the discussion, we agree that it is worthwhile presenting at *CONF*.	decision

2020-371	A new method of calculating saliency maps for deep networks trained through RL (for example to play games) is presented.	abstract
2020-371	The method is aimed at explaining why moves were taken by showing which salient features influenced the move, and seems to work well based on experiments with Chess, Go, and several Atari games. <sep>	abstract
2020-371	Reviewer 2 had a number of questions related to the performance of the method under various conditions, and these were answered satisfactorily by the reviewers. <sep>	rebuttal_process
2020-371	This is a solid paper with good reasoning and results, though perhaps not super novel, as the basic idea of explaining policies with saliency is not new.	strength
2020-371	It should be accepted for poster presentation.	decision

2020-383	This paper first discusses some concepts related to disentanglement.	abstract
2020-383	The authors propose to decompose disentanglement into two distinct concepts: consistency and restrictiveness.	abstract
2020-383	Then, a calculus of disentanglement is introduced to reveal the relationship between restrictiveness and consistency.	abstract
2020-383	The proposed concepts are applied to analyze weak supervision methods. <sep>	abstract
2020-383	The reviewers ultimately decided this paper is well-written and has content which is of general interest to the *CONF* community.	strength

2020-385	This paper provides a valuable survey, summary, and empirical comparison of many generalization quantities from throughout the literature.	strength
2020-385	It is comprehensive, thorough, and will be useful to a variety of researchers (both theoretical and applied).	strength

2020-392	This paper theoretically studied one of the fundamental issue in CycleGAN (recently gained much attention for image-to-image translation).	abstract
2020-392	The authors analyze the space of exact and approximated solutions under automorphisms. <sep>	abstract
2020-392	Reviewers mostly agree with theoretical value of the paper.	strength
2020-392	Some concerns on practical values are also raised, eg, limited or no-surprising experimental results.	weakness
2020-392	In overall, I think this is a boarderline paper.	misc
2020-392	But, I am a bit toward acceptance as the theoretical contribution is solid, and potentially beneficial to many future works on unpaired image-to-image translation.	decision

2020-405	The paper describes a method to train a convolutional network with large capacity, where channel-gating (input conditioned) is implemented - thus, only parts of the network are used at inference time.	abstract
2020-405	The paper builds over previous work, with the main contribution being a "batch-shaping" technique that regularizes the channel gating to follow a beta distribution, combined with L0 regularization.	abstract
2020-405	The paper shows that ResNet trained with this technique can achieve higher accuracy with lower theoretical MACs.	abstract
2020-405	Weakness of the paper is that more engineering would be required to convert the theoretical MACs into actual running time - which would further validate the practicality of the approach.	weakness

2020-410	The paper proposes an approach for N-D continuous convolution on unordered particle set and applies it to Lagrangian fluid simulation.	abstract
2020-410	All reviewers found the paper to be a novel and useful contribution towards the problem of N-D continuous convolution on unordered particles.	strength
2020-410	I recommend acceptance.	decision

2020-422	The paper makes a reasonable contribution to generative modeling for unsupervised scene decomposition.	strength
2020-422	The revision and rebuttal addressed the primary criticisms concerning the qualitative comparison and clarity, which caused some of the reviewers to increase their rating.	rebuttal_process
2020-422	I think the authors have adequately addressed the reviewer concerns.	rebuttal_process
2020-422	The final version of the paper should still strive to improve clarity, and strengthen the evaluation and ablation studies.	suggestion

2020-424	The paper proposes an embedding for nodes in a directed graph, which takes into account the asymmetry.	abstract
2020-424	The proposed method learns an embedding of a node as an exponential distribution (eg Gaussian), on a statistical manifold.	abstract
2020-424	The authors also provide an approximation for large graphs, and show that the method performs well in empirical comparisons. <sep>	abstract
2020-424	The authors were very responsive in the discussion phase, providing new experiments in response to the reviews.	rebuttal_process
2020-424	This is a nice example where a good paper is improved by several extra suggestions by reviewers.	misc
2020-424	I encourage the authors to provide all the software for reproducing their work in the final version. <sep>	suggestion
2020-424	Overall, this is a great paper which proposes a new graph embedding approach that is scalable and provides nice empirical results.	strength

2020-435	This paper proposes a learning framework for spiking neural networks that exploits the sparsity of the gradient during backpropagation to reduce the computational cost of training.	abstract
2020-435	The method is evaluated against prior works that use full precision gradients and shown comparable performance.	abstract
2020-435	Overall, the contribution of the paper is solid, and after a constructive rebuttal cycle, all reviewers reached a consensus of weak accept.	rating_summary
2020-435	Therefore, I recommend accepting this submission.	decision

2020-448	Three reviewers have assessed this paper and they have scored it 6/6/6 after rebuttal.	rating_summary
2020-448	Nonetheless, the reviewers have raised a number of criticisms and the authors are encouraged to resolve them for the camera-ready submission.	suggestion

2020-471	Four reviewers have assessed this paper and they have scored it as 6/6/6/6 after rebuttal.	rating_summary
2020-471	Nonetheless, the reviewers have raised a number of criticisms and the authors are encouraged to resolve them for the camera-ready submission.	suggestion

2020-475	This is a nice paper on the classical problem of universal approximation, but giving a direct proof with good approximation rates, and providing many refinements and ties to the literature. <sep>	strength
2020-475	If possible, I urge the authors to revise the paper further for camera ready; there are various technical oversights (eg, 1/lambda should appear in the approximation rates in theorem 3.1), and the proof of theorem 3.1 is an uninterrupted 2.5 page block (splitting it into lemmas would make it cleaner, and also those lemmas could be useful to other authors).	suggestion

2020-478	This paper proposes to discover causal mechanisms through meta-learning, and suggests an approach for doing so.	abstract
2020-478	The reviewers raised concerns about the key hypothesis (that the right causal model implies higher expected online likelihood) not being sufficiently backed up through theory or through experiments on real data.	weakness
2020-478	The authors pointed to a recent paper that builds upon this work and tests on a more realistic problem setting.	rebuttal_process
2020-478	However, the newer paper measures not the online likelihood of adaptation, but just the training error during adaptation, suggesting that the approach in this paper may be worse.	rebuttal_process
2020-478	Despite the concerns, the reviewers generally agreed that the paper included novel and interesting ideas, and addressed a number of the reviewers' other concerns about the clarity, references, and experiments.	rebuttal_process
2020-478	Hence, it makes a worthwhile contribution to *CONF*.	misc

2020-500	The paper proposes a simple and effective way to stabilize training by adding consistency term to discriminator.	abstract
2020-500	Given the stochastic augmentation procedure T(x) the loss is just a penalty on D. The main unsolved question why it help to make discriminator "smoother" in the consistency case for a standard GAN (since typically, no constraints are enforced).	weakness
2020-500	Nevertheless, at the moment this a working heuristics that gives new SOTA, and that is the main strength.	strength
2020-500	The reviewer all agree to accept, and so do I.	decision

2020-501	This paper presents a number of improvements on existing approaches to neural logic programming.	abstract
2020-501	The reviews are generally positive: two weak accepts, one weak reject.	rating_summary
2020-501	Reviewer 2 seems wholly in favour of acceptance at the end of discussion, and did not clarify why they were sticking to their score of weak accept.	rating_summary
2020-501	The main reason Reviewer 1 sticks to 6 rather than 8 is that the work extends existing work rather than offering a "fundamental contribution", but otherwise is very positive.	rating_summary
2020-501	I personally feel that <sep> a) most work extends existing work <sep>	strength
2020-501	b) there is room in our conferences for such well executed extensions (standing on the shoulders of giants etc). <sep>	misc
2020-501	Reviewer 3 is somewhat unconvinced by the nature of the evaluation.	weakness
2020-501	While I understand their reservations, they state that they would not be offended by the paper being accepted in spite of their reservations. <sep>	misc
2020-501	Overall, I find that the review group leans more in favour of acceptance, and an happy to recommend acceptance for the paper as it makes progress in an interesting area at the intersection of differentiable programming and logic-based programming.	decision

2020-507	The paper provides a language for optimizing through physical simulations.	abstract
2020-507	The reviewers had a number of concerns related to paper organization and insufficient comparisons to related work (jax).	weakness
2020-507	During the discussion phase, the authors significantly updated their paper and ran additional experiments, leading to a much stronger paper.	rebuttal_process

2020-511	The authors study neural networks with binary weights or activations, and the so-called "differentiable surrogates" used to train them. <sep>	abstract
2020-511	They present an analysis that unifies previously proposed surrogates and they study critical initialization of weights to facilitate trainability. <sep>	abstract
2020-511	The reviewers agree that the main topic of the paper is important (in particular initialization heuristics of neural networks), however they found the presentation of the content lacking in clarity as well as in clearly emphasizing the main contributions. <sep>	weakness
2020-511	The authors imporved the readability of the manuscript in the rebuttal. <sep>	rebuttal_process
2020-511	This paper seems to be at acceptance threshold and 2 of 3 reviewers indicated low confidence. <sep>	rating_summary
2020-511	Not being familiar with this line of work, I recommend acceptance following the average review score.	decision

2020-523	This paper introduces a way to measure dataset similarities.	abstract
2020-523	Reviewers all agree that this method is novel and interesting.	strength
2020-523	A few questions initially raised by reviewers regarding models with and without likelihood, geometric exposition, and guarantees around GW, are promptly answered by authors, which raised the score to all weak accept.	rating_summary

2020-570	The paper proposes a doubly robust off-policy evaluation method that uses both stationary density ratio as well as a learned value function in order to reduce bias. <sep>	abstract
2020-570	The reviewers unanimously recommend acceptance of this paper.	rating_summary

2020-581	The authors challenge the idea that good representation in RL lead are sufficient for learning good policies with an interesting negative result -- they show that there exist MDPs which require an exponential number of samples to learn a near-optimal policy even if a good-but-not-perfect representation is given to the agent for both value-based and policy-based learning.	abstract
2020-581	Reviewers had some minor technical questions which were clarified sufficiently by the authors, leading to a consensus of the contribution and quality of this work.	rebuttal_process
2020-581	Thus, I recommend this paper for acceptance.	decision

2020-592	The paper proposes a new algorithm for adversarial training of language models.	abstract
2020-592	This is an important research area and the paper is well presented, has great empirical results and a novel idea.	strength

2020-611	The paper proposes a nice and easy way to regularize spectral graph embeddings, and explains the effect through a nice set of experiments.	strength
2020-611	Therefore, I recommend acceptance.	decision

2020-615	The paper addresses individual fairness scenario (treating similar users similarly) and proposes a new definition of algorithmic fairness that is based on the idea of robustness, ie by perturbing the inputs (while keeping them close with respect to the distance function), the loss of the model cannot be significantly increased. <sep>	abstract
2020-615	All reviewers and AC agree that this work is clearly of interest to *CONF*, however the reviewers have noted the following potential weaknesses: (1) presentation clarity -- see R3's detailed suggestions eg comparison to Dwork et al, see R2's comments on how to improve, (2) empirical evaluations -- see R1's question about using more complex models, see R3's question on the usefulness of the word embeddings. <sep>	weakness
2020-615	Pleased to report that based on the author respond with extra experiments and explanations, R3 has raised the score to weak accept.	rebuttal_process
2020-615	All reviewers and AC agree that the most crucial concerns have been addressed in the rebuttal, and the paper could be accepted - congratulations to the authors!	decision
2020-615	The authors are strongly urged to improve presentation clarity and to include the supporting empirical evidence when preparing the final revision.	suggestion

2020-624	This work studies small but critical subnetworks, called winning tickets, that have very similar performance to an entire network, even with much less training.	abstract
2020-624	They show how to identify these early in the training of the entire network, saving computation and time in identifying them and then overall for the prediction task as a whole. <sep>	abstract
2020-624	The reviewers agree this paper is well-presented and of general interest to the community.	strength
2020-624	Therefore, we recommend that the paper be accepted.	decision

2020-633	The paper proposes a data-driven approach to learning atomic-resolution energy functions.	abstract
2020-633	Experiment results show that the proposed energy function is similar to the state-of-art method (Rosetta) based on physical principles and engineered features. <sep>	abstract
2020-633	The paper addresses an interesting and challenging problem.	strength
2020-633	The results are very promising.	strength
2020-633	It is a good showcase of how ML can be applied to solve an important application problem. <sep>	strength
2020-633	For the final version, we suggest that the authors can tune down some claims in the paper to fairly reflect the contribution of the work.	suggestion

2020-642	The authors develop a framework for off-policy value estimation for infinite horizon RL tasks, for estimating the stationary distribution of a Markov chain.	abstract
2020-642	Reviewers were uniformly impressed by the work, and satisfied by the author response.	rebuttal_process
2020-642	Congratulations!	misc

2020-653	This paper proposes a novel stochastic gradient Markov chain Monte Carlo method incorporating a cyclical step size schedule (cyclical SG-MCMC).	abstract
2020-653	The authors argue that this step size schedule allows the sampler to cross modes (when the step size is large) and locally explore modes (when the step size is smaller).	abstract
2020-653	SG-MCMC is a very promising method for Bayesian deep learning as it is both scalable and easily to incorporate into existing models.	abstract
2020-653	However, the stochastic setting often leads to the sampler getting stuck in a local mode due to a requirement of a small step size (which itself is often due to leaving out the Metropolis-Hastings accept / reject step).	abstract
2020-653	The cyclic learning rate intuitively helps the sampler escape local modes.	abstract
2020-653	This property is demonstrated on synthetic problems in comparison to existing SG-MCMC baselines.	abstract
2020-653	The authors demonstrate improved negative log likelihood on larger scale deep learning benchmarks, which is appreciated as the related literature often restricts experiments to small scale problems.	abstract
2020-653	The reviewers all found the paper compelling and argued for acceptance and thus the recommendation is to accept.	decision
2020-653	Some questions remain for future work.	rebuttal_process
2020-653	eg all experiments were performed using a very low temperature, which implies that the methods are not sampling from the true Bayesian posterior.	rebuttal_process
2020-653	Why is such a low temperature needed for reasonable performance?	rebuttal_process
2020-653	In any case a very nice paper.	misc

2020-654	This paper uses energy based model to interpret standard discriminative classifier and demonstrates that energy based model training of the joint distribution improves calibration, robustness, and out-of-distribution detection while generating samples with better quality than GAN-based approaches.	abstract
2020-654	The reviewers are very excited about this work, and the energy-based perspective of generative and discriminative learning.	strength
2020-654	There is a unanimous agreement to strongly accept this paper after author response.	rating_summary

2020-656	This paper concerns the problem of defending against generative "attacks": that is, falsification of data for malicious purposes through the use of synthesized data based on "leaked" samples of real data.	abstract
2020-656	The paper casts the problem formally and assesses the problem of authentication in terms of the sample complexity at test time and the sample budget of the attacker.	abstract
2020-656	The authors prove a Nash equillibrium exists, derive a closed form for the special case of multivariate Gaussian data, and propose an algorithm called GAN in the Middle leveraging the developed principles, showing an implementation to perform better than authentication baselines and suggesting other applications. <sep>	abstract
2020-656	Reviewers were overall very positive, in agreement that the problem addressed is important and the contribution made is significant.	rating_summary
2020-656	Most criticisms were superficial.	ac_disagreement
2020-656	This is a dense piece of work, and presentation could still be improved.	weakness
2020-656	However this is clearly a significant piece of work addressing a problem of increasing importance, and is worthy of acceptance.	decision

2020-657	The authors present an approach for learning graph embeddings by first fusing the graph to generate a new graph with encodes structural information as well as node attribution information.	abstract
2020-657	They then iteratively merge nodes based spectral similarities to  obtain coarser graphs.	abstract
2020-657	They then use existing methods to learn embeddings from this coarse graph and progressively refine the embeddings to finer graphs.	abstract
2020-657	They demonstrate the performance of their method on standard graph datasets. <sep>	abstract
2020-657	This paper has received positive reviews from all reviewers.	rating_summary
2020-657	The authors did a good job of addressing the reviewers' concerns and managed to convince the reviewers about their contributions.	rebuttal_process
2020-657	I request the authors to take the reviewers suggestions into consideration while preparing the final draft of the paper and recommend that the paper be accepted.	decision

2020-673	Transformer models have proven to be quite successful when applied to a variety of ML tasks such as NLP.	abstract
2020-673	However, the computational and memory requirements can at times be prohibitive, such as when dealing with long sequences.	abstract
2020-673	This paper proposes locality-sensitive hashing to reduce the sequence-length complexity, as well as reversible residual layers to reduce storage requirements.	abstract
2020-673	Experimental results confirm that the performance of Transformer models can be preserved even with these new efficiencies in place, and hence, this paper will likely have significant impact within the community. <sep>	strength
2020-673	Some relatively minor points notwithstanding, all reviewers voted for acceptance which is my recommendation as well.	decision
2020-673	Note that this paper was also vetted by several detailed external commenters.	misc
2020-673	In all cases the authors provided reasonable feedback, and the final revision of the work will surely be even stronger.	rebuttal_process

2020-676	The authors introduce a framework for automatically detecting diverse, self-organized patterns in a continuous Game of Life environment, using compositional pattern producing networks (CPPNs) and population-based Intrinsically Motivated Goal Exploration Processes (POP-IMGEPs) to find the distribution of system parameters that produce diverse, interesting goal patterns. <sep>	abstract
2020-676	This work is really well-presented, both in the paper and on the associated website, which is interactive and features source code and demos.	strength
2020-676	Reviewers agree that it's well-written and seems technically sound.	strength
2020-676	I also agree with R2 that this is an under-explored area and thus would add to the diversity of the program. <sep>	strength
2020-676	In terms of weaknesses, reviewers noted that it's quite long, with a lengthy appendix, and could be a bit confusing in areas.	weakness
2020-676	Authors were responsive to this in the rebuttal and have trimmed it, although it's still 29 pages.	rebuttal_process
2020-676	My assessment is well-aligned with those of R2 and thus I'm recommending accept.	decision
2020-676	In the rebuttal, the authors mentioned several interesting possible applications for this work; it'd be great if these could be included in the discussion. <sep>	rebuttal_process
2020-676	Given the impressive presentation and amazing visuals, I think it could make for a fun talk.	strength

2020-687	The authors presented a Federate Learning algorithm which constructs the global model layer-wise by matching and averaging hidden representations.	abstract
2020-687	They empirically demonstrate their method outperforms existing federated learning algorithms <sep>	strength
2020-687	This paper has received largely positive reviews.	rating_summary
2020-687	Unfortunately one reviewer wrote a very short review but was generally appreciative of the work.	misc
2020-687	Fortunately, R1 wrote a detailed review with very specific questions and suggestions.	misc
2020-687	The authors have addresses most of the concerns of the reviewers and I have no hesitation in recommending that this paper should be accepted.	decision
2020-687	I request the authors to incorporate all suggestions made by the reviewers.	suggestion

2020-693	This paper proves that fully-connected wide ReLU-NNs trained with squared loss can be decomposed into two parts: (1) the minimum complexity solution of an interpolating kernel method, and (2) a term depends heavily on the initialization.	abstract
2020-693	The main concerns of the reviewers include (1) the contribution are not significant at all given prior work; (2) flawed proof,  and (3) lack the comparison with prior work.	weakness
2020-693	Even the authors addressed some of the concerns in the revision, it still does not gather sufficient support from the reviewers after author response.	rebuttal_process
2020-693	Thus I recommend reject.	decision

2020-706	This paper makes a claim that the iid assumption for NN parameters does not hold.	abstract
2020-706	The paper then expresses the joint distribution as a Gibbs distribution and PoE.	abstract
2020-706	Finally, there are some results on SGD as VI.	abstract
2020-706	Reviewers have mixed opinion about the paper and it is clear that the starting point of the paper (regarding iid assumption) is unclear.	weakness
2020-706	I myself read through the paper and discussed this with the reviewer, and it is clear that there are many issues with this paper. <sep>	misc
2020-706	Here are my concerns: <sep> - The parameters of DNN are not iid *after* training.	weakness
2020-706	They are not supposed to be.	weakness
2020-706	So the empirical results where the correlation matrix is shown does not make the point that the paper is trying to make. <sep>	weakness
2020-706	- I agree with R2 that the prior is subjective and can be anything, and it is true that the "trained" NN may not correspond to a GP.	weakness
2020-706	This is actually well known which is why it is difficult to match the performance of a trained GP and trained NN. <sep>	weakness
2020-706	- The whole contribution about connection to Gibbs distribution and PoE is not insightful.	weakness
2020-706	These things are already known, so I don't know why this is a contribution. <sep>	weakness
2020-706	- Regarding connection between SGD and VI, they do *not* really prove anything.	weakness
2020-706	The derivation is *wrong*.	weakness
2020-706	In eq 85 in Appendix J2, the VI problem is written as KL(P||Q), but it should be KL(Q||P).	weakness
2020-706	Then this is argued to be the same as eq 88 obtained with SGD.	weakness
2020-706	This is not correct. <sep>	weakness
2020-706	Given these issues and based on reviewers' reaction to the content, I recommend to reject this paper.	decision

2020-713	This paper describes a method for generating adversarial examples from images and text such that they maintain the semantics of the input. <sep>	abstract
2020-713	The reviewers saw a lot of value in this work, but also some flaws.	misc
2020-713	The review process seemed to help answer many questions, but a few remain: there are some questions about the strength of the empirical results on text after the author's updates.	rebuttal_process
2020-713	Wether the adversarial images stay on the manifold is questioned (are blurry or otherwise noisy images "on manifold"?).	rebuttal_process
2020-713	One reviewer raises good questions about the soundness of the comparison to the Song paper. <sep>	rebuttal_process
2020-713	I think this review process has been very productive, and I hope the authors will agree.	misc
2020-713	I hope this feedback helps them to improve their paper.	misc

2020-729	This paper proposed an auxiliary loss based on mutual information for graph neural network.	abstract
2020-729	Such loss is to maximize the mutual information between edge representation and corresponding edge feature in GNN 'message passing' function.	abstract
2020-729	GNN with edge features have already been proposed in the literature.	weakness
2020-729	Furthermore,  the reviewers think the paper needs to improve further in terms of explain more clearly the motivation and rationale behind the method.	weakness

2020-735	The paper suggests an RL-based approach to design a data valuation estimator.	abstract
2020-735	The reviewers agree that the proposed method is new and promising, but they also raised concerns about the empirical evaluations, including not comparing with other approaches of data valuation and limited ablation study. <sep>	weakness
2020-735	The authors provided a rebuttal to address these concerns.	rebuttal_process
2020-735	It improves the evaluation of one of the reviewers, but it is difficult to recommend acceptance given that we did not have a champion for this paper and the overall score is not high enough.	decision

2020-737	The authors attempt to unify graph convolutional networks and label propagation and propose a model that unifies them.	abstract
2020-737	The reviewers liked the idea but felt that more extensive experiments are needed.	weakness
2020-737	The impact of labels needs to be specially studied more in-depth.	weakness

2020-742	This paper considers solving the minimax formulation of adversarial training, where it proposes a new method based on a generic learning-to-learn (L2L) framework.	abstract
2020-742	Particularly, instead of applying the existing hand-designed algorithms for the inner problem, it learns an optimizer parametrized as a convolutional neural network.	abstract
2020-742	A robust classifier is learned to defense the adversarial attack generated by the learned optimizer.	abstract
2020-742	The idea is using L2L is sensible.	abstract
2020-742	However, main concerns on empirical studies remain after rebuttal.	rebuttal_process

2020-747	Borderline decision.	misc
2020-747	The idea is nice, but the theory is not completely convincing.	weakness
2020-747	That makes the results in this paper not be significant enough.	weakness

2020-759	The paper proposes hierarchical Bayesian optimization (HiBO) for learning control policies from a small number of environment interaction and applies it to the postural control of a humanoid.	abstract
2020-759	Both reviewers raised issues with the clarity of presentation, as well as contribution and overall fit to this venue.	weakness
2020-759	The authors' response helped to clarify these issues only marginally.	rebuttal_process
2020-759	Therefore, primarily due to lack of clarity, I recommend rejecting this paper, but encourage the authors to improve the presentation as per the reviewers' suggestions and resubmitting.	decision

2020-772	The submission presents an approach to accelerating convolutional networks.	abstract
2020-772	The framework is related to depthwise separable convolutions.	abstract
2020-772	The reviews are split.	misc
2020-772	R3 expresses concerns about the experimental evaluation and results.	weakness
2020-772	The AC agrees with these concerns.	weakness
2020-772	The AC also notes that the submission is 10 pages long.	weakness
2020-772	Taking all factors into account, the AC recommends against accepting the paper.	decision

2020-773	The authors argue that directly optimizing the IS proposal distribution as in RWS is preferable to optimizing the IWAE multi-sample objective.	abstract
2020-773	They formalize this with an adaptive IS framework, AISLE, that generalizes RWS, IWAE-STL and IWAE-DReg <sep>	abstract
2020-773	Generally reviewers found the paper to be well-written and the connections drawn in this paper interesting.	strength
2020-773	However, all reviewers raised concerns about the lack of experiments (Reviewer 3 suggested several experiments that could be done to clarify remaining questions) and practical takeaways. <sep>	weakness
2020-773	The authors responded by explaining that "the main "practical" takeaway from our work is the following: If one is interested in the bias-reduction potential offered by IWAEs over plain VAEs then the adaptive importance-sampling framework appears to be a better starting point for designing new algorithms than the specific multi-sample objective used by IWAE.	rebuttal_process
2020-773	This is because the former retains all of the benefits of the latter without inheriting its drawbacks."	rebuttal_process
2020-773	I did not find this argument convincing as a primary advantage of variational approaches over WS is that the variational approach optimizes a unified objective.	weakness
2020-773	At least in principle, this is a serious drawback of the WS approaches.	weakness
2020-773	Experiments and/or a discussion of this is warranted. <sep>	weakness
2020-773	This paper is borderline, and unfortunately, due to the high number of quality submissions this year, I have to recommend rejection at this point.	decision

2020-792	The authors focus on low-resource text classifications tasks augmented with "rationales".	abstract
2020-792	They propose a new technique that improves performance over existing approaches and that allows human inspection of the learned weights. <sep>	abstract
2020-792	Although the reviewers did not find any major faults with the paper, they were in consensus that the paper should be rejected at this time.	rating_summary
2020-792	Generally, the reviewers' reservations were in terms of novelty and extent of technical contribution. <sep>	weakness
2020-792	Given the large number of submissions this year, I am recommending rejection for this paper.	decision

2020-793	This paper proposes two contributions to improve uncertainty in deep learning.	abstract
2020-793	The first is a Mahalanobis distance based statistical test and the second a model architecture.	abstract
2020-793	Unfortunately, the reviewers found the message of the paper somewhat confusing and particularly didn't understand the connection between these two contributions.	weakness
2020-793	A major question from the reviewers is why the proposed statistical test is better than using a proper scoring rule such as negative log likelihood.	weakness
2020-793	Some empirical justification of this should be presented.	weakness

2020-796	The paper theoretically shows that the data (embedded by representations learned by GANs) are essentially the same as a high dimensional Gaussian mixture.	abstract
2020-796	The result is based on a recent result from random matrix theory on the covariance matrix of data, which the authors extend to a theorem on the Gram matrix of the data.	abstract
2020-796	The authors also provide a small experiment comparing the spectrum and principle 2D subspace of BigGAN and Gaussian mixtures, demonstrating that their theorem applies in practice. <sep>	abstract
2020-796	Two of the reviews (with confident reviewers) were quite negative about the contributions of the paper, and the reviewers unfortunately did not participate in the discussion period. <sep>	rating_summary
2020-796	Overall, the paper seems solid, but the reviews indicate that improvements are needed in the structure and presentation of the theoretical results.	weakness
2020-796	Given the large number of submissions at *CONF* this year, the paper in its current form does not pass the quality threshold for acceptance.	decision

2020-800	This paper makes a connection between one-class neural networks and the unsupervised approximation of the binary classifier risk under the hinge loss.	abstract
2020-800	An important contribution of the paper is the algorithm to train a binary classifier without supervision by using class prior and the hypothesis that class conditional classifier scores have normal distribution.	abstract
2020-800	The technical contribution of the paper is novel and brings an increased understanding into one-class neural networks.	strength
2020-800	The equations and the modeling present in the paper are sound and the paper is well-written. <sep>	strength
2020-800	However, in its current form, as pointed out by the reviewers, the experimental section is rather weak and can be substantially improved by adding extra experiments as suggested by reviewers #1, #2.	weakness
2020-800	Since its submission the paper has not yet been updated to incorporate these comments.	rebuttal_process
2020-800	Thus, for now, I recommend rejection of this paper, however on improvements I'm sure it can be a good contribution in other conferences.	decision

2020-806	This paper explores a post-processing method for word vectors to "smooth the spectrum," and show improvements on some downstream tasks. <sep>	abstract
2020-806	Reviewers had some questions about the strength of the results, and the results on words of differing frequency.	weakness
2020-806	The reviewers also have comments on the clarity of the paper, as well as the exposition of some of the methods. <sep>	weakness
2020-806	Also, for future submissions to *CONF* and other such conferences, it is more typical to address the authors comments in a direct response rather than to make changes to the document without summarizing and pointing reviewers to these changes.	misc
2020-806	Without direction about what was changed or where to look, there is a lot of burden being placed on the reviewers to find your responses to their comments.	misc

2020-813	This paper presents a new graph pooling method, called HaarPooling.	abstract
2020-813	Based on the hierarchical HaarPooling, the graph classification problems can be solved under the graph neural network framework. <sep>	abstract
2020-813	One major concern of reviewers is the experiment design.	weakness
2020-813	Authors add a new real world dataset in revision.	rebuttal_process
2020-813	Another concern is computational performance.	weakness
2020-813	The main text did not give a comprehensive analysis and the rebuttal did not fully address these problems. <sep>	rebuttal_process
2020-813	Overall, this paper presents an interesting graph pooling approach for graph classification while the presentation needs further polish.	weakness
2020-813	Based on the reviewers' comments, I choose to reject the paper.	decision

2020-870	This paper introduces a new adaptive variational dropout approach to balance accuracy, sparsity and computation. <sep>	abstract
2020-870	The method proposed here is sound, the motivation for smaller (perhaps sparser) networks is easy to follow.	strength
2020-870	The paper provides experiments in several data-sets and compares against several other regularization/pruning approaches, and measures accuracy, speedup, and memory.	strength
2020-870	The reviewers agreed on all these points, but overall they found the results unconvincing.	weakness
2020-870	They requested (1) more baselines (which the authors added), (2) larger tasks/datasets, and (3) more variety in network architectures.	weakness
2020-870	The overall impression was it was hard to see a clear benefit of the proposed approach, based on the provided tables of results. <sep>	weakness
2020-870	The paper could sharpen its impact with several adjustments.	misc
2020-870	The results are much more clear looking at the error vs speedup graphs.	weakness
2020-870	Presenting "representative results" in the tables was confusing, especially considering the proposed approach rarely dominated across all measures.	weakness
2020-870	It was unclear how the variants of the algorithms presented in the tables were selected---explaining this would help a lot.	weakness
2020-870	In addition, more text is needed to help the reader understand how improvements in speed, accuracy, and memory matter.	weakness
2020-870	For example in LeNet 500-300 is a speedup of ~12 @ 1.26 error for BB worth-it/important compared a speedup of ~8 for similar error for L_0?	weakness
2020-870	How should the reader think about differences in speedup, memory and accuracy---perhaps explanations linking to the impact of these metrics to their context in real applications.	weakness
2020-870	I found myself wondering this about pretty much every result, especially when better speedup and memory could be achieved at the cost of some accuracy---how much does the reduction in accuracy actually matter?	weakness
2020-870	Is speed and size the dominant thing?	weakness
2020-870	I don't know. <sep>	weakness
2020-870	Overall the analysis and descriptions of the results are very terse, leaving much to the reader to figure out.	weakness
2020-870	For example (fig 2 bottom right).	misc
2020-870	If a result is worth including in the paper it's worth explaining it to the reader.	weakness
2020-870	Summary statements like "BB and DBB either achieve significantly smaller error than the baseline methods, or significant speedup and memory saving at similar error rates."	weakness
2020-870	Is not helpful where there are so many dimensions of performance to figure out.	weakness
2020-870	The paper spends a lot of time explaining what was done in a matter of fact way, but little time helping the reader interpret the results. <sep>	weakness
2020-870	There are other issues that hurt the paper, including reporting the results of only 3 runs, sometimes reporting median without explanation, undefined metrics like speedup ,%memory (explain how they are calculated), restricting the batchsize for all methods to a particular value without explanation, and overall somewhat informal and imprecise discussion of the empirical methodology. <sep>	weakness
2020-870	The authors did a nice job responding to the reviewers (illustrating good understanding of the area and the strengths of their method), and this could be a strong paper indeed if the changes suggested above were implemented.	rebuttal_process
2020-870	Including SSL and SVG in the appendix was great, but they really should have been included in the speedup vs error plots throughout the paper.	suggestion
2020-870	This is a nice direction and was very close.	misc
2020-870	Keep going!	misc

2020-878	The paper is rejected based on unanimous reviews.	decision

2020-882	main summary:  method for quantizing GAN <sep>	abstract
2020-882	discussion: <sep> reviewer 1: well-written paper, but reviewer questions novelty <sep>	weakness
2020-882	reviewer 2: well-written, but some details are missing in the paper as well as comparisons to related work <sep>	weakness
2020-882	reviewer 3: well-written and interesting topic, related work section and clarity of results could be improved <sep>	weakness
2020-882	recommendation: all reviewers agree paper could be improved by better comparison to related work and better clarity of presentation.	weakness
2020-882	Marking paper as reject.	decision

2020-892	The paper proposes an RL-based algorithm for training neural networks that is able to match the performance of backprop on CIFAR and MNIST datasets. <sep>	abstract
2020-892	The reviewers generally found the algorithm and motivations interesting, but some had issue with the imprecision of the notion of "biologically plausible" used by the authors.	weakness
2020-892	One reviewer had issues with missing discussion of related work and also doubts about the meaningfulness of the experiments, since the networks were quite shallow. <sep>	weakness
2020-892	For this type of paper, clarity and precision of exposition is crucial in my opinion, and so I recommend rejection at this time, but encourage the authors to take the feedback into account and resubmit to a future venue.	decision

2020-907	Reviewers raise the serious issue that the proof of Theorem 2 is plagiarized from Theorem 1 of "Demystifying MMD GANs" (https://arxiv.org/abs/1801.01401).	weakness
2020-907	With no response from the authors, this is a clear reject.	decision

2020-909	The main contribution of this paper is the training of a supervised model jointly with deep CCA for improving the representations learned in a setting where the training data is multi-view.	abstract
2020-909	The claimed technical contribution is modifications to deep CCA to enable it to play nicely with the minibatch gradient-based training used for the supervised loss.	abstract
2020-909	Pros:  This is an important problem with many applications.	strength
2020-909	Cons:  The novelty is minimal.	weakness
2020-909	Some previous work has done joint training of supervised models with CCA, and some has addressed training deep CCA in a stochastic setting.	weakness
2020-909	The reviewers (and I) are unconvinced that the differences from previous work are sufficient, and the paper does not carefully compare with the previous work.	weakness
2020-909	The contribution to the tasks may be quite significant, however, so the paper may fit in well in an application-oriented conference/journal.	decision

2020-913	The submission proposes a dynamic approach to training a neural net which switches between half and full-precision operations while maintaining the same classifier accuracy, resulting in a speed up in training time.	abstract
2020-913	Empirical results show the value of the approach, and the authors have added additional sensitivity analysis by sweeping over hyperparameters. <sep>	abstract
2020-913	The reviewers were concerned about the novelty of the approach as well as the robustness of the claims that accuracy can be maintained even in the accelerated, dynamic regime.	weakness
2020-913	After discussion there were still concerns about the sensitivity analysis and the significance of the results. <sep>	rebuttal_process
2020-913	The recommendation is to reject the paper at this time.	decision

2020-914	This article studies universal approximation with deep narrow networks, targeting the minimum width.	abstract
2020-914	The central contribution is described as providing results for general activation functions.	abstract
2020-914	The technique is described as straightforward, but robust enough to handle a variety of activation functions.	strength
2020-914	The reviewers found the method elegant.	strength
2020-914	The most positive position was that the article develops non trivial techniques that extend existing universal approximation results for deep narrow networks to essentially all activation functions.	strength
2020-914	However, the reviewers also expressed reservations mentioning that the results could be on the incremental side, with derivations similar to previous works, and possibly of limited interest.	weakness
2020-914	In all, the article makes a reasonable theoretical contribution to the analysis of deep narrow neural networks.	strength
2020-914	Although this is a reasonably good article, it is not good enough, given the very high acceptance bar for this year's *CONF*.	decision

2020-922	(I acknowledge reading authors' recent note on decaNLP.) <sep>	misc
2020-922	This paper proposes a span extraction approach (SpExBERT) to unify question answering, text classification and regression.	abstract
2020-922	Paper includes a significant number of experiments (including low-resource and multi-tasking experiments) on multiple benchmarks.	abstract
2020-922	The reviewers are concerned about lack of support on author's claims from the experimental results due to seemingly insignificant improvements and lack of analysis regarding the results.	weakness
2020-922	Hence, I suggest rejecting the paper.	decision

2020-932	The authors provide an analysis of a cross-lingual data augmentation technique which they call XLDA.	abstract
2020-932	This consists of replacing a segment of an input text with its translation in another language.	abstract
2020-932	They show that when fine-tuning, it is more beneficial to train on the cross-lingual hypotheses than on the in-language pairs, especially for low resource languages such as Greek, Turkish and Urdu.	abstract
2020-932	The paper explores an interesting idea however they lack comparison with other techniques such as backtranslation and XLM models, and would benefit from a wider range of tasks.	weakness
2020-932	I feel like this paper is more suitable for an NLP-focussed venue.	decision

2020-950	The paper proposes a parallelization approach for speeding up scheduled sampling, and show significant improvement over the original.	abstract
2020-950	The approach is simple and a clear improvement over vanilla schedule sampling.	abstract
2020-950	However, the reviewers point out that there are more recent methods to compare against or combine with, and that the paper is a bit thin on content and could have addressed this.	weakness
2020-950	The proposed approach may well combine well with newer techniques, but I tend to agree that this should be tested.	suggestion

2020-958	This paper proposes applying potential flow generators in conjunction with L2 optimal transport regularity to favor solutions that "move" input points as little as possible to output points drawn from the target distribution.	abstract
2020-958	The resulting pipeline can be effective in dealing with, among other things, image-to-image translation tasks with unpaired data.	abstract
2020-958	Overall, one of the appeals of this methodology is that it can be integrated within a number of existing generative modeling paradigms (eg, GANs, etc.). <sep>	abstract
2020-958	After the rebuttal and discussion period, two reviewers maintained weak reject scores while one favored strong acceptance.	rating_summary
2020-958	With these borderline/mixed scores, this paper was discussed at the meta-review level and the final decision was to side with the majority, noting that a revision which fully addresses reviewer comments could likely be successful at a future venue.	decision
2020-958	As one important lingering issue, R1 pointed out that the optimality conditions of the proposed approach are only enforced on sampled trajectories, not actually on the entire space.	rebuttal_process
2020-958	The rebuttal concedes this point, but suggests that the method still seems to work.	rebuttal_process
2020-958	But as an improvement, the suggestion is made that randomly perturbed trajectories could help to mitigate this issue.	suggestion
2020-958	However, no experiments were conducted using this modification, which could be helpful in building confidence in the reliability of the overall methodology. <sep>	suggestion
2020-958	Additionally, from my perspective the empirical validation could also be improved to help solidify the contribution in a revision.	suggestion
2020-958	For example, the image-to-image translation experiments with CelebA were based on a linear (PCA) embedding and feedforward networks.	suggestion
2020-958	It would have been nice to have seen a more sophisticated setup for this purpose (as discussed in Section 5), especially for a non-theoretical paper with an ostensibly practically-relevant algorithmic proposal.	suggestion
2020-958	And consistent with reviewer comments, the paper definitely needs another pass to clean up a number of small grammatical mistakes.	suggestion

2020-960	This paper studies the trade-off between the model size and quantization levels in quantized CNNs by varying different channel width multipliers.	abstract
2020-960	The paper is well  motivated and draws interesting observations but can be improved in terms of evaluation.	weakness
2020-960	It is a borderline case and rejection is made due to the high competition.	decision

2020-979	The paper proposed an efficient way of generating graphs.	abstract
2020-979	Although the paper claims to propose simplified mechanism, the reviewers find that the generation task to be relatively very complex, and the use of certain module seems ad-hoc.	weakness
2020-979	Furthermore, the results on the new metric is at times inconsistent with other prior metrics.	weakness
2020-979	The paper can be improved by addressing those concerns concerns.	suggestion

2020-987	This paper considers how to learn the structure of deep network by beginning with a simple network and then progressively adding layers and filters as needed.	abstract
2020-987	The paper received three reviews by expert working in this area.	misc
2020-987	R1 recommends Weak Reject due to concerns about novelty, degree of contribution, clarity of technical exposition, and experiments.	rating_summary
2020-987	R2 recommends Weak Accept and has some specific suggestions and questions.	rating_summary
2020-987	R3 recommends Weak Reject, also citing concerns with experiments and writing.	rating_summary
2020-987	The authors submitted a response that addressed many of these comments, but R1 and R3 continue to have concerns about contribution and the experiments, while R2 maintains their Weak Accept rating.	rebuttal_process
2020-987	Given the split decision, the AC also read the paper.	misc
2020-987	While we believe the paper has significant merit, we agree with R1 and R3 on the need for additional experimentation, and believe another round of peer review would help clarify the writing and contribution.	weakness
2020-987	We hope the reviewer comments will hep authors prepare a revision for a future venue.	misc

2020-994	The paper studies Positron Emission Tomography (PET) in medical imaging.	abstract
2020-994	The paper focuses on the challenges created by gamma-ray photon scattering, that results in poor image quality.	abstract
2020-994	To tackle this problem and enhance the image quality, the paper suggests using generative adversarial networks.	abstract
2020-994	Unfortunately due to poor writing and severe language issues, none of the three reviewers were able to properly assess the paper [see the reviews for multiple examples of this].	weakness
2020-994	In addition, in places, some important implementation details were missing. <sep>	weakness
2020-994	The authors chose not to response to reviewers' concerns.	rebuttal_process
2020-994	In its current form, the submission cannot be well understood by people interested in reading the paper, so it needs to be improved and resubmitted.	decision

2020-995	This paper tackles neural response generation with Generative Adversarial Nets (GANs), and to address the training instability problem with GANs, it proposes a local distribution oriented objective.	abstract
2020-995	The new objective is combined with the original objective, and used as a hybrid loss for the adversarial training of response generation models, named as LocalGAN.	abstract
2020-995	Authors responded with concerns about reviewer 3's comments, and I agree with the authors explanation, so I am disregarding review 3, and am relying on my read through of the latest version of the paper.	ac_disagreement
2020-995	The other reviewers think the paper has good contributions, however they are not convinced about the clarity of the presentations and made many suggestions (even after the responses from the authors).	weakness
2020-995	I suggest a reject, as the paper should include a clear presentation of the approach and technical formulation (as also suggested by the reviewers).	decision

2020-1004	This paper proposes a new method for code generation based on structured language models. <sep>	abstract
2020-1004	After viewing the paper, reviews, and author response my assessment is that I basically agree with Reviewer 4.	misc
2020-1004	(Now, after revision) This work seems to be (1) a bit incremental over other works such as Brockschmidt et al (2019), and (2) a bit of a niche topic for *CONF*.	weakness
2020-1004	At the same time it has (3) good engineering effort resulting in good scores, and (4) relatively detailed conceptual comparison with other work in the area.	strength
2020-1004	Also, (5) the title of "Structural Language Models for Code Generation" is clearly over-claiming the contribution of the work -- as cited in the paper there are many language models, unconditional or conditional, that have been used in code generation in the past.	weakness
2020-1004	In order to be accurate, the title would need to be modified to something that more accurately describes the (somewhat limited) contribution of the work. <sep>	suggestion
2020-1004	In general, I found this paper borderline.	misc
2020-1004	*CONF*, as you know is quite competitive so while this is a reasonably good contribution, I'm not sure whether it checks the box of either high quality or high general interest to warrant acceptance.	decision
2020-1004	Because of this, I'm not recommending it for acceptance at this time, but definitely encourage the authors to continue to polish for submission to a different venue (perhaps a domain conference that would be more focused on the underlying task of code generation?)	decision

2020-1011	This work proposes a VAE-based model for learning transformations of sequential data (the main here intuition is to have the model learn changes between frames without learning features that are constant within a time-sequence).	abstract
2020-1011	All reviewers agreed that this is a very interesting submission, but have all challenged the novelty and rigor of this paper, asking for more experimental evidence supporting the strengths of the model.	weakness
2020-1011	After having read the paper, I agree with the reviewers and I currently see this one as a weak submission without potentially comparing against other models or showing whether the representations learned from the proposed model lead in downstream improvements in a task that uses this representations.	weakness

2020-1017	The paper proposes a VAE with a mixture-of-experts decoder for clustering and generation of high-dimensional data.	abstract
2020-1017	Overall, the reviewers found the paper well-written and structured , but in post rebuttal discussion questioned the overall importance and interest of the work to the community.	rebuttal_process
2020-1017	This is genuinely a borderline submission.	misc
2020-1017	However, the calibrated average score currently falls below the acceptance threshold, so I'm recommending rejection, but strongly encouraging the authors to continue the work, better motivating the importance of the work, and resubmitting.	decision

2020-1019	The authors present an approach to multi-task learning.	abstract
2020-1019	Reviews are mixed.	misc
2020-1019	The main worries seem to be computational feasibility and lack of comparison with existing work.	weakness
2020-1019	Clearly, one advantage to Cross-stitch networks over the proposed approach is that their approach learns sharing parameters in an end-to-end fashion and scales more efficiently to more tasks.	weakness
2020-1019	Note: The authors mention SluiceNets in their discussion, but I think it would be appropriate to directly compare against this architecture - or DARTS [https://arxiv.org/abs/1806.09055], maybe - since the offline RSA computations only seem worth it if better than *anything* you can do end-to-end.	suggestion
2020-1019	I would encourage the authors to map out this space and situate their proposed method properly in the landscape of existing work.	suggestion
2020-1019	I also think it would be interesting to think of their approach as an ensemble learning approach and look at work in this space on using correlations between representations to learn what and how to combine.	suggestion
2020-1019	Finally, some work has suggested that benefits from MTL are a result of easier optimization, eg, [3]; if that is true, will you not potentially miss out on good task combinations with your approach? <sep>	suggestion
2020-1019	Other related work: <sep>	misc
2020-1019	[0] https://www.aclweb.org/anthology/C18-1175/ <sep>	misc
2020-1019	[1] https://www.aclweb.org/anthology/P19-1299/ <sep>	misc
2020-1019	[2] https://www.aclweb.org/anthology/N19-1355. pdf - a somewhat similar two-stage approach <sep>	misc
2020-1019	[3] https://www.aclweb.org/anthology/E17-2026/	misc

2020-1020	This paper proposes a model that can learn predicates (symbolic relations) from pixels and can be trained end to end.	abstract
2020-1020	They show that the relations learned generate a representation that generalizes well, and provide some interpretation of the model. <sep>	abstract
2020-1020	Though it is reasonable to develop a model with synthetic data, the reviewers did wonder if the findings would generalize to new data from real situations.	weakness
2020-1020	The authors argue that a new model should be understood (using synthetic data) before it can reasonably be applied to natural data.	rebuttal_process
2020-1020	I hope the reviews have shown the authors which areas of the paper need further explanation, and that the use of a synthetic dataset needs to strong justification, or perhaps show some evidence that the method will probably work on real data (eg how it could be extended to natural images).	misc

2020-1036	The reviewers have uniformly had significant reservations for the paper.	rating_summary
2020-1036	Given that the authors did not even try to address them, this suggests the paper should be rejected.	decision

2020-1043	The paper proposes a platform for benchmarking, and in particular hardware-agnostic evaluation of machine learning models.	abstract
2020-1043	This is an important problem as our field strives for more reproducibility. <sep>	strength
2020-1043	This was a very confusing paper to discuss and review, since most of the reviewers (and myself) do not know much about the area.	misc
2020-1043	Two of the reviewers found the paper contributions sufficient to be (weakly) accepted.	rating_summary
2020-1043	The third reviewer had many issues with the work and engaged in a lengthy debate with the authors, but there was strong disagreement regarding their understanding of the scope of the paper as a Tools/Systems submission. <sep>	rebuttal_process
2020-1043	Given the lack of consensus, I must recommend rejection at this time, but highly encourage the authors to take the feedback into account and resubmit to a future venue.	decision

2020-1044	The paper combines graph convolutional networks with noisy label learning.	abstract
2020-1044	The reviewers feel that novelty in the work is limited and there is a need for further experiments and  extensions.	weakness

2020-1046	The paper pursues an interesting approach, but requires additional maturation.	weakness
2020-1046	The experienced reviewers raise several concerns about the current version of the paper.	misc
2020-1046	The significance of the contribution was questioned.	weakness
2020-1046	The paper missed key opportunities to evaluate and justify critical aspects of the proposed approach, via targeted ablation and baseline studies.	weakness
2020-1046	The quality and clarity of the technical exposition was also criticized.	weakness
2020-1046	The comments submitted by the reviewers should help the authors strengthen the paper.	misc

2020-1047	This paper considers the information plane analysis of DNNs.	abstract
2020-1047	Estimating mutual information is required in such analysis which is difficult task for high dimensional problems.	abstract
2020-1047	This paper proposes a new "matrix–based Renyi's entropy coupled with ´tensor kernels over convolutional layers" to solve this problem.	abstract
2020-1047	The methods seems to be related to an existing approach but derived using a different "starting point".	abstract
2020-1047	Overall, the method is able to show improvements in high-dimensional case. <sep>	abstract
2020-1047	Both R1 and R3 have been critical of the approach.	weakness
2020-1047	R3 is not convinced that the method would work for high-dimensional case and also that no simulation studies were provided.	rebuttal_process
2020-1047	In the revised version the authors added a new experiment to show this.	rebuttal_process
2020-1047	R3's another comment makes an interesting point regarding "the estimated quantities evolve during training, and that may be interesting in itself, but calling the estimated quantities mutual information seems like a leap that's not justified in the paper."	rebuttal_process
2020-1047	I could not find an answer in the rebuttal regarding this. <sep>	rebuttal_process
2020-1047	R1 has also commented that the contribution is incremental in light of existing work.	rebuttal_process
2020-1047	The authors mostly agree with this, but insist that the method is derived differently. <sep>	rebuttal_process
2020-1047	Overall, I think this is a reasonable paper with some minor issues.	misc
2020-1047	I think this can use another review cycle where the paper can be improved with additional results and to take care of some of the doubts that reviewers' had this time. <sep>	suggestion
2020-1047	For now, I recommend to reject this paper, but encourage the authors to resubmit at another venue after revision.	decision

2020-1051	This manuscript personalization techniques to improve the scalability and privacy preservation of federated learning.	abstract
2020-1051	Empirical results are provided which suggests improved performance. <sep>	abstract
2020-1051	The reviewers and AC agree that the problem studied is timely and interesting, as the tradeoffs between personalization and performance are a known concern in federated learning.	strength
2020-1051	However, this manuscript also received quite divergent reviews, resulting from differences in opinion about the novelty and clarity of the conceptual and empirical results.	weakness
2020-1051	Reviewers were also unconvinced by the provided empirical evaluation results.	weakness

2020-1066	This submission proposes a method for detecting adversarial attacks using saliency maps. <sep>	abstract
2020-1066	Strengths: <sep> -The experimental results are encouraging. <sep>	strength
2020-1066	Weaknesses: <sep> -The novelty is minor. <sep>	weakness
2020-1066	-Experimental validation of some claims (eg robustness to white-box attacks) is lacking. <sep>	weakness
2020-1066	These weaknesses were not sufficiently addressed in the discussion phase.	rebuttal_process
2020-1066	AC agrees with the majority recommendation to reject.	decision

2020-1073	The authors address the problem of CTR prediction by using a Transformer based encoder to capture interactions between features.	abstract
2020-1073	They suggest simple modifications to the basic Multiple Head Self Attention (MSHA) mechanism and show that they get the best performance on two publicly available datasets. <sep>	abstract
2020-1073	While the reviewers agreed that this work is of practical importance, they had a few objections which I have summarised below: <sep> 1) Lack of novelty: The reviewers felt that the adoption of MSHA for the CTR task was straightforward.	weakness
2020-1073	The suggested modifications in the form of Bilinear similarity and max-pooling were viewed as incremental contributions. <sep>	weakness
2020-1073	2) Lack of comparison with existing work: The reviewers suggested some additional baselines (Deep and Cross) which need to be added (the authors have responded that they will do so later). <sep>	weakness
2020-1073	3) Need to strengthen experiments: The reviewers appreciated the ablation studies done by the authors but requested for more studies to convincingly demonstrate the effect of some components.	weakness
2020-1073	One reviewer also pointed that the authors should control form model complexity to ensure an apples-to-apples comparison (I agree that many papers in the past have not done this but going froward I have a hunch that many reviewers will start asking for this) . <sep>	suggestion
2020-1073	IMO, the above comments are important and the authors should try to address them in subsequent submissions. <sep>	suggestion
2020-1073	Based on the reviewer comments and lack of any response from the authors, I recommend that the paper in it current form cannot be accepted.	decision

2020-1088	The paper combines several recent optimizer tricks to provide empirical evidence that goes against the common belief that adaptive methods result in larger generalization errors.	abstract
2020-1088	The contribution of this paper is rather small: no new strategies are introduced and no new theory is presented.	weakness
2020-1088	The paper makes a good workshop paper, but does not meet the bar for publication at *CONF*.	decision

2020-1101	This paper studies the "suspended animation limit" of various graph neural networks (GNNs) and provides some theoretical analysis to explain its cause.	abstract
2020-1101	To overcome the limitation, the authors propose Graph Residual Network (GRESNET) framework to involve nodes' raw features or intermediate representations throughout the graph for all the model layers.	abstract
2020-1101	The main concern of the reviewers is: the assumption made for theoretical analysis that the fully connected layer is identical mapping is too stringent.	weakness
2020-1101	The paper does not gather sufficient support from the reviewers to merit acceptance, even after author response and reviewer discussion.	rating_summary
2020-1101	I thus recommend reject.	decision

2020-1109	The paper attacks the important problem of learning time series models with missing data and proposes two learning frameworks, RISE and DISE, for this problem.	abstract
2020-1109	The reviewers had several concerns about the paper and experimental setup and agree that this paper is not yet ready for publication.	rating_summary
2020-1109	Please pay careful attention to the reviewer comments and particularly address the comments related to experimental design, clarity, and references to prior work while editing the paper.	suggestion

2020-1130	This paper highlights the problem of penalizing the total correlation of sampled latent variables for unsupervised learning of disentangled representations.	abstract
2020-1130	Authors prove a theorem on how sample representations with bounded total correlation may have arbitrarily large total correlation when computed with the underlying mean.	abstract
2020-1130	As a fix, the authors propose RTC-VAE method that penalizes total covariance of sampled latent variables. <sep>	abstract
2020-1130	R2 appreciated the simplicity of the idea, making it easy to understand and implement, but raises serious concerns on empirical evaluation of the method.	weakness
2020-1130	Specifically, very limited datasets (initially dsprites and 3d shapes) and with no evaluation of disentanglement performance and no comparison against other disentangling methods like DIP-VAE-1.	weakness
2020-1130	While the authors added another dataset (3d face) in their revised versions, the concerns about disentanglement performance evaluation and its comparison against baselines remained as before, and R2 was not convinced to raise the initial score. <sep>	rebuttal_process
2020-1130	Similarly, while R1 and R3 appreciate author's response, they believe the response was not convincing enough for them, and maintained their initial ratings. <sep>	rebuttal_process
2020-1130	Overall, the submission has room for improvement toward a clear evaluation of the proposed method against related baselines.	suggestion

2020-1131	The submission proposes to improve generalization in RL environments, by addressing the scenario where the observations change even though the underlying environment dynamics do not change.	abstract
2020-1131	The authors address this by learning an adaptation function which maps back to the original representation.	abstract
2020-1131	The approach is empirically evaluated on the Mountain Car domain. <sep>	abstract
2020-1131	The reviewers were unanimously unimpressed with the experiments, the baselines, and the results.	weakness
2020-1131	While they agree that the problem is well-motivated, they requested additional evidence that the method works as described and that a simpler approach such as fine-tuning would not be sufficient. <sep>	weakness
2020-1131	The recommendation is to reject the paper at this time.	decision

2020-1141	The paper proposes to model uncertainty using expected Bayes factors, and empirically show that the proposed measure correlates well with the probability that the classification is correct. <sep>	abstract
2020-1141	All the reviewers agreed that the idea of using Bayes factors for uncertainty estimation is an interesting approach.	strength
2020-1141	However, the reviewers also found the presentation a bit hard to follow.	weakness
2020-1141	While the rebuttal addressed some of these concerns, there were still some remaining concerns (see R3's comments). <sep>	rebuttal_process
2020-1141	I think this is a really promising direction of research and I appreciate the authors' efforts to revise the draft during the rebuttal (which led to some reviewers increasing the score).	rebuttal_process
2020-1141	This is a borderline paper right now but I feel that the paper has the potential to turn into a great paper with another round of revision.	misc
2020-1141	I encourage the authors to revise the draft and resubmit to a different venue.	decision

2020-1170	This paper proposes a GAN-based approach to producing poisons for neural networks.	abstract
2020-1170	While the approach is interesting and appreciated by the reviewers, it is a legitimate and recurring criticism that the method is only demonstrated on very toy problems (MNIST and Fashion MNIST).	weakness
2020-1170	During the rebuttal stage, the authors added results on CIFAR, although the results on CIFAR were not convincing enough to change the reviewer scores; the SOTA in GANs is sufficient to generate realistic images of cars and trucks (even at the ImageNet scale), while the demonstrated images are sufficiently far from the natural image distribution on CIFAR-10 that it is not clear whether the method benefits from using a GAN.	rebuttal_process
2020-1170	It should be noted that a range of poisoning methods exist that can effectively target CIFAR, and SOTA methods (eg, poison polytope attacks and backdoor attacks) can even target datasets like ImageNet and CelebA.	rebuttal_process

2020-1184	This paper proposes an MCTS method for neural architecture search (NAS).	abstract
2020-1184	Evaluations on NAS-Bench-101 and other datasets are promising.	strength
2020-1184	Unfortunately, no code is provided, which is very important in NAS to overcome the reproducibility crisis. <sep>	weakness
2020-1184	Discussion: <sep> The authors were able to answer several questions of the reviewers.	rebuttal_process
2020-1184	I also do not share the concern of AnonReviewer2 that MCTS hasn't been used for NAS before; in contrast, this appears to be a point in favor of the paper's novelty.	ac_disagreement
2020-1184	However, the authors' reply concerning Bayesian optimization and the optimization of its acquisition function is strange: using the ConvNet-60K dataset with 1364 networks, it does not appear to make sense to use only 1% or even only 0.01% of the dataset size as a budget for optimizing the acquisition function.	rebuttal_process
2020-1184	The reviewers stuck to their rating of 6,3,3. <sep>	rating_summary
2020-1184	Overall, I therefore recommend rejection.	decision

2020-1190	One of the reviewers pointed out similarity to existing very recent work which would require significant reframing of the current paper.	weakness
2020-1190	Hence, this work is below the bar at the moment.	decision

2020-1203	This paper presents and evaluates a technique for semantic parsing, and in particular proposes a model to re-rank the candidates generated by beam search.	abstract
2020-1203	The paper was reviewed by 3 experts and received Reject, Weak Reject, and Weak Reject opinions.	rating_summary
2020-1203	The reviews identified strengths of the paper but also significant concerns, mostly centered around the experimental evaluation (including choice of datasets, lack of direct comparison to baselines, need for more methodical and quantitative analysis, need for additional analysis, etc.)	weakness
2020-1203	and some questions about the design of the technical approach.	weakness
2020-1203	The authors submitted responses that addressed some of these concerns, but indicated that additional experimentation would be needed to address all of them.	rebuttal_process
2020-1203	In light of these reviews, we are not able to recommend acceptance at this time, but I hope authors use the detailed, constructive feedback to improve the paper for another venue.	decision

2020-1209	This manuscript proposes a strategy for fitting predictive models on data separated across nodes, with respect to both samples and features. <sep>	abstract
2020-1209	The reviewers and AC agree that the problem studied is timely and interesting, and were impressed by the size and scope of the evaluation dataset (particularly for a medical application).	strength
2020-1209	However, reviewers were unconvinced about the novelty and clarity of the conceptual and empirical results.	weakness
2020-1209	On the conceptual end, the AC also suggests that the authors look into closely related work on split learning (https://splitlearning.github.io/) which has also been applied to medical data settings.	suggestion

2020-1211	The paper considers the setting of constrained MDPs and proposes using backward value functions to keep track of the constraints. <sep>	abstract
2020-1211	All reviewers agreed that the idea of backward value functions is interesting, but there were a few technical concerns raised, and the reviewers remained unconvinced after the rebuttal.	weakness
2020-1211	In particular, there were doubts whether the method actually makes sense for the considered problem (the backward VF averaging constraints over all trajectories, instead of only considering the current one), and a concern about insufficient baseline comparisons. <sep>	weakness
2020-1211	I recommend rejection at this time, but encourage the authors to take the feedback into account, make the paper more crisp, and resubmit to a future venue.	decision

2020-1221	This paper received three reviews.	misc
2020-1221	R1 recommends Weak Reject, and identifies a variety of concerns about the motivation, presentation, clarity and soundness of results, and experimental design (eg choice of metrics).	rating_summary
2020-1221	In a short review, R2 recommends Weak Accept, but indicates they are not an expert in this area.	rating_summary
2020-1221	R3 also recommends Weak Accept, but identifies concerns also centering around clarity and completeness of the paper as well as some specific technical questions.	rating_summary
2020-1221	In their response, authors address these issues, and have a constructive back-and-forth conversation with R1, who remains unconvinced about significance of the empirical results and thus the conclusion of the overall paper.	rebuttal_process
2020-1221	After the discussion period, R3 indicated that they weakly favored acceptance but agreed that the paper had significant presentation issues and would not strongly advocate for it.	rating_summary
2020-1221	R1 advocated for Reject, given the concerns identified in their reviews and followup comments.	rating_summary
2020-1221	Given the split decision, the AC also read the paper.	misc
2020-1221	While the work clearly has merit, we agree with R1's comment that it is overall a "potentially interesting idea, but the justification and presentation/quantification of results is not good enough in the submitted paper," and feel the paper really needs a revision and another round of peer review before publication.	decision

2020-1226	This paper introduces a new variant of autoencoders with an topological loss term. <sep>	abstract
2020-1226	The reviewers appreciated part of the paper and it is borderline.	rating_summary
2020-1226	However, there are enough reservations to argue for it will be better for the paper to updated and submitted to next conference. <sep>	rating_summary
2020-1226	Rejection is recommended.	decision

2020-1231	This paper has been assessed by three reviewers who scored it as 3/3/3, and they did not increase their scores after the rebuttal.	rating_summary
2020-1231	The main criticism lies in novelty of the paper, lack of justification for MM^T formulation, speed compared to gradient descent (ie theoretical analysis plus timing).	weakness
2020-1231	Other concerns point to overlaps with Baydin et al 2015 and the question about the validity of Theorem 1.	weakness
2020-1231	On balance, this paper requires further work and it cannot be accepted to *CONF*2020.	decision

2020-1247	The paper is not overly well written and motivated.	weakness
2020-1247	A guiding thread through the paper is often missing.	weakness
2020-1247	Comparisons with constrained BO methods would have improved the paper as well as a more explicit link to multi-objective BO.	weakness
2020-1247	It could have been interesting to evaluate the sensitivity wrt the number of samples in the Monte Carlo estimate.	weakness
2020-1247	What happens if the observations of the function are noisy?	weakness
2020-1247	Is there a natural way to deal with this? <sep>	weakness
2020-1247	Given that the paper is 10+ pages long, we expect a higher quality than an 8-pages paper (reviewing and submission guidelines).	misc

2020-1248	The reviewers all agree that this is an interesting paper with good results.	strength
2020-1248	The authors' rebuttal response was very helpful.	rebuttal_process
2020-1248	However, given the competitiveness of the submissions this year, the submission did not make it.	decision
2020-1248	We encourage the authors to resubmit the work including the new results obtained during the rebuttal.	suggestion

2020-1252	While there was some support for the ideas presented, the majority of reviewers felt that this submission is not ready for publication at *CONF* in its present form. <sep>	rating_summary
2020-1252	Concerns were raised as to the generality of the approach, thoroughness of experiments, and clarity of the exposition.	weakness

2020-1265	The paper proposes two methods for link prediction in knowledge hypergraphs.	abstract
2020-1265	The first method concatenates the embedding of all entities and relations in a hyperedge.	abstract
2020-1265	The second method combines an entity embedding, a relation embedding, and a weighted convolution of positions.	abstract
2020-1265	The authors demonstrate on two datasets (derived by the authors from Freebase), that the proposed methods work well compared to baselines.	abstract
2020-1265	The paper proposes direct generalizations of knowledge graph approaches, and unfortunately does not yet provide a comprehensive coverage of the possible design space of the two proposed extensions. <sep>	weakness
2020-1265	The authors should be commended for providing the source code for reproducibility.	strength
2020-1265	One of the reviewers (who was unfortunately also the most negative), was time pressed.	rating_summary
2020-1265	Unfortunately, the discussion period was not used by the reviewers to respond to the authors' rebuttal of their concerns. <sep>	rebuttal_process
2020-1265	Even discounting the most negative review, this paper is on the borderline, and given the large number of submissions to *CONF*, it unfortunately falls below the acceptance threshold in its current form.	decision

2020-1267	The paper presents a semi-supervised data streaming approach.	abstract
2020-1267	The proposed architecture is made of a layer-wise k-means structure (more specifically a epsilon-means approach, where the epsilon is adaptively defined from the distortion percentile).	abstract
2020-1267	Each layer is associated a scope (patch dimensions); each patch of the image is associated its nearest cluster center (or a new cluster is created if needed); new cluster centers are adjusted to fit the examples (Short Term Memory); clusters that have been visited sufficiently many time are frozen (Long Term Memory).	abstract
2020-1267	Each cluster is associated a label distribution from the labelled examples.	abstract
2020-1267	The label for each new image is obtained by a vote of the clusters and layers. <sep>	abstract
2020-1267	Some reviews raise some issues about the robustness of the approach, and its sensitivity wrt hyper-parameters.	weakness
2020-1267	Some claims ("the distribution associated to a class may change with time") are not experimentally confirmed; it seems that in such a case, the LTM size might grow along time; a forgetting mechanism would then be needed to enforce the tractability of classification. <sep>	weakness
2020-1267	Some claims (the mechanism is related to how animal learn) are debatable, as noted by Rev#1; see hippocampal replay. <sep>	weakness
2020-1267	The area chair thinks that a main issue with the paper is that the Unsupervised Progressive Learning is considered to be a new setting ("none of the existing approaches in the literature are directly applicable to the UPL problem"), preventing the authors from comparing their results with baselines. <sep>	weakness
2020-1267	However, after a short bibliographic search, some related approaches exist under another name: <sep>	suggestion
2020-1267	* Incremental Semi-supervised Learning on Streaming Data, Pattern Recognition 88, Li et al, 2018; <sep>	misc
2020-1267	* Incremental Semi-Supervised Learning from Streams for Object Classification, Chiotellis et al, 2018; <sep>	misc
2020-1267	* Online data stream classification with incremental semi-supervised learning, Loo et al, 2015. <sep>	misc
2020-1267	The above approaches seem able to at least accommodate the Uniform UPL scenario.	suggestion
2020-1267	I therefore encourage the authors to consider some of the above as baselines and provide a comparative validation of STAM.	suggestion

2020-1282	This paper proposes a response generation approach that aims to tackle the generic response problem.	abstract
2020-1282	The approach is learning a latent semantic space by maximizing the correlation between features extracted from prompts and responses.	abstract
2020-1282	The reviewers were concerned about the lack of comparison with previous papers tackling the same problem, and did not change their decision (ie, were not convinced) even after the rebuttal.	rebuttal_process
2020-1282	Hence, I suggest a reject for this paper.	decision

2020-1317	The paper introduces an interesting application of GNNs, but the reviewers find that the contribution is too limited and the motivation is too weak.	weakness

2020-1336	Novelty of the proposed model is low.	weakness
2020-1336	Experimental results are weak.	weakness

2020-1342	The paper shows that overparameterized autoencoders can be trained to memorize a small number of training samples, which can be retrieved via fixed point iteration.	abstract
2020-1342	After rounds of discussion with the authors, the reviewers agree that the idea is interesting and overall quality of writing and experiments is reasonable, but they were skeptical regarding the significance of the finding and impact to the field and thus encourage studying the phenomenon further and resubmitting in a future conference.	rating_summary
2020-1342	I thus recommend rejecting this submission for now.	decision

2020-1357	This paper addresses the classic medial image segmentation by combining Neural Ordinary Differential Equations (NODEs) and the level set method.	abstract
2020-1357	The proposed method is evaluated on kidney segmentation and salient object detection problems.	abstract
2020-1357	Reviewer #1 provided a brief review concerning *CONF* is not the appropriate venue for this work.	weakness
2020-1357	Reviewer #2 praises the underlying concept being interesting, while pointing out that the presentation and experiments of this work is not ready for publication yet.	weakness
2020-1357	Reviewer #3 raises concerns on whether the methods are presented properly.	weakness
2020-1357	The authors did not provide responses to any concerns.	rebuttal_process
2020-1357	Given these concerns and overall negative rating (two weak reject and one reject), the AC recommends reject.	decision

2020-1364	This paper introduces a new ECG dataset.	abstract
2020-1364	While I appreciate the efforts to clarify several points raised by the reviewers, I still believe this contribution to be of limited interest to the broad *CONF* community.	weakness
2020-1364	As such, I suggest this paper to be submitted to a more specialised venue.	decision

2020-1370	While the reviewers appreciated the ideas presented in the paper and their novelty, there were major concerns raised about the experimental evaluation.	weakness
2020-1370	Due to the serious doubts that the reviewers raised about the effectiveness of the proposed approach, I do not think that the paper is quite ready for publication at this time, though I would encourage the authors to revise and resubmit the work at the next opportunity.	decision

2020-1377	This paper presents Capacity-Limited Reinforcement Learning (CLRL) which builds on methods in soft RL to enable learning in agents with limited capacity. <sep>	abstract
2020-1377	The reviewers raised issues that were largely around three areas: there is a lack of clear motivation for the work, and many of the insights given lack intuition; many connections to related literature are missing; and the experimental results remain unconvincing. <sep>	weakness
2020-1377	Although the ideas presented in the paper are interesting, more work is required for this to be accepted.	decision
2020-1377	Therefore at this point, this is unfortunately a rejection.	decision

2020-1390	The article studies a student-teacher setting with over-realised student ReLU networks, with results on the types of solutions and dynamics.	abstract
2020-1390	The reviewers found the line of work interesting, but they also raised concerns about the novelty of the presented results, the description of previous works, settings and claims, and experiments.	weakness
2020-1390	The revision clarified some of the definitions, the nature of the observations, experiments, and related works, including a change of the title.	rebuttal_process
2020-1390	However, the reviewers still were not convinced, in particular with the interpretation of the results, and keep their original ratings.	rebuttal_process
2020-1390	With many points that were raised in the original reviews, the article would benefit from a more thorough revision.	suggestion

2020-1394	This paper proposes an end-to-end deep reinforcement learning-based algorithm for the 2D and 3D bin packing problems.	abstract
2020-1394	Its main contribution is conditional query learning (CQL) which allows effective decision over mutually conditioned action spaces through policy expressed as a sequence of conditional distributions.	abstract
2020-1394	Efficient neural architectures for modeling of such a policy is proposed.	abstract
2020-1394	Experiments validate the effectiveness of the algorithm through comparisons with genetic algorithm and vanilla RL baselines. <sep>	abstract
2020-1394	The presentation is clear and the results are interesting, but the novelty seems insufficient for *CONF*.	weakness
2020-1394	The proposed model is based on transformer with the following changes: <sep> * encoder: position embedding is removed, state embedding is added to the multi-head attention layer and feed forward layer of the original transformer encoder; <sep>	weakness
2020-1394	* decoder: three decoders one for the three steps, namely selection, rotation and location. <sep>	weakness
2020-1394	* training: actor-critic algorithm	weakness

2020-1416	The paper proposed an attention-forcing algorithm that guides the sequence-to-sequence model training to make it more stable.	abstract
2020-1416	But as pointed out by the reviewers, the proposed method requires alignment which is normally unavailable.	weakness
2020-1416	The solution to address that is using another teacher-forcing model, which can be expensive. <sep>	suggestion
2020-1416	The major concern about this paper is the experimental justification is not sufficient: <sep> * lack of evaluations of the proposed method on different tasks; <sep>	weakness
2020-1416	* lack of experiments on understanding how it interact with existing techniques such as scheduled sampling etc; <sep>	weakness
2020-1416	* lack of comparisons to related existing supervised attention mechanisms.	weakness

2020-1448	Using ideas from mean-field theory and statistical mechanics, this paper derives a principled way to analyze signal propagation through gated recurrent networks.	abstract
2020-1448	This analysis then allows for the development of a novel initialization scheme capable of mitigating subsequent training instabilities.	abstract
2020-1448	In the end, while reviewers appreciated some of the analytical insights provided, two still voted for rejection while one chose accept after the rebuttal and discussion period.	rating_summary
2020-1448	And as AC for this paper, I did not find sufficient evidence to overturn the reviewer majority for two primary reasons. <sep>	misc
2020-1448	First, the paper claims to demonstrate the efficacy of the proposed initialization scheme on multiple sequence tasks, but the presented experiments do not really involve representative testing scenarios as pointed out by reviewers.	weakness
2020-1448	Given that this is not a purely theoretical paper, but rather one suggesting practically-relevant initializations for RNNs, it seems important to actually demonstrate this on sequence data people in the community actually care about.	weakness
2020-1448	In fact, even the reviewer who voted for acceptance conceded that the presented results were not too convincing (basically limited to toy situations involving Cifar10 and MNIST data). <sep>	weakness
2020-1448	Secondly, all reviewers found parts of the paper difficult to digest, and while a future revision has been promised to provide clarity, no text was actually changed making updated evaluations problematic.	weakness
2020-1448	Note that the rebuttal mentions that the paper is written in a style that is common in the physics literature, and this appears to be a large part of the problem.	rebuttal_process
2020-1448	*CONF* is an ML conference and in this respect, to the extent possible it is important to frame relevant papers in an accessible way such that a broader segment of this community can benefit from the key message.	suggestion
2020-1448	At the very least, this will ensure that the reviewer pool is more equipped to properly appreciate the contribution.	suggestion
2020-1448	My own view is that this work can be reframed in such a way that it could be successfully submitted to another ML conference in the future.	suggestion

2020-1452	This is an interesting paper on an important topic.	strength
2020-1452	The reviewers identified a variety of issues both before and after the feedback period; I urge the authors to consider their comments as they continue to refine and extend their work.	rebuttal_process

2020-1461	The paper addresses the question of why warm starting could result in worse generalization ability than training from scratch.	abstract
2020-1461	The reviewers agree that increasing the circumstances in which warm starting could be applied is of interest, in particular to reduce training time and computational resources.	strength
2020-1461	However, the reviewers were unanimous in their opinion that the paper is not suitable for publication at *CONF* in its current form.	rating_summary
2020-1461	Concerns included that the analysis was not sufficiently focused and the experiments too small scale.	weakness
2020-1461	As the analysis component of the paper was considered to be limited, the experimental results were insufficient on the balance to push the paper to an acceptable state.	decision

2020-1463	The paper is on the borderline.	rating_summary
2020-1463	A rejection is proposed due to the percentage limitation of *CONF*.	decision

2020-1484	This paper addresses the problem of differential private data generator.	abstract
2020-1484	The paper presents a novel approach called G_PATE which builds on the existing PATE framework.	abstract
2020-1484	The main contribution is in using a student generator with an ensemble of teacher discriminators and in proposing a new private gradient aggregation mechanism which ensures differential privacy in the information flow from discriminator to generator. <sep>	abstract
2020-1484	Although the idea is interesting, there are significant concerns raised by the reviewers about the experiments and analysis done in the paper which seem to be valid and have not been addressed yet in the final revision.	rebuttal_process
2020-1484	I believe upon making significant changes to the paper, this could be a good contribution.	suggestion
2020-1484	Thus, as of now, I am recommending a Rejection.	decision

2020-1486	Thanks for your detailed feedback to the reviewers, which clarified us a lot in many respects. <sep>	misc
2020-1486	However, the novelty of this paper is rather marginal and given the  high competition at *CONF*2020, this paper is unfortunately below the bar. <sep>	decision
2020-1486	We hope that the reviewers' comments are useful for improving the paper for potential future publication.	misc

2020-1490	The authors present a new training procedure for generative models where the target and generated distributions are first mapped to a latent space and the divergence between then is minimised in this latent space.	abstract
2020-1490	The authors achieve state of the art results on two datasets. <sep>	abstract
2020-1490	All reviewers agreed that the idea was vert interesting and has a lot of potential.	strength
2020-1490	Unfortunately, in the initial version of the paper the main section (section 3) was not very clear with confusing notation and statements.	weakness
2020-1490	I thank the authors for taking this feedback positively and significantly revising the writeup.	misc
2020-1490	However, even after revising the writeup some of the ideas are still not clear.	rebuttal_process
2020-1490	In particular, during discussions between the AC and reviewers it was pointed out that the training procedure is still not convincing.	rebuttal_process
2020-1490	It was not clear whether the heuristic combination of the deterministic PGA parts of the objective (3) with the likelihood/VAE based terms (9) and (12,13), was conceptually very sound.	rebuttal_process
2020-1490	Unfortunately, most of the initial discussions with the authors revolved around clarity and once we crossed the "clarity" barrier there wasn't enough time to discuss the other technical details of the paper.	rebuttal_process
2020-1490	As a result, even though the paper seems interesting, the initial lack of clarity went against the paper. <sep>	rebuttal_process
2020-1490	In summary, based on the reviewer comments, I recommend that the paper cannot be accepted.	decision

2020-1492	This paper presents a method to compress DNNs by quantization.	abstract
2020-1492	The core idea is to use NAS techniques to adaptively set quantization bits at each layer.	abstract
2020-1492	The proposed method is shown to achieved good results on the standard benchmarks. <sep>	abstract
2020-1492	Through our final discussion, one reviewer agreed to raise the score from 'Reject' to 'Weak Reject',  but still on negative side.	rating_summary
2020-1492	Another reviewer was not satisfied with the author's rebuttal, particularly regarding the appropriateness of training strategy and evaluation.	rebuttal_process
2020-1492	Moreover, as reviewers pointed out, there were so many unclear writings and explanations in the original manuscript.	rebuttal_process
2020-1492	Although we admit that authors made great effort to address the comments, the revision seems too major and need to go through another complete peer reviewing.	rebuttal_process
2020-1492	As there was no strong opinion to push this paper, I'd like to recommend rejection.	decision

2020-1515	This manuscript outlines procedures to address fairness as measured by disparity in risk across groups.	abstract
2020-1515	The manuscript is primarily motivated by methods that can achieve "no-harm" fairness, ie, achieving fairness without increasing the risk in subgroups. <sep>	abstract
2020-1515	The reviewers and AC agree that the problem studied is timely and interesting.	strength
2020-1515	However, in reviews and discussion, the reviewers noted issues with clarity of the presentation, and sufficient justification of the results.	rebuttal_process
2020-1515	The consensus was that the manuscript in its current state is borderline, and would have to be significantly improved in terms of clarity of the discussion, and possibly improved methods that result in more convincing results.	rating_summary

2020-1517	This paper proposes a technique for training embodied agents to play Visual Hide and Seek where a prey must navigate in a simulated environment in order to avoid capture from a predator.	abstract
2020-1517	The model is trained to play this game from scratch without any prior knowledge of its visual world, and experiments and visualizations show that a representation of other agents automatically emerges in the learned representation.	abstract
2020-1517	Results suggest that, although agent weaknesses make the learning problem more challenging, they also cause useful features to emerge in the representation. <sep>	abstract
2020-1517	While reviewers found the paper explores an interesting direction, concerns were raised that many claims are unjustified.	weakness
2020-1517	For example, in the discussion phase a reviewer asked how can one infer "hider learns to first turn away from the seeker then run away" from a single transition frequency?	weakness
2020-1517	Or, the rebuttal mentions "The agent with visibility reward does not get the chance to learn features of self-visibility because of the limited speed hence the model received samples with significantly less variation of its self-visibility, which makes learning to discriminate self-visibility difficult".	weakness
2020-1517	What is the justification for this?	weakness
2020-1517	There could be more details in the paper and I'd also like to know if these findings were reached purely by looking at the histograms or by combining visual analysis with the histograms. <sep>	weakness
2020-1517	I suggest authors address these concerns and provide quantitative results for all of the claims in an improved iteration of this paper.	suggestion

2020-1518	This paper uses GAN for data augmentation to improve the performance of knowledge distillation. <sep>	abstract
2020-1518	Reviewers and AC commonly think the paper suffers from limited novelty and insufficient experimental supports/details. <sep>	weakness
2020-1518	Hence, I recommend rejection.	decision

2020-1527	The authors propose to use numerical differentiation to approximate the Jacobian while estimating the parameters for a collection of Hidden Markov Models (HMMs).	abstract
2020-1527	Two reviewers provided detailed and constructive comments, while unanimously rated weak rejection.	rating_summary
2020-1527	Reviewer #1 likes the general idea of the work, and consider the contribution to be sound.	strength
2020-1527	However, he concerns the reproducibility of the work due to the niche database from e-commerce applications.	weakness
2020-1527	Reviewer #2 concerns the poor presentation, especially section 3.	weakness
2020-1527	The authors respond to Reviewers' concerns but did not change the rating.	rebuttal_process
2020-1527	The ACs concur the concerns and the paper can not be accepted at its current state.	decision

2020-1530	This paper proposes a new type of Polynomial NN called Ladder Polynomial NN (LPNN) which is easy to train with general optimization algorithms and can be combined with techniques like batch normalization and dropout.	abstract
2020-1530	Experiments show it works better than FMs with simple classification and regression tasks, but no experiments are done in more complex tasks.	abstract
2020-1530	All reviewers agree the paper addresses an interesting question and makes some progress but the contribution is limited and there are still many ways to improve.	weakness

2020-1539	This paper introduces a further regularizer, retrospection loss, for training neural networks, which leverages past parameter states.	abstract
2020-1539	The authors added several ablation studies and extra experiments during the rebuttal, which are helpful to show that their method is useful.	rebuttal_process
2020-1539	However, this is still one of those papers that essentially proposes an additional heuristic to train deep news, which is helpful but not clearly motivated from a theoretical point of view (despite the intuitions).	weakness
2020-1539	Yes, it provides improvements across tasks but these are all relatively small, and the method is more involved.	weakness
2020-1539	Therefore, I am recommending rejection.	decision

2020-1558	The paper addresses image translation by extending prior models, eg CycleGAN, to domain pairs that have significantly different shape variations.	abstract
2020-1558	The main technical idea is to apply the translation directly on the deep feature maps (instead of on the pixel level). <sep>	abstract
2020-1558	While acknowledging that the proposed model is potentially useful, the reviewers raised several important concerns: <sep> (1) ill-posed formulation of the problem and what is desirable, (2) using fine-tuned/pre-trained VGG features, (3) computational cost of the proposed approach, ie training a cascade of pairs of translators (one pair per layer). <sep>	weakness
2020-1558	AC can confirm that all three reviewers have read the author responses.	misc
2020-1558	AC suggests, in its current state the manuscript is not ready for a publication.	decision
2020-1558	We hope the reviews are useful for improving and revising the paper.	misc

2020-1560	This paper proposes to use more varied geometric structures of latent spaces to capture the manifold structure of the data, and provide experiments with synthetic and real data that show some promise in terms of approximating manifolds. <sep>	abstract
2020-1560	While reviewers appreciate the motivation behind the paper and see that angle as potentially resulting in a strong paper in the future, they have concerns that the method is too complicated and that the experimental results are not fully convincing that the proposed method is useful, with also not enough ablation studies.	weakness
2020-1560	Authors provided some additional results and clarified explanations in their revisions, but reviewers still believe there is more work required to deliver a submission warranting acceptance in terms of justifying the complicated architecture experimentally. <sep>	rebuttal_process
2020-1560	Therefore, we do not recommend acceptance.	decision

2020-1569	This paper proposes Restricted AutoEncoders (REAs) for unsupervised feature selection, and applies and evaluates it in applications in biology.	abstract
2020-1569	The paper was reviewed by three experts.	misc
2020-1569	R1 recommends Weak Reject, identifying some specific technical concerns as well as questions about missing and unclear experimental details.	rating_summary
2020-1569	R2 recommends Reject, with concerns about limited novelty and unconvincing experimental results.	rating_summary
2020-1569	R3 recommends Weak Accept saying that the overall idea is good, but also feels the contribution is "severely undermined" by a recently-published paper that proposes a very similar approach.	rating_summary
2020-1569	Given that that paper (at ECMLPKDD 2019) was presented just one week before the deadline for *CONF*, we would not have expected the authors to cite the paper.	ac_disagreement
2020-1569	Nevertheless, given the concerns expressed by the other reviewers and the lack of an author response to help clarify the novelty, technical concerns, and missing details, we are not able to recommend acceptance.	decision
2020-1569	We believe the paper does have significant merit and hope that the reviewer comments will help authors in preparing a revision for another venue.	decision

2020-1572	The authors introduce a notion of stability to pruning and argue through empirical evaluation that pruning leads to improved generalization when it introduces instability.	abstract
2020-1572	The reviewers were largely unconvinced, though for very different reasons.	weakness
2020-1572	The idea that "Bayesian ideas" explain what's going on seems obviously wrong to me.	weakness
2020-1572	The third reviewer seems to think there's a tautology lurking here and that doesn't seem to be true to me either.	weakness
2020-1572	It is disappointing that the reviewers did not re-engage with the authors after the authors produced extensive rebuttals.	rebuttal_process
2020-1572	Unfortunately, this is a widespread pattern this year. <sep>	misc
2020-1572	Even though I'm inclined to ignore aspects of these reviews, I feel that there needs to be a broader empirical study to confirm these findings.	weakness
2020-1572	In the next iteration of the paper, I believe it may also be important to relate these ideas to [1].	suggestion
2020-1572	It would be interesting to compare also on the networks studied in [1], which are more diverse. <sep>	suggestion
2020-1572	[1] The Lottery Ticket Hypothesis at Scale (Jonathan Frankle, Gintare Karolina Dziugaite, Daniel M. Roy, and Michael Carbin) https://arxiv.org/abs/1903.01611	misc

2020-1573	This paper introduces the idea of "empathy" to improve learning in communication emergence.	abstract
2020-1573	The reviewers all agree that the idea is interesting and well described.	strength
2020-1573	However, this paper clearly falls short on delivering the detailed and sufficient experiments and results to demonstrate whether and how the idea works. <sep>	weakness
2020-1573	I thank the authors for submitting this research to *CONF* and encourage following up on the reviewers' comments and suggestions for future submission.	decision

2020-1581	This submission analyses the numerical invertibility of analytically invertible neural networks and shows that analytical invertibility does not guarantee numerical invertibility of some invertible networks under certain conditions (eg adversarial perturbation). <sep>	abstract
2020-1581	Strengths: <sep> -The work is interesting and the theoretical analysis is insightful. <sep>	strength
2020-1581	Weaknesses: <sep> -The main concern shared by all reviewers was the weakness of the experimental section including (i) insufficient motivation of the decorrelation task; (ii) missing comparisons and experimental settings. <sep>	weakness
2020-1581	-The paper clarity could be improved. <sep>	weakness
2020-1581	Both weaknesses were not sufficiently addressed in the rebuttal.	rebuttal_process
2020-1581	All reviewer recommendations were borderline to reject.	rating_summary

2020-1596	Quoting from R3: "This paper proposes and analyzes a new loss function for linear autoencoders (LAEs) whose minima directly recover the principal components of the data.	abstract
2020-1596	The core idea is to simultaneously solve a set of MSE LAE problems with tied weights and increasingly stringent masks on the encoder/decoder matrices."	abstract
2020-1596	With two weak acceptance recommendations and a recommendation for rejection, this paper is borderline in terms of its scores. <sep>	rating_summary
2020-1596	The approach and idea are interesting.	strength
2020-1596	The main shortcoming of the paper, as highlighted by the reviewers, is that the approach and theoretical analysis are not properly motivated to solve an actual problem faced in real-world data.	weakness
2020-1596	The approach does not provide a better algorithm for recovering the eigenvectors of the data, nor is it proposed as part of a learning framework to solve a real-world problem.	weakness
2020-1596	Experiments are shown on synthetic data and MNIST.	weakness
2020-1596	As a stand-alone theoretical result, it leaves open questions as to the proposed utility.	weakness

2020-1598	This submission proposes a statistically consistent saliency estimation method for visual model explainability. <sep>	abstract
2020-1598	Strengths: <sep> -The method is novel, interesting, and passes some recently proposed sanity checks for these methods. <sep>	strength
2020-1598	Weaknesses: <sep> -The evaluation was flawed in several aspects. <sep>	weakness
2020-1598	-The readability needed improvement. <sep>	weakness
2020-1598	After the author feedback period remaining issues were: <sep> -A discussion of two points is missing: (i) why are these models so sensitive to the resolution of the saliency map?	weakness
2020-1598	How does the performance of LEG change with the resolution (eg does it degrade for higher resolution?)?	rebuttal_process
2020-1598	(ii) Figure 6 suggests that SHAP performs best at identifying "pixels that are crucial for the predictions".	rebuttal_process
2020-1598	However, the authors use Figure 7 to argue that LEG is better at identifying salient "pixels that are more likely to be relevant for the prediction".	rebuttal_process
2020-1598	These two observations are contradictory and should be resolved. <sep>	rebuttal_process
2020-1598	-The evaluation is still missing some key details for interpreting the results.	rebuttal_process
2020-1598	For example, how representative are the 3 images chosen in Figure 7?	rebuttal_process
2020-1598	Also, in section 5.1 the authors don't describe how many images are included in their sanity check analysis or how those images were chosen. <sep>	rebuttal_process
2020-1598	-The new discussion section is not actually a discussion section but a conclusion/summary section. <sep>	rebuttal_process
2020-1598	Because of these issues, AC believes that the work is theoretically interesting but has not been sufficiently validated experimentally and does not give the reader sufficient insight into how it works and how it compares to other methods.	weakness
2020-1598	Note also that the submission is also now more than 9 pages long, which requires that it be held to a higher standard of acceptance. <sep>	weakness
2020-1598	Reviewers largely agreed with the stated shortcomings but were divided on their significance. <sep>	weakness
2020-1598	AC shares the recommendation to reject.	decision

2020-1599	This manuscript proposed biologically-inspired modifications to convolutional neural networks including differences of Gaussians convolutional filter, a truncated ReLU, and a modified projected normalization layer.	abstract
2020-1599	The authors' results indicate that the modifications improve performance as well as improved robustness to adversarial attacks. <sep>	abstract
2020-1599	The reviewers and AC agree that the problem studied is timely and interesting, and closely related to a variety of recent work on robust model architectures.	strength
2020-1599	However, this manuscript also received quite divergent reviews, resulting from differences in opinion about the novelty and importance of the results.	weakness
2020-1599	In reviews and discussion, the reviewers noted issues with clarity of the presentation and sufficient justification of the approach and results.	weakness
2020-1599	In the opinion of the AC,  the manuscript in its current state is borderline and could be improved with more convincing empirical justification.	weakness

2020-1605	The paper proposed a method for training multiple transformers with tied parameters and enabling dynamic choice of the number of encoder and decoder layers.	abstract
2020-1605	The method is evaluated in neural machine translation and shown to reduce decoding costs without compromising translation quality.	abstract
2020-1605	The reviewers generally agreed that the proposed method is interesting, but raised issues regarding the significance of the claimed benefits and the quality of overall presentation of the paper.	weakness
2020-1605	Based on a consensus reached in a post rebuttal discussion with the reviewers, I am recommending rejecting this paper.	decision

2020-1627	This paper offers likely novel schemes for image resizing.	abstract
2020-1627	The performance improvement is clear.	strength
2020-1627	Unfortunately two reviewers find substantial clarity issues in the manuscript after revision, and the AC concurs that this is still an issue.	rebuttal_process
2020-1627	The paper is borderline but given the number of higher ranked papers in the pool is unable to be accepted unfortunately.	decision

2020-1646	The submission performs empirical analysis on f-VIM (Ke, 2019), a method for imitation learning by f-divergence minimization.	abstract
2020-1646	The paper especially focues on a state-only formulation akin to GAILfO (Torabi et al, 2018b).	abstract
2020-1646	The main contributions are: <sep> 1) The paper identifies numerical proplems with the output activations of f-VIM and suggest a scheme to choose them such that the resulting rewards are bounded. <sep>	strength
2020-1646	2) A regularizer that was proposed by Mescheder et al (2018) for GANs is tested in the adversarial imitation learning setting. <sep>	strength
2020-1646	3) In order to handle state-only demonstrations, the technique of GAILfO is applied to f-VIM (then denoted f-VIMO) which inputs state-nextStates instead of state-actions to the discriminator. <sep>	strength
2020-1646	The reviewers found the submitted paper hard to follow, which suggests a revision might make more apparent the author's contributions in later submissions of this work.	decision

2020-1654	This paper proposes a very interesting alternative to feed-forward network layers, based on Quaternion methods and Hamilton products, which has the benefit of reducing the number of parameters in the neural network (more than 50% smaller) without sacrificing performance.	abstract
2020-1654	They conducted extensive experiments on language tasks (NMT and NLI, among others) using transformers and LSTMs. <sep>	abstract
2020-1654	The paper appears to be clearly presented and have extensive results on a variety of tasks.	strength
2020-1654	However all reviewers pointed out that there is a lack of in-depth analysis and thus insight into why this approach works, as well as questions on the specific effects of regularization.	weakness
2020-1654	These concerns were not addressed in the rebuttal period, instead leaving it to future work.	rebuttal_process
2020-1654	My assessment is that, with further analysis, ablation studies, and comparison to alternative methods for reducing model size (quantization, etc), this paper has the potential to be quite impactful, and I look forward to future versions of this work.	suggestion
2020-1654	As it currently stands, however, I don't believe it's suitable for publication at *CONF*.	decision

2020-1662	All three reviewers, even after the rebuttal, agreed that the paper did not meet with bar for acceptance.	rating_summary
2020-1662	A common complaint was lack of clarity being a major problem.	weakness
2020-1662	Unfortunately, the paper cannot be accepted in its current form.	decision
2020-1662	The authors are encouraged to improve the presentation of their approach  and resubmit to a new venue.	decision

2020-1667	All the reviewers reach a consensus to reject the current submission. <sep>	rating_summary
2020-1667	In addition, there are two assumptions in the proof which seemed never included in Theorem conditions or verified in typical cases. <sep>	weakness
2020-1667	1) Between Eq (16) and (17), the authors assumed the 'extended restricted strong convexity' given by the un-numbered equation. <sep>	weakness
2020-1667	2) In eq (25), the authors assume the existence of \\sigma making the inequality true. <sep>	weakness
2020-1667	However those assumptions are neither explicitly stated in theorem conditions, nor verified for typical cases in applications, eg even the square or logistic loss.	weakness
2020-1667	The authors need to address these assumptions explicitly rather than using them from nowhere.	suggestion

2020-1676	The authors made no response to reviewers.	rebuttal_process
2020-1676	Based on current reviews, the paper is suggested a rejection as majority.	rating_summary

2020-1677	Agreement by the reviewers: although the idea is good, the paper is very hard to read and not accurately enough formulated to merit publication. <sep>	rating_summary
2020-1677	This can be repaired, and the authors should try again after a thorough revision and rewrite.	suggestion

2020-1678	The paper proposes a text normalisation model for Amharic text.	abstract
2020-1678	The model uses word classification, followed by a character-based GRU attentive encoder-decoder model.	abstract
2020-1678	The paper is very short and does not present reproducible experiments.	weakness
2020-1678	It also does not conform to the style guidelines of the conference.	weakness
2020-1678	There has been no discussion of this paper beyond the initial reviews, all of which reject it with a score of 1.	rating_summary
2020-1678	It is not ready to publish and the authors should consider a more NLP focussed venue for future research of this kind.	decision

2020-1685	The paper proposed a new seq2seq method to implement natural language to formal language translation.	abstract
2020-1685	Fixed length Tensor Product Representations are used as the intermediate representation between encoder and decoder.	abstract
2020-1685	Experiments are conducted on MathQA and AlgoList datasets and show the effectiveness of the methods.	abstract
2020-1685	Intensive discussions happened between the authors and reviewers.	rebuttal_process
2020-1685	Despite of the various concerns raised by the reviewers, a main problem pointed by both reviewer#3 and reviewer#4 is that there is a gap between the  theory and the implementation in this paper.	weakness
2020-1685	The other reviewer (#2) likes the paper but is less confident and tend to agree with the other two reviewers.	rating_summary

2020-1689	This work considers the popular LQR objective but with [A,B] unknown and dynamically changing.	abstract
2020-1689	At each time a context [C,D] is observed and it is assumed there exist a linear map Theta from [C,D] to [A,B].	abstract
2020-1689	The particular problem statement is novel, but is heavily influenced by other MDP settings and the also follows very closely to previous works.	weakness
2020-1689	The algorithm seems computationally intractable (a problem shared by previous work this work builds on) and so in experiments a gross approximation is used. <sep>	weakness
2020-1689	Reviewers found the work very stylized and did not adequately review related work.	weakness
2020-1689	For example, little attention is paid to switching linear systems and the recent LQR advances are relegated to a list of references with no discussion.	weakness
2020-1689	The reviewers also questioned how the theory relates to the traditional setting of LQR regret, say, if [C,D] were identity at all times so that Theta = [A,B]. <sep>	weakness
2020-1689	This paper received 3 reviews (a third was added late to the process) and my own opinion influenced the decision.	misc
2020-1689	While the problem statement is interesting, the work fails to put the paper in context with the existing work, and there are some questions of algorithm methods.	weakness

2020-1690	The paper proposes a new problem setting of predicate zero-shot learning for visual relation recognition for the setting when some of the predicates are missing, and a model that is able to address it. <sep>	abstract
2020-1690	All reviewers agreed that the problem setting is interesting and important, but had reservations about the proposed model.	weakness
2020-1690	In particular, the reviewers were concerned that it is too simple of a step from existing methods.	weakness
2020-1690	One reviewer also pointed towards potential comparisons with other zero-shot methods. <sep>	weakness
2020-1690	Following that discussion, I recommend rejection at this time but highly encourage the authors to take the feedback into account and resubmit to another venue.	decision

2020-1694	The method proposed and explored here is to introduce small spatial distortions, with the goal of making them undetectable by humans but affecting the classification of the images.	abstract
2020-1694	As reviewers point out, very similar methods have been tested before.	weakness
2020-1694	The methods are also only tested on a few low-resolution datasets. <sep>	weakness
2020-1694	The reviewers are unanimous in their judgement that the method is not novel enough, and the authors' rebuttals have not convinced the reviewers or me about the opposite.	rebuttal_process

2020-1701	The authors propose a hybrid model-free/model-based policy gradient method that attempts to reduce sample complexity without degrading asymptotic performance.	abstract
2020-1701	They evaluate their approach on a collection of benchmark tests. <sep>	abstract
2020-1701	The reviewers raised concerns about limited novelty of the proposed approach and flaws in the evaluation.	weakness
2020-1701	The authors need to compare to more baselines and ensure that the baseline algorithms are performing as previously reported.	suggestion
2020-1701	Even then, the reported improvements were small. <sep>	weakness
2020-1701	Given the issues raised by the reviewers, this paper is not ready for publication at *CONF*.	decision

2020-1708	The authors present an approach to large scale bitext extraction from Wikipedia.	abstract
2020-1708	This builds heavily on previous work, with the novelty being somewhat minor efficient approximate K-nearest neighbor search and language agnostic parameters such as cutoffs.	weakness
2020-1708	These techniques have not been validated on other data sets and it is unclear how well they generalise.	weakness
2020-1708	The major contribution of the paper is the corpus created, consisting of 85 languages, 1620 language pairs and 135M parallel sentences, of which most do not include English.	strength
2020-1708	This corpus is very valuable and already in use in the field, but IMO *CONF* is not the right venue for this kind of publication.	decision
2020-1708	There were four reviews, all broadly in agreement, and some discussion with the authors.	rating_summary

2020-1735	The authors propose an approach for anomaly detection in the setting where the training data includes both normal and anomalous data.	abstract
2020-1735	Their approach is a fairly straightforward extension of existing ideas, in which they iterate between clustering the data into normal vs. anomalous and learning an autoencoder representation of normal data that is then used to score normality of new data.	abstract
2020-1735	The results are promising, but the experiments are fairly limited.	weakness
2020-1735	The authors argue that their experimental settings follow those of prior work, but I think that for such an incremental contribution, more empirical work should be done, regardless of the limitations of particular prior work.	rebuttal_process

2020-1742	This paper tackles the problem of exploration in RL.	abstract
2020-1742	In order to maximize coverage of the state space, the authors introduce an approach where the agent attempts to reach some self-set goals.	abstract
2020-1742	The empirically show that agents using this method uniformly visit all valid states under certain conditions.	abstract
2020-1742	They also show that these agents are able to learn behaviours without providing a manually-defined reward function. <sep>	abstract
2020-1742	The drawback of this work is the combined lack of theoretical justification and limited (marginal) algorithmic novelty given other existing goal-directed techniques.	weakness
2020-1742	Although they highlight the performance of the proposed approach, the current experiments do not convey a good enough understanding of why this approach works where other existing goal-directed techniques do not, which would be expected from a purely empirical paper.	weakness
2020-1742	This dampers the contribution, hence I recommend to reject this paper.	decision

2020-1781	This paper proposes a new formulation of the non-local block and interpret it from the graph view.	abstract
2020-1781	The idea is interesting and the experimental results seems to be promising. <sep>	strength
2020-1781	Reviewer has two major concerns.	misc
2020-1781	The first is the presentation, which is not clear enough.	weakness
2020-1781	The second is the experimental design and analysis.	weakness
2020-1781	The authors add more video dataset in the revision, but still lack comprehensive experimental analysis for video-based applications. <sep>	rebuttal_process
2020-1781	Overall, the idea of non-local block from graph view is interesting.	strength
2020-1781	However, the presentation of the paper needs further polish and thus does not meet the standard of *CONF*	decision

2020-1785	This paper was assessed by three reviewers who scored it as 6/3/6. <sep>	rating_summary
2020-1785	The reviewers liked some aspects of this paper eg, a good performance, but they also criticized some aspects of work such as inventing new names for existing pooling operators, observation that large parts of improvements come from the pre-processing step rather than the proposed method, suspected overfitting.	weakness
2020-1785	Taking into account all positives and negatives, AC feels that while the proposed idea has some positives, it also falls short of the quality required by *CONF*2020, thus it cannot be accepted at this time.	decision
2020-1785	AC strongly encourages authors to go through all comments (especially these negative ones), address them and resubmit an improved version to another venue.	decision

2020-1787	The paper considers planning through the lenses both of a single and multiple objectives.	abstract
2020-1787	The paper then discusses the pareto frontiers of this optimization.	abstract
2020-1787	While this is an interesting direction, the reviewers feel a more careful comparison to related work is needed.	weakness

2020-1788	The paper proposes to get universal adversarial examples using few test samples.	abstract
2020-1788	The approach is very close to the Khrulkov & Oseledets, and the abstract for some reason claims that it was proposed independently, which looks like a very strange claim.	weakness
2020-1788	Overall, all reviewers recommend rejection, and I agree with them.	decision

2020-1790	The paper is proposed a rejection based on majority reviews.	decision

2020-1796	The authors demonstrate how neural networks can be used to learn vectorial representations of a set of items given only triplet comparisons among those items.	abstract
2020-1796	The reviewers had some concerns regarding the scale of the experiments and strength of the conclusions:  empirically, it seemed like there should be more truly large-scale experiments considering that this is a selling point; there should have been more analysis and/or discussion of why/how the neural networks help; and the claim that deep networks are approximately solving an NP-hard problem seemed unimportant as they are routinely used for this purpose in ML problems.	weakness
2020-1796	With a combination of improved experiments and revised discussion/analysis, I believe a revised version of this paper could make a good submission to a future conference.	decision

2020-1851	This paper proposes to address the issue of biases and artifacts in benchmark datasets through the use of adversarial filtering.	abstract
2020-1851	That is, removing training and test examples that a baseline model or ensemble gets wright. <sep>	abstract
2020-1851	The paper is borderline, and could have flipped to an accept if the target acceptance rate for the conference were a bit higher.	rating_summary
2020-1851	All three reviewers ultimately voted weakly in favor of it, especially after the addition of the new out-of-domain generalization results.	rating_summary
2020-1851	However, reviewers found it confusing in places, and R2 wasn't fully convinced that this should be applied in the settings the authors suggest.	weakness
2020-1851	This paper raises some interesting and controversial points, but after some private discussion, there wasn't a clear consensus that publishing it as is would do more good than harm.	decision

2020-1858	The paper proposes a method to produce embeddings of discrete objects, jointly learning a small set of anchor embeddings and a sparse transformation from anchor objects to all the others.	abstract
2020-1858	While the paper is well written, and proposes an interesting solution, the contribution seems rather incremental (as noted by several reviewers), considering the existing literature in the area.	weakness
2020-1858	Also, after discussions the usefulness of the method remains a bit unclear - it seems some engineering (related to sparse operations) is still required to validate the viability of the approach.	weakness

2020-1869	This paper focuses on finding universal adversarial perturbations, that is, a single noise pattern that can be applied to any input to fool the network in many cases.	abstract
2020-1869	Further more, it focuses on the data-free setting, where such a perturbation is found without having access to data (images) from the distribution that train- and test data comes from. <sep>	abstract
2020-1869	The reviewers were very conflicted about this paper.	misc
2020-1869	Among others, the strong experimental results and the clarity of writing and analysis were praised.	strength
2020-1869	However, there was also criticism of the amount of novelty compared to GDUAP, on the strong assumptions needed (potentially limiting the applicability), and on some weakness in the theoretical analysis. <sep>	weakness
2020-1869	In the end, the paper seems in current form not convincing enough for me to recommend acceptance for *CONF*.	decision

2020-1873	A defense against of adversarial attacks is presented, which builds mostly on combining known methods in a novel way.	abstract
2020-1873	While the novelty is somewhat limited, this would be fine if the results were unequivocally good and other parts of the problematic.	weakness
2020-1873	However, reviewers were not entirely convinced by the results, and had a number of minor complaints with various parts of the paper. <sep>	weakness
2020-1873	In sum, this paper is not currently at a stage where it can be accepted.	decision

2020-1882	This paper demonstrates a framework for optimizing designs in auction/contest problems.	abstract
2020-1882	The approach relies on considering a multi-agent learning process and then simulating it. <sep>	abstract
2020-1882	To a large degree there is agreement among reviewers that this approach is sensible and sound, however lacks substantial novelty.	weakness
2020-1882	The authors provided a rebuttal which clarified the aspects that they consider novel, however the reviewers remained mostly unconvinced.	rebuttal_process
2020-1882	Furthermore, it would help if the improvement over past approaches is demonstrated in a more convincing way, for example with increased scope experiments that also involve richer analysis.	suggestion

2020-1895	The paper proposes ATR-CSPD, which learns a low-dimensional representation of seasonal pattern, for detecting changes with clustering-based approaches. <sep>	abstract
2020-1895	While ATR-CSPD is simple and intuitive, it lacks novel contribution in methodology.	weakness
2020-1895	It is unclear how it is different from existing approaches.	weakness
2020-1895	The evaluation and the writing could be improved significantly. <sep>	weakness
2020-1895	In short, the paper is not ready for publication.	decision
2020-1895	We hope the reviews can help improve the paper for a strong submission in the future.	misc

2020-1905	This work presents a method for generating an (approximately) minimal adversarial perturbation for neural networks.	abstract
2020-1905	During the discussion period, the AC raised additional concerns that were not originally addressed by the reviewers.	rebuttal_process
2020-1905	The method is an iterative first order method for solving constrained optimization problems, however when considered as a new first order optimization method the contribution seems minimal.	rebuttal_process
2020-1905	Most of the additions are rather straightforward---eg using a line search at each step to determine the optimal step size---and the reported gains over PGD are unconvincing.	rebuttal_process
2020-1905	PGD can be considered as a "universal" first order optimizer [1], as such we should be careful that the reported gains are substantial and not just a question of tuning.	rebuttal_process
2020-1905	Given that using a line search at each step increases the computational cost by a multiplicative factor, the comparison with PGD should take this into account. <sep>	rebuttal_process
2020-1905	The AC notes several plots in the Appendix show PGD having better performance (particularly on restricted Imagenet), and for others there remain questions on how PGD is tuned (for example the CIFAR-10 plots in Figure 5).	rebuttal_process
2020-1905	One of two things explains the discrepancies in Figure 5: either PGD is finding a worse local optimum than FAB, or PGD has not converged to a local optimum.	rebuttal_process
2020-1905	There needs to be provided experiments to rule out the second possibility, as this is evidence that PGD is not being tuned properly.	rebuttal_process
2020-1905	Some standard things to check are the step size and number of steps.	rebuttal_process
2020-1905	Additionally, enforcing a constant step size after projection is an easy way to improve the performance of PGD.	rebuttal_process
2020-1905	For example, if the gradient of the loss is approximately equal to the normal vector of the constraint, then proj(x_i+ lambda * g) ~ x_i will result in an effective step size that is too low to make progress. <sep>	rebuttal_process
2020-1905	Finally, it is unclear what practical use there is for a method that finds an approximately minimum norm perturbation.	rebuttal_process
2020-1905	There are no provable guarantees so this cannot be used for certification.	rebuttal_process
2020-1905	Additionally, in order to properly assess the security and reliability of ML systems, it is necessary to consider larger visual distortions, occlusions, and corruptions (such as the ones in [2]) as these will actually be encountered in practice. <sep>	rebuttal_process
2020-1905	1. https://arxiv.org/pdf/1706.06083. pdf <sep>	misc
2020-1905	2. https://arxiv.org/abs/1807.01697	misc

2020-1929	Thanks for the detailed replies to the reviewers, which significantly helped us understand your paper better. <sep>	misc
2020-1929	However, after all, we decided not to accept your paper due to weak justification and limited experimental validation.	decision
2020-1929	Writing should also be improved significantly.	weakness
2020-1929	We hope that the feedback from the reviewers help you improve your paper for potential future submission.	decision

2020-1937	This paper proposes a relation-based model that extends VAE to explicitly alleviate the domain bias problem between seen and unseen classes in the setting of generalized zero-shot learning. <sep>	abstract
2020-1937	Reviewers and AC think that the studied problem is interesting, the reported experimental results are strong, and the writing is clear, but the proposed model and its scientific reasoning for convincing why the proposed method is valuable is somewhat limited.	weakness
2020-1937	Thus the authors are encouraged to further improve in these directions.	suggestion
2020-1937	In particular: <sep> - The idea of using a variant of the widely-used domain discriminator to make seen and unseen classes distinguishable is somewhat contradicted to the basic principle of zero-shot learning.	weakness
2020-1937	How to trade off the balance between seen and unseen classes has been an important problem in generalized ZSL.	weakness
2020-1937	These problems need further elaboration. <sep>	suggestion
2020-1937	- The proposed model itself is not a real "VAE", making the value of an extensive derivation based on variational inference less prominent. <sep>	weakness
2020-1937	- There is also the need to compare with the baselines mentioned by the reviewers. <sep>	suggestion
2020-1937	Overall, this is a borderline paper.	misc
2020-1937	Since the above concerns were not addressed convincingly in the rebuttal, I am leaning towards rejection.	decision

2020-1955	This paper proposes a method to learn sentence representations that incorporates linguistic knowledge in the form of dependency trees using contrastive learning.	abstract
2020-1955	Experiments on SentEval and probing tasks show that the proposed method underperform baseline methods. <sep>	abstract
2020-1955	All reviewers agree that the results are not strong enough to support the claim of the paper and have some concerns about the scalability of the implementation.	weakness
2020-1955	They also agree that the writing of the paper can be improved (details included in their reviews below). <sep>	weakness
2020-1955	The authors acknowledged these concerns and mentioned that they will use them to improve the paper for future work, so I recommend rejecting this paper for *CONF*.	decision

2020-1956	A nice paper, but quite some unclarities; it's unclear  in particular if the paper improves wrt SOTA.	weakness
2020-1956	Esp. scaling is an issue here.	weakness
2020-1956	Also, the understandability is below par and more work can make this into an acceptable submission.	decision

2020-1960	The authors discuss how to predict generalization gaps.	abstract
2020-1960	Reviews are mixed, putting the submission in the lower half of this year's submissions.	rating_summary
2020-1960	I also would have liked to see a comparison with other divergence metrics, for example, L1, MMD, H-distance, discrepancy distance, and learned representations (eg, BERT, Laser, etc., for language).	weakness
2020-1960	Without this, the empirical evaluation of FD is a bit weak.	weakness
2020-1960	Also, the obvious next step would be trying to minimize FD in the context of domain adaptation, and the question is if this shouldn't already be part of your paper?	weakness
2020-1960	Suggestions: The Amazon reviews are time-stamped, enabling you to run experiments with drift over time.	suggestion
2020-1960	See [0] for an example. <sep>	misc
2020-1960	[0] https://www.aclweb.org/anthology/W18-6210/	misc

2020-1963	The paper received mixed reviews.	misc
2020-1963	On one hand, there is interesting novelty in relation to biological vision systems.	strength
2020-1963	On the other hand, there are some serious experimental issues with the machine learning model.	weakness
2020-1963	While reviewers initially raised concerns about the motivation of the work, the rebuttal addressed those concerns.	rebuttal_process
2020-1963	However, concerns about experiments remained.	rebuttal_process

2020-1964	The author propose a method to first learn policies for intrinsically generated goal-based tasks, and then leverage the learned representations to improve the learning of a new task in a generalized policy iteration framework.	abstract
2020-1964	The reviewers had significant issues about clarity of writing that were largely addressed in the rebuttal.	rebuttal_process
2020-1964	However, there were also concerns about the magnitude of the contribution (especially if it was added anything significant to the existing literature on GPI, successor features, etc), and the simplicity (and small number of) test domains.	weakness
2020-1964	These concerns persisted after the rebuttal and discussion.	rebuttal_process
2020-1964	Thus, I recommend rejection at this time.	decision

2020-1965	This paper proposes a new method for zero-shot policy transfer in RL.	abstract
2020-1965	The authors propose learning the policy over a disentangled representation that is augmented with attention.	abstract
2020-1965	Hence, the paper is a simple modification of an existing approach (DARLA).	abstract
2020-1965	The reviewers agreed that the novelty of the proposed approach and the experimental evaluation are limited.	weakness
2020-1965	For this reason I recommend rejection.	decision

2020-1967	This paper studies the impact of embedding complexity on domain-invariant representations by incorporating embedding complexity into the previous upper bound explicitly. <sep>	abstract
2020-1967	The idea of embedding complexity is interesting, the exploration has some useful insight, and the paper is well-written.	strength
2020-1967	However, Reviewers and AC generally agree that the current version can be significantly improved in several ways: <sep> - The proposed upper bound has several limitations such as looser than existing ones. <sep>	weakness
2020-1967	- The embedding complexity is only addressed implicitly, which shares similar idea with previous works. <sep>	weakness
2020-1967	- The claim of implicit regularization has not been explored in-depth. <sep>	weakness
2020-1967	- The proposed MDM method seems to be incremental and related closely with the embedding complexity. <sep>	weakness
2020-1967	- There is no analysis about the generalization when estimating this upper bound from finite samples. <sep>	weakness
2020-1967	There are important details requiring further elaboration.	weakness
2020-1967	So I recommend rejection.	decision

2020-2005	I had a little bit of difficulty with my recommendation here, but in the end I don't feel confident in recommending this paper for acceptance, with my concerns largely boiling down to the lack of clear description of the overall motivation. <sep>	decision
2020-2005	Standard adversarial attacks are meant to be *imperceptible* changes that do not change the underlying semantics of the input to the human eye.	abstract
2020-2005	In other words, the goal of the current work, generating "semantically meaningful" perturbations goes against the standard definition of adversarial attacks.	abstract
2020-2005	This left me with two questions: <sep> 1. Under the definition of semantic adversarial attacks, what is to prevent someone from swapping out the current image with an entirely different image?	weakness
2020-2005	From what I saw in the evaluation measures utilized in the paper, such a method would be judged as having performed a successful attack, and given no constraints there is nothing stopping this. <sep>	weakness
2020-2005	2. In what situation would such an attack method would be practically useful? <sep>	weakness
2020-2005	Even the reviewers who reviewed the paper favorably were not able to provide answers to these questions, and I was not able to resolve this from my reading of the paper as well.	weakness
2020-2005	I do understand that there is a challenge on this by Google.	weakness
2020-2005	In my opinion, even this contest is somewhat ill-defined, but it also features extensive human evaluation to evaluate the validity of the perturbations, which is not featured in the experimental evaluation here. <sep>	weakness
2020-2005	While I think this work is potentially interesting, it seems that there are too many open questions that are not resolved yet to recommend acceptance at this time, but I would encourage the authors to tighten up the argumentation/evaluation in this regard and revise the paper to be better accordingly!	decision

2020-2012	The paper proposes a top-down approach to train deep neural networks -- freezing top layers after supervised pre-training, then re-initializing and retraining the bottom layers.	abstract
2020-2012	As mentioned by all the reviewers, the novelty is on the low side.	weakness
2020-2012	The paper is purely experimental (no theory), and the experimental section is currently too weak.	weakness
2020-2012	In particular: <sep> - Experiments on different domains should be performed. <sep>	weakness
2020-2012	- Different models should be evaluated. <sep>	weakness
2020-2012	- Ablation experiments should be performed to understand better under which conditions the proposed approach works. <sep>	weakness
2020-2012	- For speech recognition, WER should be reported - even if it is without a LM - such that one can compare with existing work.	weakness

2020-2027	The reviewers kept their scores after the author response period, pointing to continued concerns with methodology, needing increased exposition in parts, and not being able to verify theoretical results.	rebuttal_process
2020-2027	As such, my recommendation is to improve the clarity around the methodological and theoretical contributions in a revision.	suggestion

2020-2031	This paper proposes a method for improving exploration by implementing intrinsic rewards based on optical flow prediction error.	abstract
2020-2031	The approach was evaluated on several Atari games, Super Mario, and VizDoom. <sep>	abstract
2020-2031	There are several strengths to this work, including the fact that it comes with open source code, and several reviewers agree it's an interesting approach.	strength
2020-2031	R1 thought it was well-written and quite easy to follow.	strength
2020-2031	I also commend the authors for being so responsive with comments and for adding the new experiments that were asked for. <sep>	rebuttal_process
2020-2031	The main issue that reviewers pointed out, and which I am also concerned about, is how these particular games were chosen.	weakness
2020-2031	R3 points out that these 5 Atari games are not known for being hard exploration games.	weakness
2020-2031	Authors did conduct further experiments on 6 Atari games suggested by the reviewer, but the results didn't show significant improvement over baselines. <sep>	rebuttal_process
2020-2031	I appreciate the authors' argument that every method has "its niche", but the environments chosen must still be properly motivated.	rebuttal_process
2020-2031	I would have preferred to see results on all Atari games, along with detailed and quantitative analysis into why FICM fails on specific tasks.	rebuttal_process
2020-2031	For instance, they state in the rebuttal that "The selection criteria of our environments is determined by the relevance of motions of the foreground and background components (including the controllable agent and the uncontrollable objects) to the performance (ie, obtainable scores) of the agent."	rebuttal_process
2020-2031	But it doesn't seem like this was assessed in any quantitative way.	rebuttal_process
2020-2031	Without this understanding, it'd be difficult for an outsider to know which tasks are appropriate to use with this approach.	rebuttal_process
2020-2031	I urge the authors to focus on expanding and quantifying the work they depict in Figure 8, which, although it begins to illuminate why FICM works for some games and not others, is still only a qualitative snapshot of 2 games.	suggestion
2020-2031	I still think this is a very interesting approach and look forward to future versions of this paper.	decision

2020-2046	The paper investigates a new approach to classification of irregularly sampled and unaligned multi-modal time series via set function mapping.	abstract
2020-2046	Experiment results on health care datasets are reported to demonstrate the effectiveness of the proposed approach. <sep>	abstract
2020-2046	The idea of extending set functions to address missing value in time series is interesting and novel.	strength
2020-2046	The paper does a good job at motivating the methods and describing the proposed solution.	strength
2020-2046	The authors did a good job at addressing the concerns of the reviewers. <sep>	rebuttal_process
2020-2046	During the discussion, some reviewers are still concerned about the empirical results, which do not match well with published results (even though the authors provided an explanation for it).	rebuttal_process
2020-2046	In addition, the proposed method is only tested on the health care datasets, but the improvement is limited.	rebuttal_process
2020-2046	Therefore it would be worthwhile investigating other time series datasets, and most important answering the important question in terms of what datasets/applications the proposed method works well. <sep>	suggestion
2020-2046	The paper is one step away for being a strong publication.	decision
2020-2046	We hope the reviews can help improve the paper for a strong publication in the future.	misc

2020-2055	The author response and revisions to the manuscript motivated two reviewers to increase their scores to weak accept.	rating_summary
2020-2055	While these revisions increased the quality of the work, the overall assessment is just shy of the threshold for inclusion.	decision

2020-2061	The present paper addresses the problem of imitation learning in multi-modal settings, combining vision, language and motion.	abstract
2020-2061	The proposed approach learns an abstract task representation, and the goal is to use this as a basis for generalization.	abstract
2020-2061	This paper was subject to considerable discussion, and the authors clarified several issues that reviewers raised during the rebuttal phase.	rebuttal_process
2020-2061	Overall, the empirical study presented in the paper remains limited, for example in terms of ablations (which components of the proposed model have what effect on performance) and placement in the context of prior work.	rebuttal_process
2020-2061	As a result, the depth of insights is not yet sufficient for publication.	decision

2020-2064	The paper is proposed a rejection based on majority reviews.	decision

2020-2066	The submission proposes a method for learning a graph structure and node embeddings through an iterative process.	abstract
2020-2066	Smoothness and sparsity are both optimized in this approach.	abstract
2020-2066	The iterative method has a stopping mechanism based on distance from a ground truth. <sep>	abstract
2020-2066	The concerns of the reviewers were about scalability and novelty.	weakness
2020-2066	Since other methods have used the same costs for optimization, as well as other aspects of this approach, there is little contribution other than the iterative process.	weakness
2020-2066	The improvement over LDS, the most similar approach, is relatively minor. <sep>	weakness
2020-2066	Although the paper is promising, more work is required to establish the contributions of the method.	weakness
2020-2066	Recommendation is for rejection.	decision

2020-2074	The paper studies the problem of modeling inter-object dynamics with occlusions.	abstract
2020-2074	It provides proof-of-concept demonstrations on toy 3d scenes that occlusions can be handled by structured representations using object-level segmentation masks and depth information.	abstract
2020-2074	However, the technical novelty is not high and the requirement of such structured information seems impractical real-world applications which thus limits the significance of the proposed method.	weakness

2020-2080	This paper provides empirical evidence on synthetic examples with a focus on understanding the relationship between the number of "good" local minima and number of irrelevant features.	abstract
2020-2080	The reviewers find the problem discussed to be important.	strength
2020-2080	One of the reviewers has pointed out that the paper does not present deep insights and is more suitable for workshops.	rating_summary
2020-2080	The authors did not provide a rebuttal, and it appears that the reviewers opinion has not changed. <sep>	rebuttal_process
2020-2080	The current score is clearly not sufficient to accept this paper in its current form.	rating_summary
2020-2080	Due to this reason, I recommend to reject this paper.	decision

2020-2083	This paper presents a transfer learning framework in neural topic modeling.	abstract
2020-2083	Authors claim and reviewers agree that this view of transfer learning in the realm of topic modeling is novel. <sep>	strength
2020-2083	However, after much deliberation and discussion among the reviewers, we conclude that this paper does not contribute sufficient novelty in terms of the method.	weakness
2020-2083	Also, reviewers find the experiments and results not sufficiently convincing. <sep>	weakness
2020-2083	I sincerely thank the authors for submitting to *CONF* and hope to see a revised paper in a future venue.	misc

2020-2085	A new setting for lifelong learning is analyzed and a new method, AOP, is introduced, which combines a model-free with a model-based  approach to deal with this setting. <sep>	abstract
2020-2085	While the idea is interesting, the main claims are insufficiently demonstrated.	weakness
2020-2085	A theoretical justification is missing, and the experiments alone are not rigorous enough to draw strong conclusions.	weakness
2020-2085	The three environments are rather simplistic and there are concerns about the statistical significance, for at least some of the experiments.	weakness

2020-2088	The paper presents a method for continual learning with a variant of VAE.	abstract
2020-2088	The proposed approach is reasonable but technical contribution is quite incremental.	weakness
2020-2088	The experimental results are limited to comparisons among methods with generative replay, and experimental results on more complex datasets (eg, CIFAR 100, CUB, ImageNet) are missing.	weakness
2020-2088	Overall, the contribution of the work in the current form seems insufficient for acceptance at *CONF*.	decision

2020-2106	An actor-critic method is introduced that explicitly aims to learn a good representation using a stochastic latent variable model.	abstract
2020-2106	There is disagreement among the reviewers regarding the significance of this paper.	misc
2020-2106	Two of the three reviewers argue that several strong claims made in the paper that are not properly backed up by evidence.	weakness
2020-2106	In particular, it is not sufficiently clear to what degree the shown performance improvement is due to the stochastic nature of the model used, one of the key points of the paper.	weakness
2020-2106	I recommend that the authors provide more empirical evidence to back up their claims and then resubmit.	decision

2020-2119	The paper proposes an approach for finding an explainable subset of features by choosing features that simultaneously are: most important for the prediction task, and robust against adversarial perturbation.	abstract
2020-2119	The paper provides quantitative and qualitative evidence that the proposed method works. <sep>	abstract
2020-2119	The paper had two reviews (both borderline), and the while the authors responded enthusiastically, the reviewers did not further engage during the discussion period. <sep>	rebuttal_process
2020-2119	The paper has a promising idea, but the presentation and execution in its current form have been found to be not convincing by the reviewers.	weakness
2020-2119	Unfortunately, the submission as it stands is not yet suitable for *CONF*.	decision

2020-2141	The paper shows an automatic piano fingering algorithm.	abstract
2020-2141	The idea is good.	strength
2020-2141	But the reviewers find that the novelty is limited and it is an incremental work.	weakness
2020-2141	All the reivewers agree to reject.	rating_summary

2020-2154	This paper proposes a method for generating text examples that are adversarial against a known text model, based on modifying the internal representations of a tree-structured autoencoder. <sep>	abstract
2020-2154	I side with the two more confident reviewers, and argue that this paper doesn't offer sufficient evidence that this method is useful in the proposed setting.	weakness
2020-2154	I'm particularly swayed by R1, who raises some fairly basic concerns about the value of adversarial example work of this kind, where the generated examples look unnatural in most cases, and where label preservation is not guaranteed.	weakness
2020-2154	I'm also concerned by the fact, which came up repeatedly in the reviews, that the authors claimed that using a tree-structured decoder encourages the model to generate grammatical sentences—I see no reason why this should be the case in the setting described here, and the paper doesn't seem to offer evidence to back this up.	weakness

2020-2167	This submission proposes a new gating mechanism to improve gradient information propagation during back-propagation when training recurrent neural networks. <sep>	abstract
2020-2167	Strengths: <sep> -The problem is interesting and important. <sep>	strength
2020-2167	-The proposed method is novel. <sep>	strength
2020-2167	Weaknesses: <sep> -The justification and motivation of the UGI mechanism was not clear and/or convincing. <sep>	weakness
2020-2167	-The experimental validation is sometimes hard to interpret and the proposed improvements of the gating mechanism are not well-reflected in the quantitative results. <sep>	weakness
2020-2167	-The submission was hard to read and some images were initially illegible. <sep>	weakness
2020-2167	The authors improved several of the weaknesses but not to the desired level. <sep>	rebuttal_process
2020-2167	AC agrees with the majority recommendation to reject.	decision

2020-2168	This paper proposes a solution to the decentralized privacy preserving domain adaptation problem.	abstract
2020-2168	In other words, how to adapt to a target domain without explicit data access to other existing domains.	abstract
2020-2168	In this scenario the authors propose MDDA which consists of both a collaborator selection algorithm based on minimal Wasserstein distance as well as a technique for adapting through sharing discriminator gradients across domains. <sep>	abstract
2020-2168	The reviewers has split scores for this work with two recommending weak accept and two recommending weak reject.	rating_summary
2020-2168	However, both reviewers who recommended weak accept explicitly mentioned that their recommendation was borderline (an option not available for *CONF* 2020).	rating_summary
2020-2168	The main issues raised by the reviewers was lack of algorithmic novelty and lack of comparison to prior privacy preserving work.	rebuttal_process
2020-2168	The authors agreed that their goal was not to introduce a new domain adaptation algorithm, but rather to propose a generic solution to extend existing algorithms to the case of privacy preserving and decentralized DA.	rebuttal_process
2020-2168	The authors also provided extensive revisions in response to the reviewers comments.	rebuttal_process
2020-2168	Though the reviewers were convinced on some points (like privacy preserving arguments), there still remained key outstanding issues that were significant enough to cause the reviewers not to update their recommendations. <sep>	rebuttal_process
2020-2168	Therefore, this paper is not recommended for acceptance in its current form.	decision
2020-2168	We encourage the authors to build off the revisions completed during the rebuttal phase and any outstanding comments from the reviewers.	suggestion

2020-2175	This paper proposes using non-Euclidean spaces for GCNs, leveraging the gyrovector space formalism.	abstract
2020-2175	The model allows products of constant curvature, both positive and negative, generalizing hyperbolic embeddings. <sep>	abstract
2020-2175	Reviewers got mixed impressions on this paper.	misc
2020-2175	Whereas some found its methodology compelling and its empirical evaluation satisfactory, it was generally perceived that this paper will greatly benefit from another round of reviewing.	strength
2020-2175	In particular, the authors should improve readability of the main text and provide a more thorough discussion on related recent (and concurrent) work.	suggestion

2020-2178	This paper presents a model for building sentence embeddings using a generative transformer model that encoders separately semantic aspects (that are common across languages)  and language-specific aspects.	abstract
2020-2178	The authors evaluate their embeddings in a non-parametric way (ie, on STS tasks by measuring cosine similarity) and find their method to outperform other sentence embeddings methods.	abstract
2020-2178	The main concern that both reviewers (and myself) have about this work relates to its evaluation part.	weakness
2020-2178	While the authors present a set of very interesting difficult evaluation and probing splits aiming at quantifying the linguistic behaviour of their model, it is unsatisfying the fact that the authors do not evaluate their model extensively in standard classification embedding benchmarks (eg, as in GLUE).	weakness
2020-2178	The authors comment: "[their model in producing embeddings] it isn't as strong when using classification for final predictions.	rebuttal_process
2020-2178	This indicates that the embeddings learned by our approach may be most useful when no downstream training is possible".	rebuttal_process
2020-2178	If this is true, why is it the case and isn't it quite restrictive?	rebuttal_process
2020-2178	I think this work is interesting with a nice analysis but the current empirical results are borderline  (yes, the model is better on STS, but this is quite limited of an idea compared to using these embeddings as features in a classification tasks).	weakness
2020-2178	As such, I do not recommend this paper for acceptance but I do hope that authors will keep improving their method and will make it work in more general problems involving classification tasks.	decision

2020-2180	The paper proposes an interesting idea of identifying repeated action sequences, or behavioral motifs, in the context of hierarchical reinforcement learning, using sparsity/compression.	abstract
2020-2180	While this is a fresh and useful idea, it appears that the paper requires more work, both in terms of presentation/clarity and in terms of stronger empirical results.	weakness

2020-2192	This paper proposes a framework for privacy-preserving training of neural networks within a Trusted Execution Environment (TEE) such as Intel SGX.	abstract
2020-2192	The reviewers found that this is a valuable research directions, but found that there were significant flaws in the experimental setup that need to be addressed.	weakness
2020-2192	In particular, the paper does not run all the experiments in the same setup, which leads to the use of scaling factor in some cases.	weakness
2020-2192	The reviewers found that this made it difficult to make sense of the results.	weakness
2020-2192	The writing of this paper should be streamlined, along with the experiments before resubmission.	decision

2020-2194	The authors present a multiple instance learning-based approach that uses weak supervison (of which skills appear in any given trajectory)  to automatically segment a set of skills from demonstrations.	abstract
2020-2194	The reviewers had significant concerns about the significance and performance of the method, as well as the metrics used for analysis.	weakness
2020-2194	Most notably, neither the original paper nor the rebuttal provided a sufficient justification or fix for the lack of analysis beyond accuracy scores (as opposed to confusion matrices, precision/recall, etc), which leaves the contribution and claims of the paper unclear.	rebuttal_process
2020-2194	Thus, I recommend rejection at this time.	decision

2020-2198	The authors present a self-supervised framework for learning a hierarchical policy in reinforcement learning tasks that combines a high-level planner over learned latent goals with a shared low-level goal-completing control policy.	abstract
2020-2198	The reviewers had significant concerns about both problem positioning (wrt existing work) and writing clarity, as well as the fact that all comparative experiments were ablations, rather than comparisons to prior work.	weakness
2020-2198	While the reviewers agreed that the authors reasonably resolved issues of clarity, there was not agreement that concerns about positioning wrt prior work and experimental comparisons were sufficiently resolved.	rebuttal_process
2020-2198	Thus, I recommend to reject this paper at this time.	decision

2020-2209	This paper focuses on hate speech detection and compares several classification methods including Naive Bayes, SVM, KNN, CNN, and many others.	abstract
2020-2209	The most valuable contribution of this work is a dataset of ~400,000 tweets from 2017 Kenyan general election, although it is unclear whether the authors plan to release the dataset in the future. <sep>	weakness
2020-2209	The paper is difficult to follow, uses an incorrect *CONF* format, and is full of typos. <sep>	weakness
2020-2209	All three reviewers agree that while this paper deals with an important topic in social media analysis, it is not ready for publication in its current state.	rating_summary
2020-2209	The authors did not provide a rebuttal to reviewers' concerns. <sep>	rebuttal_process
2020-2209	I recommend rejecting this paper for *CONF*.	decision

2021-7	This paper proposes a technique of decomposing the nonsymmetric kernel of determinantal point processes, which enables inference and learning in time and space linear with respect to the size of the ground set.	abstract
2021-7	This substantially improves upon existing work.	strength
2021-7	The proposed method is well supported both with theory and experiments.	strength
2021-7	All of the reviewers find that the contributions are significant, and no major flaws are identified through reviews and discussion.	strength
2021-7	The determinantal point process might not be one of the most popular topics in the *CONF* community today but certainly is relevant.	strength

2021-11	The paper presents a variance reduction technique to the Straight-Through version of the Gumbel-Softmax estimator.	abstract
2021-11	The technique is relying on the truncated Gumbel of Maddison et al I share the excitement of the reviewers about this work and I expect this technique to further influence the field.	misc

2021-19	This paper investigates a speech synthesis approach that directly generates raw audios from text or phoneme inputs in an end-to-end fashion.	abstract
2021-19	The approach first maps the input texts/phonemes into a representation sequence that is aligned with the output at a lower sampling frequency by a differentiable aligner and then upsamples the representation sequence to the full audio frequency by a decoder.	abstract
2021-19	A number of techniques including adversarial training and soft DTW are applied to improve the training.	abstract
2021-19	The experimental results are good.	strength
2021-19	There are raised concerns from the reviewers which are mostly cleared by the rebuttal of the authors.	rebuttal_process
2021-19	After the rebuttal and discussion, all reviewers are supportive on accepting the paper.	rating_summary

2021-22	The paper studies the problem of being able to control text generated by pre-trained language models. <sep>	abstract
2021-22	The problem is timely and important.	abstract
2021-22	The paper   frames the problem as constraint satisfaction over a probability distribution.	abstract
2021-22	Both pointwise and distributional constraints can be imposed.	abstract
2021-22	The proposed algorithm,  Generation with Distributional Control (GDC), is elegant, and is an interesting new addition to this line of work.	strength
2021-22	Overall, the paper brings forth news ideas, and could have impact.	strength

2021-28	The paper designs a new way (in some sense a new perspective) on how neural networks can be used to model intervention variables when the goal is to estimate ADRF.	abstract
2021-28	Basically, the idea is to emphasize the importance of the intervention variable by ensuring that it appears not just in every layer but also in every neural of a neural network. <sep>	abstract
2021-28	Reviewers mostly agree that this is a good paper with varying degrees, although there are some criticisms on eg, assuming away the confounders.	weakness
2021-28	However, I believe the authors address the criticisms of R4 satisfactorily. <sep>	rebuttal_process
2021-28	Overall I find the idea new and interesting and the experimental results strong, hence I happily recommend accepting the paper. <sep>	decision
2021-28	I do have a few quips myself and some comments that may help the authors to further improve the paper.<sep>	suggestion
2021-28	Re: the design that models each parameter as a spline. <sep>	suggestion
2021-28	This is equivalent to introducing additional parameters (coefficients for spline basis) and adding a fixed linear layer (spline basis themselves) to every layer of the neural networks.	suggestion
2021-28	t is taken as an input in all layers thus it makes sure that the model prioritizes on learning the impact of t.<sep>	suggestion
2021-28	If you use a B-spline basis (that comes with kernels of bounded support), then the proposed method is very similar to stratifying the data according to different bins of t, and then fitting a separate model for each t. The only difference is that the different bins are now smooth kernels and they overlap somewhat.	suggestion
2021-28	As a side note,  the authors should clearly write out how they are choosing the knots to specify the basis functions.	suggestion
2021-28	Otherwise the paper will not be reproducible.<sep>	suggestion
2021-28	I am not sure how this method would compare to naive (non-deep) baselines.	suggestion
2021-28	Maybe this was considered in a prior work?	suggestion
2021-28	If not, then I tend to side with Reviewer 4 that the evaluations are mostly ablation studies and they are not really comparing to representative work in this domain.	suggestion
2021-28	Given that there is a large body of work on this before deep learning takes over, it is important to somehow compare with the right baselines.	suggestion

2021-34	The paper is studying a new intrinsic motivation RL setup in a dynamic environment, where the authors minimize the state entropy instead of the common approach of maximizing it.	abstract
2021-34	The resulting idea is simple but also surprising that it works so well.	strength
2021-34	All reviewers appreciated the new problem formulation of using dynamic environments and found the idea very promsing.	strength
2021-34	In addition, they identified the following strengths of the paper: <sep> The experiments are exhaustive, identifying many domains where the approach can be applied <sep> 	strength
2021-34	The presented results are compelling <sep>	strength
2021-34	The paper is well written <sep>	strength
2021-34	The paper introduces a new problem setup that has not been studied before<sep>	strength
2021-34	I agree with the reviewers that this paper contains many interesting contributions and therefore recommend acceptance.	decision

2021-38	This paper introduces a novel game-theoretic view on PCA which yields an algorithm (EigenGame; Algorithm 2) that allows evaluation of singular vectors in a decentralized manner.	abstract
2021-38	The proposed algorithm is significant in its scalability, as demonstrated in the experiment on a large-scale dataset (ResNet-200 activations).	abstract
2021-38	This paper is generally clearly written, and in particular Section 2 provides an easy-to-follow reasoning leading to the proposed game-theoretic reformulation of PCA.	strength
2021-38	I felt that the later sections are a bit condensed, including the figures.	weakness
2021-38	In the authors response major concerns raised by the reviewers have been appropriately addressed.	rebuttal_process
2021-38	I would thus recommend acceptance of this paper. <sep>	decision
2021-38	What I found particularly interesting in their game-theoretic reformulation is that in the utility functions shown in (6) the orthogonality constraints u^j⊤u^i=0 have been removed and replaced with the soft constraints represented as the regularizer terms encouraging the orthogonality.	strength
2021-38	Although several alternative forms for the regularizers would be possible, it is this particular form that allows an efficient gradient-ascent algorithm which does not require explicit orthonormalization or matrix inversion is straightforwardly parallelizable. <sep>	strength
2021-38	Pros: <sep> Provides a novel game-theoretic reformulation of PCA. <sep> 	strength
2021-38	Proposes a sequential algorithm and a decentralized algorithm for PCA on the basis of the game-theoretic reformulation. <sep>	strength
2021-38	Provides theoretical guarantee for the global convergence of the sequential algorithm. <sep>	strength
2021-38	Demonstrates that the proposed decentralized algorithm is scalable to large-scale problems.<sep>	strength
2021-38	Cons: <sep> The latter statement of Theorem 4.1 requires conditions on the initialization, which are hard to satisfy in high-dimensional settings. <sep> 	weakness
2021-38	Significance of the proposed game-theoretic formulation in the context of game theory does not seem to be well explored.	weakness

2021-47	This paper tackles the important problem of endowing deep RL agents with added interpretability.	abstract
2021-47	Action values are decomposed as the combination of GVFs learned on externally-specified features, offering action explanations in terms of discounted future returns in the space of interpretable quantities.	abstract
2021-47	Reviewers praised the approach, as well as the level of detail for reproducibility purposes.	strength
2021-47	R3 had concerns about the generality of the method but follow-up experiments have allayed these concerns.	rebuttal_process
2021-47	Given the reviewer response and the central importance of the problem considered to the field, I can wholeheartedly recommend acceptance.	decision

2021-79	This paper provides theoretical justifications on why the data augmentation technique, Mixup (convex combinations of pairs of data examples) , can help in improving robustness and generalization of GLMs and ReLUs.	abstract
2021-79	The authors rewrote a Mixup loss function as the summation of a standard empirical loss and some regularization terms regularizing gradient, Hessian and some higher order terms.	abstract
2021-79	Using the quadratic approximation of the Mixup loss (ignoring the higher order terms), the authors proved that the quadratic approximation of the Mixup loss was equivalent to an upper bound of the second order Taylor expansion of an adversarial loss, providing justifications for why Mixup loss training could improve robustness against small attacks.	abstract
2021-79	Using the same quadratic approximation of the Mixup loss, the regularization term controlled the hypothesis class to have a smaller Rademacher complexity. <sep>	abstract
2021-79	Overall, the paper provides insightful theoretical interpretations for a commonly used data augmentation technique in DL.	strength
2021-79	The paper also supports its claims by numerical experiments.	abstract
2021-79	Although there is some minor concerns on using the quadratic approximation of the Mixup loss, as well as R3 term's regularization effect on a broader family of models, the paper provides unique and novel insights on Mixup.	strength
2021-79	; all reviewers acknowledge the authors applying the existing proof techniques to analyze Mixup's effect on robustness and generalization. <sep>	strength
2021-79	Therefore, I recommend accepting this paper.	decision

2021-85	This paper describes a new and experimentally useful way to propose masked spans for MLM pretraining, by masking spans of text that co-occur more often than would be expected given their components - ie that are statistically likely to be non-compositional phrases. <sep>	abstract
2021-85	The authors should make some attempt to connect their PMI heuristic with prior methods for statistical phrase-finding and term recognition, eg https://www.aaai.org/Papers/IJCAI/2007/IJCAI07-439. pdf or https://link.springer.com/chapter/10.1007/978-3-540-85287-2_24 in the final paper.	suggestion

2021-120	The paper introduces a learning framework for solving incompressible Navier-Stokes fluid using a physics informed loss formulation.	abstract
2021-120	The PDE is solved on a grid, and the model, implemented via convolutions and a U-Net, is trained to minimize the NS residual.	abstract
2021-120	The model is trained on a variety of randomized contexts, in a way that allows training to explore a large number of configurations.	abstract
2021-120	The paper presents original contributions compared to previous Physics informed framework (discrete formulation, conditioning on the domain conditions, …).	abstract
2021-120	All the reviewers agree that the detailed rebuttal provides answers to their questions and that the contribution is significant, they all have a positive assessment of the paper.	rebuttal_process

2021-138	The paper describes a new data augmentation approach for image based RL.	abstract
2021-138	The approach is both simple and effective.	abstract
2021-138	It improves significantly the performance of several algorithms across a number of tasks.	abstract
2021-138	The reviewers were unanimous about the benefits of the proposed technique.	strength
2021-138	This represents an important advance for RL.	strength

2021-146	This paper presents a density ratio estimation approach to make the early decision for sequential data.	abstract
2021-146	The main contribution of this paper is the mathematical soundness of the proposed algorithm and all reviewers are unanimously positive about this paper with pretty good scores (7, 8, 6, 9, 7).	rating_summary
2021-146	However, despite the good scores, the verbal comments by the reviewers are not very strong except for one reviewer (R2); the reviewer with the highest score (9) did not provide detailed information about his/her rating.	misc
2021-146	Also, the evaluation of this work is relatively weak because synthetic or simple datasets were employed for the experiment and the baseline methods are too straightforward.	weakness
2021-146	Also, it is not clear how the proposed algorithm can handle the data with sparse observations (data with idle times in the middle).	weakness
2021-146	Moreover, it does not provide rigorous stopping criteria although the authors proposed a simple method to determine the threshold, which is contradictory to the main objective of the proposed algorithm---making early predictions on sequential data---because the method requires "plotting the speed-accuracy tradeoff curve on the test dataset."	weakness
2021-146	This response implies that it at least requires a withheld dataset although this issue can be regarded as a separate problem, the paper could have provided an ablation study with respect to the criteria. <sep>	weakness
2021-146	Considering all these facts--high scores but relatively low supports and confidences, and practical limitations, I would recommend accepting this paper as a spotlight presentation.	decision

2021-149	This paper addresses a central problem in inference in implicit models-- classical approaches on such problems ('ABC') rely on computation of summary statistics, and multiple methods for automatically finding summary statistics have been proposed.	abstract
2021-149	This paper provides a fresh take on this classical problem, by providing a methods for finding information-maximising summary stats.	abstract
2021-149	The work is original, likely impactful, and carried out rigorously and carefully.	strength
2021-149	The reviewers flagged some issues with empirical comparisons, as well as discussion or relevant work-- those issues mainly seem to have been resolved in the review process.	rebuttal_process
2021-149	Moreover, given the originality of the approach, and provided that the description of empirical comparisons and relationship with other work are carefully and conservatively worded, I believe this will be worth publishing even if it is not always the 'best' method on all problems.	decision

2021-150	The paper discusses a new threat model for multi-exit DNNs: attacks against efficiency of inference.	abstract
2021-150	The proposed attack increases the inference time of such networks by the factor of 1.5-5, while at the same reducing the accuracy of attacked networks.	abstract
2021-150	Unlike classical adversarial examples, the new type of attack cannot be thwarted by adversarial training. <sep>	abstract
2021-150	Overall, the paper exhibits a novel contribution, is well written and methodically sound.	strength
2021-150	Its practical motivation is somewhat weak, as it is currently unclear for which applications such attacks may be feasible.	weakness
2021-150	However, the novelty of the threat model addressed by this paper makes it an interesting methodical contribution.	strength

2021-155	The paper presents a sound and efficient (but not complete) algorithm for verifying that a piecewise-linear neural network is constant in an Lp ball around a given point.	abstract
2021-155	This is a significant contribution towards practical protection from adversarial attacks with theoretical guarantees.	abstract
2021-155	The proposed algorithm is shown to be sound (that is, when it returns a result, that result is guaranteed to be correct) and efficient (it is easily parallelizable and can scale to large networks), but is not complete (there exist cases where the algorithm will return "I don't know").	abstract
2021-155	The experiments show good results in practice.	abstract
2021-155	The reviewers are positive about the paper, and most initial concerns have been addressed in the rebuttal, with the paper improving as a result.	rebuttal_process
2021-155	Overall, this is an important contribution worth communicating to the *CONF* community, so I'm happy to recommend acceptance.	decision

2021-176	This paper proposes a new idea for performing knowledge distillation by leveraging teacher's classifier to train student's penultimate layer feature via proposing suitable loss functions.	abstract
2021-176	Reviewers appreciate the simultaneous simplicity and effectiveness of the method.	strength
2021-176	A comprehensive set of studies are performed to empirically show the effectiveness of the method.	strength
2021-176	Specifically, the proposed distillation method is shown to outperform state-of-the-art across various network architectures, teacher-student capacities, datasets, and domains.	strength
2021-176	The paper is well-written and is easy to follow.	strength
2021-176	All reviewers rate the paper on the accept side (after the rebuttal) and believe the new perspective this work provides on distillation and its simplicity to implement can lead it to gain high impact.	rating_summary
2021-176	I concur with the reviewers and find this submission a convincing empirical work, and thus recommend for accept.	decision

2021-180	The paper considers the problem of learning a new task with few examples by using related tasks which can exploit shared representations for which more data is available.	abstract
2021-180	The paper proves a number of interesting (primarily theoretical) results.	strength

2021-191	The authors provide four rigorous upper bounds on the operator norm of the linear transformation associated with a 2D convolutional layer of a neural network.	abstract
2021-191	One of these is a heuristic proposed in earlier work by Miyato et al, and widely used, so, among other things, their result provides theoretical context for that method which will be of broad interest.	abstract
2021-191	All four of their bounds can be efficiently computed and have easily computed gradients, so they propose using the minimum of the four bounds for various purposes.	abstract
2021-191	Since, for standard architectures, the Lipschitz constant of a network can be bounded above by the product of the operator norms of its layers, there are a variety of applications of differentiable bounds on these operator norms.	abstract
2021-191	They show that their new bound is sometimes much tighter than the bound of Miyato et al, and can be computed much more efficiently than two known methods for exact computation.	abstract
2021-191	The paper is written well, which will facilitate future work building on this work.	strength
2021-191	The analysis builds on earlier work, but insight was required to obtain the new results;  the fundamental novelty of the mathematical development was confirmed by an expert reviewer. <sep>	strength
2021-191	While they experimentally compared the accuracy of their approximations to those of the method of Miyato, et al, the case for the practical utility of their method would have been stronger if they had shown that their regularizer led to better results for some tasks.	weakness
2021-191	However, I believe that the paper should be accepted purely on the basis of its theoretical contribution, which enhances our understanding of this important topic, and, even if it cannot be directly applied, seems like to inspire practically useful methods in the future.	decision

2021-193	This paper proposes an interesting unified framework for meta-learning with commentaries, which contains information helpful for learning about new tasks or new data points.	abstract
2021-193	The authors present three kinds of different instantiations, ie, example weighting, example blending, and attention mask, and show the effectiveness with the extensive experiments.	abstract
2021-193	The proposed method has a potential to be used for a wide variety of tasks.	strength

2021-194	This work explores the distillation of language models using MixUp for data augmentation.	abstract
2021-194	Distillation with MixUp seems to be novel in the narrow context of distilling language models, although it has been used before in different contexts as the reviewers point out.	strength
2021-194	The results of the experimental validation are encouraging, and the application is valuable and of wide interest to the *CONF* audience.	strength
2021-194	I therefore recommend accepting this paper for a poster presentation.	decision

2021-212	The paper identifies the phenomenon of oversquashing in GNNs and relate it to bottleneck.	abstract
2021-212	While this phenomenon has been previously observed, the analysis is new and insightful.	abstract
2021-212	The authors conclude that standard message passing may be inefficient in cases where the graphs exhibit an exponentially growing number of neighbors and long-range dependencies, and propose a solution in the form of a fully-adjacent layer.	abstract
2021-212	While the paper does not offer much methodologically, it is the observation of bottleneck that is of importance. <sep>	strength
2021-212	We therefore believe that the criticism raised by some reviewers of the observation not being novel and the solution "too simple" rather unsubstantiated.	ac_disagreement
2021-212	The authors have well addressed these issues in their rebuttal.	rebuttal_process
2021-212	The AC recommends accepting the paper.	decision

2021-237	The paper offers a new take on generalization, motivated by the empirical success of self-supervised learning.	abstract
2021-237	Two reviewers found the contribution novel and interesting, and recommend acceptance (with one reviewer championing for it).	rating_summary
2021-237	Two reviewers remain skeptical about the value of the paper, and the authors are encouraged to add a discussion about the points made in these reviews. <sep>	weakness
2021-237	I agree with the positive reviewers and would like to recommend acceptance.	decision

2021-277	All the reviewers and I agree that the proposed approach is interesting and the paper is overall well written.	strength
2021-277	However, I agree with R3 that the paper  need further re-working the theoretical part (see the post-rebuttal comments of R4).	weakness
2021-277	Thus, I would encourage the authors to carefully address the comments of the reviewers in the revised version of the paper, which would ultimately improve the quality of the paper.	suggestion

2021-284	This paper studies the reasons for failure of trained neural network models on out of distribution tasks.	abstract
2021-284	While the reviewers liked the theoretical aspects of the paper, one important concern is about the applicability of these insights to real datasets.	weakness
2021-284	The authors added an appendix to the paper showing results on a real dataset that mitigates this concern to an extent.	rebuttal_process
2021-284	Further, there are interesting insights in the paper to merit acceptance.	decision

2021-293	In order to learn good exploratory behaviors in settings where agents encounter diverse environments, the authors propose an approach which involves learning from episodes that exhibit good episode-level exploratory behaviors.	abstract
2021-293	The innovation is in the scoring and learning from these episode-level behaviors rather than trying to come up with shorter timescale proxies of exploration.	abstract
2021-293	In making this concrete, the authors propose to score trajectories based effectively on state coverage within an episode (ie good exploration corresponds to good state coverage) as well as by scoring episodes relative to one another and giving preference to episodes that explore less often encountered states.	abstract
2021-293	To learn, the core algorithm interleaves standard RL updates with behavioral cloning updates using the best episodes of data, thereby training the policy to both solve the task and explore well at the episode level. <sep>	abstract
2021-293	A weakness is that the paper uses low-level state in grid worlds and there is some ambiguity in applying this to settings with continuous states.	weakness
2021-293	The authors discuss general strategies for dealing with these limitations as potential future work. <sep>	rebuttal_process
2021-293	The reviewers were positive about the clarity of the text and felt the core idea that was proposed was simple and effective.	strength
2021-293	The authors put in solid effort to address reviewer concerns.	rebuttal_process
2021-293	The most salient remaining concern, which I share, is that there will be challenges in scaling this approach to more complex environments with continuous state/observation spaces. <sep>	rebuttal_process
2021-293	Overall, this paper had a consensus "accept" rating (7,7,7,6), and I endorse this as my decision.	decision

2021-295	The paper explores how to effectively conduct negative sampling in learning for text retrieval.	abstract
2021-295	The paper shows that negative examples sampled locally are not informative, and proposes ANCE, a new learning mechanism that samples hard negative examples globally, using an asynchronously updated ANN index. <sep>	abstract
2021-295	Pros • The problem studied is important.	strength
2021-295	• Paper is generally clearly written.	strength
2021-295	• Solid experimental results.	strength
2021-295	• There is theoretical analysis. <sep>	strength
2021-295	Cons • The idea might not be so new.	weakness
2021-295	The contribution is mainly from its empirical part. <sep>	weakness
2021-295	During rebuttal, the authors have addressed the clarity issues pointed out by the reviewers.	rebuttal_process
2021-295	They have also added additional experimental results.	rebuttal_process

2021-312	The paper introduces LEAD, a decentralized optimizer with communication compression that can achieve linear convergence rate in the strongly convex setting.	abstract
2021-312	In terms of novelty, the authors should still add a discussion of Magnusson et al, 2019, On Maintaining Linear Convergence of Distributed Learning and Optimization under Limited Communication,  which is a related linear convergence result in the deterministic (full gradient) case, and relates to the analysis here which is stochastic but also exploits the deterministic case.	suggestion
2021-312	Nevertheless, reviewers reached consensus-with communication compression in the given time-that the paper in its current form is well written and the results are presented clearly in both experiments and theory (which builds up on the earlier NIDS algorithm).	strength
2021-312	The presentation of the algorithm can be slightly improved.	weakness
2021-312	We hope the authors will incorporate the remaining smaller open points such as mentioned by R1, such as making the constants in the convergence bounds explicit when comparing with other methods.	suggestion

2021-317	This paper proposes an input-dependent dropout strategy, using variational inference to infer the rates.	abstract
2021-317	While the idea is a fairly straightforward variant of recent probabilistic dropout methods, the paper demonstrates consistent improvements across several types of NN layers (dense, convolutional, and attention) in large-scale experiments (eg ImageNet).	strength
2021-317	The reviewers unanimously agreed on accepting the paper.	rating_summary

2021-318	This work presents a novel approach to improving text decoding.	abstract
2021-318	This is backed up by a solid analysis of cross-entropy growth with top-k vs top-p and an interesting demonstration of repetition correlating with probability.	strength
2021-318	The paper is well written and well organized.	strength
2021-318	The authors' rebuttal was effective in convincing the reviewers.	rebuttal_process
2021-318	The human evaluation (added during the rebuttal phase) is a good demonstration of the effectiveness of the approach and so this paper's proposed decoding algorithm is likely to be impactful. <sep>	rebuttal_process
2021-318	Pros: <sep> Well written. <sep> 	strength
2021-318	Solid theoretical analysis of cross-entropy and its relation to top-p and top-k decoding.	strength
2021-318	Good demonstration of how repetition is related to probability. <sep>	strength
2021-318	Interesting, novel and effective decoding algorithm. <sep>	strength
2021-318	Human evaluation of the algorithm's output.<sep>	strength
2021-318	Cons: <sep> The approach has not been tested with a variety of language models. <sep> 	weakness
2021-318	Decoding quality still depends on a target perplexity which may need to be tuned. <sep>	weakness
2021-318	Unnecessary dependence on Zipf's law in the basic decoding algorithm.	weakness

2021-326	This paper studies the robustness of CapsNets under adversarial attacks.	abstract
2021-326	It is found that the votes from primary capsules in CapsNets are manipulated by adversarial examples and that the computationally expensive routing mechanism in CapsNets incurs high computational cost.	abstract
2021-326	As such, a new adversarial attack is specially designed by attacking the votes of CapsNets without having to involve the routing mechanism, making the method both effective and efficient. <sep>	abstract
2021-326	Strengths: <sep> This is the first work which proposes an attack specifically designed for CapsNets by exploiting their special properties. <sep> 	strength
2021-326	The proposed vote attack is more effective and efficient than the other attacks originally proposed for CNNs rather than CapsNets. <sep>	strength
2021-326	The paper is generally well written. <sep>	strength
2021-326	The experimental study is quite comprehensive. <sep>	strength
2021-326	The code will be made available to facilitate reproducibility.<sep>	strength
2021-326	Weaknesses: <sep> The study is mostly for only one type of CapsNets.	weakness
2021-326	It is not clear whether the observations in this paper still hold generally for other types of CapsNets even after some additional experiments have been added. <sep>	weakness
2021-326	The presentation of the paper has room for improvement.<sep>	weakness
2021-326	The authors are recommended to proofread the references thoroughly to ensure style consistency such as the consistent use of capitalization, eg "Star-caps" -> "STAR-Caps" <sep>	suggestion
2021-326	"ieee symposium on security and privacy (sp)" -> "IEEE Symposium on Security and Privacy (SP)"<sep>	suggestion
2021-326	Despite its weaknesses especially those pointed out by Reviewer 2, this paper would be of interest to other researchers as it is the first paper that studies adversarial attacks on CapsNets.	strength

2021-347	The paper introduces "Concept Embeddings"  to  Prototypical Network, which are part-based representations and are learnt by a set of independent networks (which can share weights).	abstract
2021-347	The method first computes the concept embeddings of an input, and then takes the summation of the distances between those concept embeddings and their corresponding concept prototypes in each class to estimate the class probability.	abstract
2021-347	The experiments validates the proposed methods on 4 benchmarks in three different domains, including vision, language and biology.	abstract
2021-347	For the biology task, the authors also develop a new benchmark on cross-organ cell type classification.	abstract
2021-347	The key novel idea of transferable concepts results in significantly improved generalization ability over the existing few-shot learning methods. <sep>	abstract
2021-347	Although some reviewers raised concerns about not using other few-shot image classification datasets such as MiniImageNet these are not appropriate benchmarks, as the method requires the "part-based concepts" to reasonably span the space of all images which is a characteristic of fine-grained image classification problem.	ac_disagreement
2021-347	Although this does limit the scope of the method, the fact that it is applicable for multiple tasks is a strong counteragument to the claim that it is too limited, so overall I disagree with the assessment of one reviewer that the choice of benchmarks is insufficient.	ac_disagreement

2021-370	The paper's main message is that some existing NLP techniques that claim to improve performance by the use of a knowledge graph may not achieve this improved performance because of the knowledge graph or at least the explanation given may be questionable.	abstract
2021-370	This is thought provoking and it will incite the community to think more carefully about the real factors of improved performance.	strength
2021-370	The initial version of the paper was not well written, but the authors improved the writing significantly.	rebuttal_process
2021-370	The paper includes a thorough empirical evaluation to support the main message.	strength
2021-370	I have read the paper and I believe that this work will be of interest to a diverse audience.	decision

2021-375	The paper attempts to improve retrieval in open domain question answering systems, which is a very important problem.	abstract
2021-375	In this regards, the authors propose to utilize cross-attention scores from a seq2seq reader models as signal for training retrieval systems.	abstract
2021-375	This approach overcomes typical low amount of labelled data available for retriever model.	abstract
2021-375	The reviewers reached a consensus that the proposed approach are interesting and novel.	strength
2021-375	The proposed approach establish new state-of-the-art performance on three QA datasets, although the improvements over previous methods are marginal.	strength
2021-375	Overall, reviewers agree that the paper will be beneficial to the community and thus I recommend an acceptance to *CONF*.	decision

2021-400	There was a predominantly positive feedback from the reviewers so I recommend acceptance of the paper.	decision
2021-400	It is well-written and well-motivated tackling an important problem: That in self-supervised learning one might encode different invariances by default, even if some of these invariances are useful for downstream tasks (eg being rotation invariant may be detrimental to predicting if an image has the correct rotation on a phone).	strength
2021-400	For this, they propose a simple, yet elegant approach and validate it on many downstream tasks.	strength
2021-400	Given the recent interest in self-supervised learning, this appears to be a relevant and interesting paper for the *CONF* community.	decision

2021-415	This paper received moderately good reviews, 3 positives (6, 6, 7) and 1 negative (5).	rating_summary
2021-415	The reviewers are generally positive about the main idea but identified several limitations; performance improvement is marginal compared to existing approaches, the proposed method incurs higher computational complexity, and the presentation is not clear enough.	weakness
2021-415	Some of these issues are addressed in the rebuttal, though.	rebuttal_process
2021-415	Overall, the merits of this work outweigh the drawbacks and I recommend accepting this paper.	decision

2021-423	This paper examines under what conditions influence estimation can be applied to deep networks and finds that, among of items, that influence estimates are poorer for deeper architectures, perhaps due to poor inverse Hessian vector approximations for poor for deeper models.	abstract
2021-423	The authors provide an extensive experimental evaluation across datasets and architectures, and demonstrates the fragility of influence estimates in a number of conditions.	abstract
2021-423	Although the reviewers noted that these issues are now "folk knowledge", there has been less scientific effort in identifying these failures. <sep>	ac_disagreement
2021-423	Of course, more theoretical understanding would help the community better understand where these fragilities lie, but the experimental evaluation is sufficiently strong to be of broad interest to the community.	decision

2021-425	The paper initially received a mixed rating, with two reviewers rate the paper below the bar and two above the bar.	rating_summary
2021-425	The raised concerns include the need for an autoregressive model for upsampling and the effect of batch sizes.	weakness
2021-425	These concerns were well-addressed in the rebuttal.	rebuttal_process
2021-425	Both of the reviewers that originally rated the paper below the bar raise the scores.	rebuttal_process
2021-425	After consulting the paper, the reviews, and the rebuttal, the AC agrees that the paper has its merits and is happy to accept the paper.	decision

2021-439	This paper proposes that we can understand the evolution of representations in deep neural networks during training using the concept of "usable information".	abstract
2021-439	This is effectively an indirect measure of how much information the network maintains about a given categorical variable, Y, and the authors show that it is in fact a variational lower bound on the amount of mutual information that the network's representations have with Y.	abstract
2021-439	The authors show that in deep neural networks the usable information that is maintained for different variables during training depends on the task, such that task irrelevant variables (but not task relevant variables) eventually have their usable information reduced, leading to "minimal sufficient representations". <sep>	abstract
2021-439	The initial reviews were mixed.	misc
2021-439	A common theme in the critiques was the lack of evidence of the generalization and scalability of these results.	weakness
2021-439	The authors addressed these concerns by including new experiments on different architectures and the CIFAR datasets, leading one reviewer to increase their score.	rebuttal_process
2021-439	The final scores stood at 3, 7 ,7, 7.	rating_summary
2021-439	Given the overall positive reviews, interesting subject matter, and relevance to understanding learned representations in deep networks, this paper seems appropriate for acceptance in the AC's opinion.	decision

2021-443	The paper proposes an approach for solving constrained optimization problems using deep learning.	abstract
2021-443	The key idea is to separate equality and inequality constraints and "solve" for the equality constraints separately.	abstract
2021-443	Empirical results are given for convex QPs and for a non-convex problem that arises in AC optimal power flow. <sep>	abstract
2021-443	There was much discussion of this paper between the reviewers and the area chair.	misc
2021-443	THe key question was whether the empirical evaluation is sufficient to convince that the method is more effective than existing solvers.	misc
2021-443	The current experiments do not show that the method achieves better solutions than existing solvers.	weakness
2021-443	For the convex case this is to be expected since solvers are optimal.	misc
2021-443	But in the non-convex case, it would have been nice to see that the method indeed can find better solutions. <sep>	misc
2021-443	This leaves the advantage of the method in its speedup over existing methods.	strength
2021-443	However, as the authors acknowledge, it is possible that this speedup is due to better use of parallelization than the methods they compare to.	weakness
2021-443	It is true that deep learning is particularly easy to parallelize, but this is not impossible for other methods (eg, for linear algebra operations etc). <sep>	weakness
2021-443	Thus, taken together the empirical support for the current method is somewhat limited.	weakness
2021-443	The method itself does make sense, and this was indeed appreciated by the reviewers.	strength

2021-450	The paper is presenting an important empirical finding.	abstract
2021-450	When the learning algorithms are initialized with the same point, the continual and multitask solutions are connected by linear and low-error paths.	abstract
2021-450	Motivated by this finding, the paper proposes a new continual learning algorithm based on path regularization.	abstract
2021-450	The paper received unanimously good scores.	rating_summary
2021-450	I agree with the reviews and recommend acceptance.	decision

2021-451	Reviewers concurred that this is an interesting paper with contributions worthy of publication.	rating_summary
2021-451	The authors also provided many details in the rebuttal which makes the paper even more strong.	rebuttal_process

2021-473	The paper proposes a sensitivity-based pruning method at initialization.	abstract
2021-473	For fully connection and and convolutional neural networks, it shows that the model is trainable only when the initialization satisfies Edge of Chaos (EOC).	abstract
2021-473	The paper also provided a rescaling method so that the pruned network is initialized on the EOC.	abstract
2021-473	For Resnet, the paper shows that the proposed pruning satisfies the EOC condition by default and further provides re-parameterization method to tackle exploding gradients.	abstract
2021-473	The experiments show the performance of the proposed method on fully connected and convolution neural network, as well as ResNet.	abstract
2021-473	There were some concerns about the contribution of the paper compared to that of [1].	weakness
2021-473	I read the two papers carefully and while both papers aim at addressing a similar problem, ie, pruning at initialization while avoiding layer collapse, the paper provides a different perspective on the problem, and provides enough theoretical contribution and insights to be found helpful and interesting by the community.	ac_disagreement

2021-483	This paper proposes a modular RNN architecture called SCOFF.	abstract
2021-483	The work was inspired by cognitive science(object file and schema) and was built upon previous work RIMs.	abstract
2021-483	The method is validated on tasks having multiple objects of the same type. <sep>	abstract
2021-483	Pros: <sep> It addresses an important problem in DNN -- systematic generalization. <sep> 	strength
2021-483	The proposal makes sense and is more flexible than RIM. <sep>	strength
2021-483	Experimental results outperform baselines.<sep>	strength
2021-483	Cons before rebuttal: <sep> The presentation of the algorithm is not very clear due to some confusing notations and missing details of algorithm steps. <sep> 	weakness
2021-483	The comparison with baselines might not be fair due to extra parameters. <sep>	weakness
2021-483	The novelty is limited, because the only difference from RIM is weight sharing.<sep>	weakness
2021-483	The reviewers raised concerns listed in Cons.	misc
2021-483	The authors successfully addressed concerns: they indicated that the comparison was fair with the same input to both; SCOFF is more flexible than RIM, and there is spatial attention to input. <sep>	rebuttal_process
2021-483	The authors added the missing details in the revised version. <sep>	rebuttal_process
2021-483	All reviewers agree that the problem is important and the idea is interesting.	strength
2021-483	Since the authors' rebuttal was very helpful in clarifying the questions raised, I recommend accept.	decision

2021-492	The authors design a framework to estimate the uncertainties in the predictions of gradient boosting models, for both classification and regression.	abstract
2021-492	The framework contains several methods, some that use sub-sampling on data to calculate the estimation, and some that use sub-sampling on the trees within one single gradient boosting model (ie virtual ensemble) to calculate the estimation.	abstract
2021-492	The different methods reveal the trade-off between faster calculation and good uncertainty estimation.	abstract
2021-492	The authors conduct extensive empirical study to demonstrate the validity of the designed framework. <sep>	abstract
2021-492	The reviewers agree that the paper is well-written on a very important topic of machine learning in practice.	strength
2021-492	The authors have done a great job addressing the comments from the reviewers, including the comparison to random forest, and adding more motivating examples.	rebuttal_process
2021-492	The reviewers believe that the work marks a good starting point for addressing this important topic.	strength
2021-492	Nevertheless, the reviewers have some concerns that the results are promising but not impressive yet, and the performance of the virtual ensemble is a bit discouraging.	weakness

2021-513	This paper proposes a new sparsity-inducing activation function, and demonstrates its benefits on continual learning and reinforcement learning tasks. <sep>	abstract
2021-513	After the discussion period, all reviewers agree that this is a solid paper, and so do I. I am thus recommending it for acceptance as a poster.	decision
2021-513	Hopefully, such visibility (combined with the open source release of the code) will encourage other researchers to try this new technique, and we will see more evidence confirming its usefulness in more varied settings and versus stronger baselines (that remain somewhat limited in the current work: this is the main weakness of the paper).	weakness

2021-523	This paper studies how to statistically test if a given model violates the constraint of individual fairness.	abstract
2021-523	This is an interesting and novel problem, and the paper leverages the technique of gradient flow to identify a "witness" pair for individual fairness violation. <sep>	abstract
2021-523	During the rebuttal, the authors have addressed many concerns raised in the reviews.	rebuttal_process
2021-523	The author should also consider discussing the runtime and improving the exposition to resolve some of the presentation issues raised in the reviews.	rebuttal_process

2021-534	Many papers have been written on calibrating neural networks recently.	misc
2021-534	This paper presents a definition of calibration that is more robust than the popular ECE measure while also being more discerning than the Brier score.	abstract
2021-534	Then it proposes a practical spline-based method of post-editing the output softmax scores to make them more calibrated.	abstract
2021-534	The method is shown to be better than existing methods both on their measure and established measure (thanks to reviewer's questions on that.). <sep>	abstract
2021-534	The paper should be of much interest to the community.	decision

2021-550	This paper proposes a graph information bottleneck (GIB) framework for subgraph recognition, including the proposal of a MI objective as well as a bi-level optimization scheme for minimizing said objective.	abstract
2021-550	The paper receive mixed reviews, with two reviewers in favor of acceptance and two reviewers in favor of rejection. <sep>	rating_summary
2021-550	One negative reviewer was too short to judge and had low-confidence.	rating_summary
2021-550	I think most of the concerns arise from lack of understanding of the work and the authors adequately address this on the rebuttal.	rebuttal_process
2021-550	The authors are encouraged to make minor modifications for clarity.	suggestion
2021-550	In particular, classical IB considers random variables x, z, y, and learns latent representation z that is maximally informative about output y and sufficiently informative about input x.	suggestion
2021-550	Therefore, it is natural to expect that the input to GIB is a random graph. <sep>	suggestion
2021-550	The other negative reviewer finds the paper lacks novelty and points to multiple references.	weakness
2021-550	The positive reviewers also ask about the connection with additional references.	weakness
2021-550	Im my opinion, the authors do an excellent job at clarifying the differences with all prior work mentioned by the reviewers, including the closest one, a GIB paper in NerurIPS 2020.	rebuttal_process
2021-550	In my view, the present submission contains sufficient novelty relative to prior work, specifically as it focuses on a different problem (sub-graph) and proposes a different optimization method.	strength
2021-550	That being said, I think it is absolutely essential that the author responses be added to the paper.	suggestion
2021-550	In other words, the final version must add citations to the relevant work mentioned by the reviewers and clarify the differences. <sep>	suggestion
2021-550	All other comments from the two remaining reviewers are very positive: the reviewers find the paper contributes with "quite interesting information theoretic objective functions that actually work on multiple graph learning tasks" and "makes a clear theoretical contribution to the field as well as provides sufficient empirical evaluation."	strength
2021-550	I share the views of the positive reviewers and recommend acceptance, subject to the authors incorporating their responses to the reviewers' comments.	decision

2021-552	This paper has been thoroughly evaluated by four expert reviewers and it had received one public comment.	misc
2021-552	The authors provided extensive explanations and added technical updates to the contents of their submission in response to constructive critiques from the reviewers.	rebuttal_process
2021-552	Even though some minor issues have not been fully resolved in the discussion between the authors and the reviewers, I consider this paper worthy of inclusion in the program of *CONF* 2021 since, albeit marginally, the apparent strengths outweigh its outstanding limitations.	decision

2021-564	The authors have done a very thorough job of responding to the comments from reviewers.	rebuttal_process
2021-564	The paper has a clear contribution, namely that attention maps predict contacts as well as existing unsupervised pipelines.	strength
2021-564	This paper deserves to be published. <sep>	decision
2021-564	In the final version, the authors should discuss briefly "BERTology Meets Biology: Interpreting Attention in Protein Language Models"(https://openreview.net/forum?id=YWtLZvLmud7) and "Improving Generalizability of Protein Sequence Models via Data Augmentations" (https://openreview.net/forum?id=Kkw3shxszSd).	suggestion
2021-564	However, the authors should also make sure that the final version respects the *CONF* length limits. <sep>	suggestion
2021-564	I am recommending poster acceptance because the results are anticlimactic given the recent success of Deepmind at CASP 2020.	decision

2021-569	The paper considers the use of adversarial self-supervised learning to render robust data representations for various tasks, in particular to integrate the Bootstrap Your Own Robust Latents (BYOL) with adversarial training, where a small amount of labeled data is available together with a sizable unlabeled dataset.	abstract
2021-569	Especially the low-data regime is of interest.	strength
2021-569	It extends a previous method with a new adversarial augmentation technique, it is compared against several methods, and the robust representations are shown to be useful more generally.	abstract
2021-569	There were some confusing presentations and questions that were resolved in a detailed discussion with the reviewers.	rebuttal_process

2021-574	This paper focuses on the problem of robust overfitting.	abstract
2021-574	The philosophy behind sounds quite interesting to me, namely, injecting more learned smoothening during adversarial training.	strength
2021-574	This philosophy leads to two simple yet effective methods: one leveraging knowledge distillation and self-training to smooth the logits, and the other performing stochastic weight averaging to smooth the weights. <sep>	strength
2021-574	The clarity and novelty are above the bar of *CONF*.	strength
2021-574	While the reviewers had some concerns on the significance, the authors did a particularly good job in their rebuttal.	rebuttal_process
2021-574	Thus, all of us have agreed to accept this paper for publication!	decision
2021-574	Please carefully address all comments in the final version.	suggestion

2021-591	This paper presents a hyperparameter optimization (HPO) method in which two search strategies: global and local optimizations, are effectively combined.	abstract
2021-591	All reviewers evaluated the proposed method positively.	rating_summary
2021-591	The experimental results clearly show the effectiveness of the proposed method, and it could be an important contribution to the AutoML research community.	strength
2021-591	On the other hand, since there is no theoretical justification for the proposed method, it is not clear why the performance of the proposed method is improved so much.	weakness
2021-591	The author's rebuttal has alleviated some of our concerns on this point, but the further theoretical analysis is desirable.	rebuttal_process

2021-602	This paper proposes benchmark tasks for offline policy evaluation.	abstract
2021-602	The proposed benchmark tasks evaluate the policy with batch data with respect to three metrics, including the standard mean squared error.	abstract
2021-602	The paper also evaluate several baseline offline reinforcement learning methods with the benchmark tasks, which will serve as standard baselines.	abstract
2021-602	All of the reviewers are in favor of the paper.	rating_summary

2021-621	This paper presents a method named LowKey, which is designed to protect user privacy.	abstract
2021-621	This is done by taking advantage of adversarial attacks to pre-process facial images against the black-box facial recognition system in social media, yet the processed facial images remain visually acceptable.	abstract
2021-621	The paper experimentally illustrates that it is effective against two existing commercial facial recognition APIs. <sep>	abstract
2021-621	The reviewers unanimously agree that this is an interesting and important problem, and recommend the paper for acceptance.	rating_summary
2021-621	The ACs agree.	decision

2021-631	The authors appreciated this submission because (a) the aspect of explainability is novel, (b) its strong performance, (c) the clarity of the paper.	strength
2021-631	I urge the authors to double check all of the reviewer comments to make sure they are all addressed in the updated version.	suggestion
2021-631	I vote to accept.	decision

2021-634	This paper provides a natural combination of conditional neural processes with LieConv models.	abstract
2021-634	It is a good step forward for stochastic processes with equivariances.	strength
2021-634	While there is still room to improve the experiments, the authors provided a good response to reviewers, and the paper is a nice contribution.	rebuttal_process

2021-637	This paper proposes a method for regularizing the pre-training of an embedding function for relation extraction from text that encourages well-formed clusters among the relation types.	abstract
2021-637	Experiments on FewRel, SemEval 2010 Task 8, and a proposed FuzzyRed dataset show that the proposed prototype method generally outperforms prior state-of-the-art, including MTB (Soares et al, 2019), which was the strongest.	abstract
2021-637	The key, novel idea is to model prototype representations for target relations as part of the learning process.	abstract
2021-637	A contribution of the work is to show that learning prototype representations are useful in supervised deep learning architectures even beyond few-shot learning.	abstract
2021-637	This additional learning objective is useful as an inductive bias, and is perhaps of interest even beyond relation extraction research. <sep>	strength
2021-637	Reviewers generally found the proposed method sound and intuitive, and the original set of experiments promising.	strength
2021-637	Some of the reviewers raised concerns about the setup of the experiments, including the relationship between the pre-training and target tasks, and the need for several additional baselines.	weakness
2021-637	The authors were able to address these concerns, and the reviewers did not raise any follow-up concerns.	rebuttal_process

2021-639	This work improves deep generative models by applying Langevin dynamics to sample in the latent space.	abstract
2021-639	The authors test their method under different configurations (different loss functions) and various generative models (VAE, flow, besides GAN).	abstract
2021-639	Experimental results demonstrate the benefits of the proposed method in different generative tasks. <sep>	abstract
2021-639	I tend to accept this solid work.	decision
2021-639	I just have two suggestions: 1) the authors should discuss the connections and the differences between the proposed method and the energy-based methods like (Arbel et al, 2020) in-depth; 2) it may be more suitable to replace "Wasserstein gradient flow" with "Discriminator gradient flow" in the title.	suggestion

2021-641	Solid work on extending AntisymmetricRNN and expanding its expressivity while controlling the global stability of the recurrent dynamics.	strength
2021-641	It contributes to the growing interest in continuous-time RNN formulations that can deal with exploding gradient problem, and worthy of *CONF* poster presentation.	decision
2021-641	Three reviewers were positive and one was slightly negative.	rating_summary
2021-641	Authors added additional experiments and strengthened the manuscript significantly during the review process.	rebuttal_process

2021-662	This paper presents a new inference mechanism for latent variable models, by taking the derivative of log-likelihood with respect to a zero-valued vector.	abstract
2021-662	Initially, the reviewers raised concerns mostly regarding the limited experimentation and missing baselines.	rebuttal_process
2021-662	However, in the revised version, the authors addressed most of these concerns. <sep>	rebuttal_process
2021-662	Given that most reviewers are positive after the revision and since the proposed method is simple and interesting, I recommend accepting this paper.	decision

2021-670	This paper investigates knowledge distillation in the context of non-autoregressive machine translations.	abstract
2021-670	All reviewers are supportive of acceptance, especially after the thoughtful author responses.	rating_summary
2021-670	A well motivated and simple to implement approach that is giving good empirical results.	strength

2021-675	The reviewers agree that the submitted paper is of high quality and provides a promising approach/framework for Bayesian IRL.	strength
2021-675	Certain concerns regarding details of the implementation and evaluation have already been addressed by the authors during the rebuttal phase, and also the title of the paper was adjusted in line with discussions with the reviewers.	rebuttal_process
2021-675	For the final paper, the authors should make sure to clearly highlight the advances of inferring a distribution over rewards (this is already partly done by the added grid world experiments) and discuss relations to VAEs as the initially had in mind and even in the paper title.	suggestion
2021-675	Beyond that, the should of course also address other reviewers' comments.	suggestion

2021-683	The authors did a good job responding to reviewer concerns.	rebuttal_process
2021-683	While the reviewers still consider the method described in the paper to not be especially novel, at least one is impressed by the practicality.	rebuttal_process
2021-683	imo the authors' attention detailed ablations and analysis post-review makes the paper worth including in the conference.	decision

2021-692	Reviewers agree that the paper excels in providing a principle pipeline that combines CNNs and GPs with a Poisson-Gamma distribution to provide a generic approach for multiresolution modelling of tumour mutation rates.	abstract
2021-692	As a whole such combination of techniques addresses a key challenge in computational biology that also scales to large datasets.	abstract

2021-698	This paper provides some theoretical perspective on the use of data augmentation in consistency regularization-based semi-supervised learning.	abstract
2021-698	The framework used in the paper argues that high-quality data augmentation should move along the data manifold.	abstract
2021-698	This generic view allows the paper's ideas to be applied across datasets (as opposed to image-specific data augmentation used in state-of-the-art semi-supervised learning algorithms).	abstract
2021-698	I am not aware of any other work raising these points, and indeed this paper is significant in that it provides a new and potentially useful perspective on the most performative semi-supervised learning approach.	strength
2021-698	Reviewers agreed that the paper was clear and useful.	strength
2021-698	The main concern was that the paper only included experiments in toy settings.	weakness
2021-698	Indeed, it would have been much more impactful to apply these ideas to state-of-the-art semi-supervised learning methods, but I think it can be excused given the theoretical focus of the work.	ac_disagreement

2021-714	The paper studies a hierarchical or multi-level version of local SGD, extending earlier work by (Wang & Joshi, 2018), (Lin et al, 2018) and  (Jiang et al 2019) among others.	abstract
2021-714	It gives novel convergence rates in relevant settings, such as by allowing different workers to take different numbers of local steps within a given time interval.	abstract
2021-714	The current analysis is restricted to the IID data case, but still insightful, and might serve as a useful building block for follow-up research in the future. <sep>	strength
2021-714	Smaller concerns remained that the presented multi-level results cannot exactly recover local SGD as a special case.	weakness
2021-714	Nevertheless the consensus remained that the overall contributions and relevance of the paper remain above the bar.	rating_summary
2021-714	In the discussion phase, several concerns were clarified and additional deep learning experiments have been added to the paper, which is appreciated.	rebuttal_process

2021-721	This paper proposes an autoregressive flow-based network, Flowtron, for TTS with style transfer.	abstract
2021-721	It integrates the Tacotron architecture with the flow-based generative model.	abstract
2021-721	Extensive experiments are carried out in a controlled manner and the results show that the proposed Flowtron framework can achieve comparable MOS scores to the SOTA TTS models and is good at generating speech with different styles.	abstract
2021-721	All reviewers consider the work interesting.	strength
2021-721	There are concerns raised on technical details which mostly have been cleared by the authors' rebuttal.	rebuttal_process
2021-721	The exposition also has been greatly improved based on the reviewers' suggestions and questions.	rebuttal_process
2021-721	Overall, this is an interesting paper and I would recommend acceptance.	decision

2021-722	This paper proposes a model for predicting edits to trees given an edit specification that comes either from the ground truth before-after state ("gold" setting, like reconstruction error of auto-encoder) or from the before-after state of an analogous edit.	abstract
2021-722	The problem setting follows mostly from Yin et al (2019). <sep>	abstract
2021-722	There are several shortcomings of this paper: <sep> The technical novelty of the model is somewhat limited, as it's an assembly of components that have been used in related work.	weakness
2021-722	Authors insist in the discussion on the novelty of the tree edit encoder (Sec 3.2), but I think this is overstated.	rebuttal_process
2021-722	The related tree-edit models (eg, Tarlow et al (2019)) perform a very similar encoding in the decoder when training with teacher-forcing.	rebuttal_process
2021-722	While it's true that decoders are typically thought of as monolithic entities that generate a sequence of edits from a state, inside the teacher-forced training, the models are computing a representation of a prefix of ground truth edits, which are then repeatedly used to predict next edits.	rebuttal_process
2021-722	AFAIU, the proposal is basically to use this hidden representation as the edit encoder.<sep>	rebuttal_process
2021-722	The claim that the approach is more language agnostic than Dinella et al (2020) also seems shaky, as the authors admit in their response that language-specific grammars need to be handled specially.	rebuttal_process
2021-722	eg, I expect that the authors of Dinella et al would find it easier to extend their existing code to use a new language than to adapt this approach.<sep>	rebuttal_process
2021-722	The submission relies too heavily on the "gold" setting (where the target output is fed as an input), and I'm skeptical of their characterization of Yin et al's intentions when the authors say in comments, "Because of this, models that are able to reproduce the desired output effectively have a demonstrably better inductive bias that allows them to do so efficiently.	rebuttal_process
2021-722	This was the original motivation expressed by Yin et al (2019)."	rebuttal_process
2021-722	I don't see this stated in the Yin et al paper.	rebuttal_process
2021-722	I see Yin et al characterizing this setting as an upper bound and saying "better performance with the gold-standard edit does not necessarily imply better (more generalizable) edit representation."	rebuttal_process
2021-722	(Yin et al, 2019).	rebuttal_process
2021-722	It's worrying that the proposed model only seems to do better in this setting, which would be very easy to game if one were aiming to directly optimize for it.<sep>	rebuttal_process
2021-722	Having said this, (1) is not a standard way to think about encoding edits, (2) is debatable, and we can hope that future work does not treat improvements in the "gold" setting as a valid research goal.	suggestion
2021-722	Further, there is another contribution around imitation learning that the reviewers appreciate.	strength
2021-722	In total, reviewers did an excellent job and generally believe the paper should be accepted.	rating_summary
2021-722	I won't go against that recommendation.	decision

2021-731	Paper summary <sep> This paper investigates theoretically and empirically the effect of increasing the number of parameters ("overparameterization") in GAN training.	abstract
2021-731	By analogy to what happens in supervised learning with neural networks, overparameterization does help to stabilize the training dynamics (and improve performance empirically).	abstract
2021-731	This paper provides an explicit threshold for the width of a 1-layer ReLU network generator so that gradient-ascent training with a linear discriminator yields a linear rate of convergence to the global saddle point (which corresponds to the empirical mean of the generator matching the mean of the data).	abstract
2021-731	The authors also provides a more general theorem that generalizes this result to deeper networks. <sep>	abstract
2021-731	Evaluation <sep> The reviewers had several questions and concerns which were well addressed in the rebuttal and following discussion, in particular in terms of clarifying the meaning of "overparameterization".	rebuttal_process
2021-731	After discussing the paper, R1, R2 and R4 recommend acceptance while R3 recommends rejection.	rating_summary
2021-731	The main concern of R3 is that the GAN formulation analyzed in the paper is mainly doing moment matching between the generator  distribution (produced from a fixed set of latent variables z_i) and the empirical mean of the data.	weakness
2021-731	R3 argues that this is not sufficient to "understanding the training of GANs".	weakness
2021-731	At least two aspects are missing: how the distribution induced by the generator converges according to other notion of divergence (like KL, Wasserstein, etc.	weakness
2021-731	); and what about the true generator distribution (not just its empirical version from a fixed finite set of samples z_i)?	weakness
2021-731	While agreeing these are problematic, the other reviewers judged that the manuscript was useful first step in understanding the role of overparameterization in GANs and thus still recommend acceptance.	rating_summary
2021-731	And importantly, this paper is the first to study this question theoretically. <sep>	strength
2021-731	I also read the paper in more details.	misc
2021-731	I have a feeling that some aspects of this work were already developed in the supervised learning literature; but the gradient descent-ascent dynamic aspect appears novel to me and the important question of the role of overparameterization here is both timely, novel and quite interesting.	strength
2021-731	I side with R1, R2 and R4: this paper is an interesting first step, and thus I recommend acceptance.	decision
2021-731	See below for additional comments to be taken in consideration for the camera ready version. <sep>	suggestion
2021-731	Some detailed comments<sep> Beginning of section 2.3: please be clearer early on that you will keep V fixed to a random initialization rather than learning it.	suggestion
2021-731	The fact that this is standard in some other papers is not a reason to not be clear about it. <sep>	suggestion
2021-731	Theorem 2.2: in the closed form of the objective when d is explicitly optimized, we are back to a more standard supervised learning formulation, for example (5) could look like regression.	suggestion
2021-731	The authors should be more clear about this, and also mention in the main text that the core technical part used to prove Theorem 2.2 is from Oymak & Soltanolkotabi 2020 (which considers supervised learning).	suggestion
2021-731	This should also a bit more clear in the introduction -- it seems to me that the main novelty of the work is to look at the gradient-descent dynamic, which is a bit different than the supervised learning setup, even though some parts are quite related (like the full maximization with respect to d). <sep>	suggestion
2021-731	p.6 equation (8): typo -- the  −μdt term is redundant and should be removed as already included from ∇dh(d,θ). <sep>	suggestion
2021-731	p.7 "numerical validations" paragraph: Please describe more clearly what is the meaning of "final MSE".	suggestion
2021-731	Is this a global saddle point (and thus shows the limit of the generator to match the empirical mean), or is this coming from slowness of convergence of the method (eg after a fixed number of iterations, or according to some stopping criterion?).	suggestion
2021-731	Please clarify.	suggestion

2021-746	This work investigates the recently proposed hypothesis that enhanced shape bias improves neural network robustness to common corruptions.	abstract
2021-746	Several interesting experiments are performed to better understand the contributing factors that lead to improved robustness of models trained with texture randomization.	abstract
2021-746	Of particular note, the authors design a data augmentation strategy that verifiably increases the shape bias of model, but for which corruption robustness is not improved.	abstract
2021-746	Reviewers agreed that this is an interesting counter-example to the shape-bias hypothesis and improves our understanding of why stylization improves robustness.	strength
2021-746	Given the carefully designed experiments investigating an important topic I recommend accept.	decision

2021-754	The paper provides a new distance preserving embedding based on a recent result called sigma-delta quantization.	abstract
2021-754	The authors notice that in many realistic scenarios, the input vectors are well-spread and under assumptions regarding the spreadness provide a fast technique to convert the input vectors into binary vectors, possibly of lower dimension.	abstract
2021-754	For completeness, the authors analyze the setting where the vectors are not spread and show that by using a randomized Walsh-Hadamard transform, their results still apply. <sep>	abstract
2021-754	The authors do not provide a completely novel approach, to quote R2 "On a technical level the results in this paper are hardly too surprising for the JL community, but it is nice to see this analysis worked out in detail".	strength
2021-754	That being said, they show that a natural idea indeed works out by providing both a theoretical analysis and experimental results.	strength
2021-754	The experiments can be more thorough but do convey the point that the result indeed works and moreover is somewhat robust in that it works well even when the formal requirements do not entirely hold. <sep>	strength
2021-754	There are a few issues mentioned by the reviewers that should be addressed: A clearer exposition of the guarantees and assumptions, some comparison with previous papers.	weakness
2021-754	However given the responses and discussions these seem minor and fixable towards a camera ready version.	rebuttal_process
2021-754	I recommend accepting the paper	decision

2021-760	This paper presents a model for video action recognition.	abstract
2021-760	The reviewers appreciated the development of a novel dynamic fusion method that examines channels from feature maps for use in temporal modeling.	strength
2021-760	After reading the authors' responses, the reviewers converged on an accept rating.	rating_summary
2021-760	The solid empirical results and analysis, the fact that is is a plug-in method that could be used in other models, and the clear exposition were deemed to be positives.	strength
2021-760	As such, this paper is accepted to *CONF* 2021.	decision

2021-767	This paper was reviewed by four experts in the field.	misc
2021-767	Based on the reviewers' feedback, the decision is to recommend the paper for acceptance to *CONF* 2021.	decision
2021-767	The reviewers did raise some valuable concerns that should be addressed in the final camera-ready version of the paper.	suggestion
2021-767	The authors are encouraged to make the necessary changes.	suggestion
2021-767	It is also very important to think about how to extend this framework to the more challenging CLEVERER dataset (http://clevrer.csail.mit.edu/).	suggestion

2021-789	This paper addresses the problem of visual object navigation by defining a novel visual transformer architecture, where an encoder consisting of a pretrained object detector extracts objects (ie their visual features, position, semantic label, confidence) that will serve as keys in an attention-based retrieval mechanism, and a decoder computes global visual features and positional descriptors as a coarse feature map.	abstract
2021-789	The visual transformer is first pretrained (using imitation learning) on simple tasks consisting in moving the state-less agent / camera towards the target object.	abstract
2021-789	Then an RL agent is defined by adding an LSTM to the VTNet and training it end-to-end on the single-room subset of the AI2-Thor environment where it achieves state-of-the-art performance. <sep>	abstract
2021-789	After rebuttal, all four reviewers converged on a score of 6.	rating_summary
2021-789	The reviewers praised the novelty of the method, extensive evaluation with ablation studies, and the SOTA results.	strength
2021-789	Main points of criticism were about clarity of writing and some explanations (which the authors improved), using DETR vs.	weakness
2021-789	Faster R-CNN, and the relative simplicity of the task (single room and discrete action space).	weakness
2021-789	There were also minor questions, a request for more recent transformer-based VLN bibliography, and a request for a new evaluation on RoboThor.	weakness
2021-789	One area of discussion -- where I empathise with the authors -- was regarding the difficulty of pure RL training of transformer-based agents and the necessity to pre-train the representations. <sep>	rebuttal_process
2021-789	Taking all this into account, I suggest this paper gets accepted.	decision

2021-809	This paper proposes an lightweight method for cross-domain few-shot learning, using a meta-learning approach to predict batch normalization statistics. <sep>	abstract
2021-809	After the extensive paper revisions and discussion, the reviewers all agreed that this paper is above the bar for acceptance, assuming that the authors will include results for both the standard and expanded target set size in the final version of the paper.	rating_summary
2021-809	The authors are strongly encouraged to include these results in the camera-ready version of the paper.	suggestion

2021-830	This paper provides a theoretically rigorous treatment of approximation properties and convergence analysis of LINEAR RNNs.	abstract
2021-830	The reviewers were divided in their evaluation.	misc
2021-830	On the positive side, the presented relation between approximation error and required memory size is not obvious and interesting.	strength
2021-830	On the less positive side, two of the reviewers raised the necessity of mathematical machinery that were invoked.	weakness
2021-830	Furthermore, its applicability is unclear in ML, since they aren't applicable to the usual nonlinear RNNs.	weakness
2021-830	However, given that the theoretical contributions are clear, the final decision was to accept.	decision

2021-836	There was some positive consensus towards this paper, which slightly improved after the very strong author rebuttal.	rebuttal_process
2021-836	Reviewers, in general, appreciate the simplicity of the approach as well as its effectiveness.	strength
2021-836	The most acute criticisms derived from several theoretical and technical points, similarity with [Mizadeh, 2020], and missing baseline comparisons.	weakness
2021-836	The author rebuttal responds to each of these points very clearly and convincingly, as well as with new experimental baseline comparisons that clearly demonstrate the effectiveness of the CPR approach.	rebuttal_process
2021-836	I encourage the authors to include the extensive comparison with [Mizadeh, 2020] provided in the rebuttal, especially given the similarity to the proposed approach.	suggestion
2021-836	and to also tone down the strong claims of novelty in light of the similarities.	suggestion

2021-844	Most reviewers found the method proposed to be technically sound, well-motivated and particularly interesting due to the interpretability of its results.	strength
2021-844	Indeed, the extraction of interpretable motifs from NAS is a valuable contribution.	strength
2021-844	One of the reviewers was particularly concerned by the lack of guarantees of the proposed method and a perceived failure mode of averaged gradients.	weakness
2021-844	We thank both the reviewer and the authors for the detailed discussion on these points.	misc
2021-844	Ultimately, the benefits of the method proposed and the magnitude of the contributions in the paper outweigh these concerns.	strength

2021-862	This paper considers the problem of identification of causal effects under the unsupervised domain adaptation setting.	abstract
2021-862	The authors assume the invariance of the causal structure and use it to regularize the predictor of causal effects.	abstract
2021-862	The method is interesting and looks effective, although this assumption may not hold always true (eg, in some domains, some causal influences may disappear, leading to extra conditional independence relations).	strength
2021-862	Hope the authors will update the paper to address the concerns raised by the reviewers, especially to conduct a sensitivity analysis of the framework to misspecification of the causal structure and make the motivation for the used evaluation metrics clear, and also provide a more thorough review of related work.	suggestion

2021-876	The reviewers agreed that the paper presents interesting ideas but the presentation of the paper needs be improved.	weakness
2021-876	Also, the experiments and the related work section need be improved.	weakness

2021-882	This paper deals with a problem of feature compatible learning, where the features produced by new model should be compatible with old features.	abstract
2021-882	As pointed out by the reviewers, there are several weaknesses with this paper: (a) the novelty is not strong enough, (b) the experimental results should be better explained and be more thorough, (c) the formulation is not well motivated.	weakness

2021-883	The authors consider view-consistency when learning graph neural networks.	abstract
2021-883	However, as mentioned by the reviewers, the novelty of the proposed method is limited and the rationality of the implementation is not convincing.	weakness
2021-883	More deep discussions about related papers and analytic experiments are required to support this work.	suggestion
2021-883	Additionally, I have concerns about the scalability of the method --- whether it can deal with more than two views and how it will perform are not studied in this work.	weakness
2021-883	I tend to reject it based on its current status.	decision

2021-885	The paper presents a new online convex optimization algorithm that uses per-coordinate learning rates.	abstract
2021-885	The learning rates are changed over time using information coming from the gradients.	abstract
2021-885	A regret upper bound is proved and the algorithm is empirically validated on deep learning experiments. <sep>	abstract
2021-885	While the analysis is in principle correct, it does not seem to provide any advantage over the guarantees of similar algorithm, for example the mirror descent version AdaGrad with diagonal matrices.	weakness
2021-885	Also, despite the intuition of the authors, the reviewers have found that the approach used in the analsysis is fundamentally bounded to give a worse guarantee than AdaGrad.	weakness
2021-885	Overall, the theoretical contribution appears to be not sufficient. <sep>	weakness
2021-885	On the empirical side, the experiments failed to convince the majority of the reviewers that the algorithm has a significative gain over similar algorithms. <sep>	weakness
2021-885	More generally, this paper suffers from the same problem of many other similar papers: There is a complete disconnect from the theory proven under restrictive assumptions (convexity, bounded domains, no stochasticity) and the experiments (non-convex functions, no projection on bounded domain, stochastic setting).	weakness
2021-885	Unfortunately, the deep learning literature is full of such papers, but the community should strive to do better and substantially raise the quality of field.	weakness
2021-885	In this view, I strongly suggest to the authors to try to improve the theoretical contribution, for example, trying to prove a convergence guarantee of the gradients to 0, rather than focusing on regret upper bounds.	suggestion
2021-885	Such analysis would also suggest better ways to design new optimization algorithms better suited to non-convex problems.	suggestion

2021-913	I thank the authors and reviewers for the lively discussions.	misc
2021-913	Although reviewers mentioned the work has potentials to improve adversarial robustness, they agreed that the current draft needs a bit more work specially to strengthen its experimental results and comparisons with related works.	weakness

2021-915	This paper proposes a contribution aiming at understanding the cause of errors in few-shot learning.	abstract
2021-915	The motivation is interesting but the reviewers pointed out many aspects that require more precisions and polishing in addition to the fact that the upper bound provided it rather loose.	weakness
2021-915	The rebuttal provided addresses some concerns, but there are still some remarks that require some clarifications en work. <sep>	rebuttal_process
2021-915	Hence, I propose rejection.	decision

2021-933	All reviewers agree that the current approach is very similar to traditional uncertainty-based active learning, and that the empirical results are inconclusive, so at this point the paper is not ready for publication.	decision

2021-937	The authors empirically analyse the properties of datasets which lead to poor calibration.	abstract
2021-937	In particular, they show that high class imbalance, high degree of label noise, and small dataset size are all likely to lead to poor overall calibration or poor per-class calibration.	abstract
2021-937	While there are some interesting insights in this work, the reviewers argued that the contribution is not substantial enough for *CONF*.	rating_summary
2021-937	To improve the manuscript the authors should consider accuracy and calibration jointly and extend the results pertaining to label noise which were appreciated by the reviewers.	suggestion
2021-937	For the former, the same conclusions hold for accuracy, instead of calibration, which raises the question of their relationship -- is there a tradeoff?	suggestion
2021-937	For the latter, the reviewers pointed to a concrete extension with structured label noise.	suggestion
2021-937	Finally, the theoretical analysis is a step in the right direction, but the assumption on the width of the network required to fit the training set is too restrictive in practice.	weakness
2021-937	Therefore, I will recommend rejection.	decision

2021-952	During the discussion phase, although the reviewers acknowledge superior empirical performance of the proposed method, they shared the two major concerns: <sep> 	rebuttal_process
2021-952	Lack of theoretical or empirical justification/proof for the key statement: "the current methods do not effectively maximize the MI objective because greedy SGD typically results in suboptimal local optima". <sep>	weakness
2021-952	Lack of comparisons with newer methods from eg ECCV2020 etc.<sep>	weakness
2021-952	In particular, the first point is crucial.	misc
2021-952	As the reviewers pointed out, since it is the main contribution and the key message of this paper, it should be carefully examined theoretically and/or empirically.	weakness
2021-952	However, in its current state, there is no theoretical analysis, and empirical evaluation is not convincing. <sep>	weakness
2021-952	About the second point, although I think it cannot be a solo reason for rejection, at least it is better to cite and discuss it fo the completeness. <sep>	suggestion
2021-952	Overall, the contribution of this paper it not significant enough for publication.	decision
2021-952	Hence I will reject the paper.	decision

2021-965	Since the authors have decided to withdraw this submission, it has been rejected from the conference.	decision

2021-985	Although the paper is clearly written overall and well motivated, reviewers raised several crucial concerns and, unfortunately, the authors did not respond to reviews. <sep>	rebuttal_process
2021-985	During the discussion, reviewers agree with that this submission is not ready for publication.	rating_summary
2021-985	In particular, empirical evaluation is not thorough as important baselines are not included and discussion is not convincing. <sep>	weakness
2021-985	I will therefore reject the paper. <sep>	decision
2021-985	For future submission, I strongly recommend the authors to do author response.	suggestion
2021-985	There are many cases where the reviewers change their scores based on the interaction between the authors and the reviewers, which is healthy for the review process.	misc

2021-990	This paper is right at the borderline: the reviewers agree it is well written, proposing a simple but interesting idea.	rating_summary
2021-990	However, there was a feeling among the reviewers (especially reviewer 1) that the paper could be strengthened considerably with a better discussion/some theory on the sufficiency of the calibration vectors, as well as experiments on larger datasets.	suggestion
2021-990	Doing one of these would have substantially strengthened the paper.	suggestion
2021-990	Due to the remaining shortcomings, the recommendation is not to accept the paper in its present state.	decision

2021-994	The authors propose a new dataset and compositional task based on the EPIC Kitchens dataset.	abstract
2021-994	The goal is to test novel compositions and to build a transformer based network specifically for this inference (by analogy).	abstract
2021-994	Specifically, the analogy here references the use of nearest neighbors in the dataset.	abstract
2021-994	There are a lot of concerns raised by reviewers which require a large number of changes to the presentation of the manuscript and they are not at present convinced by the current setup or experiments.	weakness
2021-994	Explicitly motivating which pretraining methods do or do not violate which aspects of composition and what role other factors like synonymy play in generalization is necessary.	weakness
2021-994	Several aspects of the claims made in the paper and in the discussion are big claims that require substantial discussion and analysis (eg the surprising weakness of pretrained models) which the reviewers do not feel can be so easily explained away (eg by domain shift).	weakness

2021-998	This paper proposes an anonymization method for federated learning based on the Indian buffet process.	abstract
2021-998	The reviewers found the idea interesting, but raised the following main concerns (please see the reviews for more details): <sep> Motivation and terminology needs clarification <sep> 	weakness
2021-998	Better comparison with secure aggregation methods <sep>	weakness
2021-998	Missing privacy guarantees <sep>	weakness
2021-998	Overall the reviewers of this paper are borderline.	rating_summary
2021-998	I hope the authors will take the reviewers' feedback into account when revising the paper.	suggestion

2021-1000	While this paper was perceived as being fairly well written, the level of novelty and the evaluation were seen as weak by many reviewers.	weakness
2021-1000	The aggregate opinions across reviewers is just too low to warrant an acceptance rating by the AC.	decision
2021-1000	The AC recommends rejection.	decision

2021-1004	This paper proposed an unsupervised domain adaptation method for 3D lidar-based object detection.	abstract
2021-1004	Four reviewers provided detailed reviews: 3 rated "Marginally above acceptance threshold", and 1 rated "Ok but not good enough - rejection".	rating_summary
2021-1004	The reviewers appreciated simple yet effective idea, the well motivated method, the comprehensiveness of the experiments, and well written paper.	strength
2021-1004	However, major concerns are also raised regarding the core technical contributions on the proposed approach.	weakness
2021-1004	The ACs look at the paper, the review, the rebuttal, and the discussion.	misc
2021-1004	Given the concerns on the core technical contributions, the high competitiveness of the *CONF* field, and the lack of enthusiastic endorsements from reviewers, the ACs believe this work is not ready to be accepted to *CONF* yet and hence a rejection decision is recommended.	decision

2021-1037	This paper proposes a hybrid algorithm that combines RL and population-based search.	abstract
2021-1037	The work is interesting and well-written.	strength
2021-1037	But, the contribution of the work is very limited, in comparison with the state-of-the-art.	weakness

2021-1041	This paper analyzes dropout and shows it selectively regularizes against learning higher-order interactions.	abstract
2021-1041	The paper received mixed reviews, with two in favor of rejection and one in favor of acceptance.	rating_summary
2021-1041	Specifically, while all reviewers find the intuitions and ideas in the paper adequate/plausible, two reviewers didn't find sufficient evident that supports the conclusions.	weakness
2021-1041	The reviewers provided very detail feedback, which the authors responded to, but it is apparent that some of the analysis needs to be reviewed again before the paper can be published.	rebuttal_process

2021-1065	This submission generated a lot of discussion. <sep>	misc
2021-1065	The main strengths of the paper: <sep> It is an interesting application of meta-learning (to 3D shape completion), and a novel one. <sep> 	strength
2021-1065	It appears to work well: it is remarkable that the proposed model can reconstruct shapes as well as it does given a point cloud with only 50 input points.<sep>	strength
2021-1065	The main concern (raised by two reviewers) is that while the method is described using the language of meta-learning, the proposed architecture ends up looking very similar to a variational autoencoder: a point cloud encoder that outputs some distribution in a latent space, which is then sampled from to produce a code which drives an implicit surface decoder.	weakness
2021-1065	The only difference appears to be that the proposed method uses a factored distribution in latent space, whereas a traditional VAE uses a non-factored one (ie a single multivariate Gaussian over all dimensions of the latent code). <sep>	weakness
2021-1065	One reviewer engaged the authors in a discussion about this point, but the resulting conversation was not satisfactory.	rebuttal_process
2021-1065	One interpretation of the authors' response is that they are simply not aware of how a VAE could be trained using variable-sized point clouds as input (which is quite possible using many standard point cloud processing networks).	rebuttal_process
2021-1065	However, at other points, they do seem to grasp this, when they write that "This flexible representation of the uncertainty [ie the one proposed by the authors] cannot be attained by the mere average or max pooling aggregation [what a PointNet encoder would do]."	rebuttal_process
2021-1065	They even go on to provide an additional ablation study where they replace their factored probabilistic encoder with a deterministic mean/max pool encoder, and show worse results.	rebuttal_process
2021-1065	Unfortunately, they never compare against probabilistic variants of such encoders (ie where the mean/max pool output is then used to compute a mean and variance). <sep>	rebuttal_process
2021-1065	Without seeing this comparison, the reviewers believe that this paper cannot be accepted, and I am inclined to agree. <sep>	decision
2021-1065	On a related note: one reviewer pointed out an issue with unfair comparisons, in that baselines were trained on high density point clouds and evaluated on low density ones.	rebuttal_process
2021-1065	The reviewer noted that these methods could be (and should have been) trained on point clouds of varying density.	rebuttal_process
2021-1065	Perhaps this relates to my hypothesis that the authors initially did not understand that training such encoders on variable-sized point clouds was possible.	rebuttal_process
2021-1065	In any event, in their rebuttals, they have reported some preliminary results from experiments which do this type of training, but these results are not conclusive.	rebuttal_process
2021-1065	Complete, conclusive results from these experiments would also need to be presented before this paper could be accepted.	decision

2021-1069	The paper proposes a new adaptive optimization algorithm which is claimed to have better convergence properties and lower susceptibility to gradient variance.	abstract
2021-1069	Reviewers found the idea of normalizing on the fly to be interesting, but raised some important concerns.	weakness
2021-1069	Although similar to AdaGrad, Expectigrad has a very important differentiation due to division by nt.	weakness
2021-1069	Assuming β=0 in my opinion is also ok and many papers assume this for analysis.	weakness
2021-1069	Even after accounting for these two facts, during discussions the reviewers considered the work to be incremental and a more thorough evaluation is needed to determine the benefits of algorithm.	weakness
2021-1069	Specifically, please compare to important and relevant baselines (like AdamNC and Yogi), because sometimes it felt like baselines were picked and dropped randomly.	weakness
2021-1069	The empirical improvement provided by Expectigrad compared to SOTA is not clear (both on synthetic problems from Reddi et al and real problems).	weakness
2021-1069	Thus, unfortunately, I cannot recommend an acceptance of the paper in the current form.	decision
2021-1069	However, I would strongly encourage authors to resubmit after improving according to reviewer suggestions. <sep>	suggestion
2021-1069	Some other minor points that came up during discussion are: <sep> choice of hyperparameters was not clear to reviewers, eg different optimizer may behave very differently for same set of hyperparameters, so it would not be fair to compare them as is. <sep> 	rebuttal_process
2021-1069	gradients would never be exactly zero in deep networks, so is current definition of nt good enough?	rebuttal_process

2021-1075	The paper introduces a new variant (SREDA-Boost) of a variance-reduced method SEDRA for nonconvex-strongly-concave min-max optimization.	abstract
2021-1075	Given that SEDRA is already optimal in the worst case, the proposed modification is intended to improve practical performance of the method, by relaxing conditions needed at initialization and allowing larger step sizes.	abstract
2021-1075	While the reviewers appreciated the main ideas of the paper, they shared concerns about the significance of the paper's technical contributions, which were ultimately not addressed by the authors in the rebuttal phase.	rebuttal_process

2021-1080	The reviewers raised a number of concerns, but the authors provided no rebuttal to the reviewers' comments. <sep>	rebuttal_process
2021-1080	One reviewer felt the experimental fitting was not thorough enough. <sep>	weakness
2021-1080	Suppose one used layers of oriented bandpass filters, separated by non-linearities, would that perform well on the task convnets are trained on? <sep>	weakness
2021-1080	The AC doesn't agree with the arguments of R3.	ac_disagreement
2021-1080	I hope the comments of the reviewers, particularly the many specific comments of reviewers R1 and R2, will be helpful to you as you revise the manuscript. <sep>	misc
2021-1080	The AC feels a more thorough experimental evaluation, and following-up on many of the suggestions of the reviewers will lead to a strong paper.	weakness
2021-1080	As it stands, however, with 3 recommendations for rejection (1 weak), and only 1 weak recommendation for acceptance, we need to reject.	decision

2021-1107	Overall the reviewers had various positive things to say about the paper, including that it was well written and easy to understand, topical, that the method was sensible, novel and interesting and that the computational efficiency (ie real time) was appealing.	strength
2021-1107	However, all the reviewers thought it wasn't quite ready for acceptance, mainly citing concerns with the empirical evaluation.	rating_summary
2021-1107	It seems they had trouble interpreting the empirical results and placing the work with respect to other relevant methods. <sep>	weakness
2021-1107	It seems in the author response, the authors did much to add to the experiments, but ultimately the reviewers were not comfortable with acceptance.	rating_summary
2021-1107	Taking the reviewers' feedback into account and adding the desired empirical evaluation would make this a much stronger submission to a subsequent conference.	decision

2021-1115	The reviewers overall appreciated the efforts of the authors in making NAS more computationally efficient.	strength
2021-1115	The paper could greatly benefit from further editing/restructuring with the goal of improving clarity, as it's currently hard to navigate and understand in places.	weakness
2021-1115	Future submissions of this work would benefit from more extensive empirical validation that motif networks mimic the original network.	suggestion
2021-1115	The reviewers also agreed that for the method to be appealing/useful, a general way to generate motif networks is needed.	suggestion
2021-1115	Overall, the outcome was that this is a very interesting idea but needs further development along the directions outlined above.	strength

2021-1117	This paper proposes enhancing contextualized word embeddings learned by Transformers by modeling long-range dependencies via a deep topic model, using a Poisson Gamma Belief Network (PGBN).	abstract
2021-1117	The experimental results show incorporating topic information can further improve the performance of Transformers.	abstract
2021-1117	While this is an interesting idea, reviewers pointed out some weaknesses: <sep> GLUE evaluation is not a test of long-term dependencies, it remains unclear whether providing topic information of preceding segments is enough to allow the model to draw information from these segments that is useful for a task. <sep> 	weakness
2021-1117	The improvement over the baseline does not seem to be significant. <sep>	weakness
2021-1117	The ablation study could be improved and more experiments could be done to understand the effect of hyperparameters choices from the topic model, such as the number of layers of PGBN as well as the topic number of each layer. <sep>	suggestion
2021-1117	A comparison of the model performance for different lengths of input sequences would be helpful. <sep>	suggestion
2021-1117	There are many recent methods for long.range transformer transformer variants, it would be interesting to compare them against the proposed latent topic-based method.<sep>	suggestion
2021-1117	Unfortunately, no answers are provided by the authors to the questions asked by the reviewers, which makes me recommend rejection.	decision

2021-1125	This work proposes a fully-explored masking strategy, segmenting the input text, which maximizes the Hamming distance between any two sampled masks on a fixed text sequence.	abstract
2021-1125	The hope is to reduce the large variance of MLM objective, based on the hypothesis that randomly sampled masks in MLM lead to undesirably large gradient variance, which typically hurts training efficiency with stochastic gradient optimization algorithms. <sep>	abstract
2021-1125	Pro<sep> A clear and interesting, novel leading theoretical idea.	strength
2021-1125	The paper has one good theme that it pursues. <sep>	strength
2021-1125	A mostly well-written paper <sep>	strength
2021-1125	Contains good theoretical discussion <sep>	strength
2021-1125	Experiments support the idea<sep>	strength
2021-1125	Con<sep> The experiments could be better, especially they don't actually measure a reduction in gradient variance only accuracy <sep>	weakness
2021-1125	The proofs are at best loose <sep>	weakness
2021-1125	Alternative methods of reducing variance like using large mini batches are not considered <sep>	weakness
2021-1125	The results might go away with use of stronger (larger) contextual LMs <sep>	weakness
2021-1125	There isn't good comparison to other methods of masking like span masking and salient term masking <sep>	weakness
2021-1125	Some of the things included seem quite haphazard (the ablations don't seem to the point, it's not really clear what this has to do with continual learning)<sep>	weakness
2021-1125	Overall, this paper feels to be in a premature state.	weakness
2021-1125	The idea is interesting, but the idea and the paper needs to be developed more, with stronger results.	weakness
2021-1125	I think it doesn't deserve to be accepted at this time.	decision

2021-1132	This paper studies the following broad question: How can we predict model performance when the data comes from different sources?	abstract
2021-1132	The reviewers agreed that the direction studied is very interesting.	strength
2021-1132	While the results presented in this work are promising, several reviewers pointed out some weaknesses in the paper, including a confusion between absolute loss and excess loss, and the limited scope of the experiments.	weakness
2021-1132	Overall, this paper does not appear to be ready for publication in its current form.	decision
2021-1132	In my personal opinion, if the concerns raised by the reviewers are appropriately addressed, this work could be publishable in a high quality venue.	misc

2021-1135	In this paper, the authors propose a theoretically principled neural network that inherently resists ℓ∞ perturbations without the help of adversarial training.	abstract
2021-1135	Although the authors insist to focus on the novel design with comprehensive theoretical supports, the reviewers still concern the insufficient empirical evaluations despite the novel idea and theoretical analysis.	rebuttal_process

2021-1140	The reviewers are in consensus that this paper is not ready for publication: cited concerns include simple (interesting) ideas but need to be carefully analyzed empirically, contextualized (other similar studies exist), identifying convincing empirical evidences, etc. <sep>	rating_summary
2021-1140	The AC recommends Reject.	decision

2021-1143	Taking all reviews and the work in consideration, unfortunately the work does not present the breadth it needs to sustain the claims it makes.	weakness
2021-1143	In particular, there work requires to analyse more architectures/variations of datasets with different properties and to provide more careful ablation studies that shows the efficiency of the 3 different proposed methods.	suggestion
2021-1143	Potentially removing one of this methods in order to give more space to analyse the others that seem more promising.	suggestion

2021-1192	The paper addresses regression in a weakly supervised setting where the correct labels are only available for examples whose prediction lie above some threshold.	abstract
2021-1192	The paper proposes a method using a gradient that is unbiased and consistent. <sep>	abstract
2021-1192	Pros: <sep> Problem setting is new and this paper is one of the first works exploring it. <sep> 	strength
2021-1192	The procedure comes with some unbiasedness and consistency guarantees. <sep>	strength
2021-1192	Experimental results on a wide variety of datasets and domains.<sep>	strength
2021-1192	Cons: <sep> Novelty and technical contribution is limited. <sep> 	weakness
2021-1192	Motivation of the problem setting was found to be unclear. <sep>	weakness
2021-1192	Some gaps in the experimental section (ie needing the use of synthetic data or synthetic modifications of the real data).<sep>	weakness
2021-1192	Overall, the reviewers felt that as presented, the paper did not convincingly motivate the proposed upper one-sided regression problem as important or relevant in practice, which was a key reason for rejection.	rating_summary
2021-1192	The paper may contain some nice ideas and I recommend taking the reviewer feedback to improve the presentation.	suggestion

2021-1211	This paper mostly received negative scores.	rating_summary
2021-1211	A few reviewers pointed out that the idea of modeling user preference in the frequency domain seems novel and interesting.	strength
2021-1211	However, there are a few concerns around the clarity of the paper, the motivation of the proposed approach, as well as the experimental results being unconvincing (both in terms of execution as well as exploration of the results).	weakness
2021-1211	The authors did not provide a response.	rebuttal_process
2021-1211	Therefore, I recommend reject.	decision

2021-1214	This paper proposed to theoretically explain why a pre-trained embedding network with self-supervised training (SSL) can provide representation for downstream few-shot learning (FSL) tasks.	abstract
2021-1214	The review process finds that the paper may over-claim the results and that the results seem unsatisfactory.	weakness
2021-1214	Both Reviewer 4 and Reviewer 5 expressed concerns regarding the writing, organizing, and grammar errors of this paper.	weakness
2021-1214	The paper needs a substantial revision to improve clarity and accessibility.	weakness
2021-1214	As pointed out by Nikunj Saunshi's public comment, this paper may benefit from discussing the differences from the previous works, including [1]. <sep>	suggestion
2021-1214	[1] Arora et al, A Theoretical Analysis of Contrastive Unsupervised Representation Learning, ICML 2019	misc

2021-1220	The reviewers are split.	misc
2021-1220	Two reviewers consider the technical contribution of the paper to be insufficient, and raise concerns about comparisons with Transformers or using more standard benchmarks for GNN experiments.	weakness
2021-1220	The other considers the experiments convincing and the method worth publishing.	strength
2021-1220	My own view is that this work is not ready for inclusion in the conference.	decision
2021-1220	In particular, I think this paper would be much stronger with either: <sep> 1: a more practical task to illustrate where this method might be applied in earnest, <sep>	suggestion
2021-1220	2: more analysis and baselines on the synthetic data.	suggestion
2021-1220	Synthetic data can be enough for a new method if it illuminates the functioning and the benefits and drawbacks.	suggestion
2021-1220	In this paper, we have synthetic data with little analysis, and imo (concurring with R5) insufficient baselines.	weakness
2021-1220	For example, while a vanilla Transformer probably could not do the matrix problems (with the matrices encoded naively), one might expect Transformers with sparse attention to do quite well on eg transpose and 90 degree rotation, especially given the training curriculum and proper positional embeddings; a convolutional network seems like a strawman.	weakness
2021-1220	I also agree with R5 that standard benchmarks for GNN exist, and these might be appropriate (or at least there should be some discussion of why they are not). <sep>	suggestion
2021-1220	3: some theoretical discussion of what the proposed model can do that other methods fundamentally cannot. <sep>	suggestion
2021-1220	I do think this is interesting work, and encourage the authors to revise and resubmit.	decision

2021-1223	The paper presents a DKL variant with a linear kernel.	abstract
2021-1223	Representations from several networks is combined through concatenation, making it not quite an ensemble.	abstract
2021-1223	It's shown that the model is a universal kernel approximator.	abstract
2021-1223	Experiments are conducted on a large number of UCI datasets. <sep>	abstract
2021-1223	Following the discussions, the paper still has the following shortcomings: <sep> some lack of clarity in the presentation (for instance, explaining the equivalence between a multi-output learner and M different single-output learners) <sep> 	weakness
2021-1223	lack of experiments on data where deep learning is typically used (images); the UCI datasets have structured data and other ensembles like XGBoost may outperform the baselines presented in this paper <sep>	weakness
2021-1223	difference in performance between DKL and DEKL, especially since DKL benefits from a larger model space, theoretically.	weakness
2021-1223	maybe DEKL has better sample complexity, but does this advantage hold in the case of the large datasets that deep learning is used for?	weakness

2021-1255	Sequence generation models trained via maximum likelihood estimation (or variants of so called 'teacher-forcing') condition on data samples during training and on model samples for predictions.	abstract
2021-1255	The susceptibility to this potential "mismatch" in input distribution is often referred to as exposure bias (EB). <sep>	abstract
2021-1255	This paper stresses that most research around EB is focused on addressing it, rather than defining and/or quantifying it.	abstract
2021-1255	Thus the submission questions the severity of EB and attempts to operationalise a testable definition for it.	abstract
2021-1255	Myself and all the reviewers strongly support the observations and the agenda, we find the question this paper asks an important one. <sep>	strength
2021-1255	Despite our appreciation for this paper's relevance, we have identified a number of problems that prevent me from recommending this paper.	misc
2021-1255	I will comment on the two most important points: <sep> The 'operational definition' of EB in this paper is not sufficiently precise to be testable.	weakness
2021-1255	It builds on the somewhat commonly accepted view that the effects of EB accumulate as the conditioning context grows longer, and that this causes a model to generate badly distorted sentences.	weakness
2021-1255	This definition still leaves quite some room for interpretation (without specifying reasonable expectation about how these effects 'accumulate' and what/how bad they are, it seems difficult to design tests).	weakness
2021-1255	We acknowledge that the submission attempts to shed light onto some of these aspects by having some 'control groups' using gold data and shuffled strings, but we did not find those sufficient (mostly in light of the next point).<sep>	weakness
2021-1255	MT evaluation metrics (essentially, string similarity metrics), most notably (but not exclusively) BLEU, are used in this work in a setting where we cannot easily grant that they have the discriminating power that the authors expect of them.	weakness
2021-1255	See this is not a criticism about the imperfections of BLEU (or any other automatic metric), but about the lack of evidence supporting its use against unrelated sentences.	weakness
2021-1255	We do not find it sufficient that some recent NLG papers have made similar use of it (I, for example, would have criticised those papers on similar grounds).<sep>	weakness
2021-1255	Overall, we believe this submission asks a relevant question, the insight about dependence on prefix is nice and might lead to a first operational definition of EB (which might be only a few refinements away from the version proposed here).	strength
2021-1255	The current evaluation is unconvincing and I believe the authors should be able to find more credible strategies, especially, strategies that have already gone through some scrutiny (for example, in literature around OOD detection and tests for distribution shift). <sep>	suggestion
2021-1255	Though I do not recommend this paper for acceptance, I hope the authors will find valuable feedback in the expert reviews attached.	decision

2021-1273	The paper solves a PDE using an additional penalty function between the derivatives of the function.	abstract
2021-1273	On toy examples and two PDEs it is shown that these additional terms help. <sep>	abstract
2021-1273	Pros: - The motivation is to include derivatives in the computationa <sep>	strength
2021-1273	- Implementation and testing on several examples, including high-dimensional ones <sep>	strength
2021-1273	- Timing is included in the latest version <sep>	strength
2021-1273	Cons: -The loss is Sobolev norm of the residuals of the equation. <sep>	weakness
2021-1273	- The usage of the norm of the residual is not 100% consistent with the smoothness properties of the corresponding equation.	weakness
2021-1273	For example, for the Poisson equation, the problem is selected in such a way the solution is analytic.	weakness
2021-1273	However, for example, if the zero boundary conditions are enforced, and right hand side is all ones, the solution will have singularities.	weakness
2021-1273	Thus, the main challenge would be the case when solution does have the singularities (and it will have it in many practical cases).	weakness
2021-1273	The L2-norm then is not the right functional for the solution to exist, not to say about the higher-order derivatives.	weakness
2021-1273	So, these functionals are not motivated by the theory of the solution of PDEs, but are rather focused on much smoother solution. <sep>	weakness
2021-1273	- Convergence.	weakness
2021-1273	There are quite a few papers on the convergence of DNN approximations to solution of PDEs.	weakness
2021-1273	The presented methods might have converged to a local minimum.	weakness
2021-1273	An important reference is the paper by Yarotsky D. Error bounds for approximations with deep ReLU networks.	weakness
2021-1273	Neural Networks.	misc
2021-1273	2017 Oct 1;94:103-14.	misc

2021-1315	The reviewers agree that the EM perspective of Federated Learning is novel and interesting.	strength
2021-1315	However, a common criticism is that the connection made is rather shallow and not sufficiently developed.	weakness
2021-1315	There look to be quite interesting potentials of the proposed framework and the specific FedSparse method, but I agree with the reviewers that both aspects need further development before they are in publishable form.	decision

2021-1325	All the reviewers found interesting the use of Shapley values to provide feature attributions for fairness, however, the reviewers brought up a number of issues, particularly in terms of presentation and clarity.	weakness
2021-1325	While the authors' responses did clarify some of these concerns, this was not enough for the reviewers to broadly support acceptance.	rating_summary

2021-1330	This paper studies the effect of the discount mismatch in actor-critics: the discount used for evaluation (often 1), the discount used for the critic and the discount used for the actor.	abstract
2021-1330	There's notably a representation learning argument supported by a series of experiments. <sep>	abstract
2021-1330	The initial reviews pointed out that this paper addresses very relevant research questions, sometimes in a quite original way, with a large set of experiments.	rebuttal_process
2021-1330	However, they also raised concerns about the organization/clarity of the paper, and possible weaknesses about the experimental studies. <sep>	rebuttal_process
2021-1330	The authors provided a rebuttal and a revision, that clarified some points and triggered additional discussions.	rebuttal_process
2021-1330	However, if the revision improved the initial submission, the shared assessment is that the clarity and experiments themselves are still somewhat lacking.	rebuttal_process
2021-1330	As such, the AC cannot recommend accepting this paper. <sep>	decision
2021-1330	Yet, this work does have interesting ideas, and the problem considered is of interest for the community and under studied.	strength
2021-1330	The authors are strongly encouraged to submit a revised version to a future venue.	suggestion

2021-1339	This paper proposes a self-supervised learning method for learning representations for graph-structured data, with both local and global objectives.	abstract
2021-1339	The local objective aims to maximize the mutual information between two correlated graphs generated with attribute masking [Hu et al 19], with the InfoNCE loss [van den Oord et al 18], and the global objective aims to cluster the graphs using the RPCL [Xu et al 93] objective, which pulls the sample toward the closest cluster while pushing it away from the rival clusters.	abstract
2021-1339	The proposed method is validated on standard graph classification benchmarks by training a linear classifier on top of the GNN pre-trained with it, and the results show that it largely outperforms existing graph pre-training methods. <sep>	abstract
2021-1339	This paper fell into a borderline case, receiving split reviews with two of the reviewers learning toward rejection, and two others proposing to accept.	rating_summary
2021-1339	The reviewers in general agreed that the experimental validation is thorough (except for one reviewer), and some of the reviewers mentioned that the proposed idea of performing self-supervised learning at both local and global level makes sense.	strength
2021-1339	However, the negative reviewers were concerned with the limited novelty of the proposed method, since the proposed method seems like a simple combination of two objectives each of which are based on existing ideas (although the latter has not been explored for GNN pre-training).	weakness
2021-1339	The reviewers had interactive discussions with the authors, and the authors provided detailed feedback.	rebuttal_process
2021-1339	Yet, the reviewers were not convinced that the method has sufficient novelty to warrant publication even after the internal discussion period, and decided to keep their negative ratings. <sep>	rating_summary
2021-1339	I believe that this is a simple yet effective pre-training method for GNNs on graph-structured data.	strength
2021-1339	The proposed method of combining the local and global objective seems like a promising solution to learn a metric space that well-captures the graph-level similarity and also is well-separated for discriminative classification, and it may have some practical impact given its good performance on benchmark datasets.	strength
2021-1339	However, as the two negative reviewers mentioned, the paper in its current form is presented as a simple combination of existing approaches.	weakness
2021-1339	The local objective is a slight modification of attribute masking strategy of [Hu et al 19], and the global objective of clustering has been explored in self-supervised learning of CNNs for image data [Asano et al 20].	weakness
2021-1339	Thus, I lean toward rejecting the paper, considering its relative novelty and quality. <sep>	decision
2021-1339	However, I find the proposed work highly promising, and encourage the authors to further develop the method while also improving on the paper writing.	strength
2021-1339	I suggest the authors to focus more on the main idea of learning with both local and global objectives, without specifically tying each objective to any of the existing methods.	suggestion
2021-1339	The authors may consider various techniques for both local and global objectives (such as hinge loss-based contrastive loss with k-means clustering as shown in the response to R3), and suggest the proposed work as a more general framework. <sep>	suggestion
2021-1339	[Asano et al 20] Self-Labeling via Simultaneous Clustering and Representation Learning, *CONF* 2020	misc

2021-1346	This paper proposes GAN-training of a non-autoregressive generator for text.	abstract
2021-1346	To circumvent the usual problems with non-differentiability of text GANs, the authors turn to Gumbel-Softmax parameterisation and straight-through estimation. <sep>	abstract
2021-1346	There are a number of aspects to this submission and they are not always clearly positioned.	weakness
2021-1346	I will concentrate on the two aspects that seem most crucial: <sep> The authors position their generator as an implicit generator, but it really isn't.	weakness
2021-1346	If we take the continuous interpretation of the output distributions: the Gumbel-Softmax transformation does correspond to a tractable density, the Concrete density of Maddison et al, with known parameter.	weakness
2021-1346	If we take the discrete interpretation of the output distribution: Gumbel-argmax is just an alternative to sampling from a Categorical distribution with known parameter.	weakness
2021-1346	In either case, the generator maps the noise source to a collection of conditionally independent distributions each of which has a known parameter and analytical density/mass function.	weakness
2021-1346	The authors do, however, train the architecture using a GAN-type objective as if the generator were implicit.<sep>	weakness
2021-1346	In the discussion phase the authors added that GAN training overcomes the independence assumptions made by the generator.	rebuttal_process
2021-1346	Whereas that makes intuitive sense, it suddenly changes the emphasis of the contributions, from proposing an implicit generator (presumably powerful for it being implicit) to proposing a way to circumvent the strong independence assumptions of the generator with a mechanism other than more traditional approximate marginalisation of VAEs.	rebuttal_process
2021-1346	In their rebuttal, the authors commented on the use of non-autoregressive VAEs in neural machine translation, and though those observations have indeed been made, they might well be specific to MT.	rebuttal_process
2021-1346	The simplest and more satisfactory response would be to ablate the use of the GAN objective (that is, to train a non-autoregressive VAE, also note that, with the same choice of likelihood, posterior collapse is rather unlikely to happen).<sep>	rebuttal_process
2021-1346	Other problems raised by reviewers were addressed in the rebuttal, and I would like to thank the authors for that.	rebuttal_process
2021-1346	For example, ablating the non-autoregressive generator and comparing to REINFORCE.	rebuttal_process
2021-1346	I believe these improved the submission. <sep>	rebuttal_process
2021-1346	Still, I cannot recommend this version for publication.	decision
2021-1346	I would suggest that the authors consider careful ablations of the components they see as precisely important for the results (that currently seems to be the GAN-like objective despite the model not, strictly speaking, requiring it).	suggestion

2021-1363	The paper deals with cross-domain few-shot learning in the case of large source-target domain shifts. <sep>	abstract
2021-1363	The paper received mostly below-threshold reviews, with one exception (R3) whose review is addressing more general aspects, but still with some concern, especially in relation to the experimental part (to which authors did not answer).	rating_summary
2021-1363	R1's review is not of much help. <sep>	ac_disagreement
2021-1363	Clarity of the presentation and missing details seem to be recurrent issues all over the reviewers, together with remarks concerning the experimental validation, which would have required a deep revision and improvement, in particular regarding the use of more backbones, better ablation (Hebbian learner contribution, unclear initialization), processing times/computational complexity, significant comparative analysis re robust baselines. <sep>	weakness
2021-1363	The rebuttal clarifies some of the raised remarks but there are still issues, especially regarding Hebbian learning rule and ensemble learning strategies, and about results too, so not all reviewers were convinced to raise their ratings. <sep>	rebuttal_process
2021-1363	Overall, given the above issues, I consider the paper not yet ready for publication in *CONF* 2021.	decision

2021-1380	The paper proposes to recalibrate predictive models by fitting a normalizing flow on top of the predictive model on a held out validation set using side information.	abstract
2021-1380	At a high level this idea has some potential, <sep>	strength
2021-1380	especially in the multivariate setting, but there are several directions for improvement: <sep> 	suggestion
2021-1380	Comparison with a broader set of baselines as suggested by the reviewers<sep>	suggestion
2021-1380	Clarity on why recalibrate with a normalizing flow especially in the 1-d case<sep>	suggestion
2021-1380	Why not any other model with explicit density?	suggestion
2021-1380	Are there other important desiderata?<sep>	suggestion
2021-1380	A motivating experiment that makes the potential value clear	suggestion

2021-1399	All three referees have provided detailed comments, both before and after the author response period.	rebuttal_process
2021-1399	While the authors have carefully revised the paper and provided detailed responses, leading to clearly improved clarity and quality, there remain clear concerns on novelty (at least not sufficiently supported with ablation study) and experiments (neither strong enough nor sufficient to support the main hypotheses).	rebuttal_process
2021-1399	The authors are encouraged to further improve their paper for a future submission.	decision

2021-1420	The paper proposes a variant of recurrent neural networks based on Long Short-Term Memory.	abstract
2021-1420	Unlike the standard LSTM, the proposed mass-conserving LSTM subtracts the output hidden state of the LSTM from its current cell state, thus preserving the "mass" stored in the cell states at each step.	abstract
2021-1420	A left-stochastic recurrent weight matrix is also used to conserve the "mass" across the time steps.	abstract
2021-1420	Empirical experiments demonstrated the effectiveness of the proposed MC-LSTM on a range of datasets such as addition & arithmetic tasks, traffic forecast, and rainfall modeling models. <sep>	abstract
2021-1420	Several issues were clarified during the rebuttal period in a way that satisfied the reviewers.	rebuttal_process
2021-1420	However, some concerns still remain unanswered: <sep> 	rebuttal_process
2021-1420	This is an empirical paper that proposes a modified LSTM that brings forward a few different ideas: L1 norm, stochastic transition matrices, and subtracting the output hidden states.	rebuttal_process
2021-1420	An ablation study is a MUST in such an applied work.	rebuttal_process
2021-1420	It has been pointed out by other reviewers that there are many prior references on LSTMs variants.	rebuttal_process
2021-1420	It would greatly strengthen the paper by considering more diverse baselines.	rebuttal_process
2021-1420	There is no experiment nor discussion on how much each modification helps wrt the final accuracy.	weakness
2021-1420	Thus it remains unclear how the results can generalize to other problems.<sep>	weakness
2021-1420	Although the results seem convincing across various datasets that mass conservation seems to help, the datasets are non-standard benchmarks in the machine learning conferences thus there is a lack of competitive prior baselines.	weakness
2021-1420	As the proposed LSTM has a different number of parameters compared to the standard LSTM, is it fair to compare the different architectures under the same number of neurons?	weakness
2021-1420	What happens if we compare the architectures with the same number of parameters?	weakness
2021-1420	And how well does the model scale as we vary the hidden size?	weakness
2021-1420	It would be helpful to keep the contributions into perspective by using standard RNN benchmark datasets such as Penn TreeBank or Wiki-8. <sep>	suggestion
2021-1420	Overall, the basic idea seems interesting, but the lack of ablation studies significantly hurt the contribution and the positioning of the paper.	weakness
2021-1420	Given the current submission, the paper needs further development, and non-trivial modifications, to be broadly appreciated by the machine learning community.	decision

2021-1434	The paper proposes a method for inference in models with GP priors and neural network likelihoods for multi-output modelling, dealing with the problem of scalability and missing data.	abstract
2021-1434	The paper builds upon previous work on inducing variables for scalability on GP models and inference networks for amortization (reducing the number of parameters to estimate) and dealing with missing data. <sep>	abstract
2021-1434	There are several concerns about the paper in terms of generality/flexibility of the approach, as the proposed model shares the NN parameters across tasks and the results on the small datasets do not show improvements wrt baseline such as GPAR.	weakness
2021-1434	The authors' comments provide somewhat satisfactory replies to these issues.	rebuttal_process
2021-1434	Nonetheless, the major drawback of this paper is its novelty as the ideas on the paper have been explored extensively in the GP literature.	weakness
2021-1434	Although the authors do make a case for scalability when using inference networks, there are other previous works that perhaps the authors are unaware of, for example, https://arxiv.org/abs/1905.10969   and even more sophisticated inference algorithms than can serve as truly state-of-the-art competing approaches (for example based on stochastic gradient Hamiltonian Monte Carlo, https://arxiv.org/abs/1806.05490).	weakness

2021-1444	The paper compares transfer learning with fine-tuning and joint training and then proposes a new approach (Merlin).	abstract
2021-1444	Reviewers have pointed to the fact that Merlin works in a setting that is different from normal transfer learning settings (it assumes some target domain data is available during training).	rebuttal_process
2021-1444	The authors acknowledge this and think it can still be a reasonable setting, but of course it makes comparisons more difficult.	rebuttal_process
2021-1444	Overall, while there are interesting analysis and results, the paper remains borderline and more work should be done to make it a good contribution, including significantly improving the presentation to make clear the distinction in settings.	suggestion
2021-1444	I therefore recommend to reject the paper.	decision

2021-1448	This paper proposes a method to update the learning rate dynamically by increasing it in areas with higher sharpness and decreasing it otherwise.	abstract
2021-1448	This would the hopefully leads to escaping sharp valleys and better generalization.	abstract
2021-1448	Authors further provide some related theoretical results and several experiments to show effectiveness of their models. <sep>	abstract
2021-1448	All reviewers find the proposed method well-motivated, novel and interesting.	strength
2021-1448	The paper is well-written and easy to follow.	strength
2021-1448	However, both theoretical results and empirical evaluations could be improved significantly: <sep>	weakness
2021-1448	1- The theoretical results as is provides little to no insight about the algorithm and unfortunately, authors do not discuss the insights from the theoretical results adequately in the paper.	weakness
2021-1448	See for eg R1's comments about this. <sep>	misc
2021-1448	2- Given that the theoretical results are not strong, the thoroughness in empirical evaluation is important and unfortunately the current empirical results is not convincing.	weakness
2021-1448	In particular, there are two main areas to improve: <sep> a) Based on the Appendix D, the choice of hyper-parameters seem to be made in an arbitrary way and all models are forced to use the same hyper-parameters.	weakness
2021-1448	This way, the choice of hyper-parameters could potentially favor one method over the other.	weakness
2021-1448	A more principled approach is to tune hyper-parameters separately for each method. <sep>	suggestion
2021-1448	b) It looks like the choice of #epochs has been made in an arbitrary way.	weakness
2021-1448	For all experiments, it would be much more informative to have a figure similar to the left panel of Fig.	suggestion
2021-1448	4 but with much more #epochs so that reader can clearly see if the benefit of SALR would disappear with longer training or not. <sep>	suggestion
2021-1448	c) Based on the current results, SALR's performance  is on par with that of Entropy-SGD on CIFAR-100 and WP and there is a very small gap between them on CIFAR-10 and PTB.	weakness
2021-1448	I highly recommend adding ImageNet results to make the empirical section stronger.	suggestion
2021-1448	The other option is to compare against other methods in fine-tuning tasks.	suggestion
2021-1448	That is, take a checkpoint of a trained model on ImageNet and compare SALR with other methods on several fine-tuning tasks. <sep>	suggestion
2021-1448	Given the above issues, my final recommendation is to reject the paper.	decision
2021-1448	I want to thank authors for engaging with reviewers during the discussion period and adding several empirical results to the revision.	rebuttal_process
2021-1448	I hope authors would address the above issues as well and resubmit their work.	decision

2021-1449	This paper investigates methods for gradient-based tuning of optimization hyperparameters.	abstract
2021-1449	This is an interesting area, and the paper isn't bad.	strength
2021-1449	The examination of hypervariance seems relatively novel and useful.	strength
2021-1449	I also appreciate the point about Bayesopt sometimes working well simply due to small ranges. <sep>	strength
2021-1449	However, I agree with the criticisms of the reviewers.	misc
2021-1449	Overall this paper isn't quite clear, thorough and impactful enough to make it in this round, but I think with more attention to baselines and scope this paper could be acceptable. <sep>	weakness
2021-1449	Some minor comments: <sep> The signed-based optimizer, while simple and sensible (which is good), seem kind of ad hoc. <sep> 	weakness
2021-1449	The authors don't seem to have properly scoped the problem and method, since greediness is only a major concern for inner optimization hyperparameters specifically.	weakness
2021-1449	It's not clear that for regularization parameters that this problem exists or that your method would apply.<sep>	weakness
2021-1449	A small nit:  Is hypervariance the right thing to look at, since the problem can exist even in deterministic settings?	weakness
2021-1449	Perhaps some sort of sensitivity analysis would be more appropriate.	suggestion
2021-1449	Also you should reference Barack Pearlmutter's thesis which first explores these issues.	suggestion
2021-1449	I would also mention that the hypervariance is generally tiny for smaller-than-optimal learning rates, and massive for larger-than-optimal learning rates, (the chaotic regime).	weakness

2021-1451	This paper introduces modifications that allow to make the training of contrastive-learning-based models practical.	abstract
2021-1451	The goal of the paper is very interesting, and the motivation clear.	strength
2021-1451	This paper tackles a very important issue with recent unsupervised feature learning methods. <sep>	strength
2021-1451	However, while the goal is great, the present submission does not provide time improvements on par with the ambitions of this work.	weakness
2021-1451	As noted by R2, many other hacks could be used in conjunction with the current work to scale this goal to the extreme, yielding time improvements which would be of a more impressive magnitude.	suggestion
2021-1451	In its current form, this paper unfortunately doesn't meet the bar of acceptance. <sep>	decision
2021-1451	Given the interesting scope of this work, I strongly encourage the authors to take the feedback from reviews and discussions into account and submit to another venue.	decision

2021-1452	We have a very well informed reviewer who strongly feels that this paper is insufficiently novel and significant further discussion on how the paper might be raised to a publishable level with more empirical results.	weakness
2021-1452	I will have to side with the more engaged reviewers who feel that the paper should be rejected.	decision

2021-1468	While the paper contains some interesting ideas, the reviewers felt that overall the paper is not theoretical well supported, and likewise the experiments are not fully convincing.	weakness
2021-1468	Even after the rebuttal, these concerns still persist.	rebuttal_process

2021-1476	This paper proposes a method to quantify transference, which is a measure of information transfer across tasks, for multi-task learning framework.	abstract
2021-1476	Specifically, the transference is measured as the change in the loss for a specific task after performing a gradient update for another.	abstract
2021-1476	The proposed transference measure is used to both understand the optimization dynamics of MTL and improve the MTL performance, either by grouping tasks or combining task gradients based on the transference.	abstract
2021-1476	The method is validated on multiple datasets and is shown to bring in some performance gains over the base MTL model (PCGrad, UW-MTL). <sep>	abstract
2021-1476	The majority of the reviewers were negative about this paper (4, 4, 5), while one reviewer gave it a positive rating (6).	rating_summary
2021-1476	The reviewers in general agreed that the idea of measuring transference as the change in the loss with gradient updates is novel and intuitive.	strength
2021-1476	Yet, the reviewers had common concerns on the 1) weak performance improvements, and the 2) high-cost of computing the transference.	weakness
2021-1476	While computing the transference requires additional computations with linear time complexity, which may be problematic with a large number of tasks, the performance gains using it were rather marginal (less than 0.5% over the baselines).	weakness
2021-1476	Another common concern from the reviewers was its insufficient experimental validation, as a comparative study against existing works that perform task grouping is missing.	weakness
2021-1476	Both the authors and reviewers actively participated in the interactive discussion.	rebuttal_process
2021-1476	However, the reviewers found that the two critical limitations persist even after the authors' feedback, and in a subsequent internal discussion, they reached a consensus that the paper is not yet ready for publication. <sep>	rating_summary
2021-1476	Thus, although the proposed method is novel and appears to be promising, it may need more developments to make it both more effective and efficient.	weakness
2021-1476	Moreover, there should be more in-depth analysis of its time-efficiency, and other benefits (eg interpretability) that could be achieved with the proposed transference measure.	weakness
2021-1476	Finally, while there exist many works on learning both hard or soft task grouping, the authors do not reference or compare against them.	weakness
2021-1476	To name a few, [Kang et al 11] propose how to learn the discrete task groupings, [Kumar and Daume III 12] propose to learn a soft grouping between tasks, [Lee et al 16] propose to learn soft grouping based on asymmetric knowledge transfer direction across the tasks, and [Lee et al 18] proposes the extension of [Lee et al 16] to a deep learning framework.	misc
2021-1476	I suggest the authors to discuss and compare against the above mentioned works, and fortify the related work section by searching for more classical works on multi-task learning.<sep>	suggestion
2021-1476	[Kang et al 11] Learning with Whom to Share in Multi-task Feature Learning, ICML 2011 <sep>	misc
2021-1476	[Kumar and Daume III 12] Learning Task Grouping and Overlap in Multi-task Learning, ICML 2012 <sep>	misc
2021-1476	[Lee et al 16] Asymmetric Multi-task Learning based on Task Relatedness and Confidence, ICML 2016 <sep>	misc
2021-1476	[Lee et al 18] Deep Asymmetric Multi-task Feature Learning, ICML 2018.	misc

2021-1507	In this paper, the authors proposed a geometric graph generator that applies a WGAN model for efficient geometric interpretation.	abstract
2021-1507	All the reviewers agree that the idea is interesting and the method has the potentials for graph generation tasks.	strength
2021-1507	Unfortunately, the experimental part is unsatisfying, which makes the paper on the borderline.	rating_summary
2021-1507	More analytic experiments should be designed to verify the properties of the proposed GG-GAN, especially its scalability.	suggestion
2021-1507	Although in the rebuttal phase the authors add a simple example to generate large but simple graphs, we would like to see more experiments and comparisons on more real-world large graphs (even if the performance may not be good, the results will be constructive for both readers and authors to understand the work).	rebuttal_process

2021-1509	The consensus view was that the reviewers were not convinced that the analysis done in the paper was sufficient motivated.	weakness

2021-1527	This paper explores a methodology for learning disentangled representations using a triplet loss to find subnetworks within a transformer.	abstract
2021-1527	The authors compare against several other methods and find that their method performs well without needing to train from scratch.	abstract
2021-1527	The reviewers thought this paper was well written and the authors were very responsive during the review period.	strength
2021-1527	However, there were some questions about the experimental setup and empirical performance of the paper, leaving the reviewers wondering if the performance was convincing.	weakness
2021-1527	We agree that there is value in exploring disentangled representations even if they do not necessarily improve performance (as the authors point out), but clearly explaining the reasoning behind all analyses (eg specifically choosing domains to introduce a spurious correlation), and justifying differences in performance is particularly important in these cases.	suggestion

2021-1530	This paper conducts a comparison between a small set of models (4 in total) for unsupervised learning.	abstract
2021-1530	Specifically, the authors focus on comparing Bayesian Confidence Propagating Neural Networks (BCPNN), Restricted Boltzmann Machines (RBM), a recent model by Krotov & Hopfield (2019) (KH), and auto-encoders (AE).	abstract
2021-1530	The authors compare trained weight distributions, receptive field structures, and linear classification on MNIST using the learned representations.	abstract
2021-1530	The first two comparisons are essentially qualitative comparisons, while on classification accuracy, the authors report similar accuracy levels across the models. <sep>	abstract
2021-1530	This paper received mixed reviews.	misc
2021-1530	Reviewers 4 and 5 felt it did not contribute enough for acceptance, while Reviewers 2 & 3 were more positive.	rating_summary
2021-1530	However, as noted by a few of the reviewers, this paper does not appear to achieve much, and provides very limited analysis and experiments on the models.	weakness
2021-1530	It isn't introducing any new models, nor does it make any clear distinctions between the models examined that would help the field to decide which directions to pursue.	weakness
2021-1530	The experiments add little insight into the differences between the models that could be used to inform new work.	weakness
2021-1530	Thus, the contribution provided here is very limited. <sep>	weakness
2021-1530	Moreover, the motivations in this paper are confused.	weakness
2021-1530	In general, it is important for researchers at the intersection of neuroscience and machine learning to decide what their goal is when building and or comparing models.	weakness
2021-1530	Specifically, is the goal: (1) finding a model that may potentially explain how the brain works, or (2) finding better machine learning tools? <sep>	weakness
2021-1530	If the goal is (1), the performance on benchmarks is less important.	weakness
2021-1530	However, clear links to experimental data, such that experimental predictions may be possible, are very important.	weakness
2021-1530	That's not to say that a model must be perfectly biologically realistic to be worthwhile, but it must have sufficient grounding in biology to be informative for neuroscience.	weakness
2021-1530	However, in this manuscript, as was noted by Reviewer 4, the links to biology are tenuous.	weakness
2021-1530	The principal claim for biological relevance for all the models considered seems to be that the update rules are local.	weakness
2021-1530	But, this is a loose connection at best.	weakness
2021-1530	There are many more models of unsupervised learning with far more physiological relevance that are not considered here (see eg Olshausen & Field, 1996, Nature; Zylberberg et al 2011, PLoS Computational Biology; George et al, 2020, bioRxiv: https://doi.org/10.1101/2020.09.09.290601).	weakness
2021-1530	It is true that some of these models use non-local information, but given the emerging evidence that locality is not actually even a strict property in real synaptic plasticity (see eg Gerstner et al, 2018, Frontiers in Neural Circuits; Williams & Holtmaat, 2018, Neuron; Banerjee et al, 2020, Nature), an obsession with rules that only use pre- and post-synaptic activity is not even clearly a desiderata for neuroscience. <sep>	weakness
2021-1530	If the goal is (2), then performance on benchmarks, and some comparison to the SotA, is absolutely critical.	weakness
2021-1530	Yet, this paper does none of this.	weakness
2021-1530	Indeed, the performance achieved with the four models considered here is, as noted by Reviewer 4, very poor.	weakness
2021-1530	In contrast, there have been numerous advances in unsupervised (or "self-supervised") learning in ML in recent years (eg Contrastive Predictive Coding, SimCLR, Bootstrap Your Own Latent, etc.	weakness
2021-1530	), all of which achieve far better results than the four models considered here.	weakness
2021-1530	Thus, the models being compared here cannot inform machine learning, as they do not appear to provide any technical advances.	weakness
2021-1530	Of course, some models may combine goals (1) & (2), eg seeking increased physiological relevance while also achieving decent benchmark performance (see eg Sacramento et al, 2018, NeurIPS), but that is not really the situation faced here, as the models considered have little biological plausibility (as noted above) and achieve poor performance at the same time. <sep>	weakness
2021-1530	Altogether, given these considerations, although this paper received mixed reviews, it is clearly not appropriate for acceptance at *CONF* in the Area Chair's opinion.	decision

2021-1537	This paper presents a secure aggregation method to ensure byzantine robustness.	abstract
2021-1537	The reviewers thought that the idea was interesting, but had the following concerns.<sep>	misc
2021-1537	Relaxing the assumptions used in the theoretical analysis as much as possible <sep>	weakness
2021-1537	Run more extensive experiments <sep>	weakness
2021-1537	I encourage the authors to their feedback into account when preparing the revised draft.	suggestion

2021-1538	This paper received 4 reviews with mixed initial ratings: 4, 8, 5, 7.	rating_summary
2021-1538	The main concerns of R1 and R2, who gave unfavorable scores, included limited methodological novelty beyond the data generation and insufficient empirical evaluation of state-of-the-art methods on the proposed dataset.	weakness
2021-1538	The authors submitted a new revision with a summary of changes and provided detailed responses to each of the reviews separately: it addressed some of the concerns, but did not change the overall position of the reviewers. <sep>	rebuttal_process
2021-1538	AC agrees with R3 and R4 that the proposed dataset and the environment may have certain practical impact and enable new research in learning CAD reconstruction.	strength
2021-1538	However, the contributions are indeed specific to a narrow CAD community, and R1 felt that the paper needs another round of peer reviews before acceptance, as a significant number of new results have been added during the discussion stage.	weakness
2021-1538	After discussion with PCs, the final recommendation is to reject.	decision

2021-1544	The paper proposes a DP method for generative modelling based on optimal transport.	abstract
2021-1544	The reviewers agree that the novelty is limited in relation to prior work, while the results are not especially compelling either.	weakness
2021-1544	So, even though this is a valid approach, correctness is not sufficient for acceptance at *CONF*.	decision

2021-1558	This paper explores the role of hyperparameters in the separate phases of a classic pruning pipeline: mask identification and retraining.	abstract
2021-1558	Key observations include a set of the hyperparameters to search relative to a standard regime as well as the identification that the layerwise pruning rates from mask finding are intertwined with these hyperparameters and are what chiefly affects the eventual performance of the pruned network. <sep>	abstract
2021-1558	The pros of this paper are that it works against the contemporary wisdom that the default hyperparameters for a model are the best for finding a mask for the model.	strength
2021-1558	Instead, there are improvements to be had by identifying a set of hyperparameters that lead to worse overall model accuracy, but better masks.	strength
2021-1558	Second, the work shows that the layerwise pruning rates are the key elements of these hyperparameters effect.	strength
2021-1558	The rates can in fact be transferred to more poorly performing network configurations and improve performance. <sep>	strength
2021-1558	The cons of this paper, as noted by the reviewers, are the somewhat unclear implications of the technique.	weakness
2021-1558	The added guidance on directions to improve hyperparameters is valuable but does not necessarily provide a cost-effect strategy to find these.	weakness
2021-1558	At its strongest, this guidance offers practitioners a recommendation to also consider hyperparameters for the initial model. <sep>	weakness
2021-1558	The stronger, forward-looking implication is, instead, the connection to layerwise pruning rates.	weakness
2021-1558	Specifically, while layerwise pruning rates have been demonstrated to be important in the literature (eg, [1]), there has been a limited study into the exact nature of a good set of pruning rates versus a bad set of pruning rates.	weakness
2021-1558	Where this paper stops short of a clear result, is if were to connect excessive pruning of the earlier layers, or simply the layerwise rates themselves to another property of the network (eg, gradient flow, or capacity) that indicates the improved eventual performance. <sep>	weakness
2021-1558	My Recommendation is to Reject.	decision
2021-1558	The paper's core experiments are well-executed.	strength
2021-1558	However, this final detail, closing the gap between the portability of these layerwise rates and a conceptual understanding, is a key missing component.	weakness
2021-1558	Once done, that will make for a very strong paper. <sep>	misc
2021-1558	[1] AMC: AutoML for Model Compression and Acceleration on Mobile Devices.	misc
2021-1558	Yihui He, Ji Lin, Zhijian Liu, Hanrui Wang, Li-Jia Li, Song Han. EECV, 2018	misc

2021-1568	While reviewers find the ideas in the paper interesting, they also raise several major concerns. <sep>	misc
2021-1568	In particular, R1 and R4 find the claims of "invertible" and "lossless" to be potentially misleading. <sep>	weakness
2021-1568	The bijective property is achieve on the first stage (L-1 layers) due to a sequence of one-to-one mappings, as is done in previous work (eg i-RevNet)  so the novelty is limited.	weakness
2021-1568	As stated by R3,  since the paper is a combination of previous methods, the writing should be substantially improved to clarify what the real, new contributions are.	weakness
2021-1568	The interpretation of the results (eg Figure 4) should also be better explained.	weakness

2021-1581	The paper studies a high-order discretization of the ODE corresponding to Nesterov's accelerated method, as introduced by Su-Boyd-Candes.	abstract
2021-1581	The main claim of the paper is that the more complex discretization scheme leads to a method that is more stable and faster.	abstract
2021-1581	However, the theoretical claims do not seem sufficiently supported, and the experimental results are insufficient to judge the usefulness of the proposed approach.	weakness
2021-1581	Thus, the reviews could not recommend acceptance, and I concur.	decision
2021-1581	The authors are advised to revise the paper to provide more theoretical and experimental evidence for usefulness/competitiveness of the proposed approach, and resubmit to a different venue.	suggestion

2021-1585	There were both positive and negative assessments of this paper by the reviewers: It was deemed a well written paper that explores cleanly rederiving the TC-VAE in the Wasserstein Autoencoder Framework and that has experiments comparing to competing approaches.	strength
2021-1585	However, there are two strong concerns with this paper: First, novelty appears to be strongly limited as it appears a rederivation using known approaches.	weakness
2021-1585	Second, two reviewers were not convinced by the experimental results and do not agree with the claim that the proposed approach is better than competing methods in providing disentangled representations.	weakness
2021-1585	I agree with this concern, in particular as assessing unsupervised disentanglement models is known to be very hard and easily leads to non-informative results (see eg the paper cited by the authors  from Locatello et al, 2019).	weakness
2021-1585	Overall, I recommend rejecting this paper.	decision

2021-1590	This paper proposes  a new mechanism, called HIRE, to  improve the down-stream performance of a pre-trained Transformer on NLP tasks.	abstract
2021-1590	Different from directly using the last layer of transformer, the proposed model allows the system to dynamically decide which intermediate layers to use based on the input through some sort of gating.	abstract
2021-1590	The model is evaluated on GLUE, a benchmark for natural language understanding.	abstract
2021-1590	My major concerns are the following<sep> the gating mechanism on using intermediate sentence representation is not new, as pointed by some reviewers, although its implementation on transformers is still interesting. <sep>	weakness
2021-1590	the empirical part is not convincing enough:  a) GLUE data set is relatively simple, the authors should try something more complex, b）the improvement over baseline is rather modest, which could be achieved with simpler modification.<sep>	weakness
2021-1590	I'd suggest to reject this paper.	decision

2021-1599	The reviewers, AC, and PCs participated in a very thorough discussion.	misc
2021-1599	AC ultimately felt that the work was unfinished, and in particular that details in the proofs still needed work before publication.	decision

2021-1603	This paper was reviewed by 4 reviewers who scored the paper below acceptance threshold even after the rebuttal.	rating_summary
2021-1603	Reviewer 4 is concerned about motivation, Reviewer 2 rightly points out that there exist numerous works that use some form of spectral layers in a deep setting on challenging datasets - something lacking in this work.	weakness
2021-1603	Reviewer 3 is concerned about limited discussion on lie groups and the overall benefit of expm(.).	weakness
2021-1603	Reviewer 1 reverberates the same comments regarding insufficient experiments,  comparisons and limited motivation.	weakness
2021-1603	We encourage authors to consider all pointers given by reviewers in any future re-submission.	suggestion

2021-1615	The paper investigates the average stability of kernel minimal norm interpolating predictors.	abstract
2021-1615	The main result establishes an upper bound on a particular notion of average stability for which it is well-known that it can be used to bound the generalization error.	abstract
2021-1615	This upper bound holds for all interpolating predictors from the RKHS, but it is minimized by the minimal norm predictor. <sep>	abstract
2021-1615	While at first glance this result looks highly interesting, a closer look reveals that the significance of the results crucially depends on the quality of the derived upper bound.	weakness
2021-1615	Here two reviewers raised concerns, since it is by no means clear that even the optimized upper bound produces meaningful bounds on the generalization performance.	weakness
2021-1615	The authors tried to address these concerns in their response and promised to update their paper accordingly.	rebuttal_process
2021-1615	As a result, they added a paragraph on page 8.	rebuttal_process
2021-1615	Unfortunately, this paragraph remains extremely vague, in particular if it comes to the more interesting case of non-linear kernels.	rebuttal_process
2021-1615	Here, the authors briefly refer to a paper by El Karoui but no details are given.	rebuttal_process
2021-1615	However, looking at El Karoui's paper it is anything but obvious whether the results of that paper lead to reasonable upper bounds on the average stability for a sufficiently general class of distributions. <sep>	rebuttal_process
2021-1615	As a result, I view the paper under review to be premature since it remains unclear if the observed optimality of the minimal norm solution is a real feature or just an artifact due to an upper bound that is simply too loose to make any conclusion.	decision

2021-1622	A line of work since 2016 has investigated learning NN-based optimisers, which produce optimisation updates by processing loss/gradient info with neural networks.	abstract
2021-1622	This paper tries to understand the learned dynamics of these NN-based optimisers by linear approximation to the learned non-linear dynamics.	abstract
2021-1622	Visualisation of these approximations are shown on 3 optimisation problems: linear regression, Rosenbrock function, and a toy neural network classification problem, with the hope of covering different types of objective landscapes. <sep>	abstract
2021-1622	Reviewers agreed that the paper studies an important research question, which would interest researchers working on meta-learning learning algorithms.	strength
2021-1622	However there are several major concerns raised by the reviewers: (1) the example optimisation problems are toyish, and (2) the paper does not explain very well the link between the visualised behaviour and the better optimisation results, ie it is unclear to the reviewers why the learned dynamics lead to better optimisation results. <sep>	weakness
2021-1622	While I am not too concerned about issue (1), I think issue (2) is a significant one, flagging that the clarity of the paper needs to be improved.	weakness
2021-1622	Ultimately, the paper is motivated by the question "How is a learned optimizer able to outperform a well tuned baseline?	weakness
2021-1622	", so a reader would expect some clear explanation towards answering this question.	weakness
2021-1622	Also some reviewers are concerned about the fact that only the RNN-based optimiser in Andrychowicz et al (2016) is analysed; since there exists other forms of learned optimisers, focusing on studying only one type of them might lead to early conclusions that are not so accurate.	weakness

2021-1624	The paper provides variance reduction techniques for GCN training.	abstract
2021-1624	When training a GCN it is common to sample nodes as in SGD, but also subsample the nodes' neighbors, due to computational reasons.	abstract
2021-1624	The entire mechanism introduces both bias and variance to the gradient estimation.	abstract
2021-1624	The authors decompose the gradient estimate into its variance and bias error, allowing them to apply more targeted variance (and bias) reduction techniques. <sep>	abstract
2021-1624	The results and improvement over existing GCN methods seem to be solid.	strength
2021-1624	The main weakness of the paper is its novelty.	weakness
2021-1624	As pointed out in the reviews the techniques seem to be quite close to papers [5],[11] (referring to the authors posted list). <sep>	weakness
2021-1624	It therefore boils down to the question of whether the authors simply applied existing techniques, achieving a better implementation than previous art, or did they develop a truly new algorithm that will encourage further research and deepen the understanding of GCNs.	weakness
2021-1624	Given the decisive opinions of reviewers 1 and 4, that remained after taking the response into account, I tend to believe that the improvement provided here is either too incremental or not stated in a crisp enough manner in order to be published in its current form	decision

2021-1627	The paper had three borderline reviews.	rating_summary
2021-1627	While the idea of posterior sampling of a neural network is potentially useful and Langevin dynamics are a way to attempt to address that, the reviewers did not appear convinced by the experiments and what the MCMC sampling was doing wasn't really front and center there.	weakness

2021-1643	The meta-reviewer agrees with the reviewers that this is a marginal case.	rating_summary
2021-1643	Conditioned on the quality of content and comparisons to other works: <sep>	misc
2021-1643	Constrained Reinforcement Learning With Learned Constraints (https://openreview.net/forum?id=akgiLNAkC7P) <sep>	misc
2021-1643	Parrot: Data-Driven Behavioral Priors for Reinforcement Learning (https://openreview.net/forum?id=Ysuv-WOFeKR) <sep>	misc
2021-1643	PERIL: Probabilistic Embeddings for hybrid Meta-Reinforcement and Imitation Learning (https://openreview.net/forum?id=BIIwfP55pp) <sep>	misc
2021-1643	We believe that the paper is not ready for publication yet.	decision
2021-1643	We would strongly encourage the authors to use the reviewers' feedback to improve the paper and resubmit to one of the upcoming conferences.	decision

2021-1654	The paper proposes a learning framework for Hypergraphs.	abstract
2021-1654	The proposed method can be viewed as generalisation of GraphSAGE to hyper graphs.	abstract
2021-1654	Though the paper emphasises that there is significant differences between Hypergraphs and Graphs and hence new methods are required.	weakness
2021-1654	However, the proposed methods are not significantly different than that used for Graphs.	weakness
2021-1654	Thus the novelty seems to be limited and hence it is difficult to strongly argue for acceptance.	decision

2021-1659	The consensus recommendation is that the paper is not ready for publication at this time.	rating_summary

2021-1670	The paper proposes a new measure of difference between two distributions using conditional transport.	abstract
2021-1670	The paper considers an important problem.	strength
2021-1670	However, some major concerns remain after the discussion among the reviewers.	rebuttal_process
2021-1670	In particular, the paper focuses on the evaluation on a toy dataset.	rebuttal_process
2021-1670	It is unclear whether the claim carries over to large real datasets.	weakness
2021-1670	The presentation of the paper also needs substantial improvement.	weakness

2021-1711	Summary: <sep> This paper introduces a method to try to learn in environments where a person specifies successful outcomes but there is no environmental reward signal. <sep>	abstract
2021-1711	I'd personally be interested in knowing where people were able to easily provide such successful outcomes instead of, for instance, providing demonstrations or reward feedback.	suggestion
2021-1711	Similarly, I'd be interested in how other methods of providing human prior knowledge compared. <sep>	suggestion
2021-1711	Discussion: <sep> Reviewers agreed the paper was interesting, but none of the 4 thought the paper should be accepted. <sep>	rating_summary
2021-1711	Recommendation: <sep> While I do not think this paper should be accepted in its current form, I hope the authors will find the comments and constructive criticism useful.	decision

2021-1719	The paper introduces an approach to counterfactual fairness based on data pre-processing, and compare it to other two counterfactual fairness approaches on the Adult and COMPAS datasets. <sep>	abstract
2021-1719	The reviewers are in agreement that, in its current state, the paper should not be accepted for publication at the venue.	rating_summary
2021-1719	Their main concerns are around the metric used to measure fairness, and these were not resolved during the discussion.	rebuttal_process
2021-1719	The reviewers would have also appreciated more experiments on real-world datasets to get a more comprehensive comparison of the methods.	suggestion
2021-1719	Finally,  discussion and comparison with other methods to achieve counterfactual fairness from the literature were limited.	weakness

2021-1721	The reviewers highly appreciated the replies and the additional experiments.	rebuttal_process
2021-1721	We also had a private discussion on the paper.	misc
2021-1721	To summarize: the replies alleviated quite a few concerns, however the consensus was that the paper still does not meet the bar for a highly competitive conference like *CONF*. <sep>	decision
2021-1721	The idea of combining MPC (on a 'wrong' model)  with a learned cost function is very interesting and a promising direction.	strength
2021-1721	On the downside the reviewers are still not entirely convinced about the contribution and believe that the paper requires a significant re-write to incorporate the discussed points as well as an additional round of reviews.	rebuttal_process

2021-1733	This paper is very pleasant to read.	strength
2021-1733	The reviewers also like the key idea discussed and find the targeted application interesting and practical.	strength
2021-1733	However, after reading the indeed interesting motivation, all four reviewers expected to see more from the evaluation section, including more challenging and realistic set-ups and clearer gains over standard methods.	weakness
2021-1733	The reviewers also discuss how both the navigation problem as well as the GP constraint problem have been tackled in the past, often in combination (eg reference [1] by R1).	weakness
2021-1733	Therefore, it would be needed to see additional experimental evaluation in line with those previous works.	suggestion

2021-1743	The paper studies the robustness of binary neural networks (BNNS), showing how quantized models suffer from gradient vanishing.	abstract
2021-1743	To solve this issue, the authors propose temperature scaling approaches that can overcome this masking, achieving near-perfect perfect success in crafting adversarial inputs for these models.	abstract
2021-1743	The problem is interesting and important.	strength
2021-1743	However, the major concerns are that the technical novelty is limited raised by two Reviewers, small improvements for linear loss functions.	weakness
2021-1743	The most related work is not compared in the experiment.	weakness

2021-1748	While the reviewer's noted a number of strengths of your paper, the approach that you took, and agreed that you had tackled an important problem, concerns remained about presentation and clarity.	weakness
2021-1748	I agree.	weakness
2021-1748	(Here are just a few miscellaneous comments: the very first paragraph of the Introduction needs to be rewritten for clarity, in my opinion.	suggestion
2021-1748	Later on page 1, you use the term "the dynamics of density" but you should not assume that the reader knows what that means.	weakness
2021-1748	There are typos as well, eg "make all predictions base[d] on Equation (6)" on page 4.	weakness
2021-1748	It would be helpful to know something about why you chose the experimental setups in Synthetic-1, 2, and 3. ) <sep>	suggestion
2021-1748	Regarding the similarities between this paper and a previously published article I believe that the authors have addressed these concerns; I hope they are careful to avoid this situation in the future.	suggestion

2021-1749	The paper proposes a new defense against adversarial attacks on graphs using a reweighting scheme based on Ricci-flow.	abstract
2021-1749	Reviewers highlighted that the paper introduces interesting ideas and that the use of Ricci-curvature/flow is a novel and promising contribution.	strength
2021-1749	Reviewers also recognized that the paper has significantly improved after rebuttal and clarified some aspects of their initial reviews. <sep>	rebuttal_process
2021-1749	However, there exist still concerns around the current version of the manuscript.	weakness
2021-1749	In particular, important aspects of the method and algorithm, as well as some design choices are currently unclear.	weakness
2021-1749	This includes evaluating and discussing robustness, training method, and practicality/improvements in real-world scenarios.	weakness
2021-1749	I agree with the majority of the reviewers that the current version requires an additional revision to iron out the aforementioned issues.	weakness
2021-1749	However, I also agree with the reviewers that the overall idea is promising and I'd encourage the authors to revise and resubmit their work with considering the feedback from this round of reviews.	decision

2021-1759	This paper proposes a transferable adversarial attack method for object detection by using the relevance map.	abstract
2021-1759	Four reviewers provided detailed reviews: 2 of them rated "Ok but not good enough - rejection", 1 rated "Marginally below" and 1 rated "Marginally above".	rating_summary
2021-1759	While reviewers consider the paper well written and using relevance map novel, a number of concerns are raised, including limited novelty, the lack of theoretical results, no use of the proposed dataset, insufficient ablation, etc.	weakness
2021-1759	During the rebuttal, the authors made efforts to response to all reviewers' comments.	rebuttal_process
2021-1759	However, the major concerns remain, and the rating were not changed.	rebuttal_process
2021-1759	The ACs concur these major concerns and agree that the paper can not be accepted at its current state.	decision

2021-1762	The focus of the paper is stochastic backpropagation for both continuous and discrete random variables.	abstract
2021-1762	By using standard results from Fourier analysis the authors rewrite the corresponding gradients in an infinite weighted sum form ((3) and (9)), extending the results of (Rezende et al 2014) and (Fellows.	abstract
2021-1762	et al, 2018).	abstract
2021-1762	The efficiency of the approach is illustrated in 2 toy examples. <sep>	abstract
2021-1762	As summarized by the reviewers, the problem tackled is interesting.	strength
2021-1762	However, they also pointed out that the novelty of the approach is quite limited and its practical usefulness is not clear (it should by demonstrated against state-of-the-art baselines, on realistic benchmarks).	weakness

2021-1765	The authors propose an RL-based approach, "Rewriting-by Generating (RBG)", to solve large-scale capacitated vehicle routing problems (CVRPs): such problems are NP-hard in general and are ubiquitous.	abstract
2021-1765	The RL agent consists of a "Generator" and "Rewriter".	abstract
2021-1765	In generation, the graph is sub-divided into several regions and in each region, an RL algorithm runs to get the best (or near-optimal) route.	abstract
2021-1765	The rewriter then patches these near-optimal sub-solutions together using "hierarchical RL". <sep>	abstract
2021-1765	The paper is generally well-written. <sep>	strength
2021-1765	One main concern is related to generalizability: the authors respond that their approach can work for other NP-hard combinatorial-optimization problems such as knapsack.	weakness
2021-1765	The authors are encouraged to do a systematic study of several such (related) problems where their approach can work.	suggestion
2021-1765	It was also a concern that the overall approach of partitioning the input instance and rewriting the CVRP solution by merging regions and recomputing routes, is also employed by commercial OR solvers.	weakness
2021-1765	The authors are encouraged to do a careful comparison (and perhaps melding) with such available solvers, to get a hybrid "OR + ML" improvement.	suggestion
2021-1765	It is also suggested that the authors include several different constraints from real-world VRP (eg, heterogeneous vehicle costs, costs of missed shipment, route limits, upper-bounded number of vehicles etc.).	suggestion

2021-1769	Reviewers generally appreciate the contributions of the paper, namely the horocycle neuron, Poisson neuron, and the universal approximation properties.	strength
2021-1769	However, there are concerns, especially by R4 and R5, that the presentation is confusing, lacks clarity, and should be substantially improved. <sep>	weakness
2021-1769	Note: Theorem 1.7 in (Helgason, 1970) is proved explicitly for the case n=2, not for general n as claimed in (9).	suggestion
2021-1769	Thus the Laplacian eigenspace motivation needs to be re-written/re-examined.	suggestion

2021-1770	A method is proposed for removing prior knowledge, presented as a distance matrix, from low-dimensional embeddings, to focus them on what is new. <sep>	abstract
2021-1770	The task of visualizing novely in data is interesting and good solutions would potentially be highly useful. <sep>	strength
2021-1770	The proposed method essentially substracts a distance matrix from another.	weakness
2021-1770	While this is sensible, it is not completely clear in what sense this is the right solution for what the embeddings will be used for. <sep>	weakness
2021-1770	In final discussions among the reviews, the main remaining concerns were considered severe: comparisons to other methods being limited, <sep>	rebuttal_process
2021-1770	and possible problems in one of the experiments.	rebuttal_process

2021-1771	This paper studies the role of "noise injection" in GANs with tools from Riemannian geometry, and derives a new noise injection approach that aims to learn a fuzzy coordinate system to model non-Euclidean geometry.	abstract
2021-1771	The new noise injection approach is shown to improve over StyleGANv2 noise injection on lower-resolution 128x128 FFHQ, LSUN, and 32x32 CIFAR-10 images. <sep>	abstract
2021-1771	Some reviewers found the experimental results a "considerable improvement on DCGAN and StyleGANv2" (R3), "extensive and convincing" (R2), while others had concerns around the experimental setup using lower resolution images (R1, R4).	weakness
2021-1771	While reviewers were mostly positive about the experimental wins of the paper, there was confusion (R3) and several concerns (R4) around the theory and the relationship between the theory and the practical noise injection algorithm.	weakness
2021-1771	I additionally had several concerns around the presentation and relation to prior work on generative models.	weakness
2021-1771	Thus in the current state I cannot recommend this paper for acceptance.	decision
2021-1771	Below I highlight concerns that should be addressed in future revisions.<sep>	misc
2021-1771	My biggest concern is the tremendous gap between the theoretical claims and the practical implementation.	weakness
2021-1771	When training a GAN with the new form of noise injection, does it learn the skeleton and fuzzy equivalence relationships you claim?	weakness
2021-1771	This paper is missing any kind of toy experimenting showing that training a GAN with fuzzy reparameterization discovers these relationships or coordinates.	weakness
2021-1771	Such an experiment would greatly strengthen the paper and help to answer the question of why this new method works (ie it's not just more parameters, a slightly better architecture, or better hyperprameters as mentioned by R3 and R4).	suggestion
2021-1771	There's also no discussion of what happens theoretically when you have multiple layers of fuzzy reparameterization, and the claims that StyleGAN2's noise injection limits to Euclidean geometry is false in this case (and thus StyleGAN2's noise injection can also overcome the "adversarial dimension trap").<sep>	weakness
2021-1771	Theoretical setting: As mentioned by R4, there is much prior work on the difficulties in fitting a lower-dimensional model manifold to a higher-dimensional data manifold (eg WGAN).	weakness
2021-1771	Theorem 1 highlights the impossibility of exactly fitting the data manifold with (smooth) neural networks, but the resulting solutions of increasing the dimensionality of the latent space is well-known and commonly used (eg StyleGAN).	weakness
2021-1771	This paper also doesn't discuss the alternative of approximately fitting the data manifold with a lower-dimensional structure, which is what is often studied in practice.<sep>	weakness
2021-1771	Clarity: The term "noise injection" is overloaded in the literature, and the current presentation of the paper does not sufficiently describe the method.	weakness
2021-1771	There's also no discussion of "instance noise" that is another solution to this problem that adds noise to inputs of the discriminator to yield finite f-divergences (Sonderby et al, 2016, Roth et al, 2017).	weakness
2021-1771	The work on instance noise is very related to the approach here, but only adds noise to the output of the generator, not at all levels. <sep>	weakness
2021-1771	There's also no discussion of how adding noise is just expanding the generative model with additional latent variables, a standard approach that is often discussed in the context of hierarchical generative models.	weakness
2021-1771	The authors mention the relation to reparameterization trick in VAEs, but argue it is doing something fundamentally different.	weakness
2021-1771	However, modern VAE architectures (IAF-VAE, Very Deep VAE), use a very similar form of modulation at multiple levels in the hierarchy.<sep>	weakness
2021-1771	Experiments: There are no error bars in experimental results, and many results are presented in a new experimental setting defined by the authors (lower resolution than prior work even if using prior code).	weakness
2021-1771	Rerunning experiments in more standard settings on full resolution images would greatly improve the confidence that the new noise injection strategy is effective.	suggestion

2021-1772	This work develops a weight-quantization method for deep neural networks that is suitable for a type of analog hardware system known as crossbar-enabled analog computing-in-memory (CACIM).	abstract
2021-1772	The goal of this work is to train models on GPUs in such a way that they retain their predictive accuracy during inference when deployed on the analogue hardware system. <sep>	abstract
2021-1772	Pros: <sep> Good adaptation of quantization methods to the CACIM system <sep> 	strength
2021-1772	Simple method <sep>	strength
2021-1772	Validation of the proposed method on multiple datasets and models<sep>	strength
2021-1772	Cons: <sep> Lack of novelty: the proposed method is a simple combination of two popular methods, LLoyd's quantization and noise-aware training <sep> 	weakness
2021-1772	All reviewers appreciate the simplicity of the method and the good fit to the hardware.	strength
2021-1772	The authors responded to all reviews and two reviewers acknowledged the authors' response.	rebuttal_process
2021-1772	The authors acknowledge some reviewer observations (motivation of quantization as reducing analogue noise, lack of experiments on the actual CACIM system), and the authors added an experimental evaluation on the actual physical CACIM system showing that their method performs well. <sep>	rebuttal_process
2021-1772	Overall the work is well-executed and the proposed method is a good fit to the CACIM system.	strength
2021-1772	However, the proposed quantization method is a straightforward adaptation of popular quantization methods.	weakness

2021-1774	We thank the authors for their detailed responses to reviewers, and for engaging in a constructive discussions. <sep>	misc
2021-1774	As explained by the reviewers, the paper is clearly written and the method is novel.	strength
2021-1774	However, the novelty is to combine existing ideas and techniques to define an objective function that allows to incorporate cluster assignment constraints, which was considered incremental.	weakness
2021-1774	Regarding quality, the discussion highlighted some possible improvements that the authors propose to do in a future version of the paper, and we encourage them to follow that direction.	rebuttal_process
2021-1774	Regarding significance, although the experimental results are promising there were some concerns that the improvement over existing techniques is marginal, and that more experiments leading to a clearer message would be useful. <sep>	weakness
2021-1774	In summary, this is not a bad paper, but it is below the standards of *CONF* in its current form.	decision

2021-1775	The paper's initial evaluation was below par, but the author feedback helped clarify several crucial points after which two of the reviewers increased their scores by a point, bringing the current evaluation to borderline. <sep>	rating_summary
2021-1775	The paper addresses a relevant and challenging problem in the RL domain.	abstract
2021-1775	However, in my opinion, from the reviewers' and authors' remarks and from my own reading of the paper, there are concerns that need to be addressed before the paper can be publication worthy.	decision
2021-1775	Primary among these is the quantum of novelty -- as many reviews point out, the key idea of viewing an episodic trajectory as a multivariate (vector) sample for running hypothesis tests is not novel in itself, as is the claim that new tests have been devised.	weakness
2021-1775	Another crucial issue is the (parametric) assumption of normality for the episodic reward sequence which is not adequately justified in the paper -- even a two time-step trajectory with normal rewards per state transition can exhibit a mixture-of-Gaussians type reward distribution for the second state, breaking the assumption.	weakness
2021-1775	As it transpired from the reviews of Reviewer4, reducing environment shift/degradation to just a mean change problem, without even considering a change in the variances (2nd order statistics), seems to be too stylized to be effective.	weakness
2021-1775	There are other, nonparametric approaches in statistics based on testing for changes in the distribution function (kernel density estimation approaches, for instance), which could perhaps be applied without normality assumptions and yield favourable results.	suggestion
2021-1775	The experimental results for detection delay often show significant overlaps of the delay distributions for different procedures (eg, Hotelling vs.	weakness
2021-1775	Mean vs. UDT etc.	weakness
2021-1775	), which does not indicate an advantage of the proposed method. <sep>	weakness
2021-1775	I would urge the author(s) to assimilate the feedback and delve deeper as to why and how parametric procedures based on normality assumptions may or may not succeed, so as to significantly strengthen the theoretical and practical message of this work.	suggestion

2021-1787	The paper studies the effectiveness of few-shot learning techniques in settings where the training labels are imbalanced.	abstract
2021-1787	While addressing an interesting practical problem, reviewers raised concerns about the paper's technical depth, insufficient distinction to existing techniques for coping with label imbalance, and limited qualitative conclusions from the results.	weakness
2021-1787	The authors incorporated some of these comments in their revision, but a more comprehensive update on the latter two points appears appropriate.	rebuttal_process

2021-1793	Overview: <sep> This paper introduces a maximum mutual information method for helping to coordinate RL agents without communication. <sep>	abstract
2021-1793	Discussion: <sep> Some reviewers leaned towards accept, but I found the two reviewers recommending rejecting to be more convincing. <sep>	rating_summary
2021-1793	Recommendation: <sep> This is an important research topic and I'm glad this paper is focusing on the problem.	strength
2021-1793	Hopefully the reviews will help improve a future version of this paper.	misc
2021-1793	I agree that this is a new way of using mutual information, but it seems more like a small improvement rather than a very significant step forward. <sep>	weakness
2021-1793	In addition, I think the setting needs to be better motivated.	weakness
2021-1793	This is a centralized training with decentralized execution (CTDE) setting, and this paper helps the agents coordinate.	weakness
2021-1793	In CTDE, the agents work in the environment and then pool their information to train before deploying on the next episode.	weakness
2021-1793	I don't understand why, eg, in multiwalker, agents would not be able to communicate while walking, can communicate after they succeed or drop the object (the episode ends), and then cannot communicate once the next episode starts.	weakness

2021-1794	This paper begins to formalize a connection between value decomposition and difference rewards.	abstract
2021-1794	Whilst we are in agreement with the authors that papers do not need to make new algorithmic contribution and purely theoretical papers that deepen our understanding of established methods can be significant contributions, all reviewers had doubts on the maturity of the theoretical contribution of this paper. <sep>	weakness
2021-1794	Given the concerns raised by the authors for the attention of the area chair, I would like to reassure the authors that the majority of reviewers engaged in discussion after the rebuttal but remained unconvinced of the significance of the theoretical results.	rebuttal_process
2021-1794	As these are representative of the potential audience at *CONF*, it is clear further improvements to the motivation given in the paper and/or weakening of the assumptions within the theory are needed to engage the interest of the wider machine learning community. <sep>	weakness
2021-1794	The empirical studies in the paper also seem disconnected from the theoretical contribution and more like a continuation of the paper "Qplex: Duplex dueling multi-agent q-learning."	weakness
2021-1794	Given the theoretical connection to difference rewards (eg COMA as explicitly noted by the authors in Implication 1) I would expect these methods to be included in the experiments to demonstrate how this theoretical connection affects performance in practical applications.	suggestion

2021-1797	This paper present novel formulations to address the problem of unbalanced Gromov.	abstract
2021-1797	The Conic formulation is very interesting but stays theoretical until optimization algorithms are available.	weakness
2021-1797	The Unbalanced Gromov is a nice extension of Gromov and comes with relatively efficient solvers.	strength
2021-1797	Some very limited numerical experiment show the proposed UGW used between 2D distributions (two moons) and graphs. <sep>	weakness
2021-1797	The paper had some mixed reviews with reviewers acknowledging the novelty of the approach (albeit an extension similar to unbalanced OT) and of the theoretical results.	strength
2021-1797	The detailed a very well written response to the reviewers comment has been appreciated.	rebuttal_process
2021-1797	But all reviewers also noted a lack of numerical experiments outside of the very simple illustrations in the paper.	weakness
2021-1797	This paper is a very nice contribution to the theory of optimal transport but fails at illustrating its relevance to the ML community.	weakness
2021-1797	Despite acknowledging the theoretical contributions of the paper, the  AC recommends a reject but strongly encourages the authors to complete the experimental section with some ML applications or at least proof of concepts (graph classification, domain adaptation,..).	decision

2021-1807	The paper studies an interesting problem motivated by VLSI design.	abstract
2021-1807	The reviewers agree that there are interesting aspects of the RC algorithm.	strength
2021-1807	Nevertheless, the paper could be improved by a clearer characterization/apples-to-apples comparison to baselines, particularly regarding computation cost, use of parallelism, as well as a more thorough contrast to state of the art in general.	weakness
2021-1807	Given the contribution is experimental, and this is a well studied problem, it is important to establish whether the solution is indeed best-in-class; cost due to training should be taken into account, and minimized to the extend possible.	suggestion
2021-1807	Going beyond the baselines considered here, as well as reviewing possible theoretical connections to other problems and guarantees, would also strengthen the paper.	suggestion

2021-1814	The paper discusses the problem of how to augment cross-modal retrieval for the task of multi-modal classification -- it uses image caption pairs to improve downstream multimodal learning, and shows improvement in the task of visual question answering.	abstract
2021-1814	However, the paper has the following weaknesses: (a) lack of novelty, (b) lack of thorough empirical evaluation, (c) the complex model did not give significant gains.	weakness

2021-1836	The paper presents a method for meta-learning the loss function.	abstract
2021-1836	The analysis mainly concerns the recently proposed TaylorGLO method on the (slightly less recent) Baikal loss.	abstract
2021-1836	There was no consensus on this paper, but no reviewer was willing to fight for acceptance either.	rating_summary
2021-1836	I found the paper not self-contained, with important non-standard elements undefined, starting with the Baikal loss, notations that are not defined in the main text, and a nomenclature that is also unusual with important terms such as "attractor" or "invariant" used in meanings that are non-standard in optimization or machine learning. <sep>	weakness
2021-1836	Regarding content, most of the analyses refer to properties of the Baikal loss (not presented in the main text) that are deemed to be positive, without any theoretical support (Theorems 1 and 2).	weakness
2021-1836	The inability to overfit is here posed as an obvious quality of a training loss.	weakness
2021-1836	Then, a way to prevent the failure of the meta-training algorithm is presented in Theorem 3.	weakness
2021-1836	Finally, an experiment is provided, showing that the proposed meta-training algorithm performs better than "vanilla" training with respect to adversarial attacks with FGSM.	weakness
2021-1836	There is no comparison with other defense mechanisms and no analysis explains the results.	weakness
2021-1836	Overall, although some interesting aspects may be developedin this paper, they are currently not well served by writing or the experimental evidences, so I recommend rejection.	decision

2021-1853	This paper clearly has great ideas and reviewers appreciated that.	strength
2021-1853	However, the lack of experiments that can be validated by the community (only 1 experiment on the proprietary dataset) is an issue.	weakness
2021-1853	We don't know if the reported accuracy is a respectable one (in the public domain).	weakness
2021-1853	Having a proprietary dataset is a plus, but no public benchmark raises concerns about reproducibility. <sep>	weakness
2021-1853	We recommend the authors to add some tasks and benchmarks for the community to check for themselves that the numbers reported are non-trivial.	suggestion

2021-1893	The paper studies the problem of identifying what information to forget in attention mechanisms, with the goal of enabling attention mechanisms to deal with longer contexts.	abstract
2021-1893	This is a simple yet intuitive extension:  self-attention is augmented with an expiration value  prediction.	abstract
2021-1893	Experiments were carried out on NLP and RL tasks. <sep>	abstract
2021-1893	Overall, the paper has novelty in the proposed idea, however, there are concerns about the strength of the experiments; that the experiments fall short.	weakness

2021-1896	There was quite a bit of internal discussion on this paper.	misc
2021-1896	To summarize: <sep> The idea is very neat and interesting and likely to work <sep> 	strength
2021-1896	The paper is likely to inspire future work <sep>	strength
2021-1896	There are still serious doubts  about the experimental evaluation that is not entirely up to par with current standards <sep>	weakness
2021-1896	The reviewers were not convinced 100% by the arguments about the 'custom' environments <sep>	weakness
2021-1896	The reviewers were not convinced 100% that the baselines were given their best shot<sep>	weakness
2021-1896	While the paper has potential to provide valuable input for the community, it needs a bit more work before being presentable at a highly competitive venue like *CONF*.	decision

2021-1909	This paper presents a method for relational inference in multi-agent/multi-object trajectory prediction tasks.	abstract
2021-1909	Different from the neural relational inference (NRI) model [1], the presented method is able to model time-varying relations.	abstract
2021-1909	Experimental results on physics simulations and sports games (basketball) show benefits over variants of the NRI model. <sep>	abstract
2021-1909	The reviewers agree that the presented method is mostly solid, that the experiments are insightful, and that this is generally a well-written paper.	strength
2021-1909	The authors, however, have apparently overlooked recent related work [2] (dNRI) that proposes a very similar model.	weakness
2021-1909	In the light of dNRI, it is difficult to argue for the novelty of the presented approach, and the paper needs to undergo a revision in order to more clearly differentiate it from the dNRI model, and to resolve the other concerns raised by the reviewers. <sep>	weakness
2021-1909	[1] Kipf et al, Neural Relational Inference for Interacting Systems (ICML 2018) <sep>	misc
2021-1909	[2] Graber et al, Dynamic Neural Relational Inference (CVPR 2020)	misc

2021-1917	Although the reviewers found the paper well-written that analyzes a relatively popular algorithm (TD(0) version of A3C), there are concerns regarding the novelty of the convergence results given those for A2C, the comparison of the results with those for A2C, and the sufficiency of the experiments.	weakness
2021-1917	Although the authors addressed some of these issues/comments during the rebuttals, it seems none of the reviewers is excited about the paper and there still exist concerns regarding the novelty of the results and how they are compared with those in the literature.	rebuttal_process
2021-1917	I would suggest that the authors take the reviewers' comments into account, have a more comprehensive discussion about the relation of their results with those in the literature (two-time scale algorithms), and prepare their work for future conferences.	suggestion

2021-1933	This paper introduces a dataset and a trained evaluation metric for evaluating discourse phenomena for MT.	abstract
2021-1933	Several context-aware MT models are compared against a sentence level baseline.	abstract
2021-1933	The paper develops metrics which evaluate the models according to their performance on four discourse phenomena: anaphora, lexical consistency, coherence and readability, and discourse connective translation.	abstract
2021-1933	Data is released for three language pairs (all using English as the target language). <sep>	abstract
2021-1933	First, I'd like to point out that creating datasets and benchmarks for analyzing/evaluating discourse-level errors in machine translation is an extremely valuable contribution.	strength
2021-1933	This paper is addressing a very relevant problem and even though there is no new model/method/algorithm being proposed, this work fits this conference - it is my opinion that the community should welcome and value more than it currently does the efforts spent in creating high quality datasets that can help make progress in the field. <sep>	strength
2021-1933	There was substantial discussion among reviewers about this paper. <sep>	misc
2021-1933	The main weaknesses raised by the reviewers were: <sep> Limited information about the process to create the anaphora test, which was a contribution of prior work (Jwalapuram et al (2019) - this was addressed in the updated version; but the anaphora challenge sets seem to be only a minor update over previous work. <sep> 	weakness
2021-1933	All language pairs use English as the target language, and it is not simple to extend this approach beyond English target languages. <sep>	weakness
2021-1933	Lack of detail on how BLEU scores were computed (tokenised?	weakness
2021-1933	true cased?	weakness
2021-1933	My recommendation is to use sacrebleu) - this was clarified in the rebuttal. <sep>	rebuttal_process
2021-1933	The evaluated NMT models all date from 2018 or earlier. <sep>	weakness
2021-1933	Two of the 4 benchmarks (anaphora and coherence) are evaluated by neural models trained on WMT outputs, which makes the interpretation of scores is opaque, and their validity is unclear.<sep>	weakness
2021-1933	While the creation of a benchmark for discourse evaluation of MT is a laudable effort as mentioned above, it is my opinion that due to some of the weaknesses above the current version of this work is not yet ready for publication.	decision
2021-1933	However, I strongly encourage the authors to improve upon these points and resubmit their work to another venue.	decision
2021-1933	I list some suggestions below to improve this paper. <sep>	misc
2021-1933	My biggest concern with the current version is the last weakness above.	weakness
2021-1933	As pointed out by a reviewer, the framework of Jwalapuram et al (2019) provides empirical support for the model's sensitivity (if there is a pronoun error, does the metric pick it up?).	weakness
2021-1933	But they don't necessarily capture model specificity (if the metric ranks one output higher, can we be confident that this is because of a pronoun translation error?).	weakness
2021-1933	For the coherence metric, authors make an argument that their metric is sensitive to coherence issue, but concerns remain about whether it is sufficiently specific to these issues.	weakness
2021-1933	In the rebuttal, authors argue that BLEURT is sentence-level, but they could easily aggregate sentence-level judgments and report correlation between BLEURT and human coherence  judgments to show that their metric correlates better with human coherence judgments than BLEURT or even just BLEU.	rebuttal_process
2021-1933	Besides BLEURT, I would add there are other recently proposed metrics that may capture discourse phenomena (neural metrics trained against MQM annotations or sentence-level human assessments with document context) and should be compared against: check COMET [1] or PRISM [2] (the latter is sentence-based but could be adapted for paragraphs or documents). <sep>	suggestion
2021-1933	There is also prior work comparing various context-aware machine translation approaches against a sentence-level baseline, some with negative findings [3,4,5].	suggestion
2021-1933	I suggest the authors look at this related work in future iterations of their paper. <sep>	suggestion
2021-1933	[1] https://arxiv.org/pdf/2009.09025. pdf <sep>	misc
2021-1933	[2] https://arxiv.org/pdf/2004.14564. pdf <sep>	misc
2021-1933	[3] https://www.aclweb.org/anthology/2020. eamt-1.24. pdf <sep>	misc
2021-1933	[4] https://arxiv.org/pdf/1910.00294. pdf <sep>	misc
2021-1933	[5] https://www.aclweb.org/anthology/2020. emnlp-main.81. pdf	misc

2021-1935	All three reviewers expressed consistent concerns on this submission in their reviews.	misc
2021-1935	In addition, none of them enthusiastically supported this work during discussion.	rating_summary
2021-1935	It is clear this submission does not make the bar of *CONF*.	decision
2021-1935	Thus a reject is recommended.	decision

2021-1937	The paper proposes an MLP based approach for data without known structure (such as tabular data).	abstract
2021-1937	At first, the data are partitioned into K blocks in a differentiable way, then the standard MLP is applied to each block.	abstract
2021-1937	The results are then aggregated recursively to produce the final output. <sep>	abstract
2021-1937	Pros: <sep> Handling less structured data is surely an important problem in machine learning and is much less explored. <sep> 	strength
2021-1937	The paper is well written, easily understandable even with a fast browsing. <sep>	strength
2021-1937	The experimental results show some improvement.<sep>	strength
2021-1937	Cons: <sep> The approach is somewhat trivial, and the framework could be improved, see, eg Reviewers #3&#4. <sep> 	weakness
2021-1937	By the structure of the approach and the type of target data, a more reasonable comparison is with random forest (echoing Reviewer #1), which the authors added during rebuttal, rather than MLP etc.	weakness
2021-1937	Maybe should even compare with deep random forest.	weakness
2021-1937	Although the comparison with MLP etc.	weakness
2021-1937	is quite favorable, the advantage over random forest is somewhat marginal (except on HAPT, which is a imagery data set and random forest may not be good at; also echoing Reviewers #1&#4's comment on why using imagery data, which do not fit the theme of the paper).	weakness
2021-1937	Reviewers #3&#4 also had some concerns with the experiments.	weakness
2021-1937	Reviewer #4 confirmed in the confidential comment that the performance improvement is incremental.<sep>	weakness
2021-1937	Although the rebuttal seemed to be successful, thus both Reviewers #1 and #4 raised their scores, the average score is still at the borderline.	rating_summary
2021-1937	Due to the limited acceptance rate, the area chair has to reject the paper.	decision

2021-1957	The paper proposes to speed-up self-supervised learning for semi-supervised learning by combining self-supervised pretraining and supervised fine-tuning into a single objective.	abstract
2021-1957	The proposed supervised loss builds on Neighbourhood Components Analysis and soft nearest neighbor losses.	abstract
2021-1957	Most reviewers are concerned about the novelty of the approach and the significance of empirical results.	weakness
2021-1957	I agree with both concerns.	weakness
2021-1957	I appreciate the comparison between log⁡∑exp and ∑log⁡exp, but it seems a simpler cross entropy loss also achieves a similar goal (potentially somewhat slower).	weakness
2021-1957	I believe adding more experiments comparing different supervised loss functions across different architectures can help improve the paper.	suggestion

2021-1966	This paper proposes to automatically determine when the SGD step-size should be decreased, by running two "threads" of SGD for a bunch of iterations, divide those into windows, and then look at the average inner-product of the gradients in the two threads in each window.	abstract
2021-1966	If the inner-product tends to be high, that indicates that there is still "signal" in the gradient and it should not be decreased.	abstract
2021-1966	If it is low, that indicates that the gradient is mostly "noise".	abstract
2021-1966	In the latter case, the learning rate is decreased by a factor of gamma and the length of the next phase is increased by gamma. <sep>	abstract
2021-1966	Theorem 3.1 essentially assumes smoothness, a bounded fourth moment for the stochastic gradient, and that the stochastic gradient error is not too far from isotropic.	abstract
2021-1966	Then it shows that if the step-size is set small enough, the standard deviation of the diagnostic (Q_i) can be upper-bounded in terms of the expected value of Q_i.	abstract
2021-1966	It follows that the probability of Q_i being negative cannot be too large (bounded in terms of the step size eta and the length of the windows l). <sep>	abstract
2021-1966	Theorem 3.2 adds the assumption of strong convexity and weakens the assumption on the gradient to a bounded second moment.	abstract
2021-1966	Then it upper-bounds the expected value of the diagnostic in terms of its standard deviation. <sep>	abstract
2021-1966	Proposition 3.4 gives a proof of convergence.	abstract
2021-1966	As far as I can tell the proof is essentially that the learning rate decay can't be much worse than what would happen if the diagnostic always set to decrease.	abstract
2021-1966	In particular: (1) It's impossible for the learning rate to decay too quickly, since the length of each phase is increased by gamma whenever the learning rate is decreased by gamma.	abstract
2021-1966	(This is a "non-adaptive result.)	abstract
2021-1966	(2) The learning rate will eventually decay with probability 1. <sep>	abstract
2021-1966	Various concerns were brought up by the reviewers.	misc
2021-1966	Perhaps the most strongly voiced concern was that the proposed method is a heuristic rather than a method with a rigorous guarantee.	weakness
2021-1966	For my part I am in agreement with the authors and other reviewers that heuristic methods for decreasing the learning rate are worthy of study given the large practical importance of this problem. <sep>	ac_disagreement
2021-1966	I concur with the concern raised by some reviewers that the theoretical component of the paper may not have little explanatory value for the results that are given.	weakness
2021-1966	The assumption of strong convexity is not a major concern to me.	ac_disagreement
2021-1966	(Though not true it can still give intuition.)	ac_disagreement
2021-1966	More concerning is that theory essentially takes a fixed step-size scheme (repeatedly decrease the step size by gamma and increasing the length of a phase by gamma) and then shows that the diagnostic can't be too much worse.	weakness
2021-1966	This isn't in keeping with the motivation of being adaptive. <sep>	weakness
2021-1966	The reviewers were also concerned about the explanation of better results due to less overfitting.	weakness
2021-1966	This may be true, but the theory makes no mention of overfitting. <sep>	weakness
2021-1966	There was a consensus that the experimental results were promising, though some minor issues were raised. <sep>	strength
2021-1966	While the direction explored in the paper has value, there are enough open questions about the relationship of the theory to the experimental results to warrant another round of review. <sep>	weakness
2021-1966	Small thoughts, not significant to acceptance: <sep> The current heuristic runs two separate threads and looks at the inner-product of those gradients.	abstract
2021-1966	An alternative to this would be to run a single thread along with a "ghost" thread that computes a different gradient at each iteration.	suggestion
2021-1966	It would be great to comment on the difference and why one might be superior to the other.	suggestion
2021-1966	A more radical alternative would be to run a single thread, but then compute the diagnostic on each half of the minibatch.	suggestion
2021-1966	A more radical alternative still would be to analytically do that splitting many times and average the results.	suggestion
2021-1966	This seems like it might simultaneously reduce the variance of the diagnostic and also reduce the computational cost.<sep>	suggestion
2021-1966	The current heuristic runs two threads.	abstract
2021-1966	Is there a tradeoff if you run more?<sep>	suggestion
2021-1966	The statement of theorems could be more user-friendly.	suggestion
2021-1966	To understand Thm 3.1, I needed to search o find the definitions of: eta, l, i, w, Q_i.	suggestion
2021-1966	With a small amount of effort this could be re-written to remind the reader that w is the number of windows, l is the length, eta is the stepsize, etc.	suggestion
2021-1966	It is particularly unfortunate that sd() is never formally defined (only by reading the appendix did I discover that this was the standard deviation.)<sep>	suggestion
2021-1966	The fact that the length of threads is always increased by a factor of gamma whenever the step size is reduced by gamma seems contrary to the spirit of the proposed diagnostic.	suggestion
2021-1966	After all, this "bakes in" a kind of "fastest possible" decay schedule.	suggestion
2021-1966	If the diagnostic were fully reliable, shouldn't this not be necessary?	suggestion
2021-1966	The decision to add this doe not get nearly enough discussion in the paper in my view.<sep>	suggestion
2021-1966	I think it might be clearer to re-state theorem 3.1 including the Chebyshev result after it.	suggestion

2021-1972	The paper is about an approach that combines successor representation with marginalized importance sampling. <sep>	abstract
2021-1972	Although the reviewers acknowledge that the paper has some merits (interesting idea, good discussion, extensive experimental analysis) and the authors' responses have solved most of the reviewers' issues, the paper is borderline and the reviewers did not reach a consensus about its acceptance.	rating_summary
2021-1972	In particular, the reviewers feel that the contributions of this paper are not significant enough. <sep>	weakness
2021-1972	I encourage the authors to modify their paper by taking into consideration the suggestions provided by the reviewers and try to submit it to one of the forthcoming machine learning conferences.	decision

2021-1984	The paper presents a new Bayesian optimization method based on the Gaussian process bandits framework for black-box adversarial attacks.	abstract
2021-1984	The method achieves good performance in the experiments, which was appreciated by all the reviewers. <sep>	strength
2021-1984	At the same time, the presentation of the method is quite confusing, which currently precludes acceptance of the paper.	decision
2021-1984	In particular, during the discussion phase the reviewers were not able to decipher the algorithm based on the description presented in the paper.	weakness
2021-1984	It is not clear how the problem is modeled as a bandit problem, what the loss function ℓ is minimized and why minimizing it makes sense (assuming, eg, that ℓ it the hinge loss as suggested and the initial prediction is good with a large margin, that is, the loss is zero, equation 6 never changes xt when the procedure is started from x).	weakness
2021-1984	This connection, since it is the fundamental contribution of the paper, should be much better explained.	weakness
2021-1984	Once the problem is set up to estimate (maximize?)	weakness
2021-1984	the reward, it is changed to calculating the difference in the minimization (cf.	weakness
2021-1984	equation 11), which is again unmotivated.	weakness
2021-1984	(Other standard aspects of the algorithm should also be explained properly, eg, the stopping condition of Algorithm 1) <sep>	weakness
2021-1984	Unfortunately, the paper is written in a mathematically very imprecise manner.	weakness
2021-1984	As an example, consider equation (6), where Bp and the projection operator are not defined, and while these can be guessed, a projection of the argmin seems to be missing as well in the end (otherwise nothing guarantees that xT, which is the final outcome of the algorithm, remains in the Lp ball).	weakness
2021-1984	Another example is the Discrete Approximate CorrAttackFlip paragraph which requires that every coordinate of x should be changed by  ±ϵ.	weakness
2021-1984	It is also not clear what "dividing the image into several blocks" means in Section 4.1 (eg, are these overlapping, do they cover the whole image, etc., not to mention that previously x was a general input, not necessarily an image).	weakness
2021-1984	It is also unlikely that the stopping condition in Algorithm 1 would use the exact same ϵ for the acquisition function as the perturbation radius for adversarial examples, etc.	weakness
2021-1984	While some of these inaccuracies and unclear definitions are also mentioned in the reviews, unfortunately there are more in the paper. <sep>	weakness
2021-1984	The authors are encouraged to resubmit the paper to the next venue after significantly improving and cleaning up the presentation.	decision

2021-1995	The paper investigates the relationship between data augmentations used during training and their effect on the accuracy when evaluated on unseen corruptions at test-time.	abstract
2021-1995	The paper proposes a metric called minimal sample distance (MSD) to measure the similarity between augmentations during training time and corruptions at test time. <sep>	abstract
2021-1995	The reviewers agree that the paper aims to solve an important problem and the paper has some interesting findings.	strength
2021-1995	However, the current version has a few shortcomings: <sep> Some of the claims about "overfitting" are confusing, especially for data augmentations that use ops similar to those in ImageNet-C.	weakness
2021-1995	This is already known and which is why some papers uses a subset of operations (eg AugMix uses a subset of AutoAugment operations). <sep>	weakness
2021-1995	The main take-home message and novelty is unclear: The initial version titled ("Is Robustness Robust?")	weakness
2021-1995	seemed to argue that we may be overfitting to Imagenet-C, but the rebuttal and the updated version revised some of the claims (see response to R3 and R4).	rebuttal_process
2021-1995	In light of the revision, I'm not sure how the main take-home messages differ from existing papers such as Yin et al 2019 or "Many faces of robustness". <sep>	rebuttal_process
2021-1995	One of the main differences is quantification of the distribution similarity, however, as pointed out by R2, this analysis does not explain when stylized corruptions would help, so the current version of the paper feels a bit incomplete to me.<sep>	weakness
2021-1995	I recommend the authors to revise the draft based on reviewer feedback and resubmit the paper to another venue.	decision

2021-2001	The paper is proposing a domain generalization method based on the intuition that an invariant model would work for any split of train/val.	abstract
2021-2001	Hence, the method uses adversarial train/val splits during training.	abstract
2021-2001	The paper is reviewed by three expert reviews and none of them championed the paper to be accepted.	rating_summary
2021-2001	I carefully checked the reviews and the authors' response and agree with the reviewers.	decision
2021-2001	Specifically: <sep> R#1: Argues that the paper is not ready for publication.	rating_summary
2021-2001	Also argues the optimization problem is only a motivation as it is not directly solved.	weakness
2021-2001	This is an important issue and it needs to be addressed in a conclusive manner. <sep>	weakness
2021-2001	R#2: Argues empirical studies do not show the value of train/val splitting.	weakness
2021-2001	I partially disagree with this issue but it is clear that more qualitative and quantitative study is needed to properly justify the proposed method. <sep>	ac_disagreement
2021-2001	R#3: Argues the contribution is not enough for publication.	rating_summary
2021-2001	The paper is clearly novel but the contribution and novelty is not presented in a clear manner.	weakness
2021-2001	Moreover, the empirical study does not complement the novelty.	weakness
2021-2001	Hence, I disagree with the comment.<sep>	ac_disagreement
2021-2001	Overall, I believe the paper proposes an interesting idea.	strength
2021-2001	However, the presentation and empirical studies need to be improved significantly.	weakness
2021-2001	I recommend authors to address these issues and submit to the next conference.	decision

2021-2008	This paper proposes a method for automatically discovering graph algorithms using GNNs.	abstract
2021-2008	In general, the reviewers find the paper well-written, and the problem and the approach interesting.	strength
2021-2008	However, there is a concern on the practical usefulness of proposed method as shown in the following comments:  "My main concerns are on Q2, ie, the practical usefulness of the algorithm"[R1]; "It sounds like the proposed model is hard to generalize to different datasets" [R3]; "The proposed explainer does not generate practically useful outputs for discovering new algorithms"[R4].	weakness

2021-2030	The authors consider local 'why' or 'abductive' explanations for a model and a given class, which identify a minimal subset of features such that they're sufficient to imply that the model predicts the class; and 'why not' or 'contrastive' explanations, which identify a minimal subset s.t.	abstract
2021-2030	they're sufficient to imply that the model predicts a different class.	abstract
2021-2030	The two types of explanation are related using earlier work on minimal hitting sets going back to Reiter (1987). <sep>	abstract
2021-2030	Reviewers were divided in their opinions.	misc
2021-2030	R4 was very positive but with little detail and only medium confidence, then did not participate in discussion.	rating_summary
2021-2030	R2 was the only reviewer with high confidence, leaning against acceptance.	rating_summary
2021-2030	The paper relies on FOL which was hard for reviewers to grapple with, and may make it challenging for readers.	weakness
2021-2030	The presentation could be improved by clearly linking to existing work and demonstrating why the new approach is important.	suggestion

2021-2046	This paper introduces a method for hierarchical classification with deep networks.	abstract
2021-2046	The idea is interesting, and as far as I know novel: namely, the authors add a regularizer to the last layer in order to enforce a hierarchical structure onto the classifiers.	strength
2021-2046	The idea of placing spheres (with a fixed radius) around each classifier and forcing the child-classifiers to lie on these spheres is quite clever. <sep>	strength
2021-2046	The reviewers have pointed out some concerns with this paper.	misc
2021-2046	Some had to do with terminology (which the authors should fix but which is no big deal), but the main weakness are the experimental results and the ablation study.	weakness
2021-2046	The reviewers were not convinced that the optimization in the Euclidean space wouldn't be sufficient.	weakness
2021-2046	A more thorough ablation study could help here. <sep>	suggestion
2021-2046	This is the kind of paper that I really want to see published eventually, but right now isn't quite ready yet.	decision
2021-2046	If you make one more iteration (in particular adding a stronger ablation study) it should be a strong submission to the next conference.	suggestion
2021-2046	Good luck!	misc

2021-2048	In general there is agreement under reviewers that the ideas/method presented are somewhat interesting/promising but also that the paper lacks a lot of clarity.	weakness
2021-2048	Reviewers agree that the paper needs more work (on the method) and more extensive experiments to be convincing, and that in its current form it is not mature enough for publication at *CONF*.	rating_summary

2021-2057	The paper discusses an extension of BERT for learning user representations based on activity patterns in a self-supervised setting.	abstract
2021-2057	All reviewers have concerns about the validity of the claims and the significance of the experimental results.	weakness
2021-2057	Overall, I agree with the reviewers that the paper needs more work to be published at *CONF*.	decision
2021-2057	I recommend rejection.	decision

2021-2093	The reviewers found this to be an interesting and clearly-written paper, but broadly agreed that it is not yet ready for acceptance.	rating_summary
2021-2093	In particular, multiple reviewers felt that the experiments don't show clear benefits of the proposed SVAE approach when compared to the VAEVAE and other baselines; nor do they sufficiently back up the central claim regarding relative benefit of PoE vs MoE for either "AND" or "OR" relations.	weakness
2021-2093	Hopefully the comments and suggestions from the reviewers, particularly regarding framing and experimental validation, will help in revising the paper.	suggestion

2021-2099	The authors propose a low-bit floating point quantization method to reduce energy and time consumption for deep learning training.	abstract
2021-2099	Dynamic quantization and MLS tensor arithmetic are used to enhance the effectiveness of MLS.	abstract
2021-2099	The motivation is clear and the efficient training is an important problem to address.	strength
2021-2099	However, the effectiveness of proposed method is not well justified and experimental results are less convincing.	weakness
2021-2099	In addition, the clarify of paper still needs to be further improved.	weakness

2021-2100	Overall, all reviewers generally agree that the idea of using visual similarity to unsupervised alignment of multiple languages is interesting and the proposed method and dataset are well-designed, while three of them raised some concerns related to the retrieval nature of the method.	weakness
2021-2100	In particular,  discussions about its place as a study of machine translation and comparison with other cross-lingual retrieval baselines were the main issues.	weakness
2021-2100	Although authors made great effort to address reviewers' concerns points and did clarify some of them, unfortunately the reviewers were not fully convinced by the response, and one reviewer decided to downgrade the initial score.	rebuttal_process
2021-2100	After all, three reviewers rate the paper as 'below the acceptance threshold'.	rating_summary
2021-2100	Based on their opinions, I decided to recommend rejection. <sep>	decision
2021-2100	I think the entire picture of the work and the logic flow could be much clearer by discussing in a top down manner why this idea should be implemented with a retrieval-based approach, rather than superficially adding "using retrieval" to some sentences.	suggestion

2021-2101	The paper proposes a multi-scale spatial-temporal joint graph convolution for spatiotemporal forecastings.	abstract
2021-2101	Many reviewers have concerns regarding novelty, baseline comparisons, and writing clarity of the draft.	weakness

2021-2103	In this paper, the authors study the behavior of the Lookahead dynamics of Zhang et al (2019) in bilinear zero-sum games.	abstract
2021-2103	These dynamics work as follows: given a base algorithm for solving the game (such as gradient descent-ascent or extra-gradient), the Lookahead dynamics perform k iterations of the base algorithm followed by an exponential moving average step with weight α.	abstract
2021-2103	The authors then provide a range of sufficient conditions for the eigenvalues of the matrix defining the game under which the Lookahead dynamics become more stable and converge faster than the base method. <sep>	abstract
2021-2103	This paper received four reviews and generated a very lively discussion between the authors and reviewers.	misc
2021-2103	Reviewer 4 was enthusiastic about the paper; the other three initially recommended rejection.	rating_summary
2021-2103	During the discussion phase, the authors revised their paper extensively, and Reviewer 3 increased their score to an "accept" recommendation as a result.	rating_summary
2021-2103	In the end, the reviewers were evenly split, and I also struggled a lot to reach a recommendation decision. <sep>	misc
2021-2103	On the plus side, the paper treats an interesting problem: prior empirical evidence suggests that the Lookahead dynamics can improve the training of some adversarial machine learning models, so a theoretical study is very welcome and of clear value.	strength
2021-2103	On the other hand, the setting treated by the paper (bilinear min-max games) is somewhat restrictive, and the authors' theoretical conclusions do not always admit as clear an interpretation as one would like. <sep>	weakness
2021-2103	The issues that ended up playing the most important role in my recommendation were as follows: <sep> 	misc
2021-2103	The Lookahead dynamics with period k involve k gradient evaluations, so their rate of convergence should be compared at a k:1 ratio to GD and EG (with an additional 2:1 ratio between GD and EG to put things on an even scale).	weakness
2021-2103	To a certain degree, this k:1 ratio is present in the last part of Lemma 3; however, the exact acceleration achieved by the "shrinkage" of the spectral radius is not clear.	weakness
2021-2103	This can also be seen in the semi-log plots provided by the authors, where the corresponding slopes of GD/EGD methods should be multiplied by k when compared to the respective LA variants.	weakness
2021-2103	In this regard, a comparison with the values of k provided in Appendix D reveal that the performance of the Lookahead variants in terms of gradient queries is very similar (if not worse) to the non-LA variants.	weakness
2021-2103	This is a cause of concern because, if LA does not accelerate convergence in simple bilinear games, it is not credible to expect faster convergence in more complicated problems.	weakness
2021-2103	During the AC/reviewer discussion of this point, Reviewer 3 pointed out that this might be due to a suboptimal tuning of α (ie, that it was not chosen "small enough"), and went out to note that this echoes the arguments of other reviewers that the characterization of acceleration may be problematic and not significant (even if it takes place). <sep>	weakness
2021-2103	Another major concern has to do with the stabilization provided by the Lookahead dynamics: using a benchmark game proposed in a recent paper by Hsieh et al (2020), the authors showed that the Lookahead dynamics converge to a point which is unstable under GDA/EG (and hence avoided).	weakness
2021-2103	This is fully consistent with the authors' theoretical analysis, but it also highlights an important problem with the Lookahead optimizer: if k and α are tuned to suitable values for stabilization, the algorithm converges to a non-desirable critical point (a max-min instead of a min-max solution).	weakness
2021-2103	This is a major cause of concern because it shows that the algorithm may, in general, converge to highly suboptimal states.<sep>	weakness
2021-2103	The above create an inconsistency in the main story of the paper.	weakness
2021-2103	In fact, it seems to me that the authors' results form more of a "cautionary tale in hiding": even in very simple bilinear problems, the lookahead step may not provide acceleration and, even worse, it could converge to highly undesirable critical points.	weakness
2021-2103	I find this "negative" contribution quite valuable from a theoretical standpoint, and I believe that a thoroughly revised paper along these lines would be of interest in the top venues of the community (though a more theoretical outlet like COLT might be more appropriate).	strength
2021-2103	However, this would require a drastic rewrite of the paper, to the extent that it should be treated as a new submission. <sep>	decision
2021-2103	In view of all this, I am recommending a rejection at this stage.	decision
2021-2103	I insist however that this should not be seen as a critique for the mathematical analysis of the authors (which was appreciated by the reviewers), but as a recommendation to reframe the paper's narrative to bring it in line with the algorithm's observed behavior.	suggestion
2021-2103	I strongly encourage the authors to resubmit at the next top-tier opportunity.	decision

2021-2104	The authors present a model-based method for cooperative multi-agent reinforcement learning and propose to use communication of future predictions (as given by a learned world model) as a way to overcome partial observability. <sep>	abstract
2021-2104	Overall, all reviewers found this work to be of great interest and the combination of planning + communication novel.	strength
2021-2104	However, all reviewers pointed that the claims that the papers makes are not fully supported by the experimental framing of the paper pointing to several shortcomings around experimental design in general and better control of appropriate baselines.	weakness
2021-2104	The authors have since clarified several aspects in their paper and also included a new RL environment. <sep>	rebuttal_process
2021-2104	However, as the paper still stands does not fully provide convincing evidence of their proposal, which is however very intriguing.	rebuttal_process
2021-2104	I would like to echo though reviewers' suggestions that the authors work a bit more on the experimental design and I really hope this work will appear at a later venue.	decision

2021-2115	This paper introduces an approach based on filter transform for designing networks equivariant to different transformation groups. <sep>	abstract
2021-2115	Especially, the authors rely on the haramonic analysis view of steerable CNNs given in Weiler & Cesa (2019) to design an equivariant filter bank by computing simple transforms over base filters. <sep>	abstract
2021-2115	The reviewers finds the paper technically solid but difficult to read and with a limited contribution. <sep>	weakness
2021-2115	The AC carefully reads the paper and discussions.	misc
2021-2115	Although the connection between steerable CNNs and filter transform are interesting, the AC considers that the main contributions of the paper should be consolidated, especially the positioning with respect to Weiler & Cesa (2019).	weakness
2021-2115	Therefore, the AC recommends rejection.	decision

2021-2117	This paper investigate the interesting problem of policy seeking in reinforcement learning via constrained optimization.	abstract
2021-2117	Conditioned on reviewers' judgements, this is a good submission but hasn't reached the bar of *CONF*.	decision

2021-2132	This work presents a straightforward and easy to understand method for using hypernetworks to adapt existing models to be able to increase their output space.	strength
2021-2132	The method itself is also interesting and is detailed enough for reproducibility.	strength
2021-2132	However, the experiments and results should be improved by expanding the demonstration of CHNs beyond the narrow P-VAE application and comparing against relevant baselines in the recommendation system literature. <sep>	suggestion
2021-2132	Pros<sep> Clear writing. <sep>	strength
2021-2132	Detailed hyperparameters to aid reproducibility. <sep>	strength
2021-2132	Straightforward model.<sep>	strength
2021-2132	Cons<sep> Lack of sufficient comparison to related work, especially to existing recommendation systems that handle the cold-start issue and to Vartak, 2017. <sep>	weakness
2021-2132	Limited results that only demonstrate application to P-VAE meaning it's still unknown if CHNs work well with other models.	weakness
2021-2132	The result on the synthetic dataset is also less persuasive. <sep>	weakness
2021-2132	Lack of sufficient ablations, ie training a SVM/linear regression model until convergence.	weakness

2021-2145	The paper addresses an important unsolved problem, ie deriving explainable features for use in graph classification.	abstract
2021-2145	It does it by providing: <sep> i) a simple to implement (local) node aggregation approach; <sep>	strength
2021-2145	ii) some theoretical support to the proposed approach; <sep>	strength
2021-2145	iii) empirical evidence that the proposed approach could be effective. <sep>	strength
2021-2145	Notwithstanding the above merits, the reported work seems to still be in a preliminary phase.	weakness
2021-2145	In fact: <sep> i) reference to literature is missing some important recent contributions to the addressed problem (eg Gated Graph Sequence Neural Networks, GNNExplainer); if possibile, also experimental comparisons vs those approaches is desirable; <sep>	weakness
2021-2145	ii) experimental results do not provide a solid evidence that the proposed approach can really help to provide a clear explanation of the output, and the overall performance in classification is mostly below SOTA models; adding more datasets could help to give a more solid support to the main statement about explainability/performance; <sep>	weakness
2021-2145	iii) presentation needs to better highlight the original contribution wrt relevant literature (which is not completely clear in the current version of the paper), to improve the explanation of proofs, to discuss (both from a theoretical and empirical perspective) some important issues, such as computational scalability with the increase of size of local structures, and robustness to noise of the proposed (local) aggregation method. <sep>	weakness
2021-2145	In summary, although the proposed approach seems to be of some value, more work is needed to better place the proposed approach in the context of current literature and to gain a stronger experimental support to the main claim of the paper wrt explainability.	weakness

2021-2155	The paper received mixed reviews that overall lean negative. <sep>	rating_summary
2021-2155	The main concern shared by reviewers is the novelty of the findings.	weakness
2021-2155	Although the paper presents a systematic study that certainly has value, reviewers do not find sufficient insights from the analysis.	weakness
2021-2155	The ACs agree with the reviewers that the paper is below the bar for acceptance.	decision

2021-2166	The paper presents an evolutionary optimization framework for training discrete VAEs, which is different to the standard way of training VAEs.	abstract
2021-2166	One of the main criticism of the paper was the choice of experiments, but the authors addressed this point by adding an inpainting benchmark. <sep>	rebuttal_process
2021-2166	Unfortunately, the reviewers' scores are borderline, and one of the reviewers pointed out the lack of scalability (more precisely, linear scalability with the number of observations) and cannot recommend acceptance based on the limited application impact.	rating_summary
2021-2166	Given the large number of *CONF* submissions, this paper unfortunately does not meet the acceptance bar.	decision
2021-2166	That said, I encourage the authors to address this point and resubmit the paper to another (or the same) venue.	decision

2021-2178	The paper considers the problem of private data sharing under local differential privacy. <sep>	abstract
2021-2178	(1) it assumes having access to a public unlabeled dataset for learning a VAE, so it reduces the dimensionality in a more meaningful way than simply running PCA.	abstract
2021-2178	(2) the LDP guarantee is coming from the standard Laplace mechanism and Randomized Responses.	abstract
2021-2178	(3) then the authors propose how to learn a model based on the privately released (encoded) data which exploits the knowledge of the noise distribution. <sep>	abstract
2021-2178	None of these components are new as far as I know, nor were they new in the context of differential privacy.	weakness
2021-2178	For example, the use of a publicly available data for DP was considered in: <sep> 	weakness
2021-2178	Amos Beimel, Kobbi Nissim, and Uri Stemmer.	misc
2021-2178	Private learning and sanitization: Pure <sep>	misc
2021-2178	vs. approximate differential privacy.	misc
2021-2178	In Approximation, Randomization, and Combinatorial Optimization.	misc
2021-2178	Algorithms and Techniques, pages 363–378.	misc
2021-2178	Springer, 2013. <sep>	misc
2021-2178	(they called it Semi-Private Learning...)<sep>	misc
2021-2178	Papernot, N., Abadi, M., Erlingsson, U., Goodfellow, I., & Talwar, K. (2017).	misc
2021-2178	Semi-supervised knowledge transfer for deep learning from private training data.	misc
2021-2178	In *CONF*-17. <sep>	misc
2021-2178	The idea of integrating out the noise by leveraging the known noise structure were considered in: <sep> 	weakness
2021-2178	Williams, O., & McSherry, F. (2010).	misc
2021-2178	Probabilistic inference and differential privacy.	misc
2021-2178	Advances in Neural Information Processing Systems, 23, 2451-2459. <sep>	misc
2021-2178	Balle, B., & Wang, Y. X. (2018).	misc
2021-2178	Improving the Gaussian Mechanism for Differential Privacy: Analytical Calibration and Optimal Denoising.	misc
2021-2178	In International Conference on Machine Learning (pp.394-403).<sep>	misc
2021-2178	And many subsequent work. <sep>	weakness
2021-2178	The contribution of this work is in combining these known pieces (without citing some of the earlier work) to achieve a reasonably strong set of experimental results (for LDP standard).	strength
2021-2178	I believe this is the first experimental study that uses VAE for the dimension reduction, however, this alone is not sufficient to carry the paper in my opinion; especially since the setting is now much easier, with access to a public dataset. <sep>	weakness
2021-2178	The reviewers question the experiments are baselines are usually not using a public dataset as well as the practicality of the proposed method.	weakness
2021-2178	Also, connections to some of the existing work on private data release (a.k.a., private synthetic data generation) were note clarified.	weakness
2021-2178	For these reasons, there were not sufficient support among the reviewers to push the paper through. <sep>	rating_summary
2021-2178	The authors are encouraged to revise the paper according to the suggestions and resubmit in the next appropriate venue.	decision

2021-2205	This paper studies different properties of the top eigenspace of the Hessian of a deep neural network and their overlap.	abstract
2021-2205	It raised quite a lot of discussion, which finally went in not very constructive way.	misc
2021-2205	The reviewers generally agree that the paper has potential, but the actual contribution is limited. <sep>	weakness
2021-2205	Pros: <sep> The idea that top eigenspaces between different models have high overlap is interesting <sep> 	strength
2021-2205	The explanation that these structures can be explained by Kronecker-product approximation of the Hessian.<sep>	strength
2021-2205	Cons: <sep> The connection to PAC-Bayes is unclear and seems artificial. <sep> 	weakness
2021-2205	Many of the related work is missing <sep>	weakness
2021-2205	The models and datasets are too simple, and general conclusions can not be made on such kind of models.	weakness
2021-2205	Much more testing is needed to verify the claims, including state-of-the art architectures and datasets.	weakness

2021-2229	In this paper, the authors study how to incorporate experimental data with interventions into existing pipelines for DAG learning.	abstract
2021-2229	Mixing observational and experimental data is a well-studied problem, and it is well-known how to incorporate interventions into eg the likelihood function, along with theoretical guarantees and identifiability.	weakness
2021-2229	Ultimately there was a general consensus amongst the reviewers that without additional theoretical results to advance the state of the art, the contribution of this work is limited.	weakness

2021-2244	The paper proposed a new way for training models that stack the same basic block for multiple times -- share the weights first and then untie the weights.	abstract
2021-2244	Ablation study shows that the proposed algorithm has marginal improvement over the baseline.	abstract
2021-2244	The authors also provide some theoretical justifications to how the proposed idea works. <sep>	abstract
2021-2244	The proposed idea is straightforward and intuitive.	strength
2021-2244	Weight sharing has been used in previous works, and what's new in this paper is to unshare the weights in the middle (with a heuristic rule).	weakness
2021-2244	The hope is that by doing so, one can achieve a better tradeoff between speedup and accuracy.	weakness
2021-2244	However the experimental supports are somehow weak and incomplete.	weakness
2021-2244	For example, in order to show the real speedup, one should provide the full training curve (until convergence) under different settings, instead of just showing one data point (at 500K).	weakness
2021-2244	It is very common that one can get some speedup at 500K, but the speedup totally disappears after another 500K steps. <sep>	weakness
2021-2244	Furthermore, the theoretical analysis is conducted in a simplified setting, and it is not very clear whether it can be used to explain what really happened during BERT training. <sep>	weakness
2021-2244	The reviewers conducted some lengthy discussions after the author rebuttal was available.	misc
2021-2244	As a final consensus, we think that there are still concerns on the paper, which makes us hesitate to give an ACCEPT recommendation.	decision

2021-2257	The paper studies benchmarking of bias mitigation methods.	abstract
2021-2257	The authors propose a synthetic dataset of images (alike colored-MNIST) that enables a controlled setup over different types of correlations between a binary sensitive attribute, dataset features, and a binary outcome label.	abstract
2021-2257	The authors have evaluated 2K models that are the variants of three recently proposed debiasing methods using fair representation learning across various settings. <sep>	abstract
2021-2257	While the reviewers acknowledged the importance of benchmarking fair learning methods in a systematic controlled setting, they have raised several concerns: <sep> (1) the proposed benchmark is too abstract/unrealistic (R4, R2, R3); it is not clear whether the findings from this benchmark can be generalized to real-world data with real sensitive features, (2) the proposed benchmark is limited to pseudo sensitive attributes (R1) that are binary (R1, R2), (3) the paper lacks in-depth analysis on why certain methods work under certain conditions (R3).	weakness
2021-2257	Among these, (2) did not have a major impact on the decision, but would be helpful to address in a subsequent revision, (3) was partially addressed in the rebuttal.	rebuttal_process
2021-2257	However, (1) makes it very difficult to assess the benefits of the proposed benchmark, and was viewed by AC as a critical issue. <sep>	rebuttal_process
2021-2257	The authors provided a detailed rebuttal addressing multiple of the reviewers' concerns.	rebuttal_process
2021-2257	AC can confirm that all four reviewers have read the author responses and have contributed to the discussion.	rebuttal_process
2021-2257	A general consensus among reviewers and AC suggests, in its current state the manuscript is not ready for a publication.	decision
2021-2257	See R1 post-rebuttal encouragement and suggestions how to strengthen the work.	suggestion
2021-2257	We hope the detailed reviews are useful for improving the paper.	suggestion

2021-2279	This paper received 2 borderline accepts, 1 accept, and 1 reject. <sep>	rating_summary
2021-2279	This paper was discussed on the forum and no consensus was reached.	misc
2021-2279	The two reviewers who rated the paper as borderline accept emphasized that the biological claims are overblown, that the intellectual contributions (the initialization scheme and partial training) are incremental from a statistical learning perspective, and that the potential applications for the future (like alternate learning rules) are too speculative.	weakness
2021-2279	I agree with both of these reviewers (and the negative reviewer) that the biological rationale is problematic and the approach is not credible as a model of biology.	weakness
2021-2279	It is not evaluated as a computer vision model either.	weakness
2021-2279	And I completely agree with the point raised by several reviewers that there is simply no data about how many synaptic updates to target.	weakness
2021-2279	Hence, statements regarding % of total synaptic updates and % of brain matches seem empty without a precise target.	weakness
2021-2279	For all these reasons, I recommend this paper be rejected.	decision

2021-2286	Overview This paper applies RL to automated theorem proving to eliminate the need for human-written proofs as training data.	abstract
2021-2286	The method uses TF-IDF for premise selections.	abstract
2021-2286	The experiments compared with supervised baseline demonstrate some good performance. <sep>	abstract
2021-2286	Pro The paper provides a side-by-side comparison of the effect of the availability of human proofs on the final theorem proving. <sep>	strength
2021-2286	The experiments compared with supervised baseline show that the proposed method has good performance even without human knowledge.	strength
2021-2286	The prosed TF-IDF selection algorithm addresses a challenging issue in exploration of RL. <sep>	strength
2021-2286	Con The reviewers primarily concern about  the novelty of the methods.	weakness
2021-2286	It appears the method is not new since there exist a body of work leveraging RL to learn theorem provers.	weakness
2021-2286	The tasks are also not novel.	weakness
2021-2286	After rebuttal, the reviewers are not convinced that the novelty is significant enough for *CONF*.	rating_summary
2021-2286	The reviewers are also concerned that the proposed method might not be easily generalized to other tasks. <sep>	weakness
2021-2286	Recommendation Although the proposed method and experiment demonstrate some merits, there is a lack of novelty in terms of approaches.	weakness
2021-2286	Since existing results already consider similar methods and similar tasks, it would make the paper stronger if thorough experimental comparisons are performed.	suggestion

2021-2307	Dear authors, <sep>	misc
2021-2307	the paper contains many interesting and novel ideas.	strength
2021-2307	Indeed, tuning step-size is very time and energy-consuming, and deriving and analyzing new adaptive algorithms has not only theoretical benefits but, more importantly, is a key when training more complicated ML models. <sep>	strength
2021-2307	The paper contains many weaknesses as noted by reviewers.	misc
2021-2307	I know that you have addressed many of them one of the reviewers is still concerned about the other issues involving Theorem 1 and the assumption of the bounded preconditioner. <sep>	rebuttal_process
2021-2307	He thinks the preconditioner bound is troublesome.	rebuttal_process
2021-2307	In the overparameterized regime, he would expect the gradients to become near zero as the algorithm converges, which would actually cause the preconditioner to NOT be bounded below.	rebuttal_process
2021-2307	It seems that the analysis might actually improve if the authors abandoned AMSGrad/Adam and instead just considered SGD for which the preconditioner assumption is not an assumption but just a property of the algorithm. <sep>	suggestion
2021-2307	Thank you	misc

2021-2310	This paper proposes a benchmark suite of offline model-based optimization problems.	abstract
2021-2310	This benchmark includes diverse and realistic tasks derived from real-world problems in biology, material science, and robotics contains a wide variety of domains, and it covers both continuous and discrete, low and high dimensional design spaces.	abstract
2021-2310	The authors provide a comprehensive evaluation of existing methods under identical assumptions and get several interesting takeaways from the results.	abstract
2021-2310	They found there exists surprising efficacy of simple baselines such as naive gradient ascent, which suggests the need for careful tuning and standardization of methods in this area, and provides a test bed for algorithms that try to solve this challenge.	abstract
2021-2310	However, most reviewers agreed that a more in-depth analysis and insightful explorations for the RL experiment results will help readers understand why their method has superiority even without trajectory data, and  that the paper needs another revision before being accepted.	rating_summary
2021-2310	Therefore, I recommend rejection although all reviewers agreed that the tasks is very interesting and a good start.	decision

2021-2312	The authors address the problem of learning environment-invariant representations in the case where environments are observed sequentially. <sep>	abstract
2021-2312	This is done by using a variational Bayesian and bilevel framework. <sep>	abstract
2021-2312	The paper is borderline, with two reviewers (R2 and R3) favoring slightly acceptance and two reviewrs (R4 and R1) favoring rejection. <sep>	rating_summary
2021-2312	R4 points out that the current experiments do not do a good job of reflecting a continual learning setup and that simple modifications on existing IRM based methods could outperform the method proposed by the authors.	weakness
2021-2312	The authors are encouraged to take into account the reviewer's suggestions to improve the paper. <sep>	suggestion
2021-2312	R1 argued initially that the proposed solution is not learning at all since it has errors very close to random guessing.	rebuttal_process
2021-2312	While the authors have improved their method in the revision, the results are still close to random guessing, which questions the practical usefulness of the proposed approach.	rebuttal_process
2021-2312	Also, in the revision, the authors managed to obtain better results when their method is combined with Environment Inference for Invariant Learning (EIIL), but these results are secondary and not the main part of the paper. <sep>	rebuttal_process
2021-2312	The authors should improve the work taking into account the reviewrs' comments.	suggestion

2021-2318	The authors introduce vPERL, a model that generates an intrinsic reward for imitation learning.	abstract
2021-2318	vPERL is trained on demonstrations to minimise a variational objective that matches a posterior formed by "action backtracking" and a forward model, with the intrinsic reward coming from the reward map.	abstract
2021-2318	The authors might be interested in related work on few shot imitation learning: eg, "One shot imitation learning", Duan et al, 2017, "Watch, try learn: meta-learning from demonstrations and rewards", Zhou et al 2019.	suggestion
2021-2318	As all reviewers pointed out, and I can confirm, the paper is quite tricky to understand in its present form, and would very much benefit the writing being re-visited to more clearly express the ideas within (in particular, section 3, which is the core of the contributions).	weakness

2021-2324	The paper present a new learning-based approach` to solve the Maximum Common Subgraph problem.	abstract
2021-2324	All the reviewers find the idea of using GCN and RL to guide the branch and bound interesting although, even after reading the rebuttal, there are some important concerns about the paper. <sep>	rebuttal_process
2021-2324	The main issue raised by many reviewers are on scalability of the methods and motivation of the problem.	rebuttal_process
2021-2324	It would be nice to add a scalability experiments on large networks(>1M nodes) to show that the method could potentially scale.	suggestion
2021-2324	In fact, the original motivation based on drug discovery, chemoinformatics etc.	rebuttal_process
2021-2324	application is a bit weak because in those area domain specific heuristic should work better. <sep>	rebuttal_process
2021-2324	Overall, the paper is interesting but it does not meet the high publication bar of *CONF*.	decision

2021-2336	Problem Significance  This paper introduces an interesting taxonomy of OODs and proposed an integrated approach to detect different types of OODs.	abstract
2021-2336	The AC agrees on the importance of a fine-grained characterization of outliers given the large OOD uncertainty space. <sep>	strength
2021-2336	Technical contribution The key idea of the paper is to combine the predictions from multiple existing OOD detection methods.	abstract
2021-2336	While the AC recognizes the effort made by the authors to address the review comments, reviewers have several major standing concerns regarding limited contributions, insufficient analysis, and lack of clarity.	rebuttal_process
2021-2336	The AC agrees with reviewers that the paper is not ready yet for *CONF* publication, and can be further strengthened by: <sep> 	decision
2021-2336	(R1) reporting the computational cost for the integrated approach.	suggestion
2021-2336	The inference time for approaches such as Mahalanobis is typically a few times more expensive than the MSP baseline.	suggestion
2021-2336	The cumulative time for calculating all four scores may be non-negligible.	suggestion
2021-2336	Authors are encouraged to analyze the performance tradeoff in a future revision. <sep>	suggestion
2021-2336	(R2 & R3) discussing the effect of hyper-parameters tuning, which be overly complicated in practice as it involves combinations of multiple methods that each have multiple parameters to tune. <sep>	suggestion
2021-2336	(R3) comparing with more recent development on OOD detection and move the new results to the main paper.	suggestion
2021-2336	The AC also thinks it's worth discussing the connection and comparison to methods on quantifying uncertainty via Bayesian probabilistic approaches. <sep>	suggestion
2021-2336	(R2 & R4) more rigorous analysis of the benefits of the proposed integrated approach, both empirically and theoretically.	suggestion
2021-2336	Based on Table 7, the performance of using Mahalanobis alone is almost competitive as the proposed approach (except for the CIFAR10-CIFAR100 pair).	suggestion
2021-2336	This may deem further careful examination to understand what value other components are adding, and in what circumstance. <sep>	suggestion
2021-2336	(R2, R3 & R4) More discussion on the implication of the taxonomy and algorithm in the high-dimensional space would be valuable.	suggestion
2021-2336	The 2D toy dataset might be too simple to reflect the decision boundary as well as uncertainty space learned by NNs.	weakness
2021-2336	Moreover, it's important to justify further how aleatoric and epistemic uncertainty manifests in the current experiments using NNs.	suggestion
2021-2336	For example, epistemic uncertainty can arise due to the use of limited samples or due to the model uncertainty associated with the model regularization.<sep>	suggestion
2021-2336	Recent work by Hsu et al [2] also attempt to define a taxonomy of OOD inputs (based on semantic shift and domain shift), which can be relevant for the authors. <sep>	suggestion
2021-2336	Recommendation Three knowledgeable reviewers have indicated rejection.	rating_summary
2021-2336	The AC discounted R4's rating due to the less familiarity in this area and lack of participation in the post-rebuttal discussion. <sep>	ac_disagreement
2021-2336	[1] Richard Harang, Ethan M. Rudd.	misc
2021-2336	Towards Principled Uncertainty Estimation for Deep Neural Networks <sep>	misc
2021-2336	[2] Hsu et al Generalized ODIN: Detecting Out-of-distribution Image without Learning from Out-of-distribution Data	misc

2021-2337	This paper studies the relationship between adversarial transferability and knowledge transferability.	abstract
2021-2337	It develops two metrics to measure adversarial transferability and a theoretical framework to justify the positive correlation between adversarial transferability and knowledge transferability.	abstract
2021-2337	Synthetic experiments show that adversarial transferability measured by the proposed metrics indicates knowledge transferability. <sep>	abstract
2021-2337	While the paper studies an interesting and fundamental problem, with a sound theoretical analysis and a clear presentation, reviewers still have several reservations to directly accept it.<sep>	rating_summary
2021-2337	Lack of interpretation.	weakness
2021-2337	How this observation can be used to gain better understanding of either fields of adversarial examples or knowledge transfer? <sep>	weakness
2021-2337	Lack of inspiration.	weakness
2021-2337	How the insights can lead to better transfer techniques, apply to practical applications, and foster future research? <sep>	weakness
2021-2337	Lack of justification.	weakness
2021-2337	Why such definitions of metrics are the intrinsic ways of measuring adversarial transferability?	weakness
2021-2337	How well do they correlate with the practical experience with advanced attack, defense, and transfer methods?<sep>	weakness
2021-2337	AC believes the endeavor made by this paper towards a fundamental problem is highly necessary to our field.	strength
2021-2337	But given the above reservations, AC would encourage the authors to further strengthen their work to make it more inspiring and useful.	suggestion

2021-2349	Reviewers found the construction is very clever and the empirical results are interesting.	strength
2021-2349	However, a more thorough theoretical explanation is needed for acceptance.	decision

2021-2361	This paper studies a timely problem and consider an interesting approach, but overall there were many concerns about technical details and the validity of the framework.	weakness
2021-2361	The positive reviewer also mentioned concerns about the experiments, which others also found to be an insular comparison with weak baselines.	weakness
2021-2361	Following the response period, in discussion there are additional concerns arising related to the lack of details, for instance related to possible unidentifiability of the model.	rebuttal_process
2021-2361	As one reviewer discusses,  the authors are attempting to use RNNs to impute missing infection status labels when the missingness mechanism is assumed to be (i) not at random, (ii) playing out over time (as it is unclear whether Y^t is assumed (conditionally) independent of Y^t' with t' << t), and (iii) subject to interference (whether someone is tested is the 'treatment' here since it's a missingess problem and one person's propensity to be tested could causally affect another person's downstream infection status since apparently no Markov independence is assumed.	rebuttal_process
2021-2361	There is also consensus that the writing quality can be greatly improved.	weakness
2021-2361	Overall this work contains some ideas with potential in a thorough revision	misc

2021-2365	The authors propose an algorithm that learns sparse patterns of images that are highly predictive of a target class, even if added to a non-target class.	abstract
2021-2365	The reviewers agree that the algorithm is novel, is tested on a wide array of experiments, and the paper well written. <sep>	strength
2021-2365	Unfortunately, it seems that some of the main claims, such as DNNs trained on clean data "learn abstract shapes along with some texture", resort to qualitative evaluation of the few examples shown in the paper.	weakness
2021-2365	Furthermore, two reviewers were concerned with how one particular design choice in the algorithm might bias the authors' claims.	weakness
2021-2365	In particular, pointed out that the patterns learned are highly to the initial canvas used, which is not necessarily strongly motivated. <sep>	weakness
2021-2365	As these two issues are integral parts of the paper, I hesitate to recommend Acceptance at this point.	decision
2021-2365	That said, the approach looks very promising and I hope the authors continue to pursue this idea.	misc

2021-2381	Reviewers have different views on the paper and after going through the reviews, replies and the papers, we believe that there is room for improvement here. <sep>	rebuttal_process
2021-2381	While the part related to indefinite symmetric kernels, and general similarity functions seems to be well covered, as well as the part on Transformers, the relation with learning in RKBS and Transformer is far from being clear and Reviewer 4 makes a strong point on this.	rebuttal_process
2021-2381	For instance,<sep>	rebuttal_process
2021-2381	what is the goal of the section 5 and Definition 1 .	rebuttal_process
2021-2381	Indeed it is not clear here if the point of the authors is to learn the kernel parameters in equation 9 or to learn to predict the output of a transformer.	rebuttal_process
2021-2381	If it is the latter, the connection with the first part is unclear.<sep>	rebuttal_process
2021-2381	In Equation 11, I can understand that x and y are the sequences t and s but what is z_ij and how it is obtained?	rebuttal_process
2021-2381	So again, the learning problem drops in without justification and it is not explained how it can be solved.	rebuttal_process
2021-2381	The theoretical results involving the representer theorem is nice though.<sep>	rebuttal_process
2021-2381	The experiment does not seem very related to the learning problem in Equation 11 introduced by the authors.it seems to me that they are just trying different kernels on top of the dot product.	rebuttal_process

2021-2387	Overall, the paper makes some interesting and intuitive observations regarding the autoencoders with a cycle consistency, and aims at achieving controllable synthesis via a disentangled representation.	abstract
2021-2387	However, the overall consensus was that the manuscript needs further iterations: <sep> In particular: <sep> The ideas should be made more precise using mathematical arguments, as it stands some ideas are (eg DEAE and UDV) disconnected. <sep>	weakness
2021-2387	The scope needs to be clarified, eg respective contributions of GSL-AE and DEAE, use of label information <sep>	weakness
2021-2387	More numerical/quantitative evaluations, the current experimentation is not convincing enough, needed for better justification (spurious and not convincing experimentations) <sep>	weakness
2021-2387	The English of the manuscript could be improved as it occasionally hampers the flow.	weakness

2021-2405	This paper proposes to learn representations in an unsupervised manner using a generative model in which observations are generated by combining independent causal mechanisms (ICMs), in combination with a global mechanism.	abstract
2021-2405	The authors introduce an unconventional mixture prior for the shared and independent components of the representation and train an encoder, discriminator and generator using a Wasserstein GAN with additional terms that enforce consistency in the data and latent space.	abstract
2021-2405	Experiments consider variations of MNIST and Fashion-MNIST and perform comparisons against a standard VAE, a β-VAE, and the Ada-GVAE. <sep>	abstract
2021-2405	Reviewers are broadly in agreement that this submission is not ready for publication in its current form.	rating_summary
2021-2405	R4 in particular has left very detailed comments regarding clarity.	rebuttal_process
2021-2405	The authors were able to in part address these comments, and R4 raised their score in response.	rebuttal_process
2021-2405	That said, from a read of the manuscript in its latest form, the metareviewer (who is very familiar with literature on disentangled representations) is inclined to agree with the reviewers that this is work that has value, but is very difficult to follow in its current form.	weakness
2021-2405	The metareviewer would like to suggest that the authors regroup, think carefully about how to improve clarity (in addition to addressing concrete points raised in reviews) and resubmit to a different venue.	decision

2021-2413	The paper proposes  to effectively learn representation of 3D data (point clouds/meshes) using a spherical GNN architecture over concentric spherical maps.	abstract
2021-2413	A method for converting point clouds to concentric spherical images is also proposed.	abstract
2021-2413	Evaluation is done via 3D classification tasks on rotated data. <sep>	abstract
2021-2413	Strengths: <sep> Interesting novel method for learning 3D representations <sep> 	strength
2021-2413	Technically sound <sep>	strength
2021-2413	Performs similarly to spherical CNNs and other STOA on the Modelnet40 dataset<sep>	strength
2021-2413	Weaknesses: <sep> Presentation of the work needs to be further improved such that it is easier for others to reproduce <sep> 	weakness
2021-2413	More in-depth experiments are needed to justify how much Spherical GNN improves over other STOA, particular given how classification accuracy is very similar to STOA.	weakness

2021-2414	This paper aims to study the convergence of deep neural networks training via a control theoretic analysis.	abstract
2021-2414	This is a very interesting approach to establish theoretical understanding of deep learning.	strength
2021-2414	However, there are several concerns raised by the reviewers: <sep> The contribution of this paper is limited.	weakness
2021-2414	The results simply follow from standard optimal control.	weakness
2021-2414	It is not clear what new insight the paper provides. <sep>	weakness
2021-2414	There are already quite a few works on control theoretic analysis of deep learning.	weakness
2021-2414	This paper did not do a good job on presenting its novelty and difference with existing works. <sep>	weakness
2021-2414	The experimental part is weak.	weakness
2021-2414	It only involves small data set and very simple networks.<sep>	weakness
2021-2414	Based on these, I am not able to recommend acceptance for the current manuscript.	decision
2021-2414	But the authors are encouraged to continue this research.	misc

2021-2423	This paper proposes routing strategies for multilingual NMT.	abstract
2021-2423	The motivation is to train a single mixture model that can serve the training and prediction of multiple models.	abstract
2021-2423	Several strategies are proposed: token-level, sentence-level and task-level.	abstract
2021-2423	This is a simple and straightforward approach (which is fine).	strength
2021-2423	The main concerns from the reviewers regard novelty and missing comparisons.	weakness
2021-2423	In their updated draft, the authors added comparisons to bilingual models and they added a discussion wrt related work.	rebuttal_process
2021-2423	However, the author's response did not address enough some of other reviewers' concerns regarding comparison with other approaches, and the lack of novelty persists (mixture models for multi-task learning have been previously proposed in the literature), which makes me lean towards rejection.	decision
2021-2423	I suggest the authors address these aspects in future iterations of their work.	suggestion

2021-2450	Overview: <sep> The paper tries to answer which mutual information (MI) objective is sufficient for representation learning (repL) in reinforcement learning (RL).	abstract
2021-2450	Three common objectives are considered: forward, state, and inverse.	abstract
2021-2450	The paper shows that only the forward objective is sufficient for learning, ie, sufficient for learning of optimal policy/value function.	abstract
2021-2450	The authors also demonstrate this phenomena using empirical experiments. <sep>	abstract
2021-2450	Quality, Clarity, Originality and Significance: <sep> All the reviewers believe this paper is novel in terms of methodology, ie, evaluate the sufficiency of the repL in terms of down stream tasks.	strength
2021-2450	However, there is a lack of clarity in the experiment sections.	weakness
2021-2450	The authors have provided more details in the rebuttal phase.	rebuttal_process
2021-2450	The reviewers also have concerns that this paper may be too far from typical experimental settings to have a real impact on the field.	weakness
2021-2450	An unofficial review pointed out there is a mistake in the proof of the paper.	weakness
2021-2450	The authors later also confirmed the flaw and claimed it is fixed. <sep>	rebuttal_process
2021-2450	Recommendation: <sep> The paper is indeed interesting and novel.	strength
2021-2450	However, the impact to the practice community might not be significant.	weakness
2021-2450	That being said, the paper should warrant publication eventually.	decision
2021-2450	However, the authors changed large amount of text about the proofs before and after rebuttal, which also introduced some additional typos, confusions, and also technique sloppiness or flaws.	rebuttal_process
2021-2450	The reviewers are concerned about this.	weakness
2021-2450	Overall I believe that the paper is not in a state to be published yet.	decision

2021-2460	The paper proposes to explain the representation for layer-aware neural sequence encoders with multi-order-graph (MoG).	abstract
2021-2460	Based on the MoG explanation, it further proposes Graph-Transformer as a graph-based self-attention network empowered Transformer.	abstract
2021-2460	As commented by the authors, a main purpose of Graph-Transformer is to show an example application of the MoG explanation. <sep>	abstract
2021-2460	During the discussion period, after reading the paper and checking the code, the AC had raised a serious concern: There is a big gap between the MoG motivation and the actual implementation.	weakness
2021-2460	The AC had urged the referees to take a careful look at the implementation details, in particular, Lines 524-561 in the attached code: "supplement/fairseq-0.6.2_halfdim_gate⁩ ▸ ⁨fairseq⁩ ▸ ⁨models⁩ ▸transformer.py".	misc
2021-2460	The AC had made the following comments to the referees: "Whether the performance gain of Graph-Transformer over Transformer is due to the MoG explanation is highly unclear.	weakness
2021-2460	There is no direct evidence, such as appropriate visualization, to support that.	weakness
2021-2460	In a high-level description, instead of using a usual skip connection that would combine beforex and x, the actual implementation is to 1) define increamental_x = x - beforex, 2) let increamental_x attend on beforex to produce x1, let beforex attend on increamental_x to produce x2, and let increamental_x attend on increamental_x to produce x3, 3) combine beforex, x1, x2, x3 in a certain way to produce the layer output." <sep>	weakness
2021-2460	Reviewer 2 responded to the AC's concern: "After examining the transformer.py and Section 2 & 3, we cannot understand why the output of self-attentions could be regarded as MoG subgraphs?	weakness
2021-2460	The authors did not explain the connection.	weakness
2021-2460	In their code, the graph transformer seems to just utilize 3 multi-head attentions (line 539-541) in their encoder.	weakness
2021-2460	Using MoG to interpret the outputs of three attentions (line 539-541) is not very convincing.	weakness
2021-2460	The link is weak.	weakness
2021-2460	We agree with your comments." <sep>	misc
2021-2460	To summarize, the link between the actual implementation in the code and all the MoG explanations is quite weak, and the technical novelty of the actual implementation is not strong enough for an *CONF* publication.	weakness
2021-2460	Therefore, the AC recommends Reject.	decision

2021-2462	This paper studies the problem of uncertainty estimation under distribution shift.	abstract
2021-2462	The proposed approach (PAD) addresses this under-estimation issue, by augmenting the training data with inputs that the network has unjustified low uncertainty estimates, and asking the model to correct this under-estimation at those augmented datapoints.	abstract
2021-2462	Results show promising improvement over a set of common benchmark tasks in uncertainty estimation, with comparisons to a number of existing approaches. <sep>	abstract
2021-2462	All the reviewer agreed that the experiments are well conducted and the empirical results are very promising.	strength
2021-2462	However, they also had a shared concern on the justification of the approach.	weakness
2021-2462	Reviewers are less willing to accept a paper merely for commending its empirical performance. <sep>	rating_summary
2021-2462	I share the above concern as the reviewers, and I personally found the presentation of the approach a bit rush and disconnected from the motivation.	weakness
2021-2462	For example, the current presentation feels like the method is motivated by BNNs but it is not clear to me how the proposed objective connects to the motivation.	weakness
2021-2462	Also no derivation of the objective is included in either main text or appendix. <sep>	weakness
2021-2462	In revision, I would suggest a focus on improving the clarity and theoretical justification of the proposed objective function.	suggestion

2021-2469	This paper proposed a novel Adversarial Deep Metric Learning approaches.	abstract
2021-2469	The reviews pointed out the paper proposes an interesting idea and it is among the rare works that address directly robust metric learning which an important topic for efficient metric learning. <sep>	strength
2021-2469	Some concerns were raised about the analysis and the lack of comparisons notably with other types of adversarial attacks. <sep>	weakness
2021-2469	The authors provide a rebuttal where they addressed some concerns raised by reviewers with some precisions on the work, its positioning with respect to other related papers and additional comparisons notably with other types of attacks. <sep>	rebuttal_process
2021-2469	A minor remark: there is a typo in Eq(13), where the z in the loss function is actually not defined and should be included in the max function. <sep>	weakness
2021-2469	That being said, the contribution is still limited in considering only the infinite norm, analysis and comparisons to prior work remain weak.	rebuttal_process
2021-2469	The paper does not meet the requirements for acceptance to *CONF* in its current form. <sep>	decision
2021-2469	I have then to propose rejection.	decision

2021-2474	This paper analyzes some design choices for neural processes, paying particular attention to their small-data performance, uncertainty, and posterior contraction.	abstract
2021-2474	This is certainly a worthwhile project, and R3 found the analysis interesting, giving the paper a score of 8.	rating_summary
2021-2474	However, R1, R2, and R4 found the experimental validation to be incomplete and insufficient to support the paper's broader recommendations.	weakness
2021-2474	As the paper is investigating the various combinations of implementations, I tend to agree with R1, R2, and R4 that this paper---while having some interesting ideas---needs a bit more precision and breadth to its experiments.	weakness

2021-2479	This paper proposes a method for conditional inference with arbitrary conditioning by creating composed flows.	abstract
2021-2479	The paper provides a hardness result for arbitrary conditional queries.	abstract
2021-2479	Motivated by the fact that conditional inference is hard the paper therefore suggests a novel relaxation where the conditioning is relaxed. <sep>	abstract
2021-2479	There were various concerns from the reviewers regarding notation, comparison algorithms, and how the hardness result motivates the smoothing operation introduced.	weakness
2021-2479	After careful study of the paper and all the comments I find that I am most concerned about the hardness result and how it motivates the smoothing operation that is done.	weakness
2021-2479	Novel computational complexity results as such are not really in the scope of *CONF*.	weakness
2021-2479	There's nothing wrong with having such a result in a paper, of course, but a paper like this should be evaluated on the basis of the algorithm proposed. <sep>	weakness
2021-2479	Like R4, I do not follow how this hardness result is meant to motivate the smoothing that's applied.	weakness
2021-2479	The paper is unambiguous that the goal is to do conditional inference.	weakness
2021-2479	A hardness result is presented for conditional inference, and so a relaxed surrogate is presented.	weakness
2021-2479	This has a minor problem that it's not clear the relaxed problem avoids the complexity boundary of the original one.	weakness
2021-2479	There's a larger problem, though.	misc
2021-2479	The hardness result has not been sidestepped!	weakness
2021-2479	The goal is still to solve conditional inference.	weakness
2021-2479	The algorithm that's presented is still an approximate algorithm for conditional inference.	weakness
2021-2479	R4 suggests that other approximation algorithms should be compared to.	suggestion
2021-2479	The authors responded to this point, but I am not able to understand the response.	rebuttal_process
2021-2479	For the same reason, I think it is valid to ask for comparison to other approximate inference algorithms (eg without smoothing) <sep>	suggestion
2021-2479	None of the above is to say that the smoothing approach is bad.	weakness
2021-2479	It may very well be.	misc
2021-2479	However, I think that either the existing argument should be clarified or a different argument should be given. <sep>	suggestion
2021-2479	Finally here are two minor points (These weren't raised by reviewers and aren't significant for acceptance of the paper.	misc
2021-2479	I'm just bringing them up in case they are useful.) <sep>	misc
2021-2479	Is eq 3 (proof in Appendix B.1) not just an example of the invariance of the KL-divergence under diffeomorphisms? <sep>	weakness
2021-2479	Proof in appendix B.2 appears to just a special case of the standard chain rule of KL-divergence (eg as covered in Cover and Thomas)	weakness

2021-2499	The paper proposes a reinforcement learning algorithm that combines trust region policy optimization and entropy maximization.	abstract
2021-2499	The starting point is the Lagrangian of a constrained optimization problem that upper bounds the change in the policy and lower bounds the entropy of the policy.	abstract
2021-2499	The paper proves that the algorithm converges, and evaluates it experimentally in MuJoCo domains. <sep>	abstract
2021-2499	The main issues raised by the reviewers were related to the proofs (see especially R1) and experimental evaluation (R4).	weakness
2021-2499	The authors did a great job improving the paper during the discussion phase, but some of the issues remain unresolved, and thus reviewers find the paper not to be ready for publication.	rating_summary
2021-2499	Thus, I'm recommending rejection.	decision

2021-2504	This work provides evidence against the hypothesis that ResNets implement iterative inference, or that iterative convergent computation is a good inductive bias to have in these models.	abstract
2021-2504	The reviewers indicate that they think this hypothesis is interesting and relevant to the *CONF* community, but they do not find the current work sufficiently convincing.	weakness
2021-2504	Both theoretically and experimentally the paper does not fully demonstrate the claim that iterative inference is not useful in ResNets, and the reviewers are unanimous in their recommendation to reject the paper until the evidence for this claim is strengthened.	weakness

2021-2514	Dear authors, <sep>	misc
2021-2514	Improving the theoretical understanding of powerful algorithms is an important contribution to our field.	strength
2021-2514	Nevertheless, most of the reviewers are inclined to reject the paper.	rating_summary
2021-2514	I somehow have to agree with them as eg, adding more restrictive assumptions can allow deriving better bounds, but the question then is how useful this result will be to the *CONF* community.	weakness
2021-2514	I would encourage you to chose maybe another venue. <sep>	decision
2021-2514	Thanks	misc

2021-2538	The paper presents a multi-agent RL algorithm where the rewards of the other agents are only known up to some accuracy.	abstract
2021-2538	The setting is somewhat restrictive, in the sense that the transition is assumed to be known.	weakness
2021-2538	It would perhaps have been more interesting for the paper to also consider unknown transitions, so as to bring it closer to work in single-agent reinforcement learning.	suggestion
2021-2538	It also seems to not be making a very good job of linking the related work to the contribution of this paper (even after looking at the appendix).<sep>	weakness
2021-2538	Authors briefly say in the introduction <sep>	weakness
2021-2538	"Alternative frameworks improve robustness, eg, to changes in environment dynamics, observation or action spaces (Pinto et al, 2017; Li et al, 2019; Tessler et al, 2019), but do not address reality gaps due to reward function mismatches, as they use inappropriate metrics on the space of adversarial perturbations"<sep>	weakness
2021-2538	Authors should try and better explain the differences with those papers.	suggestion
2021-2538	Do  they consider changes in dynamics rather than the reward?	suggestion
2021-2538	It appears that the former is more general than the latter.	suggestion
2021-2538	Couldn't the authors compare with them with an appropriate experiment? <sep>	suggestion
2021-2538	It is also hard to see how this exactly connects with a reality gap.	suggestion
2021-2538	What is the 'training' environment?	suggestion
2021-2538	What is the 'testing' environment?	suggestion
2021-2538	This is simply a robust optimisation algorithm applied to multi-agent games with partially unknown reward functions. <sep>	weakness
2021-2538	In addition the experiments themselves are not explained clearly. <sep>	weakness
2021-2538	On the plus side, I think the algorithmic details and experimental are interesting.	strength
2021-2538	If there was a better explanation and discussion/comparison with related work, then it would have been a good paper.	weakness
2021-2538	Authors are encouraged to make a stronger effort to compare with other methods both in terms of the algorithm and experimentally.	suggestion

2021-2552	Knowledge distillation (KD) has been widely used in practice for deployment.	abstract
2021-2552	In this paper, a variant of KD is proposed: given a student network, an auxiliary teacher architecture is temporarily generated via dynamic additive convolutions; dense feature connections are introduced to co-train the teacher and student models.	abstract
2021-2552	The proposed method is novel and interesting.	strength
2021-2552	Empirical results showed that the proposed method can perform better than several KD variants.	strength
2021-2552	However, it is unclear why the proposed method works, although the authors tried to address this issue in their rebuttal.	rebuttal_process
2021-2552	Besides this,  a bigger concern on this work is that it missed a comparison with a recent approach in [1] which looks much simpler and performs significantly better on similar experiments.	weakness
2021-2552	In [1], their ResNet50 (0.5x) is smaller than the student model in this paper (which used more filters on the top) but showed much stronger performance on both relative and absolute improvements over the same baseline (training from scratch) for the ImageNet classification task.	weakness
2021-2552	On the technical side, the method in [1] simply uses the original ResNet50 as the teacher model,  and the student model ResNet50 (0.5x) progressively mimics the intermediate outputs of the teacher model from layer to layer.	weakness
2021-2552	[1] also contains a  theoretic analysis  (mean-field analysis based) to support their method.	weakness
2021-2552	Comparing with the method in [1], the proposed method here is more complicated, less motivated, and less efficient. <sep>	weakness
2021-2552	[1] D. Zhou, M. Ye, C. Chen, T. Meng, M. Tan, X. Song, Q. Le, Q. Liu and D. Schuurmans. Go Wide, Then Narrow: Efficient Training of Deep Thin Networks. ICML 2020.	misc

2021-2561	This paper studies various graph measures in depth.	abstract
2021-2561	The paper was reviewed by three expert reviewers who complemented the ease of understanding because of clear writing.	strength
2021-2561	But they also expressed concerns for limited novelty, theoretical justification, and unrealistic setting.	weakness
2021-2561	The authors are encouraged to continue research, taking into consideration the detailed comments provided by the reviewers.	suggestion

2021-2582	All the reviewers agree that the paper presents an interesting idea, and the main concern raised by the reviewers was the clarity of the paper.	weakness
2021-2582	I believe that the authors have improved the presentation of the paper after rebuttal, however, I still believe that the paper woudl require another round of reviews before being ready for publication, in order to properly assess its contributions.	decision

2021-2611	The paper proposes a trick for stabilizing GAN training and reports experiment results on spectrogram synthesis.	abstract
2021-2611	All the reviewers rate the paper below the bar, citing various concerns, including a lack of clarity and unconvincing results.	rating_summary
2021-2611	Several reviewers suggest conducting evaluations in the image domain as most of the GAN training techniques are proposed in the image domain.	suggestion
2021-2611	After consolidating the reviews and rebuttal, the area chair finds the reviewer's argument convincing and would not recommend acceptance of the paper.	decision

