2018-4	This paper proposes a method to train neural networks with low precision.	abstract
2018-4	However, it is not clear if this work obtains significant improvements over previous works.	weakness
2018-4	Note that: 1) Working with 16bit, one can train neural networks with little to no reduction in performance.	weakness
2018-4	For example, on ImageNet with AlexNet one gets 45.11% top-1 error if we don't do anything else, and 42.34% (similar to the 32-bit result) if we additionally adjust the loss scale (e.g., see Boris Ginsburg, Sergei Nikolaev, and Paulius Micikevicius.	weakness
2018-4	"Training of deep networks with halfprecision float." NVidia GPU Technology Conference, 2017).	misc
2018-4	2) ImageNet with AlexNet top-1 error (53.5%) in this paper seems rather high in comparison to previous works.	weakness
2018-4	Specifically, DoReFA and QNN, which used mostly lower precision (k_W=1, k_A=2 and k_E=6, k_G=32)  one can get much lower performance (47% and 49%, respectively).	weakness
2018-4	So, the main innovation here, in comparison, is k_G=12.	strength
2018-4	3) Comparison using other datasets is made with different architectures then previous works, so it is hard to quantify what is the contribution of the proposed method.	weakness
2018-4	For example, on MNIST, the authors use a convolutional neural network, while BC and BNN used a fully connected neural network (the so called "permutation invariant mnist" problem).	weakness
2018-4	4) Cifar performance is good, but may seem less remarkable, given that "Gated XNOR Networks: Deep Neural Networks with Ternary Weights and Activations under a Unified Discretization Framework" already showed that k_G=k_W=k_A=2, k_E=32 is sufficient to get 7.5% error on CIFAR.	weakness
2018-4	So the main novelty, in comparison, is that k_E=12.	weakness
2018-4	Taking all the above into account, it hard to be sure whether the proposed methods meaningfully improve existing methods.	weakness
2018-4	Moreover, I am not sure if decreasing the precision from 16bit to 12bit (as was done on ImageNet) is very useful for hardware applications, especially if there is such a degradation in accuracy.	weakness
2018-4	If, for example, the authors would have demonstrated all-8bit training on all datasets with little performance degradation, this would seem much more useful.	suggestion
2018-4	Minor: there are some typos that should be corrected, e.g.: "Empirically, We demonstrates" in abstract.	weakness
2018-4	%%% Following the authors response %%%	rebuttal_process
2018-4	The authors have improved their results and have addressed my concerns.	rebuttal_process
2018-4	I therefore raised my scores.	rebuttal_process
2018-4	The authors describe a method called WAGE, which quantize all operands and operators in a neural network, specifically, the weights (W), activations (A), gradients (G), and errors (E) .	abstract
2018-4	The idea is using quantizers with clipping (denoted in the paper with Q(x,k)) and some additional operators like shift (denoted with shift(x)) and stochastic rounding.	abstract
2018-4	The main motivation of the authors in this work is to reduce the number of bits for representation in a network for all the WAGE operations and operands which influences the power consumption and silicon area in hardware implementations.	abstract
2018-4	After introducing the idea and related work, the authors in Section 3 give details about how to perform the quantization.	abstract
2018-4	They introduce the additional operators needed for training in such network.	abstract
2018-4	Since quantization may loss some information, the authors need to quantize the signals in the network around the dynamic range in order not to "kill" the signal.	abstract
2018-4	The authors describe how to do that.	abstract
2018-4	Afterward, as in other techniques for quantization, they describe how to initialize the network values.	abstract
2018-4	Also, they argue that batch normalization in this network is replaced with the shift-quantize operations, and what is matter in this case is (1) the relative values ("orientations") and not the absolute values and (2) small values in errors are negligible.	abstract
2018-4	Afterward, the authors conduct experiments on MNIST, SVHN, CIFAR10, and ILSVRC12 datasets, where they show promising results compared to the errors provided by previous works.	abstract
2018-4	The WAGE parameters (i.e., the quantized no.	abstract
2018-4	of bits used) are 2-8-8-8, respectively.	abstract
2018-4	For understand more the WAGE, the authors compare on CIFAR10 the test error rate with vanilla CNN and show is small loss in using their network.	abstract
2018-4	The authors investigate mainly the bitwidth of errors and gradients.	abstract
2018-4	In overall, this paper is an accept since it shows good performance on standard problems and invent some nice tricks to implement NN in hardware, for *both* training and inference.	decision
2018-4	For inference only, other works has more to offer but this is a promising technique for learning.	strength
2018-4	The things that are still missing in this work are some power reduction estimates as well as area reduction estimations.	rebuttal_process
2018-4	This will give the hardware community a clear vision of how such methods may be implemented both in data centers as well as on end portable devices.	suggestion
2018-4	The authors propose WAGE, which discretized weights, activations, gradients, and errors at both training and testing time.	abstract
2018-4	By quantization and shifting, SGD training without momentum, and removing the softmax at output layer as well, the model managed to remove all cumbersome computations from every aspect of the model, thus eliminating the need for a floating point unit completely.	abstract
2018-4	Moreover, by keeping up to 8-bit accuracy, the model performs even better than previously proposed models.	abstract
2018-4	I am eager to see a hardware realization for this method because of its promising results.	strength
2018-4	The model makes a unified discretization scheme for 4 different kinds of components, and the accuracy for each of the kind becomes independently adjustable.	strength
2018-4	This makes the method quite flexible and has the potential to extend to more complicated networks, such as attention or memory.	strength
2018-4	One caveat is that there seem to be some conflictions in the results shown in Table 1, especially ImageNet. Given the number of bits each of the WAGE components asked for, a 28.5% top 5 error rate seems even lower than XNOR.	weakness
2018-4	I suspect it is due to the fact that gradients and errors need higher accuracy for real-valued input, but if that is the case, accuracies on SVHN and CIFAR-10 should also reflect that.	weakness
2018-4	Or, maybe it is due to hyperparameter setting or insufficient training time?	weakness
2018-4	Also, dropout seems not conflicting with the discretization.	weakness
2018-4	If there are no other reasons, it would make sense to preserve the dropout in the network as well.	weakness
2018-4	In general, the paper was written in good quality and in detail, I would recommend a clear accept.	decision

2018-5	This paper presents a set of studies on emergent communication protocols in referential games that use either symbolic object representations or pixel-level representations of generated images as input.	abstract
2018-5	The work is extremely creative and packed with interesting experiments.	strength
2018-5	I have three main comments.	misc
2018-5	* CLARITY OF EXPOSITION The paper was rather hard to read.	weakness
2018-5	I'll provide some suggestions for improvement in the minor-comments section below, but one thing that could help a lot is to establish terminology at the beginning, and be consistent with it throughout the paper: what is a word, a message, a protocol, a vocabulary, a lexicon?	suggestion
2018-5	etc. * RELATION BETWEEN VOCABULARY SIZE AND PROTOCOL SIZE	misc
2018-5	In the compositional setup considered by the authors, agents can choose how many basic symbols to use and the length of the "words" they will form with these symbols.	weakness
2018-5	There is virtually no discussion of this interesting interplay in the paper.	weakness
2018-5	Also, there is no information about the length distribution of words (in basic symbols), and no discussion of whether the latter was meaningful in any way.	weakness
2018-5	* RELATION BETWEEN CONCEPT-PROPERTY AND RAW-PIXEL STUDIES	misc
2018-5	The two studies rely on different analyses, and it is difficult to compare them.	weakness
2018-5	I realize that it would be impossible to report perfectly comparable analyses, but the authors could at least apply the "topographic" analysis of compositionality in the raw-pixel study as well, either by correlating the CNN-based representational similarities of the Speaker with its message similarities, or computing similarity of the inputs in discretized, symbolic terms (or both?	suggestion
2018-5	). * MINOR/DETAILED COMMENTS Section 1	suggestion
2018-5	How do you think emergent communication experiments can shed light on language acquisition?	suggestion
2018-5	Section 2 In figure 1, the two agents point at nothing.	rebuttal_process
2018-5	\\mathbf{v} is a set, but it's denoted as a vector.	weakness
2018-5	Right below that, h^S is probably h^L?	weakness
2018-5	all candidates c \\in C: or rather their representations \\mathbf{v}?	weakness
2018-5	Give intuition for the reward function.	weakness
2018-5	Section 3 We use the dataset of Visual Attributes...: drop "dataset"	weakness
2018-5	I think the pre-linguistic objects are not represented by 1-hot, but binary vectors.	weakness
2018-5	do care some inherent structure: carry	weakness
2018-5	Note that symbols in V have no pre-defined semantics...: This is repeated multiple times.	weakness
2018-5	Section 3 I couldn't find simulation details: how many training elements, and how is training accuracy computed?	weakness
2018-5	Also, "training data", "training accuracy" are probably misleading terms, as I suppose you measured performance on new combinations of objects.	weakness
2018-5	I find "Protocol Size" to be a rather counterintuitive term: maybe call Vocabulary Size "Alphabet Size", and Protocol Size "Lexicon Size"?	weakness
2018-5	State in Table 1 caption that the topographic measure will be explained in a later section.	weakness
2018-5	Also, the -1 is confusing: you can briefly mention when you introduce the measure that since you correlate a distance with a similarity you expect an inverse relation?	weakness
2018-5	Also, you mention in the caption that all Spearman rhos are significant, but where are they presented again?	weakness
2018-5	Section 3.2 Does the paragraph starting with "Note that the distractor" refer to a figure or table that is not there?	weakness
2018-5	If not, it should be there, since it's not clear what are the data that support your claims there.	weakness
2018-5	Also, you should explain what the degenerate strategy the agents find is.	weakness
2018-5	Next paragraph: - I find the usage of "obtaining" to refer to the relation between messages and objects strange.	weakness
2018-5	- in which space are the reported pairwise similarities computed?	weakness
2018-5	- make clear that in the non-uniform case confusability is less influenced by similarity since the agents must learn to distinguish between similar objects that naturally co-occur (sheep and goats)	weakness
2018-5	- what is the expected effect on the naturalness of the emerged language?	weakness
2018-5	Section 3.3 adhere to, the ability to: "such as" missing?	weakness
2018-5	Is the unigram chimera distribution inferred from the statistics over the distribution of properties across all concepts or what?	weakness
2018-5	(please clarify.) In Tables 2 and 3, why is vocabulary size missing?	weakness
2018-5	In Table 2, say that the protocol size columns report novel message percentage **for the "test" conditions***	weakness
2018-5	Figure 2: spelling of Levensthein	weakness
2018-5	Section 3.3.2 while for languages (c,d)... something missing.	weakness
2018-5	with a randomly initialized...: no a	weakness
2018-5	More importantly, I don't understand this "random" setup: if architecture was fixed and randomly initialized, how could something be learned about the structure of the data?	weakness
2018-5	Section 4 Refer to the images the agents must communicate about as "scenes", since objects are just a component of them.	weakness
2018-5	What are the absolute sizes of train and test splits?	weakness
2018-5	Section 4.1 we do not address this issue: the issue	weakness
2018-5	Section 4.2 at least in the game C&D: games	weakness
2018-5	Why is Appendix A containing information that logically follows that in Appendix B?	weakness
2018-5	-------------- Summary: -------------- This paper presents a series of experiments on language emergence through referential games between two agents.	abstract
2018-5	They ground these experiments in both fully-specified symbolic worlds and through raw, entangled, visual observations of simple synthetic scenes.	strength
2018-5	They provide rich analysis of the emergent languages the agents produce under different experimental conditions.	strength
2018-5	This analysis (especially on raw pixel images) make up the primary contribution of this work.	strength
2018-5	-------------- Evaluation: -------------- Overall I think the paper makes some interesting contributions with respect to the line of recent 'language emergence' papers.	strength
2018-5	The authors provide novel analysis of the learned languages and perceptual system across a number of environmental settings, coming to the (perhaps uncontroversial) finding that varying the environment and restrictions on language result in variations in the learned communication protocols.	abstract
2018-5	In the context of existing literature, the novelty of this work is somewhat limited -- consisting primarily of the extension of multi-agent reference games to raw-pixel inputs.	weakness
2018-5	While this is a non-trivial extension, other works have demonstrated language learning in similar referring-expression contexts (essentially modeling only the listener model [Hermann et.al 2017]).	weakness
2018-5	I have a number of requests for clarification in the weaknesses section which I think would improve my understanding of this work and result in a stronger submission if included by the authors.	suggestion
2018-5	-------------- Strengths: -------------- - Clear writing and document structure.	strength
2018-5	- Extensive experimental setting tweaks which ablate the information and regularity available to the agents.	strength
2018-5	The discussion of the resulting languages is appropriate and provides some interesting insights.	strength
2018-5	- A number of novel analyses are presented to evaluate the learned languages and perceptual systems.	strength
2018-5	-------------- Weaknesses: -------------- - How stable are the reported trends / languages across multiple runs within the same experimental setting?	weakness
2018-5	The variance of REINFORCE policy gradients (especially without a baseline) plus the general stochasticity of SGD on randomly initialized networks leads me to believe that multiple training runs of these agents might result is significantly different codes / performance.	weakness
2018-5	I am interested in hearing the author's experiences in this regard and if multiple runs present similar quantitative and qualitative results.	suggestion
2018-5	I admit that expecting identical codes is unrealistic, but the form of the codes (i.e. primarily encoding position) might be consistent even if the individual mappings are not).	weakness
2018-5	- I don't recall seeing descriptions of the inference-time procedure used to evaluate training / test accuracy.	weakness
2018-5	I will assume argmax decoding for both speaker and listener.	weakness
2018-5	Please clarify or let me know if I missed something.	weakness
2018-5	- There is ambiguity in how the "protocol size" metric is computed.	weakness
2018-5	In Table 1, it is defined as 'the effective number of unique message used'.	weakness
2018-5	This comes back to my question about decoding I suppose, but does this count the 'inference-time' messages or those produced during training?	weakness
2018-5	Furthermore, Table 2 redefines "protocol size" as the percentage of novel message.	weakness
2018-5	I assume this is an editing error given the values presented and take these columns as counts.	weakness
2018-5	It also seems "protocol size" is replaced with the term "lexicon" from 4.1 onward.	weakness
2018-5	- I'm surprised by how well the agents generalize in the raw pixel data experiments.	weakness
2018-5	In fact, it seems that across all games the test accuracy remains very close to the train accuracy.	weakness
2018-5	Given the dataset is created by taking all combinations of color / shape and then sampling 100 location / floor color variations, it is unlikely that a shape / color combo has not been seen in training.	weakness
2018-5	Such that the only novel variations are likely location and floor color.	weakness
2018-5	However, taking Game A as an example, the probe classifiers are relatively poor at these attributes -- indicating the speaker's representation is not capturing these attributes well.	weakness
2018-5	Then how do the agents effectively differentiate so well between 20 images leveraging primarily color and shape?	weakness
2018-5	I think some additional analysis of this setting might shed some light on this issue.	suggestion
2018-5	One thought is to compute upper-bounds based on ground truth attributes.	suggestion
2018-5	Consider a model which knows shape perfectly, but cannot predict other attributes beyond chance.	suggestion
2018-5	To compute the performance of such a model, you could take the candidate set, remove any instances not matching the ground truth shape, and then pick randomly from the remaining instances.	suggestion
2018-5	Something similar could be repeated for all attributes independently as well as their combinations -- obviously culminating in 100% accuracy given all 4.	suggestion
2018-5	It could be that by dataset construction, object location and shape are sufficient to achieve high accuracy because the odds of seeing the same shape at the same location (but different color) is very low.	suggestion
2018-5	Given these are operations on annotations and don't require time-consuming model training, I hope to see this analysis in the rebuttal to put the results into appropriate context.	suggestion
2018-5	- What is random chance for the position and floor color probe classifiers?	suggestion
2018-5	I don't think it is mentioned how many locations / floor colors are used in generation.	weakness
2018-5	- Relatively minor complaint: Both agents are trained via the REINFORCE policy gradient update rule; however, the listener agent makes a fairly standard classification decision and could be trained with a standard cross-entropy loss.	weakness
2018-5	That is to say, the listener policy need not make intermediate discrete policy decisions.	weakness
2018-5	This decision to withhold available supervision is not discussed in the paper (as far as I noticed), could the authors speak to this point?	weakness
2018-5	-------------- Curiosities: -------------- - I got the impression from the results (specifically the lack of discussion about message length) that in these experiments agents always issued full length messages even though they did not need to do so.	weakness
2018-5	If true, could the authors give some intuition as to why?	weakness
2018-5	If untrue, what sort of distribution of lengths do you observe?	weakness
2018-5	- There is no long term planning involved in this problem, so why use reinforcement learning over some sort of differentiable sampler?	weakness
2018-5	With some re-parameterization (i.e. Gumbel-Softmax), this model could be end-to-end differentiable.	suggestion
2018-5	-------------- Minor errors: -------------- [2.2 paragraph 1] LSTM citation should not be in inline form.	weakness
2018-5	[3 paragraph 1] 'Note that these representations do care some' -> carry	weakness
2018-5	[3.3.1 last paragraph] 'still able comprehend' --> to	rating_summary
2018-5	------- Edit ------- Updating rating from 6 to 7.	rating_summary
2018-5	This paper presents an analysis of the communication systems that arose when neural network based agents played simple referential games.	abstract
2018-5	The set up is that a speaker and a listener engage in a game where both can see a set of possible referents (either represented symbolically in terms of features, or represented as simple images) and the speaker produces a message consisting of a sequence of numbers while the listener has to make the choice of which referent the speaker intends.	abstract
2018-5	This is a set up that has been used in a large amount of previous work, and the authors summarize some of this work.	abstract
2018-5	The main novelty in this paper is the choice of models to be used by speaker and listener, which are based on LSTMs and convolutional neural networks.	strength
2018-5	The results show that the agents generate effective communication systems, and some analysis is given of the extent to which these communications systems develop compositional properties – a question that is currently being explored in the literature on language creation.	strength
2018-5	This is an interesting question, and it is nice to see worker playing modern neural network models to his question and exploring the properties of the solutions of the phone.	strength
2018-5	However, there are also a number of issues with the work.	misc
2018-5	1. One of the key question is the extent to which the constructed communication systems demonstrate compositionality.	weakness
2018-5	The authors note that there is not a good quantitative measure of this.	weakness
2018-5	However, this is been the topic of much research of the literature and language evolution.	weakness
2018-5	This work has resulted in some measures that could be applied here, see for example Carr et al. (2016): http://www.research.ed.ac.uk/portal/files/25091325/Carr_et_al_2016_Cognitive_Science.pdf	suggestion
2018-5	2. In general the results occurred be more quantitative.	weakness
2018-5	In section 3.3.2 it would be nice to see statistical tests used to evaluate the claims.	suggestion
2018-5	Minimally I think it is necessary to calculate a null distribution for the statistics that are reported.	suggestion
2018-5	3. As noted above the main novelty of this work is the use of contemporary network models.	strength
2018-5	One of the advantages of this is that it makes it possible to work with more complex data stimuli, such as images.	strength
2018-5	However, unfortunately the image example that is used is still very artificial being based on a small set of synthetically generated images.	weakness
2018-5	Overall, I see this as an interesting piece of work that may be of interest to researchers exploring questions around language creation and language evolution, but I think the results require more careful analysis and the novelty is relatively limited, at least in the way that the results are presented here.	weakness

2018-41	Summary: The paper presents a modification of the Winograd convolution algorithm that enables a reduction of multiplications in a forward pass of 10.8x almost without loss of accuracy.	abstract
2018-41	This modification combines the reduction of multiplications achieved by the Winograd convolution algorithm with weight pruning in the following way: - weights are pruned after the Winograd transformation, to prevent the transformation from filling in zeros, thus preserving weight sparsity	abstract
2018-41	- the ReLU activation function associated with the previous layer is applied to the Winograd transform of the input activations, not directly to the spatial-domain activations, also yielding sparse activations	weakness
2018-41	This way sparse multiplication can be performed.	abstract
2018-41	Because this yields a network, which is not mathematically equivalent to a vanilla or Winograd CNN, the method goes through three stages: dense training, pruning and retraining.	abstract
2018-41	The authors highlight that a dimension increase in weights and ReLU activations provide a more powerful representation and that stable dynamic activation densities over layer depths benefit the representational power of ReLU layers.	abstract
2018-41	Review: The paper shows good results using the proposed method and the description is easy to follow.	strength
2018-41	I particularly like Figure 1.	misc
2018-41	I only have a couple of questions/comments: 1) I'm not familiar with the term m-specific ("Matrices B, G and A are m-specific.") and didn't find anything that seemed related in a very quick google search.	weakness
2018-41	Maybe it would make sense to add at least an informal description.	suggestion
2018-41	2) Although small filters are the norm, you could add a note, describing up to what filter sizes this method is applicable.	suggestion
2018-41	Or is it almost exactly the same as for general Winograd CNNs?	weakness
2018-41	3) I think it would make sense to mention weight and activation quantization in the intro as well (even if you leave a combination with quantization for future work), e.g. Rastegari et al. (2016), Courbariaux et al. (2015) and Lin et al. (2015)	suggestion
2018-41	4) Figure 5 caption has a typo: "acrruacy"	weakness
2018-41	References: Courbariaux, Matthieu, Yoshua Bengio, and Jean-Pierre David.	misc
2018-41	"Binaryconnect: Training deep neural networks with binary weights during propagations." In Advances in Neural Information Processing Systems, pp.	misc
2018-41	3123-3131. 2015. Lin, Zhouhan, Matthieu Courbariaux, Roland Memisevic, and Yoshua Bengio.	misc
2018-41	"Neural networks with few multiplications." arXiv preprint arXiv:1510.03009 (2015).	misc
2018-41	Rastegari, Mohammad, Vicente Ordonez, Joseph Redmon, and Ali Farhadi.	misc
2018-41	"Xnor-net: Imagenet classification using binary convolutional neural networks." In European Conference on Computer Vision, pp.	misc
2018-41	525-542. Springer International Publishing, 2016. This paper proposes to combine Winograd transformation with sparsity to reduce the computation for deep convolutional neural network.	abstract
2018-41	Specifically, ReLU nonlinearity was moved after Winograd transformation to increase the dynamic sparsity in the Winograd domain, while an additional pruning on low magnitude weights and re-training procedure based on pruning is used to increase static sparsity of weights, which decreases computational demand.	abstract
2018-41	The resulting Winograd-ReLU CNN shows strong performance in three scenarios (CIFAR10 with VGG, CIFAR100 with ConvPool-CNN-C, and ImageNEt with ResNet-18).	abstract
2018-41	The proposed method seems to improve over the two baseline approaches (Winograd and sparsity, respectively).	abstract
2018-41	Overall, the paper is well-written and the experiments seems to be quite thorough and clear.	strength
2018-41	Note that I am not an expert in this field and I might miss important references along this direction.	misc
2018-41	I am leaving it to other reviewers to determine its novelty.	misc
2018-41	Putting ReLU in the Winograd domain (or any transformed domain, e.g., Fourier) seems to be an interesting idea, and deserves some further exploration.	strength
2018-41	Also, I am curious about the performance after weight pruning but before retraining).	weakness
2018-41	This paper proposes a method to build a CNN in the Winograd domain, where weight pruning and ReLU can be applied in this domain to improve sparsity and reduce the number of multiplication.	abstract
2018-41	The resultant CNN can achieve ~10x theoretical speedup with little performance loss.	abstract
2018-41	The paper is well-written. It provides a new way to combine the Winograd transformation and the threshold-based weight pruning strategy.	strength
2018-41	Rather than strictly keeping the architecture of ordinary CNNs, the proposed method applied ReLU to the transform domain, which is interesting.	strength
2018-41	The results on Cifar-10 and ImageNet are promising.	strength
2018-41	In particular, the pruned model in the Winograd domain performs comparably to the state-of-the-art dense neural networks and shows significant theoretical speedup.	strength
2018-41	The results on ImageNet using ResNet-18 architecture are also promising.	strength
2018-41	However, no results are provided for deeper networks, so it is unclear how this method can benefit the computation of very deep neural networks	weakness
2018-41	A general limitation of the proposed method is the network architecture inconsistency with the ordinary CNNs. Due to the location change of ReLUs, it is unclear how to transform a pretrained ordinary CNNs to the new architectures accurately.	weakness
2018-41	It seems training from scratch using the transformed architectures is the simplest solution.	weakness
2018-41	The paper does not report the actual speedup in the wall clock time.	weakness
2018-41	The actual implementation is what matters in the end.	weakness
2018-41	It will be more informative to present Figure 2,3,4 with respect to the workload in addition to the weight density.	suggestion

2018-104	This paper proposes an approach to generating the first section of Wikipedia articles (and potentially entire articles).	abstract
2018-104	First relavant paragraphs are extracted from reference documents and documents retrieved through search engine queries through a TD-IDF-based ranking.	abstract
2018-104	Then abstractive summarization is performed using a modification of Transformer networks (Vasvani et al 2017).	abstract
2018-104	A mixture of experts layer further improves performance.	abstract
2018-104	The proposed transformer decoder defines a distribution over both the input and output sequences using the same self-attention-based network.	abstract
2018-104	On its own this modification improves perplexity (on longer sequences) but not the Rouge score; however the architecture enables memory-compressed attention which is more scalable to long input sequences.	abstract
2018-104	It is claimed that the transformer decoder makes optimization easier but no complete explanation or justification of this is given.	weakness
2018-104	Computing self-attention and softmaxes over entire input sequences will significantly increase the computational cost of training.	weakness
2018-104	In the task setup the information retrieval-based extractive stage is crucial to performance, but this contribution might be less important to the *CONF* community.	weakness
2018-104	It willl also be hard to reproduce without significant computational resources, even if the URLs of the dataset are made available.	weakness
2018-104	The training data is significantly larger than the CNN/DailyMail single-document summarization dataset.	weakness
2018-104	The paper presents strong quantitative results and qualitative examples.	strength
2018-104	Unfortunately it is hard to judge the effectiveness of the abstractive model due to the scale of the experiments, especially with regards to the quality of the generated output in comparison to the output of the extractive stage.	weakness
2018-104	In some of the examples the system output seems to be significantly shorter than the reference, so it would be helpful to quantify this, as well how much the quality degrades when the model is forced to generate outputs of a given minimum length.	weakness
2018-104	While the proposed approach is more scalable, it is hard to judge the extend of this.	weakness
2018-104	So while the performance of the overall system is impressive, it is hard to judge the significance of the technical contribution made by the paper.	weakness
2018-104	--- The additional experiments and clarifications in the updated version give substantially more evidence in support of the claims made by the paper, and I would like to see the paper accepted.	decision
2018-104	The main significance of this paper is to propose the task of generating the lead section of Wikipedia articles by viewing it as a multi-document summarization problem.	abstract
2018-104	Linked articles as well as the results of an external web search query are used as input documents, from which the Wikipedia lead section must be generated.	abstract
2018-104	Further preprocessing of the input articles is required, using simple heuristics to extract the most relevant sections to feed to a neural abstractive summarizer.	abstract
2018-104	A number of variants of attention mechanisms are compared, including the transofer-decoder, and a variant with memory-compressed attention in order to handle longer sequences.	abstract
2018-104	The outputs are evaluated by ROUGE-L and test perplexity.	abstract
2018-104	There is also a A-B testing setup by human evaluators to show that ROUGE-L rankings correspond to human preferences of systems, at least for large ROUGE differences.	abstract
2018-104	This paper is quite original and clearly written.	strength
2018-104	The main strength is in the task setup with the dataset and the proposed input sources for generating Wikipedia articles.	strength
2018-104	The main weakness is that I would have liked to see more analysis and comparisons in the evaluation.	weakness
2018-104	Evaluation: Currently, only neural abstractive methods are compared.	weakness
2018-104	I would have liked to see the ROUGE performance of some current unsupervised multi-document extractive summarization methods, as well as some simple multi-document selection algorithms such as SumBasic.	suggestion
2018-104	Do redundancy cues which work for multi-document news summarization still work for this task?	suggestion
2018-104	Extractiveness analysis: I would also have liked to see more analysis of how extractive the Wikipedia articles actually are, as well as how extractive the system outputs are.	suggestion
2018-104	Does higher extractiveness correspond to higher or lower system ROUGE scores?	suggestion
2018-104	This would help us understand the difficulty of the problem, and how much abstractive methods could be expected to help.	suggestion
2018-104	A further analysis which would be nice to do (though I have less clear ideas how to do it), would be to have some way to figure out which article types or which section types are amenable to this setup, and which are not.	suggestion
2018-104	I have some concern that extraction could do very well if you happen to find a related article in another website which contains encyclopedia-like or definition-like entries (e.g., Baidu, Wiktionary) which is not caught by clone detection.	weakness
2018-104	In this case, the problem could become less interesting, as no real analysis is required to do well here.	weakness
2018-104	Overall, I quite like this line of work, but I think the paper would be a lot stronger and more convincing with some additional work.	weakness
2018-104	---- After reading the authors' response and the updated submission, I am satisfied that my concerns above have been adequately addressed in the new version of the paper.	rebuttal_process
2018-104	This is a very nice contribution.	strength
2018-104	This paper considers the task of generating Wikipedia articles as a combination of extractive and abstractive multi-document summarization task where input is the content of reference articles listed in a Wikipedia page along with the content collected from Web search and output is the generated content for a target Wikipedia page.	abstract
2018-104	The authors at first reduce the input size by using various extractive strategies and then use the selected content as input to the abstractive stage where they leverage the Transformer architecture with interesting modifications like dropping the encoder and proposing alternate self-attention mechanisms like local and memory compressed attention.	abstract
2018-104	In general, the paper is well-written and the main ideas are clear.	strength
2018-104	However, my main concern is the evaluation.	weakness
2018-104	It would have been nice to see how the proposed methods perform with respect to the existing neural abstractive summarization approaches.	suggestion
2018-104	Although authors argue in Section 2.1 that existing neural approaches are applied to other kinds of datasets where the input/output size ratios are smaller,  experiments could have been performed to show their impact.	rebuttal_process
2018-104	Furthermore, I really expected to see a comparison with Sauper & Barzilay (2009)'s non-neural extractive approach of Wikipedia article generation, which could certainly strengthen the technical merit of the paper.	suggestion
2018-104	More importantly, it was not clear from the paper if there was a constraint on the output length when each model generated the Wikipedia content.	weakness
2018-104	For example, Figure 5-7 show variable sizes of the generated outputs.	suggestion
2018-104	With a fixed reference/target Wikipedia article, if different models generate variable sizes of output, ROUGE evaluation could easily pose a bias on a longer output as it essentially counts overlaps between the system output and the reference.	weakness
2018-104	It would have been nice to know if the proposed attention mechanisms account for significantly better results than the T-ED and T-D architectures.	suggestion
2018-104	Did you run any statistical significance test on the evaluation results?	suggestion
2018-104	Authors claim that the proposed model can generate "fluent, coherent" output, however, no evaluation has been conducted to justify this claim.	weakness
2018-104	The human evaluation only compares two alternative models for preference, which is not enough to support this claim.	weakness
2018-104	I would suggest to carry out a DUC-style user evaluation (http://www-nlpir.nist.gov/projects/duc/duc2007/quality-questions.txt) methodology to really show that the proposed method works well for abstractive summarization.	suggestion
2018-104	Does Figure 8 show an example input after the extractive stage or before?	suggestion
2018-104	Please clarify. --------------- I have updated my scores as authors clarified most of my concerns.	rebuttal_process

2018-127	Thanks for addressing most of the issues.	misc
2018-127	I changed my given score from 3 to 6.	rating_summary
2018-127	Summary: This work explores the use of learned compressed image representation for solving 2 computer vision tasks without employing a decoding step.	abstract
2018-127	The paper claims to be more computationally and memory efficient compared to the use of original or the decompressed images.	abstract
2018-127	Results are presented on 2 datasets "Imagenet" and "PASCAL VOC 2012".	abstract
2018-127	They also jointly train the compression and classification together and empirically shows it can improve both classification and compression together.	abstract
2018-127	Pros: + The idea of learning from a compressed representation is a very interesting and beneficial idea for large-scale image understanding tasks.	strength
2018-127	Cons: - The paper is too long (13 pages + 2 pages of references).	weakness
2018-127	The suggested standard number of pages is 8 pages + 1 page of references.	weakness
2018-127	There are many parts that are unnecessary in the paper and can be summarized.	weakness
2018-127	Summarizing and rewording them makes the paper more consistent and easier to read: ( 1.	weakness
2018-127	A very long introduction about the benefits of inferring from the compressed images and examples.	weakness
2018-127	2. A large part of the intro and Related work can get merged.	weakness
2018-127	3. Experimental setup part is long but not well-explained and is not self-contained particularly for the evaluation metrics.	weakness
2018-127	"Please briefly explain what MS-SSIM, SSIM, and PSNR stand for".	weakness
2018-127	There is a reference to the Agustsson et al 2017 paper	weakness
2018-127	"scalar quantization", which is not well explained in the paper.	weakness
2018-127	It is better to remove this part if it is not an important part or just briefly but clearly explain it.	suggestion
2018-127	4. Fig. 4 is not necessary.	weakness
2018-127	4.3 contains extra information and could be summarized in a more consistent way.	weakness
2018-127	5. Hyperparameters that are applied can be summarized in a small table or just explain the difference between the architectures that are used.)	weakness
2018-127	- There are parts of the papers which are confusing or not well-written.	weakness
2018-127	It is better to keep the sentences short and consistent: E.g: subsection 3.2, page 5: "To adapt the ResNet … where k is the number of … layers of the network" can be changed to 3 shorter sentences, which is easier to follow.	weakness
2018-127	There are some typos: e.g: part 3.1, fever ---> fewer,	weakness
2018-127	- As it is mentioned in the paper, solving a Vision problem directly from a compressed image, is not a novel method (e.g: DCT coefficients were used for both vision and audio data to solve a task without any decompression).	weakness
2018-127	However, applying a deep representation for the compression and then directly solving a vision task (classification and segmentation) can be considered as a novel idea.	weakness
2018-127	- In the last part of the paper, both compression and classification parts are jointly trained, and it is empirically presented that both results improved by jointly training them.	weakness
2018-127	However, to me, it is not clear if the trained compression model on this specific dataset and for the task of classification can work well for other datasets or other tasks.	weakness
2018-127	The experimental setup and the figures are not well explained and well written.	weakness
2018-127	Neural-net based image compression is a field which is about to get hot, and this paper asks the obvious question: can we design a neural-net based image compression algorithm such that the features it produces are useful for classification & segmentation?	abstract
2018-127	The fact that it's an obvious question does not mean that it's a question that's worthless.	abstract
2018-127	In fact, I am glad someone asked this question and tried to answer it.	rebuttal_process
2018-127	Pros: - Clear presentation, easy to follow.	strength
2018-127	- Very interesting, but obvious, question is explored.	strength
2018-127	- The paper is very clear, and uses building blocks which have been analyzed before, which leaves the authors free to explore their interactions rather than each individual building block's property.	strength
2018-127	- Results are shown on two tasks (classification / segmentation) rather than just one (the obvious one would have been to only discuss results on classification), and relatively intuitive results are shown (i.e., more bits = better performance).	strength
2018-127	What is perhaps not obvious is how much impact does doubling the bandwidth have (i.e., initially it means more, then later on it plateaus, but much earlier than expected).	weakness
2018-127	- Joint training of compression + other tasks.	weakness
2018-127	As far as I know this is the first paper to talk about this particular scenario.	weakness
2018-127	- I like the fact that classical codecs were not completely discarded (there's a comparison with JPEG 2K).	weakness
2018-127	- The discussion section is of particular interest, discussing openly the pros/cons of the method (I wish more papers would be as straightforward as this one).	strength
2018-127	Cons: - I would have liked to have a discussion on the effect of the encoder network.	weakness
2018-127	Only one architecture/variant was used.	weakness
2018-127	- For PSNR, SSIM and MS-SSIM I would like a bit more clarity whether these were done channel-wise, or on the grayscale channel.	weakness
2018-127	- While runtime is given as pro, it would be nice for those not familiar with the methods to provide some runtime numbers (i.e., breakdown how much time does it take to encode and how much time does it take to classify or segment, but in seconds, not flops).	weakness
2018-127	For example, Figure 6 could be augmented with actual runtime in seconds.	weakness
2018-127	- I wish the authors did a ctrl+F for "??" and fixed all the occurrences.	suggestion
2018-127	- One of the things that would be cool to add later on but I wished to have beeyn covered is whether it's possible to learn not only to compress, but also downscale.	suggestion
2018-127	In particular, the input to ResNet et al for classification is fixed sized, so the question is -- would it be possible to produced a compact representation to be used for classification given arbitrary image resolutions, and if yes, would it have any benefit?	suggestion
2018-127	General comments: - The classification bits are all open source, which is very good.	strength
2018-127	However, there are very few neural net compression methods which are open sourced.	weakness
2018-127	Would you be inclined to open source the code for your implementation?	weakness
2018-127	It would be a great service to the community if yes (and I realize that it could already be open sourced -- feel free to not answer if it may lead to break anonymity, but please take this into consideration).	suggestion
2018-127	This is a well-written and quite clear work about how a previous work on image compression using deep neural networks can be extended to train representations which are also valid for semantic understanding.	strength
2018-127	IN particular, the authors tackle the classic and well-known problems of image classification and segmentation.	strength
2018-127	The work evolves around defining a loss function which initially considers only a trade-off between reconstruction error and total bit-rate.	abstract
2018-127	The representations trained with the loss function, at three different operational points, are used as inputs for variations of ResNet (image classification) and DeepLab (segmentation).	abstract
2018-127	The results obtained are similar to a ResNet trained directly over the RGB images, and actually with a slight increase of performance in segmentation.	abstract
2018-127	The most interesting part is a joint training for both compression and image classification.	strength
2018-127	PROS P.1 Joint training for both compression and classification.	strength
2018-127	First time to the authors knowledge.	strength
2018-127	P.2 Performance on classification and segmentation tasks are very similar when compared to the non-compressed case with state-of-the-art ResNet architectures.	strength
2018-127	P.3 Text is very clear.	strength
2018-127	P.4 Experimentation is exhaustive and well-reported.	strength
2018-127	CONS C1. The authors fail into providing a better background regarding the metrics MS-SSIM and SSIM (and PSNR, as well) and their relation to the MSE used for training the network.	weakness
2018-127	Also, I missed an explanation about whether high or low values for them are beneficial, as actually results compared to JPEG and JPEG-2000 differ depending on the experiment.	weakness
2018-127	C2. The main problem is of the work is that, while the whole argument is that in an indexing system it would not be necessary to decompress the representation coded with a DNN, in terms of computation JPEG2000 (and probably JPEG) are much lighter that coding with DNN, even if considering both the compression and decompression.	weakness
2018-127	The authors already point at another work where they explore the efficient compression with GPUs, but this point is the weakest one for the adoption of the proposed scheme.	weakness
2018-127	C3. The paper exceeds the recommendation of 8 pages and expands up to 13 pages, plus references.	weakness
2018-127	An effort of compression would be advisable, moving some of the non-core results to the appendixes.	suggestion
2018-127	QUESTIONS Q1. Do you have any explanation for the big jumps on the plots of Figure 5 ?	suggestion
2018-127	Q2. Did you try a joint training for the segmentation task as well ?	suggestion
2018-127	Q3. Why are the dots connected in Figure 10, but not in Figure 11.	suggestion
2018-127	Q4. Actually results in Figure 10 do not seem good...	weakness
2018-127	or maybe I am not understanding them properly.	weakness
2018-127	This is related to C1.	weakness
2018-127	Q5. There is a broken reference in Section 5.3.	weakness
2018-127	Please fix.	suggestion

2018-149	SUMMARY The paper considers the problem of using cycle GANs to decipher text encrypted with historical ciphers.	abstract
2018-149	Also it presents some theory to address the problem that discriminating between the discrete data and continuous prediction is too simple.	abstract
2018-149	The model proposed is a variant of the cycle GAN in which in addition embeddings helping the Generator are learned for all the values of the discrete variables.	abstract
2018-149	The log loss of the GAN is replaced by a quadratic loss and a regularization of the Jacobian of the discriminator.	abstract
2018-149	Experiments show that the method is very effective.	abstract
2018-149	REVIEW The paper considers an interesting and fairly original problem and the overall discussion of ciphers is quite nice.	strength
2018-149	Unfortunately, my understanding is that the theory proposed in section 2 does not correspond to the scheme used in the experiments (contrarily to what the conclusion suggest and contrarily to what the discussion of the end of section 3, which says that using embedding is assumed to have an equivalent effect to using the methodology considered in the theoretical part).	weakness
2018-149	Another important concern is with the proof: there seems to be an unmotivated additional assumption that appears in the middle of the proof of Proposition 1 + some steps need to be clarified (see comment 16 below).	weakness
2018-149	The experiments do not have any simple baseline, which is somewhat unfortunate.	weakness
2018-149	DETAILED COMMENTS: 1- The paper makes a few bold and debatable statements: line 9 of section 1	weakness
2018-149	"Such hand-crafted features have fallen out of favor (Goodfellow et al., 2016) as a result of their demonstrated inferiority to features learned directly from data in end-to-end learning frameworks such as neural networks"	weakness
2018-149	This is certainly an overstatement and although it might be true for specific types of inputs it is not universally true, most deep architectures rely on a human-in-the-loop and there are number of areas where human crafted feature are arguably still relevant, if only to specify what is the input of a deep network: there are many domains where the notion of raw data does not make sense, and, when it does, it is usually associated with a sensing device that has been designed by a human and which implicitly imposes what the data is based on human expertise.	weakness
2018-149	2- In the last paragraph of the introduction, the paper says that previous work has only worked on vocabularies of 26 characters while the current paper tackles word level ciphers with 200 words.	weakness
2018-149	But, isn't this just a matter of scalability and only possible with very large amounts of text?	weakness
2018-149	Is it really because of an intrinsic limitation or lack of scalability of previous approaches or just because the authors of the corresponding papers did not care to present larger scale experiments?	weakness
2018-149	3- The discussion at the top of page 5 is difficult to follow.	weakness
2018-149	What do you mean when you say "this motivates the benefits of having strong curvature globally, as opposed to linearly between etc"	weakness
2018-149	Which curvature are we talking about?	weakness
2018-149	and what how does the "as opposed to linearly" mean?	weakness
2018-149	Should we understand "as opposed to having curvature linearly interpolated between etc" or "as opposed to having a linear function"?	weakness
2018-149	Please clarify. 4- In the same paragraph: what does "a region that has not seen the Jacobian norm applied to it" mean?	weakness
2018-149	How is a norm applied to a region?	weakness
2018-149	I guess that what you mean is that the generator G might creates samples in a part of the space where the function F has not yet been learned and is essentially close to 0.	weakness
2018-149	Is this what you mean?	weakness
2018-149	5- I do not understand why the paper introduces WGAN since in the end it does not use them but uses a quadratic loss, introduced in the first display of section 4.3.	weakness
2018-149	6- The paper makes a theoretical contribution which supports replacing the sample y by a sample drawn from a region around y.	weakness
2018-149	But it seems that this is not used in the experiment and that the authors consider that the introduction of the embedding is a substitution for this.	weakness
2018-149	Indeed, in the last paragraph of section 3.1, the paper says "we make the assumption that the training of the embedding vectors approximates random sampling similar to what is described in Proposition 1".	weakness
2018-149	This does not make any sense to me because the embedding vectors map each y deterministically to a single point, and so the distribution on the corresponding vectors is still a fixed discrete distribution.	weakness
2018-149	This gives me this impression that the proposed theory does not match what is used in the experiments.	weakness
2018-149	(The last sentence of section 3.1, which is commenting on this and could perhaps clarify the situation is ill formed with two verbs.)	weakness
2018-149	7- In the definitions: "A discriminator is said to perform uninformative discrimination" etc.	weakness
2018-149	-> It seems that the choice of the word uninformative would be misleading: an uninformative discrimination would be a discrimination that completely fails, while what the condition is saying it that it cannot perform perfect discrimination.	weakness
2018-149	I would thus suggest to call this "imperfect discrimination".	weakness
2018-149	8- It seems that the same embedding is used in X space and in Y space (from equations 6 and 7).	weakness
2018-149	Is there any reason for that?	weakness
2018-149	I would seem more natural to me to introduce two different embeddings since the objects are a priori different...	weakness
2018-149	Actually I don't understand how the embeddings can be the same in the Vignere code case since time taken into account one one side.	weakness
2018-149	9- On the 5th line after equation (7), the paper says "the embeddings...	weakness
2018-149	are trained to minimize L_GAN and L_cyc, meaning...	weakness
2018-149	and are easy to discriminate" -> This last part of the sentence seems wrong to me.	weakness
2018-149	The discriminator is trying to maximize L_GAN and so minimizing w.r.t. to the embedding is precisely trying to prevent to the discriminator to tell apart too easily the true elements from the estimated ones.	weakness
2018-149	In fact the regularization of the Jacobian that will be preventing the discriminator to vary too quickly in space is more likely to explain the fact that the discrimination is not too easy to do between the true and mapped embeddings.	weakness
2018-149	This might be connected to the discussion at the top of page 5.	weakness
2018-149	Since there are no experiments with alpha different than the default value = 10, this is difficult to assess.	weakness
2018-149	10-The Vigenere cipher is explained again at the end of section 4.2 when it has already been presented in section 1.1	weakness
2018-149	11- Concerning results in Table 2: I do not see why it would not be possible to compare the performance of the method with classical frequency analysis, at least for the character case.	weakness
2018-149	12- At the beginning of section 4.3, the text says that the log loss was replaced with the quadratic loss, but without giving any reason.	weakness
2018-149	Could you explain why. 13- The only comparison of results with and without embeddings is presented in the curves of figure 3, for Brown-W with a vocabulary of 200 words.	weakness
2018-149	In that case it helps.	weakness
2018-149	Could the authors report systematically results about all cases?	weakness
2018-149	(I guess this might however be the only hard case...)	misc
2018-149	14- It would be useful to have a brief reminder of the architecture of the neural network (right now the reader is just refered to Zhu et al., 2017): how many layers, how many convolution layers etc.	suggestion
2018-149	The same comment applies for the way the position of the letter/word in the text appear is in encoded in a feature that is provided as input to the neural network: it would be nice if the paper could provide a few details here and be more self contained.	suggestion
2018-149	(The fact that the engineering of the time feature can "dramatically" improve the performance of the network should be an argument to convince the authors that hand-crafted feature have not fallen out of favor completely yet...)	suggestion
2018-149	15- I disagree with the statement made in the conclusion that the proposed work "empirically confirms [...] that the use of continuous relaxation of discrete variable facilitates [...] and prevents [...]" because for me the proposed implementation does not use at all the theoretical idea of continuous relaxation proposed in the paper, unless there is a major point that I am missing.	weakness
2018-149	16- I have two issues with the proof in the appendix a) after the first display of the last page the paper makes an additional assumption which is not announced in the statement of the theorem, which is that two specific inequality hold...	weakness
2018-149	Unless I am mistaken this assumption is never proven (later or earlier).	weakness
2018-149	Given that this inequality is just "the right inequality to get the proof go through" and given that there are no explanation for why this assumption is reasonable, to me this invalidates the proof.	weakness
2018-149	The step of going from G(S_y) to S_(G(y)) seems delicate...	weakness
2018-149	b) If we accept these inequalities, the determinant of the Jacobian (the notation is not defined) of F at (x_bar) disappears from the equations, as if it could be assumed to be greater than one.	weakness
2018-149	If this is indeed the case, please provide a justification of this step.	weakness
2018-149	17- A way to address the issue of trivial discrimination in GANs with discrete data has been proposed in	weakness
2018-149	Luc, P., Couprie, C., Chintala, S., & Verbeek, J.	misc
2018-149	(2016). Semantic segmentation using adversarial networks.	misc
2018-149	arXiv preprint arXiv:1611.08408. The authors should probably reference this paper.	suggestion
2018-149	18- Clarification of the Jacobian regularization: in equation (3), the Jacobian computed seems to be w.r.t D composed with F while in equation (8) it is only the Jacobian of D.	weakness
2018-149	Which equation is the correct one?	weakness
2018-149	TYPOS: Proposition 1: the if-then statement is broken into two sentences separated by a full point and a carriage return.	weakness
2018-149	sec. 4.3 line 10 we use a cycle loss *with a regularization coefficient* lambda=1 (a piece of the sentence is missing)	weakness
2018-149	sec. 4.3 lines 12-13 the learning rates given are the same at startup and after "warming up"...	weakness
2018-149	In the appendix: 3rd line of proof of prop 1: I don' understand "countably infinite finite sequences of vectors lying in the vertices of the simplex" -> what is countable infinite here?	weakness
2018-149	The vertices? The paper shows an application of GANs to deciphering text.	weakness
2018-149	The goal is to arrive at a ```"hands free" approach to this problem; i.e., an approach that does not require any knowledge of the language being deciphered such as letter frequency and such.	weakness
2018-149	The authors start from a CycleGAN architecture, which may be used to learn mapping between two probability spaces.	weakness
2018-149	They point out that using GANs for discrete distributions is a challenging problem since it can lead to uninformative discriminants.	weakness
2018-149	They propose to  resolve this issue by using a continuous embedding space to approximate (or convert) the discrete random variables into continuous random variables.	weakness
2018-149	The new proposed algorithm, called CipherGAN, is then shown to be stable and achieve deciphering of substitution ciphers and Vigenere ciphers.	weakness
2018-149	I did not completely understand how the embedding was performed, so perhaps the authors could elaborate on that a bit more.	weakness
2018-149	Apart from that, the paper is well written and well motivated.	strength
2018-149	It used some recent ideas in deep learning such as Cycle GANs and shows how to tweak them to make them work for discrete problems and also make them more stable.	strength
2018-149	One comment would be that the paper is decidedly an applied paper (and not much theory) since certain steps in the algorithm (such as training the discriminator loss along with the Lipschitz conditioning term) are included because it was experimentally  observed to lead to stability.	weakness
2018-149	The paper proposed to replace the 2-dim convolutions in CycleGAN by one dimension variant and reduce the filter sizes to 1, while leave the generator convex embedding and using L2 loss function.	abstract
2018-149	The proposed simple change help with the dealing of discrete GAN.	abstract
2018-149	The benefit of increased stability by adding Jacobian norm regularization term to the discriminator's loss is nice.	strength
2018-149	The paper is well written.	strength
2018-149	A few minor ones to improve: * The original GAN was proposed/stated as min_max, while in Equation 1 didn't defined F and was not clear about min_{F}.	weakness
2018-149	Similar for Equations 2 and 3.	misc
2018-149	* Define abbreviation when first appear, e.g. WGAN (Wasserstein ...).	suggestion
2018-149	* Clarify x- and y- axis label in Figure 3.	suggestion

2018-150	The paper proposes a way to speed up the inference time of RNN via Skim mechanism where only a small part of hidden variable is updated once the model has decided a corresponding word token seems irrelevant w.r.t. a given task.	abstract
2018-150	While the proposed idea might be too simple, the authors show the importance of it via thorough experiments.	abstract
2018-150	It also seems to be easily integrated into existing RNN systems without heavy tuning as shown in the experiments.	abstract
2018-150	* One advantage of proposed idea claimed against the skip-RNN is that the Skim-RNN can generate the same length of output sequence given input sequence.	abstract
2018-150	It is not clear to me whether the output prediction on those skimmed tokens is made of the full hidden state (updated + copied) or a first few dimensions of the hidden state.	abstract
2018-150	I assume that the full hidden states are used for prediction.	suggestion
2018-150	It is somehow interesting because it may mean the prediction heavily depends on small (d') part of the hidden state.	strength
2018-150	In the second and third figures of Figure 10, the model made wrong decisions when the adjacent tokens were both skimmed although the target token was not skimmed, and it might be related to the above assumption.	suggestion
2018-150	In this sense, it would be more beneficial if the skimming happens over consecutive tokens (focus on a region, not on an individual token).	suggestion
2018-150	* This paper would gain more attention from practitioners because of its practical purpose.	weakness
2018-150	In a similar vein, it would be also good to have some comments on training time as well.	weakness
2018-150	In a general situation where there is no need of re-training, training time would be meaningless, however, if one requires updating the model on the fly, it would be also meaningful to have some intuition on training time.	weakness
2018-150	* One obvious way to reduce the computational complexity of RNN is to reduce the size of the hidden state.	weakness
2018-150	In this sense, it makes this manuscript more comprehensive if there are some comparisons with RNNs with limited-sized hidden dimensions (say 10 or 20).	suggestion
2018-150	So that readers can check benefits of the skim RNN against skip-RNN and small-sized RNN.	suggestion
2018-150	Summary: The paper proposes a learnable skimming mechanism for RNN.	abstract
2018-150	The model decides whether to send the word to a larger heavy-weight RNN or a light-weight RNN.	abstract
2018-150	The heavy-weight and the light-weight RNN each controls a portion of the hidden state.	abstract
2018-150	The paper finds that with the proposed skimming method, they achieve a significant reduction in terms of FLOPS.	abstract
2018-150	Although it doesn't contribute to much speedup on modern GPU hardware, there is a good speedup on CPU, and it is more power efficient.	abstract
2018-150	Contribution: - The paper proposes to use a small RNN to read unimportant text.	weakness
2018-150	Unlike (Yu et al., 2017), which skips the text, here the model decides between small and large RNN.	weakness
2018-150	Pros: - Models that dynamically decide the amount of computation make intuitive sense and are of general interests.	strength
2018-150	- The paper presents solid experimentation on various text classification and question answering datasets.	strength
2018-150	- The proposed method has shown reasonable reduction in FLOPS and CPU speedup with no significant accuracy degradation (increase in accuracy in some tasks).	strength
2018-150	- The paper is well written, and the presentation is good.	strength
2018-150	Cons: - Each model component is not novel.	weakness
2018-150	The authors propose to use Gumbel softmax, but does compare other gradient estimators.	weakness
2018-150	It would be good to use REINFORCE to do a fair comparison with (Yu et al., 2017 ) to see the benefit of using small RNN.	suggestion
2018-150	- The authors report that training from scratch results in unstable skim rate, while Half pretrain seems to always work better than fully pretrained ones.	suggestion
2018-150	This makes the success of training a bit adhoc, as one need to actively tune the number of pretraining steps.	weakness
2018-150	- Although there is difference from (Yu et al., 2017), the contribution of this paper is still incremental.	weakness
2018-150	Questions: - Although it is out of the scope for this paper to achieve GPU level speedup, I am curious to know some numbers on GPU speedup.	weakness
2018-150	- One recommended task would probably be text summarization, in which the attended text can contribute to the output of the summary.	weakness
2018-150	Conclusion: - Based on the comments above, I recommend Accept This paper proposes a skim-RNN, which skims unimportant inputs with a small RNN while normally processes important inputs with a standard RNN for fast inference.	decision
2018-150	Pros. - The idea of switching small and standard RNNs for skimming and full reading respectively is quite simple and intuitive.	strength
2018-150	- The paper is clearly written with enough explanations about the proposal method and the novelty.	strength
2018-150	- One of the most difficult problems of this approach (non-differentiable) is elegantly solved by employing gumbel-softmax	strength
2018-150	- The effectiveness (mainly inference speed improvement with CPU) is validated by various experiments.	strength
2018-150	The examples (Table 3 and Figure 6) show that the skimming process is appropriately performed (skimmed unimportant words while fully read relevant words etc.)	strength
2018-150	Cons. - The idea is quite simple and the novelty is incremental by considering the difference from skip-RNN.	weakness
2018-150	- No comments about computational costs during training with GPU (it would not increase the computational cost so much, but gumbel-softmax may require more iterations).	weakness
2018-150	Comments: - Section 1, Introduction, 2nd paragraph: 'peed' -> 'speed'(?)	weakness
2018-150	- Equation (5): It would be better to explain why it uses the Gumbel distribution.	weakness
2018-150	To make (5) behave like argmax, only temperature parameter seems to be enough.	weakness
2018-150	- Section 4.1: What is "global training step"?	weakness
2018-150	- Section 4.2, "We also observe that the F1 score of Skim-LSTM is more stable across different configurations and computational cost.": This seems to be very interesting phenomena.	weakness
2018-150	Is there some discussion of why skim-LSTM is more stable?	weakness
2018-150	- Section 4.2, the last paragraph: "Table 6 shows" -> "Figure 6 shows"	weakness

2018-167	The paper presents a new technique for anomaly detection where the dimension reduction and the density estimation steps are jointly optimized.	abstract
2018-167	The paper is rigorous and ideas are clearly stated.	strength
2018-167	The idea to constraint the dimension reduction to fit a certain model, here a GMM, is relevant, and the paper provides a thorough comparison with recent state-of-the-art methods.	strength
2018-167	My main concern is that the method is called unsupervised, but it uses the class information in the training, and also evaluation.	weakness
2018-167	I'm also not convinced of how well the Gaussian model fits the low-dimensional representation and how well can a neural network compute the GMM mixture memberships.	weakness
2018-167	1. The framework uses the class information, i.e., "only data samples from the normal class are used for training", but it is still considered unsupervised.	weakness
2018-167	Also, the anomaly detection in the evaluation step is based on a threshold which depends on the percentage of known anomalies, i.e., a priori information.	weakness
2018-167	I would like to see a plot of the sample energy as a function of the number of data points.	suggestion
2018-167	Is there an elbow that indicates the threshold cut?	suggestion
2018-167	Better yet it would be to use methods like Local Outlier Factor (LOF) (Breunig et al., 2000 – LOF:Identifying Density-based local outliers) to detect the outliers (these methods also have parameters to tune, sure, but using the known percentage of anomalies to find the threshold is not relevant in a purely unsupervised context when we don't know how many anomalies are in the data).	suggestion
2018-167	2. Is there a theoretical justification for computing the mixture memberships for the GMM using a neural network?	suggestion
2018-167	3. How do the regularization parameters \\lambda_1 and \\lambda_2 influence the results?	suggestion
2018-167	4. The idea to jointly optimize the dimension reduction and the clustering steps was used before neural nets (e.g., Yang et al., 2014 -  Unsupervised dimensionality reduction for Gaussian mixture model).	weakness
2018-167	Those approaches should at least be discussed in the related work, if not compared against.	weakness
2018-167	5. The authors state that estimating the mixture memberships with a neural network for GMM in the estimation network instead of the standard EM algorithm works better.	weakness
2018-167	Could you provide a comparison with EM?	weakness
2018-167	6. In the newly constructed space that consists of both the extracted features and the representation error, is a Gaussian model truly relevant?	weakness
2018-167	Does it well describe the new space?	weakness
2018-167	Do you normalize the features (the output of the dimension reduction and the representation error are quite different)?	weakness
2018-167	Fig. 3a doesn't seem to show that the output is a clear mixture of Gaussians.	weakness
2018-167	7. The setup of the KDDCup seems a little bit weird, where the normal samples and anomalies are reversed (because of percentage), where the model is trained only on anomalies, and it detects normal samples as anomalies ...	weakness
2018-167	I'm not convinced that it is the best example, especially that is it the one having significantly better results, i.e. scores ~ 0.9 vs.	weakness
2018-167	scores ~0.4/0.5 score for the other datasets.	weakness
2018-167	8. The authors mention that "we can clearly see from Fig. 3a that DAGMM is able to well separate ..." - it is not clear to me, it does look better than the other ones, but not clear.	weakness
2018-167	If there is a clear separation from a different view, show that one instead.	weakness
2018-167	We don't need the same view for all methods.	weakness
2018-167	9. In the experiments the reduced dimension used is equal to 1 for two of the experiments and 2 for one of them.	weakness
2018-167	This seems very drastic! Minor comments: 1. Fig.1: what dimension reduction did you use?	weakness
2018-167	Add axis labels. 2. "DAGMM preserves the key information of an input sample" - what does key information mean?	weakness
2018-167	3. In Fig. 3 when plotting the results for KDDCup, I would have liked to see results for the best 4 methods from Table 1, OC-SVM performs better than PAE.	suggestion
2018-167	Also DSEBM-e and DSEBM-r seems to perform very well when looking at the three measures combined.	suggestion
2018-167	They are the best in terms of precision.	strength
2018-167	4. Is the error in Table 2 averaged over multiple runs?	strength
2018-167	If yes, how many? Quality – The paper is thoroughly written, and the ideas are clearly presented.	strength
2018-167	It can be further improved as mentioned in the comments.	weakness
2018-167	Clarity – The paper is very well written with clear statements, a pleasure to read.	strength
2018-167	Originality – Fairly original, but it still needs some work to justify it better.	weakness
2018-167	Significance – Constraining the dimension reduction to fit a certain model is a relevant topic, but I'm not convinced of how well the Gaussian model fits the low-dimensional representation and how well can a neural network compute the GMM mixture memberships.	weakness
2018-167	1. This is a good paper, makes an interesting algorithmic contribution in the sense of joint clustering-dimension reduction for unsupervised anomaly detection	strength
2018-167	2. It demonstrates clear performance improvement via comprehensive comparison with state-of-the-art methods	strength
2018-167	3. Is the number of Gaussian Mixtures 'K' a hyper-parameter in the training process?	strength
2018-167	can it be a trainable parameter?	strength
2018-167	4. Also, it will be interesting to get some insights or anecdotal evidence on how the joint learning helps beyond the decoupled learning framework, such as what kind of data points (normal and anomalous) are moving apart due to the joint learning Summary	suggestion
2018-167	This applications paper proposes using a deep neural architecture to do unsupervised anomaly detection by learning the parameters of a GMM end-to-end with reconstruction in a low-dimensional latent space.	abstract
2018-167	The algorithm employs a tailored loss function that involves reconstruction error on the latent space, penalties on degenerate parameters of the GMM, and an energy term to model the probability of observing the input samples.	abstract
2018-167	The algorithm replaces the membership probabilities found in the E-step of EM for a GMM with the outputs of a subnetwork in the end-to-end architecture.	abstract
2018-167	The GMM parameters are updated with these estimated responsibilities as usual in the M-step during training.	abstract
2018-167	The paper demonstrates improvements in a number of public datasets.	abstract
2018-167	Careful reporting of the tuning and hyperparameter choices renders these experiments repeatable, and hence a suitable improvement in the field.	abstract
2018-167	Well-designed ablation studies demonstrate the importance of the architectural choices made, which are generally well-motivated in intuitions about the nature of anomaly detection.	abstract
2018-167	Criticisms Based on the performance of GMM-EN, the reconstruction error features are crucial to the success of this method.	abstract
2018-167	Little to no detail about these features is included.	weakness
2018-167	Intuitively, the estimation network is given the latent code conditioned and some (probably highly redundant) information about the residual structure remaining to be modeled.	suggestion
2018-167	Since this is so important to the results, more analysis would be helpful.	suggestion
2018-167	Why did the choices that were made in the paper yield this success?	rebuttal_process
2018-167	How do you recommend other researchers or practitioners selected from the large possible space of reconstruction features to get the best results?	suggestion
2018-167	Quality This paper does not set out to produce a novel network architecture.	weakness
2018-167	Perhaps the biggest innovation is the use of reconstruction error features as input to a subnetwork that predicts the E-step output in EM for a GMM.	weakness
2018-167	This is interesting and novel enough in my opinion to warrant publication at *CONF*, along with the strong performance and careful reporting of experimental design.	decision

2018-257	After reading rebuttals from the authors: The authors have addressed all of my concerns.	rebuttal_process
2018-257	THe additional experiments are a good addition.	rebuttal_process
2018-257	************************ The authors provide an algorithm-agnostic active learning algorithm for multi-class classification.	abstract
2018-257	The core technique is to construct a coreset of points whose labels inform the labels of other points.	abstract
2018-257	The coreset construction requires one to construct a set of  points which can cover the entire dataset.	abstract
2018-257	While this is NP-hard problem in general, the greedy algorithm is 2-approximate.	weakness
2018-257	The authors use a variant of the greedy algorithm along with bisection search to solve a series of feasibility problems to obtain a good cover of the dataset each time.	weakness
2018-257	This cover tells us which points are to be queried.	abstract
2018-257	The reason why choosing the cover is a good idea is because under suitable Lipschitz continuity assumption the generalization error can be controlled via an appropriate value of the covering radius in the data space.	weakness
2018-257	The authors use the coreset construction with a CNN to demonstrate an active learning algorithm for multi-class classification.	abstract
2018-257	The experimental results are convincing enough to show that it outperforms other active learning algorithms.	strength
2018-257	However, I have a few major and minor comments.	misc
2018-257	Major comments: 1. The proof of Lemma 1 is incomplete.	weakness
2018-257	We need the Lipschitz constant of the loss function.	weakness
2018-257	The loss function is a function of the CNN function and the true label.	weakness
2018-257	The proof of lemma 1 only establishes the Lipschitz constant of the CNN function.	weakness
2018-257	Some more extra work is needed to derive the lipschitz constant of the loss function from the CNN function.	weakness
2018-257	2. The statement of Prop 1 seems a bit confusing to me.	weakness
2018-257	the hypothsis says that the loss on the coreset = 0.	weakness
2018-257	But the equation in proposition 1 also includes the loss on the coreset.	weakness
2018-257	Why is this term included.	weakness
2018-257	Is this term not equal to 0?	weakness
2018-257	3. Some important works are missing.	weakness
2018-257	Especially works related to pool based active learning, and landmark results on labell complexity of agnostic active learning.	weakness
2018-257	UPAL: Unbiased Pool based active learning by Ganti & Gray.	misc
2018-257	http://proceedings.mlr.press/v22/ganti12/ganti12.pdf Efficient active learning of half-spaces by Gonen et al. http://www.jmlr.org/papers/volume14/gonen13a/gonen13a.pdf	misc
2018-257	A bound on the label complexity of agnostic active learning.	weakness
2018-257	http://www.machinelearning.org/proceedings/icml2007/papers/375.pdf 4.  The authors use L_2 loss as their objective function.	weakness
2018-257	This is a bit of a weird choice given that they are dealing with multi-class classification and the output layer is a sigmoid layer, making it a natural fit to work with something like a cross-entropy loss function.	weakness
2018-257	I guess the theoretical results do not extend to cross-entropy loss, but the authors do not mention these points anywhere in the paper.	weakness
2018-257	For example, the ladder network, which is one of the networks used by the authors is a network that uses cross-entropy for training.	weakness
2018-257	Minor-comment: 1. The feasibility program in (6) is an MILP.	weakness
2018-257	However, the way it is written it does not look like an MILP.	weakness
2018-257	It would have been great had the authors mentioned that u_j \\in {0,1}.	suggestion
2018-257	2. The authors write on page 4, "Moreover, zero training error can be enforced by converting average loss into maximal loss".	weakness
2018-257	It is not clear to me what the authors mean here.	weakness
2018-257	For example, can I replace the average error in proposition 1, by maximal loss?	weakness
2018-257	Why can I do that?	weakness
2018-257	Why would that result in zero training error?	weakness
2018-257	On the whole this is interesting work and the results are very nice.	strength
2018-257	But, the proof for Lemma 1 seems incomplete to me, and some choices (such as choice of loss function) are unjustified.	weakness
2018-257	Also, important references in active learning literature are missing.	weakness
2018-257	This paper studies active learning for convolutional neural networks.	abstract
2018-257	Authors formulate the active learning problem as core-set selection and present a novel strategy.	abstract
2018-257	Experiments are performed on three datasets to validate the effectiveness of the proposed method comparing with some baselines.	abstract
2018-257	Theoretical analysis is presented to show the performance of any selected subset using the geometry of the data points.	abstract
2018-257	Authors are suggested to perform experiments on more datasets to make the results more convincing.	suggestion
2018-257	The initialization of the CNN model is not clearly introduced, which however, may affect the performance significantly.	weakness
2018-257	Active learning for deep learning is an interesting topic and there is few useful tool available in the literature.	strength
2018-257	It is happy to see such paper in the field.	misc
2018-257	This paper proposes a batch mode active learning algorithm for CNN as a core-set problem.	abstract
2018-257	The authors provide an upper bound of the core-set loss, which is the gap between the training loss on the whole set and the core-set.	abstract
2018-257	By minimizing this upper bound, the problem becomes a K-center problem which can be solved by using a greedy approximation method, 2-OPT.	abstract
2018-257	The experiments are performed on image classification problem (CIFAR, CALTECH, SVHN datasets), under either supervised setting or weakly-supervised setting.	abstract
2018-257	Results show that the proposed method outperforms the random sampling and uncertainty sampling by a large margin.	abstract
2018-257	Moreover, the authors show that 2-OPT can save tractable amount of time in practice with a small accuracy drop.	abstract
2018-257	The proposed algorithm is new and writing is clear.	strength
2018-257	However, the paper is not flawless.	weakness
2018-257	The proposed active learning framework is under ERM and cover-set, which are currently not supported by deep learning.	weakness
2018-257	To validate such theoretical result, a non-deep-learning model should be adopted.	weakness
2018-257	The ERM for active learning has been investigated in the literature, such as "Querying discriminative and representative samples for batch mode active learning" in KDD 2013, which also provided an upper bound loss of the batch mode active learning and seems applicable for the problem in this paper.	weakness
2018-257	Another interesting question is most of the competing algorithm is myoptic active learning algorithms.	weakness
2018-257	The comparison is not fair enough.	weakness
2018-257	The authors should provide more competing algorithms in batch mode active learning.	suggestion

2018-307	This paper studies the problem of learning a single convolutional filter using SGD.	abstract
2018-307	The main result is: if the "patches" of the convolution are sufficiently aligned with each other, then SGD with a random initialization can recover the ground-truth parameter of a convolutional filter (single filter, ReLU, average pooling).	abstract
2018-307	The convergence rate, and how "sufficiently aligned" depend on some quantities related to the underlying data distribution.	abstract
2018-307	A major strength of the result is that it can work for general continuous distributions and does not really rely on the input distribution being Gaussian; the main weakness is that some of the distribution dependent quantities are not very intuitive, and the alignment requirement might be very high.	strength
2018-307	Detailed comments: 1. It would be good to clarify what the angle requirement means on page 2.	suggestion
2018-307	It says the angle between Z_i, Z_j is at most \\rho, is this for any i,j?	suggestion
2018-307	From the later part it seems that each Z_i should be \\rho close to the average, which would imply pairwise closeness (with some constant factor).	weakness
2018-307	2. The paper first proves result for a single neuron, which is a clean result.	weakness
2018-307	It would be interesting to see what are values of \\gamma(\\phi) and L(\\phi) for some distributions (e.g. Gaussian, uniform in hypercube, etc.) to give more intuitions.	suggestion
2018-307	3. The convergence rate depends on \\gamma(\\phi_0), from the initialization, \\phi_0 is probably very close to \\pi/2 (the closeness depend on dimension), which is  also likely to make \\gamma(\\phi_0) depend on dimension (this is especially true of Gaussian).	weakness
2018-307	4. More precisely, \\gamma(\\phi_0) needs to be at least 6L_{cross} for the result to work, and L_{cross} seems to be a problem dependent constant that is not related to the dimension of the data.	weakness
2018-307	Also \\gamma(\\phi_0) depends on \\gamma_{avg}(\\phi_0) and \\rho, when \\rho is reasonable (say a constant), \\gamma(\\phi_0) really needs to be a constant that is independent of dimension.	weakness
2018-307	On the other hand, in Theorem 3.4 we can see that the upperbound on \\alpha (the quality of initialization) depends on the dimension.	weakness
2018-307	5. Even assuming \\rho is a constant strictly smaller than \\pi/2 seems a bit strong.	weakness
2018-307	It is certainly plausible that nearby patches are highly correlated, but what is required here is that all patches are close to the average.	weakness
2018-307	Given an image it is probably not too hard to find an almost all white patch and an almost all dark patch so that they cannot both be within a good angle to the average.	weakness
2018-307	Overall I feel the result is interesting but hard to interpret correctly.	weakness
2018-307	The details of the theorem do not really support the high level claims very strongly.	weakness
2018-307	The paper would be much better if it goes over several example distributions and show explicitly what are the guarantees.	suggestion
2018-307	The reviewer tried to do that for Gaussian and as I mentioned above (esp.	suggestion
2018-307	4) the result does not seem very impressive, maybe there are other distributions where this result works better?	weakness
2018-307	After reading the response, I feel the contribution for the single neuron case does not require too much assumptions and is itself a reasonable result.	rebuttal_process
2018-307	I am still not convinced by the convolution case (which is the main point of this paper), as even though it does not require Gaussian input (a major plus), it still seems very far from "general distribution".	weakness
2018-307	Overall this is a first step in an interesting direction, so even though it is currently a bit weak I think it is OK to be accepted.	decision
2018-307	I hope the revised version will clearly discuss the limitations of the approach and potential future directions as the response did.	suggestion
2018-307	(a) Significance This is an interesting theoretical deep learning paper, where the authors try to provide the theoretical insights why SGD can learn the neural network well.	abstract
2018-307	The motivation is well-justified and clearly presented in the introduction and related work section.	strength
2018-307	And the major contribution of this work is the generalization to the non-Gaussian case, which is more in line with the real world settings.	strength
2018-307	Indeed, this is the first work analyzing the input distribution beyond Gaussian, which might be an important work towards understanding the empirical success of deep learning.	strength
2018-307	(b) Originality The division of the input space and the analytical formulation of the gradient are interesting, which are also essential for the convergence analysis.	strength
2018-307	Also, the analysis framework relies on novel but reasonable distribution assumptions, and is different from the relevant literature, i.e., Li & Yuan 2017, Soltanolkotabi 2017, Zhong et al. 2017.	strength
2018-307	I curious whether the angular smoothness assumptions can be applied to a more general network architecture, say two-layer neural network.	suggestion
2018-307	(c) Clarity & Quality Overall, this is a well-written paper.	strength
2018-307	The theoretical results are well-presented and followed by insightful explanations or remarks.	strength
2018-307	And the experiments are demonstrated to justify the theoretical findings as well.	strength
2018-307	The authors did a really good job in explaining the intuitions behind the imposed assumptions and justifying them based on the theoretical and experimental results.	strength
2018-307	I think the quality of this work is above the acceptance bar of *CONF* and it should be published in *CONF* 2018.	decision
2018-307	Minor comments: 1. Figure 3 looks a little small.	weakness
2018-307	It is better to make them clearer.	weakness
2018-307	2. In the appendix, ZZ^{\\top} and the indicator function are missing in the first equation of page 13.	weakness
2018-307	This paper considers the convergence of (stochastic) gradient descent for learning a convolutional filter with ReLU activations.	abstract
2018-307	It doesn't assume the input is Gaussian as in most previous work and shows that starting from random initialization, the (stochastic) gradient descent can learn the underlying convolutional filter in polynomial time.	abstract
2018-307	It is also shown that the convergence rate depends on the smoothness of the input distribution and the closeness of the patches.	abstract
2018-307	The main contribution and the most intriguing part is that the result doesn't require assuming the input is Gaussian.	strength
2018-307	Also, the guarantee holds for random initialization.	strength
2018-307	The analysis that achieves these results can potentially provide better techniques for analyzing more general deep learning optimizations.	strength
2018-307	The main drawback is that the assumptions are somewhat difficult to interpret, though significantly more general than those made in previous work.	weakness
2018-307	It will be great if more explanations/comments are provided for these assumptions.	suggestion
2018-307	It will be even better if one can get a simplified set of assumptions.	suggestion
2018-307	The presentation is clear but can be improved.	weakness
2018-307	Especially, more remarks would help readers to understand the paper.	weakness
2018-307	minor: -- Thm 2.1: what are w_1 w_2 here?	weakness
2018-307	-- Assumption 3.1: the statement seems incomplete.	weakness
2018-307	I guess it should be "max_...	weakness
2018-307	\\lambda_max(...) \\geq \\beta for some beta > 0"?	weakness
2018-307	-- Just before Section 2.1: " This is also consistent with empirical evidence in which more data are helpful for optimization."	weakness
2018-307	I don't see any evidence that more data help the optimization by filling in the holds in the distribution; they may help for other reasons.	weakness
2018-307	This statement here is not rigorous.	weakness

2018-315	This paper introduces a method for learning new tasks, without interfering previous tasks, using conceptors.	abstract
2018-315	This method originates from linear algebra, where a the network tries to algebraically infer the main subspace where previous tasks were learned, and make the network learn the new task in a new sub-space which is "unused" until the present task in hand.	abstract
2018-315	The paper starts with describing the method and giving some context for the method and previous methods that deal with the same problem.	abstract
2018-315	In Section 2 the authors review conceptors.	abstract
2018-315	This method is algebraic method closely related to spanning sub spaces and SVD.	abstract
2018-315	The main advantage of using conceptors is their trait of boolean logics: i.e., their ability to be added and multiplied naturally.	strength
2018-315	In section 3 the authors elaborate on reviewed ocnceptors method and show how to adapt this algorithm to SGD with back-propagation.	abstract
2018-315	The authors provide a version with batch SGD as well.	abstract
2018-315	In Section 4, the authors show their method on permuted MNIST.	abstract
2018-315	They compare the method to EWC with the same architecture.	abstract
2018-315	They show that their method more efficiently suffers on permuted MNIST from less degradation.	abstract
2018-315	Also, they compared the method to EWC and IMM on disjoint MNIST and again got the best performance.	abstract
2018-315	In general, unlike what the authors suggest, I do not believe this method is how biological agents perform their tasks in real life.	weakness
2018-315	Nevertheless, the authors show that their method indeed reduce the interference generated by a new task on the old learned tasks.	strength
2018-315	I think that this work might interest the community since such methods might be part of the tools that practitioners have in order to cope with learning new tasks without destroying the previous ones.	strength
2018-315	What is missing is the following: I think that without any additional effort, a network can learn a new task in parallel to other task, or some other techniques may be used which are not bound to any algebraic methods.	weakness
2018-315	Therefore, my only concern is that in this comparison the work bounded to very specific group of methods, and the question of what is the best method for continual learning remained open.	weakness
2018-315	[Reviewed on January 12th] This article applies the notion of "conceptors" -- a form of regulariser introduced by the same author a few years ago, exhibiting appealing boolean logic pseudo-operations -- to prevent forgetting in continual learning,more precisely in the training of neural networks on sequential tasks.	abstract
2018-315	It proposes itself as an improvement over the main recent development of the field, namely Elastic Weight Consolidation.	abstract
2018-315	After a brief and clear introduction to conceptors and their application to ridge regression, the authors explain how to inject conceptors into Stochastic Gradient Descent and finally, the real innovation of the paper, into Backpropagation.	abstract
2018-315	Follows a section of experiments on variants of MNIST commonly used for continual learning.	abstract
2018-315	Continual learning in neural networks is a hot topic, and this article contributes a very interesting idea.	strength
2018-315	The notion of conceptors is appealing in this particular use for its interpretation in terms of regularizer and in terms of Boolean logic.	strength
2018-315	The numeric examples, although quite toy, provide a clear illustration.	strength
2018-315	A few things are still missing to back the strong claims of this paper: * Some considerations of the computational costs: the reliance on the full NxN correlation matrix R makes me fear it might be costly, as it is applied to every layer of the neural networks and hence is the largest number of units in a layer.	weakness
2018-315	This is of course much lighter than if it were the covariance matrix of all the weights, which would be daunting, but still deserves to be addressed, if only with wall time measures.	weakness
2018-315	* It could also be welcome to use a more grounded vocabulary, e.g. on p.2 "Figure 1 shows examples of conceptors computer from three clouds of sample state points coming from a hypothetical 3-neuron recurrent network that was drive with input signals from three difference sources" could be much more simply said as "Figure 1 shows the ellipses corresponding to three sets of R^3 points".	suggestion
2018-315	Being less grandiose would make the value of this article nicely on its own.	suggestion
2018-315	* Some examples beyond the contrived MNIST toy examples would be welcome.	suggestion
2018-315	For example, the main method this article is compared to (EWC) had a very strong section on Reinforcement learning examples in the Atari framework, not only as an illustration but also as a motivation.	suggestion
2018-315	I realise not everyone has the computational or engineering resources to try extensively on multiple benchmarks from classification to reinforcement learning.	misc
2018-315	Nevertheless, without going to that extreme, it might be worth adding an extra demo on something bigger than MNIST.	suggestion
2018-315	The authors transparently explain in their answer that they do not (yet!) belong to the deep learning community and hope finding some collaborations to pursue this further.	rebuttal_process
2018-315	If I may make a suggestion, I think their work would get much stronger impact by  doing it the reverse way: first finding the collaboration, then adding this extra empirical results, which then leads to a bigger impact publication.	suggestion
2018-315	The later point would normally make me attribute a score of "6: Marginally above acceptance threshold" by current DL community standards, but because there is such a pressing need for methods to tackle this problem, and because this article can generate thinking along new lines about this, I give it a 7 : Good paper, accept.	decision
2018-315	The paper leaves me guessing which part is a new contribution, and which one is already possible with conceptors as described in the Jaeger 2014 report.	weakness
2018-315	Figure (1) in the paper is identical to the one in the (short version of) the Jaeger report but is missing an explicit reference.	weakness
2018-315	Figure 2 is almost identical, again a reference to the original would be better.	weakness
2018-315	Conceptors can be trained with a number of approaches (as described both in the 2014 Jaeger tech report and in the JMLR paper), including ridge regression.	weakness
2018-315	What I am missing here is a clear indication what is an original contribution of the paper, and what is already possible using the original approach.	weakness
2018-315	The fact that additional conceptors can be trained does not appear new for the approach described here.	weakness
2018-315	If the presented approach was an improvement over the original conceptors, the evaluation should compare the new and the original version.	weakness
2018-315	The evaluation also leaves me a little confused in an additional dimension: the paper title and abstract suggested that the contribution is about overcoming catastrophic forgetting.	weakness
2018-315	The evaluation shows that the approach performs better classifying MNIST digits than another approach.	weakness
2018-315	This is nice but doesn't really tell me much about overcoming catastrophic forgetting.	weakness

2019-117	The paper proposes a method for learning embedding of hierarchies.	abstract
2019-117	Specifically, the paper builds on a a geometrically inspired embedding method using box representations.	abstract
2019-117	The key contribution of the paper is facilitating optimization of these models by gradient based methods, which eventually leads to improved accuracy on relevant benchmark data (on par or beyond SOTA).	abstract
2019-117	The observation is that when two boxes are disjoint in the model but have overlap in the ground truth, no gradient can flow to the model to correct the problem (which is happens in case of sparse-data.	abstract
2019-117	To alleviate the above problem, the paper proposes smoothing the model.	abstract
2019-117	That is, transforming the original model constructed from indicator functions (hence difficult to optimize by gradient based method) to a smooth differentiable function by diffusing the landscape.	abstract
2019-117	The diffusion process corresponds to convincing the objective function with the Gaussian kernel.	abstract
2019-117	I find the idea of converting such combinatorial problems to differentiable, specially when gradient methods can succeed in optimizing them afterward, very fascinating.	strength
2019-117	I believe this paper is taking a theoretically sound path to construct the differentiable form of the originally non-differentiable problem.	strength
2019-117	As the authors find, the smoothed function leads to improved performance against SOTA on relevant benchmark data such as WordNet hypernymy, Flick caption entailment and MovieLnes market basket data.	strength
2019-117	One downside of the current submission is that the details of optimization are now provided at all.	weakness
2019-117	What algorithm do you use to optimize the objective function?	weakness
2019-117	What are the hyper parameters?	weakness
2019-117	What value of sigma (for diffusion) do you use [or maybe you use the continuation method to gradually anneal sigma from large toward zero?).	weakness
2019-117	These are important details that I ask the authors to include.	weakness
2019-117	Also, I think some graphical illustration of the embedding would be very helpful, perhaps something like Figure 2 of "Probabilistic Embedding of Knowledge Graphs with Box Lattice Measures".	suggestion
2019-117	I hope such illustration is added to the submission.	misc
2019-117	This paper proposes a soft relaxation of the box lattice (BL) model of Vilnis et al. 2018 and applies it to several graph prediction tasks.	abstract
2019-117	Results are comparable to the BL model on existing artificially-balanced data but significantly better on more natural unbalanced data with a large number of negatives.	abstract
2019-117	The paper assumes some familiarity with the problem domain and existing works (there is not a lot of exposition for an unfamilar reader), but should be of strong interest to anyone working on embeddings or graph prediction.	weakness
2019-117	The paper is well-written, with clear explanations of the desired properties of the model and a concise set of experiments that are easy to follow.	strength
2019-117	The strongest result is that on unbalanced WordNet, while the Flickr and MovieLens results are a little less clear but do show that this technique does not cause any loss in performance.	strength
2019-117	A few points of feedback: - Missing citation / comparison: https://arxiv.org/pdf/1804.01882.pdf (Ganea et al. 2018) is an alternative way of generalizing order embeddings.	weakness
2019-117	They also report very high numbers on WordNet, though I'm not sure they are directly comparable.	weakness
2019-117	- The Gaussian relaxation (Eq. (2) and (3)) defines a particular length scale, \\sigma.	weakness
2019-117	It's not clear if this is also implicit in the softplus derivation (by analogy with Eq.	weakness
2019-117	(4), should we assume that it approximates the \\sigma = 1 case?).	weakness
2019-117	What effect does this have on the embedding space?	weakness
2019-117	Without it, it would seem that the normal BL model is scale invariant, which might be a desirable property for representing hierarchical data.	weakness
2019-117	- The main thrust of section 5.2 is that smoothed box embeddings retain better performance with increasing numbers of negatives.	weakness
2019-117	Could you include the ratio of positive / negative examples on the Flickr dataset, and some measure of the distribution of P(A|B) values on MovieLens to get a sense of how these datasets compare?	suggestion
2019-117	- Flickr data: what is the encoder model that produces the embeddings here, and how does it handle unseen captions?	suggestion
2019-117	(Why would we expect the smoothed box model to handle unseen captions better?)	suggestion
2019-117	- There's a strong emphasis on how smoothing makes training easier.	weakness
2019-117	Do you have any metrics to directly support this, such as variance under random restarts?	weakness
2019-117	- In the abstract and introduction, it's easy to gloss over "inspired by" and assume that the actual model is a Gaussian convolution.	weakness
2019-117	Could be more direct here that it's a softplus approximation.	weakness
2019-117	Post-rebuttal revision: All my concerns were adressed by the authors.	rebuttal_process
2019-117	This is a great paper and should be accepted.	decision
2019-117	------ The paper presents smoothing probabilistic box embeddings with softplus functions, which make the optimization landscape continuous, while also presenting the theoretical background of the proposed method well.	strength
2019-117	The paper presents the overall idea beautifully and is very easy to follow.	strength
2019-117	The overall idea of smoothed sotfplus boxes is well-founded, elegant and practical.	strength
2019-117	The results on standard WordNet do not improve upon state-of-the-art, however imbalanced WordNet with abundance of negative examples gain remarkable improvements.	strength
2019-117	Similarly in Flickr and MovieLens the method performs well.	strength
2019-117	This paper presents a novel, theoretically well-justified idea with excellent results, and is likely going to be a high-impact paper.	strength
2019-117	An illustrating figure would still be nice to include, also for the convolutions of eq 2.	suggestion
2019-117	The paper does not comment on running times, some kind of scalability comparison should be included since the paper claims that the model is easier to train.	weakness
2019-117	The paper should clarify that the \\prod in 3.3.	weakness
2019-117	meet and join definitions seems to refer to a set product, while the p(a) equation has a standard product (or does it?).	weakness
2019-117	What is the "a" in the p(a), should it be "p(x)" ?	weakness
2019-117	I have trouble understanding eq 1: the difference inside the function is always negative, while the hinge function seems to clip negative values away.	weakness
2019-117	The definition of the m(x) is too clever, please clarify the function in more conventional notation.	weakness

2019-130	It was believed that sparse architectures generated by pruning are difficult to train from scratch.	weakness
2019-130	The authors show that there exist sparse subnetworks that can be trained from scratch with good generalization performance.	abstract
2019-130	To explain the difficulty of training pruned networks from scratch or why training needs the overparameterized networks that make pruning necessary,  the authors propose a lottery ticket hypothesis: unpruned, randomly initialized NNs contain subnetworks that can be trained from scratch with similar generalization accuracy.	abstract
2019-130	They also present an algorithm to identify the winning tickets.	abstract
2019-130	The conjecture is interesting and it is still a open question for whether a pruned network can reach the same accuracy when trained from scratch.	abstract
2019-130	It may helps to explain why bigger networks are easier to train due to "having more possible subnetworks from which training can recover a winning ticket".	abstract
2019-130	It also shows the importance of both the pruned architecture and the initialization value.	abstract
2019-130	Actually another submission (https://openreview.net/forum?id=rJlnB3C5Ym) made the opposite conclusions.	weakness
2019-130	The limitations of this paper are several folds: - The paper seems a bit preliminary and unfinished.	weakness
2019-130	A lot of notations seems confusing, such as "when pruned to 21%".	weakness
2019-130	The author defines a winning lottery ticket as a sparse subnetwork that can reaching the same performance of the original network when trained from scratch with the "original initialization".	weakness
2019-130	It is quite confusing as there is no definition anywhere about the "original initialization".	weakness
2019-130	It would be clearer if the author can use some math notations.	weakness
2019-130	- As identified by the authors themself, lacking of supporting experiments on large-scale dataset and real-world models.	weakness
2019-130	Only MNIST/CIFAR-10 and toy networks like LeNet, Conv2/Conv4/Conv6 are used.	weakness
2019-130	The author has done experiments on resnet, I would be better to move it to the main paper.	weakness
2019-130	- There is no explanation about why the "lottery ticket" can perform well when trained with the "original initialization" but not with random initialization.	weakness
2019-130	Is it because the original initialization is not far from the pruned solution?	weakness
2019-130	Then this is a kind of overting to the obtained solution.	weakness
2019-130	- The other problem is that the implications are not clearly useful without showing any applications.	weakness
2019-130	The paper could be stronger if the authors can provide more results to support the applications of this conjecture.	suggestion
2019-130	- The authors only explore the sparse networks.	weakness
2019-130	Model compression by sparsification has good compression rate, especially for networks with large FC layers.	weakness
2019-130	However, the acceleration relies on specific hardware/libraries.	weakness
2019-130	It would be more complete if the author can provide experiments on structurally pruned networks, especially for CNNs.	weakness
2019-130	- The x-axis of pruning ratios in Figure 1/4/5 could be uniformly sampled and make the figure easier to read.	weakness
2019-130	Questions: - Does the winning tickets always exist?	weakness
2019-130	- What is the size of winning tickets for a very thin network?	weakness
2019-130	Would it also be less than 10%?	weakness
2019-130	------update---------- I appreciate the author's efforts on providing detailed response and more experiments.	rebuttal_process
2019-130	After reading the rebuttal and the revised version, though the paper has been improved, my concerns are not fully addressed to safely accept it.	decision
2019-130	It can be summarized that there exists a sparse network that can be trained well only provided with certain weight initialization.The winning tickets can only be found via iterative pruning of the trained network.	weakness
2019-130	This is a chicken-egg problem and I failed to see how it can improve the network design.	weakness
2019-130	It still feels incomplete to me by just providing a hypothesis with limited sets of experiments.	weakness
2019-130	The implications are actually the most valuable/attractive part, such as "Improve our theoretical understanding of neural networks", however, they are very vague with no clear instructions even after accepting this hypothesis.	weakness
2019-130	I would expect analysis of the reason behind failure and success.	weakness
2019-130	I understand that it could be left for another paper, but the observations/experiments only are not strong enough for confirming the the hypothesis.	weakness
2019-130	Specifically, the experiments are conducted on relatively wide and shallow CNNs. Note that VGG-16/19 and ResNet-18 are designed for ImageNet but not CIFAR-10, which are much wider than normal CIFAR-10 networks, such as ResNet-56.	weakness
2019-130	Even "resnet18 has 16x fewer parameters than conv2 and 75x fewer than VGG19", it is mainly due to the removal of FC layers with average pooling and cannot be claimed as "much thinner" networks.	weakness
2019-130	As increasing the wideness usually ease the optimization, and the pruned sparse network still enjoy this property unless significantly pruned.	weakness
2019-130	Thus, I still doubt whether the conclusion can hold for much thinner network, i.e., "winning tickets near or below 10-20%, depending on the level of overparameterization of the original network."	weakness
2019-130	The observation of "winning ticket weights tend to change by a larger amount then weights in the rest of the network" in Figure 19 seems natural and the conjecture of the reason "magnitude-pruning biases the winning tickets we find toward those containing weights that change in the direction of higher magnitude" sounds reasonable.	strength
2019-130	It would be great if the authors can dig into this and make more comparison with the distribution of random weights initialization.	suggestion
2019-130	The figures could also be improved and simplified as the lines are hard to read and compare.	weakness
2019-130	==== Summary ==== It is widely known that large neural networks can typically be compressed into smaller networks that perform as well as the original network while directly training small networks can be complicated.	weakness
2019-130	This paper proposes a conjecture to explain this phenomenon that the authors call "The Lottery Ticket Hypothesis":  large networks that can be trained successfully contain at initialization time small sub-networks — which are defined by both connectivity and the initial weights that the authors call "winning tickets" — that if trained separately for similar number of iterations could reach the same performance as the large network.	weakness
2019-130	The paper follows by proposing a method to find these winning tickets by pruning methods, which are typically used for compressing networks, and then proceed to test this hypothesis on several architectures and tasks.	abstract
2019-130	The paper also conjectures that the reason large networks are more straightforward to train is that when randomly initialized large networks have more combinations for subnetworks which makes have a winning ticket more likely.	weakness
2019-130	==== Detailed Review ==== I have found the hypothesis that the paper puts forth to be very appealing, as it articulates the essence of many ideas that have been floating around for quite a while.	strength
2019-130	For example, the notion that having a large network makes it more probable for some of the initialized weights to be in the "right" direction for the beginning of the training, as mentioned in [1] that was cited in this submission.	strength
2019-130	Given our lack of understanding of the optimization and generalization properties of neural networks, as well as how these two interact, then any insight into this process, like this paper suggests, could have a significant impact on both theory and practice.	strength
2019-130	To that effect, I generally found the experiments in support of the hypothesis to be pretty convincing, or at the very least that there is some truth to it.	strength
2019-130	Most importantly, the hypothesis and experiments presented in this paper gave me a new perspective on both the generalization and optimization problem, which as a theoretician gave me new ideas on how to approach analyzing them rigorously — and that is why I strongly vote for the acceptance of this paper.	decision
2019-130	Though I have very much enjoyed reading this submission, which for the most part is very well written, it does have some issues: 1. Though this is an empirical paper about an observed phenomenon, it should contain a bit more background and discussion on the theoretical implications of its subject.	weakness
2019-130	For example, see [2] which is also an empirical work about a theoretical hypothesis, but still includes the right theoretical context that helps the reader judge the meaning of their results.	weakness
2019-130	The same should be done here.	weakness
2019-130	For instance, there is a growing interest in the link between compression and generalization that is relevant to this work [3,4], and the effect of winning ticket leading to better generalization could be explained via other works which link structure to inductive bias [5,6].	weakness
2019-130	2. The lottery ticket hypothesis is described in the paper as being both about optimization (faster "convergence") and about generalization (better "generalization accuracy").	weakness
2019-130	However, there is a slight issue with how these terms are treated in the paper.	weakness
2019-130	First, "convergence" is defined as the point at which the test accuracy reaches to a minimum and before it begins to rise again, but it does not mean (and most likely not) that it is the point at which the optimization algorithm converged to its minimum — it is better to write that early stopping regularization was used in this case.	weakness
2019-130	Second, the convergence point is chosen according to the test set which is bad methodology, because the test set cannot be used for choosing the final model (only the training and validation sets).	weakness
2019-130	Third, the training accuracies are not reported in the paper, and without them, it is difficult to judge if a given model fails to generalize is simply fails to converge to 100% accuracy on the training set.	weakness
2019-130	As a minor note, "generalization accuracy" as a term is not that common and might be a bit confusing, so it is better to write "test accuracy".	weakness
2019-130	To conclude, even though I urge the authors to address the above issues, which could significantly improve its quality and clarity, I think that this article thought-provoking and highly deserving of being accepted to *CONF*.	decision
2019-130	[1] Bengio et al. Convex neural networks. NIPS 2006. [2] Zhang et al. Understanding deep learning requires rethinking generalization.	misc
2019-130	*CONF* 2017. [3] Arora et al. Stronger generalization bounds for deep nets via a compression approach.	misc
2019-130	ICML 2018. [4] Zhou et al. Compressibility and Generalization in Large-Scale Deep Learning.	misc
2019-130	Arxiv preprint 2018. [5] Cohen et al. Inductive Bias of Deep Convolutional Networks through Pooling Geometry.	misc
2019-130	*CONF* 2017. [6] Levine et al. Deep Learning and Quantum Entanglement: Fundamental Connections with Implications to Network Design.	misc
2019-130	*CONF* 2018. ==== Updated Review Following Rebuttal ====	misc
2019-130	The authors have addressed all of the concerns that I have mentioned above, and so I have updated my score accordingly.	rebuttal_process
2019-130	The additional background on related works, as well as the additional experiments in response to the other reviews will help readers appreciate the observations that are raised by the authors.	suggestion
2019-130	The new revision is a very strong submission, and I highly recommend accepting it to *CONF*.	decision
2019-130	(Score raised from 8 to 9 after rebuttal)	rating_summary
2019-130	The paper examines the hypothesis that randomly initialized (feed-forward) neural networks contain sub-networks that train well in the sense that they converge equally fast or faster and reach the same or better classification accuracy.	abstract
2019-130	Interestingly, such sub-networks can be identified by simple, magnitude-based pruning.	abstract
2019-130	It is crucial that these sub-networks are initialized with their original initialization values, otherwise they typically fail to be trained, implying that it is not purely the structure of the sub-networks that matters.	abstract
2019-130	The paper thoroughly investigates the existence of such "winning-tickets" on MNIST and CIFAR-10 on both, fully connected but also convolutional neural networks.	abstract
2019-130	Winning-tickets are found across networks, various optimizers, at different pruning-levels and across various other hyper-parameters.	abstract
2019-130	The experiments also show that iterative pruning (with re-starts) is more effective at finding winning-tickets.	abstract
2019-130	The paper adds a novel and interesting angle to the question of why neural networks apparently need to be heavily over-parameterized for training.	strength
2019-130	This question is intriguing and of high importance to further the understanding of how neural networks train.	strength
2019-130	Additionally, the findings might have practical relevance as they might help avoid unnecessary over-parameterization which, in turn, might save use of computational resources and energy.	strength
2019-130	The main idea is simple (which is good) and can be tested with relatively simple experiments (also good).	strength
2019-130	The experiments conducted in the paper are clean (averaging over multiple runs, controlling for a lot of factors) and should allow for easy reproduction but also for clean comparison against future experiments.	strength
2019-130	The experimental section is well executed, the writing is clear and good and related work is taken into account to a sufficient degree.	strength
2019-130	The paper touches upon a very intriguing "feature" of neural networks and, in my opinion, should be relevant to theorists and practitioners across many sub-fields of deep learning research.	strength
2019-130	I therefore vote and argue for accepting the paper for presentation at the conference.	decision
2019-130	The following comments are suggestions to the authors on how to further improve the paper.	misc
2019-130	I do not expect all issues to be addressed in the camera-ready version.	suggestion
2019-130	1) The main "weakness" of the paper might be that, while the amount of experiments and controls is impressive, the generality of the lottery ticket hypothesis remains somewhat open.	weakness
2019-130	Even when restricting the statement to feed-forward networks only, the networks investigated in the paper are relatively "small" and MNIST and CIFAR-10 bear the risk of finding patterns that do not hold when scaling to larger-scale networks and tasks.	weakness
2019-130	I acknowledge and support the author's decision to have thorough and clean experiments on these small models and tasks, rather than having half-baked results on ImageNet, etc.	weakness
2019-130	The downside of this is that the experiments are thus not sufficient to claim (with reasonable certainty) that the lottery ticket hypothesis holds "in general".	weakness
2019-130	The paper would be stronger, if the existence of winning tickets on larger-scale experiments or tasks other than classification were shown - even if these experiments did not have a large number of control experiments/ablation studies.	suggestion
2019-130	2)  While the paper shows the existence of winning tickets robustly and convincingly on the networks/tasks investigated, the next important question would be how to systematically and reliably "break" the existence of lottery tickets.	weakness
2019-130	Can they be attributed to a few fundamental factors?	weakness
2019-130	Are they a consequence of batch-wise, gradient-based optimization, or an inherent feature of neural networks, or is it the loss functions commonly used, …?	weakness
2019-130	On page 2, second paragraph, the paper states: "When randomly reinitialized, our winning tickets no longer match the performance of the original network, explaining the difficulty of training pruned networks from scratch".	weakness
2019-130	I don't fully agree - the paper certainly sheds some light on the issue, but an actual explanation would result in a testable hypothesis.	weakness
2019-130	My comment here is intended to be constructive criticism, I think that the paper has enough "juice" and novelty for being accepted	decision
2019-130	- I am merely pointing out that the overall story is not yet conclusive (and I am aware that it might need several more publications to find these answers).	weakness
2019-130	3) Do the winning tickets generalize across hyper-parameters or even tasks.	weakness
2019-130	I.e. if a winning ticket is found with one set of hyper-parameters, but then Optimizer/learning-rate/etc.	weakness
2019-130	are changed, does the winning-ticket still lead to improved convergence and accuracy?	weakness
2019-130	Same question for data-sets: do winning-tickets found on CIFAR-100 also work for CIFAR-10 and vice versa?	weakness
2019-130	If winning-tickets turn out to generalize well, in the extreme this could allow "shipping" each network architecture with a few good winning-tickets, thus making it unnecessary to apply expensive iterative pruning every time.	weakness
2019-130	I would not expect generalization across data-sets, but it would be highly interesting to see if winning tickets generalize in any way (after all I am still surprised by how well adversarial examples generalize and transfer).	suggestion
2019-130	4) Some things that would be interesting to try: 4a) Is there anything special about the pruned/non-pruned weights at the time of initialization?	suggestion
2019-130	Did they start out with very small values already or are they all "behind" some (dead) downstream neuron?	suggestion
2019-130	Is there anything that might essentially block gradient signal from updating the pruned neurons?	suggestion
2019-130	This could perhaps be checked by recording weights' "trajectories" during training to see if there is a correlation between the "distance weights traveled" and whether or not they end up in the winning ticket.	suggestion
2019-130	4b) Do ARD-style/Bayesian approaches or second-order methods to pruning identify (roughly) the same neurons for pruning?	suggestion
2019-130	5) Typo (should be through): "we find winning tickets though a principled search process"	suggestion
2019-130	6) For the standard ConvNets I assume you did not use batchnorm.	suggestion
2019-130	Does batchnorm interfere in any way with the existence of winning tickets?	weakness
2019-130	(at least on ResNet they seem to exist with batchnorm as well)	weakness

2019-138	This work introduces SNIP, a simple way to prune neural network weights before training according to a specific criterion.	abstract
2019-138	SNIP identifies prunable weights by the normalised gradient of the loss w.r.t. an implicit multiplicative factor "c" on the weights, denoted as the "sensitivity".	abstract
2019-138	Essentially, this criterion takes two factors into account when determining the relevance of each weight; the scale of the gradient and the scale of the actual weight.	abstract
2019-138	The authors then rank the weights according to their sensitivity and remove the ones that are not in the top-k.	abstract
2019-138	They then proceed to train the surviving weights as normal on the task at hand.	abstract
2019-138	In experiments they show that this method can offer competitive results while being much simpler to implement than other methods in the literature.	abstract
2019-138	This paper is well written and explains the main idea in a clear and effective manner.	strength
2019-138	The method seems to offer a viable tradeoff between simplicity of implementation and effective sparse models.	strength
2019-138	The experiments done are also extensive, as they cover a broad range of tasks: MNIST / CIFAR 10 classification with various architectures, ablation studies on the effects of different initialisations, visualisations of the pruning patterns and exploration of regularisation effects on a task involving fitting random labels.	strength
2019-138	However, this work has also an, I believe important, omission w.r.t. prior work.	weakness
2019-138	The idea of using that particular gradient as a guide to selecting which parameters to prune is actually not new and has been previously proposed at [1].	weakness
2019-138	The authors of [1] considered unit pruning but the modification for weight pruning is trivial.	weakness
2019-138	It is worth pointing out that [1] is also discussed in one of the other citations of this work, namely [2].	weakness
2019-138	For this reason, I believe that the main contribution of this paper is more on the thorough experimental evaluation of an existing idea rather than the proposed sensitivity metric.	weakness
2019-138	As for other general comments: - The authors argue that SNIP can offer training time speedups by only optimising the remaining parameters.	weakness
2019-138	In this spirit, the authors might also want to discuss about other works that seem relevant to this task, e.g. [3, 4].	suggestion
2019-138	They also allow for pruned and sparse networks during training (thus speeding it up), without needing to conform to a specific sparsity pattern.	suggestion
2019-138	- SNIP seems to be a good candidate for applying it to randomly initialised networks; nevertheless, a lot of times we are also interested in pruning pre-trained networks.	suggestion
2019-138	Given that SNIP is relying on the magnitude of the gradient to determine relevance, how good does it handle this particular case (given that the magnitude of the gradients is close to zero at convergence)?	weakness
2019-138	- Why is the normalisation of the magnitude of the gradients necessary?	weakness
2019-138	The normalisation doesn't change the relative ordering so we could simply just rank according to |g_j(w; D)|.	weakness
2019-138	- While the experiment at section 5.6 is interesting, the result is still dependent on the a-priori chosen cut-off point "k".	weakness
2019-138	For this reason it might be worthwhile to plot the behaviour of the network as a function of "k".	suggestion
2019-138	Furthermore, the authors should also refer to [5] as they originally did the same experiment and showed that they can obtain the same behaviour without any hyper parameters.	misc
2019-138	[1] Skeletonization: A Technique for Trimming the Fat from a Network via Relevance Assessment.	misc
2019-138	[2] A Simple Procedure for Pruning Back-Propagation Trained Neural Networks.	misc
2019-138	[3] Learning Sparse Neural Networks through L_0 Regularization.	misc
2019-138	[4] Generalized Dropout. [5] Variational Dropout Sparsifies Deep Neural Networks.	misc
2019-138	Post rebuttal update/comment: I thank the authors for the revision and have updated the score (twice!)	rebuttal_process
2019-138	One genuinely perplexing result to me is that the method behaves better than random pruning, yet after selecting the salient neurons the weights can be reinitialized, as per the rebuttal: > # Initialization procedure	rebuttal_process
2019-138	- It is correct that the weights used to train the pruned model are possibly different from the ones used to compute the connection sensitivity.	rebuttal_process
2019-138	Given (variance scaled) initial weights, SNIP finds the architecturally important parameters in the network, then the pruned network is established and trained in the standard way.	rebuttal_process
2019-138	First, there is work which states quite the opposite (e.g. https://arxiv.org/abs/1803.03635).	weakness
2019-138	Please relate to it. Fundamentally, if you decouple weight pruning from initialization it also means that: - the first layer will be pruned out of connections to constant pixels (which is seen in the visualizations), this remains meaningful even after a reinitialization	weakness
2019-138	- the second and higher layers will be pruned somewhat randomly - even if the connections pruned were meaningful with the original weights, after the reinitialization the functions computed by the neurons in lower layers will be different, and have no relation to pruned weights.	weakness
2019-138	Thus the pruning will be essentially random (though possibly from a very specific random distribution).	weakness
2019-138	In other words - then neurons in a fully connected layer can be freely swapped, each neuron in the next layer behaves on al of them anyway we are thinking here about the uninitialized neurons, with each of them having a distribution over weights and not a particular set of sampled weights, this is valid because we will reinitialize the neurons).	weakness
2019-138	Because of that, I wouldn't call any particular weight/connection architecturally important and find it strange that such weights are found.	weakness
2019-138	I find this behavior really perplexing, but I trust that your experiments are correct.	weakness
2019-138	however, please, if you have the time, verify it.	suggestion
2019-138	Original review: The paper presents an intriguing result in which a salient, small subset of weights can be selected even in untrained networks given sensible initialization defaults are used.	abstract
2019-138	This result is surprising - the usual network pruning procedure assumed that a network is pretrained, and only then important connections are removed.	abstract
2019-138	The contributions of the paper are two-fold: 1) it reintroduces a multiplicative sensitivity measure similar to the Breiman garotte	strength
2019-138	2) and shows which other design choices are needed to make it work on untrained networks, which is surprising.	strength
2019-138	While the main idea of the paper is clear and easy to intuitively understand, the details are not.	weakness
2019-138	My main concern is that paper differentiates between weights and connections (both terms are introduced on page iv to differentiate from earlier work).	weakness
2019-138	However, it is not clear what are the authors referring to: - a conv layer has many repeated applications of the same weight.	weakness
2019-138	Am I correct to assume that a conv layer has many more connections, than weights?	weakness
2019-138	Furthermore, are the dramatic sparsities demonstrated over connections counted in this manner?	weakness
2019-138	This is important - on MNIST each digit has a constant zero border, all connections to the border are not needed and can be trivially removed (one can crop the images to remove them for similar results).	weakness
2019-138	Thus we can trivially remove connections, without removing weights.	weakness
2019-138	- in paragraph 5.5 different weight initialization schemes are used for the purpose of saliency estimation, but the paragraph then says "Note that for training VS-X initialization is used in all the cases."	weakness
2019-138	Does it mean that first a set of random weights is sampled, then the sensitivities are computed, then a salient set of connections is established and the weights are REINITIALIZED from a distribution possibly different than the one used to compute the sensitivity?	weakness
2019-138	The fact that it works is very surprising and again suggests that the method identifies constant background pixels rather than important weights.	weakness
2019-138	- on the other hand, if there is a one-to-one correspondence between connections and weights, then the differentiation from Karnin (1990) at the bottom of p.	weakness
2019-138	iv is misleading. I would also be cautious about extrapolating results from MNIST to other vision datasets.	weakness
2019-138	MNIST has dark backgrounds. Let f(w,c) = 0*w*c.	weakness
2019-138	Trivially, df/dw = df/dc = 0.	weakness
2019-138	Thus the proposed sensitivity measure picks non-background pixels, which is also demonstrated in figure 2.	weakness
2019-138	However, this is a property of the dataset (which encodes background with 0) and not of the method!	weakness
2019-138	This should be further investigated - a quick check is to invert MNIST (make the images black-on-white, not white-on-black) and see if the method still works.	suggestion
2019-138	Fashion MNIST behaves in a similar way.	suggestion
2019-138	Thus the only non-trvial experiments are the ones on CIFAR10 (Table 2), but the majority of the analysis is conducted on white-on-black MNIST and Fashion-MNIST.	suggestion
2019-138	Finally, no experiment shows the benefit of introducing the variables "c", rather than using the gradient with respect to the weights.	weakness
2019-138	let f be the function computed by the network.	weakness
2019-138	Then: - df/d(cw) is the gradient passed to the weights if the "c" variables were not introduced	weakness
2019-138	- df/dw = df/d(cw) d(cw)/dw = df/d(cw) * c = df/d(cw)	ac_disagreement
2019-138	- df/dc = df/d(cw) d(cw)/dc = df/d(cw) * w	misc
2019-138	Thus the proposed change seems to favor a combination of weight magnitude and the regular df/dw magnitude.	weakness
2019-138	I'd like to see how using the regular df/dw criterion would fare in single-shot pruning.	suggestion
2019-138	In particular, I expect using the plain gradient to lead to similar selections to those in Figure 2, because for constant  pixels 0 = df/d(cw) = df/dc = df/dw.	suggestion
2019-138	Suggested corrections: In related work (sec.	suggestion
2019-138	2) it is pointed that Hessian-based methods are unpractical due to the size od the Hessian.	weakness
2019-138	In fact OBD uses a diagonal approximation to the hessian, which is computed with complexity similar to the gradient, although it is typically not supported by deep learning toolkits.	weakness
2019-138	Please correct. The description of weight initialization schemes should also be corrected (sec.	weakness
2019-138	4.2). The sentence "Note that initializing neural networks is a random process, typically done using normal distribution with zero mean and a fixed variance." is wrong and artificially inflates the paper's contribution.	weakness
2019-138	Variance normalizing schemes had been known since the nineties (see efficient backprop) and are the default in many toolkits, e.g. Pytorch uses the Kaiming rule which sets the standard deviation according to the fan-in: https://github.com/pytorch/pytorch/blob/master/torch/nn/modules/linear.py#L56.	weakness
2019-138	Please enumerate the datasets (MNIST, Fashion-MNIST, CIFAR10) in the abstract, rather than saying "vision datasets", because MNIST in particular is not representative of vision datasets due to the constant zero padding, as explained before.	weakness
2019-138	Missing references: - Efficient Backprop http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf discusses variance scaling initialization, and approximations to the hessian.	weakness
2019-138	Since both are mentioned in the text this should be cited as well.	weakness
2019-138	- the Breiman non-negative garotte (https://www.jstor.org/stable/1269730) is a similar well-known technique in statistics	weakness
2019-138	Finally, I liked the paper and wanted to give it a higher score, but reduced it because of the occurrence of many broad claims made in the paper, such as: 1) method works on MNIST => abstract claims it generally works on vision datasets	weakness
2019-138	2) paper states "typically used is fixed variance init", but the popular toolkits (pytorch, keras) actually use the variance scaling one by default	weakness
2019-138	3) the badly explained distinction between connection and weight and the relation that it implies to prior work.	weakness
2019-138	I will revise the score if these claims are corrected.	suggestion
2019-138	Summary The paper focuses on pruning neural networks.	abstract
2019-138	They propose to identify the nodes to be pruned even before training the whole network (conventionally, it is done as a separate step after the nn was trained and involves a number of iterations of retraining pruned nn).	abstract
2019-138	This initial step that identifies the connections to be pruned works off a mini-batch of data.	abstract
2019-138	Authors introduce  a criterion to be used for identifying important parts of the network (connection sensitivity), that does not depend on the magnitude of the weights for neurons: they start by introducing a set of binary weights (one per a weight from a neuron) that indicate whether the connection is on or off and can be removed.	abstract
2019-138	Reformulating the optimization problem and relaxing the constraints on the binary weights, they approximate the sensitivity of the loss with respect to these indicator variables via the gradient.	abstract
2019-138	Then the normalized magnitude of these gradients is used to chose the connections to keep (keeping top k connections)	abstract
2019-138	Clarity: Well written, easy to follow	strength
2019-138	Detailed comments Overall, very interesting. Seemingly very simple idea that seem to work well.	strength
2019-138	Table 2 does look impressive and it seems that it also reduces the overfiting, and the experiment with random labels on mnist seem to demonstrate that the method indeed preserves only connections relevant to the real labels, simplifying the architecture to a point when it cant just memorize the data	strength
2019-138	Several questions/critiques: - When you relax the binary constraints, it becomes an approximation to an optimization problem, any indication of how far you are off solving it this way?	weakness
2019-138	- For the initialization method of the weights, you seem to state that VS-H is the one to use.	weakness
2019-138	I wonder if it actually task dependent and architecture dependent.	weakness
2019-138	If yes, then the propose method still has a hyperparameter - how to initialize the weights initially	weakness
2019-138	- How does it compare with just randomly dropping the connections or dropping them based on the magnitude of the initial weights.	weakness
2019-138	It seems that the meat comes from the fact that you are able to use the label and good initial values, i wonder if just doing a couple of iterations of forward-backprop and then dropping the weights based on their magnitude can give you comparable results	weakness
2019-138	- How does it compare to a distillation - it does not involve many cycles of retraining and can speed up inference time too	weakness
2019-138	-Can it replace the architecture search - initialize a large architecture, use the method to prune the connections and here you go.	weakness
2019-138	Did you try that instead of using already pre-tuned architectures like AlexNet.	suggestion

2019-139	Overview: I thank the authors for their interesting and detailed work in this paper.	misc
2019-139	I believe it has the potential to provide strong value to the community interested in using VAEs with an explicit and simple parameterization of the approximate posterior and likelihood as Gaussian.	strength
2019-139	Gaussianity can be appropriate in many cases where no sequential or discrete structure needs to be induced in the model.	strength
2019-139	I find the mathematical arguments interesting and enlightening.	strength
2019-139	However, the authors somewhat mischaracterize the scope of applicability of VAE models in contemporary machine learning, and don't show familiarity with the broad literature around VAEs outside of this case (that is, where a Gaussian model of the output would be manifestly inappropriate).	weakness
2019-139	Since the core of the paper is valuable and salvageable from a clarity standpoint, my comments below are geared towards what changes the authors may make to move this paper into the "pass" category.	misc
2019-139	Pros: - Mathematical insights are well reasoned and interesting.	strength
2019-139	Based on the insight from the analysis in the supplementary materials, the authors propose a two-stage VAE which separate learning the a parsimonious representation of the low-dimensional (lower than the ambient dimension of the input space), and the training a second VAE to learn the unknown approximate posterior.	strength
2019-139	The two-stage training procedure is both theoretically motivated and appears to enhance the output quality of VAEs w.r.t. FID score, making them rival GAN architectures on this metric.	strength
2019-139	Cons: - The title and general tone of the paper is too broad: it is only VAE models with Gaussian approximate posteriors and likelihoods.	weakness
2019-139	This is hardly the norm for most applications, contrary to the claims of the authors.	weakness
2019-139	VAEs are commonly used for discrete random variables, for example.	weakness
2019-139	Many cases where VAEs are applied cannot use a Gaussian assumption for the likelihood, which is the key requirement for the proofs in the supplement to be valid (then, the true posterior is also Gaussian, and the KL divergence between that and the approximate posterior can be driven to zero during optimization--clearly a Gaussian approximate posterior will never have zero KL divergence with a non-Gaussian true posterior).	weakness
2019-139	- None of the proofs consider the approximation error garnered by only having access to empirical samples through a sample of the ground truth population.	weakness
2019-139	(The ground-truth distribution must be defined with respect to the population rather just the dataset in hand, otherwise we lose all generalizability from a model.) Moreover, the proofs hold asymptotically.	weakness
2019-139	Generalization bounds and error from finite time approximations are very pertinent issues and these are ignored by the presented analyses.	weakness
2019-139	Such concerns have motivated many of the recent developments in approximate posterior distributions.	weakness
2019-139	Overall, the paper contains little evidence of familiarity with the recent advances in approximate Bayesian inference that have occurred over the past two years.	weakness
2019-139	- A central claim of the paper is that the two-stage VAE obviates the need for highly adaptive approximate posteriors.	weakness
2019-139	However, no comparison against those models is done in the paper.	weakness
2019-139	How does a two-stage VAE compare against one with, e.g., a normalizing flow approximate posterior?	weakness
2019-139	I acknowledge that the purpose of the paper was to argue for the Gaussianity assumption as less stringent than previously believed, but all of the mathematical arguments take place in an imagined world with infinite time and unbounded access to the population distribution.	weakness
2019-139	This is not really the domain of interest in modern computational statistics / machine learning, where issues of generalization and computational efficiency are paramount.	weakness
2019-139	- While the mathematical insights are well developed, the specifics of the algorithm used to implement the two-stage VAE are a little opaque.	weakness
2019-139	Ancestral sampling now takes place using latent samples from a second VAE.	weakness
2019-139	An algorithm box is badly needed for reproducibility.	weakness
2019-139	Recommendations / Typos I noted a few typos and omissions that need correction.	weakness
2019-139	- Generally, the mathematical proofs in section 7 of the supplement are clear.	strength
2019-139	At the top of page 11, though, the paragraph correctly begins by stating that the composition of invertible functions is invertible, but fails to establish that G is also invertible.	weakness
2019-139	Clearly it is so by construction, but the explicit reasons should be stated (as a prior sentence promises), and so I assume this is an accidental omission.	weakness
2019-139	- The title of Section 8.1 has a typo: clearly is it is the negative log of p_{theta_t} (x) which approaches its infimum rather than p_{theta_t} (x) approaching negative infinity.	weakness
2019-139	- Equation (4): the true posterior has an x as its argument instead of the latent z.	weakness
2019-139	- Missing parenthesis under Case 2 and wrong indentation.	weakness
2019-139	This analysis also seems to be cut off.	weakness
2019-139	Is the case r > d relevant here?	weakness
2019-139	* EDIT: I have read the authors' detailed response.	rebuttal_process
2019-139	It has clarified a few key issues, and convinced me of the value to the community for publication in its present (slightly edited according to the reviwers' feedback) form.	decision
2019-139	I would like to see this published and discussed at *CONF* and have revised my score accordingly.	misc
2019-139	* The paper provides a number of novel interesting theoretical results on "vanilla" Gaussian Variational Auto-Encoders (VAEs) (sections 1, 2, and 3), which are then used to build a new algorithm called "2 stage VAEs" (Section 4).	abstract
2019-139	The resulting algorithm is as stable as VAEs to train (it is free of any sort of adversarial training, it comes with a little overhead in terms of extra parameters), while achieving a quality of samples which is *very impressive* for an Auto-Encoder (AE) based generative modeling techniques (Section 5).	abstract
2019-139	In particular, the method achieves FID score 24 on the CelebA dataset which is on par with the best GAN-based models as reported in [1], thus sufficiently reducing the gap between the generative quality of the GAN-based and AE-based models reported in the literature.	abstract
2019-139	Main theoretical contributions: 1. In some cases the variational bound of Gaussian VAEs can get tight (Theorem 1).	strength
2019-139	In the context of vanilla Gaussian VAEs (Gaussian prior, encoders, and decoders) the authors show that if (a) the intrinsic data dimensionality r is equal to the data space dimensionality d and (b) the latent space dimensionality k is not smaller than r then there is a sequence of encoder-decoder pairs achieving the global minimum of the VAE objective and simultaneously (a) zeroing the variational gap and (b) precisely matching the true data distribution.	strength
2019-139	In other words, in this setting the variational bound and the Gaussian model does not prevent the true data distribution from being recovered.	strength
2019-139	2. In other cases Gaussian VAEs may not recover the actual distribution, but they will recover the real manifold (Theorems 2, 3, 4 and discussions on page 5).	strength
2019-139	In case when r < d, that is when the data distribution is supported on a low dimensional smooth manifold in the input space, things are quite different.	strength
2019-139	The authors show that there are still sequences of encoder-decoder pairs which achieves the global minimum of the VAE objective.	abstract
2019-139	However, this time only *some* of these sequences converge to the model which is in a way indistinguishable from the true data distribution (and thus again Gaussian VAEs do not fundamentally prevent the true distribution from being recovered).	weakness
2019-139	Nevertheless, all sequences mentioned above recover the true data manifold in that (a) the optimal encoder learns to use r dimensional linear subspace in the latent space to encode the inputs in a lossless and noise-free way, while filling the remaining k	strength
2019-139	- r dimensions with a white Gaussian noise and (b) the decoder learns to ignore the k	strength
2019-139	- r noisy dimensions and use the r "informative" dimensions to produce the outputs perfectly landing on the true data manifold.	strength
2019-139	Main algorithmic contributions: (0) A simple 2 stage algorithm, where first a vanilla Gaussian VAE is trained on the input dataset and second a separate vanilla Gaussian VAE is trained to match the aggregate posterior obtained after the first stage.	abstract
2019-139	The authors support this algorithm with a reasonable theoretical argument based on theoretical insights listed above (see end of page 6 - beginning of page 7).	abstract
2019-139	The algorithm achieves state-of-art FID scores across several data sets among AE based models existing in the literature.	abstract
2019-139	Review summary: I would like to say that this paper was a breath of fresh air to me.	strength
2019-139	I really liked how the authors make a strong point that *it is not the Gaussian assumptions that harm the performance of VAEs* in contrast to what is usually believed in the field nowadays.	strength
2019-139	Also, I think *the reported FID scores alone may be considered as a significant enough contribution*, because to my knowledge this is the first paper significantly closing the gap between generative quality of GAN-based models and non-adversarial AE-based methods.	strength
2019-139	*************** *** Couple of comments and typos: ***************	misc
2019-139	(0) Is the code / checkpoints going to be available anytime soon?	misc
2019-139	(1) I would mention [2] which in a way used a very similar approach, where the aggregate posterior of the implicit generative model was modeled with a separate implicit generative model.	weakness
2019-139	Of course, two approaches are very different ([2] used an adversarial training to match the aggregate posterior), however I believe the paper is worth mentioning.	suggestion
2019-139	(2) In light of the discussion on page 6 as well as some of the conclusions regarding commonly reported blurriness of the VAE models, results of Section 4.1 of [3] look quite relevant.	rebuttal_process
2019-139	(3) It would be nice to specify the dimensionality of the Sz matrix in definition 1.	suggestion
2019-139	(4) Line ater Eq. 3: I think it should be ∫pgt(x)log⁡pθ(x)dx ?	weakness
2019-139	(5) Eq 4: p_\\theta(x|x) (6) Page 4: "...	misc
2019-139	mass to most all measurable...".	weakness
2019-139	(7) Eq 34. Is it sqrt(\\gamma_t) or just \\gamma_t?	weakness
2019-139	(8) Line after Eq 40.	weakness
2019-139	Why exactly D(u^*) is finite?	weakness
2019-139	I only checked proofs of Theorems 1 and 2 in details and those looked correct.	weakness
2019-139	[1] Lucic et al., 2018.	misc
2019-139	[2] Zhao et al., Adversarially regularized autoencoders, 2017, http://proceedings.mlr.press/v80/zhao18b.html	misc
2019-139	[3] Bousquet et al., From optimal transport to generative modeling: the VEGAN cookbook.	misc
2019-139	2017, https://arxiv.org/abs/1705.07642 This paper proposed a two-stage VAE method to generate high-quality samples and avoid blurriness.	abstract
2019-139	It is accomplished by utilizing a VAE structure on the observation and latent variable separately.	abstract
2019-139	The paper exploited a collection of interesting properties of VAE and point out the problem existed in the generative process of VAE.	abstract
2019-139	I have several concerns about the paper: 1. It is necessary to explain why the second-stage VAE can have its latent variable more closely resemble N(u|0,I).	weakness
2019-139	Even if the latent variable closely resemble N(u|0,I), How does it make sure the generated images are realistic?	weakness
2019-139	I admit that the VAE model can reconstruct realistic data based on its inferred latent variable, however, when given a random sample from N(u|0,I), the generated images are not good, which is true when the dimension of the latent space is high.	weakness
2019-139	I still can't understand why a second-stage VAE can relief this problem.	weakness
2019-139	2. The adversarial auto-encoder is also proposed to solve the latent space problem, by comparison, what is the advantage of this paper?	weakness
2019-139	3. Why do you set the model as two separate stages?	weakness
2019-139	Will it enhance the performance if we train theses two-stages all together?	weakness
2019-139	4. The proofs for the theory 2 and 3 are under the assumption that the manifold dimension of the observation is r, while in reality it is difficult to obtain this r, do these theories applicable if we choose a value for the dimension of the latent space that is smaller than the real manifold dimension of the observation?	weakness
2019-139	How will it affect the performance of the proposed method?	weakness
2019-139	5. The value of r and k in each experiment should be specified.	weakness

2019-193	The paper under review builds useful insights and novel methods for graph convolutional networks, based on the Lanczos algorithm for efficient computations involving the graph Laplacian matrices induced by the neighbor edge structure of graph networks.	abstract
2019-193	While previous work [35] has explored the Lanczos algorithm from numerical linear algebra as a means to accelerate computations in graph convolutional networks, the current paper goes further by: (1) exploring in significant more depth the low rank decomposition underlying the Lanczos algorithm.	abstract
2019-193	(2) learning the spectral filter (beyond the Chebychev design) and potentially also the graph kernel and node embedding.	abstract
2019-193	(3) drawing interesting connections with graph diffusion methods which naturally arise from the matrix power computation inherent to the Lanczos iteration.	abstract
2019-193	The paper includes a systematic evaluation of the proposed approach and comparison with existing methods on two tasks: semi-supervised learning in citation networks and molecule property prediction from interactions in atom networks.	abstract
2019-193	The main advantage of the proposed method as illustrated in particular by the experimental results in the citation network domain is its ability to generalize well in the presence of a small  amount of training data, which the authors attribute to its efficient capturing of both short- and long-range interactions.	strength
2019-193	In terms of presentation quality, the paper is clearly written, the proposed methods are well explained, and the notation is consistent.	strength
2019-193	Overall, a good paper. Minor comment: page 3, footnote: "When faced with a non-symmetric matrix, one can resort to the Arnoldi algorithm.": I was wondering if the authors have tried that?	weakness
2019-193	I think that the Arnoldi algorithm for non-symmetric matrices are significantly less stable than their Lanczos counterparts for symmetric matrices.	weakness
2019-193	The authors propose a novel method for learning graph convolutional networks.	abstract
2019-193	The core idea is to use the Lanczos algorithm to obtain a low-rank approximation of the graph Laplacian.	abstract
2019-193	The authors propose two ways to include the Lanczos algorithm.	abstract
2019-193	First, as a preprocessing step where the algorithm is applied once on the input graph and the resulting approximation is fixed during learning.	abstract
2019-193	Second, by including a differentiable version of the algorithm into an end-to-end trainable model.	abstract
2019-193	The proposed method is novel and achieves good results on a set of experiments.	strength
2019-193	The authors discuss related work in a thorough and meaningful manner.	strength
2019-193	There is not much to criticize.	ac_disagreement
2019-193	This is a very good paper.	strength
2019-193	The almost 10 pages are perhaps a bit excessive considering there was an (informal) 8 page limit.	weakness
2019-193	It might make sense to provide a more accessible discussion of the method and Theorem 1, and move some more detailed/technical parts in pages 4, 5, and 6 to an appendix.	suggestion
2019-193	This paper proposes to use a Lanczos alogrithm, to get approximate decompositions of the graph Laplacian, which would facilitate the computation and learning of spectral features in graph convnets.	abstract
2019-193	It further proposes an extension with back propagation through the Lanczos algorithm, in order to train end to end models.	abstract
2019-193	Overall, the idea of using Lanczos algorithm to bypass the computation of the eigendecomposition, and thus simplify filtering operations in graph signal processing is not new [e.g., 35].	weakness
2019-193	However, using this algorithm in the framework of graph convents is new, and certainly interesting.	strength
2019-193	The authors seem to claim that their method permits to learn spectral filters, what other methods could not do - this is not completely true and should probably be rephrased more clearly: many graph convnets, actually learn features.	weakness
2019-193	The general construction and presentation of the algorithms are generally clear, and pretty complete.	strength
2019-193	A few things that could be clarified are the following: - in the spectral filters of Eq (4), what gets fundamentally different from polynomial filters proposed in other graph convnets architectures?	weakness
2019-193	- what happens when the graph change?	weakness
2019-193	Do the learned features make sense on different graphs?	weakness
2019-193	And if yes, why? If not, the authors should be more explicit in their presentation	weakness
2019-193	- what is the complexity of the proposed methods?	weakness
2019-193	that should be minimally discussed (at least), as it is part of the key motivations for the proposed algorithms	weakness
2019-193	- how is the learning done in 3.2?	weakness
2019-193	If there is any learning at all?	weakness
2019-193	(btw, S below Eq (6) is a poor notation choice, as S is used earlier for something else)	weakness
2019-193	- the results are not very impressive - they are good, but not stellar, and could benefit from showing an explicit tradeoff in terms of complexity too?	weakness
2019-193	The discussion in the related work, and the analogy with manifold learning are interesting.	strength
2019-193	However, that brings probably to one of the main issues with the papers - the authors are obviously very knowledgeable in graph convnets, graph signal processing, and optimisation.	weakness
2019-193	However, there are really too many things in this paper, which leads to numerous shortcuts, and some time confusion.	weakness
2019-193	Given the page limits, not everything can be treated with the level of details that it would deserve.	weakness
2019-193	It might be good to consider trimming down the paper to its main and core aspects for the next version.	suggestion

2019-231	Let me start by apologizing for the delayed review - in fact I was asked today to replace an earlier assigned reviewer.	misc
2019-231	Hopefully the clarifications I request won't be too time consuming to meet the deadline coming up.	misc
2019-231	### First of all, the problem which the authors are attempting to answer is quite important: the effect of over-parametrization is not well understood on a theoretical level.	strength
2019-231	As the paper illustrate, 2-layer networks are already capable of generalizing while being over-parameterized, therefore justifying their setting.	strength
2019-231	Next this paper motivates the study of complexity quantities that tend to decrease with the number of parameters, in particular figure 3 motivates the conjecture that the complexity measure in Theorem 2 can control generalization error.	strength
2019-231	The paper also does a great job comparing related work, motivating their results.	strength
2019-231	### At this point, I would like to request a couple of clarifications in the proofs.	weakness
2019-231	Perhaps it's due to the fact that I only spent a day reading, but at least I think we could improve on its readability.	weakness
2019-231	Regardless, I currently do not yet trust a couple of the proofs, and I believe the acceptance of this paper should be conditioned on confirming the correctness of these proofs.	decision
2019-231	(1) Let's start with Lemma 10.	weakness
2019-231	In the middle equation block, we obtain a bound	ac_disagreement
2019-231	\\| alpha^prime \\|_p^p <= beta^p ( 1 + D/K )	suggestion
2019-231	and the proof concludes alpha^prime is in Q.	weakness
2019-231	However this cannot be the case for all alpha^prime.	suggestion
2019-231	Consider x=0 which is in S_{p, beta}^D, then we have alpha^prime = 0 as well.	weakness
2019-231	In the definition of Q, we require all the j's to sum up to K+D, which is not met here.	weakness
2019-231	At the same time, the next claim	weakness
2019-231	\\| alpha \\|_2 <= D^{1/2 - 1/p} \\| alpha^prime \\|_p does not seem to follow from the above calculations.	weakness
2019-231	In particular, alpha^prime seems to be defined with respect to an x in S_{p, beta}, however in this case we did not specify such an x.	weakness
2019-231	Perhaps did you mean there exist such an alpha^prime?	weakness
2019-231	(2) In the proof of Theorem 3, there is an important inequality needed to complete the proof max{ <s, f_i> , <s, -f_i> } >= 1/2 * ( <s, [f_i]_+> + <s, [-f_i]_+> )	weakness
2019-231	Perhaps I am missing something obvious, but I believe this inequality fails when we choose s as a constant vector, and f_i to have the same number of positive and negative signs (which is possible in a Hadamard matrix).	weakness
2019-231	In this case, the left hand side should be equal to zero, where as the right hand side will be positive.	weakness
2019-231	### To summarize, if these proofs can be confirmed, I believe this paper would have made significant contribution to the problem of over-parametrization in deep learning, and of course should be accepted.	decision
2019-231	### I corrected several typos and found minor issues as I read, perhaps this will be useful to improve readability as well.	rebuttal_process
2019-231	Page 13, proof of Lemma 8	misc
2019-231	- after the V_0 term is separated, there is a sup over \\|V_0\\|_F <= r in the expectation, which should be \\|V-V_0\\|_F <= r instead.	rebuttal_process
2019-231	Page 14, Lemma 9 - the lemma did not define rho_{ij} in the statement	weakness
2019-231	Page 15, proof of Lemma 9	misc
2019-231	- in equation (12), there is an x_y vector that should x_t	weakness
2019-231	Page 15, proof of Theorem 1	weakness
2019-231	- while I eventually figured it out, it's unclear how Lemma 8 is applied here.	weakness
2019-231	Perhaps one more step identifying the exact matrices in the statement of Lemma 8 will be helpful to future readers, and maybe explain where the sqrt(2) factor come from as well.	suggestion
2019-231	Page 16, proof of Lemma 10	misc
2019-231	- in the beginning of the proof, to stay consistent with the notation, we should replace S_{p, beta} with S_{p, beta}^D	suggestion
2019-231	- I believe the cardinality of Q should be (K + D - 1) choose (D - 1), as we need to choose positive j's to sum up to (K+D) in the definition of Q.	weakness
2019-231	This reduces down to the problem of choosing natural numbers j's summing K, which is (K+D-1) choose (D-1).	suggestion
2019-231	Consider the stack exchange post here: https://math.stackexchange.com/questions/919676/the-number-of-integer-solutions-of-equations	suggestion
2019-231	Page 16, proof and statement of Lemma 11	misc
2019-231	- I believe in the first term, the factor should be m instead of sqrt(m).	weakness
2019-231	I think the mistake happened when applying the union bound, as it should only affect the term containing delta	weakness
2019-231	Page 17, Lemma 12 - same as Lemma 11, we should have m instead of sqrt(m)	weakness
2019-231	Page 18, proof of Theorem 3	weakness
2019-231	- at the bottom the statement "F is orthogonal" does not imply the norm is less than 1, but rather we should say "F is orthonormal"	weakness
2019-231	Page 19, proof of Theorem 3	misc
2019-231	- at the top, "we will omit the index epsilon" should be "xi" instead	suggestion
2019-231	- in the final equation block, we have the Rademacher complexity of F_{W_2}, instead it should be F_{W^prime} It is shown empirically that common algorithms used in supervised learning (SGD) yield networks for which such upper bound decreases as the number of hidden units increases.	suggestion
2019-231	This might explain why in some cases overparametrized models have better generalization properties.	suggestion
2019-231	This paper tackles the important question of why in the context of supervised learning, overparametrized neural networks in practice generalize better.	abstract
2019-231	First, the concepts of \\textit{capacity} and \\textit{impact} of a hidden unit are introduced.	abstract
2019-231	Then, {\\bf Theorem 1} provides an upper bound for the empirical Rademacher complexity of the class of 1-layer networks with hidden units of bounded \\textit{capacity} and \\textit{impact}.	abstract
2019-231	Next, {\\bf Theorem 2} which is the main result, presents a new upper bound for the generalization error of 1-layer networks.	abstract
2019-231	An empirical comparison with existing generalization bounds is made and the presented bound is the only one that in practice decreases when the number of hidden units grows.	abstract
2019-231	Finally {\\bf Theorem 3} is presented, which provides a lower bound for the Rademacher complexity of a class of neural networks, and such bound is compared with existing lower bounds.	abstract
2019-231	## Strengths - The paper is theoretically sound, the statement of the theorems are clear and the authors seem knowledgeable when bounding the generalization error via Rademacher complexity estimation.	strength
2019-231	- The paper is readable and the notation is consistent throughout.	strength
2019-231	- The experimental section is well described, provides enough empirical evidence for the claims made, and the plots are readable and well presented, although they are best viewed on a screen.	strength
2019-231	- The appendix provides proofs for the theoretical claims in the paper.	strength
2019-231	However, I cannot certify that they are correct.	strength
2019-231	- The problem studied is not new, but to my knowledge the presented bounds are novel and the concepts of capacity and impact are new.	strength
2019-231	Theorem 3 improves substantially over previous results.	strength
2019-231	- The ideas presented in the paper might be useful for other researchers that could build upon them, and attempt to extend and generalize the results to different network architectures.	strength
2019-231	- The authors acknowledge that there might be other reasons that could also explain the better generalization properties in the over-parameterized regime, and tone down their claims accordingly.	strength
2019-231	## Weaknesses \\begin{itemize} - The abstract reads "Our capacity bound correlates with the behavior of test error with increasing network sizes ...", it should be pointed out that the actual bound increases with increasing network size (because of a sqrt(h/m) term), and that such claim holds only in practice.	weakness
2019-231	- In page 8 (discussion following Theorem 3) the claim	weakness
2019-231	"... all the previous capacity lower bounds for spectral norm bounded classes of neural networks (...) correspond to the Lipschitz constant of the network.	weakness
2019-231	Our lower bound strictly improves over this ...", is not clear.	weakness
2019-231	Perhaps a more concise presentation of the argument is needed.	weakness
2019-231	In particular it is not clear how a lower bound for the Rademacher complexity of F_W translates into a lower bound for the rademacher complexity of l_\\gamma F_W.	weakness
2019-231	This makes the claim of tightness of Theorem 1 not clear.	weakness
2019-231	Also this makes the initial claim about the tightness of Theorem 2 not clear.	weakness
2019-231	The authors aim to shed light on the role of over-parametrization in generalization error.	abstract
2019-231	They do so for the special case of 2 layer fully connected ReLU networks, a "simple" setting where one still sees empirically that the test error decreasing as over-parametrization increases.	abstract
2019-231	Based on empirical observations of norms (and norms relative to initialization) in trained overparametrized networks, the authors are led to the definition of a new norm-bounded class of neural networks.	suggestion
2019-231	Write u_i for the vector of weights incoming to hidden node i.	suggestion
2019-231	Write v_i for the weights outgoing from hidden node i.	suggestion
2019-231	They study classes where the Euclidean norm of v_i is bounded by a constant alpha_i and where the Euclidean norm of u_i - u^0_i is bounded by beta_i, where u^0_i is the value of u_i after random initialization.	abstract
2019-231	Call this class F_{alpha,beta} where alpha,beta are specific vectors of bounds.	suggestion
2019-231	The main result is a bound on the empirical Rademacher complexity of F_{alpha,beta}.	abstract
2019-231	The authors also given lower bounds on the empirical Rademacher complexity for carefully chosen data points, showing that the bounds are tight.	abstract
2019-231	These Rademacher bounds yield standard bounds on the ramp loss for fixed alpha,beta, and margin, and then a union bound argument extends the bound to data-dependent alpha,beta and margin.	abstract
2019-231	The authors compare the bounds to existing norm-based bounds in the literature.	abstract
2019-231	The basic argument is that the terms in other bounds tend to grow as networks get much larger, while their terms shrink.	abstract
2019-231	Note that at no point are the bounds in this paper "nonvacuous", ie they are always larger than one.	abstract
2019-231	In summary, I think this is a strong paper.	strength
2019-231	The explanatory power of the results are still oversold in my opinion, even if they use hedged language like "could explain the role...".	weakness
2019-231	But the work is definitely pointing the way towards an explanation and deserves publication.	decision
2019-231	The technical results in the appendix will be of interest to the learning theory community.	strength
2019-231	issues: "could explain role of over-parametrization".	weakness
2019-231	Perhaps this work might point the way to an explanation, but it does not yet provide an explanation.	weakness
2019-231	It is a big improvement it seems.	weakness
2019-231	"bound improves over the existing bounds".	weakness
2019-231	From this statement and the discussion comparing the bounds, it is not clear whether this bound formally dominates existing bounds or merely does so empirically (or under empirical conditions).	weakness
2019-231	typos: bigger than the Lipschitz CONSTANT of the network class	weakness
2019-231	H undefined Rademacher defined for H but must be defined on loss class (or a generic function class, not H)	weakness
2019-231	"we need to cover" --> "it suffices to"	weakness
2019-231	"the following two inequaliTIES hold by Lemma 8"	weakness
2019-231	bibliography is a mess: half of the arxiv papers are published.	weakness
2019-231	typos everywhere, very sloppy. (This review was requested late in the process due to another reviewer dropping out of the process.)	weakness
2019-231	[UPDATE]. The authors addressed my concerns stated in my review above.	rebuttal_process
2019-231	I think the bibliography has improved and I recommend acceptance.	decision

2019-301	This paper presents a class of neural networks that does not have bad local valleys.	abstract
2019-301	The "no bad local valleys" implies that for any point on the loss surface there exists a continuous path starting from it, on which the loss doesn't increase and gets arbitrarily smaller and close to zero.	abstract
2019-301	The key idea is to add direct skip connections from hidden nodes (from any hidden layer) to the output.	abstract
2019-301	The good property of loss surface for networks with skip connections is impressive and the authors present interesting experimental results pointing out that	strength
2019-301	* adding skip connections doesn't harm the generalization.	strength
2019-301	* adding skip connections sometimes enables training for networks with sigmoid activation functions, while the networks without skip connections fail to achieve reasonable performance.	strength
2019-301	* comparison of the generalization performance for the random sampling algorithm vs SGD and its connection to implicit bias is interesting.	strength
2019-301	However, from a theoretical point of view, I would say the contribution of this work doesn't seem to be very significant, for the following reasons: * In the first place, figuring out "why existing models work" would be more meaningful than suggesting a new architecture which is on par with existing ones, unless one can show a significant performance improvement over the other ones.	weakness
2019-301	* The proof of the main theorem (Thm 3.3) is not very interesting, nor develops novel proof techniques.	weakness
2019-301	It heavily relies on Lemma 3.2, which I think is the main technical contribution of this paper.	weakness
2019-301	Apart from its technicality in the proof, the statement of Lemma 3.2 is just as expected and gives me little surprise, because having more than N hidden nodes connected directly to the output looks morally "equivalent" to having a layer as wide as N, and it is known that in such settings (e.g. Nguyen & Hein 17') it is easy to attain global minima.	weakness
2019-301	* I also think that having more than N skip connections can be problematic if N is very large, for example N>10^6.	weakness
2019-301	Then the network requires at least 1M nodes to fall in this class of networks without bad local valleys.	weakness
2019-301	If it is possible to remove this N-hidden-node requirement, it will be much more impressive.	suggestion
2019-301	Below, I'll list specific comments/questions about the paper.	misc
2019-301	* Assumption 3.1.2 doesn't make sense.	weakness
2019-301	Assumption 3.1.2 says "there exists N neurons satisfying…" and then the first bullet point says "for all j = 1, …, M".	weakness
2019-301	Also, the statement "one of the following conditions" is unclear.	weakness
2019-301	Does it mean that we must have either "N satisfying the first bullet" or "N satisfying the second bullet", or does it mean we can have N/2 satisfying the first and N/2 satisfying the second?	weakness
2019-301	* The paper does not describe where the assumptions are used.	weakness
2019-301	They are never used in the proof of Theorem 3.3, are they?	weakness
2019-301	I believe that they are used in the proof of Lemma 3.2 in the appendix, but if you can sketch/mention how the assumptions come into play in the proofs, that will be more helpful in understanding the meaning of the assumptions.	weakness
2019-301	* Are there any specific reasons for considering cross-entropy loss only?	weakness
2019-301	Lemma 3.2 looks general, so this result seems to be applicable to other losses.	weakness
2019-301	I wonder if there is any difficulty with different losses.	weakness
2019-301	* Are hidden nodes with skip connections connected to ALL m output nodes or just some of the output nodes?	weakness
2019-301	I think it's implicitly assumed in the proof that they are connected to all output nodes, but in this case Figure 2 is a bit misleading because there are hidden nodes with skip connections to only one of the output nodes.	weakness
2019-301	* For the experiments, how did you deal with pooling layers in the VGG and DenseNet architectures?	weakness
2019-301	Does max-pooling satisfy the assumptions?	weakness
2019-301	Or the experimental setting doesn't necessarily satisfy the assumptions?	weakness
2019-301	* Can you show the "improvement" of loss surface by adding skip connections?	weakness
2019-301	Maybe coming up with a toy dataset and network WITH bad local valleys will be sufficient, because after adding N skip connections the network will be free of bad local valleys.	suggestion
2019-301	Minor points * In the Assumption 3.1.3, the N in r≠s∈N means [N]?	weakness
2019-301	* In the introduction, there is a sentence "potentially has many local minima, even for simple models like deep linear networks (Kawaguchi, 2016)," which is not true.	weakness
2019-301	Deep linear networks have only global minima and saddle points, even for general differentiable convex losses (Laurent & von Brecht 18' and Yun et al. 18').	weakness
2019-301	* Assumption 3.1.3 looked a bit confusing to me at first glance.	weakness
2019-301	You might want to add some clarification such as "for example, in the fully connected network case, this means that all data points are distinct.	suggestion
2019-301	The paper analyzes the loss landscape of a class of deep neural networks with skip connections added to the output layer.	abstract
2019-301	It proves that with the proposed structure of DNN, there are uncountably many solutions with zero training error, and the landscape has no bad local valley or local extrema.	abstract
2019-301	Overall I really enjoy reading the paper.	misc
2019-301	The assumptions to aid the proof are very natural and much softer than the existing literature.	weakness
2019-301	As far as I'm concerned, the setting is very close to real deep neural networks and the paper is a breakthrough in the area.	strength
2019-301	The experiments also consolidate that the theoretical settings are natural and useful, namely, with enough skip connections and specially chosen activation functions.	strength
2019-301	The presentation of the paper is intuitive and easy to follow.	strength
2019-301	I've also checked all the proof and think it's brilliantly and elegantly written.	strength
2019-301	My only complaint is about the experiments.	weakness
2019-301	As we all know that both VGG and the sigmoid activation are commonly used DL tools, and why do they fail to generalize when used together?	weakness
2019-301	Does the network fail to converge or is it overfitting?	weakness
2019-301	The authors should try tuning the parameters and present a proper result.	suggestion
2019-301	With that said, since the paper is more about theoretical findings, this issue doesn't influence my recommendation to accept the paper.	decision
2019-301	Minor issues: I think it's better to formally define "bad local valley" somewhere in the paper.	weakness
2019-301	From what I read, the definition of "bad local valley" is implied by the abstract and in the proof of Theorem 3.3(2), but I did not find a formal definition anywhere else.	weakness
2019-301	In proof number 4 (of Theorem 3.3), the statement should be "any *principle* submatrices of negative semi-definite matrices are also NSD", and it's not true otherwise.	weakness
2019-301	But this typo doesn't influence the proof.	weakness
2019-301	Also, it seems the proof of 3 is somewhat redundant, since local minimum is a special case of your "bad local valley".	weakness
2019-301	It seems the analysis could not possibly be extended to the ReLU activation, since it will break the analytical property of the function.	weakness
2019-301	Just out of curiosity, do the authors have some further thoughts on non-differentiable activations?	weakness
2019-301	This paper shows that a class of deep neural networks have no spurious local valleys –--implying no strict local-minima.	abstract
2019-301	The family of neural networks studied includes a wide variety of network structure such as (a variant of) DenseNet. Overall, this paper makes some progress, improving previous results on over-parametrized networks.	abstract
2019-301	Pros: The flexibility of the network structure is an interesting point.	strength
2019-301	Cons: CNN was covered in previous related works (so weight sharing is not a new contribution); DenseNet is not explicitly covered in this work (I mean current DenseNet does not have N skip-connections to output; correct me if wrong).	weakness
2019-301	The simulation part is not that clear, and I have a few questions that I hope the authors can answer.	weakness
2019-301	Some comments/suggestions: 1) Training error needs to be discussed.	weakness
2019-301	Page 8 says "This effect can be directly related to our result of Theorem 3.3 that the loss landscape of skip-networks has no bad local valley and thus it is not difficult to reach a solution with zero training error".	weakness
2019-301	This relation is not justified.	weakness
2019-301	The implication of Thm 3.3 is that getting zero training error is easier, but the tables are only for test error.	weakness
2019-301	Showing training error is the only way to connect to Thm 3.3.	weakness
2019-301	I expect to see a high training error for C-10, original VGG and sigmoid activation functions, and zero training error for both skip-SGD (rand) and skip-SGD (SGD).	weakness
2019-301	This paper has no theory on generalization, thus if a whole section is just about "investigating generalization error", then the connection to theoretical parts is weak --btw, one connection is the comparison of two algorithms, which fits the context well, and thus interesting (though comparison result itself probably not surprising).	weakness
2019-301	2) Data augmentation. "Note that the rand algorithm cannot be used with data augmentation in a straightforward way and thus we skip it for this part." Why?	weakness
2019-301	With data augmentation, is M still larger than N?	weakness
2019-301	If yes, then the number of added skip connection is different for C-10 and C-10-plus, which is not mentioned in the instruction of Table 2.	weakness
2019-301	3)It may be better to mention explicitly that "it is possible to have bad local min" –perhaps in abstract and/or introduction.	weakness
2019-301	--Although "no sub-optimal strict local minima" is mentioned, readers, especially non-optimizers, might not notice "strict".	weakness
2019-301	--In fact, in the 1st round read, I do not have a strong impression of "strict".	weakness
2019-301	Later I realized it. Mentioning this can be helpful.	weakness
2019-301	4) Some references I suggest to include: [R1] Yu, X. and Chen, G.	suggestion
2019-301	On the local minima free condition of backpropagation learning.	suggestion
2019-301	1995. --related work. [R2] Lu, H., Kawaguchi, K.	misc
2019-301	Depth creates no bad local minima.	misc
2019-301	2017. --also deep nets. [R3] Liang, S., Sun, R., Li, Y., & Srikant, R.	misc
2019-301	"Understanding the loss surface of neural networks for binary classification." 2018.	misc
2019-301	--Also study SoftPlus neurons. [R4] Nouiehed, M., & Razaviyayn, M.	misc
2019-301	Learning Deep Models: Critical Points and Local Openness.	misc
2019-301	2018. --also deep nets. Minor questions: --Exact 10% test accuracy for a few cases.	weakness
2019-301	Why exact 10%?	weakness

2019-312	Revision: The authors have taken my advice and addressed my concerns wholeheartedly.	rebuttal_process
2019-312	It is clear to me that they have taken efforts to make notable progress during the rebuttal period.	rebuttal_process
2019-312	Summary of their improvements: - They have extended their methodology to handle multiple strokes	rebuttal_process
2019-312	- The model has been converted to a latent-space generative model (similar to Sketch-RNN, where the latent space is from a seq2seq VAE, and SPIRAL where the latent space is used by an adversarial framework)	rebuttal_process
2019-312	- They have ran addition experiments on a diverse set of datasets (now includes Kanji and QuickDraw), in addition to omniglot and mnist.	rebuttal_process
2019-312	- Newer version is better written, and I like how they are also honest to admit limitations of their model rather than hide them.	strength
2019-312	I think this work is a great companion to existing work such as Sketch-RNN and SPIRAL.	strength
2019-312	As mentioned in my original review, the main advantage of this is the ability to train with very limited compute resources, due to the model-based learning inspired by model-based RL work (they cited some work on world models).	strength
2019-312	Taking important concepts from various different (sub) research areas and synthesizing them into this nice work should be an inspiration to the broader community.	strength
2019-312	The release of their code to reproduce results of all the experiments will also facilitate future research into this exciting topic of vector-drawing models.	strength
2019-312	I have revised my score to 8, since I believe this to be at least in the better half of accepted papers at *CONF* based on my experience of publishing and attending the conference in the past few years.	rating_summary
2019-312	I hope the other reviewers can have some time to reevaluate the revision.	misc
2019-312	Original review: Summary: they propose a differentiable learning algorithm that can output a brush stroke that can approximate a pixel image input, such as MNIST or Omniglot.	abstract
2019-312	Unlike sketch-pix2seq[3] (which is a pixel input -> sketch output model based on sketch-rnn[2]), their method trains in an unsupervised manner and does not require paired image/stroke data.	abstract
2019-312	They do this via training a "world model" to approximate brush painting software and emulate it.	abstract
2019-312	Since this emulation model is differentiable, they can easily train an algorithm to output a stroke to approximate the drawing via back propagation, and avoid using RL and costly compute such in earlier works such as [1].	abstract
2019-312	The main strength of this paper is the original thought that went into it.	strength
2019-312	From reading the paper, my guess is the authors came from a background that is not pure ML research (for instance, they are experts in Javascript, WebGL, and their writing style is easy to read), and it's great to see new ideas into our field.	strength
2019-312	While research from big labs [1] have the advantage of having access to massive compute so that they can run large scale RL experiments to train an agent to "sketch" something that looks like MNIST or Omniglot, the authors probably had limited resources, and had to be more creative to come up with a solution to do the same thing that trains in a couple of hours using a single P40 GPU.	weakness
2019-312	Unlike [1] that used an actual software rendering package that is controlled by a stroke-drawing agent, their creative approach here is to train a generator network to learn to approximate a painting package they had built, and then freeze the weights of this generator to efficiently train an agent to draw.	weakness
2019-312	The results for MNIST and Omniglot look comparable to [1] but achieved with much fewer resources.	weakness
2019-312	I find this work refreshing, and I think it can be potentially much more impactful than [1] since people can actually use it with limited compute resources, and without using RL.	strength
2019-312	That being said, things are not all rosy, and I feel there are things that need to be done for this work to be ready for publication in a good venue like *CONF*.	decision
2019-312	Below are a few of my suggestions that I hope will help the authors improve their work, for either this conference, or if it gets rejected, I encourage the authors to try the next conference with these improvements: 1) multiple strokes, longe strokes.	decision
2019-312	I don't think having a model that can output only a single stroke is scalable to other (simple) datasets such pixel versions of KangiVG [4] or QuickDraw [5].	weakness
2019-312	The authors mentioned the need for an RNN, but couldn't the encoder just output the stroke in a format that contains the pen-down / pen-up event, like the stroke format suggested in [2]?	weakness
2019-312	Maybe, maybe not, but in either case, for this work to matter, multiple stroke generation is needed.	weakness
2019-312	Most datasets are also longer than 16 points, so you will need to show that your method works for say 80-120 points for this method to be comparable to existing work.	weakness
2019-312	If you can't scale up 16 points, would like to see a detailed discussion as to why.	weakness
2019-312	2) While I like this method and approach, to play devil's advocate, what if I simply use an off the shelf bmp-to-svg converter that is fast and efficient (like [6]), and just build a set of stroke data from a dataset of pixel data, and train a sketch-rnn type model described in [3] to convert from pixel to stroke?	weakness
2019-312	What does this method offer that my description fails to offer?	weakness
2019-312	Would like to see some discussion there.	misc
2019-312	3) I'll give a hint for as to what I think for (2).	misc
2019-312	I think the value in this method is that it can be converted to a full generative model with latent variables (like a VAE, GAN, sketch-rnn) where you can feed in a random vector (gaussian or uniform), and get a sketch as an output, and do things like interpolate between two sketches.	strength
2019-312	Correct me if I'm wrong, but I don't think the encoder here in the first figure outputs an embedding that has a Gaussian prior (like a VAE), so it fails to be a generative model (check out [1], even that is a latent variable model).	weakness
2019-312	I think the model can be easily converted to one though to address this issue, and I strongly encourage the authors to try enforcing a Gaussian prior to an embedding space (that can fit right between the 16x16x128 average pooling op to the fully connected 1024 sized layer), and show results where we can interpolate between two latent variables and see how the vector sketches are interpolated.	suggestion
2019-312	This has also been done in [2].	suggestion
2019-312	If the authors need space, I suggest putting the loss diagrams near the end into the appendix, since those are not too interesting to look at.	suggestion
2019-312	4) As mentioned earlier, I would love to see experimental results on [4] KangiVG and [5] QuickDraw datasets, even subsets of them.	suggestion
2019-312	An interesting result would be to compare the stroke order of this algorithm with the natural stroke order for human doodles / Chinese characters.	suggestion
2019-312	Minor points: a) The figures look like they are bitmap, pixel images, but for a paper advocating stroke/vector images, I recommend exporting the diagrams in SVG format and convert them to PDF so they like crisp in the paper.	suggestion
2019-312	b) Write style: There are some terms like "huge" dataset that is subjective and relative.	weakness
2019-312	While I'm happy about the writing style of this paper, maybe some reviewers who are more academic types might not like it and have a negative bias against this work.	misc
2019-312	If things don't work out this time, I recommend the authors asking some friends who have published (successfully) at good ML conferences to proof read this paper for content and style.	suggestion
2019-312	c) It's great to see that the implementation is open sourced, and put it on github.	strength
2019-312	Next time, I recommend uploading it to an anonymous github profile/repo, although personally (and for the record, in case area chairs are looking), I don't mind at all in this case, and I don't think the author's github address revealed any real identity (I haven't tried digging deeper).	suggestion
2019-312	Some other reviewers / area chairs might not like to see a github link that is not anonymized though.	weakness
2019-312	So in the end, even though I really like this paper, I can only give a score of 6 (edit: this has since been revised upward to 8).	rating_summary
2019-312	If the authors are able to address points 1-4, please do what you can in the next few weeks and give it your best shot.	suggestion
2019-312	I'll look at the paper again and will revise the score upwards by a point or two if I think the improvements are there.	misc
2019-312	If not, and this work ends up getting rejected, please consider improving the work later on and submitting to the next venue.	decision
2019-312	Good luck! [1] SPIRAL https://arxiv.org/abs/1804.01118 [2] sketch-rnn https://arxiv.org/abs/1704.03477 [3] sketch-pix2seq https://arxiv.org/abs/1709.04121 [4] http://kanjivg.tagaini.net/ [5] https://quickdraw.withgoogle.com/data [6] https://vectormagic.com/ Revision: The addition of new datasets and the qualitative demonstration of latent space interpolations and algebra are quite convincing.	strength
2019-312	Interpolations from raster-based generative models such as the original VAE tend to be blurry and not semantic.	strength
2019-312	The interpolations in this paper do a good job of demonstrating the usefulness of structure.	strength
2019-312	The classification metric is reasonable, but there is no comparison with SPIRAL, and only a comparison with ablated versions of the StrokeNet agent.	weakness
2019-312	I see no reason why the comparison with SPIRAL was removed for this metric.	weakness
2019-312	Figure 11 does a good job of showing the usefulness of gradients over reinforcement learning, but should have a better x range so that one of the curves doesn't just become a vertical line, which is bad for stylistic reasons.	weakness
2019-312	The writing has improved, but still has stylistic and grammatical issues.	rebuttal_process
2019-312	A few examples, "there're", "the network could be more aware of what it's exactly doing", "discriminator loss given its popularity and mightiness to achieve adversarial learning".	rebuttal_process
2019-312	A full enumeration would be out of scope of this review.	rebuttal_process
2019-312	I encourage the authors to iterate more on the writing, and get the paper proofread by more people.	suggestion
2019-312	In summary, the paper's quality has significantly improved, but some presentation issues keep it from being a great paper.	weakness
2019-312	The idea presented in the paper is however interesting and timely and deserves to be shared with the wider generative models community, which makes me lean towards an accept.	decision
2019-312	Original Review: This paper deals with the problem of strokes-based image generation (in contrast to raster-based).	abstract
2019-312	The authors define strokes as a list of coordinates and pressure values along with the color and brush radius of a stroke.	abstract
2019-312	Then the authors investigate whether an agent can learn to produce the stroke corresponding to a given target image.	abstract
2019-312	The authors show that they were able to do so for the MNIST and OMNIGLOT datasets.	abstract
2019-312	This is done by first training an encoder-decoder pair of neural networks where the latent variable is the stroke, and the encoder and decoder have specific structure which takes advantage of the known stroke structure of the latent variable.	abstract
2019-312	The paper contains no quantitative evaluation, either with existing methods or with any baselines.	abstract
2019-312	No ablations are conducted to understand which techniques provide value and which don't.	abstract
2019-312	The paper does present some qualitative examples of rendered strokes but it's not clear whether these are from the training set or an unseen test set.	weakness
2019-312	It's not clear whether the model is generalizing or not.	weakness
2019-312	The writing is also very unclear.	weakness
2019-312	I had to fill in the blanks a lot.	weakness
2019-312	It isn't clear what the objective of the paper is.	weakness
2019-312	Why are we generating strokes?	weakness
2019-312	What use is the software for rendering images from strokes?	weakness
2019-312	Is it differentiable? Apparently not. The authors talk about differentiable rendering engines, but ultimately we learn that a learnt neural network decoder is the differentiable renderer.	weakness
2019-312	To improve this paper and make it acceptable, I recommend the following: 1. Improve the presentation so that it's very clear what's being contributed.	suggestion
2019-312	Instead of writing the chronological story of what you did, instead you should explain the problem, explain why current solutions are lacking, and then present your own solutions, and then quantify the improvements from your solution.	suggestion
2019-312	2. Avoid casual language such as "Reason may be", "The agent is just a plain", "since neural nets are famouse for their ability to approximate all sorts of functions".	suggestion
2019-312	3. Show that strokes-based generation enables capabilities that raster-based generation doesn't.	suggestion
2019-312	For instance, you could show that the agent is able to systematically generalize to very different types of images.	suggestion
2019-312	I'd also recommend presenting results on datasets more complex than MNIST and OMNIGLOT.	suggestion
2019-312	The paper proposes to use a differentiable drawing environment to synthesize images and provides information about some initial experiments.	abstract
2019-312	Not yet great about this paper: - the paper feels premature: There is a nice idea, but restricting the drawing environment to be	weakness
2019-312	- Some of the choices in the paper are a bit surprising, e.g. the lines in the drawing method are restricted to be at most 16 points long.	weakness
2019-312	If you look at real drawing data (e.g. the quickdraw dataset: https://quickdraw.withgoogle.com/data) you will find that users draw much longer lines typically.	weakness
2019-312	EDIT: the new version of the paper is much better but still feels like a bit incomplete.	rebuttal_process
2019-312	I personally would prefer a more complete evaluation and discussion of the proposed method.	weakness
2019-312	- the entire evaluation of this paper is purely qualitative (and that is not quite very convincing either).	weakness
2019-312	I feel it would be important for this paper to add some quantitative measure of quality.	suggestion
2019-312	E.g. train an MNIST recognizer synthesized data and compare that to a recognizer trained on the original MNIST data.	suggestion
2019-312	- a proper discussion of how the proposed environment is different from the environment proposed by Ganin et al (Deepmind's SPIRAL)	suggestion
2019-312	Minor comments: - abstract: why is it like "dreaming" -> I do agree with the rest of that statement, but I don't see the connection to dreaming	weakness
2019-312	- abstract: "upper agent" -> is entirely unclear here.	weakness
2019-312	- abstract: the footnote at the end of the abstract is at a strange location	weakness
2019-312	- introduction: and could thus -> and can thus	weakness
2019-312	- introduction: second paragraph - it would be good to add some citations to this paragraph.	suggestion
2019-312	- resulted image-> resulting image	suggestion
2019-312	- the sentence: "We can generate....data is cheap" - is quite unclear to me at this time.	weakness
2019-312	Most of it becomes clearer later in the paper - but I feel it would be good to put this into proper context here (or not mention it)	weakness
2019-312	- we obtained -> we obtain	weakness
2019-312	- called a generator -> call a generator	weakness
2019-312	- the entire last paragraph on the first page is completely unclear to me when reading it here.	weakness
2019-312	- equations 1, 2: it's unclear whether coordinates are absolute or relative coordinates.	weakness
2019-312	- fig 1: it's very confusing that the generator, that is described first is represented at the right.	weakness
2019-312	- sec 3.2 - first line: wrong figure reference - you refer to fig 2 - but probably mean fig 1	weakness
2019-312	- page 3 bottom: by appending the encoded color and radius data we have a feature with shape 64x64xn -> I don't quite see how this is true.	weakness
2019-312	The image was 64x64 -> and I don't quite understand why you have a color/radius for each pixel.	weakness
2019-312	- sec 3.3 - it seem sthat there is a partial sentence missing	weakness
2019-312	- sec 3.4 - is it relevant to the rest of the paper that the web application exists (and how it was implemented).	weakness
2019-312	- fig 2 / fig 3: these figures are very hard to read.	weakness
2019-312	Maybe inverting the images would help.	weakness
2019-312	Also fig 3 has very little value.	weakness

2019-350	This paper studies a Mixed Integer Linear Programming (MILP) approach to verifying the robustness of neural networks with ReLU activations.	abstract
2019-350	The main contribution of the paper is a progressive bound tightening approach that results in significantly faster MILP solving.	abstract
2019-350	This in turn allows for verifying the robustness of larger networks than previously studied, and even larger datasets such as CIFAR-10.	abstract
2019-350	This paper is a solid contribution and should be accepted to *CONF*.	decision
2019-350	It is quite well-written, addresses an important problem using a principled method, and achieves strong experimental results that were previously elusive, despite the large body of work in adversarial learning.	strength
2019-350	In particular, the paper has the following strengths: - Clarity: the paper is well-written and easy to read.	strength
2019-350	Tables, figures and pseudocode are nice and easy to understand.	strength
2019-350	- Methodology: the authors take care of a number of bottlenecks in the scalability of MIP solvers for the verification problem.	weakness
2019-350	This is the standard approach in the Operations Research (OR) community, and I am really glad to see it in an *CONF* submission!	decision
2019-350	- Results: the efficiency of the MIP on the tightened model, and the improvements in the bounds on the adversarial error as compared to very recent methods from the literature are both very strong points in favor of the paper.	strength
2019-350	I do not have any further questions for the authors - good job!	misc
2019-350	The authors perform a careful study of mixed integer linear programming approaches for verifying robustness of neural networks to adversarial perturbations.	abstract
2019-350	They propose three enhancements to MILP formulations of neural network verification: Asymmetric bounds, restricted domain and progressive bound tightening, which lead to significantly more scalable verification algorithms vis-a-vis prior work.	abstract
2019-350	They study the effectiveness of MILP solvers both in terms of verifying robustness (compared to other complete/incomplete verifiers) and generating adversarial attacks (compared to PGD attacks) and show that their approach compares favorable across a number of architectures on MNIST and CIFAR-10.	abstract
2019-350	They perform careful ablation studies to validate the importance of the	abstract
2019-350	Quality: The paper is very well written and organized.	strength
2019-350	The problem is certainly of great interest to the deep learning community, given the difficulty of properly evaluating (and then improving) defenses against adversarial attacks.	strength
2019-350	The experiments are done carefully with convincing ablation studies.	strength
2019-350	Clarity: The authors explain the relevant concepts carefully and all the experimental results are clearly written and explained.	strength
2019-350	Originality: The authors propose conceptually simple but practically significant enhancements to MILP formulations of neural network verification.	abstract
2019-350	However, the novelty wrt https://arxiv.org/pdf/1711.00455.pdf is not discussed carefully in my view (the  asymmetric bounds were already studied in this paper, as well as a novel branch and bound strategy).	weakness
2019-350	The progressive bound tightening is a novel idea as far as I can see - however, the ablation experiments show that this idea is not significant in terms of performance improvement.	weakness
2019-350	In terms of experiments, the authors indeed obtain strong results on verified adversarial error rates and generate attacks that PGD is unable to	weakness
2019-350	- however, again the results do not outperform latest results (in terms of the  best achievable upper bounds on verified error rates) available well before the *CONF* deadline	weakness
2019-350	- https://arxiv.org/pdf/1805.12514.pdf . It would be great if the authors addressed these issues in a revised version of the paper.	suggestion
2019-350	Significance: The work does establish a strong algorithm for complete verification of neural networks along with several ideas that are critical to obtain strong performance with this approach.	strength
2019-350	Question: 1. I am unclear on the "restricted domain" contribution claimed in the paper - is this just exploiting the fact that the inputs to the classifier are normalized to a given range, in addition to being no more than eps away from the nominal input?	weakness
2019-350	Cons 1. The authors do not compare their approach to that of https://arxiv.org/pdf/1711.00455.pdf , both in terms of conceptual novelty and in terms of experimental results.	weakness
2019-350	In particular, it is not clear to me whether the authors' approach remains superior on domains where tight bounds on the neural networks inputs are not available, like the problems studied in the ACAS system in the ReLuPlex paper.	weakness
2019-350	2. The authors' MILP solution approach relies on having access to the state of the art commercial MILP solver Gurobi.	weakness
2019-350	While Gurobi is free for academic research use, for large scale neural network verification applications, this does restrict use of the approach (particularly due to limited licenses being available).	weakness
2019-350	It would be interesting to see a comparison that uses a freely available MILP solver (like scip.zib.de) to see how critical the approach's scalability depends on the quality of the MILP solver.	suggestion
2019-350	3. The authors do not outperform the latest SOA numbers in terms of verified adversarial error rates on MNIST and CIFAR classifers.	weakness
2019-350	It would be good to see a comparison on results from https://arxiv.org/pdf/1711.00455.pdf  (I believe the training code and trained networks are available online).	suggestion
2019-350	This paper presents a mixed integer programming technique for verification of piecewise linear neural networks.	abstract
2019-350	This work uses progressive bounds tightening approach to determine bounds for inputs to units.	abstract
2019-350	The authors also show that this technique speeds up the bound determination by orders of magnitude as compared to other complete and incomplete verifiers.	abstract
2019-350	They also compare the advercerial accuracies on MNIST and CIFAR and improve on the lower bounds as compared to PGD and upper bounds as compared to SOA.	abstract
2019-350	The paper is well written and presents a valuable technique for evaluating robustness of classifiers to adversarial attacks.	strength

2019-363	Summary: This work considers the problem of learning in input-driven environments -- which are characterized by an addition stochastic variable z that can affect the dynamics of the environment and the associated reward the agent might see.	abstract
2019-363	The authors show how the PG theorem still applied for a input-aware critic and then they show that the best baseline one can use in conjecture with this critic is a input-dependent one.	abstract
2019-363	My main concerns are highlighted in points (3) and (4) in the detailed comments below.	misc
2019-363	Clarity: Generally it reads good, although I had to go back-and-forth between the main text and appendix several times to understand the experimental side.	strength
2019-363	Even with the supplementary material, examples in Section 3 and Sections 6.2 could be improved in explanation and discussion.	strength
2019-363	Originality and Significance: Limited in this version, but could be improved significantly by something like point (3)&(4) in detailed comments.	weakness
2019-363	Fairly incremental extension of the PG (and TRPO) with the conditioning on the potentially (unobserved) input variables.	weakness
2019-363	The fact that a input-aware critic could benefit from a input-aware baseline is not that surprising.	weakness
2019-363	The fact that it reduces variance in the PG update is an interesting result; nevertheless I strongly feel the link or comparison needed is with the standard PG update.	weakness
2019-363	Disclaimer: I have not checked the proofs in the appendix.	weakness
2019-363	Detailed comments: 1) On learning the input-dependent baselines: Generalising over context via a parametric functional approximation, like UVFAs [1] seems like a more natural first choice.	weakness
2019-363	Also these provide a zero-shot generalisation, bypassing the need for a burn-in period of the task.	weakness
2019-363	Can you comment on why something like that was not used at least as baseline?	weakness
2019-363	2) Motivating example. The exposition of this example lacks a bit of clarity and can use some more details as it is not a standard MDP example, so it's harder to grasp the complexity of this task or how standard methods would do on it and where would they struggle.	weakness
2019-363	I think it's meant to be an example of high variance but the performance in Figure 2 seems to suggest this is actually something manageable for something like A2C.	weakness
2019-363	It is also not clear in this example how the comparison was done.	weakness
2019-363	For instance, are the value functions used, input-dependent?	weakness
2019-363	Is the policy input aware?	weakness
2019-363	3) Input-driven MDP. Case 1/Case 2 : As noted by the authors, in case 1 if both s_t and z_t are observed, this somewhat uninteresting as it recovers a particular structured state variable of a normal MDP.	weakness
2019-363	I would argue that the more interesting case here, is where only s_t is observed and z_t is hidden, at least in acting.	weakness
2019-363	This might still be information available in hindsight and used in training, but won't be available 'online' -- similar to slack variable, or privileged information at training time.	weakness
2019-363	And in this case it's not clear to me if this would still result in a variance reduction in the policy update.	weakness
2019-363	Case 2 has some of that flavour, but restricts z_t to an iid process.	weakness
2019-363	Again, I think the more interesting case is not treated or discussed at all and in my opinion, this might add the best value to this work.	weakness
2019-363	4) Now, as mentioned above the interesting case, at least in my opinion, is when z is hidden.	weakness
2019-363	From the formulae(eq. (4),(5)), it seems to be that the policy is unaware of the input variables.	weakness
2019-363	Thus we are training a policy that should be able to deal with a distribution of inputs z.	weakness
2019-363	How does this compare with the normal PG update, that would consider a critic averaged over z-s and a z-independent baseline?	weakness
2019-363	Is the variance of the proposed update always smaller than that of the standard PG update when learning a policy that is unaware of z?	weakness
2019-363	References: [1] Schaul, T., Horgan, D., Gregor, K.	misc
2019-363	and Silver, D., 2015, June.	misc
2019-363	Universal value function approximators. In International Conference on Machine Learning (pp.	misc
2019-363	1312-1320). [POST-rebuttal] I've read the author's response and it clarified some of the concerns.	rebuttal_process
2019-363	I'm increase the score accordingly.	rebuttal_process
2019-363	The paper introduces and develops the notion of input-dependent baselines for Policy Gradient Methods in RL.	abstract
2019-363	The insight developed in the paper is clear: in environments such as data centers or outside settings external factors (traffic load or wind) constitute high magnitude perturbations that ultimately strongly change rewards.	strength
2019-363	Learning an input-dependent baseline function helps clear out the variance created by such perturbations in a way that does not bias the policy gradient estimate (the authors provide a theoretical proof of that fact).	strength
2019-363	The authors propose different methods to train the input dependent baseline function: o) a multi-value network based approach o) a meta-learning approach	abstract
2019-363	The performance of these two methods is compared on simulated robotic locomotion tasks as well as a load balancing and video bitrate adaptation task.	abstract
2019-363	The input dependent baseline strongly outperforms the state dependent baseline in both cases.	abstract
2019-363	Strengths: o) The paper is well written o) The method is novel and simple while strongly reducing variance in Monte Carlo policy gradient estimates without inducing bias.	strength
2019-363	o) The experiment evidence is strong.	strength
2019-363	Weaknesses: o) Vehicular traffic has been the subject of recent development through deep reinforcement learning (e.g. https://arxiv.org/pdf/1701.08832.pdf and https://arxiv.org/pdf/1710.05465.pdf).	weakness
2019-363	In this particular setting exogenous noise (demand for throughput and accidents) could strongly benefit from input dependent baselines.	weakness
2019-363	I believe the authors should mention such potential applications of the method which may have major societal impact.	suggestion
2019-363	o) There is a lot of space dedicated to well know facts about policy gradient methods.	weakness
2019-363	I believe it could be more impactful to put the proof of Theorem 1 in the main body of the paper as it is clearly a key theoretical property.	suggestion
2019-363	Introduction: "Since the state dynamics and rewards depend on the input process" -> why do the rewards depend on the input process conditioned on the state?	suggestion
2019-363	Does the scenario being considered basically involve any scenario with stochastic dynamics?	suggestion
2019-363	Or is the fact that the disturbances may come from a stateful process what makes this distinct?	weakness
2019-363	if the input sequence following the action -> vague, would help if this would just be written a bit more clearly.	weakness
2019-363	Is just the baseline input dependent or does the policy need to be input dependent as well?	weakness
2019-363	From later reading, this point is still quite confusing.	weakness
2019-363	One line says "At time t, the policy only depends only on (st, zt).".	weakness
2019-363	Another line says that the policy is pi_theta(a|s), with no mention of z.	weakness
2019-363	I'm pretty confused by the consistency here.	weakness
2019-363	This is also important in the proof of Lemma 1, because P(a|s,z) = pi_theta(a|s).	weakness
2019-363	Please clarify this. Section 4: Is the IID version of Figure 3 basically the same as stochastic dynamics?	weakness
2019-363	(Case 2) Section 4.1 "In input-driven MDPs, the standard input-agnostic baseline is ineffective at reducing variance" -> can you give some more intuition/proof as to why.	weakness
2019-363	In Lemma 2, how come the Q function is dependent on z, but the policy is only dependent on s (not even the current and past z's).	weakness
2019-363	I think the proof of theorem 1 should be included in the main paper rather than unnecessary details about policy gradient.	weakness
2019-363	Theorem 1 and theorem 2 are really some of the most important parts of the paper, and they deserve a more thorough discussion besides the 2 lines that are in there right now.	weakness
2019-363	Algorithm 1 -> should it be eqn 4?	weakness
2019-363	The meta-algorithm provided in Section 5 is well motivated and well described.	strength
2019-363	An experimental result including what happens with LSTM baselines would be very helpful.	suggestion
2019-363	One question is whether it is actually possible to know what the z's are at different steps?	suggestion
2019-363	In some cases these might be latent and hard to infer?	weakness
2019-363	Can you compare to Clavera et al 2018?	weakness
2019-363	It seems like it might be a relevant comparison.	strength
2019-363	The difference between MAML and the 10 value network seems quite marginal.	weakness
2019-363	Can the authors discuss why this is?	weakness
2019-363	And when we would expect to see a bigger difference.	weakness
2019-363	Related work: Another relevant piece of work	strength
2019-363	Meta-Learning Priors for Efficient Online Bayesian Regression	misc
2019-363	Major todos: 1. Improve clarity of what z's are observed, which are not and whether the policy is dependent on these or not.	suggestion
2019-363	2. Compare with other prior work such as Clavera et al, Harrison et al. 3. Add more naive baselines such as training an LSTM, etc.	suggestion
2019-363	4. Provide more analysis of the meta-learning component, how much does it actually help.	suggestion
2019-363	Overall impression:  I think this paper covers an interesting problem, and proposes a simple, straightforward approach conditioning the baseline and the critic on the input process.	strength
2019-363	What bothers me in the current version of the paper is the lack of clarity about the observability of z, where it comes from and also some lack of comparisons with other prior methods.	weakness
2019-363	I think these would make the paper stronger.	suggestion

2019-375	Here the authors define a BMI that uses an autoencoder -> LSTM -> EMG.	abstract
2019-375	The authors then address the problem of data drift in BMI and describe a number of domain adaptation algorithms from simple (CCA to more complex ADAN) to help ameliorate it.	abstract
2019-375	There are a lot of extremely interesting ideas in this paper, but the paper is not particularly well written, and the overall effect to me was confusion.	weakness
2019-375	What problem is being solved here?	weakness
2019-375	Are we describing using latent variables (AE approach) for BMI?	weakness
2019-375	Are we discussing domain adaptation, i.e. handling the nonstationarity that so plagues BMI and array data?	weakness
2019-375	Clearly the issue of stability is being addressed but how?	weakness
2019-375	A number of different approaches are described from creating a pre-execution calibration routine whereby trials on the given day are used to calibrate to an already trained BMI (e.g. required for CCA) to putting data into an adversarial network trained on data from earlier days.	weakness
2019-375	Are we instead attempting to show that a single BMI can be used across multiple days?	weakness
2019-375	This paper is extremely interesting but suffers from lack of focus, rigor, and clarity.	weakness
2019-375	Focus : AE to RNN to EMG is that the idea to compare vs.	weakness
2019-375	Domain adaptation via CCA/KLDM/ADAM. Of course a paper can explore multiple ideas, but in this case the comparisons and controls for both are not adequate.	weakness
2019-375	Rigor: What are meaningful comparisons for all for the AE and DA portions?	weakness
2019-375	The AE part is strongly related to either to Kao 2017 or Pandarinath 2018 but nothing like that is compared.	weakness
2019-375	The domain adaptation part evokes data augmentation strategies of Sussillo 2016 but that is not compared.	weakness
2019-375	If I were reviewing this manuscript for a biological journal a rigorous standard would be online BMI results in two animals.	weakness
2019-375	Is there a reason why this isn't the standard for *CONF*?	weakness
2019-375	Is the idea that non-biological journals / conferences are adequate to vet new ideas before really putting them to the test in a biological journal?	weakness
2019-375	The manuscript is concerned with the vexing problem of BMI stability of time, which seems to be a problem where online testing in two animals would be critical.	weakness
2019-375	(I appreciate this is a broader topic relevant to the BMI field beyond just this paper, but it would be helpful to get some thinking on this in the rebuttal).	suggestion
2019-375	Clarity : This paper needs to be pretty seriously clarified.	weakness
2019-375	The mathematical notation is not adequate to the job, nor is the motivation for the varied methodology.	weakness
2019-375	I cannot tell if the subscript is for time or for day.	weakness
2019-375	Also, what is the difference between z_0 vs.	weakness
2019-375	Z_0? I do not know what exactly is going into the AE or the ADAN.	weakness
2019-375	The neural networks are not described to a point where one could reproduce this work.	weakness
2019-375	The notation for handling time is inadequate.	weakness
2019-375	E.g. despite repeated readings I cannot tell how time is handled in the auto-encoder, e.g. nxt is vectorized vs feeding n-sized vector one time step at a time?	weakness
2019-375	Questions What is the point of the latent representation in the AE if it is just fed to an LSTM?	weakness
2019-375	Is it to compare to not using it?	weakness
2019-375	Page 3, how precisely is time handled in the AE?	weakness
2019-375	If time is just vectorized, how can one get real-time readouts?	weakness
2019-375	In general there is not enough detail to understand what is implemented in the AE.	weakness
2019-375	If only one time slice is entered into AE, then it seems clear AE won't be very good because one desires latent representation of the dynamics, not single time slices.	weakness
2019-375	How big is the LSTM used to generate the EMG?	weakness
2019-375	It seems like a the most relevant baseline is to compare to the data perturbation strategies in Sussillo 2016.	weakness
2019-375	If you have an LSTM already up and running to predict EMG, this seems very doable.	weakness
2019-375	Page 4, "We then use an ADAN to align either the distribution of latent variables or the distributions of the residuals of the reconstructed neural data, the latter a proxy for the alignment of the neural latent variables."	weakness
2019-375	This sentence is not adequate to explain the concepts of the various distributions, the residuals of reconstructed neural data (where do the residuals come from?	weakness
2019-375	), and why is one a proxy for the other.	weakness
2019-375	Please expand this sentence into a few sentences, if necessary to define these concepts for the naive reader.	suggestion
2019-375	Page 5, What parameters are minimized in equation (2)?	suggestion
2019-375	Please expand the top sentence of page 5.	suggestion
2019-375	Page 6, top - "In contrast, when the EMG predictor is  trained simultaneously with the AE…" Do you mean there is again a loss function defined by both EMG prediction and AE and summed, and then backprop is used to train both in an end-to-end fashion?	suggestion
2019-375	Please clarify. Page 8, How do the AE results and architecture fit into the EMG reconstruction "BMI" results?	suggestion
2019-375	Is that all decoding results are first put through the AE -> LSTM -> EMG pipeline?	suggestion
2019-375	I.e. your BMI is neural data -> AE -> LSTM -> EMG?	weakness
2019-375	If so, then how does the ADAN / CCA and KLDM fit in?	weakness
2019-375	You first run those three DA algorithms and then pipe it through the BMI?	suggestion
2019-375	Page 8, How can you say that the BMI improvement of 6% is meaningful to the BMI user if you did not test the BMI online?	weakness
2019-375	The paper considers invasive BMIs and studies various ways to avoid daily recalibration due to changes in the brain signals.	abstract
2019-375	While I like the paper and studied methods -- using adverserial domain adaptation is interesting to use in this context --, I think that the authors oversell a bit.	weakness
2019-375	The problem of nonstationarity rsp.	weakness
2019-375	stability is an old one in non-invasive BCIs (shenoy et al JNE 2006 was among the first) and a large number of prior methods have been defined to robustify feature spaces, to project to stable subspaces etc.	weakness
2019-375	Clearly no Gans at that time.	weakness
2019-375	The least the authors could do is to make reference to this literature, some methods may even apply also for the invasive data of the paper.	suggestion
2019-375	While the authors did not clearly say that they present an offline analysis; one method, the GAN, gets 6% better results then the competitors.	weakness
2019-375	I am not sure whether this is practically relevant in an online setting.	weakness
2019-375	But this needs to be clearly discussed in the paper and put into perspective  to avoid wrong impression.	weakness
2019-375	Only an online study would be convincing.	weakness
2019-375	Overall, I think the paper could be accepted, the experiments are nice, the data is interesting, if it is appropriately toned down (avoiding statements about having done something for the first time) and properly references to prior work are given.	decision
2019-375	It is an interesting application domain.	strength
2019-375	I additionally recommend releasing the data upon acceptance.	decision
2019-375	This contribution describes a novel approach for implanted brain-machine interface in order to address calibration problem and covariate shift.	abstract
2019-375	A latent representation is extracted from SEEG signals and is the input of a LTSM trained to predict muscle activity.	abstract
2019-375	To mitigate the variation of neural activities across days, the authors compare a CCA approach, a Kullback-Leibler divergence minimization and a novel adversarial approach called ADAN.	abstract
2019-375	The authors evaluate their approach on 16-days recording of neurons from the motor cortex of rhesus monkey, along with EMG recording of corresponding the arm and hand.	abstract
2019-375	The results show that the domain adaptation from the first recording is best handled with the proposed adversarial scheme.	abstract
2019-375	Compared to CCA-based and KL-based approaches, the ADAN scheme is able to significantly improve the EMG prediction, requiring a relatively small calibration dataset.	abstract
2019-375	The individual variability in day-to-day brain signal is difficult to harness and this work offers an interesting approach to address this problem.	abstract
2019-375	The contributions are well described, the limitation of CCA and KL are convincing and are supported by the experimental results.	strength
2019-375	The important work on the figure help to provide a good understanding of the benefit of this approach.	strength
2019-375	Some parts could be improved.	weakness
2019-375	The results of Fig. 2B to investigate the role of latent variables extracted from the trained autoencoder are not clear, the simultaneous training could be better explained.	weakness
2019-375	As the authors claimed that their method allows to make an unsupervised alignment neural recording, independently of the task, an experiment on another dataset could enforce this claim.	weakness

2019-401	Summary: Train a multilingual NMT system using the technique of Johnson et al (2017), but augment the standard cross-entropy loss with a distillation component based on individual (single-language-pair) teacher models.	abstract
2019-401	Periodically compare the validation BLEU score of the multilingual model with that of each individual model, and turn off distillation for language pairs where the multilingual model is better.	abstract
2019-401	On three different corpora (IWSLT, WMT, TED) with into-English translation from numbers of source languages ranging from 6 (WMT) to 44 (TED), this technique outperforms standard distillation for every language pair, and outperforms the individual models for most language pairs.	abstract
2019-401	Supplementary experiments justify the strategy of selectively turning off distillation, and quantify the effect using only the top 8 vocabulary items in distillation.	abstract
2019-401	The main idea makes sense, and the results are very convincing, especially since it appears that hyper-parameters were not tuned extensively (eg, weight of 0.5 on the distillation loss, for all language pairs).	strength
2019-401	Implementation should be very straightforward, especially with the trick of pre-computing top-k probabilities from the teacher model at each corpus position.	strength
2019-401	One small barrier to practical application that the authors fail to acknowledge is the requirement to train individual models, which will at least double training time compared to a single multilingual model.	weakness
2019-401	The main missing experiment is higher-capacity multilingual models, which Johnson et al show to be beneficial in settings with a large number of language pairs.	weakness
2019-401	Using a multilingual model of the same (relatively small) size as the individual models as is done here is likely to be suboptimal, especially for the 44-language pair TED setting.	weakness
2019-401	A related point is that the corpora used seem to be quite small (eg 4.5M and 1M sentences for WMT Czech and German, respectively, while the available training corpora are closer to 15M and 4.5M).	weakness
2019-401	Although performance relative to individual models is still impressive - and seems to be better than than in previous work - this makes the experiments comparing to the multilingual baseline less meaningful.	weakness
2019-401	Also missing are experiments on out-of-English translation, which would establish the viability of the proposed technique for many-to-many translation via bridging.	weakness
2019-401	Out-of-English is a more difficult problem than into-English.	weakness
2019-401	I can't see any reason the proposed technique wouldn't also work in this setting, but this remains to be shown.	weakness
2019-401	Although it's great that the technique is shown to work without embellishments, there are a few obvious strategies it would have been interesting to explore, such as making the weight on the distillation loss dependent on the difference in performance between the multilingual and individual models;	suggestion
2019-401	and allowing for the distillation loss to be turned back on if the performance of the multilingual model starts to drift back down for a particular language pair.	suggestion
2019-401	I also wondered about the effect of the gradient accumulation strategy in algorithm 1, where individual batches from each language pair are effectively grouped into one giant batch for the purpose of parameter updates.	weakness
2019-401	I can see that this could stabilize training, but it would be good to know whether it's crucial for success, especially when the number of language pairs is large.	weakness
2019-401	Further details: As aforementioned -> As mentioned	misc
2019-401	(1) 2nd line: Doesn't make sense as written.	weakness
2019-401	You need to distinguish the gold y_t from hypothesized ones in the 1() function.	weakness
2019-401	Above (2): is served as -> serves as	strength
2019-401	3.2 First paragraph. Since D presumably consists of D^l for all languages l,	weakness
2019-401	L_ALL(D,...) should be a function of teacher parameters theta^l for all languages l rather than just one as written.	weakness
2019-401	In top-K distillation, is the teacher distribution renormalized or simply truncated?	weakness
2019-401	Generalization analysis, pg 8: presumably you are sampling from N(0, sigma^2) -	weakness
2019-401	this should be described as such.	weakness
2019-401	Reference: Johnson et al, "Google's Multilingual Neural Machine Translation System: Enabling Zero-Shot Translation" TACL, 2017.	misc
2019-401	... I would have liked to see some more insights.	weakness
2019-401	The authors present a method for distilling knowledge from individual models to train a multilingual model.	abstract
2019-401	The motivation stems from the fact that while most s-o-t-a multilingual models are compact (as compared to k individual models) they fall short of the performance of the individual models.	abstract
2019-401	The authors demonstrate that using knowledge distillation, the performance of the multilingual model can actually be better than the individual models.	abstract
2019-401	Please find below my comments and questions.	misc
2019-401	1) The authors have done a commendable job of validating their hypothesis on multiple datasets.	strength
2019-401	Solid experimentation is definitely the main strength of this paper.	strength
2019-401	2) However, this strength also makes way for a weakness.	strength
2019-401	The entire experimental section is just filled with tables and numbers.	weakness
2019-401	The same message is repeated across these multiple tables (multi+distill > single > multi).	weakness
2019-401	Beyond this message there are no other insights.	weakness
2019-401	For example, - How does the performance depend on the divergence between source and target language?	weakness
2019-401	- Why is there more important on some languages and less on others ?	weakness
2019-401	- Why are the improvements on the TED dataset so much higher as compared to the other 2 datasets.	weakness
2019-401	- What happens when the target language is something other than English?	weakness
2019-401	All the experiments report results from X-->English, why not in the other direction?	weakness
2019-401	The model then is not really "completely" multilingual.	weakness
2019-401	It is multi-source-->single target. - Can you comment on the total training time ?	weakness
2019-401	- What happens when you do not stop the distillation even when the accuracy of the student crosses that of the teachers ?	weakness
2019-401	What do you mean by accuracy here?	weakness
2019-401	Only later when you mention that \\threshold = 1 BLEU it became clear that accuracy means BLEU in this context ?	weakness
2019-401	3) Is it all worth it?	weakness
2019-401	One disappointing factor is that end of all this effort where you train K individual models and one monolithic model with distillation, the performance gain for most language pairs is really marginal (except on the TED dataset).	weakness
2019-401	I wonder if the same improvements could have been obtained by even more carefully fine tuning the baseline models itself.	suggestion
2019-401	4) On a positive note, I like the back-distillation idea and the experiments on top-K distillation	strength
2019-401	+++++++++++++++++++ I have updated my rating after reading author's responses The authors apply knowledge distillation for many-to-one multilingual neural machine translation, first training separate models for each language pair.	rebuttal_process
2019-401	For most language pairs, performance matches or improves upon single-task baselines.	rebuttal_process
2019-401	Strengths: Improvements upon the baselines are fairly impressive, especially for the	strength
2019-401	44-language model. The approach is quite simple and could be easily implemented by other groups.	strength
2019-401	The paper is well-written and easy to understand.	strength
2019-401	At inference, only a single model needs to be retained, which is memory-efficient.	strength
2019-401	Weaknesses: The authors only test distillation in a many-to-one scenario.	weakness
2019-401	I believe that providing results for many-to-many multilingual NMT would be valuable.	weakness
2019-401	Overall, this approach increases training time as all single-task models must have converged before beginning distillation.	weakness
2019-401	The authors provide no direct comparison to other work, which makes it hard to know how strong the baselines are.	weakness
2019-401	At least for WMT, I would suggest reporting results with mteval-v13a (or SACREBLEU), so that results can be compared against official results.	suggestion
2019-401	Questions: For the top-K approach, do you normalize the top K probabilities so that they sum to 1 or not?	weakness
2019-401	Did you consider applying sequence knowledge distillation (Kim and Rush, 2016)	suggestion
2019-401	(using the baseline beam search output as references) instead of word knowledge distillation?	weakness
2019-401	*** EDIT: In my opinion, the changes made after the review period clearly improve the quality of the paper.	rebuttal_process
2019-401	I am increasing my rating from 6 to 7.	rating_summary

2019-479	This work adds to a growing literature on biologically plausible (BP) learning algorithms.	strength
2019-479	Building off a study by Bartunov et al. that shows the deficiencies of some BP algorithms when scaled to difficult datasets, the authors evaluate a different algorithm, sign-symmetry, and conclude that there are indeed situations in which BP algorithms can scale.	abstract
2019-479	This seemingly runs counter to the conclusions of Bartunov et al.; while the authors state that their results are "complementary", they also state that the findings "directly conflict" with the results of Bartunov, concluding that BP algorithms remain viable options for both learning in artificial networks and the brain.	abstract
2019-479	To reach these conclusions the authors report results on a number of experiments.	abstract
2019-479	First, they show successful training of a ResNet-18 architecture on ImageNet using sign-symmetry, with their model performing nearly as well as one trained with backpropagation.	abstract
2019-479	Next, they demonstrate decent performance on MS COCO object detection using RetinaNet. Finally, they end with a discussion that seeks to explain the differences in their approach and the approach of Batunov et al, and with a potential biological implementation of sign symmetry.	abstract
2019-479	Overall the clarity of the writing is sufficient.	strength
2019-479	The algorithm is properly explained, and there are sufficient citations to reference prior work.	strength
2019-479	The results are generally clear (though there is an incomplete experiment, I agree with the authors that it is unlikely for the preliminary results to change).	strength
2019-479	I believe that there is enough detail for this work to be reproducible.	strength
2019-479	The work is also sufficiently novel in that experiments using sign-symmetry on difficult datasets have not been undertaken, to my knowledge.	strength
2019-479	Unfortunately, the clarity and rigor of the *scientific argument* is insufficient for a number of reasons.	weakness
2019-479	These will be enumerated below.	misc
2019-479	First, the explicit writing and underlying tone of the paper reveal a misrepresentation of the scientific argument in Bartunov et al. The scientific question in Bartunov et al. is not a matter of whether BP algorithms can be useful in purely artificial settings, but rather whether they can say anything about the way in which the brain learns.	weakness
2019-479	In this work, on the other hand, there seems to be two scientific questions: first, to assess whether BP algorithms can be useful in artificial settings, and second, to determine whether they can say anything about how the brain learns, as in Bartunov (indeed, the author's conclusions highlight precisely these two points).	weakness
2019-479	Unfortunately, the experiments and underlying experimental logic push towards addressing the first question, and use this as evidence towards a conclusion to the second question.	weakness
2019-479	More concretely, experiments are run on biologically problematic architectures such as ResNet-18, often with backpropagation in the final layer (though admittedly this doesn't seem to be an important detail with sign-symmetry, for reasons explained below).	weakness
2019-479	This is fine under the pretense of answering the first question, but to seriously engage with the results of Bartunov et al. and assess sign-symmetry's merit as a BP algorithm for learning in the brain, the work requires the authors the algorithms to be tested under similar conditions before claiming that there is a "direct conflict".	weakness
2019-479	To this end, though the authors claim that the conditions on which Bartunov et al tested are "somewhat restrictive", this logic can equally be flipped on its head: the conditions under which this paper tests sign-symmetry are not restrictive enough to productively move in the direction of assessing sign-symmetry's usefulness as a description of learning in the brain, and so the conclusion that the algorithm remains a viable option for describing learning in the brain is not sufficiently supported.	weakness
2019-479	On the other hand, I think the conclusions regarding the first question -- whether sign-symmetry can be useful in artificial settings -- are fine given the experiments.	strength
2019-479	Second, the work does not sufficiently weigh the "degree" of implausibility of sign-symmetry compared to the other algorithms, and implicitly speaks of feedback alignment, target propagation, and sign-symmetry as equally realistic members of a class of BP algorithms.	weakness
2019-479	Of course, one doesn't want to go down the road of declaring that "algorithm A is more plausible than algorithm B!", but the nuances should at least be seriously discussed if the algorithms are to be properly compared.	weakness
2019-479	In backpropagation the feedback connections must be similar in sign and magnitude.	weakness
2019-479	Sign-symmetry eliminates the requirement that the connections be similar in magnitude.	weakness
2019-479	However, this factor is arguably the least important of the two (the direction of the gradient is more important than the magnitudes), and we are still left with feedback weights that somehow have to tie their sign to their feedforward counterparts, which is not an issue in target propagation or feedback alignment.	weakness
2019-479	The authors try to explain away this difficulty with an appeal to molecular biology, which leads into my third point.	rebuttal_process
2019-479	Third, the appeal to molecular mechanisms to explain how sign-symmetry can arise is not rigorous.	weakness
2019-479	There is a plethora of molecular mechanisms at play in our cells; indeed, there are enough mechanisms to hand-craft *any* sort of circuit one likes.	weakness
2019-479	Thus, it is somewhat vacuous to conclude that a particular circuit can be "easily implemented" in the brain simply by appealing to a hand-crafted circuit.	weakness
2019-479	For this argument to hold one needs to appeal to biological data to demonstrate that such a circuit either a) exists already, b) most probably exists because of reasons X, Y, Z.	weakness
2019-479	Unfortunately there is no biological backing, rendering this argument a possibly fun thinking exercise, but not a serious scientific proposal.	weakness
2019-479	But perhaps most problematic, the argument leaves the problem of sign-switching in the feedforward network to "future work".	weakness
2019-479	This is perhaps *the most* important problem at play here, and until it is answered, these arguments don't have sufficient impact.	weakness
2019-479	Altogether the scientific argument of this work needs tightening.	weakness
2019-479	The tone, the title, and the overall writing should be modified to better tackle the nuances underlying the arguments of biologically plausible learning algorithms.	weakness
2019-479	The claims and conclusions need to be more explicit, and the work needs to better seated in the context of both the previous literature, and the important questions at play for assessing biologically plausible learning algorithms.	weakness
2019-479	In the submitted manuscript, the authors compare the performance of sign-symmetry and feedback alignment on ImageNet and MS COCO datasets using different network architectures, with the aim of testing biologically-plausible learning algorithms alternative to the more artificial backpropagation.	abstract
2019-479	The obtained results are promising and quite different to those in (Bartunov , 2018) and lead to the conclusion that biologically plausible learning algorithms in general and sign- symmetry in particular are effective alternatives for ANN training.	strength
2019-479	Although all the included ideas are not fully novel, the manuscript shows a relevant originality, paving the way for what can be a major breakthrough in deep learning theory and practice in the next few years.	strength
2019-479	The paper is well written and organised, with the tackled problem well framed into the context.	strength
2019-479	The suite of experiments is broad and diverse and overall convincing, even if the performances are not striking.	strength
2019-479	Very interesting the biological interpretation and the proposal for the construction in the brain.	strength
2019-479	A couple of remarks: I would be interested in understanding the robustness of the sign-symmetry algorithm w.r.t.	suggestion
2019-479	for instance dropout and (mini)batch size, and to see the behaviour of the algorithm on datasets with small sample size;	suggestion
2019-479	second, there is probably too much stress on comparing w/ (Bartunov , 2018), while the manuscript is robust enough not to need such motivation.	weakness
2019-479	Minor: refs are not homogeneous, first names citations are not consistent.	weakness
2019-479	Summary: The authors are interested in whether particular biologically plausible learning algorithms scale to large problems (object recognition and detection using ImageNet and MS COCO, respectively).	abstract
2019-479	In particular, they examine two methods for breaking the weight symmetry required in backpropagation: feedback alignment and sign-symmetry.	abstract
2019-479	They extend results of Bartunov et al 2018 (which found that feedback alignment fails on particular architectures on ImageNet), demonstrating that sign-symmetry performs much better, and that preserving error signal in the final layer (but using FA or SS for the rest) also improves performance.	abstract
2019-479	The paper is clear, well motivated, and significant in that it advances our understanding of how recently proposed biologically plausible methods for getting around the weight symmetry problem work on large datasets.	strength
2019-479	In particular, I appreciated: the clear introduction and explanation of the weight symmetry problem and how it arises in the context of backprop, the thorough experiments on two large scale problems, the clarity of the presented results, and the discussion about future directions of study.	strength
2019-479	Minor comments: - s/there/therefore in the first paragraph on page 2	suggestion
2019-479	- The authors claim that their conclusions "largely disagree with results from Bartunov et al 2018".	weakness
2019-479	I would suggest a slight rewording here: the authors' results *extend* our understanding of Bartunov et al 2018.	suggestion
2019-479	They do not disagree in the sense that this paper also finds that feedback alignment alone is insufficient to train large models on ImageNet.	rating_summary
2019-479	- Figure 1: I was expecting to see a curve for performance of feedback alignment on AlexNet	weakness
2019-479	- Figure 1: The colors are hard to follow.	weakness
2019-479	For example, the two shades of purple represent the two FA models, which makes sense, but then there are two separate hues (black and blue) for the sign-symmetry models.	weakness
2019-479	Instead, I would suggest keeping black (or gray) for backpropagation (the baseline), and then using two hues of one color (e.g. light blue and dark blue) for the two sign-symmetry models.	suggestion
2019-479	This would make it easier to group the related models.	suggestion
2019-479	- Figure 2: Would be nice if these colors (for backprop/FA/SS) matched the colors in Figure 1.	suggestion
2019-479	- Figure 3: Why is there such a small change in the average alignment angle (2 degrees?) I found that surprising.	suggestion
2019-479	- Figure 3: The right two panels would be clearer on the same panel.	weakness
2019-479	That is, instead of showing the std.	weakness
2019-479	dev. separately, show it as the spread (using error bars) on the plot with the mean.	weakness
2019-479	This makes it easier to get a sense if the distributions overlap or not.	weakness
2019-479	- Figure 3 (b/c): Could also use the same colors for BP/SS as Figs 1 and 2.	weakness
2019-479	- Figure 3 (caption): I think the blue/red labels in the caption are mixed up for panel (a).	weakness

2019-513	This paper looks at solving optimization problems that arise in GANs, via a variational inequality perspective (VIP).	abstract
2019-513	VIP entails solving an optimization problem that is related to the first order condition of the optimization problem that we wish to solve.	abstract
2019-513	VIP have been very successful in solving min-max style problems.	abstract
2019-513	Given that, GAN formulations tend to be min-max style problems (though not necessarily 0 sum) the VIP perspective is very natural, though under-explored in machine learning.	abstract
2019-513	Two techniques that have been widely used to solve VIP problems are averaging and extragradient methods.	abstract
2019-513	The authors look at a simple GAN setup where both the generator and the discriminator are linear models.	abstract
2019-513	In this case two kinds of gradient updates can be derived.	abstract
2019-513	First are simultaneous updates, and the other is alternated updates.	abstract
2019-513	The authors show that simultaneous updates are not even bounded and diverge to infinity, whereas alternated updates are more stable and stay bounded, but need not necessarily converge.	abstract
2019-513	However, I think this behaviour is limited to only linear discriminator/generator and might not extend beyond the linear case.	weakness
2019-513	The second key idea is the use of extra-gradient updates.	weakness
2019-513	Extra-gradient updates perform an "extra" or fake gradient step to get to a new point, and then kind of retracks back and perform a gradient step using the gradient step obtained from the "extra step".	weakness
2019-513	This extra-gradient method is a close approximation to Euler's method, though far more computationally efficient.	weakness
2019-513	However, the extragradient step requires one to calculate gradient twice, which can be expensive in large models.	weakness
2019-513	For this reason, the authors suggest using gradients from past as the "extragradient" in the extragradient method.	weakness
2019-513	For strongly-monotone operators (a generalization of strongly-convex functions) extrapolation updates are shown to have linear convergence.	weakness
2019-513	Furthermore, the authors show that using extrapolation and averaging under the assumption that the operator is monotonic, and using constant step size SGD the rates of convergence are better than the rates obtained using plain SGD with averaging but without extrapolation.	weakness
2019-513	Authors also show how one can use these ideas using other first order methods such as ADAM instead of SGD.	abstract
2019-513	Experiments are shown on the DCGAN architecture.	abstract
2019-513	On the whole this is a really nice paper, that shows how standard ideas from VIP can be useful for training GANs. I recommend acceptance Overall, the paper is well-written and of high quality, therefore I recommend acceptance.	decision
2019-513	Pros: + The work gives an accessible but still rigorous introduction to the literature on VIs which I find highly valuable, as it creates a bridge between the classical mathematical programming literature and applications in AI.	strength
2019-513	+ The theory for optimization of VIs with stochastic gradients (though only in monotone setting) was very interesting to me and contains some novel results (Theorem 2, Theorem 4)	strength
2019-513	Cons: - I'm a bit skeptical about the experiments on GANs. They indicate that for the specific choice of architectures and hyper-parameters "ExtraAdam" works better, but the chosen architectures are not state-of-the art.	weakness
2019-513	What would convince me if the algorithm can be used to improve a current best inception score of 8.2 reached with SNGANs. Also with WGAN-GP, scores of ~7.8 are reported which are much higher than the 6.4 reported in the paper.	weakness
2019-513	But I understand that producing state-of-the-art inception scores is not the focus of the paper, therefore I would suggest that the authors release an implementation of the proposed new optimizers (ExtraAdam) for a popular DL framework (e.g. pytorch) such that practitioners working with GANs can quickly try them out in a "plug-and-play" fashion.	suggestion
2019-513	- Proposition 2 is a bit misleading.	weakness
2019-513	While for \\eta \\in (0, 1) implicit and extrapolation are similar, adding the remark that implicit method is stable for any \\eta > 0 (and therefore can lead to an arbitrary fast convergence) would give a more balanced view.	suggestion
2019-513	Right now, only the advantages of extrapolation method and disadvantages of implicit method are mentioned which I find unfair for the implicit method.	weakness
2019-513	- The theory is presented for variational inequalities with monotone operators.	weakness
2019-513	For clarity it should be mentioned that GANs parametrized with neural nets lead to non-monotone VIs. A provably convergent algorithm for that setting is still an open problem, no?	weakness
2019-513	Summary: The authors take a variational inequality perspective to the study of the saddle point problem that defines a GAN.	abstract
2019-513	By doing so, they are able to profit from the corresponding literature and propose a few methods that are variants of SGD.	abstract
2019-513	The authors show in a simple example (a bilinear function) these exhibit better performance than Adam and a basic gradient method.	abstract
2019-513	After showing theoretical guarantees of these methods (linear convergence) the authors propose to combine them with existing techniques, and show in fact this leads to better results.	abstract
2019-513	Evaluation This is a very good paper and I cannot but recommend its acceptance: It is clear and well written.	decision
2019-513	It has the right level of balance between theory and experiments.	strength
2019-513	Theoretical results are far from trivial.	strength
2019-513	I haven't seen something similar.	strength
2019-513	The authors's do not make overstatements: they do not claim to have solved the GAN problem, but they do report improvements which are due to a thorough analysis (see above points).	strength
2019-513	These results are much appreciated.	misc

2019-594	This paper proposes a novel clustering technique that combines the self-organising map (SOM) (Kohonen, 1998) ideas with the differentiable quantized clustering ideas of VQ-VAE (van den Oord et al, 2017).	abstract
2019-594	The resulting algorithm is able to achieve better unsupervised clustering than either technique on its own.	abstract
2019-594	It also beats the k-means clustering approach.	abstract
2019-594	The authors also suggest augmenting their setup with a model of cluster transition dynamics for time-series data, which seems to improve the clustering further, as well as providing an interpretable 2D visualisation of the system's dynamics.	abstract
2019-594	This approach addresses an important problem of easy interpretable visualisation of complex dynamics of a multi-dimensional system.	abstract
2019-594	This solution can have immediate wide spread real life applications, for example in fields like medicine or finance.	abstract
2019-594	The paper is very well written and the model clearly outperforms its baselines.	strength
2019-594	The authors also include very nice evaluation of the importance of the different parts of the model for the final performance.	strength
2019-594	This is one of the best papers I have reviewed in a while.	misc
2019-594	The only question I have is in terms of the medical data.	weakness
2019-594	The map learnt by SOM-VAE-prob presented in Fig. 4 appears to have 2 clusters with 'less healthy' patients (near the top left and top right edges).	weakness
2019-594	It would be good to have an analysis of what differences there are between these two clusters, and whether they are recovered consistently.	suggestion
2019-594	This work addresses the problem of learning latent embeddings of high-dimensional time series data.	abstract
2019-594	The paper emphasises the need of interpretable representations accounting for the correlated nature of temporal data.	abstract
2019-594	To this scope, the study proposes to cluster the data in a latent space estimated through an auto-encoder.	abstract
2019-594	The clustering is obtained by leveraging on the idea of self-organising maps (SOM).	abstract
2019-594	Within this setting, the data is mapped into a 2D lattice where each coordinate point represents the center of an inner cluster.	abstract
2019-594	This construction motivates the formulation of the auto encoder through the definition of several cost terms promoting reconstruction, clustering, and consistency across latent mappings.	abstract
2019-594	This definition of the problem allows an heuristic for circumventing the non-differentiability of the discrete mapping.	abstract
2019-594	The enhance consistency over time, the model is further equipped with an additional cost term enforcing transition smoothness across data points and latent embeddings.	abstract
2019-594	The experiments are carried out with respect to synthetic 2D time-series, chaotic time-series from dynamical systems, and clinical data.	abstract
2019-594	In each case the proposed method shows promising results with respect to the proposed benchmark.	strength
2019-594	The study presents some interesting methodological and technical ideas.	strength
2019-594	On the other hand the manuscript presentation is quite convoluted, at the expense of a lacks of clarity in the details about the implementation of the methodology.	weakness
2019-594	Moreover, motivated by practical aspects, the model optimisation relies on computational strategies not completely supported from the theoretical point of view (such as the zeroing of the gradient in backpropagation, or the approximation of the clustering function to overcome non-differentiability).	weakness
2019-594	The impact of these modeling choices would deserve more investigation and discussion.	weakness
2019-594	Detailed comments: - As also stated by the authors, the use of a 2D latent representation is completely arbitrary.	weakness
2019-594	It may be true that a 2D embedding provides a simple visualisation, however interpretability can be obtained also with much richer representations in a number of different ways (e.g. sparsity, parametric representations, …).	weakness
2019-594	Therefore the feeling is that the proposed structure may be quite ad-hoc, and one may wonder whether the algorithm would still generalise to more complex latent representations.	weakness
2019-594	- Related to the previous comment, the number of latent points seems to be crucial to the performance of the method.	weakness
2019-594	However this aspect is not discussed in detail, while it would be beneficial to provide experiment about the sensitivity and accuracy with respect to the choice if this parameters.	weakness
2019-594	- The method relies on several cost terms plugged together.	weakness
2019-594	While each of them takes care of specific consistency aspects of the model, their mutual relation and balance may be very critical.	weakness
2019-594	This is governed by a series of trade-off parameters whose effect is not discussed  nor explored throughout the study.	weakness
2019-594	I guess that the optimisation stability may be also quite sensitive to this trade-off, and it would be important to provide more details about this aspect.	weakness
2019-594	- Surprisingly, k-means seems to perform quite well in spite of its simplicity.	weakness
2019-594	Also, there is no mention about initialisation and choice of the parameter "k".	weakness
2019-594	The authors may want to better discuss the performance of this algorithm, especially compared to its much lower modeling complexity with respect to the proposed method.	weakness
2019-594	- Still related to the comparison with respect to the state-of-art, interpretability in time series analysis can be achieved with much lesser assumptions and parameters by using standard approaches such as independent component analysis.	weakness
2019-594	I would expect this sort of comparison, especially in case of long-term data such as the one provided in the Lorenz system.	weakness
2019-594	- Clustering of short-term time series, such as the clinical ones, is a challenging task.	weakness
2019-594	The feeling is that a highly parametrised model, such as the proposed one,  may still not be superior with respect to classical methods, such as the mixture of linear regressions.	weakness
2019-594	This sort of comparison would be quite informative to appreciate the real value of the proposed methodology.	weakness
2019-594	This paper proposes a deep learning method for representation learning in time series data.	abstract
2019-594	The goal is to learn a discrete two-dimensional representation of the time series data in an interpretable manner.	abstract
2019-594	The model is constructed on the basis of self-organizing maps (SOM) and involves reconstruction error in the training.	abstract
2019-594	In order to address the non-differentiability in the discrete representation assignment, the authors propose to include an extra reconstruction loss term w.r.t. the discrete representation.	abstract
2019-594	The authors conduct experiments on both static and time series data and validate that the method perform better than related methods in terms of clustering results as well as interpretability.	abstract
2019-594	This paper deals with an interesting problem as learning an interpretable representation in time series data is important in areas such as health care and business.	abstract
2019-594	However, I am afraid the presentation of this paper is a bit difficult to follow.	weakness
2019-594	Some concerns/questions as below: 1) As the paper is based on SOM, some illustration of this method would be helpful for readers to understand the idea and learn the major contribution;	weakness
2019-594	2) The authors use NMI and purity to evaluate the clustering performance.	weakness
2019-594	I was curious why not use the clustering accuracy as well?	weakness
2019-594	3) Some more explanation on Fig. 4(d) would be helpful.	weakness

2020-81	This paper studies impossibility results of GNN in the worst-case sense.	abstract
2020-81	In particular, it reduces GNN to a distributed computing model CONGEST and adapt the impossibility result from distributed computing to GNNs.	abstract
2020-81	The impossibility results show that for certain problems, e.g., subgraph detection, there exists a graph such that GNN can not solve the problem unless the product of a GNN's depth and width exceeds (a function of) the graph size.	abstract
2020-81	I am not an expert of distributed computing and I did not check all the proofs thoroughly.	misc
2020-81	But I do think this paper provides a solid contribution to broaden the community's understanding about what the limitations of GNNs are.	strength
2020-81	Overall, I tend to accept it and would like to increase the score based on authors' feedback.	decision
2020-81	Pros: 1, I like the contribution of the paper which tries to build connections between GNNs and distributed computing models.	strength
2020-81	From the perspective of computation, GNNs and distributed algorithms do share a lot of similarities.	strength
2020-81	Therefore, some algorithm design choices in distributed computing would shed some light on designing novel GNNs. This may open a new direction for the community.	strength
2020-81	2, The depth and width dependency results are novel in the context of GNNs. Cons & Questions & Suggestions: 1, Since these impossibility results for a certain subclass of GNNs are in the worst-case sense, it is not clear how it would be useful for practical machine learning problems.	weakness
2020-81	Some discussion along this line would be very helpful.	suggestion
2020-81	2, It would be great to discuss the relationship between the Turing universality and the universality of function approximation studied in [1].	suggestion
2020-81	3, For people who have no background of distributed computing, it would be great to describe CONGEST before going to the impossibility results reduced from CONGEST to GNNs. 4, I do not recommend authors to refer to the computation model 1 as GNN.	suggestion
2020-81	You could name it as MPNN in order to make the claim more accurate.	suggestion
2020-81	GNN in general has a few variants which does not fall into this category and could have higher capacity than MPNN.	suggestion
2020-81	For example, the authors claim that "graph neural networks always sum received messages before any local computation".	suggestion
2020-81	However, this is not true in GraphSAGE [2] where the aggregation is a LSTM rather than a simple sum.	suggestion
2020-81	It makes the model resemble more to the computational model 2.	weakness
2020-81	Recent spectral graph convolutional networks [3,4] leverages Krylov subspace methods to compute approximated eigenvalues and eigenvectors of the graph Laplacian which are further used to compute long-range propagation / high-power Laplacian to improve representation power.	abstract
2020-81	The results on depth may not hold for these models any more since one layer graph convolution could aggregate multi-hop information.	weakness
2020-81	Therefore, being more specific on the model class would make the conclusion more accurate.	suggestion
2020-81	It would be great to discuss these models separately from the computation model 1.	suggestion
2020-81	[1] Chen, Z., Villar, S., Chen, L.	misc
2020-81	and Bruna, J., 2019. On the equivalence between graph isomorphism testing and function approximation with GNNs. arXiv preprint arXiv:1905.12560.	misc
2020-81	[2] Hamilton, W., Ying, Z.	misc
2020-81	and Leskovec, J., 2017. Inductive representation learning on large graphs.	misc
2020-81	In Advances in Neural Information Processing Systems (pp.	misc
2020-81	1024-1034). [3] Liao, R., Zhao, Z., Urtasun, R.	misc
2020-81	and Zemel, R.S., 2019. Lanczosnet: Multi-scale deep graph convolutional networks.	misc
2020-81	arXiv preprint arXiv:1901.01484. [4] Luan, S., Zhao, M., Chang, X.W. and Precup, D., 2019.	misc
2020-81	Break the Ceiling: Stronger Multi-scale Deep Graph Convolutional Networks.	misc
2020-81	arXiv preprint arXiv:1906.02174. ====================================================================================================== The response from authors address most of my concerns.	rebuttal_process
2020-81	I improved the score. This paper studies theoretical properties of GNN in particular their expressive power.	rebuttal_process
2020-81	There are many recent works on this topic and the 2019 *CONF* paper 'How Powerful are Graph Neural Networks?' is the closes related to this paper.	rebuttal_process
2020-81	In the 2019 paper connects GNN with the  Weisfeiler-Lehman graph isomorphism test in theoretical computer science.	abstract
2020-81	This paper makes a connection between GNN and the locality notion developed in distributed computing.	abstract
2020-81	This connection is rather obvious and GNN being particular local algorithms, their expressive power is at least as limited as the expressive power of local algorithms.	abstract
2020-81	In this paper, results in distributed computing are reformulated in a GNN framework mapping the number of rounds required by a local algorithm to the depth of the GNN in order to solve a given graph problem in a worst case scenario.	abstract
2020-81	In my opinion, this paper is rather incremental.	weakness
2020-81	In order to improve it, it would be nice to see experiments supporting the theoretical results in section 4.	suggestion
2020-81	Here it is not clear at all if the bounds given are tight in practice.	weakness
2020-81	### The authors added experiments supporting their theoretical results.	weakness
2020-81	I am upgrading my rating to weak accept.	weakness
2020-81	Summary This paper studied the expressive power of graph NNs, specifically, their universality and limitations under the non-anonymous setting, via the theory of distributed computations.	abstract
2020-81	For the universality, it proved the Turing completeness of graph NNs if messaging and aggregation functions are sufficiently strong.	abstract
2020-81	For the limitation, it characterized the lower bound of width for solving graph-theoretic tasks (such as subgraph detection, subgraph verification, approximate, and exact optimization problems) using graph NNs.	weakness
2020-81	The key idea is to reduce the computation model of graph NNs to LOCAL (for Turing completeness) or CONGEST (for limitations), which are well-studied in the literature of distributed computations and use the known results for these models.	weakness
2020-81	Decision This paper gave us a new approach to analyzing the expressive power of graph NNs. Not only does this paper give new theoretical results, but also it opens the door to a new research direction by bridging the theories of graph NNs and distributed computations.	strength
2020-81	However, I cannot confirm the correctness of the proof of Theorem 3.1 (see Suggestions section).	weakness
2020-81	For now, I am tending to accept the paper.	decision
2020-81	But I want to determine the final decision after I am certain that the proof of the theorem is correct.	misc
2020-81	We can roughly divide existing approaches for studying the expressive power of graph NNs into two.	suggestion
2020-81	One is to compare the power of discriminating non-isomorphic graph pairs with isomorphism tests such as the WL isomorphism test (Xu et al., 2019).	ac_disagreement
2020-81	The other one is to theoretically justify the oversmoothing phenomena (Li et al., 2018).	suggestion
2020-81	The proof techniques the authors used are different from both of the two.	suggestion
2020-81	It related a graph NN to the computational models LOCAL and CONGEST, and enabled to incorporate the theory of distributed computations.	strength
2020-81	By doing so, the authors successfully derived many lower bounds in a systematic way, proving the effectiveness of their strategy.	strength
2020-81	I think we can expect that a more refined analysis inspired by this approach will appear in the future.	misc
2020-81	Regarding the Experience Assessment: I have published several papers in graph NNs (4).	abstract
2020-81	But I do not know much about the area of the theory of distributed algorithms (1--3).	weakness
2020-81	Suggestions - Section 3.2 - Theorem 3.1 proves the equivalence of GNN_n and LOCAL.	suggestion
2020-81	However, the definition of equivalence is missing.	weakness
2020-81	Please write it in the main part, since this theorem is the key result of this paper.	suggestion
2020-81	- I could not find any reference for the Turing completeness of the LOCAL model.	weakness
2020-81	Could you add the reference for it?	suggestion
2020-81	- The description of the CONGEST model is only available in the appendix informally (Appendix B.3).	weakness
2020-81	Could you write it in the main part?	suggestion
2020-81	- The authors emphasized the importance of the universality and limitation results in the introduction and paragraph after Corollary 3.1.	suggestion
2020-81	In my opinion, the importance of such tasks is in machine learning community (Cybenko's paper on the universality of MLPs (Cybenko, 1989) is one of the most cited papers in the community).	strength
2020-81	Rather, I think many graph NN researchers who are expected to read this paper are not familiar with the theory of distributed computations.	weakness
2020-81	Therefore, I would recommend to use page resources to explain the basic concepts of the distributed computation theory.	suggestion
2020-81	Questions - Is there any existing work which tries to solve the graph theoretical tasks using graph NNs?	suggestion
2020-81	If there is, can the theorems in this paper give explanations for the results?	suggestion
2020-81	[Cybenko, 1989] Cybenko, George. Approximation by superpositions of a sigmoidal function.	misc
2020-81	Mathematics of control, signals and systems 2.4 (1989): 303-314.	misc
2020-81	[Li et al., 2018] Qimai Li, Zhichao Han, and Xiao-Ming Wu. Deeper Insights into Graph Convolutional Networks for Semi-Supervised Learning.	misc
2020-81	In Proceedings of the 32nd AAAI Conference on Artificial Intelli- gence, pp.	misc
2020-81	3538–3545, 2018. [Xu et al., 2019] Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka.	misc
2020-81	How powerful are graph neural networks?	misc
2020-81	In International Conference on Learning Representations, 2019.	misc

2020-286	Disclosure on reviewer's experience: I am not an expert on adversarial attack methods or defenses, but I am well read in the general literature on robustness and uncertainty in deep neural networks.	misc
2020-286	The authors present a biologically inspired sleep algorithm for artificial neural networks (ANNs) that aims to improve their generalization and robustness in the face of noisy or malicious inputs.	abstract
2020-286	They hypothesize that "sleep" could aid in generalization by decorrelating noisy hidden states and reducing the overall impact of imperceptible perturbations of the input space.	abstract
2020-286	The proposed sleep algorithm broadly involves	abstract
2020-286	1) converting the trained ANN to a "spike" neural network (SNN),	abstract
2020-286	2) converting the input signal (pixels) to a Poisson distributed "spike train" where brighter pixels have higher firing rates than darker pixels,	abstract
2020-286	3) propagating the neuronal spikes through the SNN, updating weights based on a simplified version of spike-timing-dependent plasticity (STDP), and	abstract
2020-286	4) converting the network back to an ANN after the sleep phase has finished.	abstract
2020-286	They present a detailed comparative study spanning three datasets, four types of adversarial attacks and distortions, and two other baseline defense mechanisms, in which they demonstrate significant improvements (in some cases) of the sleep algorithm over the baselines.	abstract
2020-286	The core concept behind the authors' work is novel and interesting, and the experimental design is thorough and well controlled.	strength
2020-286	Although the results are (I would argue) somewhat mixed, they are nonetheless positive enough to encourage more work in applying "sleep" and other relevant ideas from neuroscience to the problem of robustness in deep neural networks.	strength
2020-286	I have some questions and concerns which I will detail per-section below, but overall, I believe that this paper is a valuable contribution to the literature and should be accepted once the authors have made a few necessary revisions.	decision
2020-286	Section 1: Introduction "We report positive results for four types of adversarial attacks tested on three different datasets (MNIST, CUB200, and a toy dataset) ..."	weakness
2020-286	It's debatable whether or not the results from the CUB-200 dataset are positive.	decision
2020-286	The sleep algorithm fails to outperform the baselines for each attack type (except for an almost negligible advantage in accuracy on JSMA) and barely even outperforms the control network in most cases (2/4 attacks it actually underperforms the control).	weakness
2020-286	I think the authors should consider rephrasing this statement to better reflect the actual results.	suggestion
2020-286	Section 2: Adversarial Attacks and Distortions	misc
2020-286	FGSM: The notation used here is somewhat inconsistent with the source paper.	weakness
2020-286	Goodfellow et al use epsilon to denote what I think the authors call eta, and call the second term, epsilon*sign(grad(J)), eta.	weakness
2020-286	Furthermore, the authors state that "this represents the direction to change each pixel in the original input in order to decrease the loss function." But this doesn't make sense.	weakness
2020-286	An adversary should want to *increase* the loss function enough to cause a misclassification.	weakness
2020-286	Goodfellow et al use this expression to formulate a L1-like regularization term and describe the training procedure "minimizing the worst case error when the data is perturbed by an adversary", which seems more sensible.	weakness
2020-286	This section should be rewritten to be more consistent with the source.	weakness
2020-286	Section 3: Adversarial defenses Regarding distillation: "We use T=50 to compare with the sleep algorithm"	weakness
2020-286	The authors should elaborate a bit more on the reasoning for this choice.	suggestion
2020-286	It seems very arbitrary. Sectioin 4: Sleep algorithm	weakness
2020-286	1. Algorithm 1: Why is line 9 inside of the for loop?	weakness
2020-286	It doesn't seem to be at all dependent on t.	weakness
2020-286	One would expect the input to only need to be converted once.	weakness
2020-286	Additionally, in lines 11-13, the l's in W(l,l-1) and similar should be unbolded.	weakness
2020-286	It's confusing that the format changes (unless I am missing something and it's actually a different variable).	weakness
2020-286	2. Spike trains should be more rigorously defined, preferably with formalized notation.	weakness
2020-286	It's a bit unclear exactly what they are from the current text.	weakness
2020-286	Are they just parameters for a Poisson?	weakness
2020-286	Or outputs from a poison over T time steps?	weakness
2020-286	Or something else? 3. "weights are scaled by a parameter to induce high firing rates in later layers"	weakness
2020-286	It would be good to include more details on this parameter, how the values are chosen, and the intuition behind this idea.	weakness
2020-286	I assume it's because of higher level feature representations in later layers of deep neural networks.	weakness
2020-286	Section 5: Results 1. It's confusing that sometimes accuracy refers to classification accuracy and sometimes adversarial attack accuracy.	weakness
2020-286	I would recommend assigning a different name to the latter, or making sure that a qualifier precedes every reference to "accuracy" in this section.	suggestion
2020-286	2. In the second section of the results table (which is missing a label), why is the JSMA value for Defensive Distillation bolded?	suggestion
2020-286	The distance measures for both the control network and for fine-tuning are higher.	suggestion
2020-286	It seems like fine-tuning should be the one bolded.	weakness
2020-286	3. Figure 1: caption is incorrect; it states "adversarial attack accuracy" and it should be "classification accuracy", otherwise the plots make no sense.	weakness
2020-286	4. "we observe that in the Patches and CUB-200 dataset, sleep has beneficial results in moving the accuracy function above the other defense methods"	weakness
2020-286	It should be noted that this is only true for eta < 0.1.	weakness
2020-286	After that, sleep and the control both converge to 50% accuracy.	weakness
2020-286	Also this sentence should be reworded to be less visual and more quantitative (e.g. sleep tends to have higher median accuracy scores than the other methods for eta < 0.1).	weakness
2020-286	5. "We observe that performance continued to drop after a sufficiently large amount of noise was added"	weakness
2020-286	More than that, the other methods converged to a small band of accuracy values; sleep continued to deteriorate.	weakness
2020-286	This is a significant difference.	weakness
2020-286	It would be a good idea to re-run this experiment with a binary classification problem (e.g. only two digits of MNIST) and see if this phenomenon still occurs.	suggestion
2020-286	Then, the noisy sleep classifier predictions could simply be inverted to get improved accuracy scores.	suggestion
2020-286	6. In the analysis of JSMA, as noted before,, it's rather dubious to claim that sleep had any kind of significant effect on the attack success rate (or distance) for CUB-200.	weakness
2020-286	I would rewrite this section to better represent the results.	suggestion
2020-286	7. Figure 2 formatting: Legend is overflowing out of the first figure.	weakness
2020-286	Additionally, the legend colors should be made to match across all three figures, and the legend should either appear in all three (if necessary for some reason) or only in one.	weakness
2020-286	8. Figure 2: The caption is incomplete and possibly incorrect.	weakness
2020-286	It's not clear why the first and last figures differ from each other, and the caption does not indicate this.	weakness
2020-286	The caption also only mentions two datasets, even though it says "for the following three datasets".	weakness
2020-286	Appendix: General formatting needs improvement. A lot of figures are off-centered, text misaligned, missing axis labels, etc.	weakness
2020-286	The paper proposes an ANN training method for improving adversarial robustness and generalization, inspired by biological sleep.	abstract
2020-286	I'm leaning towards accepting as it seems to be an original concept and has fairly extensive empirical results that are somewhat promising.	decision
2020-286	The idea of a sleep phase as an alternative to explicit adversarial or generalization training is interesting.	strength
2020-286	The results suggest that the approach works reasonably well in many cases.	strength
2020-286	Suggestions for improvement / clarification: - The mapping from biological sleep to the actual algorithm + pseudocode used could benefit from more thorough explanation.	weakness
2020-286	It is not clear which choices are arbitrary vs well-principled.	weakness
2020-286	- Was the optimal sleep duration determined empirically for each experiment?	weakness
2020-286	- I agree with the authors' proposed future work of better understanding and standardizing this approach.	weakness
2020-286	- Consider combining this approach with the existing adversarial or generalizing approaches (instead of as an alternative).	weakness
2020-286	Do they complement each other?	weakness

2020-328	The authors provide a comprehensive study, with theory and classical simulation of the quantum system, on how to increaese the speed of CNN inference and training using qubits.	abstract
2020-328	They proved an intrigued compilation of a quantized convolutional system.	abstract
2020-328	For an audiance who are not quantum experts one could clarify a few properties of the described "quantization".	abstract
2020-328	It is not able to take multiple images in at the same time as quantum superpositions.	abstract
2020-328	Sometimes the expectence of a quantum machine leanring is that it would train the system at a single instance.	abstract
2020-328	Here the increase in effciency comes from being able to perform marix multiplication in quantum realm.	abstract
2020-328	The manuscripd describes a creative way to bring bolean operartions-defined non-linearities into quantum neural networks, at the cost of having to force the system back to classical domain at each layer - and then encoding it back to qubits for the next layer operations.	abstract
2020-328	The price has to payed as unitary operators  (the ones that preserve entaglement) are inherently linear.	abstract
2020-328	Remarks: The authors should point out how this is specific to convolutional neural networks.	suggestion
2020-328	It looks to me that the same algorithm could be used for fully connected or even attention based systems, as it is just a matrix multiplication as well.	weakness
2020-328	Anyway, the manuscript provides the take into account the steps required specifically for a CNN.	strength
2020-328	Some minor remarks: For classical systems the capped Relu is inferior as it reduces the range of values of activations where there is driving force.	weakness
2020-328	Sometimes one is using a parametrized version of ReLU that has a small positive slope for negative values.	weakness
2020-328	It is not clear to me how this would not be the case with a quantum implementions.	weakness
2020-328	In your simulations, does the value of the saturation constant C, change the speed of convergence to a good solutions.	weakness
2020-328	The capped Relu reminds me a lot of a Tanh non-linearity  that works well for LSTMs but are not very good for CNNs.	weakness
2020-328	I would suggest that for the conference presentation the authors try to bring out the essential within a less formal setting to open it to a wider ML audience.	suggestion
2020-328	For the manuscript the level is good with a proper use of appendix to shorten the main narrative.	suggestion
2020-328	The authors present a quantum algorithm for approximating the forward pass and gradient computation of a classical convolutional neural network layer with pooling and a bounded rectifier activation.	abstract
2020-328	This algorithm has complexity bounds that would open up (for instance) the possibility of exponentially large filter banks, and the authors show through a simple, classical simulation approach that the resulting network is also likely to be trainable.	abstract
2020-328	Feedback: A few typos/formatting issues: - The title accidentally includes "Conference Submissions"	weakness
2020-328	- The in-text citation format frequently has the parentheses in the wrong place; this is surprisingly distracting!	weakness
2020-328	Preliminaries: - Maybe explain what the ith vector in the standard basis is in terms of |0> and |1>?	suggestion
2020-328	I assume the answer is along the lines of |000>, |001>, |010>, etc.?	suggestion
2020-328	Main results: - The sentence "a speedup compared to the classical CNN for both the forward pass and for training using backpropagation in certain cases" is ambiguous; does "in certain cases" qualify only training speed or also forward pass speed?	weakness
2020-328	- There's a clear separation of background (which is concise and well explained) and contributions, but maybe it would be worth connecting the introduced algorithm more closely to existing work in non-convolutional quantum neural networks?	suggestion
2020-328	- Can you briefly justify (or cite) the claim that "most of the non linear functions in the machine learning literature can be implemented using small sized boolean circuits"?	suggestion
2020-328	- I'm a little confused about the discussion of quantum importance sampling on page 4.	weakness
2020-328	Could you give some intuition for the relationship between eta and the fraction of output values that are on average flushed to zero (is this 1 minus sigma?), and perhaps connect this to the literature about activation pruning and sparse NNs?	suggestion
2020-328	- Maybe define what you mean by "tomography" for ML folks without the quantum background?	suggestion
2020-328	- I'm convinced by the simulations, even though I shouldn't really be convinced by anything on MNIST...	suggestion
2020-328	It just seems like the perturbations you're applying are all things that modern neural networks take in stride.	weakness
2020-328	- The discussion of using a sigma-based classical sampling rather than the eta-based quantum importance sampling mentions a "Section C.1.15" which does not exist (I think you mean the end of Section C.1.5).	weakness
2020-328	- Re: "We will use this analogy in the numerical simulations (Section	weakness
2020-328	6) to estimate, for a particular QCNN architecture and a particular dataset of images, which values of σ are enough to allow the neural network to learn." My understanding is that you're getting empirical estimates of which values of sigma are enough;	weakness
2020-328	it would be valuable to convert those to estimates of which values of eta would be enough (given quantum networks of the size used in the classical simulation experiment, or given larger networks).	suggestion
2020-328	- The sampling procedure based on sigma might be inefficient in your PyTorch implementation, but it's certainly something that GPUs are fairly well suited to computing.	suggestion
2020-328	There might be other PyTorch operators that would help here (perhaps Bernoulli sampling?) or if nothing else you could write a small custom CUDA kernel.	suggestion
2020-328	This paper presents a quantum version of the convolutional neural networks.	abstract
2020-328	They derive equivalent versions the computation of both the convolution operation and the back-propagation in the quantum computing framework.	abstract
2020-328	The authors claim a potential exponential speed up in the computation in the size of the kernel which would make possible to process much bigger inputs or just speed up current tasks involving images mainly.	abstract
2020-328	They exemplify the method on MNIST using quantum artifacts simulation using PyTorch and show their method is competitive with SoA.	abstract
2020-328	I think the paper is well written and provides a nice discussion about how quantum computing can be applied to CNNs. I appreciate that both the forward and backward passes are studied although most of the technical details are in appendices.	strength
2020-328	So, I felt the paper  itself was a bit optimistic about the impact of quantum computing on CNNs. Especially, I found the experimental section was missing details.	weakness
2020-328	I am not a quantum computing expert but I was a bit surprised that, in the context of CNNs, the introduction of quantum noise was just considered as introducing noise in the image and the gradient and then applying normal CNNs to the resulting image.	weakness
2020-328	Standard CNNs can probably deal with such noise and noisy gradient descent is not a big issue as such (it can even avoid local minima).	weakness
2020-328	But CNNs are notoriously known to reduce the number of weights in a network because of weight sharing.	weakness
2020-328	So, it is not all about making one convolution faster but also to compute invariant representations by sharing weights over different parts of the inputs.	weakness
2020-328	I would have been interested by a discussion about how quantum noise may impact this property and I didn't find this in the paper nor the appendices.	weakness
2020-328	Also the author confess that the learning is not stable and results on MNIST to be the best they could get.	rebuttal_process
2020-328	I think it would be worth testing on large scale problems and see whether larger kernels with such noisy conditions would really improve the performance.	suggestion
2020-328	This submission proposed a quantum convolutional neural network (QCNN).	abstract
2020-328	The theoretical results in section 3 state the existence of the QCNN satisfying certain conditions.	abstract
2020-328	The QCNN is given by sections 4 and 5, with empirical evaluates in section 6.	abstract
2020-328	This subject is out of my usual area.	misc
2020-328	However, I tend to think this subject is interesting to the *CONF* audience due to the recent advancement in quantum computing.	strength
2020-328	title, remove "conference submissions" Section 2, introduce QRAM.	suggestion
2020-328	Somewhere around section 2-3, and in section 6, it has to be mentioned whether the proposed QCNN requires special hardware, and what is the hardware, and why it is required.	weakness
2020-328	Note the cited Cong et al. (2018) has been published in nature physics.	weakness
2020-328	As you both used the term "QCNN", it is better to explain more clearly what is the main difference in the main text.	weakness

2020-412	This paper investigates a so-called "compressive transformer" approach.	abstract
2020-412	The idea is to compress distant past memories into a coarse-grained representation while keeping a fine-grained representation for close past memories.	abstract
2020-412	A variety of compression techniques and training strategies have been investigated in the paper and verified using tasks from multiple domains including language modeling, speech synthesis and reinforcement learning.	abstract
2020-412	Particularly, the authors propose a new benchmark PG-19 for long-term sequence modeling.	abstract
2020-412	Overall, I found the work interesting and experiments are thorough and strong.	strength
2020-412	It is always great to see a new benchmark released to the community.	strength
2020-412	That being said, I have concerns regarding the paper.	misc
2020-412	The authors put huge amount of effort into the experiments but only describe the proposed technique in a very rough and abstract way, lacking necessary technical details to formulate the technique.	weakness
2020-412	What is the mathematical formulation of the problem?	weakness
2020-412	How exactly the compression is carried out on various network architectures is not clear after reading the paper.	weakness
2020-412	Also, I guess many readers including me do not have a perfect understanding of Fig. 1 although it shows something intuitively.	weakness
2020-412	(What is the difference between different colors?	weakness
2020-412	What is the difference between sequence, memory, and compressed memory?	weakness
2020-412	What do the arrows mean?	weakness
2020-412	There is no explanation whatsoever either in the figure or in the caption).	weakness
2020-412	This is the major concern I have regarding the paper.	misc
2020-412	Despite of the strong experimental presentation, lacking the technical details has significantly hurt the quality of the paper.	weakness
2020-412	P.S. Thanks for the rebuttal. I have lifted my score.	rebuttal_process
2020-412	## Updated review I have read the rebuttal.	rebuttal_process
2020-412	First I'd like to thank the authors for the detailled rebuttal.	misc
2020-412	The latest version of the paper adressed all my concerns, hence I change my rating to Accept.	rebuttal_process
2020-412	## Original review This paper presents a new variation of the Transformer model, named Compressive Transformer.	abstract
2020-412	The key novelty of this model is to preserve long range memory in a compressed form, instead of discarding them as previous models have done.	abstract
2020-412	This improves the long-range dependencies modelling capabilities of the approach.	abstract
2020-412	The model is evaluated on two common language modelling benchmarks and yields state of the art results in both of them.	abstract
2020-412	The paper also introduces a new benchmark for long-range dependencies modelling composed of thousands of books.	abstract
2020-412	The paper finally presents an analysis of the compressed memory and provide some insights, including the fact that the attention model uses the compressed memory.	abstract
2020-412	The model is also evaluated on two other tasks: speech generation and reinforcement learning on videos.	abstract
2020-412	I think this paper should be accepted, mainly because: - The proposed model is novel as far as I can tell.	decision
2020-412	- The presented approach is significant, as modelling long-range dependencies is an important milestone in sequence modelling.	strength
2020-412	- The new benchmark is a good addition.	strength
2020-412	- The comparison with the relevant literature is thorough and well done.	strength
2020-412	- The experiments are convincing and demonstrate the viability of the approach, although some aspects can be improved (see below).	strength
2020-412	Detailed comments: - About the character-level language modelling on Enwik8, the improvement is very small, it seems that the task doesn't benefit from have long-range memory, could it be because character-level modelling is less dependent on the long-range past?	weakness
2020-412	can the authors comment on that?	suggestion
2020-412	It would also been interesting to evaluate the gain of the memory, for instance by varying the size of the compressed memory from 0 to 1152.	suggestion
2020-412	- The WikiText-103 evaluation is interesting, specially Table 6, which shows the advantages of the model.	strength
2020-412	However when comparing with the literature, it's not clear if the performance gain is due to the compressed memory or to the network capacity.	weakness
2020-412	A study with different lengths of the compressed memory (starting at 0) would bring some insights about that.	suggestion
2020-412	- In Section 5.6.2: can the authors justify why the attention weights were split in only 6 bins?	suggestion
2020-412	creating a trended curve on only 6 points could be problematic, and I don't see why more bins couldn't be used.	suggestion
2020-412	- The speech analysis section (5.7) is not very insightful.	weakness
2020-412	It shows that the proposed model is on par with WaveNet on unconstrained speech generation, which is not very useful and feels a bit half-finished.	weakness
2020-412	I think that the authors should either commit to this study by constraining the model with linguistic features like in (Oord et al. 2018) and evaluate it in a TTS framework with subjective evaluation or discard this section entirely.	suggestion
2020-412	This paper proposes a way to compress past hidden states for modeling long sequences.	abstract
2020-412	Attention is used to query the compressed representation.	abstract
2020-412	The authors introduce several methods for compression such as convolution, pooling etc.	abstract
2020-412	The outcome is a versatile model that enables long-range sequence modeling, achieving strong results on not only language model tasks but also RL and speech.	abstract
2020-412	For testing and evaluating the modeling of really long context sequence modeling, the authors introduce PG-19, a new benchmark based on Project Gutenberg narratives.	abstract
2020-412	The idea is a simple and straightforward one.	strength
2020-412	The choices of compression functions are intuitive and natural.	strength
2020-412	The probably more interesting part of this paper is the training schemes designed to train the memory compression network.	strength
2020-412	Results are very strong and there is a pretty diverse set of experiments.	strength
2020-412	That said,  it seems like a huge amount of resources were spent on this work alone.	weakness
2020-412	It also seems like these models are not trivial to train (or get them to work).	weakness
2020-412	It would be interesting to find out how much resources were spent (in terms of preliminary experiments) to getting these models to start working decently.	suggestion
2020-412	There are also no reports of parameter counts, which might make the experiments unfair.	weakness
2020-412	Achieving SOTA is one thing, which could be attributed to large resource pools and maybe larger parameter sizes of models.	weakness
2020-412	Overall, I am voting for a weak accept.	decision
2020-412	While this paper is more incremental and novelty may be slightly lacking, I think the breadth of experiments and competitive results warrants an acceptance.	decision
2020-412	Several issues and questions for the authors: 1) Why are the results on PG-19 not reported in a Table format?	weakness
2020-412	Why are there no results of the base Transformer on PG-19?	weakness
2020-412	I think this is really necessary and should be reported.	weakness
2020-412	2) The authors mention that this memory compression architecture enables long sequence modeling.	weakness
2020-412	However, is there an intended way of use for long-text that is not necessarily framed as a LM problem?	weakness
2020-412	For instance, results on NarrativeQA benchmark would be nice.	suggestion
2020-412	UPDATE: I have read the author response and other reviewer's comments.	misc
2020-412	I am happy with the efforts made by the authors and I am raising my score to 8 (accept).	rebuttal_process

2020-452	Summary: This paper is about developing VAEs in non-Euclidean spaces.	abstract
2020-452	Fairly recently, ML researchers have developed non-Euclidean embeddings, initially in hyperbolic space (constant negative curvature), and then in product spaces that have varying curvatures.	abstract
2020-452	These ideas were developed for embeddings, and recent attempts have been made to build entire models that operate in non-Euclidean spaces.	abstract
2020-452	The authors develop VAEs for the product spaces case.	abstract
2020-452	There's largely two aspects here: one is to be able to write down the equivalents for the operations in models (e.g., the equivalent of adding or multiplying matrices and vectors in Euclidean space have to be lifted to other spaces which no longer have a linear structure).	abstract
2020-452	The other are VAE-specific choices, particularly choosing a normal distribution on the manifolds.	abstract
2020-452	The authors consider several of these choices and then run a variety of experiments on small latent-dimension cases for VAEs. These reveal that sometimes non-Euclidean and in particular product spaces improve performance.	abstract
2020-452	Strengths, Weakness, Recommendation I like what the authors are trying to do here; embeddings and discriminative models on non-Euclidean spaces have been developed, offer credible benefits, and generative models are the next step.	strength
2020-452	The authors push forward the machinery needed to do this, and the results seem like there's something there.	strength
2020-452	On the other hand, the entire work seems quite preliminary.	weakness
2020-452	It's hard to say what the takeaway is, or any suggestions for users.	weakness
2020-452	The paper is written in a pretty frustrating way.	weakness
2020-452	There's an enormous amount of stuff in a sprawling appendix (there are 43 results in the first appendix?!), and checking all of these details will take a great deal of time.	weakness
2020-452	Overall, I recommended weak accept, since a lot of these issues seem like they can be cleaned up.	decision
2020-452	EDIT: I increased my score based on the authors' response.	rebuttal_process
2020-452	Comments: - The approach taken here is quite similar to another *CONF* submission this year, which basically does the same thing but applies these operations to GCNs instead of VAEs.	weakness
2020-452	- A better way to define curvature is just to talk about the sectional curvature, instead of the Gaussian curvature the authors mention at the beginning of section 2.	weakness
2020-452	Fortunately for the constant case all of these definitions will be the same.	weakness
2020-452	- It's not quite clear in Section 2.1 why we should care about the fact that you can't fully take K->0 there---why does this hurt anything?	weakness
2020-452	You can approximate flat curvature arbitrarily well even without K exactly 0.	weakness
2020-452	- On a similar theme, what's the point of doing the product of {E,S,D,H,P}, instead of just {E,S,H} or {E,D,P}?	weakness
2020-452	Seems a bit weird to consider all 5, given the equivalence between S-D and H-P.	weakness
2020-452	- In 2.3, the products of spaces section, the distance decomposition in the 2nd paragraph should have squares (it's an l2): d_M(x,y)^2 = \\sum_{i=1}^k d_{M_k_i^n_i)^2(x^i,y^i).	weakness
2020-452	- The discussion in 2.3 should be expanded and made more concrete (some of these you can write out the expressions for), and more pros and cons explained, e.g., which theoretical properties are lost for the wrapped distributions?	weakness
2020-452	- On page 6, I don't understand the first problem with the learnable curvature approach.	weakness
2020-452	Why is there no gradient w.r.t to K?	weakness
2020-452	Isn't the idea that you'll write this thing as a piecewise function (presumably it's continuous, since that's why the authors built those models that deform to flat), and differentiate the whole thing?	weakness
2020-452	Why wouldn't there be a gradient at ELBO(K)?	weakness
2020-452	Is it not differentiable at K=0?	weakness
2020-452	That doesn't follow directly from just saying the curvature is 0.	weakness
2020-452	-  What's the intuition for the component learning algorithm using 2 dimensions for each of the spaces?	weakness
2020-452	- The experiment section was written in a way where I couldn't understand why the choices being made were there.	weakness
2020-452	Why 6 and 12 dimensions here?	weakness
2020-452	More clarity here would be great.	weakness
2020-452	Also, are there any other models to compare against for these datasets?	weakness
2020-452	I'm not a VAE expert; what do other models typically obtain in the authors' regime?	weakness
2020-452	Summary: This paper devised a framework towards modeling probability distributions in products of spaces with constant curvature and showed how to generalize the VAE to learn latent representations on such product spaces using Gaussian-like priors generalized for this case.	abstract
2020-452	Empirically the authors evaluate the VAEs on four different datasets (a synthetic tree dataset, binarized MNIST, Omniglot, and CIFAR-10) for various choices of product spaces (fixed curvature and learnable curvature) and choices of latent space dimensionality.	abstract
2020-452	Evaluation: Overall this seems to be a nice work, with balanced discussion of the empirical results, and is clearly written.	strength
2020-452	--Past works have considered VAEs on single constant curvature spaces and hence it is well-motivated to consider a more flexible model that enables usage of products of such spaces.	strength
2020-452	--Empirical evaluations seems fair as far as I can tell, but I am not familiar with benchmarks for VAEs. It was interesting to see the variability in best performing models, e.g. cases in which the mixed curvature models did well vs.	strength
2020-452	the Euclidean one. --Paper is quite readable, though in a few parts seems to delve a bit unnecessarily into geometric formalism/definitions (e.g. I did not really follow or appreciate the relevance of gyrovector distances).	weakness
2020-452	--Main text is 10 pages long and I'm not sure the extra length is necessary.	weakness
2020-452	--I would have appreciated a more clearly delineated discussion on how the technical details of this work overlap with past papers, both those that have investigated product spaces (Gu et al 2019) and single curvature spaces in VAEs (spherical & hyperbolic)?	weakness
2020-452	How did the latter approaches deal with modified prior distributions and/or smoothly recovering the Euclidean K=0 limit?	weakness
2020-452	As a result, I'm a bit unsure as to the novelty or technical obstacles that are overcome in the proposed framework in comparison to these.	weakness
2020-452	This paper introduces a general formulation of the notion of a VAE with a latent space composed by a curved manifold.	abstract
2020-452	It follows the current trend of learning representations on curved spaces by proposing a formulation of the latent distributions of the VAE in a variety of fixed-curvature spaces, and introduces an approach to learn the curvature of the space itself.	abstract
2020-452	Extensive mathematical derivations are provided, as well as experiments illustrating the impact of various choices of latent manifolds on the performance of the VAE.	abstract
2020-452	I believe this work should be accepted, as while the numerical results are not particularly impressive, it provides some clear foundational work for further exploration of the use of non-euclidean latent spaces in VAEs.	decision
2020-452	This paper provides extensive and detailed theoretical grounding for their work, ensuring that it is a well-founded extension the VAE formalism.	strength
2020-452	It explores numerous alternatives and compares them, providing detailed experimental results on 4 datasets.	strength
2020-452	The appendices provided a much welcome refreshing on non-euclidean geometry, as well as more details & experimental results.	strength
2020-452	The paper is already quite dense, especially with the appendices, however there are a few points that could still be detailed in my opinion: First of all, what were the observation models used for the reconstruction loss in the experiments?	weakness
2020-452	I suspect a bernouilli likelhood was used for the binarized dataset, but what about the other ones, and notably CIFAR?	weakness
2020-452	Was it a Gaussian observation, a discretized logistic, ...?	weakness
2020-452	Was its variance learned? This kind of information is in my opinion crucial for assessing a construction to the latent space of VAE model, as it can have a lot of influence on the kind of information the model will try to store in its latent space.	weakness
2020-452	Secondly, for the model using product of spaces, do you observe some preference of the VAE to store more information in some of the sub-component?	weakness
2020-452	This can be explored by comparing the values of the KL term in each of these subspaces.	weakness
2020-452	Third, the VAE with a factorized Gaussian euclidean latent space has a well-known tendency to sparcify its latent representations: unneeded dimensions of the latent space are ignored by the decoder and set to the prior by the encoder.	weakness
2020-452	This allows one to not worry too much about the size of the latent space as long as it is "large enough".	weakness
2020-452	Does this property remain in curved spaces?	weakness
2020-452	Especially in the case the VAE on MNIST with a 72-dimensional latent, as I suspect the 6 and 12 dimensional spaces are not "large enough" for this phenomenon to appear.	weakness

2020-518	After reading all the reviews and the comments, I feel more positive about the paper.	decision
2020-518	I appreciate the feedback of the Authors and I have decided to increase the rating.	rebuttal_process
2020-518	============================ The paper proposes using Robust Subspace Recovery in combination with an autoencoder (and possibly GANs) for anomaly detection.	abstract
2020-518	The encoder maps input data to the latent space of dimensionality D, which then is linearly projected to a subspace of dimensionality d (d < D).	abstract
2020-518	The projection of the latent space then goes to a decoder that reconstructs the input.	abstract
2020-518	A transformation matrix A is trained jointly with the autoencoder.	abstract
2020-518	Two additional terms are added to the loss: one to encourage the subspace of A^TA to approximate the latent space z and the second one to force it to be an orthogonal projector.	abstract
2020-518	The paper claims to generalize the existing RSR framework to the nonlinear case.	abstract
2020-518	However, the linear RSR is applied to the latent space of the autoencoder.	abstract
2020-518	In addition to that, all the following discussion and proofs are limited to the linear case.	weakness
2020-518	Since the proposed method is using RSB as it's core part, and claims to be a non-linear extension of it, it would be crucial to have a comparison with RSB, at least on those experimental setups, where high-level features are used (Tiny Imagenet with ResNET features, Reuters-21578, and 20 Newsgroups).	suggestion
2020-518	However, there is no such comparison.	weakness
2020-518	Since autoencoders can potentially learn any, arbitrary entangled latent space, it is not clear why outliers should necessarily have such embedding that is outside of the learned subspace.	weakness
2020-518	In the case of the original RSR it happens due to the dimensionality reduction by the orthogonal projector.	weakness
2020-518	However, autoencoders already perform dimensionality reduction at each layer down to the bottleneck layer.	weakness
2020-518	The matrix A and the parameters of the AE are trained jointly.	weakness
2020-518	So, it can be seen that two processes can occur: - The AE in order to minimize the reconstruction error would learn such latent space z, that would fit into the subspace of A^TA, so that projection \\tilde z =Az doesn't cause data loss.	weakness
2020-518	- The AE in order to minimize the reconstruction error would learn such A, so that the subspace that z approximates is the best possible.	weakness
2020-518	It is not clear, which of the two cases would take place.	weakness
2020-518	If the first one would dominate, then it is not clear if such method would have any discriminating capabilities.	weakness
2020-518	My point is mainly that the presented work is not really a generalization of RSR as it claims to be, but rather it is just using RSR on a leaned embedding of the data.	weakness
2020-518	Some citations are missing, as well as it is missing a comparison to some state-of-the art methods such as OCNN 'Robust, Deep and Inductive Anomaly Detection' ECML 2017;	weakness
2020-518	'Adversarially Learned One-Class Classifier for Novelty Detection' CVPR 2017;	misc
2020-518	DSVDD 'Deep one-class classification.' ICML, 2018;	misc
2020-518	ODIN  'Enhancing The Reliability  Of Out-of-distribution Image Detection  In Neural Networks' *CONF* 2018;	misc
2020-518	'Generative Probabilistic Novelty Detection with Adversarial Autoencoders' NeurIPS 2018.	misc
2020-518	This paper adapts the concept of Robust Subspace Recovery (RSR) as a layer in an auto-encoder model for anomaly detection.	abstract
2020-518	A loss function is proposed that combines reconstruction error and a regularizer that enforces robustness against outliers.	abstract
2020-518	The reconstruction error expresses the accuracy of the nonlinear dimensionality reduction imposed by the autoencoder.	abstract
2020-518	The regularizer is the sum of absolute deviations from the latent subspace that represents a linear structure robust against outliers.	abstract
2020-518	An alternative procedure is applied where the loss terms are applied iteratively during training.	abstract
2020-518	Once trained, the reconstruction error is used directly for anomaly detection with a threshold.	abstract
2020-518	The AUC is used as a performance measure.	abstract
2020-518	The method is compared against 6 other methods (LOF, OCSVM, IF, DESBM, GT, DAGMM).	abstract
2020-518	The setting is fully unsupervised, meaning that the training data contains various amounts of anomalies, and the results are parametrized with the amount of corruption.	abstract
2020-518	The results show that the proposed approach outperforms the other methods in most cases, especially for larger amounts of corruption.	abstract
2020-518	An ablation study compares the approach with auto-encoder-only and a non-alternating gradient descent (fixed factors for each part of the loss function) and shows that the alternating method outperfroms all by a wide margin.	abstract
2020-518	PROS: * A novel approach to fully unsupervised anomaly detection that beats the state of the art.	strength
2020-518	* The RSR layer is a simple fully connected layer and the loss function is simple to calculate, making the approach computationally efficient.	strength
2020-518	* A pseudo-code algorithm is provided in the appendix, which should help reproducibility.	strength
2020-518	* The paper is well written and the math is clearly laid out.	strength
2020-518	* The result benchmarks are sufficiently exhaustive in both the methods that are compared and the datasets used.	strength
2020-518	* The ablation study is informative and shows the effect of the regularization term of the loss function as well as the effect of alternating the gradient descent with the separate losses.	strength
2020-518	CONS: * There is a serious problem in the results (Figure 1) as the AP curves show better scores for larger corruption factors.	weakness
2020-518	Are the AP-score graphs flipped ?	weakness
2020-518	Please explain. * The AUC and AP scores need to be defined.	weakness
2020-518	* The results should include the case where the training data is not contaminated with outliers (c=0).	weakness
2020-518	This would correspond to the semi-supervised scenario and it would be very interesting to see how the method compares to DAGMM and GT which are build for that scenario.	weakness
2020-518	* It would be interesting to see the effect of varying the subspace dimension.	suggestion
2020-518	The authors chose 10 for all experiments, why is this number chosen, what would be the effect of choosing a smaller one ?	suggestion
2020-518	This is a key parameter as it defines the structure of the projection subspace.	abstract
2020-518	Should this parameter be systematically tuned for each dataset ?	suggestion
2020-518	Overall this is a good paper proposing a novel approach to fully unsupervised anomaly detection with state-of-the art results.	strength
2020-518	This paper proposes to use the robust subspace recovery layer (RSR) in the autoencoder model for unsupervised anomaly detection.	abstract
2020-518	This paper is well written overall.	strength
2020-518	Presentation is clear and it is easy to follow.	strength
2020-518	The proposed approach is a simple combination of existing approaches.	strength
2020-518	Although its theoretical analysis with respect to the performance of anomaly detection is limited, experiments show that the proposed method is effective and superior to the existing anomaly detection methods.	strength
2020-518	I have the following comments: - Parameter sensitivity should be examined.	weakness
2020-518	The proposed method has the number of parameters including \\lambda_1, \\lambda_2, and parameters in neural networks.	weakness
2020-518	Since parameter tuning is fundamentally difficult in the unsupervised setting, the sensitivity of the proposed method with respect to changes of such parameters should be examined.	weakness
2020-518	- Since the efficiency is also an important issue for anomaly detection methods, runtime comparison would be interesting.	weakness
2020-518	- It would be also interesting whether the proposed method is also effective for non-structured data, where a dataset is given as just a set of (real-valued) feature vectors, with its comparison to the standard anomaly detection methods such as LOF and iForest.	weakness

2020-622	This paper introduces a structured drop-in replacement for linear layers in a neural network, referred to as Kaleidoscope matrices.	abstract
2020-622	The class of such matrices are proven to be highly expressive and includes a very general class of sparse matrices, including convolution, Fastfood, and permutation matrices.	abstract
2020-622	Experiments are carried in a variety of settings: (i) can nearly replace a series of hand-designed feature extractor,	abstract
2020-622	(ii) can perform better than fixed permutation matrices (though parameter count also increased by 10%),	abstract
2020-622	(iii) can learn permutations, and	abstract
2020-622	(iv) can help reduce parameter count and increase inference speed with a small performance degradation of 1.0 BLEU on machine translation.	abstract
2020-622	This appears to be a solid contribution in terms of both theory and practical use.	strength
2020-622	As I have not thought much about expressiveness in terms of arithmetic circuits (though I was unable to fully follow or appreciate the derivations, the explanations all seem reasonable), my main comments are regarding experiments.	weakness
2020-622	Though there are experiments in different domains, each could benefit from some additional ablations, especially to existing parameterizations of structured matrices such as Fastfood, ACDC, and any of the multiple works on permutation matrices and/or orthogonal matrices.	suggestion
2020-622	Though Kaleidoscope include these as special cases, it is not clear whether when given the same resources (either memory or computational cost), Kaleidoscope would outperform them.	weakness
2020-622	There is also a matter of ease of training compared to existing approximations or relaxations, e.g. Gumbel-Sinkhorn.	weakness
2020-622	Pros: - The writing is easy to follow and concise, with contributions and place in the literature clearly stated.	strength
2020-622	- The Kaleidoscope matrix seem generally applicable, both proven theoretically and shown empirically (experiments are spread across a wide range of domains).	strength
2020-622	- The code includes specific C++ and CUDA kernels for computing K matrices, which will be very useful for adaptation.	strength
2020-622	- The reasoning using arithmetic circuits seems interesting, and the Appendix includes a primer.	strength
2020-622	Cons: - For the squeezenet and latent permutation experiments, would be nice if there is a comparison to other parameterizations of permutation matrices, e.g. gumbel-sinkhorn.	weakness
2020-622	- For the speed processing experiment, did you test what the performance would be if K matrix is replaced by a fully connected layer?	weakness
2020-622	This comparison appears in other experiments, but seems to be missing here for some reason.	weakness
2020-622	It would lead to better understanding than only comparing to SincNet.	weakness
2020-622	- The setup for the learning to permute experiment is not as general as it would imply in the main text.	weakness
2020-622	The matrices are constrained so that an actual permutation matrix is always sampled, and the permutation is (had to be?) pretrained to reduce total variation for 100 epochs before jointly trained with the classifier.	weakness
2020-622	Though this is stated very clearly in the Appendix, I hope the authors can also communicate this clearly in the main text as it appears to be a crucial component of the experimental setup.	suggestion
2020-622	Comments: - How easy is it to train with K matrices?	suggestion
2020-622	Did you have to change optimizer hyperparameter compared to existing baselines?	suggestion
2020-622	- There seems to be some blurring between the meaning of structure (used to motivate K matrices in the introduction) and sparsity (used to analyze K matrices).	weakness
2020-622	Structure might also include parameter sharing, orthogonality, and maybe other concepts.	weakness
2020-622	For instance, while Kaleidoscope matrices might include the subclass of circulant matrices, can they also capture the same properties or "inductive bias" (for lack of better word) as convolutional layers when trained?	weakness
2020-622	The authors propose learnable "kaleidoscope matrices" (K-matrices) in place of manually engineered structured and sparse matrices.	abstract
2020-622	By capturing "all" structured matrices in a way that can be learned, and without imposing a specific structure or sparsity pattern, these K-matrices can improve on existing systems by	strength
2020-622	* capturing more structure (that was not handled by the existing manually engineered architecture),	strength
2020-622	* running faster than dense implementations.	strength
2020-622	The claim that "all" structured matrices can be represented efficiently is a strong one, and in section 2.3 the authors make it clear what they mean by this.	strength
2020-622	Although the proof is long and beyond the expertise of this reviewer, the basic explanation given in section 2.3 makes their point clear for the non-expert reader.	strength
2020-622	The balance of the paper empirically tests the claims of learnable structure and efficiency.	strength
2020-622	On the basis that these experiments essentially bear out the claims of the paper, I selected to accept the paper.	decision
2020-622	Weaknesses: 1. Regarding the ISWLT translation task result: With this dataset, it's a bit of a stretch to say there was "only a 1 point drop in BLEU score".	weakness
2020-622	That's a significant drop, and in fact the DynamicConv paper goes to significant lengths to make a smaller 0.8 point improvement.	weakness
2020-622	There are probably many other ways to trade BLEU score for efficiency, and without showing those other methods (and the point drops they have), it's not clear that K-matrices are a good way to speed up decoding a bit.	weakness
2020-622	Summary The authors introduce kaleidoscope matrices (K-matrices) and propose to use them as a substitute for structured matrices arising in ML applications (e.g. circulant matrix used for the convolution operation).	abstract
2020-622	The authors prove that K-matrices are expressive enough to capture any structured matrix with near-optimal space and matvec time complexity.	abstract
2020-622	The authors demonstrate that learnable K-matrices achieve similar metrics compared to hand-crafted features on speech processing and computer vision tasks, can learn from permuted images, achieve performance close to a CNN trained on unpermuted images and demonstrate the improvement of inference speed of a transformer-based architecture for a machine translation task.	abstract
2020-622	Review The overall quality of the paper is high.	strength
2020-622	The main contribution of the paper is the introduction of a family of matrices called kaleidoscope matrices (or K-matrices) which can be represented as a product of block-diagonal matrices of a special structure.	strength
2020-622	Because of the special structure, the family allows near-optimal time matvec operations with near-optimal space complexity for structured matrices which are commonly used in deep architectures.	strength
2020-622	The proposed approach is novel.	strength
2020-622	It gives a new characterization of sparse matrices with optimal space complexity up to a logarithmic term.	strength
2020-622	Moreover, the proposed characterization is able to learn any structured matrix and matvec time complexity of the K-matrix representation is near-optimal matvec time complexity of the structured matrix.	strength
2020-622	Even though in the worst-case complexity is not optimal, the authors argue that for matrices that are commonly used in machine learning architectures (e.g. circulant matrix in a convolution layer) the characterization is optimal.	strength
2020-622	This results in a new differentiable layer based on a K-matrix that can be trained with the rest of an architecture using standard stochastic gradient methods.	strength
2020-622	However, it is worth noting that the reviewer is not an expert in the field, and it is hard for him to compare the proposed approach with previous work.	weakness
2020-622	The paper is generally easy to follow.	strength
2020-622	Even though the introduction of K-matrices requires a lot of definitions, they are presented clearly and Figure 1 helps to understand the concept of K-matrices.	strength
2020-622	The experimental pipeline is also clear.	strength
2020-622	Given the special structure of the family, the reviewer might guess that having K-matrices can slow down the training, i.e. it might require more epochs to achieve the reported results compared to baselines.	weakness
2020-622	Providing training plots might increase the quality of the paper.	suggestion
2020-622	The experimental results are convincing.	strength
2020-622	First, the authors show that K-matrices can be used instead of a handcrafted MFSC featurization in an LSTM-based architecture on the TIMIT speech recognition benchmark with only a 0.4% loss of phoneme error rate.	strength
2020-622	Then, the authors evaluate K-matrices on ImageNet dataset.	strength
2020-622	In order to do so, they compare a lightweight ShuffleNet architecture which uses a handcrafted permutation layer to the same architecture but with a learnable K-matrix instead of the permutation layer.	strength
2020-622	The authors demonstrate the 5% improvement of accuracy over the ShuffleNet with 0.46M parameters with only 0.05M additional parameters of the K-matrix and the 1.2% improvement of accuracy over the ShuffleNet with 2.5M parameters with only 0.2M additional parameters of the K-matrix.	strength
2020-622	Next, the authors show that K-matrices can be used to train permutations in image classification domains.	abstract
2020-622	In order to demonstrate so, they take the Permuted CIFAR-10 dataset and ResNet-18 architecture, insert a trainable K-matrix at the beginning of the architecture and compare against ResNet-18 with an inserted FC-layer (attempting to learn the permutation as well) and ResNet-18 trained on the original, unpermuted CIFAR-10 dataset.	abstract
2020-622	With K-matrix, the authors achieve a 7.9% accuracy improvement over FC+ResNet-18 and only a 2.4% accuracy drop compared to ResNet-18 trained on the original CIFAR-10.	abstract
2020-622	Finally, the authors demonstrate that K-matrices can be used instead of the decoder's linear layers in a Transformer-based architecture on the IWSLT-14 German-English translation benchmark which allows obtaining 30% speedup of the inference using a model with 25% fewer parameters with 1.0 drop of BLEU score.	abstract
2020-622	Overall, the analysis and the empirical evaluations suggest that K-matrices can be a practical tool in modern deep architectures with a variety of potential benefits and tradeoffs between a number of parameters, inference speed and accuracy, and ability to learn complex structures (e.g. permutations).	strength
2020-622	Improvements 1. Even though K-matrices are aimed at structured matrices, it would be curious either to empirically compare K-matrices to linear transformations in fully-connected networks (i.e. dense matrices) or to provide some theoretical analysis.	suggestion
2020-622	2. Section 3.3 argues that K-matrices allow to obtain an improvement of inference speed, however, providing the results of convergence speed (e.g. training plots with a number of epochs) will allow a better understanding of the proposed approach and will improve the quality of the paper.	suggestion

2020-640	The paper introduces CATER: a synthetically generated dataset for video understanding tasks.	abstract
2020-640	The dataset is an extension of CLEVR using simple motions of primitive 3D objects to produce videos of primitive actions (e.g. pick and place a cube), compositional actions (e.g. "cone is rotated during the sliding of the sphere"), and finally a 3D object localization tasks (i.e. where is the "snitch" object at the end of the video).	abstract
2020-640	The construction of the dataset focuses on demonstrating that compositional action classification and long-term temporal reasoning for action understanding and localization in videos are largely unsolved problems, and that frame aggregation-based methods on real video data in prior work datasets, have found relative success not because the tasks are easy but because of dataset bias issues.	abstract
2020-640	A variety of models from recent work are evaluated on the three proposed tasks, demonstrating the validity of the above motivation for the construction of the dataset.	abstract
2020-640	The primitive action classification task is "solved" by nearly all methods and only serves for debugging purposes.	abstract
2020-640	The compositional action classification task is harder and shows that incorporating LSTMs for temporal reasoning leads to non-trivial performance improvements over frame averaging.	abstract
2020-640	Finally, the localization task is challenging, especially when camera motion is introduced, with much space for improvement left for future work.	abstract
2020-640	I am positive with respect to acceptance of this paper.	decision
2020-640	It is a well-argued, thoughtful dataset contribution that sets up a reasonable video understanding dataset.	strength
2020-640	The authors recognize that since the dataset is synthetically generated it is not necessarily predictive of how methods would perform with real-world data, but still it can serve a useful and complementary role similar to the one CLEVR has served in image understanding.	strength
2020-640	I have a few minor comments / questions / editing notes that would be good to address: - The random baseline isn't described in the main text, it would be good to briefly mention it (this will also help to clarify why the value is particularly high for tasks 1 and 2)	weakness
2020-640	- The grid resolution ablation results presented in the supplement are actually quite important -- they demonstrate that with a small increase in granularity of the grid the traditional tracking methods begin to be the best performers.	strength
2020-640	As this direction (of increased resolution to make the problem less artificial) is likely to be important, a brief discussion of this finding from the main paper text would be appropriate	suggestion
2020-640	- p3 resiliance -> resilience	strength
2020-640	- p4 objects is moved -> object is moved	strength
2020-640	- p6 actions itself -> actions themselves; builds upon -> build upon	strength
2020-640	- p7 looses all -> loses all; suited our -> suited to our; render's camera parameters -> render camera parameters; to solve it -> to solve the problem	weakness
2020-640	- p8 (Xiong, b;a) and (Xiong, b) -> these references are missing the year; models needs to -> models need to	weakness
2020-640	- p9 phenomenon -> phenomena; the the videos -> the videos; these observation -> these observations; of next -> of the next; in real world -> in the real world This paper introduces a new synthetic video understanding dataset, borrowing many ideas from the visual question answering dataset CLEVR.	weakness
2020-640	The new dataset is the first to account for all of the following fundamental aspect of videos: temporal ordering, short- and long term reasoning, and control for scene biases.	weakness
2020-640	Due to the inherent biases in available action recognition datasets, models that simply averages video frames do nearly as well as models that take temporal dependencies into account.	weakness
2020-640	In contrast, the authors show that with the proposed dataset, models without spatiotemporal reasoning largely fail.	weakness
2020-640	The paper should be accepted as it addresses a major shortcoming of all existing video understanding datasets.	decision
2020-640	It does a good job at summarizing the deficiencies in existing datasets, clearly motivating the need for a new dataset.	strength
2020-640	The claims are backed up with solid experiments, ablating models and data parameters adequately.	strength
2020-640	It is mostly well-written (except for section 4 which would benefit from extensive proofreading) and does a good job at covering relevant work.	strength
2020-640	One drawback is of course the synthetic nature and limited domain of objects and actions.	weakness
2020-640	On the other hand, this makes the setup highly controllable and reliable.	weakness
2020-640	I like the fact that each task comes both with both static and moving camera.	strength
2020-640	Improvements and Questions: Some relevant datasets are missing.	weakness
2020-640	For example, the Moving MNIST and Robot Pushing datasets could be added to Table 1.	suggestion
2020-640	I suggest having a train / validation / test split (like CLEVR), rather than just a train and validation split.	suggestion
2020-640	In particular for Task 3 more frames seem to give dramatic improvement.	suggestion
2020-640	Why did you not run with more than 64 frames?	weakness
2020-640	Did you consider downsampling the videos to allow running on all the frames?	suggestion
2020-640	I'm missing details on the resolution of the generated videos?	weakness
2020-640	This paper proposed a new synthetic dataset (CATER) for video understanding.	abstract
2020-640	The authors argue that since current video datasets are heavily biased over static scenes and object structures, it is unclear whether modern spatial-temporal video models can learn to reason over temporal dimension.	abstract
2020-640	In order to address this problem, they design this fully observable synthetic dataset which is built upon CLEVER, along with three tasks that are customized for temporal understanding.	abstract
2020-640	They further conduct a variety of experiments to benchmark state-of-the-art video understanding models and show how those models more or less struggle on temporal reasoning.	abstract
2020-640	Overall this paper is well-written and easy to follow.	strength
2020-640	The problem is well-motivated, and the claims are mostly supported.	strength
2020-640	The diagnosis in this paper provides useful insights that could be contributive to both vision and learning communities.	strength
2020-640	My primary concern is to what extent can the new dataset (CATER) add to existing video datasets that are also explicitly designed for long term spatial-temporal reasoning, such as video VQA datasets TGIF-QA[1]/SVQA[2].	weakness
2020-640	In addition to the comparison between CATER and three action recognition datasets (Kinetics/UCF101/HMDB51) as presented in Table 3., it would be more interesting to see how video understanding models that are specifically designed for those video VQA datasets will perform on CATER.	suggestion
2020-640	[1] Yunseok Jang, Yale Song, Youngjae Yu, Youngjin Kim, and Gunhee Kim. Tgif-qa: Toward spatio-temporal reasoning in visual question answering.	misc
2020-640	In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2758–2766, 2017.	misc
2020-640	[2] Xiaomeng Song, Yucheng Shi, Xin Chen, and Yahong Han. Explore multi-step reasoning in video question answering.	misc
2020-640	In 2018 ACM Multimedia Conference on Multimedia Conference, pages 239–247.	misc
2020-640	ACM, 2018.	misc

2020-670	The authors proposed an end-to-end method (E2Efold) to predict RNA secondary structure.	abstract
2020-670	The method consists of a Deep Score Network and a Post-Process Network (PPN).	abstract
2020-670	The two networks are trained jointly.	abstract
2020-670	The score network is a deep learning model with transformer and convolution layers, and the post-process network is solving a constrained optimization problem with an T-step unrolled algorithm.	abstract
2020-670	Experimental results demonstrate that the proposed approach outperforms other RNA secondary structure estimation approaches.	abstract
2020-670	Overall I found the paper interesting.	strength
2020-670	Although the writing can be improved and some important details are missing.	weakness
2020-670	Major comments As the authors point out, several existing approaches for unrolling optimization problems have been proposed.	rebuttal_process
2020-670	It would be helpful to clarify the methodological novelty of the proposed algorithm compared to those.	suggestion
2020-670	Training details and implementation details are missing; these hinder the reproducibility of the proposed approach.	suggestion
2020-670	The author stated pre-training of the score network, how is the PPN and score network updated during the joint training?	weakness
2020-670	Does the model always converge?	weakness
2020-670	The authors vaguely mentioned add additional logistic regression loss to Eq9 for regularization.	weakness
2020-670	What is a typical number of T?	weakness
2020-670	How does varying T affect the performance, both in terms of training time (and convergence) and in terms of accuracy/F1?	weakness
2020-670	Minor comments The 29.7% improvement of F1 score overstates the improvements compared to non-learning approaches..	weakness
2020-670	This performance was computed on the dataset (RNAStralign) on which E2Efold was trained.	weakness
2020-670	A fair comparison, as the authors also stated, is on the independent ArchiveII data.	weakness
2020-670	On this data, E2Efold has F1 score 0.686 versus 0.638 for CONTRAfold.	weakness
2020-670	The author should report performance improvement under this line.	weakness
2020-670	It would be helpful to report performance per RNA category, both for RNAstralign data and ArchiveII data, while the ArchiveII data should still remain independent.	suggestion
2020-670	Different models may have their strengths and weaknesses on different RNA types.	misc
2020-670	It is not obvious to me how the proximal gradient was derived to (3)-(5).	weakness
2020-670	It would be helpful if the authors show some details in the supplements.	weakness
2020-670	Why is there a need to introduce an l_1 penalty term to make A sparse?	weakness
2020-670	On which data is Table 6?	weakness
2020-670	Typos, etc. The references are not consistently formatted	weakness
2020-670	"structure a result" -> "structure is a result"	weakness
2020-670	"a few hundred." -> "a few hundred base pairs."	weakness
2020-670	"objective measure the" -> "objective measures the"	weakness
2020-670	"section 5" -> "Section 5" (in several places)	weakness
2020-670	In the equation above Equation 2, should it be -\\rho||\\hat{A}||_{1} instead of plus?	weakness
2020-670	Otherwise, the "max" could be made arbitrarily large.	weakness
2020-670	This paper introduces an end-to-end method to predict the secondary structure of RNA, by mapping the nucleotide sequence to a binary affinity matrix.	abstract
2020-670	The authors decompose this problem into two part: (i) predicting an affinity score between each base pair in the input sequence, using a combination of a transformer sequence encoder network and a convolutional decoder, and (ii) a post-processing step that ensures that structural local and global constraints are enforced.	abstract
2020-670	An innovation is to express this post-processing as an unrolled sequence of proximal gradient descent steps, which are fully differentiable, and allow the full combination of (i)+(ii) to be trained end-to-end.	abstract
2020-670	A thorough set of experiments validate the approach.	abstract
2020-670	Overall, the paper is well written and easy to follow.	strength
2020-670	The approach of unrolling structural constraints as shown in the paper is interesting and applicable to much wider domains than secondary structure prediction.	strength
2020-670	The proposed approach appears to provide a novel, convincing and non-obvious solution to RNA secondary structure prediction, and subject to suggestions below, would represent a valuable contribution to *CONF*.	strength
2020-670	The principal area for improvement would be to include additional detail (perhaps in appendix) on the model hyperparameter configurations that were used in the experiments.	suggestion
2020-670	Moreover, more details on the set of \\psi functions, and the MLP details for P_i (e.g. number of hidden units, activation function, the use of dropout, batch normalization, etc) should be given, as well as more information on the specifics how how the "pairwise concatenation" is carried out in the output layer.	suggestion
2020-670	What unrolling constant T is used?	suggestion
2020-670	Finally, in the ablation study (p.	suggestion
2020-670	8) details on how U_\\theta is trained by itself (without the post-processing step) should be given.	weakness
2020-670	Detailed comments: * Overall, the whole paper should be thoroughly reviewed for English grammar and writing style; a subset of suggested changes follow.	weakness
2020-670	* p. 1: structure a result ==> structure is a result	weakness
2020-670	* p. 2: energy based methods ==> energy-based methods	misc
2020-670	* p. 2: energy function based approaches ==> energy function-based approaches	misc
2020-670	* p. 2: view point ==> viewpoint	misc
2020-670	* p. 2: E2Efold is flexible ==> E2Efold are flexible	strength
2020-670	* p. 2: nearly efficient ==> nearly efficiently	strength
2020-670	* p. 3: typically scale ==> typically scale as	weakness
2020-670	* p. 3: few hundred. ==> few hundreds. * p.	weakness
2020-670	4: all binary matrix ==> all binary matrices	weakness
2020-670	* p. 4: output space can help ==> output space could help	weakness
2020-670	* p. 5: formulation are the ==> formulation are that the	weakness
2020-670	* p. 6: eq. (7) should contain quantities indexed by t in the RHS	weakness
2020-670	* p. 8: pesudoknotted ==> pseudoknotted * p.	weakness
2020-670	9 ff: in the bibliography, all lowercase rna should be uppercase RNA.	weakness
2020-670	Use {RNA} in bibtex entries.	suggestion
2020-670	*Summary* The authors perform RNA secondary prediction using deep learning.	abstract
2020-670	The outputs are subject to hard constraints on which nucleotides can be in contact with others.	abstract
2020-670	They unroll a sophisticated optimization algorithm for a relaxation of the task of finding the optimal contact map subject to these constraints.	abstract
2020-670	This work is in a long line of work demonstrating that end-to-end training of models that incorporate application-specific optimization routines as sub-modules is very useful.	strength
2020-670	In particular, it outperforms an approach where the inputs to this optimization problem come from a network that was trained using a simple loss that ignores the fact that it will feed into this structured optimizer.	strength
2020-670	The paper also considers an application domain that will be unfamiliar to many *CONF* readers interested in deep structured prediction, and may serve as a call to arms for the community engaging with additional problems in this field.	strength
2020-670	*Overall Assessment* The paper is well written, well executed, and part of a general research thread that *CONF* readers care about.	strength
2020-670	There are a number of technical details, such as the loss function in (8) that will be of general interest.	strength
2020-670	I advocate for acceptance. *Comments* The actual specification of the output constraints doesn't occur until late in the paper.	decision
2020-670	Before then, the discussion of them is very abstract.	rating_summary
2020-670	Given that the constraints are easy to describe, the exposition would be improved notably if you described the specific constraints earlier on.	suggestion
2020-670	This would help me understand the problem domain better.	suggestion
2020-670	Fyi, the idea of nested structures vs.	suggestion
2020-670	non-nested structures appears in NLP in terms of projective parsing vs.	suggestion
2020-670	non-projective parsing. There may be some relevant reading for you to do there.	suggestion
2020-670	Your specific work (minus the unrolled constraint enforcement) is similar to "Dozat et al. 2017.	weakness
2020-670	Deep biaffine attention for neural dependency parsing."	misc
2020-670	The idea of backpropping through some constraint-enforcing process is reminiscent of backpropping through belief propagation.	weakness
2020-670	See, for example, Domke's "Learning Graphical Model Parameters with Approximate Marginals Inference." Or Hershey et al. "Deep Unfolding: Model-Based Inspiration of Novel Deep Architectures." You should also cite work using unrolled ISTA to learn sparse coding dictionaries.	suggestion
2020-670	They have terms similar to (5).	suggestion
2020-670	What exactly was your motivation for the setup in "Test On ArchiveII Without Re-training?"	rebuttal_process
2020-670	How sensitive is performance to the number of optimizer iterations?	suggestion
2020-670	Does it work to train with a fixed number of unrolled iters, but at test time run the optimizer until convergence?	suggestion
2020-670	(8) is cool! RNA Secondary Structure Prediction by Learning Unrolled Algorithms	misc
2020-670	This paper proposes E2Efold, which is an RNA secondary structure prediction algorithm based on an unrolled algorithm.	abstract
2020-670	Previous methods rely on dynamic programming (which does not work for molecular configurations that do not factorize) or rely on energy-based models (which require a minimization step, e.g. by using MCMC to traverse the energy landscape and find minima).	abstract
2020-670	The former does not work for all molecules and the latter can be difficult to optimize.	abstract
2020-670	The method presented here is novel, shows strong SOTA performance, and would be of interest to the wider deep learning community.	strength
2020-670	The method is based on an unrolled algorithm, which is motivated by the inclusion of three inductive biases / constraints important underlying RNA folding.	strength
2020-670	These constraints limit the wide RNA search space.	strength
2020-670	The first component of the method is a "Deep Score Network" which uses a stack of Transformer encoders (with relative and exact positional embeddings) followed by 2D convolutional layers to output a L x L symmetric matrix describing the "scores" of base pairing.	abstract
2020-670	As these scores may not obey the rules of RNA folding, a second post-processing network is trained end-to-end together with the "Deep Score Network" to enforce constraints.	abstract
2020-670	This network starts with a transformation that symmetrizes the matrix and applies a constraint-enforcing mask.	abstract
2020-670	The problem is transformed into an unconstrained problem by using Lagrange multipliers; it is then solved using a proximal gradient.	abstract
2020-670	Finally, a recurrent cell is defined that implements this algorithm in a deep learning framework.	abstract
2020-670	This method is creative, could be applied to other tasks with constraints, and would be interesting to the wider deep learning community.	strength
2020-670	In addition to developing the deep score network and post-processing network, the authors also develop a differentiable F1 loss, so that the network can directly optimize for precision and recall on the task.	abstract
2020-670	The performance of this method significantly outperforms previous methods.	strength
2020-670	There was a fruitful discussion on OpenReview regarding whether this was a result of overfitting on the task.	rebuttal_process
2020-670	Indeed, it is critical in deep learning applications to carefully construct train/test sets to avoid high performance by memorization alone.	rebuttal_process
2020-670	To address this, the authors train on RNAStralign and test on ArchiveII.	rebuttal_process
2020-670	As the original ArchiveII dataset contains subsequences of other RNA sequences, which can result in overfitting, the authors re-ran their experiment with that removed, and similar results were achieved.	rebuttal_process
2020-670	To support the hypothesis that ArchiveII and RNAStralign capture different distributions, they perform a permutation test on the unbiased empirical Maximum Mean Discrepancy estimator, finding that the distributions are different.	rebuttal_process
2020-670	I do wonder why they did not check if P(ArchiveII) = P(ArchiveII) as they do check if P(RNAStr_train) = P(RNAStr_train).	rebuttal_process
2020-670	On the specific task of pseudoknot prediction, the method also performs well (F1 is >0.23 over the baseline).	rebuttal_process
2020-670	On sequence length-weighted F1, the model does even better.	rebuttal_process
2020-670	The paper is rich with ablations.	strength
2020-670	The analysis of the number of unrolling iterations T helps support the use of an unrolled method and builds intuition for its importance - it would be useful to include this in the appendix of the paper.	strength
2020-670	I also appreciated the visualizations, which are a good sanity check that the model correctly handles pseudoknots.	strength
2020-670	The performance of the method is broken down by RNA family, which is also quite interesting -- the method outperforms LinearFold on all classes, besides 5S RNA, SRP, and Group I intron.	strength
2020-670	Further analysis is required to better understand why the method is weaker on those datasets.	weakness
2020-670	Additionally, further work should explore training on one set of families and testing on a held-out set of families.	suggestion
2020-670	This was pointed out by public comments on this paper.	misc
2020-670	This is potentially a limitation of E2EFold (the authors do not seem to have tried this suggested experiment) and further exploration is required.	weakness
2020-670	Exploring this limitation (even if it is not overcome) would make this paper even more rich.	suggestion
2020-670	That said, I recommend acceptance of this paper due to the extensive experiments, polished writing, novel method, and strong results, which can inspire future research.	decision

2020-677	Summary --- (motivation) Lots of methods produce attribution maps (heat maps, saliency maps, visual explantions) that aim to highlight input regions with respect to a given CNN.	abstract
2020-677	These methods produce scores that highlight regions that are in a vague sense "important."	abstract
2020-677	While that's useful (relative importance is interesting), the scores don't mean anything by themselves.	weakness
2020-677	This paper introduces another new attribution method that measures the amount of information (in bits!) each input region contains, calibrating this score by providing a reference point at 0 bits.	abstract
2020-677	Non-highlighted regions contribute 0 bits of information to the task, so they are clearly irrelevant in the common sense that they have 0 mutual information with the correct output.	abstract
2020-677	(approach - attribution methods) An information bottleneck is introduced by replacing a layer's (e.g., conv2) output X with a noisy version Z of that output.	abstract
2020-677	In particular, Z is a convex combination of the feature map (e.g., conv2) with Gaussian noise with the same mean and variance as that feature map.	abstract
2020-677	The weights of the combination are found so they minimize the information shared between the input and Z and maxmimize information shared between Z and the task output Y.	abstract
2020-677	These weights are either optimized on	abstract
2020-677	1) a per-image basis (Per-Sample) or	abstract
2020-677	2) predicted by a model trained on the entire dataset (Readout).	abstract
2020-677	(approach - evaluation) The paper uses 3 metrics with differing degrees of novelty: 1) The bbox metric rewards attribution methods that put a lot of mass in ground truth bounding boxes.	abstract
2020-677	2) The original Sensitivity-n metric from (Ancona et al. 2017) is reported with a version that uses 8x8 occlusions.	abstract
2020-677	3) Least relevant image degredation is compared to most relevant image degredation (e.g., from (Ancona et al. 2017)) to form a new occlusion style metric.	abstract
2020-677	(experiments) Experiments consider many of the most popular baselines, including Occlusion, Gradients, SmoothGrad, Integrated Gradients, GuidedBP, LRP, Grad-CAM, and Pattern Attribution.	abstract
2020-677	They show: 1) Qualitatively, the visualizations highlight only regions that seem relevant.	abstract
2020-677	2) Both Per-Sample and Readout approaches put higher confidence into ground truth bounding boxes than all other baselines.	abstract
2020-677	3) Both Per-Sample and Readout approaches outperform all baselines almost all the time according to the new image degredation metric.	abstract
2020-677	Strengths --- The idea makes a lot of sense.	strength
2020-677	I think heat maps are often thought of in terms of the colloquial sense of information, so it makes sense to formalize that intuition.	strength
2020-677	The related work section is very well done.	strength
2020-677	The first paragraph is particularly good because it gives not just a fairly comprehensive view of attribution methods, but also because it efficiently describes how they all work.	strength
2020-677	The results show that proposed approaches clearly outperform many strong baselines across different metrics most of the time.	strength
2020-677	Weaknesses --- * I'm not sure why the new degredation metric is a useful addition.	weakness
2020-677	What does it add that MoRF and LeRF don't capture on their own independently?	weakness
2020-677	* I think [1] would be a nice addition to the evaluation section as it tests for something qualitatively different than the various metrics from section 4.	suggestion
2020-677	It would also be a good addition to the related work.	suggestion
2020-677	Missing Details / Points of Confusion	weakness
2020-677	--- * I think there's an extra p(x) in eq.	weakness
2020-677	11 in appendix D. * I think the variable X is overloaded.	weakness
2020-677	In eq. 1 it refers to the input (e.g., the pixels of an image) while in eq.	weakness
2020-677	2 it refers to an intermediate feature map (e.g., conv2) even though it later seems to refer to the input again (e.g., eq.	weakness
2020-677	3). Different notation should be used for intermediate feature maps and inputs.	weakness
2020-677	Presentation Weaknesses --- * In section 3.1 is lambda meant to be constrained in the range [0, 1]?	weakness
2020-677	This is only mentioned later (section 3.2) and should probably be mentioned when lambda is introduced.	weakness
2020-677	* "indicating that all negative evidence was removed." I think this should read "indicating that only negative evidence was removed."	weakness
2020-677	Suggestions --- "The bottleneck is inserted into an early layer to ensure that the information in the network is still local"	suggestion
2020-677	I'd like this to be explored a bit more.	weakness
2020-677	Though deeper feature maps are certainly more spatially coarse they still might be somewhat "local".	weakness
2020-677	To what degree to they loose localization information?	weakness
2020-677	My equally vague alternative intuition goes a bit differently: The amount of relevant information flowing through any spatial location seems like it shouldn't change that much, only the way its represented should change.	weakness
2020-677	If the proposed visualizations were the same for every choice of layer then it would confirm this intuition.	weakness
2020-677	That would also be an interesting result because most if not all of the cited baseline approaches (where applicable) produce qualitatively different attributions at different layers (e.g., see Grad-CAM).	weakness
2020-677	[1]: Adebayo, Julius et al. "Sanity Checks for Saliency Maps." NeurIPS (2018).	misc
2020-677	Preliminary Evaluation --- Clarity: The paper is clearly written.	strength
2020-677	Originality: The idea of using the formal notion of information in attribution maps is novel, as is the bbox metric.	strength
2020-677	Significance: This method could be quite significant.	strength
2020-677	I can see it becoming an important method to compare to.	strength
2020-677	Quality: The idea is sound and the evaluation is strong.	strength
2020-677	This is a very nice paper in all the ways listed above and it should be accepted!	decision
2020-677	Post-rebuttal comments --- The author responses and other reviews have only increased my confidence that this paper should be accepted.	decision
2020-677	Summary The paper proposes a novel perturbation-based method for computing attribution/saliency maps for deep neural network based image classifiers.	abstract
2020-677	In contrast to most previous work on perturbation-based attribution, the paper proposes to inject carefully crafted noise into an early layer of the network.	abstract
2020-677	Importantly, the noise is chosen such that it optimizes an information-theoretically motivated objective (rate-distortion/info bottleneck) that ensures that decision-relevant signal is flowing while constraining the overall channel-capacity, such that decision-irrelevant signal is blocked from flowing.	abstract
2020-677	The flow of signal is controlled by the amount of noise injected, which translates into a certain amount of mutual information between input image regions and noisy activations/features.	abstract
2020-677	This mutual information can be visualized in the input image, but it also has a clear, quantitative meaning that is readily interpretable.	abstract
2020-677	The paper introduces two ways to construct the injected noise, based on the information bottleneck.	abstract
2020-677	Resulting attribution maps are computed and evaluated on VGG-16 and ResNet-50 (on ImageNet), and are compared against an impressive number of previously proposed attribution methods.	abstract
2020-677	Importantly, the paper uses three different quantitative measures to compare the quality of attribution maps.	abstract
2020-677	The proposed method performs well on all three measures.	strength
2020-677	Contributions i) Derivation of a novel method for constructing attribution maps.	strength
2020-677	Importantly, the method is grounded on solid theoretical footing for extracting minimal relevant information (rate-distortion theory / information bottleneck method).	strength
2020-677	ii) Proposal of a novel quantitative measure to compare quality of pixel-level attribution maps in image classification, and extension of a previously reported method.	strength
2020-677	iii) Evaluation and comparison against a large body of state-of-the-art attribution methods.	strength
2020-677	Quality, Clarity, Novelty, Impact The paper is clear and well written, with a nice introduction to the information bottleneck method.	strength
2020-677	Experiments are well described and hyper-parameter settings are given in the appendix.	strength
2020-677	To the best of my knowledge, the proposed method is sufficiently novel and the application of the information bottleneck framework to pixel-level attribution has not been reported before.	strength
2020-677	Some of the design- and implementation-choices needed to render the intractable info bottleneck objective tractable could perhaps be discussed and potentially even improved in light of recent results in other fields (Bayesian DL, deep latent-variable generative models, and variational methods for deep neural network compression), but I currently don't consider this a major issue.	ac_disagreement
2020-677	To me personally the work in convincing and mature enough to vote for acceptance	decision
2020-677	- perhaps most importantly it lays important groundwork for important connections to the theory of relevant information and puts a lot of much needed emphasis on objective evaluation of attribution methods (i.e. without subjective visual judgement of saliency maps).	strength
2020-677	My suggestions below are aimed at helping improve the paper even further.	misc
2020-677	Improvements I) A short section of current shortcomings/limitations could be added to the discussion.	rebuttal_process
2020-677	II) Perturbation-based approaches that inject noise (into the input image directly) have been proposed previously.	suggestion
2020-677	Most notably: Visualizing and Understanding Atari Agents, Greydanus et al. 2018 and potentially follow-up citations.	weakness
2020-677	It would be interesting to compare both works empirically, but perhaps also theoretically/conceptually.	suggestion
2020-677	Could the Greydanus work be related to applying the noise directly to the input image along with some additional constraints?	suggestion
2020-677	Minor Comments a) Is there a particular reason for this choice of colormap?	weakness
2020-677	While it seems to be roughly perceptually uniform (which is of course good), why not choose a simple sequential colormap (instead of a rainbow-like one)?	suggestion
2020-677	At least the use of red and green at the same time should rather be avoided to maximize colormap readability under the most common forms of color vision deficiencies.	suggestion
2020-677	b) Just a pointer - no need to act on this for the current paper.	suggestion
2020-677	Large parts of the field of neural network compression are concerned with a similar kind of attribution - the question is which weights/neurons/filters are relevant and which ones are not and can thus be removed from the network without loss in accuracy.	suggestion
2020-677	Information-bottleneck style objectives (or the closely related ELBO / variational free energy) in conjunction with sparsity inducing priors have been proven to be quite fruitful.	suggestion
2020-677	See e.g. Variational Dropout Sparsifies Deep Neural Networks, Molchanov et al. 2017 for interesting work, that aims at learning the variance of Gaussian noise that is injected into neural network weights using a similar construction and variational objective as shown in this paper.	abstract
2020-677	Perhaps some ideas can be borrowed/translated for future, improved versions of the method from that body of literature (Molchanov 2017, but also more sophisticated follow-up work).	suggestion
2020-677	This paper presents an information-bottleneck-based approach to infer the regions/pixels that are most relevant to the output.	abstract
2020-677	For all the metrics listed in the paper, the proposed approaches all achieve very good performance.	strength
2020-677	It turns out, the proposed two architectures are better (at least alternative) choices to the other existing attribution methods.	strength
2020-677	I do agree that the proposed two models (Per-Sample and Readout) can be used to roughly infer regions of interest, which has been strongly supported by the comprehensive experiments.	strength
2020-677	To minimize equation (6), we need to make beta*L_I small.	strength
2020-677	Minimizing L_{CE} in (6) tries to maximize the mutual information between Z and output (labels); while minimizing L_I with respect to weight beta would try to inject noise to each dimension of Z.	strength
2020-677	However, L_{CE} needs to ensure it can get enough information for prediction, and thus would prevent the noise injection process for "the key regions".	abstract
2020-677	By choosing reasonable beta (similar to variational information bottleneck), the proposed approaches are capable to highlight key regions used for prediction.	abstract
2020-677	Overall, I think the method is elegant for approximately estimating the relevance score map.	strength
2020-677	Below are some of my (minor) questions/concerns: 1. What we learned = What we want?	weakness
2020-677	The proposed approach seeks a sort of "sparse heatmap".	weakness
2020-677	The larger the beta, the more regions/pixels would be suppressed while smaller beta might fail to suppress non-important regions in the image.	weakness
2020-677	In the paper, the beta used for calculating the per-sample bottleneck is among [100/k , 10/k, 1/k].	weakness
2020-677	The beta for ReadOut bottleneck is 10/k.	weakness
2020-677	However, according to Table 1, only when beta is smaller than 1/k, the accuracy of the model does not degrade too much.	weakness
2020-677	When using beta=10/k to get the "heat map" (where 10/k is the best choice of per-smaple bottleneck for degradation task), how close is the "heat map in beta=10/k" to the "ground-truth heatmap"?	weakness
2020-677	To better understand the proposed methods, I have a small suggestion: ------ Try betas in a broader range including very small betas, e.g. [0.0001/k, 0.001/k,....,1/k,10/k], for both Table one and visualization.	suggestion
2020-677	Fix a few images and visualize the heatmap given different betas.	suggestion
2020-677	We might better see how the growth of beta changes the heatmap.	suggestion
2020-677	2. About zero-valued attributions. I agree with you that equation (5) is an upper bound of MI (eq (4)).	weakness
2020-677	However, I am not sure if I totally agree with the claim "If L_1 is zero for an area, we can guarantee that no information from this area is used for prediction."	weakness
2020-677	----- Given L_1=0 really implies that no information of the corresponding region is used for the certain beta, but is this true for the original model (beta=0)?	weakness
2020-677	Table one shows that different beta would lead to very different downstream task accuracy.	suggestion
2020-677	3. Specific to the two approaches you proposed, can you explain/motivate in what situations per-sample bottle would be better and in what cases we should prefer ReadOut bottleneck?	suggestion

2021-1	The paper presents an empirical evaluation of many algorithmic choices made in the implementations of on-policy actor-critic algorithms in deep reinforcement learning (RL).	abstract
2021-1	The authors group those choices in clusters in which they expect some interactions.	abstract
2021-1	For each cluster, they test sets of randomly made choices while assuming that choices outside a cluster are set to competitive default values.	abstract
2021-1	Based on those experimental results, the authors formulate recommendations about how to make those choices for each cluster.	abstract
2021-1	PROS The paper is well-written and clear.	strength
2021-1	This paper is part of the string of recent papers that discuss the difficulty of evaluating deep RL algorithms.	abstract
2021-1	I appreciate the breadth of the choices that the authors consider.	misc
2021-1	The justification for their overall experimental design (i.e., evaluating per choice clusters) is reasonable.	strength
2021-1	While some findings are as expected, others are indeed unexpected and not discussed in the deep RL literature.	weakness
2021-1	CONS I have a doubt about the robustness of the results.	weakness
2021-1	The authors decided to use the median over 3 seeds for the evaluation.	weakness
2021-1	Although the median is used, is it reliable given the observations made by Henderson et al., which implies that performance can vary a lot with respect to seeds?	weakness
2021-1	Could the authors comment on that point?	weakness
2021-1	I think one important missing experiment is the evaluation of the combinations of all the recommendations made in the paper.	weakness
2021-1	Do the recommendations depend on the default setting for other choices, do they have a synergetic effect or could there be some negative interactions?	weakness
2021-1	This paper carries out a large-scale study for understanding of on-policy deep actor-critic.	abstract
2021-1	The study looks into a large choices of many implementation settings and design decisions, and investigate their impact on the task performance.	abstract
2021-1	The evaluations are done with 250000 RL agents on 5 different continuous control tasks.	abstract
2021-1	For each evaluation category, there is a finding summary that provides practical recommendations.	abstract
2021-1	In overall, this study is exhaustive and helpful to both RL researchers and practitioners.	strength
2021-1	The experiment organization which separates all design choices into 7 main categories is very excellent in a systematic way.	strength
2021-1	They cover most design choices in recent works of on-policy RL methods.	strength
2021-1	The reports and the interpretation of results are very interesting and easy to read.	strength
2021-1	The main and important findings are summarized concisely and expected to play important hints.	strength
2021-1	The only performance metric studied in the paper is a score that is proportional to the area under the learning curve.	strength
2021-1	I was wondering if there should be an additional metric, i.e the final policy or an average reward of the final 100 policies?	weakness
2021-1	Would the final or best policy be of more interest to the choice of a practitioner?	weakness
2021-1	As many recent work investigates the design choice of only on-policy RL methods, it would be interesting if in introduction there is discussion on why off-policy methods are not considered or should it be addressed in a different way in another research?	suggestion
2021-1	Beside the focus on only the performance in terms of rewards, it would be interesting if the discussion can be expanded to look at other matters, e.g. numerical stability of design/hyper-parameter choices, convergence behaviors (it might requires plot to see if a method show premature convergence, fast learning but sub-optimal, fluctuating, etc.).	suggestion
2021-1	Although the paper only uses Mujoco simulator, would the hyperparameters' domains be subjective to it, e.g. inertia, fiction, joint limits, contacts, etc.?	suggestion
2021-1	It would be helpful if the discussion can show if such those factors play any role in the results?	suggestion
2021-1	It would lead to more helpful finding summary.	suggestion
2021-1	As a final comment, this is a solid work and will be very helpful to the community.	strength
2021-1	Given that it is implemented on the new SEED RL framework, so it would be better if the implementation code can be published.	strength
2021-1	The authors survey a wide variety of implementation-level and hyperparameter decisions in reinforcement learning for continuous control tasks.	abstract
2021-1	They train over 250.000 agents with different settings and suggest empirical guidelines.	abstract
2021-1	As the authors indicate, there's not too much related work so one could call this work pioneering: The sort of work conducted by the authors is crucially important for a field afloat with tricks and tweaks, many of which are typically not discussed in the scientific literature due to a misplaced conceit around this being "not research, just engineering" entirely absent from established fields of science such as experimental Physics.	weakness
2021-1	It is also typically not done as it is just plain hard to do.	weakness
2021-1	Combined with the often lackluster response from the community, the cost-benefits trade-off has just not been worth it, especially for junior researchers.	weakness
2021-1	It's all the more commendable that the authors have engaged with this formidable task of bringing some of the "secret sauce" out of the heads of senior engineers in the various labs and into published and peer-reviewed science.	strength
2021-1	The authors compare various choices of configurations obtained from the Cartesian product of 8 factors which they call thematic groups: Policy Losses (Sec.	strength
2021-1	3.1), Networks architecture (Sec. 3.2), Normalization and clipping (Sec.	strength
2021-1	3.3), Advantage Estimation (Sec. 3.4), Training setup (Sec. 3.5), Timesteps handling (Sec. 3.6), Optimizers (Sec. 3.7), and Regularization (Sec. 3.8).	strength
2021-1	The high-variance nature of training RL agents makes it such that the individual factors in these configurations often have surprising non-linear cross-relations such that the problem space cannot be evaluated incrementally (i.e., it's often not possible to establish "the best" architecture first and select the right learning rate afterwards).	strength
2021-1	The authors propose a novel approach of considering for each choice the distribution of values among the top 5% configurations trained in that experiment.	abstract
2021-1	Their experimental design is such that the values for each choice are distributed uniformly at random and thus if certain values are over-represented in the top models this indicates that the specific choice is important in guaranteeing good performance.	abstract
2021-1	As for improvements on the paper, I have one major and only a few minor comments.	misc
2021-1	My major comment is that the paper does not indicate anywhere that the research code is released, only that it's based on SEED RL.	weakness
2021-1	I believe an authoritative public implementation of the configurations considered would be extremely worthwhile, both for the community and the authors.	suggestion
2021-1	If they haven't already done so (there's no supplements to this submission and I refrained from doing any web searches to preserve anonymity), I'd urge the authors to invest the time to release a (possibly cleaned-up) version of their code.	suggestion
2021-1	As for minor comments, I'm not clear about the philosophical distinction of something being "due to the algorithms or due to their implementations" (in the Introduction).	weakness
2021-1	I very much see the point the authors are making, which is an important one -- what makes RL results work is often "nitty-gritty" details not mentioned in the main part of the relevant publications (and often just barely mentioned in appendices).	weakness
2021-1	However, in the strictest sense, the algorithm very much is the implementation -- that's what produces a given result.	weakness
2021-1	It's worthwhile to keep the distinction between an idea (say, PPO) and a given implementation of that idea (e.g., presumably the authors had to re-implement PPO in TF2 when using SEED RL and couldn't use OpenAI's original implementation).	weakness
2021-1	It's also fine to call the idea "the algorithm", but I'd have preferred to see this distinction more clearly defined.	weakness
2021-1	Somewhat related: The authors are very much correct about what they call "standard modus operandi of algorithms [...] such as PPO", namely iterating between generating experience using the current policy and using the experience to improve the policy.	weakness
2021-1	I'd add that strictly speaking no iteration is necessary, as for instance IMPALA, coming from the A3C line of development, does both asynchronously in parallel, and I suspect so do the authors given their use of SEED RL.	weakness
2021-1	My suggestion would be to slightly rephrase this sentence and mention IMPALA along with PPO.	suggestion
2021-1	Perhaps there could also be a comment somewhere about what constitutes "PPO" (or "IMPALA")	suggestion
2021-1	-- e.g., IMPALA consists of	suggestion
2021-1	(1) an asynchronous actor/learner split [with further choices of when/how the weights are copied from learner to actor, see e.g. this comment],	suggestion
2021-1	(2) a specific type of policy-gradient loss, v-trace,	suggestion
2021-1	(3) a specific neural network architecture, optionally including recurrence via an LSTM and potentially even	suggestion
2021-1	(4) a specific type of preprocessing for environments such as Atari or DMLab [according to some papers, swapping out the implementation of the frame-downscaling algorithm in Atari has measurable impact on final performance	suggestion
2021-1	-- this will matter a lot when evaluating re-implementations of an "algorithm"].	suggestion
2021-1	I'd like for the authors to take on this opportunity and propose a common language to discuss these distinctions, which in practise are often confusing to junior researchers (and some senior researchers, too).	suggestion
2021-1	Further related to IMPALA and v-trace, I was surprised about the word "unsurprisingly" and the explanation in "Perhaps unsurprisingly, PG and V-trace perform worse on all tasks.	weakness
2021-1	This is likely caused by their inability to handle data that become off-policy in one iteration, either due to multiple passes over experience [...] or a large experience buffer in relation to the batch size."	weakness
2021-1	While the results speak for themselves, my understanding of v-trace was that it was specifically designed for the very goal of dealing with the "slight" off-policiness produced by asynchronous actor/learner splits in a PG setting.	weakness
2021-1	Perhaps the authors have an intuition I'm lacking at this point, but if so I'd appreciate further elaboration.	weakness
2021-1	As a final and perhaps trivial comment, I was slightly irritated by the notation/typography for the inverse cumulative density function of a binomial distribution.	weakness
2021-1	In LATEX, the symbols icdf read as the in-context nonsensical i⋅c⋅d⋅f while the authors would presumably want to use icdf (compare exp(x) vs exp⁡(x) or sin(x) vs sin⁡(x)).	weakness
2021-1	I'd propose \\mathrm{icdf}  as the correct syntax for this in LATEX.	suggestion
2021-1	In follow-up work, I'd like to see a similar paper for various "discrete RL" tasks (a subset of Atari, VizDoom, DMLab, MiniGrid, BabyAI, ProcGen, and perhaps even Obstacle Tower, Minecraft, StarCraft (I or II) or the recent NetHack environment) with similar factors of configurations.	suggestion
2021-1	I assume this is a task yet more daunting, but no less useful to the overall community of researchers.	weakness
2021-1	Overall, this is a strong paper and I recommend it for publication.	decision
2021-1	########################################################################## Summary: This paper conducted a large-scale empirical study that provides insights and practical recommendations for the training of on-policy deep actor-critic RL agents	abstract
2021-1	########################################################################## Reasons for score: Overall, I'd vote for acceptance to the paper.	decision
2021-1	The paper is informative and practical; however, I'm not sure that the paper meets *CONF*'s requirement.	decision
2021-1	Pros: 1. Reproducibility is one of the main issues for various RL algorithms.	strength
2021-1	This paper conducts a large-scale empirical study for popular on-policy algorithms.	abstract
2021-1	2. The paper is well-written, and the suggestion is useful to me.	strength
2021-1	Sorry, but I didn't go through all the details in the appendix.	misc

2021-33	The paper demonstrates a mathematical equivalence between Hopfield nets and RBMs, and it shows how this connection can be leveraged for better training of RBMs. What a great paper - well written, an enlightening mathematical connection between two well-known models that to my knowledge was not previously known.	strength
2021-33	Hopfield nets and RBM's have been around for decades, and I don't think we've been aware of this connection, so it seems like a pretty important finding.	strength
2021-33	The paper explores the utility of this connection by applying to an MNIST task.	strength
2021-33	Interestingly, the connection yields important insights in both directions: stochastic sampling in an RBM is faster than Hopfield due to a smaller matrix and parallel layer wise updates, whereas initializing an RBM with the projection rule from Hopfield allows it to find a better solution faster.	strength
2021-33	I really enjoyed reading the paper, I learned something new, and I think others will too!	misc
2021-33	It is an important advance in our understanding of Hopfield nets and RBMs. Summary	strength
2021-33	This paper shows a relationship between the project rule weights of a Hopfield network (HN) and the interaction weights in a corresponding restricted Boltzmann machine (RBM).	abstract
2021-33	The mapping from HN to RBM is facilitated by realising that the partition function of BN can be seen as the partition function of a binary-continuous (Bernoulli-Gaussian) RBM.	abstract
2021-33	The authors comments on the mapping from RBM to BN.	abstract
2021-33	The experiments show the advantages of training RBM with weights initialised from BN projection weights in generation and classification.	abstract
2021-33	Strong points: I am not familiar with the literature, but the results seem new to me.	strength
2021-33	The experiments show advantages of BN initialisation, pointing to new directions of improving RBM training.	strength
2021-33	The paper is fairly clearly written.	strength
2021-33	Weak points The HN -> RBM mapping is quite clear, but the reverse RBM	weakness
2021-33	-> HN mapping is not very well established, and there are no experiments showing how effective the approximate reverse mapping works on associative memory tasks typical for HNs.	weakness
2021-33	I also believe this lowers the impact of this paper, given that the forward mapping is based on a simple revelation.	weakness
2021-33	The authors' description of the experimental results are not accurate enough.	weakness
2021-33	and the results raise several questions to be addressed.	weakness
2021-33	Recommendation I'm in favour of rejection, but some concerns can be addressed fairly easily (with experiments) so I'm open to raising my score if questions are well-addressed.	decision
2021-33	Issues and questions to address	weakness
2021-33	The authors should provide experiments on the reverse mapping as suggested above.	suggestion
2021-33	I do not agree that figure 3 shows that RBM training "simply 'fine tunes'" the weights -- the difference is quite stark.	suggestion
2021-33	How about increasing the batch size so that there is little SGD noise?	suggestion
2021-33	Figure 4a: traces are cut-off just when random initialization is catching up with HN initialization.	weakness
2021-33	This also applies to Figure 5.	suggestion
2021-33	There are a few descriptions suggesting "HN init.	weakness
2021-33	appears to train much faster than random init".	weakness
2021-33	However, the rate of increase in of likelihood in Figure 4 is shallower for HN than for rand init.	weakness
2021-33	Is the advantage only at the 0'th RBM epoch?	weakness
2021-33	The author only compared with purely random initialisation, which is perhaps the most naive baseline.	weakness
2021-33	I would suggest comparing to a (slightly) more clever initialisation, perhaps PCA or something better (those mappings in previous work the authors cited and in Appendix B).	suggestion
2021-33	Or, the authors could also initialise the RBM by first training it on the within-class cluster centres (using a very large number of sleep samples for the sleep-phase) which may also be a more fair comparison?	suggestion
2021-33	In the classification objective, if I understand correctly, the feature function is essentially quadratic in the input patterns.	suggestion
2021-33	Should there be an ideal test error that is computed by a quadratic neural network trained with supervision by backpropagation?	weakness
2021-33	If the HN classifier (blue in Figure 5) is approaching this idealization, then this will strengthen the claim.	suggestion
2021-33	The discussion on the extension to more generic, deep architectures is not well supported, and I do not see the extension to be so straightforward given the content of the current paper.	weakness
2021-33	In generation, supervised labels and clustering are used to simplify learning.	suggestion
2021-33	Is the network able to learn just on the MNIST digits, even for the real images within a single class (e.g. "7")?	weakness
2021-33	Can the authors try to characterise whether the HN initialisation is related to log-likelihood training?	suggestion
2021-33	I wonder if there is any interesting theory; otherwise, measuring model performance by log-likelihood seems a bit arbitrary (though it makes the comparison to contrastive divergence easier).	weakness
2021-33	Detailed suggestions (not to affect decision)	misc
2021-33	Reference to RBMs should include more historic ones from Hinton (e.g. 2006)	suggestion
2021-33	I do not see the purpose of (7) and (8), and they are only referred to in the Appendix (the review content in the Appendix is informative by itself though).	weakness
2021-33	Eqn (11), should it be wμwμT in the sum?	weakness
2021-33	Third line above (16), should H and Z be indexed by μ?	weakness
2021-33	Line above (D.4), WWT=…BpT? ==== update ====	weakness
2021-33	I thank the authors for providing such detailed response.	misc
2021-33	All my concerns are addressed and reflected in the revision (though some are much better done than the rest).	rebuttal_process
2021-33	I congratulate the authors on their spirit of maintaining a high standard on the theory, experiments and descriptions, and therefore significantly raise my score.	rebuttal_process
2021-33	I hope to see this paper accepted.	decision
2021-33	This paper considers a mapping between the well known Hopfield Neural Networks and Restricted Boltzmann Machines.	abstract
2021-33	In contrast with previous literature that consider the case where the patterns / data features to memorize were uncorrelated, the authors extend the mapping to arbitrarily correlated patterns, which allows to consider much more realistic settings.	abstract
2021-33	The mapping is computationally speaking relatively cheap.	abstract
2021-33	This mapping is shown to allow for significantly better initialization (than random) of the weights of a RBM, in the sense that the training is then much faster to reach comparable generative and/or generalization performance.	abstract
2021-33	In this sense the mapping is not only interesting from a theoretical point of view, but also practically.	strength
2021-33	This paper should be considered as an applied one, as there is no real analytic theory of why this mapping helps the learning, but the experiments are well carried: the boost in learning is demonstrated through experiments in MNIST data, and the results are well explained and convincing.	strength
2021-33	The appendices are also well written and are a good addition to the main part.	strength
2021-33	Overall the paper is well written (the paper can be used by non-specialists also as introduction to Hopfield NNs and RBMs), the results are interesting and relevant to the ML community, the paper can be read without much effort.	strength
2021-33	Even if RBM are not anymore state-ot-the art generative models, the results are encouraging and might lead to future improvements in more modern architectures.	strength
2021-33	I have no specific concern.	strength
2021-33	The paper is overall very well written.	strength
2021-33	The paper is slightly incremental as similar mappings were known, but it remains a relevant contribution, and the aspect of using this mapping as a way to boost learning in RBM seems new, and interesting.	strength
2021-33	I recommend publication after slight corrections, see below.	decision
2021-33	Typos and corrections: _text below (1): J=1/N Xi^T Xi^T ->1/N Xi Xi^T	weakness
2021-33	_(3): please detail the last equality	suggestion
2021-33	_(8) is true only for the lambda that verify the fixed point / saddle point equations: please mention it	weakness
2021-33	_below (11): the p the columns of -> the p columns of	ac_disagreement
2021-33	_(12): please explain what is GL_p(R)	suggestion
2021-33	_"At the other end, 0 ≪ 10k/N < 1, ..." : Any x > 0 is >> 0, so please be more precise	weakness
2021-33	_Above Fig 3: "appear qualitatively similar" : this is not obvious...	weakness

2021-36	Pros: This paper nicely unifies two different classes of approaches (NAS + sparsity) for determining the topology of neural networks.	strength
2021-36	They are combined into a single optimization problem, with binary indicators on network components and connections.	strength
2021-36	Experiments illustrate the behavior of the method.	strength
2021-36	It is good to see that the experiments dig a big deeper than end-result accuracy.	strength
2021-36	For instance, the "budget-aware growing" is shown well to work as described by Fig 3.	strength
2021-36	Cons: No attention to random seeding.	weakness
2021-36	The sparsification dynamics seem likely to change somewhat from one run to the next.	weakness
2021-36	The submission does not describe how random seeding was done for training.	weakness
2021-36	Multiple runs with different seeds are not shown, and the distribution of accuracies across runs is unknown.	weakness
2021-36	Attention to randomness for this kind of training process seems especially important given the variances in results in the Lottery Ticket hypothesis paper.	weakness
2021-36	No comparison to simple random baseline.	weakness
2021-36	A large portion of the method consists of a search method over the space of possible sparse networks, combining it with growing the network to get a NAS-like method.	weakness
2021-36	It has been observed, though, that in a sufficiently general space of this kind one can randomly sample connections and see high accuracies.	weakness
2021-36	So to identify the sources of empirical gains, it is good to consider experimental baselines, such as random sampling, that separate the contribution of the search space and the search method: Xie et. al.	weakness
2021-36	"Exploring randomly wired neural networks for image recognition"	misc
2021-36	Li & Talkwaker "Random search and reproducibility for neural architecture search"	misc
2021-36	Yu et. al. "Evaluating the Search Phase of Neural Architecture Search"	misc
2021-36	Radosavovic et. al. "On Network Design Spaces for Visual Recognition."	misc
2021-36	Note that the submission's method also is randomly choosing connections, through a somewhat involved process that also accounts for the observed sparsity of G during training.	weakness
2021-36	The simplest baseline seems to be the "uniform pruning" described in Section 4.4.	weakness
2021-36	This only ablates part of the method that doesn't seem to meet the same criterion here.	weakness
2021-36	Incomplete illustration of the cost/accuracy tradeoff.	strength
2021-36	The gold standard for comparison in both sparse-neural-network papers and NAS is to consider the accuracy at a range of different model costs.	weakness
2021-36	See for example: Blalock et. al.	misc
2021-36	"What is the state of neural network pruning?" Figs 1, 3	misc
2021-36	(Yang et. al., 2018) Figs 5-9	misc
2021-36	This clearly illustrates whether a method is overall better (i.e. produces better models across the entire pareto frontier), or is only better for some ranges or on one metric.	rebuttal_process
2021-36	For a result like the first one in Table 5 in the submission, it is unclear which model is better: they may simply be considering different points on the same cost/accuracy curve.	rebuttal_process
2021-36	As a less important aside, "budget-aware growing" seems to be an ad-hoc reinvention of something similar to an Augmented Lagrangian method.	rebuttal_process
2021-36	Explicitly describing the differences from standard optimization techniques might be good.	suggestion
2021-36	Reasoning for rating: While the experiments are extensive, I think they miss the key comparisons that show how useful the method and each of its components is.	weakness
2021-36	Given that many different innovations are included in the submission, it may be a muddle for follow-up research to sort out how good each individual one is.	weakness
2021-36	Misc comments Check spacing around (6) in Algorithm 1	suggestion
2021-36	Colon instead of comma after "trainable variables" in §4.1	weakness
2021-36	"For better analyze the growing patterns" -> "To better analyze the growing patterns" on page 14	suggestion
2021-36	Wortsman et. al. "Discovering Neural Wirings" is another closely related work at the intersection of NAS and pruning.	weakness
2021-36	(with major differences from the submission)	misc
2021-36	After rebuttal The authors have gone above and beyond in providing additional experimental results.	rebuttal_process
2021-36	All of the points raised above that deal with methodological issues are completely addressed.	rebuttal_process
2021-36	The sole significant weakness that remains is the lack of the kind of ablation/component studies that would justify individual design decisions.	weakness
2021-36	I do not disagree with the authors that this will be difficult for this work, but I still feel they would have been helpful for researchers who will be building upon this method.	ac_disagreement
2021-36	Summary: This paper proposes a NAS-type work for growing a small network to a large network by adding channels and layers gradually.	abstract
2021-36	The authors apply the method to both CNN and LSTM networks.	abstract
2021-36	Strong points: This paper is well-written and shows good results.	strength
2021-36	The proposed algorithm is sound and effective.	strength
2021-36	E.g. use less wall-time as compared to other NAS approaches.	strength
2021-36	Weak points: It seems that the number of channels and number of layers still need to be predefined (the mask size).	weakness
2021-36	Questions: How does the FLOPs reduction translate to runtime saving?	weakness
2021-36	What is the target sparsity u in the experiments?	weakness
2021-36	When growing with layers, does the author observe any middle layer is dropped and then recovered?	weakness
2021-36	If so, does it happen frequently?	weakness
2021-36	At section 4.2 and 4.3, what is the size of the channel/layer mask?	weakness
2021-36	I believe the author still needs to define the upper bound of the network can grow.	weakness
2021-36	If so, does the upper bound affect the optimization?	weakness
2021-36	Or the proposed method gradually expands the mask?	weakness
2021-36	In table 4, I think Efficient-B0 should be taken into consideration as it is a recent representative approach.	weakness
2021-36	After rebuttal: The authors' rebuttal addressed all my questions and I upgrade my rating.	rebuttal_process
2021-36	This paper proposes a novel NAS method that searches the model architectures by grows the networks.	abstract
2021-36	This searching strategy determines the channel and layer configurations by assigning a binary learnable parameter m for each channel or layer.	abstract
2021-36	The objective is to optimize a trade-off between the model performance on the given task and the regularization on the binary indicator m.	abstract
2021-36	Pros: The general idea of searching the architectures by growing the networks sounds very interesting.	strength
2021-36	The authors propose a novel framework to achieve their idea, and also apply some tricks to speed up and simplify the optimization (e.g. budget-aware growing and learning by continuation).	strength
2021-36	The paper is well-written and easy to follow.	strength
2021-36	The authors conduct a series of solid experiments to verify the effectiveness of their proposed methods.	strength
2021-36	The experiments show the performance of channel pruning, the remarkable improvement on AutoGrow model, and the comparsion with other NAS methods.	strength
2021-36	Cons: Compared to ProxylessNet, the proposed model can reduce half of the training time but does harm to the model performance.	weakness
2021-36	Questions: What's the exact meaning of "Top-1 valiadation accuracy"?	weakness
2021-36	What's the different with Top-1 accuracy?	weakness
2021-36	Is this metric evaluated on the valiadation set?	weakness
2021-36	This paper proposes a new principled approach to growing deep network architectures based on continuous relaxation of discrete structure optimization combined with a sparse subnetwork sampling scheme.	abstract
2021-36	It starts from a simple seed architecture and dynamically grows/prunes both the layers and filters during training.	abstract
2021-36	Through extensive experiments, the authors show that this method produces more efficient networks while reducing the computational cost of training, still maintaining good validation accuracy, compared to other NAS or pruning/growing methods.	abstract
2021-36	Strength: (+) The proposed idea of formulating the problem as a continuous relaxation of discrete structure optimization is interesting.	strength
2021-36	It seems to be a more principled approach than previous NAS or separate pruning/growing approaches.	strength
2021-36	(+) Extensive experimental results are provided to verify the superiority over recent other methods and also to show the performance behavior of the proposed method.	strength
2021-36	The overall experimental setup is systematic and comprehensive.	strength
2021-36	The experiments were done on widely used deep networks on various tasks.	strength
2021-36	I only have concerns about the clarity of the notation and the representation of the figures.	weakness
2021-36	Specific examples are as follows: In Eq. (3), it is said that f is the operation in Eq. (1). However, I couldn't find f in Eq. (1).	weakness
2021-36	It should be clarified how many temperature parameters β are in the proposed model.	weakness
2021-36	Only one or as many as channels and layers?	weakness
2021-36	If it is only one, it does not seem reasonable that all the probabilities growing or pruning channels and layers are the same.	weakness
2021-36	Equation (7) seems to imply β to be a vector, but earlier notations (e.g. in Algo 1, Equation 6, etc.) seem to present it as a scalar.	weakness
2021-36	Overall, there are confusing symbols, whether it's a scalar or a vector.	weakness
2021-36	I recommend the channel and layer indicators are denoted as vectors.	suggestion
2021-36	It seems that each channel and each layer has its unique indicator, respectively.	weakness
2021-36	Also, notations should include channel and layer index if they are different depending on channels and layers.	weakness
2021-36	Additionally, all the experimental results shown in the main manuscript are on convolutional neural networks while the abstract mentions recurrent neural networks.	weakness
2021-36	The appendix has some, but very little has the main manuscript.	weakness
2021-36	If it's an important part of this manuscript, the authors should include at least a brief summary of the results.	suggestion
2021-36	Some figures (and the text inside) are too small while containing many details, probably because of the space limit.	weakness
2021-36	For example, Figure 3 has many lines that are hard to analyze and texts that are not readable.	weakness
2021-36	In Table 1, what's the meaning of the underlines?	weakness
2021-36	I guess the second best results, but for RestNet-20, the method with the second-best FLOPs is SoftNet, not Provable.	weakness
2021-36	And the explanation about the boldface and underlines should be included.	weakness

2021-43	This paper generalizes a family of score-based generative models that rely on sequences of noise scalings of the data and extends them to the continuous domain, which leads to an SDE-based framework.	abstract
2021-43	By using score-matching, the forward SDE, which transforms data into a tractable noise distribution, can be reversed and thus used as a generative model.	abstract
2021-43	This is then improved by employing a two-phase algorithm with a prediction step, followed by a tunable number of correction steps.	abstract
2021-43	Further, reformulating the problem as a neural ODE allows for exact likelihood computations and reduces the number of required function evaluations.	abstract
2021-43	The framework enables unconditional, as well as conditional samples.	abstract
2021-43	I find the paper to be very well written and straightforward.	strength
2021-43	As someone who does not have neural SDEs or Langevin Samplers as a core competence, I was able to follow all of the writeup, which is remarkable.	strength
2021-43	I think the framework is nice and there is substantial novel innovation to justify accepting this paper.	decision
2021-43	The experiments are convincing. A few questions and remarks: You claim that you unify current methods into a common framework.	strength
2021-43	While I see that you attempt to do this (i.e. putting the algorithms side-by-side, etc.), but in essence, you still handle VE and VP SDEs separately throughout.	strength
2021-43	My suggestion would be to either really try to unify them into a single formulation, or alternatively, tone down the claims of unification, maybe just say that you show commonalities.	suggestion
2021-43	In Figure 2, you claim that the results are best when computation is "split" between the predictor and corrector.	suggestion
2021-43	However, this is a very imprecise statement.	weakness
2021-43	An equal split would mean just 1 step of corrector, but I don't see clear evidence that that's best.	weakness
2021-43	Do you have numerical evidence that a 1-to-1 split is best, or what do you mean by "split"?	weakness
2021-43	Otherwise, you could just say that M is a tunable hyperparameter.	weakness
2021-43	Also in Figure 2, it seems that there is a clear shift at some point where the samples go from low to high quality.	weakness
2021-43	Do you have any numerical indication (without looking at a test set, FID, etc.) of how a practicioner could notice that running for more steps would or wouldn't help?	weakness
2021-43	In Table 1, what is the last row?	weakness
2021-43	I guess it's just employing the corrector, but maybe a label for the row would be nice.	weakness
2021-43	Also in Table 1, it looks like the corrector helps for VE SDEs, but hurts for VP SDEs. Do you have an explanation for this?	weakness
2021-43	Maybe it's somewhere in the text, but if it is, I've missed it, so maybe point me to it.	weakness
2021-43	Given that you can compute exact likelihoods, is it possible to compute the exact NLL for any real dataset, like a test dataset?	weakness
2021-43	Summary: This paper presents a generative model based on stochastic differential equations (SDEs), which generalizes two other score-based generative models score matching with Langevin dynamics (SMLD) and denoising diffusion probabilistic modeling (DDPM).	abstract
2021-43	The recipe to sample from the data distribution is based on (i) the observation that both SMLD and DDPM can be formulated as the discretization of an SDE, (ii) the finding from Anderson (1982) about the reverse of an Ito process, (iii) a score model.	abstract
2021-43	A novel aspect of the presented technique is the use of the score model as "predictor", which gives the initial sample from the MCMC sampler that serves as "corrector".	abstract
2021-43	Finally, the Ito process induced by the reverse SDE is formulated in a deterministic manner, leading to a neural-ODE based generative model.	abstract
2021-43	Pros: The authors did a good job at showing connections between the previous score-based generative models and their model.	strength
2021-43	I believe on its own this is a nice contribution.	strength
2021-43	The method is thoroughly analyzed.	strength
2021-43	I went through the derivations and didn't find any errors.	strength
2021-43	Experiments show that combining the predictor and corrector routines leads to better performance, a nice validation of the theoretical claims.	strength
2021-43	As promised, the model achieves SOTA on several tasks.	strength
2021-43	Cons: I'm having difficulty seeing the transformation of the reverse SDE into an ODE (from eq.10 to eq.12).	weakness
2021-43	Is it as simple as multiplying the second term with 1/2 and discarding the Brownian motion?	weakness
2021-43	Also, eq.12 is a simple ODE system, which has nothing to with a process as far as I understand.	weakness
2021-43	Maybe more explanation or pointers in Maoutsa et al., 2020 would be nice.	suggestion
2021-43	The paper lacks the discussion on the benefits/downsides of different SDE solvers, discretization time steps, etc.	weakness
2021-43	As such, the paper lacks a "toy example" experiment, for example, on a simple 2D dataset like half-moons.	weakness
2021-43	A visual demonstration of the SDEs and probability flow (maybe corresponding vector fields and Brownian motion over time) would be interesting.	suggestion
2021-43	Additional comments: A typo (intead) right below eq.9	weakness
2021-43	Best performing rows can be bold in Table 1.	weakness
2021-43	Summary and contributions This paper proposes a generalized framework for score-based generative modeling (SBGM).	abstract
2021-43	The proposed method subsumes previous SBGM techniques of score matching with Langevin dynamics (SMLD aka NCSN) and denoising diffusion probabilistic modeling (DDPM) and shows how they correspond to different discretizations of Stochastic Differential Equations (SDEs).	abstract
2021-43	The  continuous-time SDE generalizes the idea of a finite number of perturbation kernels used by previous methods to a continuum of them.	abstract
2021-43	The authors propose a forward SDE that transforms the data distribution into a known noise distribution and the corresponding reverse-time SDE that converts samples from this noise distribution to the data distribution.	abstract
2021-43	A predictor-corrector sampling framework is studied that leads to improved performance of both NCSN and DDPM frameworks.	abstract
2021-43	The paper also shows the equivalence of the proposed SDE to Neural ODEs which allows exact computation of the log-likelihood using the continuous change of variables formula.	abstract
2021-43	Quantitative experiments on the CIFAR10 dataset show that the proposed framework leads to significant improvements over previous SBGMs. Qualitative results on the CelebA-HQ dataset demonstrate the ability of the method to scale to high resolution images.	abstract
2021-43	Strengths This paper makes significant technical and empirical contributions to the emerging area of score-based generative models.	strength
2021-43	The generalized SDE framework subsumes recent works in this area and is also connected to Neural ODEs, enjoying exact likelihood calculation, which may be relevant to the normalizing flows and generative modeling community is general.	strength
2021-43	The empirical evaluation is particularly well-done.	strength
2021-43	It bridges the gap between the performance of NCSN and DDPM models leading to state-of-the-art performance.	strength
2021-43	The authors also demonstrate the ability of the method to generate high quality images of human faces when trained on CelebA-HQ dataset.	strength
2021-43	Preliminary experiments on class conditional generation, imputation, and image colorization demonstrate the wide applicability of the proposed method.	strength
2021-43	Weaknesses The paper does not suffer from any obvious weaknesses.	misc
2021-43	The quantitive experiments could be strengthened by the addition of results on another dataset but the empirical evaluation is sufficient in its current state.	strength
2021-43	Additional feedback Questions: In equation 11, how is the weighting function λ chosen?	misc
2021-43	In equation 11, apart from being able to sample from the transition kernel, it should also have a closed-form density for the evaluation of the score.	weakness
2021-43	Is my understanding correct? In equation 21, should there be a discretization step-size corresponding to Δt?	weakness
2021-43	In table 1 (a), why does SMLD with corrector only perform so poorly?	weakness
2021-43	As far as I understand it is equivalent to NCSN.	weakness
2021-43	Can the authors clarify if I misunderstood something?	suggestion
2021-43	Post Rebuttal: I thank the authors for clarifying on my questions and updating the manuscript.	rebuttal_process
2021-43	SUMMARY The submission proposes a score-based generative model, which uses an SDE to map the data distribution to a simple noise distribution and the corresponding reverse-time SDE to generate samples by mapping the noise to the data space.	abstract
2021-43	The proposed model builds upon and generalises two existing models (SMLD and DDPM) by transforming the data using a continuous SDE dynamics as opposed to perturbing the data with a finite number of noise distributions utilized by these models.	abstract
2021-43	################################################################## REASON FOR SCORE The paper provides a clear motivation for the proposed modifications to SMLD and DDPM,	abstract
2021-43	as well as a clear technical description of these modifications, their analysis and discussion.	abstract
2021-43	I think the proposed model and sampling algorithms offer substantial conceptual improvements to the existing models and should be of interest to the community.	strength
2021-43	The paper is well written and structured.	strength
2021-43	################################################################## PROS Clear motivation for the work.	strength
2021-43	Detailed technical description of the proposed models.	strength
2021-43	Interesting discussion of similarities and differences between SMLD, DDPM, and the SDE	strength
2021-43	based model, as well as corresponding sampling algorithms.	strength
2021-43	Extensive experiments. ################################################################## CONS I found the discussion of equivalent neural ODE and its differences to reverse SDE	weakness
2021-43	a bit short, especially given that it is used in multiple experiments.	weakness
2021-43	The case of using general SDEs (not only those derived from SMLD and DDPM) is mentioned only briefly, leaving it unclear if using a general SDE would require relatively simple changes, or if the proposed model is limited to SDEs derived from	weakness
2021-43	SMLD and DDPM. ################################################################## QUESTIONS and COMMENTS Is it correct that the function \\sigma(t) in Eq. (6) is assumed to be monotonic and bounded by \\sigma_max, while \\beta(t) in Eq.	weakness
2021-43	(8) is bounded by 1, but doesn't have to be monotonic?	weakness
2021-43	In the case of general SDE for noise perturbations in Eq.	weakness
2021-43	(9), are there any assumptions on drift and diffusion function (such as monotonicity or boundedness)?	weakness
2021-43	In the case of SDEs (6) and (8), \\nabla_x p_{0t} (x(t)) is available in closed-form.	weakness
2021-43	Is it always necessary to have such a closed form expression in order to compute the objective (11), or can it be estimated somehow without it?	weakness
2021-43	(I guess for a general SDE, there is typically no closed-form \\nabla_x p_{0t} (x(t)) available)	weakness
2021-43	Why do you think the FID values for the PC sampler with corrector are higher than without it for VP SDE?	weakness
2021-43	(Table 1b) In section 4.2: "[...] PC samplers significantly outperform the corrector-only method, and can improve over predictor-only approaches for most cases without extra computation." Why does full PC sampler (with predictor and corrector) not incur extra computation in comparison to prediction-only approaches.	weakness
2021-43	Don't we need to evaluate the approximate score function s_\\theta(x, i) for each of the M steps in the corrector sampler?	weakness
2021-43	In section 4.3: "[...] deterministic process whose trajectories induce the same evolution of densities".	weakness
2021-43	Does it mean that ODE (12) and reverse SDE (10) map the same noise distribution p(x(T)) to the same distribution in the data space?	weakness
2021-43	If so, are there any advantages of using a reverse SDE instead of equivalent ODE if the latter is easier to solve numerically and admits an exact computation of likelihoods?	weakness

2021-65	Pros: this method proposed to use VAE+EBM for generative modelling.	strength
2021-65	Unlike other VAE+GAN/EBM-liked model, it added a EBM after VAE.	strength
2021-65	Overall method is easy to understand and follow.	strength
2021-65	To accelerate the training, the authors also applied a buffer to store the previous examples for easy sampling.	strength
2021-65	Cons: In the experiment, the authors compared other models with VAEBM, it is reasonable to compare the results with reported scores in other works, however, since the architecture is a fairly important factor (such that swish instead relu, resblock instead of cnn, weight norm instead of spectral norm), etc, is it also reasonable that the improvement is partially contributed by such design of architecture.	weakness
2021-65	So I will suggest that the authors should use the same architecture design (choose other one or two models for all tasks), and test whether the proposed method can actually gain that much of improvement.	suggestion
2021-65	Strengths: The paper provides a thorough overview of recent work towards training EBMs.The approach generates high quality image samples by combining EBMs and VAE based models.	strength
2021-65	The paper is well written and is easy to follow	strength
2021-65	I find it quite interesting that a combination of both models leads to significant overall improved generative performance	strength
2021-65	I also enjoyed the proposed change in the paper -- and it seems to elegantly solve several problem in EBM training.	strength
2021-65	Weaknesses: My most major concern is that since we are utilizing a maximum likelihood objective to train models, it would be good to evaluate  the overall likelihood of the trained model, even if only in the  2D domain.	weakness
2021-65	The histogram of likelihoods of data points is a bit disappointing -- it falls a similar trend of other EBM models, but it would nicer if it followed a Gaussian distribution	weakness
2021-65	What happens when more Langevin sampling steps are applied to the model?	weakness
2021-65	(greater than the few used in training)	weakness
2021-65	I'm also curious on what sampling only the trained energy model looks like (without using the trained VAE parameterization) at evaluation time	weakness
2021-65	I would also be curious to see how the trained EBM, with the VAE generator  can compose together with other models.	suggestion
2021-65	See for example [1]. [1] Yilun Du, Shuang Li, Igor Mordatch.	misc
2021-65	Compositional Visual Generation and Inference with Energy Based Models.	misc
2021-65	NeurIPS 2020 Post Rebuttal-Update I thank the authors for responding to my concerns.	misc
2021-65	I enjoyed reading the paper and maintain my rating.	misc
2021-65	This paper proposes a model that corrects VAE by an energy-based model defined on image space.	abstract
2021-65	The model is learned in two phase.	abstract
2021-65	The first phase learns the VAE model, while the second phase learns the EBM correction term by MLE.	abstract
2021-65	Experimental results show that the proposed method outperforms pure EBM defined on image space and also pure VAE models by large margins.	abstract
2021-65	pros: the paper is clear written and easy to follow.	strength
2021-65	The ablation study shows clearly the advantage over baseline methods.	strength
2021-65	Sampling from EBM on image space is hard.	strength
2021-65	With VAE as a backbone, the sampling can be transferred to the latent space and the residual \\epsilon in the image space, which is much more friendly to MCMC sampling.	strength
2021-65	cons: The energy term is used to correct only on image space.	weakness
2021-65	Would be interesting to see if VAE can be corrected by a latent EBM where the energy function is defined on (x, z).	suggestion
2021-65	After learning, would long-run MCMC sampling chain remain stable and mix well?	suggestion
2021-65	It would be interesting to diagnose the long run chain behavior, and compare the difference of sampling in the space (\\epsilon_x, \\epsilon_z) and (x, z).	suggestion
2021-65	For the synthesized results of CIFAR-10, it seems that some patterns appear repeatedly (e.g., the white dog face).	suggestion
2021-65	Is the model suffered from mode collapsing problem?	suggestion
2021-65	Overall, it is a good submission that proposes a principled method to combine VAE and EBM and demonstrates strong empirical results.	strength
2021-65	I tend to accept this paper.	decision
2021-65	The authors propose a generative model that is a combination (product) of a VAE and an EBM, where the goal of the EBM is to reduce the probability of out-of-manifold samples, which are typically generated by VAEs.	abstract
2021-65	The authors propose efficient training and sampling procedures, in which the VAE is trained first and during the EBM negative-phase, samples are drawn from the joint (x,	abstract
2021-65	z) VAE space using reparameterization.	abstract
2021-65	The method is shown to achieve high quality samples on several modern image datasets, good FID scores and mode coverage.	abstract
2021-65	Ablation studies show the contribution of the different elements.	abstract
2021-65	This is, in my opinion, a very good work, which combines a novel and well-motivated idea with clear writing and extensive experimental evidence.	strength
2021-65	Some comments and questions: Does the separate twos-stage training enable the model to reach the optimal point that can be reached in joint training, or is it an approximation?	weakness
2021-65	If its an approximation, I think it should be discussed or perhaps bounded.	weakness
2021-65	Does the combined model allow computing the likelihood?	weakness
2021-65	Can it be evaluated and compared to other models in terms of bits/dimension (e.g. as in VAE or NVAE)?	weakness
2021-65	It might be interesting (not something that I think is mandatory) to measure the NVAE log-likelihood of samples generated by the combined model compared to samples generated just by the NVAE.	suggestion
2021-65	To summarize: pros: novelty significance experimental evidence quality of writing cons: combining two separately trained models - perhaps sub-optimal	weakness
2021-65	Update: I thanks the authors for their answers and revised version and keep my positive rating.	rebuttal_process

2021-109	Summary of the paper: This paper studies the problem of robustness against word substitutions.	abstract
2021-109	The authors propose a novel Adversarial Sparse Convex Combination (ASCC) method in which they model the word substitution attack space as a convex hull and leverages a regularization term to enforce perturbation towards an actual substitution.	abstract
2021-109	Based on the ASCC, they also propose ASCC-defense, which leverages ASCC to generate worst-case perturbations and incorporates adversarial training towards robustness.	abstract
2021-109	Experimental results show that their method outperforms the existing SOTA on two tasks -- sentiment analysis and natural language inference.	abstract
2021-109	Strength of the paper: The idea proposed in the paper is quite straightforward yet effective.	strength
2021-109	Using a convex hull could satisfy the three aspects as stated by the authors -- Inclusiveness, Exclusiveness, and Optimization.	suggestion
2021-109	The experimental results do show the advantage of using the proposed convex hull.	strength
2021-109	Besides the robustness of the model, it also achieves robustness over word vectors.	strength
2021-109	The experiments are well designed including both qualitative analysis, quantitative results, and reasonable ablation to show the effectiveness of their method.	strength
2021-109	In general, the paper is well-structured and easy to follow.	strength
2021-109	Question for the authors: I am curious about how much headroom there still exists for this task?	weakness
2021-109	Let's say if we have a perfect way to defend against such kind of attacks, how good are the current approaches?	weakness
2021-109	What is the computational cost of the proposed method compared to others?	weakness
2021-109	In equation (3), the authors compute wij through a softmax parameterized by wij^.	weakness
2021-109	If the substitution set of a word is infinite or too large, how are you going to deal with such kind of situations?	weakness
2021-109	Seen from Table 1, the proposed method is much better than other models at PWWS attacks than the genetic attacks.	abstract
2021-109	Can you give an intuitive explanation of why?	suggestion
2021-109	Reason for score: Overall, I vote for accepting this paper.	decision
2021-109	I like the idea of using a convex hull and the way they regularize the model to achieve sparsity.	strength
2021-109	It would be helpful if the authors could address the questions raised above.	suggestion
2021-109	The authors answered my questions so I am increasing my score to 7.	rebuttal_process
2021-109	The paper presents a new defense for being robust to adversarial examples in NLP.	abstract
2021-109	This is a very exciting topic and I am glad to see more work in this space.	strength
2021-109	The paper presents a technique to make the model robust to word substitutions coming from a set S(u).	abstract
2021-109	The authors propose to use a convex hull which the authors claim is a better bound than using l2-ball or axis-aligned rectangles.	abstract
2021-109	The authors derive a optimization objective for their problem and show experimental results that shows their model achieves higher performance under two types of attacks (Genetic and PWWS) on the IMDB and SNLI datasets for various standard neural architectures.	abstract
2021-109	I have a couple of questions: (1) Is this setting different than the one explored in Jia et al. 2019 (Certified Robustness to Adversarial Word Substitutions)?	weakness
2021-109	It seems Jia et al. 2019 explores the case where multiple positions in the sentence can be perturbed whereas here only one word can be perturbed?	weakness
2021-109	I think clarifying this and discussing its implications would be useful for the reader.	suggestion
2021-109	(2) In training does the model have access to the set of all possible substitutions S(u) or not?	suggestion
2021-109	Summary: In this paper, the authors aim to build a robust model against word substitution attacks.	abstract
2021-109	Unlike previous work, they consider a convex hull as the perturbation region instead of a norm-ball or a hyper-rectangle.	abstract
2021-109	From their derivation, perturbed words can be viewed as the linear combinations of substitutions and perturbations can be viewed as the corresponding normalized weights.	abstract
2021-109	Therefore, they can adversarially train the perturbations and model to obtain a robust model and robust word embeddings.	abstract
2021-109	The authors also design a regularizer to encourage the sparsity on perturbation weights.	abstract
2021-109	The experimental results show that the proposed model is indeed more robust than other baselines.	abstract
2021-109	In addition, they show that the learned word embedding can be a good initialization for training robust models.	abstract
2021-109	I very like the idea to convert the perturbations into the linear combination weights.	strength
2021-109	I only have some minor questions: Can the proposed method be applied to more complex models, such as Transformer?	weakness
2021-109	Can the proposed technique be extended to optimize the certification bound (the lower bound of robust accuracy)?	weakness
2021-109	Maybe it is interesting to report the percentage of reduced perturbation region by using the convex hull rather than norm-ball or hyper-rectangle.	suggestion

2021-110	Summary: This work focuses on the study of (global) convergence and gradient dynamics of a recently proposed family of models, the deep equilibrium  (linear) models (DELM) under common classes of loss functions.	abstract
2021-110	Exploiting the Neumann series convergence and the PL inequality analysis, the authors proved convergence to global optima of DELM without prior assumption on the width m of the model (relative to the number of data n).	abstract
2021-110	Here is my general opinion: While deep linear models in general (as the authors acknowledged) has been widely studied, and despite the extreme simplicity of the DELM when compared to the original DEQ model that Bai et al. [1] studied, I found this work interesting and important in establishing a solid foundation for the theoretical study of this class of implicit-depth models.	strength
2021-110	The authors managed to demonstrate that the gradient dynamics and convergence assumptions of DELM is indeed different from typical "stacked" deep linear models that prior approaches study, such as deep linear ResNet.	strength
2021-110	And throughout the arguments of the paper and the proof in the appendix, I can tell how the "equilibrium" property of DELM is making the story different, and think this paper sets a good starting point for future similar in this direction for general implicit-depth models.	strength
2021-110	But still, the paper has a limited scope in terms of the structure it studies.	weakness
2021-110	Pros: One of the first theoretical works on the gradient dynamics and convergence properties of the deep equilibrium models [1] (and implicit models in general [2,3]), which are quite different from conventional deep networks.	strength
2021-110	Clear notation and theoretical insights, with proof relatively easy to follow.	strength
2021-110	The proof seems overall correct (there are some that I didn't check closely though).	strength
2021-110	Clear discussion of the relation, including (and especially) the differences of the prior analysis on deep linear neural networks.	strength
2021-110	Cons: The very definition of DELM, which the author provided a particular formulation of, is of limited scope (see my comment below that expands on this point).	weakness
2021-110	The empirical studies to validate the conclusion of the theoretical results could be strengthened.	weakness
2021-110	I have some comments/questions for the authors, detailed below: The major limitation that I found while reading the discussion and the proof of this paper is that while the authors claim to study deep equilibrium linear models, the insights mostly only apply to the models converging with Neumann series guarantee, and can be written in the form BU−1ϕ(x).	weakness
2021-110	I understand the motivation for fixating a provably convergent equilibrium model formulation.	weakness
2021-110	But: a) A provably convergent deep equilibrium linear model doesn't need to be Neumann (for its Jacobian) for the implicit function theorem to work.	weakness
2021-110	In the simplest case, the fixed point of a function h(x) on 2D can have a local derivative with absolute value > 1.	weakness
2021-110	This certainly implies that repeatedly unrolling the function h(x) may not converge, and yet there still is a unique fixed point and one can reliably solve for it (see [4]).	weakness
2021-110	However, without the nice Neumann series form, which allows one to write (I−γσ(A))−1 as a closed-form representation for the "infinite-depth" network forward pass, I don't think the theorems will hold directly.	weakness
2021-110	Typically, the (I−J)−1 term should only appear in the implicit function theorem, which is used for the backward pass.	weakness
2021-110	I expect the authors to clarify this further.	suggestion
2021-110	b) The very design of σ(A) in the model the authors study is a bit bizarre to me.	weakness
2021-110	Why applying a "softmax" on the weight?	weakness
2021-110	Is it just to ensure that proposition 1 holds (i.e., that you have a handy, provably-convergent linear model)?	weakness
2021-110	The authors stressed a few times that the h(⋅) function is thus "non-linear" w.r.t.	weakness
2021-110	σ(A), but I fail to directly see why it matters so much as the model is still linear w.r.t.	weakness
2021-110	the input (it's really a one-linear layer;	weakness
2021-110	though the inverse from Neumann does make a difference on A), and in terms of the gradient dynamics, the major difference this makes will merely be ∂σ(A)∂A.	weakness
2021-110	I might have missed something here and would appreciate if the authors can clarify.	misc
2021-110	I didn't quite get the specific point the authors were trying to make in Section 3.2 in terms of the implicit bias.	weakness
2021-110	Could you expand on that?	suggestion
2021-110	Overall, I feel that the empirical support of the theoretical findings can be stronger, for instance, by inspecting different initialization (A0,B0), or validating the radius discussion at the end of Section 3.1 for the logistic loss.	suggestion
2021-110	Like in Zou et al. [5], some synthetic data could probably work just fine.	suggestion
2021-110	What is the reason for only using 200 images from the MNIST/CIFAR datasets?	weakness
2021-110	Is it to keep the size of Φ small (but I didn't see the authors report anything about it in Section 2.2).	weakness
2021-110	And since the primary purpose of Sec. 2.2.	weakness
2021-110	is to "discuss whether the model would also make sense in practice" (which I take to mean that you only want to compare the test accuracies coming out of these models), wouldn't a 200-sample version of MNIST/CIFAR too small to draw a robust conclusion on this?	weakness
2021-110	One that I think could be useful for further thought is the convergence property not just for GD, but also SGD, like Zou et al. provided in [5] (up to a probability).	weakness
2021-110	Minor things: i) Page 3: "the outputs of the deep equilibrium linear models fθ(x)=⋯ are nonlinear and non-multilinear in the optimization variables (A,B)." Non-linear in even B?	weakness
2021-110	ii) Page 14: Vq∗ --> Bq∗	weakness
2021-110	iii) Page 16: ∇FL(A,B) --> ∇AL(A,B)	misc
2021-110	[1] https://arxiv.org/abs/1909.01377 [2] https://arxiv.org/abs/1908.06315 [3] https://arxiv.org/abs/2009.06211 [4] https://arxiv.org/abs/2006.08591 [5] https://arxiv.org/abs/2003.01094 Overview	misc
2021-110	This paper purports to study training deep equilibrium models by studying the optimization dynamics of deep linear equilibrium models.	abstract
2021-110	The original deep equilibrium model is formalized as follows: Given a training dataset (xi,yi) for i = 1,...,n where xi∈X⊆Rmx and yi∈Y⊆Rmy are the i-th input and output, respectively.	abstract
2021-110	The goal is to learn a predictor from a family H={fθ:Rmx→Rmy|θ∈Θ}.	abstract
2021-110	Then, instead of trying to map x to y using finite amount of layers, deep equilibrium models assume infinite number of layers, and the output z∗ of the last hidden layer is defined by z∗=liml→∞z(l)=liml→∞h(z(l−1);x,θ)=h(z∗;x,θ)	abstract
2021-110	where h is some continuous function of choice.	abstract
2021-110	In particular with deep equilibrium linear models, h is constrained as follows: h(z(l−1);x,θ)=γσ(A)z(l−1)+ϕ(x)	abstract
2021-110	where ϕ(x) is a feature map of x and transforms x∈Rmx into ϕ(x)∈Rm. θ=(A,B) are two trainable matrices, where A∈Rm×m is for computing each hidden output and B∈Rmy×m is for computing the final output of the network.	abstract
2021-110	γ∈(0,1) is some positive real number, and σ is a nonlinear function to ensure the existence of the fixed point z∗.	abstract
2021-110	This model is linear in z.	abstract
2021-110	The objective function of this deep equilibrium linear model can be written as follows: L(A,B)=∑i=1nℓ(B(liml→∞z(l)(xi,A)),yi)	abstract
2021-110	where ℓ is some choice of loss function.	abstract
2021-110	Then this paper provides some motivations behind studying the dynamics of these deep equilibrium linear models by presenting some interesting comparisons between deep equilibrium linear models and "normal" linear models and additional, less interesting comparisons with fully-connected feed-forward deep neural networks (FNN), using standard image datasets CIFAR-10, CIFAR-100 and Kuzushiji-MNIST.	abstract
2021-110	In their tests, the deep equilibrium linear models outperformed both linear models and FNNs. The main results from this paper is uncovering the dynamics behind these deep equilibrium linear models.	abstract
2021-110	This paper provided a sequence of proofs that shows linear convergence of these models step by step, under the assumption that the loss functions ℓ are differentiable, which is satisfied by some standard loss functions, such as square loss, losgistic loss, and smoothed hinge loss ℓ(fθ(x),y)=(max0,1−fθ(x)y)k with k≥2.	abstract
2021-110	Also, despite of non-convexity of the loss functions, it is shown in this paper that a global minimum L∗ always exists and under their assumptions, the deep equilibrium linear models will always converge to the global minimum linearly.	abstract
2021-110	Strengths The main strength of this paper is that it brought  some interesting insights into deep equilibrium  models, and they showed their results rigorously.	strength
2021-110	The definitions and propositions are clear enough for readers with some analysis and machine learning background to fully understand.	strength
2021-110	Since the dynamics of deep learning models is an open field of research and isn't discovered fully, this paper will definitely contribute to the deep learning field in understanding the dynamics and convergence theory of these deep equilibrium linear models, and potentially benefiting researches on understanding more general deep learning models.	strength
2021-110	Weakness Although this paper brought some nice ideas, some of the methodologies are not quite convincing.	weakness
2021-110	For example, in the experiments shown in this paper, they  compare performance of deep equilibrium linear models with linear models and deep neural networks.	weakness
2021-110	Especially in the comparison with the deep networks, it doesn't seem that the networks are deep enough for the comparison to be compelling.	weakness
2021-110	Also in the same experiments, they assumed the true data distribution is approximately given by a deep equilibrium linear model and generated data according to this model -- i.e. the data is only semi-"real".	weakness
2021-110	It would be better to show that the deep equilibrium linear model outperforms other models under a more general setting.	weakness
2021-110	Throughout the entire paper, it's unclear what are the contributions.	weakness
2021-110	To be specific, in the first two sections, it's unclear whether they want to show the trainability of the deep equilibrium linear models or learn the dynamics of these models.	weakness
2021-110	Also, some of the interesting aspects of this paper were missing some details.	weakness
2021-110	For example, one would be interested in seeing why exactly can the deep equilibrium linear models outperform linear models, since they are both linear and the only difference is how they are trained.	weakness
2021-110	In the last two sections of this paper, they also brought up implicit bias, which would be another interesting topic to dive into.	weakness
2021-110	It would be great seeing more comparisons on those aspects.	suggestion
2021-110	This submission studies the dynamics and convergence properties of "deep equilibrium models", which are parametric fixed-point iterations corresponding to the infinite depth limit of "weight-tied" neural networks.	abstract
2021-110	As the authors point out, these networks differ from deep linear networks and networks in the NTK scaling in that the optimization remains nonlinear w/r/t the parameters.	abstract
2021-110	The authors prove two results: first, they establish linear convergence to the global minimum under the relatively strict assumption of a "local" PL-inequality; secondly, they show that the dynamics of the deep equilibrium models differs from gradient descent dynamics and, in fact, is related to a trust region Newton method.	abstract
2021-110	The first theorem is nice and well-presented, but I think the issue of the radius over which the PL-inequality holds could be better discussed.	weakness
2021-110	I could not tell whether or not the convergence depends on starting within the locally smooth and quadratically bounded region of the loss.	weakness
2021-110	The second theorem, regarding the nature of the dynamics, lacks a clear interpretation.	weakness
2021-110	Basically, all that is said is that the dynamics is distinct from what would be seen in a linear model, and it's not clear that this dynamics has anything to do with implicit regularization, as the authors suggest.	weakness
2021-110	I would recommend the authors clarify the discussion of this result.	suggestion
2021-110	The experiments are somewhat bizarre and I felt that they were a little misleading about the representative power / potential of these models, but perhaps I did not fully understand the set-up and intent.	weakness
2021-110	The authors randomly sample a deep equilibrium model and then use it to represent a conditional probability distribution where the conditioning is with image data.	abstract
2021-110	Then, the authors fit various models to this distribution and show that the deep equilibrium models (which is precisely the underlying function representing the distribution) has better performance than other classes of functions.	abstract
2021-110	I think the description in this section could be vastly improved, perhaps presenting this more akin to a student-teacher problem.	weakness
2021-110	The paper discusses the theory of deep equilibrium models with linear activations.	abstract
2021-110	The model weights are softmaxed to ensure that inference converges to a fixed point, a necessary condition for training deep equilibrium models.	abstract
2021-110	The paper then analyzes the gradient flow dynamics of such models.	abstract
2021-110	The main result is that linear-rate convergence is guaranteed for a class of loss functions, including quadratic and logistic losses, when training with gradient flow.	abstract
2021-110	This conclusion is supported by experiments conducted in a teacher-student-like setup, where the labels are generated by a teacher deep equilibrium model, showing that training does converge in practice.	abstract
2021-110	Deep equilibrium models represent a novel way to train neural networks, and not much is known about them yet theoretically.	abstract
2021-110	It is important that we understand the dynamics of such models better, and this paper is a good step in that direction.	abstract
2021-110	Suggested improvements: Definition 1 has a typo: On the right-hand side of the inequality there shouldn't be a gradient.	weakness
2021-110	The main results are presented clearly, and the paper is generally easy to read.	strength
2021-110	The only exception is section 3.2 on the connection with trust region Newton methods, which I did not understand.	strength
2021-110	I recommend clarifying the intuition behind Theorem 2, as well as the main message of this section.	suggestion

2021-113	Summary: This paper considers communication games when agents use experience replay.	abstract
2021-113	The agents' communication protocol may change over time, leaving outdated symbols in the replay buffer which are then trained on.	abstract
2021-113	This paper proposes replacing the old communication actions with up-to-date actions as the transitions are sampled, and shows that this leads to greatly improved convergence speed and higher performance plateaus.	abstract
2021-113	Positives: The problem of multiagent communication and how to learn it is important and relevant to the *CONF* community.	strength
2021-113	The solution presented here seems like a natural fit with the problem and popular agent architectures and is well presented.	strength
2021-113	The paper is well motivated and well written.	strength
2021-113	Overall it was an enjoyable and easy read!	strength
2021-113	The experiments in Figures 4, 5, and 6 seem like great choices to show the strengths of the approach.	strength
2021-113	They're simple, well described, and well targeted.	strength
2021-113	Negatives: I feel like there's a pretty obvious question about "What happens in richer domains?" that (unless I missed it) isn't addressed in the paper - I'll expand on that in my 'Questions to clarify recommendation' section below.	weakness
2021-113	While the technique seems to work very well in the experiments chosen for the paper, I wish the paper touched a bit more on upcoming challenges, possible foreseen problems, and next steps.	weakness
2021-113	Recommendation and Justification: Overall, I feel like this was a strong paper and should be accepted.	decision
2021-113	My only real negative was that I am excited to know more about what comes next.	misc
2021-113	Questions to clarify recommendation: The three environments presented in the paper, if I've understood them correctly, are pretty straightforward in that	rebuttal_process
2021-113	1) the speaker only has communication actions (and no environment actions), and	rebuttal_process
2021-113	2) seems to only have one consistent message to communicate during the entire episode (after perhaps waiting to receive a message from others, in Hierarchical Communication).	weakness
2021-113	But right from the abstract onwards, I was wondering about possible problems in richer domains, where it seems like this technique could be harmful.	weakness
2021-113	Specifically, what if by updating the old communication action to one chosen by the current policy, we present a communication action that no longer aligns with the old environment action, which we do not update?	weakness
2021-113	I felt like this was a pretty natural question, but unless I missed it, the paper doesn't mention possible problems like this.	weakness
2021-113	I'll ground this in an example, similar to Cooperative Communication.	suggestion
2021-113	Imagine a two-player gridworld where the players cannot see each other, but are rewarded for arriving at the same map location.	suggestion
2021-113	Similar to Bach and Stravinsky / Battle of the Sexes, each player has a different preference over locations, but being at the same location is most important.	weakness
2021-113	Let's call the locations Left and Right.	misc
2021-113	To enable coordination, let one player be a Speaker that can take communication actions to signal the other player as to where they should meet in that episode.	suggestion
2021-113	While that permits greedy Speaker policies (always announce their preferred location and then go there) and greedy Listener policies (always go to their preferred location, regardless of Speaker's announcement), it would also allow the speaker to arrange a correlated equilibrium: announce a randomly chosen location in each episode and then go there, to maximize joint reward beyond any greedy Nash equilibrium policy.	suggestion
2021-113	Here's where I see a possible failure with the technique in this paper.	weakness
2021-113	Assume that the replay buffer contains an episode where the speaker emitted symbol L (for left) and then took environment actions to move to the Left location.	suggestion
2021-113	Later in training, using the technique presented here, we might sample this experience, update the symbol to L', and still move Left.	suggestion
2021-113	As described in this paper, I would expect that should work, and converge faster than by using the out-of-date symbol L.	suggestion
2021-113	However, it seems possible that the newer Speaker policy might prefer to move Right on that episode instead.	suggestion
2021-113	Updating the symbol would change it from old L to new R', but since the technique does not (and cannot, without a world model) update the environment actions, it seems like the listener and speaker would then train on this misaligned tuple of communication action R' and environment actions to move Left.	suggestion
2021-113	I would expect this confusing example to be much worse than training on the original example with the outdated but still aligned communication actions.	suggestion
2021-113	More generally: how can we make sure that the updated communication actions still align in intent with the agent's environment actions that we cannot change?	suggestion
2021-113	It seems like conditioning the communication action on the agent's environment action for that timestep might help, but would only be a partial solution: if an agent must speak now but take their first significant environment action in the future, we would have the same problem.	suggestion
2021-113	My questions regarding this point are: Do you agree that this could be a problem in richer environments than those presented in the paper?	weakness
2021-113	If so, do you foresee an easy solution, or will this be a challenge for future work?	suggestion
2021-113	I think the paper is strong enough as-is, and does not need an experiment in this paper to investigate richer games like this.	weakness
2021-113	However, if the authors agree that the technique could fail to help or could harm convergence in richer settings than those presented in the paper to support the technique, then I think a couple of sentences about future challenges and future work are warranted.	suggestion
2021-113	Issues and Suggestions: Nit: Pg2, Experience Replay.	suggestion
2021-113	The first sentence describes the agent as receiving (s_t, a_t, ..., s_t+1) at each time step.	suggestion
2021-113	Should this be (o_t, ..., o_t+1), since the agent receives observations and not environment states?	suggestion
2021-113	Typo: Pg2, MADDPG. 'uses deterministic polices' --> policies	misc
2021-113	Suggestion: Pg3, Methods section and Equation 4.	suggestion
2021-113	Equation 4 describes the tuple as containing r^e_t+1, r^m_t+1, but the text in the paragraph above only mentions r_t'+1, and the text below only indirectly clarifies what r^e and r^m are when it describes the cheap talk setting where r^m=0.	weakness
2021-113	This threw me for a while when I read the equation, and scanned back up the page to try to see where r^e and r^m were defined, and they aren't.	weakness
2021-113	I suggest changing the sentence in the previous paragraph from "receives rewards r_t'+1" to something like "receives rewards r_t'+1 (split into an environmental reward r^e and a messaging cost r^m)..." to clarify this before the symbols are used.	suggestion
2021-113	Typo: Pg4, Ordered Relabelling. "may themselves by conditioned" --> "may themselves be conditioned"	suggestion
2021-113	Clarify: Pg4, under equation 6.	suggestion
2021-113	The sentence "...we sample an extra o^m_t-1 in order to determine (using the other agents' policies) the new ^{o}^m_t, which allows us to relabel...".	suggestion
2021-113	I don't understand what this sentence is trying to say.	weakness
2021-113	Which player is this for?	weakness
2021-113	The symbol ^{o}^m_t doesn't appear in equations 5 or 6, so I don't understand what sampling an extra o^m_t-1 would do, since it's to compute a symbol that doesn't connect with the equations being discussed.	weakness
2021-113	Maybe I'm just missing something obvious, but I spent a couple of minutes trying to figure this out, before giving up and moving on.	misc
2021-113	Nit: Pg5, Implementation. Extremely minor, but the phrasing "we can therefore only relabel..." suggests a limitation of the approach (e.g., we are only able to do this...) whereas I think you're suggesting a performance win (we can do this using only...).	weakness
2021-113	I feel like flipping the words to "we can therefore relabel only a single..." better communicates that.	suggestion
2021-113	Typo: Pg9 and 10, References.	misc
2021-113	In both of the references including Pieter Abbeel, his affiliation is prefixed (OpenAI Pieter Abbeel).	weakness
2021-113	No other authors' affiliations are listed, so this just seems like a .bib typo.	weakness
2021-113	---- Summary ---- The paper proposes a method for modifying an experience replay when learning in communication environments, by relabelling messages using the latest policy.	abstract
2021-113	---- Reasons for score ----	misc
2021-113	The paper addresses the problem of non-stationarity in an important class of multiagent environment.	abstract
2021-113	The correction proposed is simple, and effective in the domains it is tested in.	strength
2021-113	My main concern is how broadly the method applies, which I am uncertain of from the paper.	weakness
2021-113	---- Pros ---- The paper provides a simple way to better leverage replay data for communication environments, addressing non-stationarity in those environments.	strength
2021-113	This is an important problem in multiagent environments.	strength
2021-113	The experiments show improvements in learning speed and final reward in some communication domains.	strength
2021-113	These cover a few important cases for the algorithm, including hierarchical communication and communication with an adversarial listener.	strength
2021-113	The paper is well situated in the literature on emergent communication, and it is clear and well written throughout.	strength
2021-113	---- Cons ---- My main worry is that the paper leaves me uncertain on when the method can be applied.	weakness
2021-113	The OCC algorithm suggests that a necessary condition is that the message graph is acyclic.	weakness
2021-113	However, this limitation is not explicitly discussed, and I am also unsure whether an acyclic message graph is a sufficient condition.	weakness
2021-113	For example, does the method apply with multiple listeners acting in the same environment, or when the speaker also acts?	weakness
2021-113	It would strengthen the paper to be more precise about the settings where these algorithms can be applied and can be expected to help.	suggestion
2021-113	The discussion on Covert Communication in the appendix significantly changed how I understood the results of this experiment;	suggestion
2021-113	in particular, the agents have not solved the task intended in the environment (using the key to communicate), but instead appear to be constantly changing strategy to outpace the listener	weakness
2021-113	- with the key being irrelevant.	weakness
2021-113	This is hinted at in the main text, but I think it is central enough to the interpretation of the experiment that it should be moved there.	weakness
2021-113	The authors present a fun and effective idea to translate a peer's message in terms of one agent's own experience.	abstract
2021-113	The benefit of doing so makes sense intuitively and is verified to be effective empirically.	strength
2021-113	Strengths: The motivation is clear, and the key idea is well-presented.	strength
2021-113	Paper is positioned well in relevant works of communication-aided MARL research.	strength
2021-113	Evaluation is thorough and indicative of the authors' claims.	strength
2021-113	Major Concerns: The reviewer has yet to discover a major issue with the paper with regard to its correctness, contribution, novelty, and effectiveness.	weakness
2021-113	The paper considers a multi-agent reinforcement learning (MARL) scenario where agents take actions based on the current observation alone.	abstract
2021-113	The paper proposes a communication correction mechanism where, during the centralized training, messages there were received in the past from other agents are reevaluated according to the updated policy.	abstract
2021-113	This way old messages can be updated instead of discarded, which is more efficient overall.	abstract
2021-113	The paper is clearly written and easy to follow.	strength
2021-113	The experiments are enough to convince that the communication correction idea is effective.	strength
2021-113	The idea is not very deep or insightful, so the significance of the contribution depends on how useful this trick will be in practice.	weakness
2021-113	This is completely fine, but since I'm not doing research in this area, it's hard for me to judge if this idea will be of value to other researchers, given the existing alternatives in the literature.	weakness
2021-113	Therefore my review focuses on the presentation of the idea.	weakness
2021-113	I'm missing some intuition regarding why applying this correction is always "safe".	weakness
2021-113	Couldn't it somehow drift since it always gives so much importance to the new learned actions?	weakness
2021-113	what if for some period of time the new actions are worse than the old ones?	weakness
2021-113	Have you encountered any scenarios where this correction (at least if not properly tuned) degraded the performance?	weakness
2021-113	Please elaborate on the assumption of actions that only depend on current observation.	weakness
2021-113	This seems very restrictive. How common is this assumption in the relevant literature?	weakness
2021-113	As a Markov strategy, it makes more sense if the state summarized the history of the game in some intelligent way.	weakness
2021-113	Is this the case for the scenarios you tested?	weakness
2021-113	I think it can be insightful to also experiment with a scenario where communication is noisy.	suggestion
2021-113	Specifically,  the scenario when there is a probability of p that a message is not received looks interesting to me.	suggestion
2021-113	The reason is that this kind of randomness seems to be essentially different than the noiseless case, as opposed to a weak Gaussian noise.	suggestion
2021-113	With this kind of noise, the sampling part of the algorithm will often sample a communication error when trying to relabel a message, effectively discarding the old message.	suggestion
2021-113	Is this desirable or acceptable?	suggestion
2021-113	Would one need to prevent this possibility if communication is random?	suggestion
2021-113	Since questions like this arise (if it makes sense), I'd be careful not to make statements about how easily the method can be applied for more general scenarios (like with noise), especially since the paper is 100% empirical so we only see that what was tested works.	ac_disagreement
2021-113	Also, please define the ~ sign explicitly to avoid confusion.	suggestion
2021-113	Is this a realistic assumption that p(o|a) is known?	suggestion
2021-113	it requires a model for the type of noise and communication failures that will occur in real-time, making the method not entirely model-free.	weakness
2021-113	Please discuss. I didn't find Fig.1 very helpful.	weakness
2021-113	What does it contribute over (5)?	weakness
2021-113	If you used the correction in Section A.3, why is (1) presented in the paper?	weakness
2021-113	seems a bit misleading to clarify that only in the appendix.	weakness
2021-113	Please add a reference for "cheap talk".	suggestion
2021-113	The abstract should what is the basis for the claim that "it substantially improves...", so at least mention this is based on experiments.	suggestion
2021-113	"We analyze the performance of this task in Figure 3" - seems to be an overstatement, there is no analysis here, just reporting the results of the experiment.	suggestion

2021-128	The authors train a neural net to predict responses of mouse V1 L2/3 neurons to visual stimulation.	abstract
2021-128	The NN has a "core" that is shared between all neurons, and a neuron-specific readout.	abstract
2021-128	They train the core on multiple animals and find that it can generalize well: it can be used in a new animal and (with sufficient training of the readouts) achieve high performance.	abstract
2021-128	They also use a neat approach of constraining the readout weights (receptive field location) using the known retinotopy of V1.	abstract
2021-128	Finally, they show that their network outperforms task-trained ones at predicting V1 responses.	abstract
2021-128	This is nice work overall.	strength
2021-128	I have a few suggestions: It might be worth considering other measures of performance, different from the normalized correlation coefficient.	suggestion
2021-128	Recent work shows that this measure can have unintended bias, being substantially affected by trial-to-trial variability.	suggestion
2021-128	See "The unbiased estimation of the fraction of variance explained by a model" from Pospisal and Bair (https://doi.org/10.1101/2020.10.30.361253) for details, and a potential solution.	suggestion
2021-128	2-photon imaging can have issues at detecting single spikes (see this preprint, for example: Relationship between spiking activity and simultaneously recorded fluorescence signals in transgenic mice expressing GCaMP6,  https://doi.org/10.1101/788802).	suggestion
2021-128	So the neural dataset could in principle show more multi-spike events than single-spike ones, or have other issues.	suggestion
2021-128	This is inevitable of course with calcium imaging, but it makes the problematic to compare with previous work that used electrical recordings.	suggestion
2021-128	E.g., I don't think it is possible to prove better performance for this work than the prior ones, because of this difference in recording methods.	weakness
2021-128	A good follow-up work should try this method on electrical recordings from (say) monkey, and compare with performance from the Cadena, Yamins, Kindel, Klindt, Batty, etc.	suggestion
2021-128	studies. The authors adopt a data-driven approach to neural system identification.	abstract
2021-128	They train a neural network consisting of a "core" and a "readout" in an end-to-end fashion to learn stimulus (visual inputs) -- response (single neuron activity) pairs.	abstract
2021-128	Since the core is shared across neurons, these stimulus-response pairs can be learnt in a massively parallel manner.	abstract
2021-128	In particular, they propose a novel readout mechanism that is parameter efficient and drives the core to learn better and generalizable features of the visual inputs.	abstract
2021-128	They find that their representations are more suited to predict neural responses in the mouse visual cortex when compared to representations derived from task-driven learning, especially in the context of transfer to previously unseen animals.	abstract
2021-128	Lastly, they also observe that the combination of their core+readout is more sample efficient than other naive alternatives.	abstract
2021-128	Pros: One of the major positives about this paper is the presented dataset.	strength
2021-128	It seems to be relatively large and well-curated.	strength
2021-128	This can certaily support several follow-up studies.	strength
2021-128	The authors identify that "global" use of features (i.e. the full hXwXc representational tensor) in the readout is a wasteful strategy (in terms of learned parameters per neuron) and instead adopt a local approach where they only select specific feature columns per neuron to drive the readout.	strength
2021-128	Though this is of minor technical novelty, this constraint forces the core to learn appropriate representations while allowing the entire module to be more data efficient, given the big reduction in the number of free parameters.	strength
2021-128	The sample efficiency studies are neat and informative.	strength
2021-128	The dissociations gleaned from diff-core/best-readout vs best-core/diff-readout scenarios are useful.	strength
2021-128	Though it needs more work to fully justify this claim, their demonstration that transferred representations seem to be more effective than direct training is surprising and interesting.	strength
2021-128	Cons: Though this study is certainly valuable, the manuscript needs several clarifications before it can be publication-ready.	decision
2021-128	(i) The authors seek to develop better core representations indirectly by controlling the readout mechanism.	weakness
2021-128	This is fine, but there is little justification as to why they chose the current "core" architecture.	weakness
2021-128	This choice contains arbitary decisions (such as including depth-separable convolutions) that are not justified.	weakness
2021-128	Was there a systematic procedure behind a search that led to this architecture?	weakness
2021-128	Were other non-standard architecures tested?	weakness
2021-128	(ii) Figure 2 currently seems to be adding very little value and needs to be improved.	weakness
2021-128	Given that the proposed readout mechanism was a major contribution of this paper, the authors could have used the Fig. 2 space to visually depict this readout procedure, on top of the readout position network.	weakness
2021-128	The arrow to a neuron is also a bit misleading.	weakness
2021-128	(iii) One of my main concerns is with respect to the liberal use of the term "generalization".	weakness
2021-128	The authors repeatedly state that train-val-test splits were based on neurons and not images.	weakness
2021-128	This, coupled with the fact that their readouts leverage retinotopy, it is surprising that the authors never discuss the spatial segregation of the "held-out" neurons (say H) from the neurons in the training set (say T).	weakness
2021-128	If most H neurons were spatially proximate to T neurons, then this amounts to an "interpolation" regime for the network as opposed to "extrapolation".	weakness
2021-128	If my understanding here is wrong, could the authors please clarify why?	weakness
2021-128	(iv) The authors report that transfered "core" representations work better than direct-training in their generalization experiments.	weakness
2021-128	This result is surprising and needs to be more strongly justified computationally.	weakness
2021-128	Is it possible that a sub-optimal training regime was used for direct-training?	weakness
2021-128	Is this anyhow related to issue (iii) raised above?	weakness
2021-128	(v) The authors report that task-driven cores (such as VGG-16 pretrained on imagenet) perform badly in generalizaing across animals.	weakness
2021-128	Is this due to impoverished data regimes?	weakness
2021-128	Or are there more systemic issues?	weakness
2021-128	Also, VGG-16 isn't the best ventral stream model that best fits neural data.	weakness
2021-128	Do the authors think that this claim would hold for more recent task-driven systems, like CORnet-S for example.	weakness
2021-128	(vi) Though not necessary for this manuscript per se, it would be helpful to test the usefulness of the generalizable core representations presented here for visual tasks supported by early visual areas.	suggestion
2021-128	Perhaps some commentary on this would be nice.	misc
2021-128	Minor: "Code for the analyses and the weights of the best generalizing representation will be shared in the final version of the paper".	misc
2021-128	The authors do not commit to making the dataset public.	weakness
2021-128	Is this oversight or intentional?	weakness
2021-128	how sensitive to number neurons in a scan?	weakness
2021-128	Clarity: (Fig. 4 caption) "a fully trained core": I think the authors are referring to a core trained on all available data, which is different from "fully training" a network as this alludes to loss saturation.	weakness
2021-128	Also language like "a sub-optimal" core is vague and misleading.	weakness
2021-128	Summarize what the paper claims to contribute.	weakness
2021-128	The paper introduces a deep-network-based approach to regression of responses to natural stimuli in mouse primary visual cortex.	abstract
2021-128	There is closely related work in the literature, but this paper achieves very good performance, partly through a new way of accounting for neurons' receptive-field positions.	strength
2021-128	The paper also provides a helpful analysis of prediction performance versus numbers of images and neurons used to train the model, and shows that the already excellent performance is not saturated with respect to the number of images.	strength
2021-128	The work also shows that features learned by a core network generalize well across different mice.	strength
2021-128	List strong and weak points of the paper.	misc
2021-128	Strong points: Empirical modelling of neural responses has a long tradition, and the results in this paper are state-of-the-art.	strength
2021-128	Thorough and insightful positioning in the recent literature.	strength
2021-128	Expert execution in terms of details of the technical work.	strength
2021-128	The method of parameterizing the receptive field location is well-motivated and effective.	strength
2021-128	The analyses are interesting and provide useful insights.	strength
2021-128	Weak points: I wouldn't characterize any part of the paper as weak, but here are some minor suggestions to further strengthen it: Say more about how the model can be used, or what insights might arise from it (there is only a short comment on inception loops).	suggestion
2021-128	Say more about limitations as a model of neural responses, particularly with respect to dynamics.	suggestion
2021-128	While the method is impressive with respect to short-time-window responses, system identification methods have long been used to study temporal responses.	weakness
2021-128	I think a short comment on this scope limitation would help to further contextualize the paper.	weakness
2021-128	An additional way to contextualize the results might be relative to the total number of neurons in L2/3 of VISp (I believe ~200K).	suggestion
2021-128	Does this number have any significance relative to the dimension of the core-network output, or the number of recorded neurons?	suggestion
2021-128	Consider adding a sentence on ethics oversight regarding the animal experiments.	suggestion
2021-128	Consider adding a few further details of the experiments.	suggestion
2021-128	Clearly state your recommendation (accept or reject) with one or two key reasons for this choice.	suggestion
2021-128	I recommend that the paper be accepted.	decision
2021-128	The paper addresses a long-standing problem very well.	strength
2021-128	It introduces a new method that is well justified and effective.	strength
2021-128	Overall, the performance is impressive, and the analyses are well done.	strength
2021-128	Ask questions you would like answered by the authors to help you clarify your understanding of the paper and provide the additional evidence you need to be confident in your assessment.	strength
2021-128	What are the kernel sizes in the core?	weakness
2021-128	Which hyperparameters are adjusted in the hyperparameter search?	suggestion
2021-128	Provide additional feedback with the aim to improve the paper.	suggestion
2021-128	I was confused by the following sentences: "… both readouts assume that the receptive field of each neuron is the same across features"	weakness
2021-128	"… readout has c + 7 parameters per neuron …" (I only see c+6.)	weakness
2021-128	"Fig 5 for the factorized readout …" (I didn't get it until reading it four times and looking for these results in Figure 5 twice.) The paper presents an experimental study on predicting the responses of mice V1 neurons with computational models.	weakness
2021-128	The paper advances a few contributions: Confirm that task-driven models based on object recognition, are outperformed by data-driven models for predicting single neuron responses.	strength
2021-128	Show that training a shared model of neural responses on data from several animals and several neurons leads to models that transfer well to data from new neurons and new animals.	abstract
2021-128	Introduce a novel readout mechanism that allows models to be shared fully across neurons which in turn helps transfer learning.	strength
2021-128	I think this paper is interesting and it should be presented at *CONF*.	decision
2021-128	I am not an expert in this specific sub-field so I am not qualified to make suggestions or evaluate the experimental design.	misc
2021-128	I will leave here a few suggestions that I hope you will consider for a camera ready version.	misc
2021-128	Is the claim that, in mice, task-driven models are outperformed by data-driven models fully justified?	weakness
2021-128	I have no trouble believing that object recognition is not the right task for mice, but there is a fair chance we just haven't found a good task yet, and that we might find it in the future.	strength
2021-128	If this is correct, it might be worth mentioning in the introduction or discussion.	strength
2021-128	I think the Introduction could do a better job of anticipating the implications of this study, similarly, the discussion is a bit dry and does little more than just repeating the results.	weakness
2021-128	As I mentioned, I am not especially familiar with the literature in this particular subfield, so I had a hard time imagining what I should learn about visual systems in general from your study.	weakness
2021-128	What can we do with this new information?	weakness
2021-128	Would it be possible to visualize the receptive fields you learn?	weakness
2021-128	Maybe some unit maximization technique could be sufficient.	suggestion
2021-128	I think it would be cool to see a few.	suggestion
2021-128	It might make sense to move the training regimes for direct training, within- and across-animal transfer to the method section and explain the data splitting, training and evaluation after those have been introduced.	suggestion
2021-128	The reader will know why you are designing your datasets and training a certain way, which might make it easier to follow.	suggestion
2021-128	Thank you again for sharing these cool ideas and results, I hope my suggestions help.	misc
2021-128	All the best!	misc

2021-141	Summary This paper proposes a linear time and space attention variant that matches (or exceeds) the accuracy of standard attention while maintaining the speedup of prior work in linear time/space attention.	abstract
2021-141	The approach is centered around a linear approximation of softmax attention, and is extended with a gating mechanism similar to a GRU.	abstract
2021-141	The accuracy improvements from the gated extension are demonstrated via language modeling on WikiText-103, while the speed-ups are demonstrated on translation.	abstract
2021-141	Contributions Demonstrates the importance of the choice of kernel in RFA, as prior results from Katharopoulos et.	strength
2021-141	al [1] underperforms softmax attention, while this work gets comparable performance by changing the kernel.	weakness
2021-141	Proposes an extension of RFA with gating that improves accuracy on language modeling, relative to softmax attention.	abstract
2021-141	Strengths The RFA-gated formulation is both more accurate and potentially faster (at least for decoding) than softmax attention, as demonstrated on language modeling.	strength
2021-141	The writing was clear and thorough.	strength
2021-141	The experiments support the claims of improved accuracy via choice of kernel and gating, and preservation of speedups inherited from the linear attention formulation.	strength
2021-141	Weaknesses The improved accuracy and speed over the baseline transformer are great, and the experiments serve to nicely illustrate those independently.	strength
2021-141	I would like to see a third application where these two qualities are demonstrated together.	suggestion
2021-141	The gating mechanism could also be applied to the output of softmax attention, but that comparison is not included.	weakness
2021-141	Please correct me if this is incorrect.	weakness
2021-141	Recommendation: Weak Accept Well-written and timely exploration of linear attention.	decision
2021-141	Empirical results demonstrate accuracy improvement over softmax attention, while preserving the linear time complexity.	weakness
2021-141	Extension of RFA with a gating mechanism appears to be effective, but I do not believe the claim that it is hard to apply this to softmax attention is valid, leaving the main contribution an exploration of a couple different kernels for linear attention.	weakness
2021-141	Questions Can you clarify how the RFA defined in section 3.1 differs from the linear attention in Katharopoulos et.	weakness
2021-141	al. [1]? What is different other than the choice of phi?	weakness
2021-141	Associative reductions such as those described in sections 3.1 and 3.2 can be computed in time logarithmic in sequence length (at the cost of n log n memory consumption) on a parallel device using a binary reduction or the prefix sum trick.	weakness
2021-141	Would this result in speed gains, or is the performance of attention in the parallelizable softmax setting already saturated?	weakness
2021-141	Are there experiments with a bidirectional gated RFA in the conditional setting, i.e. in the encoder for translation?	weakness
2021-141	Nits The formatting of equation 5 is a bit strange, as the start of the equation is inline while the rest is not.	weakness
2021-141	[1] Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and Francois Fleuret.	misc
2021-141	Transformers are rnns: Fast autoregressive transformers with linear attention.	misc
2021-141	In Proc. of ICML, 2020. Edit: I have updated my rating based on author response.	misc
2021-141	Summary: This paper presents Random Kernel Attention which is based on replacing the kernel function in the Linear Attention with random projection kernels.	abstract
2021-141	In general, I think the method is novel and quite impactful.	strength
2021-141	Nowadays, some people are still staying away from attention because of its quadratic time and space complexity.	strength
2021-141	To the best of my knowledge, it is the first attention method with linear complexity that can match or even outperforms the conventional attention.	strength
2021-141	Strengths: The method is intuitive and interesting to me.	strength
2021-141	The results are strong. Unlike Linear Attention, the proposed RFA outperforms the original multi-head attention baseline on both LM and MT tasks.	strength
2021-141	This is quite impressive. Based on my experience the Linear Attention with ELU non-linearity can bearly match the performance of the original attention mechanism.	strength
2021-141	The authors provide several in-depth analyses in the appendix.	strength
2021-141	I like the experiments in C.2.	misc
2021-141	It is nice to see that the authors confess that the training is actually increased when using the RFA compared to the original Transformer.	rebuttal_process
2021-141	Usually, the inference time and memory usage are more important in practice.	rebuttal_process
2021-141	It is great to see that the authors compare with the baselines that cache the query/key/value representations.	rebuttal_process
2021-141	Nowadays, some papers avoid it make their speedup look better.	rebuttal_process
2021-141	Weaknesses & suggestions: It is not clear what D is used in the experiments.	weakness
2021-141	The authors just vaguely say that they don't observe the improvement by setting it great than 2d.	weakness
2021-141	However, it would be better to see plots at least in the appendix.	weakness
2021-141	Also, I wonder if it would behave differently with different d.	weakness
2021-141	Also, it would be great to see what exactly the number is in the experimental setup to make this paper more reproducible.	suggestion
2021-141	The arccos feature maps have only D-dimensional features, unlike the Gaussian feature maps which have 2D.	weakness
2021-141	It is not clear whether the authors use the same D for both variants or double the D of arccos to keep the feature dimensions the same.	weakness
2021-141	After introducing the random projection weights, the number of parameters would increase.	weakness
2021-141	It would be better if the number of parameters and the inference speed are both provided in Tables 1 & 2.	weakness
2021-141	The authors should clarify that the time complexity in Table 3 is based on the assumption that we have infinite number of threads or GPT/TPU cores that can be scaled up when M is increased.	suggestion
2021-141	Otherwise, the time complexity of training the softmax model is still O(M^2) because there is a matrix multiplication between matrices of sizes M-by-M and M-by-d.	rebuttal_process
2021-141	Questions: Based on the experiments, it seems that the Gaussian random feature maps don't really try to approximate	weakness
2021-141	How would the gating mechanism perform on the encoder side?	weakness
2021-141	Similar to BiLSTM, half of the dimensions can be applied in a backward manner to make it bidirectional.	weakness
2021-141	Do you resample the random weights during the time?	weakness
2021-141	I wonder if the authors will release their implementation.	weakness
2021-141	Based on my quick re-implementation, the proposed RFA doesn't really converge on some other dataset.	weakness
2021-141	I believe there might be some differences in how the parameters are initialized which is not clearly described in the paper.	weakness
2021-141	Admittedly, there is a chance that I have a bug in the code.	weakness
2021-141	Based on the conclusion in C.2 that RFA is not approximating the softmax kernel, would it be better if we just trained those projection matrices instead of fixing them as random matrices?	weakness
2021-141	The paper presents a linear time and space attention mechanism based on random features to approximate the softmax.	abstract
2021-141	The paper is clearly written and easy to follow.	strength
2021-141	The results are convincing: not chasing SOTA, but comparing to sensible baselines, namely [Baevski & Auli 2019] for language modeling on Wikitext-103, and [Vaswani et al. 2017] for machine translation on WMT14 EN-DE/EN-FR and IWSLT14 DE-EN.	strength
2021-141	The difference between theoretical speed-up and experimental speed-up is honestly discussed, and the overhead of the random features is not swept under the rug.	strength
2021-141	However, having an experimental study on the random features dimensions' impact on empirical compute time vs.	strength
2021-141	approximation performance vs. end-task performance would have been a plus.	strength
2021-141	I had read "Rethinking Attention with Performers" when I reviewed this paper, and I originally thought it was the same paper, but (along with notation being different) they start to differ on page 3.	weakness
2021-141	Where "Performers" goes with positive orthogonal random features (to improve over vanilla RFA), this paper adds a gating mechanism: this adds the possibility to learn some monotically decaying attention over older context, similar to learned receptive fields of attention (as in e.g. [Sukhbaatar et al. 2019]).	weakness
2021-141	Overall, this is a good paper, and I don't see why we should downplay it in light of simultaneous ("Performers" got on ArXiV on September 30th, the *CONF* deadline was October 2nd) quite similar contribution that the authors took the time to discuss.	weakness
2021-141	(It would be even better if they could compare to it in a future version.) The logic in the introduction is a bit contradictive to me: Some are able to achieve better asymptotic complexity (citations).	weakness
2021-141	while it is more challenging to improve on shorter sequences: the additional computation steps required by some approaches can overshadow the time and memory they save.	weakness
2021-141	Doesn't this simply mean that for short sequences there is no such computational burden?	weakness
2021-141	I think the story starts with pointing out the importance for long-sequence but turns to the topic on short sequence which is confusing.	weakness
2021-141	The need for short sequence acceleration needs to be justified IMO.	weakness
2021-141	Following 1, the baseline should be added.	weakness
2021-141	For a fair comparison, I think the baseline should add those methods as claimed in the introduction	suggestion
2021-141	(Lee et al., 2019; Child et al., 2019; Sukhbaatar et al., 2019; Beltagy et al., 2020, inter alia), (Kitaev et al., 2020; Wang et al.,	misc
2021-141	2020; Roy et al., 2020, inter alia) and let us know how badly they performed under the short sequence.	misc
2021-141	In particular we don't know if the sacrifice of short sequence time would benefit a lot in long sequences for existing methods.	weakness
2021-141	The current experimental baseline can't reflect this.	weakness
2021-141	Is the speedup over total computational time or just the attention part?	weakness
2021-141	To best of my knowledge, under many circumstances in particular for short sequence, attention alone might not be the most time-consuming part of the model.	weakness
2021-141	I think it will be helpful for authors to have a complete graph of the computational model used instead of only figure 1 concept graph.	suggestion
2021-141	Specifically, is there any feed-forward computation involved and how many layers of the models used in comparison.	suggestion
2021-141	Introduction of Eq 6,7 is confusing.	weakness
2021-141	Up to eq 5 it's clear whatr's going, but it comes from nowhere to intorduce these 2 modules in eq 6,7.	weakness
2021-141	So my understanding is that	weakness
2021-141	RFA simply refers to the approximation of computing the softmax.	weakness
2021-141	So the statement: for softmax-attention. The latter is typically used in two different ways in the transformer architecture, each resulting in a different computation for RF	weakness
2021-141	is confusing as the RFA is now redefined.	weakness
2021-141	I believe RFA should only refer one thing and I don't think eq(6) and eq(5) leads to the same result.	weakness
2021-141	On the other hand, eq 7 should be the same as eq 5.	weakness
2021-141	Is this correct? In addition, the notation in (6) looks wrong to me.	weakness
2021-141	\\phi(x) as introduced in eq 2 is in R^{2D} but S_{t-1} is in R{D}, not sure what does + mean in this context.	weakness
2021-141	I couldn't find out where you properly define the meaning of D either.	weakness
2021-141	Clarification of contribution Eq 6,7 reads like RNN style update but the intuition is lacking.	weakness
2021-141	Do you want to claim that this structure design is inspired by RNN and it leads to a better result?	weakness
2021-141	Put in another way, using RFA in transformer is from Rawat et al., 19 so do you think your major contribution is to design such a gated usage of RFA?	weakness
2021-141	Discussion of D Since RF is not the major contribution, you summarize existing results of FA in sec2.2.	weakness
2021-141	I think I'd like to see a discussion of sufficient number D analytically or empirically.	suggestion
2021-141	Could you also cite the convergence bound on this approximation?	suggestion
2021-141	To me, D looks to be an important efficiency tradeoff.	strength
2021-141	Say sequence length is M	strength
2021-141	and feature in d dimension.	strength
2021-141	Original Attention is O(M^2 d).	strength
2021-141	The computation of RFA requires outer product, which is O(D^2d) so overall it's O(M D^2 d), if M is around 64 or 128 (common usage) and D is 64, I actually don't see why RFA could improve 2x.	weakness
2021-141	Do you pre-compute and pre-store anything?	weakness
2021-141	Time analysis on language modeling is not presented.	weakness
2021-141	Since it's a efficiency paper, I think it should be complete.	weakness
2021-141	Overall, I think the paper provides an interesting view of discussion, but there are many flaws in the current version which needs to be corrected before a more serious consideration.	weakness
2021-141	Especially, in terms novelty, the paper is relatively limited as the RF is explored in Rawat et al., 19.	weakness
2021-141	So my point 5 is important to answer and I would like to see all the details are clarified in order to make the contribution stronger.	suggestion

2021-166	The work presents a novel message passing GNN operator for meshes that is equivariant under gauge transformations.	abstract
2021-166	It achieves that by parallel transporting features along edges and spanning a space of gauge equivariant kernels.	abstract
2021-166	Further, a DFT-based non-linearity is proposed, which preserves the equivariance in the limit of sampling density.	abstract
2021-166	The method is evaluated on an MNIST toy experiment and the Faust shape correspondence task.	abstract
2021-166	Strengths: The method is very elegant and novel and seems to be one of the most sophisticated mesh GNN operator so far.	strength
2021-166	At the same time it looks like that the operator stays reasonably efficient, still keeping linear time complexity in number of edges.	strength
2021-166	I would welcome, though, if computational efficiency would be analyzed in the work.	suggestion
2021-166	It bridges the more theoretical equivariant convolutions with graph neural networks for mesh processing, which are more commonly used in practice.	abstract
2021-166	An equivariant non-linearity based on the discrete Fourier transform is presented.	abstract
2021-166	The work seems to be technically and mathematically sound.	strength
2021-166	The paper is well written.	strength
2021-166	The figures complement the text well and are helpful for understanding.	strength
2021-166	Weaknesses: Neither the MNIST nor the FAUST experiment verify the gauge equivariance.	weakness
2021-166	I think a toy experiment verifying it would be necessary, especially since equivariance of the non-linearity is only approximated (maybe by showing appropriate feature histograms after the non-linearity for changing reference points).	weakness
2021-166	In addition to a missing verification experiment, it is also hard to follow the theoretical reasoning that the whole approach is equivariant.	weakness
2021-166	The line of reasoning needs to be gathered from the main text, appendix, and referenced related work.	weakness
2021-166	I think with more concrete theorems, clarity could be improved.	weakness
2021-166	The Faust shape correspondence task is not sufficient to evaluate a novel mesh operator.	weakness
2021-166	From personal experience I know that at least SpiralNet++ (anisotropic, intrinsic, fixed topology) and SplineCNN (anisotropic, extrinsic, arbitrary/varying topology) can be tuned to reach near perfect accuracy on this task.	weakness
2021-166	Such small differences in performance can come down to architecture design and might not come from more principled differences in the method.	weakness
2021-166	Therefore, I would highly welcome one additional comparison on a different mesh task.	suggestion
2021-166	Since the experiments are scarse, I wonder if the operator is hard to apply to larger tasks or if the kernel restrictions weaken the approach on other tasks.	suggestion
2021-166	Questions and comments: As usual in this area, the kernels are restricted in trade for equivariance.	weakness
2021-166	I wonder how expressive the kernels still are.	weakness
2021-166	Is there a way to compare the expressiveness (visually or quantitatively) to lets say an MLP kernel function mapping polar or Cartesian coordinates to a full C x C matrix?	weakness
2021-166	Is it the/a minimal restriction that ensures equivariance or is it more restrictive?	weakness
2021-166	Would it be possible to come up with a space of two-dimensional kernels K_neigh (dependent on the full polar coordinates, including r) while keeping the gauge equivariance?	weakness
2021-166	How efficient is the non-linearity based on DFT?	weakness
2021-166	I would be interested in a execution time breakdown for the whole method, showing the bottlenecks.	suggestion
2021-166	In the MNIST experiment, pooling is applied (appendix D.1).	suggestion
2021-166	How does this interfere with the equivariance property?	suggestion
2021-166	Minor/Typo: Appendix page 13, equation above eq 8, parenthesis missing	weakness
2021-166	Overall, I like the paper, it is nice to read and the method is interesting, building on mathematically elegant concepts.	strength
2021-166	I actually would like to give an accept score.	decision
2021-166	However, in my opinion there are experiments missing: (1, crucial) verifying the gauge equivariance and (2) an additional comparison on a more complex task.	weakness
2021-166	Without those experiments (especially 1), I see the paper in a borderline state.	misc
2021-166	This paper presents Gauge Equivariant Mesh CNNs. The method is motivated by the fact that graph convolutions can be modified for meshes to take into account the angular arrangement of local neighborhoods.	abstract
2021-166	The result is a Mesh-CNN that is equivalent to GCNs with anisotropic gauge equivariant kernels.	abstract
2021-166	STRENGTHS The problem tackled here is very important and well motivated.	strength
2021-166	The authors identify the issues related to existing networks and devise a sensible approach.	strength
2021-166	The approach is detailed and carefully patches the problems in mesh convolutions.	strength
2021-166	Introducing non-linearity to such networks is far from being trivial.	strength
2021-166	I thank the authors for conveying an analysis on this front.	misc
2021-166	WEAKNESSES I believe that this paper (as well as many other prior works) are missing an important connection to the rich literature of 3D vision.	weakness
2021-166	From Ajmal Mian's work to Spin images, from SHOT to the recent deeply learned PPF-FoldNet (and point pair features thereon), there are just too many works that compute local reference frames to canonically orient the local geometry.	weakness
2021-166	Many of these works compute some form of a point cloud normal (thus a tangent plane) and choose a direction orthogonal to it (hence fix a gauge).	weakness
2021-166	Traditionally, this is known as 'local reference frame' and have set an important milestone in 3D descriptors, learned or not.	weakness
2021-166	Simply changing the terminology and rephrasing the established results (e.g. connection of local frames via a rotation) through the lense of Riemannian geometry should not distinguish this paper.	weakness
2021-166	I like this principled explanation (only a personal preference) but the ties to all those works should be made concrete.	weakness
2021-166	For example, recently, Zhao et al. have used local reference frames on meshes and point clouds to design equivariant point cloud networks: Zhao, Y., Birdal, T., Lenssen, J.	weakness
2021-166	E., Menegatti, E., Guibas, L., & Tombari, F.	misc
2021-166	(2020). Quaternion Equivariant Capsule Networks for 3D Point Clouds.	misc
2021-166	European Conference on Computer Vision (ECCV)	misc
2021-166	It would then be nice to mention clearly that one would like to omit certain local reference frame choices and directly convolve as there is an ambiguity in the tangent plane.	suggestion
2021-166	There are now many works which can exploit Riemannian geometry to craft convolution operators.	misc
2021-166	Technically, there are subtle differences which makes the contributions of the paper less clearer.	weakness
2021-166	I would strongly suggest that the paper is revised such that the contributions are stated very clearly.	suggestion
2021-166	Otherwise, it is hard to figure out which parts already existed and which are novel.	weakness
2021-166	I consider Sec. 6.1 to be a synthetic dataset because changing the underlying lattice for MNIST does not have practical use.	weakness
2021-166	And the results presented in Tab. 2 have some interesting accuracies such as 1.40.	weakness
2021-166	Why is that so low?	weakness
2021-166	Can we just fix this by picking a naive baseline: Choose an LRF (see above, use SHOT's frame for instance), canonicalize the points locally and apply convolution.	suggestion
2021-166	How would this simple approach perform?	suggestion
2021-166	I think some similar idea is already mentioned in: Yang, Z., Litany, O., Birdal, T., Sridhar, S., & Guibas, L.	weakness
2021-166	(2020). Continuous Geodesic Convolutions for Learning on 3D Shapes.	misc
2021-166	arXiv preprint arXiv:2002.02506. Would be possible to briefly summarize Mesh-CNN?	suggestion
2021-166	This paper gives an improvement over that so it could be beneficial to see (even in a supplementary) how it is improved.	suggestion
2021-166	Or is Mesh-CNN is used just to refer to the category of works?	weakness
2021-166	There is a form of 'discretization' of the parallel transport going on.	weakness
2021-166	Can't we compute the vector heat to do a better and faster discretization: Sharp, Nicholas, Yousuf Soliman, and Keenan Crane.	weakness
2021-166	"The vector heat method." ACM Transactions on Graphics (TOG) 38.3 (2019): 1-19.	misc
2021-166	In general it would be great to have an understanding of the proposed transport operator.	weakness
2021-166	Some more focused explanation of the representation theory could be great.	weakness
2021-166	My recommendation is positive because the community needs principled ways of convolution on non-Euclidean domains, and this paper seems to make an incremental contribution towards that direction.	decision
2021-166	However, lack of thorough evaluations, the missing links to the literature and the rather inaccessible presentation of the material should definitely be improved.	weakness
2021-166	Although a mesh embedded in 3D space may be treated as a graph, a graph convolution network uses the same weights for each neighbor and is thus permutation invariant, which is the incorrect inductive bias for a mesh: the neighbors of a node are spatially related and may not be arbitrarily permuted.	weakness
2021-166	CNNs, GCNs, and G-CNNs demonstrate the value of a weight sharing scheme which correctly reflects the symmetry of the underlying space of the data.	weakness
2021-166	The authors argue convincingly that for a signal on a mesh, the appropriate bias is symmetry to local change-of-gauge.	weakness
2021-166	In short, the weights should depend on the relative orientation of a node's neighbors.	weakness
2021-166	They design a network GEM-CNN which is equivariant to change of gauge.	weakness
2021-166	The design is similar to a GCN but incorporates parallel transport to account for underlying geometry and uses kernels similar to those of SO(2)-equivariant E(2)-CNN (Weiler & Cesa 2019).	abstract
2021-166	The experiments show the network is able to adapt to different mesh geometries and obtain very high accuracy in the shape correspondence task.	abstract
2021-166	I suggest accepting this paper.	decision
2021-166	The architecture is an elegant way to incorporate gauge symmetry on meshes and RegularNonlinearity addresses an important issue for equivariant neural nets.	abstract
2021-166	Though I would prefer more interesting experiments, they are sufficient to validate the design.	suggestion
2021-166	Specific Strength, Weaknesses, Points, and Questions: The symmetry of a graph convolutional network can be broken by including spatial coordinates as features.	weakness
2021-166	If xp and xq are inputs, then a function F(fq,xq,xp) can process data in an orientation aware way even though F is isotropic.	weakness
2021-166	How would this compare to the proposed method in the paper?	suggestion
2021-166	Page 2: One strength of the paper is the argument for why gauge symmetry is necessary in the first place.	strength
2021-166	Features defined on a mesh may be vector-valued and defined with respect to a frame of reference at each point on the mesh.	strength
2021-166	However, since the geometry is curved, there is no consistent way to assign a frame of reference.	weakness
2021-166	Thus an arbitrary choice must be made when recording data.	weakness
2021-166	Since this choice is arbitrary, the output of the network should clearly be independent of it.	weakness
2021-166	Gauge equivariance encodes this symmetry.	weakness
2021-166	The argument for encoding the geometry of the mesh is reasonable.	strength
2021-166	But then why only parameterize K by θq; why not also include the distance rq?	strength
2021-166	Page 6: RegularNonlinearity is an important contribution of the paper.	strength
2021-166	The authors are correct that non-linearities have been a bottleneck for using equivariant neural networks with representations other than the regular representation.	ac_disagreement
2021-166	Transforming to sample space (i.e. embedding in the regular representation) to apply a pointwise non-linearity and then transforming back is a nice idea for addressing this.	suggestion
2021-166	Furthermore, Theorem E.1 provides a nice theoretical analysis of the asymptotic error.	strength
2021-166	It would be nice to include practical non-asymptotic error bounds as well.	suggestion
2021-166	The experiments are okay, but are a weaker part of the paper.	weakness
2021-166	The argument for a geometry-aware NN is that it processes signals on the geometry better.	weakness
2021-166	It is not clear that embedding MNIST on a mesh illustrates data which is best understood in terms of the underlying mesh.	weakness
2021-166	Far better would be to develop a signal natively on the mesh, for example by solving a PDE directly on the mesh.	suggestion
2021-166	Arguably the FAUST shape correspondence data addresses this issue better since the signal is inherently linked to the geometry.	suggestion
2021-166	Both experiments also treat only scalar data of type ρ0.	weakness
2021-166	While it is plausible and reasonable to model such data using vector features of type ρi in the hidden layers, the argument for the necessity of gauge equivariance would be even stronger if the input and/or output signal was itself vector valued, for example a velocity or gradient on the mesh.	suggestion
2021-166	Changing roughness to the embedded MNIST distorts the signal in the geometry of the manifold (changing distances and angles), so why should we expect generalization across different roughnesses?	weakness
2021-166	Page 7: I don't understand the argument for the value of symmetry breaking.	weakness
2021-166	The gauge equivariant network can be orientation aware by encoding ρ1 features.	weakness
2021-166	Why is it desirable to be dependent on arbitrary gauge choices?	weakness
2021-166	What breaks down about the original argument for incorporating equivariance in this case?	weakness
2021-166	Is it possible the improvement is due to a different trade off between bias and expressivity at lower layers?	weakness
2021-166	Page 7, Para 4: The paper argues other high performing methods in shape correspondence use complicated pipelines.	abstract
2021-166	It is not clear to me (probably from lack of familiarity) which is most complicated.	weakness
2021-166	It seems both this method and the other method contain different complexities and subtleties.	weakness
2021-166	Minor Points: All of the citations in the paper use \\citet, but it would be more readable to use \\citep.	weakness
2021-166	Page 3: Should not ρ(gq→p) be invertible in Rcin×cin?	weakness
2021-166	Page 5: The notation kρl is non-standard, compared to ρlk, but it is more readable.	weakness
2021-166	Page 13, Kneighθpq−g) is missing a parenthesis	weakness
2021-166	Page 13, "which is true for any features, if".	weakness
2021-166	if could be if and only if, correct?	weakness
2021-166	Update From Author Reply I am grateful for the author's replies, edits, and additional evaluation, all performed within limited time.	rebuttal_process
2021-166	This helps me feel confident in my accept (7) recommendation.	decision
2021-166	My reason for not giving a higher score remains the limited experiments (which is likely not something to be addressed in two weeks), but even so I think the work is quite worthy of being accepted.	decision
2021-166	The methods are a significant contribution and the experiments are sufficient to demonstrate they work.	strength
2021-166	Graph convolution has been defined to be permutation equivariant to the neighborhood vertices.	strength
2021-166	If one were to define an anisotropic kernel then a reference edge would have to be defined corresponding to edge angle 0.	strength
2021-166	Figure 1 is very illuminating.	strength
2021-166	Equivariance with respect to this reference can be achieved only with a special mechanism.	strength
2021-166	The authors here proposed message passing via edge transporters.	abstract
2021-166	The crux of the approach is in equation (2) and in particular in ρ(gp→q∈[0,2π).	abstract
2021-166	What happens is that the feature vectors of the adjacent vertices have to be transported to the center node, namely a transformation from the local coordinates in p to the local coordinates in q correcting, thus, the underlying gauge difference.	abstract
2021-166	This step includes also an alignment of the tangent planes.	abstract
2021-166	I strongly believe that appendix A belongs to the main paper.	abstract
2021-166	The second crucial point is that equivariance imposes a linear constraint on the kernels KSelf and KNeigh.	abstract
2021-166	This allows a kernel to be written as a linear combination of 20 kernels for KNeigh and KSelf resulting in 24 only unknowns for one layer.	abstract
2021-166	The related work section is comprehensive.	abstract
2021-166	The experiment on shape correspondence achieves performance comparable to the state of the art spiralNet++.	abstract
2021-166	The paper would benefit from other experiments on manifold like performing mesh convolutions for human or object reconstruction from images.	suggestion
2021-166	Faust is pretty standard in the GML community but it is an easy task in terms of feature learning.	weakness
2021-166	The paper contribution is elegant and significant: Gauge equivariance  is a necessity if you want an anisotropic diffusion.	strength
2021-166	The paper is unreadable without the appendix and somehow it would be better to make it self-contained and move the experiments in the appendix.	weakness

2021-187	The paper under review is a very technical contribution to the study of group-equivariance of convolution kernels.	strength
2021-187	The problem of group-equivariance is studied in the most general setup, thus encompassing the previous achievements of Cohen and Welling, Cohen, Geiger, and Weiler, Weiler and Cesa, etc.	strength
2021-187	The most general tools from classical harmonic analysis and Lie group representations are put to work in order to provide the most general framework for the analysis of equivariance.	strength
2021-187	I learnt a lot about the mathematics of equivariance, a very interesting topic.	strength
2021-187	However, I wonder wether the *CONF* conference is the most appropriate venue for such thorough study.	strength
2021-187	The application section is in particular way too sketchy to convince the novice that this impressive work will be useful to the machine learning community and an effort in this direction should be made to clarify the expected impact.	weakness
2021-187	For a physicist familiar with machine learning, the title of this work says it all.	weakness
2021-187	It is a lengthy explanation of how to use well known techniques from physics in constructing convolutional neural networks with a group symmetry.	abstract
2021-187	The paper also spells out what are likely to be the most used cases of U(1) and SU(2) and their quotients by discrete groups.	abstract
2021-187	I give the paper high marks on the value of the topic it addresses, but middling marks on presentation.	rating_summary
2021-187	The results I spot checked were correct, but the paper reads like a compilation of material from the many math and physics textbooks on the topic, and lacks the coherence of a good textbook.	weakness
2021-187	If I had a student who needed this material I would instead give him or her the original Cohen Welling paper and a representation theory textbook such as Hall 2015 or one of the several textbooks they cite.	suggestion
2021-187	In addition the explicit results of appendix E would be rather more useful if implemented in a ML package, and I hope that is on the author's to-do list.	suggestion
2021-187	To summarize, a serviceable reference work which will probably be made obsolete by the appearance of a proper textbook on group theory in ML before long.	suggestion
2021-187	The paper considers Group Equivariant Convulation Neural Networks (GCNNs) which are convolutional neural networks that are equivariant wrt group symmetries of the underlying space.	abstract
2021-187	The equivariance requirement places constraints on the parameterization of the corresponding CNN.	abstract
2021-187	This work extends previous results for particular symmetries and outlines a method for obtaining these parameterizations for any compact group symmetry.	abstract
2021-187	Technically, the paper establishes a Wigner-Eckert Theorem for G-steerable kernels, which in turn allows any admissible kernel to be expressed using a basis of kernels thereby establishing a natural parameterization.	abstract
2021-187	This procedure is carried out for U(1), SO(2), SO(3) among others.	abstract
2021-187	The paper highlights important ideas from representation theory that can be used to obtain paramaterizations for symmetry-constrained learning models, and the mathematical methods could be of independent interest.	strength
2021-187	The results obtained here are significant for any learning problem where there are inherent natural symmetries, as the authors point out this could be especially beneficial for data arising from physical processes.	strength
2021-187	The authors prove a theorem (thm 4.1) which describes a basis for the space of kernels in a G-steerable CNN for any compact group G.	abstract
2021-187	Steerable CNNs are similar to CNNs but replace channels with G-reps and enforce an equivariance constraint on the kernels.	abstract
2021-187	Though Cohen et al 2019 state the constraint, and Cohen et al 2019, Cohen & Welling 2016, and Weiler & Cesa 2019 and several other papers solve this constraint for different groups and representations, there has not been a general formulation which applies to all compact groups.	abstract
2021-187	Here, solving the constraint means to construct a basis of the space of steerable kernels.	abstract
2021-187	Any steerable kernel is then a linear combination of this basis and the network can then be trained by learning the coefficients.	abstract
2021-187	The problem of finding a basis for the space of steerable kernels is non-trivial and critical for constructing G-steerable CNNs.	abstract
2021-187	Up until now this has done group by group.	abstract
2021-187	The theorem proved in this work unifies such previous efforts and provides a useful method for approaching further G.	abstract
2021-187	This paper is a significant contribution to the field.	strength
2021-187	The appendix provides a complete and approachable background in the area as well as detailed and precise proofs.	strength
2021-187	Moreover, the effort by Cohen & Welling to frame equivariant deep learning in the proper context of representation theory is continued and extended here to good profit.	strength
2021-187	The appendix is quite verbose and the language is more casual than I am accustomed to in a mathematical text or research paper, but it serves the goal of being didactic and approachable.	strength
2021-187	A practical consideration remains. Though theorem 4.1 reduces construction of a basis of steerable kernels to 1) finding Clebsch-Gordon decomposition of tensor products, 2) describing endomorphisms of irreps, and 3) describing harmonic functions, none of these problems is trivial (or even necessarily solved) for a general compact group G.	strength
2021-187	That said, Appendix E does a good job providing evidence that this can be done for many individual groups.	strength
2021-187	However, in that case, we are still back to solving the problem on a group by group basis.	misc
2021-187	Specific Additional Points: 1.My opinion is that the language of physics does not add to the paper.	weakness
2021-187	While Clebsch-Gordan and harmonic functions first arose in physics, they can be described in terms of representation theory.	weakness
2021-187	In this way, both steerable CNN and quantum mechanics are applications of rep.	weakness
2021-187	theory and so it is not necessary to use physics here to describe steerable CNN.	weakness
2021-187	2.Page 5, the notation [j]=dim(Vj) seems unusual to me.	weakness
2021-187	It would be better to use something more standard.	weakness
2021-187	In particular, in Defn 3.5, brackets are used as parenthesis, making this more confusing.	weakness
2021-187	3.Page 5, the fact that input and output representations Vin and Vout decompose into irreps does not immediately explain how to construct a steerable kernel basis for Vin→Vout given ones between irreps Vi→Vj.	weakness
2021-187	Though it is not complicated, I would include this.	weakness
2021-187	4.Page 5, I was confused by the inclusion of EndG,K(Vj) at first since it does not appear when working over C due to Shurr's lemma.	weakness
2021-187	It is explained in the paper and more so in the appendix that it is necessary over R, but it could be a bit clearer and earlier in the paper.	weakness
2021-187	Namely, you could note that over C EndG,K(Vj)=C and give the possibilities over R.	weakness
2021-187	5.Page 5, Thm 3.4 and Page 43, Thm C.7.	misc
2021-187	Given that the purpose of this paper is partially to formulate Steerable CNN in precise terms, the fact that δx is informally considered as in L2(X) is very imprecise.	weakness
2021-187	Not only does this make the proof informal, it means the maps in the theorem are not even defined.	weakness
2021-187	Can you replace L2(X) with an appropriate space of distributions in order to make the statement precise?6.Page 6, "which is zero for almost all J" should be "which is zero for all but finitely many J"	suggestion
2021-187	7.Appendix E, U(1) is isomorphic to SO(2), so it is strange to use both notations.	weakness
2021-187	The difference between these subsections is whether the representations are real or complex.	weakness
2021-187	Updates from Author Feedback While I would have liked to see a draft with the changes, I feel reasonably sure the authors will improve Appendix C to make the statements mathematically precise.	suggestion
2021-187	I am confident in the statements and proofs.	suggestion
2021-187	While the presentation can be verbose and casual, I think it is justified to increase accessibility, so long as the proofs and statements are formal and precise.	suggestion
2021-187	Based on Author responses I have increased my confidence.	rebuttal_process

2021-505	The paper focuses on the problem of multi-task control with a shared policy in the continuous action setting.	abstract
2021-505	Unlike current assumption of compatible state-action spaces, the proposed architecture is transferable across different morphologies.	abstract
2021-505	The paper includes ablation experiments that clearly show that current works that use the body morphology structure to constrain the graph structure of graph neural network based approaches do not actually improve the performance.	abstract
2021-505	The paper instead forgoes trying to input the body structure and uses a transformer based architecture that is capable of learning the appropriate (even dynamic) graph structure actually useful for control.	abstract
2021-505	I enjoyed the relatively simple experiments showing how the specific graph struture based on body morphology wasn't important at all.	abstract
2021-505	Although would be useful to know how many runs were performed given the noisy nature of RL.	suggestion
2021-505	Similarly the cyclical structure noticed in Fig. 6 definitely points towards the powerful nature of transformer architectures at learning this relations.	suggestion
2021-505	The strong performances (none of which seem to have converged yet) compared to baselines speak for themselves.	strength
2021-505	Although, again not clear about the number of seeds the experiments were repeated.	weakness
2021-505	The architecture description is somewhat unclear.	weakness
2021-505	Both actor and critic seem to have three parts to their architecture.	weakness
2021-505	But for a critic you need to output value information which might be scalar unlike decoder MLPs for independent node action.	weakness
2021-505	If there is some sort of aggregation going on, it needs to be clarified as to specifically how.	weakness
2021-505	Although Fig. 5 shows changing attention patterns, it doesn't warrant the confirmation that the proposed architecture benefits from "state-dependent message passing of transformers" which itself consists of two things.	weakness
2021-505	One can do state-dependent message passing in such architectures without the transformers (see DICG [1] for an example with attention and graph convolutions).	weakness
2021-505	Second, there could be a static graph structure which is better than the dynamic masks: the paper didn't actually perform the experiments to rule that out.	weakness
2021-505	Maybe the obvious morphology is the wrong graph structure but there is something else which would work better.	weakness
2021-505	Again, the paper's claim is probably true, but the causal language is not justified from Fig 5.	weakness
2021-505	[1] https://arxiv.org/abs/2006.11438 Edit: Updated score to reflect the changes from the revision.	strength
2021-505	This paper proposes that recent methods that used graphical neural networks to help solve the multitask reinforcement learning problem and assume that there's an advantage from being able to encode the agent's morphology using a graphical neural network do not provide additional generalization and benefits for learning.	abstract
2021-505	Instead, they claim that the benefits from being able to encode this morphology are counteracted by the difficulty in having to train the graphical neural network using the message passing system.	abstract
2021-505	This paper instead proposes to use Transformers as a simpler mechanism to be able to train and discover the helpful morphological distinctions between agents in order to better solve multitask reinforcement learning problems.	abstract
2021-505	The motivation that graphical neural networks are bogged down by their message passing framework is not necessarily a motivation for using transformers.	abstract
2021-505	There needs to be a separate motivation for why you want to use transformers and why they should perform better than GNNs or normal networks.	abstract
2021-505	The author claims that the SMP paper does not work better due to the morphology encoding and then they point out that it instead works because of the encoding of the subtrees and some specific detail related to message passing.	abstract
2021-505	This could be correct the explanation but the paper so far hasn't gone into enough detail for the reader to understand the importance of this message passing and how it works and how it is not improving training for GNNs.	weakness
2021-505	Figure one does provide some information related to how the transformer is used with respect to some morphology it would be far more helpful if this figure method was described well enough so that anyone can understand how to apply this to a different morphology.	weakness
2021-505	One of the challenges with reading and understanding this paper is the lack of information on how graphical neural networks are used and designed to understand later comments in the paper.	weakness
2021-505	There needs to be an ablation with respect to the residual connections added to the Transformer based network to make sure the improvement for amorphous is not working well just because of these residual connections.	suggestion
2021-505	While I do agree that training a graphical neural network to be able to produce a quality policy for a number of control tasks from the opening item environment is difficult the author of the paper might be missing at least one of the key points from the previous work in that you can learn a stronger modularization of policy.	weakness
2021-505	And that a goal of the SMP work was to understand how more modular policies or policies with modular components could be learned.	weakness
2021-505	It is stated in the paper that amorphous does better for state of the art incompatible continuous control?	weakness
2021-505	What is meant by incompatible continuous control?	weakness
2021-505	This term has not been defined anywhere in the paper and without this definition, it's difficult to understand the contribution this paper is making.	weakness
2021-505	----- Post Discussion ---- I have updated my rating for the paper after the authors have provided additional discussion and experiments to address my concerns.	weakness
2021-505	The manuscript studies the usefulness of Graph Neural Networks (GNN) in incomplete environments for Multitask Reinforcement Learning (MTRL).	abstract
2021-505	First, authors explore to what extend morphology information improves performance in GNN.	abstract
2021-505	By use of the Shared Modular Policies and the NerveNet methods, authors find that restricting morphology information does not improve performances.	abstract
2021-505	Based on this finding, authors apply GNN to fully connected graphs with memory/attention, i.e. the use transformers.	abstract
2021-505	Simulation results in different environments show that the proposed approach outperforms conventional methods.	abstract
2021-505	The paper is well written, and methods and analysis approach used are clear.	strength
2021-505	The selected approach and the authors findings are meaningful in a neuroscientific context, as the transformers approach better resembles the function of a human brain.	strength
2021-505	The contribution is original and relevant for the community.	strength
2021-505	The introduction does not clearly explain why the assumption that restricting the model and encoding morphological information may be beneficial.	weakness
2021-505	A weakness of the manuscript is the assessment of the results.	weakness
2021-505	No statistical tests were calculated to support the authors claims, nor is it clear how the performance of the different methods was compared.	weakness
2021-505	For example, the comparison f the curves depicted in Figure 3.	weakness
2021-505	It is also not clear to the reviewer what specifically the highlighted areas in the figures represents?	weakness
2021-505	Confidence intervals or standard deviation?	weakness
2021-505	Moreover, a few statements are vague.	weakness
2021-505	For example, what is a "good MTRL policy" (page 6).	weakness
2021-505	Overall an interesting approach that is expected to improve performance in MTRL and needs further exploring.	strength
2021-505	This work considers continuous control environments in which each agent limb (actuator) is associated with one action and a set of observation factors.	abstract
2021-505	As in prior work, the proposed policy class is modular, where each module is mapped to one limb (or the root), and the modules share information through some GNN message-passing schedule.	abstract
2021-505	The experimental results indicate that fully connected, transformer-style message passing is more effective in this setting than message passing restricted to directly connected pairs of limbs.	abstract
2021-505	Pros Good framing of the problem and choice of experiments.	strength
2021-505	Insightful discussion of related work.	strength
2021-505	Cons There is no discussion of the additional computational requirements of transformers over SMP.	weakness
2021-505	The results would be stronger if hyperparameters had been systematically tuned.	weakness
2021-505	Questions Why were no results provided for the Walker-Hopper or Walker-Hopper-Humanoid combinations tested by Huang et al?	weakness
2021-505	Suggestions The paper mentions in passing that this work involves agents "with each non-torso node having an action output".	suggestion
2021-505	This limitation probably deserves to be highlighted more prominently.	suggestion
2021-505	As the paper explains, "transformers can be seen as GNNs operating on fully connected graphs".	suggestion
2021-505	In other places, the paper contrasts transformers with GNN-based methods ("substantially outperforms GNN-based methods"), as if transformers were not GNNs. To avoid confusing readers, it would help to explain that GNNs are a broad class that includes both transformers and SMP, which differ in their message passing schedules, etc.	suggestion
2021-505	It would help to have a more detailed description of the limb torques and observation factors, so that readers don't have to look in Huang et al (section 4) for these details.	suggestion
2021-505	The following phrases are important but unclear for readers who are not very familiar with GNNs or transformers:  "Having an implicit structure that is state dependent is one of the benefits of AMORPHEUS."  and  "the implicit state-dependent message-passing schema learnt by AMORPHEUS can be better"	weakness
2021-505	The paper says "We use entity to denote both vertices and edges." But the term "entity" appears nowhere else in the paper.	weakness

2021-560	Summary: The authors introduce a framework for sufficient conditions for proving universality of a general class of neural networks that operate on point clouds which takes as input a set of coordinates of points and as output a feature for each point, such that the network is invariant to joint translation of the coordinates, equivariant to permutation of the points and equivariant to joint SO(3) transformations of the coordinates and output features of all points.	abstract
2021-560	Notably, this class contains Tensor Field Networks (TFN).	abstract
2021-560	The authors accomplish this by writing the network as a composition of an equivariant function from a class F_feat and followed by a linear pooling layer.	abstract
2021-560	When the F_feat class satisfies a "D-spanning" criterion and the pooling layer is universal, the network is universal.	abstract
2021-560	For a simple class of networks and for TFNs, the authors prove D-spanning.	abstract
2021-560	Linear universality of the pooling layer follows from simple representation theory.	abstract
2021-560	Strengths: It is useful to know whether prevalent classes of neural networks are universal	strength
2021-560	The authors use a general construction for proving universality of equivariant networks for the point cloud group, rather than being specific to certain architectures.	abstract
2021-560	Reading the proofs along with the main text, the argumentation is clear and relatively easy to follow for me as reviewer, unfamiliar with similar universality proofs.	strength
2021-560	Weaknesses: The paper would benefit from providing more intuition behind the proposed constructions, lemmas and theorems, in particular this holds for: theorem 1 based on the split between the linear universality and D-spanning; the construction of Segol & Lipman (2019) and how this relates to the Q functions and lemma 2	weakness
2021-560	In addition to the previous point, the proofs, currently critical for understanding the paper, are given in the appendix, which is not ideal for the self-containedness of the main paper.	weakness
2021-560	As the authors of the TFN paper note: in practice not all higher order irreps of the tensor product of the filter and the features are computed.	weakness
2021-560	This seems to indicate a big difference between the theoretical analysis - which includes all irreps and thus is computationally intensive even when modelling low order polynomials – and the practical application of TFNs. It would be interesting to know how expressive such practical low order TFNs are.	weakness
2021-560	Another difference between the described networks and practical TFN is that in the described networks, all relevant parameters are in the pooling layer, which sums a large number of terms (looking at the proof of lemma 2, exponential in D), while in practical TFNs, the parameters are in the filters.	weakness
2021-560	Recommendation: The authors proof the useful statement of universality of a prominent class of neural networks, which is why I recommend the acceptance of this paper.	decision
2021-560	Suggestion for improvement: Make the big picture clearer by providing more intuition.	suggestion
2021-560	Comment on the differences between the class of networks described and TFNs used in practice.	suggestion
2021-560	Minor points/suggestions: P3 Add definition of W^n_T as n direct sums of W_T	suggestion
2021-560	P3 "where W_feat is a lifted representation of SO(3)", what does lifted representation here mean?	suggestion
2021-560	Just any rep? I get a bit confused by the wording in Def 1.	weakness
2021-560	Unless I am mistaken, it appears like the quantifiers are reversed.	weakness
2021-560	Should it mean "for every polynomial …, there exists f_1, … in F_feat and linear functionals Lambda_1, …, : W_feat -> R "?	weakness
2021-560	Around Eq 5, perhaps the authors could clarify the clarify the domain of the Q functions, which I suppose is Q^r : R^3n -> T_T, where T=||r||_1	suggestion
2021-560	Around Eq 7, are X_j and x_j the same?	suggestion
2021-560	In lemma 4, is A_k any linear map or an equivariant linear map?	weakness
2021-560	In Appendix B, perhaps a new subsection B.2 would make sense before theorem 1?	suggestion
2021-560	In the proof of thm 1, it says "p: R^{d \\times n} \\to W_T", should that be W_T^n?	suggestion
2021-560	In the proof of lemma 2, it says "we see that that exists a linear functional"	weakness
2021-560	Post rebuttal I thank the authors for their response and revised version, which has been improved notably with the inclusion of the proof ideas.	rebuttal_process
2021-560	My previous rating still applies.	rebuttal_process
2021-560	This paper mainly explores the representation ability of invariability of a point cloud network from the theoretical perspective.	abstract
2021-560	The universal approximation property for equivariant architectures under shape-preserving transformations is discussed.	abstract
2021-560	First, the authors derived two sufficient conditions for equivariant architectures with the universal approximation property.	abstract
2021-560	Then, they examined two methods based on the Tensor Field Network to prove that such a property holds for both of them.	abstract
2021-560	At last, the authors propose alternative methods which also satisfy the universal approximation property.	abstract
2021-560	This paper is full of theoretical analysis, which is based on investigating the equivariant polynomials in the group theory.	abstract
2021-560	The proofs are similar to the previous works, except that more transformations, including rotation, translation and permutation, are considered together.	weakness
2021-560	However, the proof of the rotation equivariance has been discussed in previous works, and the theorems for translation and permutation are much easier than the rotation analysis.	weakness
2021-560	Moreover, the proposed method based on TFN is simply modified by the authors, resulting in alternative architectures which also satisfies the universal approximation property.	weakness
2021-560	However, this paper fails to provide any numerical experiments to demonstrate the performance and the influence of parameters \\theta.	weakness
2021-560	pros: provide sufficient conditions for equivariance of shape-preserving architectures to satisfy the universal approximation property prove two methods based of Tensor Field Networks that satisfy the universal approximation property raise two simple models based on the TFN	strength
2021-560	cons: This paper provides complete poofs to the TFN network theoretically, but lacks auxiliary experimental verification.	weakness
2021-560	Therefore, it is difficult to verify the correctness and feasibility of the proofs.	weakness
2021-560	The authors do not provide experimental results to demonstrate the performance of the proposed new methods.	weakness
2021-560	This work is a theoretical paper investigating the sufficient conditions for an equivariant structure to have the universal approximation property.	abstract
2021-560	They show that the recent works: Tensor Field networks and Fuchs et al., are universal under the proposed framework.	abstract
2021-560	A simple theoretical architecture is presented as another universal architecture.	abstract
2021-560	Pros: Achieving rotation equivariance is important to point cloud network as it is the key to improve the expressiveness of point cloud features.	strength
2021-560	Hence the study of the universality of network with such property is important to the community.	strength
2021-560	Overall, the proposed proof looks plausible to me.	strength
2021-560	A minimal universal architecture is proposed that satisfies the D-spanning property.	strength
2021-560	This provides the theoretical starting point to design a more advanced and complex equivariant point cloud network.	strength
2021-560	Cons: The paper is quite difficult to follow.	weakness
2021-560	I'm not an expert in group theory and had a difficult time understanding some of the theorems and proofs.	weakness
2021-560	It would be great if the writing can be broken down into more fundamental modules and provide more illustrations.	weakness
2021-560	In addition, the paper doesn't provide any evaluation of the proposed new universal architectures.	weakness
2021-560	Though this is a theoretical paper, it would be nice to show the proposed theory have some practical use.	suggestion
2021-560	For instance, it would be great to provide a simple implementation of the minimal universal architecture and show it indeed achieves the rotation equivariant features on the point cloud data.	suggestion
2021-560	That would make this work much stronger and more practical.	suggestion
2021-560	Minors: There are some typos in the inset figure on page 1: "Equivarint" -> "Equivariant".	weakness
2021-560	Summarize what the paper claims to contribute.	suggestion
2021-560	The authors claim to: (1) introduce a general approach for proving universality of rotation-translation-permutation equivariant models for point clouds; (2) prove universality of two recent rotation equivariant point cloud networks and (3) introduce two rotation equivariant architectures for point cloud processing	abstract
2021-560	Strengths: The paper is well organized and both the language and notation are clear	strength
2021-560	The authors consider equivariant representation learning which is of growing interest to the community	abstract
2021-560	The authors present theoretical insights for the success of recent architectures	abstract
2021-560	The insights themselves are leveraged to support the introduction of two new architectures	abstract
2021-560	Weaknesses: The novel architectures are described but not evaluated; it is therefore unclear what impact the simplification will have in practice	weakness
2021-560	It might be nice to point out rotation equivariant architecture that is not universal	suggestion
2021-560	Clearly state your recommendation. Accept. See Strengths. Arguments for your recommendation.	decision
2021-560	Theoretical issues in deep learning is a relevant topic area for *CONF*.	suggestion
2021-560	This paper provides a theoretical framework for interpreting the success of deep learning frameworks for equivariant representation learning on point clouds.	abstract
2021-560	Moreover, the paper leverages the insights gathered to propose two novel approaches.	strength
2021-560	The paper is written in a clear and accessible way.	strength
2021-560	Possible typos: (just before Thm. 1) When these two necessary → when these two sufficient	weakness
2021-560	(just after Lemma 3)  linear function → linear functions	weakness
2021-560	Post rebuttal With consideration of the authors' responses to reviewer questions and revisions to the submitted work I have changed my rating to clear accept.	decision

2021-624	Summary: This paper investigates the role of scale in generalization of neural networks.	abstract
2021-624	It shows that the initial scale of a 2-layers MLP, with sinusoid or ReLU activation, can control the memorization behavior of the network, from very little overfitting to complete memorization.	abstract
2021-624	It then proposes an alignment measure which correlates with generalization for different initial scale.	abstract
2021-624	It shows that this alignment measure can capture generalization performances for other architecture such as ResNet or DenseNet on the CIFAR-10 dataset.	abstract
2021-624	Reasons for score: Overall, I find the paper to be a bit borderline.	rating_summary
2021-624	The observation regarding the scale impacting generalization is novel and interesting as I would have assumed that large initial scale would lead to bad optimization rather than a lack of generalization.	strength
2021-624	However, all the experiments regarding the scale are carried out on a two-layers MLP models and it is not clear to me if similar conclusion would be true for deeper architecture.	weakness
2021-624	Pros: interesting observation regarding the impact of the initial scale on generalization clearly show the effect in a two-layers MLP with various activation and loss functions propose an alignment measure which have some promising correlation with generalization	strength
2021-624	Cons: experiments investigating the impact of the initialization scales only for two layers MLPs	weakness
2021-624	Alignment is not compared with other generalization metrics in section 5.	weakness
2021-624	Overview The paper studies how the generalization of the neural network trained with SGD is affected by the scale of the random initialization.	abstract
2021-624	Specifically, that when the scale of the initialization is big, the network overfits to the training set with bad performance on the test set.	abstract
2021-624	The paper also provides a hypothesis why does it happen -- that the graidents of difference examples are orthogonal in the "bad" mode and proposes a measure called "alignment" to predict the generalization reghime of the network.	abstract
2021-624	The problem, tackled in paper is interesting and the paper itself is thought-provoking.	strength
2021-624	The central model studied is 2 layer fully-connected neural network with {sin, ReLU} activation, where the 1st layer is initialized with variable (studies) scale and the 2nd - with Xavier init.	strength
2021-624	Both layers are without biases.	strength
2021-624	The model is optimized with SGD w/o momentum and constant learning rate and the loss function is one of the classification losses: CE, hinge loss.	strength
2021-624	Strong points The paper formulates hypothesis and verifies it via series of controlled experiments.	strength
2021-624	Besides "toy" 2-layer model, the similar results are get with more powerful architectures, like CNN, DenseNet and so on on the set of middle-sized datasets like SVHN and CIFAR.The paper clearly states its place among related works and proposes a useful metric for diagnosing model training.	strength
2021-624	Questions (and possible weak points	misc
2021-624	Figure 2 shows that the norm(grad)/norm(weight) decreases significantly with the scaling-up the initalization.	abstract
2021-624	This is hypothesized to be one of the problems with large scale initialization: the weights do not go far from the original (random) values.	abstract
2021-624	If this is the case and the problem, may be scaling up the learning rate with the initalization to keep the norm constant, would help?	weakness
2021-624	Why is this an important question?	weakness
2021-624	Because the problem with large init is the bad gradient direction (as hypothesized in paper), then scaling the learning rate would not help.	weakness
2021-624	If, otherwise, the problem is the learning rate scale, then the work could be seen as indirect confirmation of the works about large-vs-small learning rate reghimes, e.g. Lewkowycz et.al.	weakness
2021-624	https://arxiv.org/pdf/2003.02218.pdf Moreover, may be then we should study not the scale of init, but the ratio (init scale/lr).	weakness
2021-624	Following Q1, one also could use different learning rates for different layers depending on the scale of weights and/or activation.	weakness
2021-624	Yes, that would significantly complicate the experimental setup, but may lead to the different conclusions.	weakness
2021-624	E.g. it is commonly know that deep sigmoid networks are hard to train because of vanishing gradient issue.	weakness
2021-624	However, as it was shown in "Revise Saturated Activation Functions" Xu et.al (https://openreview.net/forum?id=D1VDjyJjXF5jEJ1zfE53), one could perfectly fine train such networks.	weakness
2021-624	The only thing, which is needed, is the proper rescaling of the learning rate per layer, up to ridiculous values such as 4^11.	weakness
2021-624	(minor) why do you don't use bias?	weakness
2021-624	Bias-less NNs are less common and give worse results.	weakness
2021-624	(just curious) Where is the actual learning with large scale init take place - in layer 1 or 2?	weakness
2021-624	If only one layer is actually trained, can we obtain the same results, when the non-training layer weights are frozen?	weakness
2021-624	When examining Figures in appendix (e.g. 17, 20, etc), it looks like that proposed "alignment" measure can be used as alarm -- if it is low, the generalization is low, but the ranking of (test accuracy) and (alignment) is not really aligned.	weakness
2021-624	Any comments on that? Overall, I like the paper, although would also like to have answers to my questions.	misc
2021-624	Update after rebuttal All my concerns have been addressed, including the possible alternative explanation of the experimental results.	rebuttal_process
2021-624	I strongly recommend the paper to be accepted.	decision
2021-624	Summary of paper: A series of empirical observations are made about the influence of scale of init on generalization (in particular, that a continuum of generalization performance from random to very good can be generated by varying only the scale of init) , and these effects are explained in detail for different activation functions.	abstract
2021-624	The authors also propose a measure of gradient alignment which they show correlates with generalization performance	abstract
2021-624	Pros/strong points: detailed explanations for each activation function provide nice insights solid experiments	strength
2021-624	Cons/weak points: overall clarity and presentation of information is the largest weakness in my opinion, although the writing is generally good.	weakness
2021-624	I think it just needs a few more passes, with an eye to making sure things are accessible/understandable/flow.	suggestion
2021-624	Could be improved substantially just with formatting/subsections or something, e.g. italicizing key insights or making sub paragraphs where each gives a particular insight some small things in related work	suggestion
2021-624	Summary of review + recommendation: Overall I think this is a good paper, and could be a very good paper with some "tightening up" and clarifications.	strength
2021-624	The combination of things is too much for me to recommend acceptance out of the box, but the things are relatively small and I think easy to address, and I'd be happy to increase my score.	decision
2021-624	Detailed review and Specific questions/recommendations: unclear what "large scale training" means	weakness
2021-624	"engendering" is an unnecessary word there observations about overparameterized models should be cited, e.g. Zhang et al. and Arpit et al. Chizat & Bach further observe (not observes), same incorrect pluralization with many citations (suggest checking the whole document)	weakness
2021-624	background work portion of the intro misses works, some poorly explained / relationship to current work not discussed, and overall feels rushed.	weakness
2021-624	The Related Work section does a good job mostly though.	weakness
2021-624	I suggest moving the 3rd p of the intro into the related work, moving the first section of it about scale of init to the first sentence of Contributions.	suggestion
2021-624	Merging them should get you some extra space for more experiments/larger figs.	suggestion
2021-624	Related work on inits should cite lottery ticket works (e.g. Frankle et al)	suggestion
2021-624	Geiger et al reference you describe what they do but not what to take away from it extreme memorization should be bolded since it's a term you're defining (and make clear if you're proposing this term and if not, where it is from), but you then  define memorization the same way you define extreme memorization, making this term ("extreme") seem unnecessary.	weakness
2021-624	"from verylittle overfitting to perfectly memorizing the training set while making zero progress on test error" this sentence is unclear, makes it sound like "while..." applies to both of the 2 extremes.	weakness
2021-624	Suggest rephrasing. I find 2nd bullet of contributions unclear about what is expected vs.	weakness
2021-624	what happens and what we learn from that in 3rd bullet briefly summarize the alignment measure.	weakness
2021-624	I suggest using a different/more precise term for this measure (e.g. gradient alignment) since just "alignment" means so many things already	suggestion
2021-624	In "related statistics" you don't mention if Chatterjee has a measure for coherence of gradients (I skimmed that paper and it seems not, but I'm not sure).	weakness
2021-624	If not, then maybe calling this a measure of gradient coherence would be appropriate?	weakness
2021-624	I googled it quickly and it seems that coherence means something specific in linear algebra and signal processing: (https://en.wikipedia.org/wiki/Mutual_coherence_(linear_algebra), https://en.wikipedia.org/wiki/Coherence_(signal_processing))	weakness
2021-624	- maybe not necessary to comment on in the paper, but if the authors are familiar with this use of the term I'd appreciate a clarification of how it relates to the mentioned measures of alignment I'd be interested	weakness
2021-624	"related statistics" should also mention Arpit et al critical sample ratio (which does take class information into account) and comment on differences (or if it's too different to include here, I'd appreciate an explanation of why)	weakness
2021-624	Mention computational cost of the different measures of alignment homogeneity is repeatedly mentioned without explanation (just a brief 1-line would do)	weakness
2021-624	"fix the scale" ambiguous whether this means fix in place (at a particular value) or fix as in correct seems obvious to me that the scaling would affect relus (especially in the absence of bias as your experiments say);	weakness
2021-624	if the scale is larger, fewer values are initialized near the non-linear region of the relus, meaning there are more 'dead relus' near the beginning (which I would guess up to a point could provide regularization, but past that point would just make learning slow and even unstable).	weakness
2021-624	Could the authors comment on this; do you think it's correct/relevant?	suggestion
2021-624	How does it fit with the argument about homogeneity?	suggestion
2021-624	State important equations in words as well as math for clarity and ease of reading (as is done for eq.6; make sure this is done consistently, especially important for your proposed measures of alignment).	suggestion
2021-624	Conclusion discusses the results strangely, without mentioning the actual results (e.g. "making it particularly interesting" - why/how is it interesting, what are the implications for people using sin?, "the loss function plays a crucial role" what role?	weakness
2021-624	what things are good for what, what should I look out for?).	suggestion
2021-624	The conclusion should stand on its own and summarize results, not reference them in a way that requires me to have read the whole paper to understand.	weakness

2021-635	The paper studies generalization properties of preconditioned gradient descent on linear/kernel regression problems.	abstract
2021-635	The main preconditioner that is studied in addition to vanilla GD is the (population) Fisher matrix (natural gradient descent or NGD), its empirical counterpart, and its interpolation with GD.	abstract
2021-635	The authors first consider the "ridgeless" regression setup in high-dimension, where the estimator corresponds to the limiting gradient flow iterate, and show that NGD leads to a smaller (and optimal) variance term, and can improve the bias term compared to GD particularly in the presence of strong misspecification.	abstract
2021-635	Among others, the authors also consider early-stopping in a non-parametric RKHS setup, showing that an appropriate interpolation between NGD and GD achieves optimal rates with a much smaller number of steps compared to GD, a difference which becomes larger for "difficult" problems (which require more weight on the Fisher preconditioner).	abstract
2021-635	The findings are further illustrated with simple experiments on neural networks.	abstract
2021-635	Overall, the paper provides a comprehensive study of the impact of preconditioning/second-order methods/natural gradient on generalization by giving a precise analysis in tractable regression settings, which illustrate conditions under which preconditioning is or is not useful for better generalization.	abstract
2021-635	This makes the paper a strong contribution, and I am in favor of acceptance.	decision
2021-635	comments/typos: section 3: 'population risk' -> should this be excess risk given the presence of noise?	weakness
2021-635	add a reference or some more details on the bias-variance decomposition?	suggestion
2021-635	the last sentence in section 3.2 "in the analogy..." could be clarified end of p.5 "lower bias compare to" -> "compared to"	suggestion
2021-635	Prop. 6: first part with theta_P holds for any P?	suggestion
2021-635	please specify theorem 7: specify conditions on eta?	suggestion
2021-635	some comments on computational difficulties of the full preconditioner would be welcome.	suggestion
2021-635	Would a diagonal preconditioner, as often used in deep learning, provide any (partial) benefits as in the full-matrix case presented here?	suggestion
2021-635	Update after rebuttal Thank you for the clarifications.	rebuttal_process
2021-635	A couple minor comments: regarding theorem 7, my comment was that it would be useful to include the conditions on eta in the theorem statement in the main text (though I do not feel strongly about it)	suggestion
2021-635	regarding "misalignment" and the relationship between the random effects model and the source condition, I appreciate the improved explanation of this analogy, but I still find that the last paragraph in section 3.2 could do a better job at providing the right intuition (skimming through the Richards et al. reference pointed out by R4 gave me a better intuition).	weakness
2021-635	Summary: The paper studies the effects of preconditioning on generalization properties in deep learning.	abstract
2021-635	By using a bias-variance decomposition of the expected risk, the paper determines optimal precondition matrix P for bias and variance.	abstract
2021-635	Then the paper analyzes the generalization performance via the aspects: clean labels, well-specified model and aligned signal.	abstract
2021-635	Finally, it extends the analysis to the reproducing kernel Hilbert.	abstract
2021-635	Pros: The theoretical results provide guidelines of choosing precondition matrix for practical problems.	strength
2021-635	In particular, by decomposing the risk into a sum of a bias and a variance, the paper addresses the following points: The asymptotic result on the variance (Theorem 1) implies that NGD achieves the minimal variance at stationary points, that suggests using NGD in the case where the variance term dominates.	strength
2021-635	Theorem 2, on the other hand, provides the asymptotic result on the bias and the optimal precondition matrix for the bias to reach minimal value at stationary.	strength
2021-635	Based on the results on the variance and the bias, Proposition 4 suggests an interpolating scheme between NGD and GD that aim at achieving better stationary risk than NGD or GD.	strength
2021-635	The efficiency of this scheme is demonstrated in a least squares regression with the regular RKHS, where the interpolating scheme achieves the optimal convergence rate in fewer step than GD.	abstract
2021-635	Cons: The paper contains a number of unclear / undefined terms such as well-specified and aligned signal, that make it difficult to read.	weakness
2021-635	The paper uses a lot of vague and unverified claims / statements which are usually the explanations after each theorem / proposition.	weakness
2021-635	For example, after theorem 1, it says that "Theorem 1 implies that preconditioning with the inverse population Fisher results in the optimal stationary variance...	weakness
2021-635	In other words, when the labels are noisy so that the risk is dominated by the variance term...	weakness
2021-635	We emphasize that this advantage is only present when the population Fisher is used, but not its sample-based counterpart".	weakness
2021-635	For me, it would be more clear if these statements could be explained in detail.	weakness
2021-635	The paper is not well-organized.	weakness
2021-635	For me, it is a collection of results that are unconnected.	weakness
2021-635	For example, after reading the analyses of bias and variance, I have no idea how they support the study of generalization or why section "3.3 misspecification" is placed along with bias and variance analyses, etc.	weakness
2021-635	I am not saying these results are irrelevant, however, there should be a better way of arranging / writing them so that they can support well the ideas of the paper.	weakness
2021-635	Summary: The authors theoretically study the prediction performance of pre-conditioned gradient descent/flow with linear models and squared loss aligning in the setting of least squares regression and non-parametric regression.	abstract
2021-635	For parametric least squares, the predication performance of the limiting solution for preconditioned gradient flow i.e. time goes to infinity, is studied in an asymptotic regime where both the number of samples and dimension go to infinity in proportion to one another.	abstract
2021-635	Meanwhile for non-parametric regression, source and capacity assumptions are leveraged to achieve finite sample guarantees.	abstract
2021-635	Experiments are also conducted on neural networks in a student and teacher setup.	abstract
2021-635	Summary of main Contributions: A1) In the case of parametric least squares, an asymptotic characterisation of the test risk is utilised to study the limiting solution of preconditioned gradient flow.	strength
2021-635	Preconditioning with the inverse Fisher information matrix (covariates population covariance) is shown to achieve the optimal variance among preconditioned updates (Theorem 1).	strength
2021-635	Meanwhile for the asymptotic bias, the optimal pre-conditioner depends upon the covariance of the ground truth parameter.	abstract
2021-635	In a mis-aligned case, where the ground truth covariance is equal to the inverse of the population covariates covariance, the optimal pre-conditioner for the bias aligns with the inverse Fisher information matrix (Theorem 2).	abstract
2021-635	A2) In the case of an Isotropic covariance for the ground truth parameter, it is found that the Bias and Variance can be traded-off by interpolating between the two aforementioned pre-conditioners (Proposition 4).	abstract
2021-635	A3) For non-parametric regression, gradient descent pre-conditioned with the inverse regularised population covariates covariance is considered.	weakness
2021-635	Mini-max optimal statistical rates are achieved with a number of iterations that grows logarithmically in the data set size i.e. linear convergence (Theorem 7).	abstract
2021-635	A4) Experiments for neural networks are conducted in support of A1).	abstract
2021-635	Specifically, gradient descent pre-conditioned with the Fisher information matrix achieves better generalisation performance when the noise is large or the model is misaligned (Section 5).	abstract
2021-635	A5) For parametric least squares with a mis-aligned ground truth parameter, it is shown that early stopping with NGD achieves lower Bias than any other pre-conditioned gradient descent (Proposition 6).	abstract
2021-635	Pros: B1) I feel contribution A1) in conjunction with A5) is novel and offers a precise interpretation of when pre-conditioning with the inverse Fisher information matrix can yield an improvement in performance.	strength
2021-635	B2) Contribution A2) is also interesting and can point towards understanding and controlling the implicit bias of gradient descent through the pre-conditioner i.e. taking a linear combination of two pre-conditioners.	strength
2021-635	B3) Contribution A4) supports the findings in A1) in a setting beyond least squares.	strength
2021-635	Concerns: C1) The authors do not compare their theoretical results for non-parametric regression (contribution A3) ) to prior work within the literature.	weakness
2021-635	Specifically, reference [1] where the generalisation performance of a pre-conditioned gradient method is considered.	weakness
2021-635	To remedy this, I feel the authors should discuss how their theoretical results and proof method differ from [1] as well as the novelty of their approach.	suggestion
2021-635	C2) The theoretical results and discussion focus on a particular type of pre-conditioner: the inverse population covariates covariance and transforms thereof.	weakness
2021-635	This limits the applicability of the insights as this quantity is often not known in practice.	weakness
2021-635	Similarly, the experiments are in a setting where Fisher information is estimated accurately using 100,000 samples while training uses 1024 samples.	weakness
2021-635	In contrast, prior work for non-parametric regression considers pre-conditioners involving estimates of the population covariance [1].	weakness
2021-635	To remedy this, I feel the authors should include a discussion on how their insights i.e. A1), A2) are impacted when the population covariance is swapped for an estimate (using unlabelled data).	suggestion
2021-635	C3) The manuscript can be difficult to read.	weakness
2021-635	For instance, the authors start with a time varying pre-conditioner while all pre-conditioners considered are constant in time.	weakness
2021-635	Tools from random matrix theory and regularity assumptions for non-parametric regression are introduced with little discussion.	weakness
2021-635	Section 3.3 "Misspecficiation \\approx Label Noise" considers misspecification that is independent and gets interpreted as additional noise.	weakness
2021-635	It is not clear what this brings to the manuscript in terms of insights and introduces another layer of complexity.	weakness
2021-635	C4) For parametric least squares regression the results focus on three cases for the ground truth covariance: well-aligned (where it equals the covariates population covariance), mis-aligned (where it equals the inverse covariance population covariance) and Isotropic.	weakness
2021-635	Whereas the theoretical results allow for a more general ground truth covariance to be considered.	weakness
2021-635	It would be natural to follow the source conditions from non-parametric regression and investigate natural gradient descent when the ground truth covariance is not fully well- or mis-aligned.	weakness
2021-635	General Comments: -Remark on page 4 states "we demonstrate generalisation properties only possed by the population Fisher", clarify which properties are /only/ held the population Fisher versus Sample Fisher.	weakness
2021-635	-In Proposition 4 possibly change the description "interpolating preconditioners" as all the preconditioned methods are interpolating the data, and thus, can be confusing.	weakness
2021-635	-Proposition 4 states for pre-conditioners (ii) and (iii) the bias is monotone for α in some range depending upon the covariates population covariance.	weakness
2021-635	What is the range of α and is the risk increasing or decreasing?	weakness
2021-635	What conclusions are we to draw from this part of the result?	weakness
2021-635	-In Figure 6 and Figure 23 how is "geometric" and "additive" interpolation defined ?	weakness
2021-635	-More discussion around Proposition 6 would be helpful.	weakness
2021-635	For instance, in the statement of the result what is choice of P ?	weakness
2021-635	The analysis is described as difficult, although no details are provided into how this result was obtained.	weakness
2021-635	Within the proof why is the ratio of Eigenvalues \\overline{\\lambda}{min}/\\widehat{\\lambda}{min} is bounded, and how many iterations are required until NGD is below, say, standard gradient descent?	weakness
2021-635	-In Section 5, the misalignment experiment in Figure 7 is conducted for MNIST but not CIFAR-10, with no discussion in the main body of the manuscript for why this is.	weakness
2021-635	Although, paragraph "Misalignment" in Appendix C.3 states the phenomena of NGD outperforming GD in the misaligned case is "...	weakness
2021-635	difficult to observe in practical neural network training on real-world data".	weakness
2021-635	The authors then go on to state that, in short, this is due to (see Appendix A) NGD moving parameters further from initialisation, and thus, no longer well described by a linear model i.e. NTK.	weakness
2021-635	Is there a link between this discussion within the Appendix and the experiments?	weakness
2021-635	-A Summary at the start of Appendix A to describe contents of A1-A4 would improve readability.	suggestion
2021-635	Similarly, for Appendix C and D.	suggestion
2021-635	-In the proof of Theorem 2 (Appendix D.2) some details on how to get from (ii) to (iii).	suggestion
2021-635	[1] - Rudi, A., Carratino, L., and Rosasco, L.	misc
2021-635	"Falkon: An optimal large scale kernel method",  Advances in Neural Information Processing System 2017.	misc
2021-635	POST REBUTTAL EDIT: I thank the authors for providing detailed answers regarding my concerns.	rebuttal_process
2021-635	I have updated my score in light these comments.	suggestion
2021-635	Below are some additional comments in response.	misc
2021-635	Response to comments regarding C1) and C2): While early stopping with pre-conditioned updates differentiates this work from (A.	rebuttal_process
2021-635	Rudi et. al 2019), the analysis still requires the knowledge of the population covariance.	rebuttal_process
2021-635	Indeed, while the authors have included a section (Appendix A.3) showing that the operator norm of the population and the inverse regularised empirical covariance can be controlled, it would be insightful to discuss to what extent this allows the analysis for the pre-conditioned gradient descent to be extended to an approximated population covariance.	rebuttal_process
2021-635	Response to comment regarding C3): I am inclined to agree with reviewer 3, in that the manuscript is difficult to read due to the larger number of fragmented results.	rebuttal_process
2021-635	In this regard, I feel the authors should focus on a single phenomenon that is supported by both the parametric and non-parametric aspects of the paper, for instance, how pre-conditioning helps against misalignment.	suggestion
2021-635	Response to comment regarding different prior on ground truth (point 4.	rebuttal_process
2021-635	third bullet point): Note that some concurrent works have studied the case of different priors on the ground truth [2,3], which are likely relevant in this case.	rebuttal_process
2021-635	Minor Comment: The pre-conditioned updates for non-parametric regression (4.1) use notation α where as Appendix D.8.1 uses notation λ, with the discussion then switching back to using α and λ being used in reference to the regularisation used within FALKON.	weakness
2021-635	The switching of notation is possibly confusing here.	weakness
2021-635	[2] - D. Richards, J. Mourtada, L.	misc
2021-635	Rosasco "Asymptotics of Ridge (less) Regression under General Source Condition", arXiv:2006.06386 (2020)	misc
2021-635	[3] - Wu, D. and Xu, J.	misc
2021-635	"On the Optimal Weighted ℓ2 Regularization in Overparameterized Linear Regression" NeurIPS 2020	misc

2021-680	Summary Using backward error analysis, the paper argues that SGD with small but finite step sizes stays on the path of a gradient flow ODE of a modified loss, which penalizes the squared norms of the mini-batch gradients.	abstract
2021-680	This offers a possible explanation of the empirically observed positive effect of (relatively) large step sizes on generalization performance.	abstract
2021-680	The paper further contests previous findings based on a vanishing step size assumption.	abstract
2021-680	Rating Similar to several recent works, this paper tries to explain certain aspects of stochastic gradient descent using a continuous time approximation.	abstract
2021-680	In contrast to existing works, it explicitly accounts for the effect of finite step sizes, which I think is a very interesting direction and surfaces several interesting aspects.	strength
2021-680	I also welcome and endorse the critical discussion of prior work based on infinitesimal step size assumptions.	strength
2021-680	Overall, the paper was interesting and pleasant to read.	strength
2021-680	To the very best of my knowledge, all mathematical derivations are technically correct.	strength
2021-680	However—as the authors themselves note in their critique of SDE approximations to SGD—the devil is in the details with continuous time approximations.	rebuttal_process
2021-680	In my opinion, that makes is absolutely crucial to discuss the scope of the results carefully and transparently, including a critical discussion on assumptions made and simplifications that go into the continuous-time model.	strength
2021-680	In my opinion, this paper fails to deliver that, which is why I recommend rejection.	decision
2021-680	Below, I am asking for clarification on various points and would encourage the authors to respond to the major points in the rebuttal phase.	suggestion
2021-680	Major Comments The main result says that the expected SGD iterate after a single epoch lands close to the path of a gradient flow ODE on a modified loss.	weakness
2021-680	Unless I am missing something, this fundamentally fails to capture the behavior over multiple epochs.	weakness
2021-680	The analysis only guarantees that, from any given starting point ω0, the expected iterate after one epoch of SGD ends up close to the ODE path starting from ω0.	weakness
2021-680	Unless I am missing something, this does not imply that two epochs of SGD starting from ω0 end up on that path.	weakness
2021-680	We can not simply chain two epochs together: The first epoch only stays on the path in expectation, but any realization of that random variable will deviate from the path, which affects the initial condition of the next epoch.	weakness
2021-680	Intuitively, one needs to get a handle on the variance of the iterate as well in order to give guarantees for multiple epochs.	weakness
2021-680	Is this understanding correct? If so, to what extent can insights about a single epoch of SGD be transferred to practical settings?	weakness
2021-680	Comment (1) hints at a larger (but vague) point that the paper is trying to characterize a stochastic optimization procedure with a solution of a deterministic gradient flow ODE.	weakness
2021-680	It does so by focusing on the expectation of the iterate, which might be an approach to highlight certain aspects, but it will never give a full picture.	weakness
2021-680	Why wouldn't we also be interested in the covariance of the iterates?	weakness
2021-680	The limitations of this characterization should be discussed thoroughly in the paper.	weakness
2021-680	In Section 2, the composition of the minibatches is assumed to be fixed and the randomness only comes from their ordering.	weakness
2021-680	The paper says: "It is standard practice to shuffle the dataset once per epoch, but this step does not affect our analysis and we omit it for brevity." I don't think that statement is justified with respect to the result in Eq.	weakness
2021-680	(1), given that the modified loss depends on the minibatch composition.	weakness
2021-680	Therefore, would we reshuffle the dataset after each epoch, the modified loss would change from one epoch to the next.	weakness
2021-680	Later, in Section 3, the expectation is additionally taken over the composition of the batches.	weakness
2021-680	Why is the result presented in these two distinct steps?	weakness
2021-680	None of the key findings of the paper seems to rely on the intermediate fixed-composition result.	weakness
2021-680	It also doesn't reflect the common practice of reshuffling the entire dataset and then traversing it, which simultaneously randomizes the composition and ordering of batches.	weakness
2021-680	So why not give the result of Eq. (22) directly?	weakness
2021-680	It is also the more intuitive result, invoking the trace of the gradient covariance matrix, which also appears in prior work on continuous time approximations of SGD.	weakness
2021-680	While the analysis tries to account for finite step sizes, it still seems to assume step sizes that are orders of magnitude smaller than those used in practice.	weakness
2021-680	In particular, when going from Eq. (12) to Eq.	weakness
2021-680	(13), each minibatch cost function is equated with its second-order Taylor approximation around the starting point ω0.	weakness
2021-680	This is a drastic approximation and I don't see any justification for why this should be anywhere near accurate for practical settings.	weakness
2021-680	For large datasets and moderate batch sizes, the number of updates in one epoch will be in the thousands.	weakness
2021-680	For realistic step size choices, a second-order Taylor expansion around the starting point will probably be rather poor after a handful of SGD updates, no?	weakness
2021-680	The paper strongly emphasizes the assumption of sampling data points without replacement.	weakness
2021-680	While sampling without replacement is indeed the usual setting in practice, most of the stochastic optimisation literature builds on the assumption of sampling with replacement.	weakness
2021-680	And to my knowledge, no major differences (in terms of generalization performance) have been reported in the literature between the two approaches.	weakness
2021-680	a) Can the analysis presented in the paper be extended to setting of sampling with replacement?	weakness
2021-680	It seems to me that this should be straight-forward.	weakness
2021-680	Equations (12) and (13) should hold also when each minibatch is obtained from sampling with replacement.	weakness
2021-680	In that case, the expectation of the second-order correction term should directly give a result akin to Eq. (22).	weakness
2021-680	If that is in fact possible, it should definitely be added to the paper.	weakness
2021-680	b) If that is not possible, what prevents the application and is this a technicality or would you actually expect substantially different behavior in terms of generalization?	weakness
2021-680	c) It would also have been nice to see the experiments repeated with sampling with replacement to check empirically whether the findings hold in that case?	weakness
2021-680	Something that bugs me from an optimization perspective is that the smoothness properties of the problem do not enter this analysis at all.	weakness
2021-680	For example, you write (near the bottom of page 4) that "our analysis assumes mϵ=Nϵ/B is small." However, any given loss function C(w) can be rescaled by a constant M≫1 while scaling the step size with 1/M.	weakness
2021-680	This leaves the behavior of SGD unaffected while making the step size arbitrarily small.	weakness
2021-680	Why does that not enter into the analysis?	weakness
2021-680	It probably relates to my comment (4), seeing that the step sizes are assumed to be so small that they are not restricted by the smoothness of the function.	weakness
2021-680	Minor Comments The paper derives the implicit regularizer and provides empirical evidence that it can partially explain the benefits of large step sizes for generalization.	strength
2021-680	However, very little attention is given to the regularization term itself and to the question why this regularizer might be beneficial.	weakness
2021-680	The only comment speaking to that is that the regularizer penalizes "sharp" regions.	weakness
2021-680	I would like to see this discussion expanded and connected to the recent literature.	weakness
2021-680	At the end of page 6, you write about the large batch size regime and say that the "we expect the optimal learning rate to be independent of the batch size in this limit."	weakness
2021-680	It would have been great to substantiate that conjecture with an experiment and/or to refer to specific experiments done in prior work.	suggestion
2021-680	You repeatedly use the phrase "small but finite learning rates".	weakness
2021-680	If my understanding is correct, that has phrase has a very precise meaning in the context of this work, namely that terms of order O(ϵ3) are vanishingly small while terms that a quadratic or linear in ϵ can not be ignored.	weakness
2021-680	(This is in contrast to prior work that also ignores quadratic terms.) Maybe this could be stated clearly the first time you use this phrase.	weakness
2021-680	Typos / Style I think you should capitalize references to sections, equations, figures, et cetera.	suggestion
2021-680	The bib file could really need some love.	suggestion
2021-680	You are citing the arXiv versions for several papers that have been published in peer-reviewed venues.	suggestion
2021-680	Capitalization in paper titles is messed up (e.g., "sgd").	weakness
2021-680	Edit after Rebuttal I thank the authors for their engagement with my review.	misc
2021-680	Many of my comments and questions have been resolved and, consequently, I have increase my score and recommend accepting this paper.	decision
2021-680	This paper studies an implicit regularization mechanism of finite learning rate SGD by introducing explicitely a regularization term, using the framework of backward analysis.	abstract
2021-680	They theoretically motivate their analysis, then empirically demonstrate it on CIFAR-10 using a Wide ResNet architecture.	abstract
2021-680	This extends a previous (Barrett and Dherin, preprint) analysis of GD using the same framework, but limited to full batch GD.	abstract
2021-680	Noticeably, this new analysis using minibatch GD highlights an additional regularization of the trace of the covariance of per-example gradients.	abstract
2021-680	In sec 2, however, I think it should be made clear that the setup is slightly different from minibatch GD, even when trained for a single epoch, in that there is an expectation accross permutations of sequences of minibatches.	abstract
2021-680	Can you discuss this assumption a bit more?	abstract
2021-680	In terms of experiments, it would be useful to include other architecture/tasks, even toyish, in order to appreciate the generality of the empirical evaluation.	suggestion
2021-680	Overall, I think this contributes new interesting insights which are very relevant for studying minibatch GD in deep learning.	strength
2021-680	This paper analyzes the implicit regularization in SGD with finite learning rates via backward error analysis.	abstract
2021-680	The modified flow introduced in this paper better approximates the practical behavior of SGD as it does not require vanishing learning rates and it allows to use random shuffling in stead of i.i.d sampling.	abstract
2021-680	The numerical experiments validates the existence of the implicit regularization and how it affects the generalization of the model trained by SGD.	abstract
2021-680	The difference from SDE analysis is also discussed.	abstract
2021-680	Reason for score: The paper is well organized.	strength
2021-680	Specially, I enjoy reading section II.	strength
2021-680	The tool of backward error analysis and the derivation of the implicit regularization in SGD flow are introduced clearly and concisely.	strength
2021-680	The analysis is based on random shuffling instead of i.i.d sampling matches the practical use of SGD.	strength
2021-680	The numerical experiments are very convincing.	strength
2021-680	The consistency of SGD with larger lr and SGD with smaller lr plus explicit regularization validates the results of theoretical analysis.	strength
2021-680	The numerical experiments also provide some insights into tuning hyper parameters such as learning rate and batch size.	strength
2021-680	Summary: To analyze why the generalization error of SGD with larger learning rates achieves better test error, this paper analyzes the implicit regularization of SGD (with a finite step size) via a first order backward error analysis.	abstract
2021-680	Under this analysis the paper shows that the mean position of SGD with m minibatches effectively follows the flow according to Eq (20) for a small but finite step size, while GD effectively follows the last inline equation in section 2.1.	abstract
2021-680	The paper shows empirically on an image classification task that by explicitly including the (implicit SGD) regularizer, SGD on the modified loss behaves similarly to using a larger learning rate when evaluating on the test set.	abstract
2021-680	The paper then extends this results to consider varying the batch size in section 3, showing that for small batchsizes the implicit regularization scales with the ratio of learning rate and batchsize ϵ/B.	abstract
2021-680	Finally in section 4, the paper analyzes SGD when for each sampled minibatch in an epoch, we apply n gradient steps with a stepsize ϵ/n and show that performance degrades as n increases, suggesting that the benefit of SGD with larger learning rates is due to the implicit regularizer and not the temperature of an associated SDE.	abstract
2021-680	This paper is clearly written and well edited.	strength
2021-680	I find the main result and the analysis technique interesting and novel.	strength
2021-680	Although the experiments are well explained and help support the theory developed, there is only one experiment setting making it difficult to believe strong general claims such as those in section 4.	weakness
2021-680	I do have concerns about equating the "mean" behavior of SGD with the actual behavior of SGD and.	weakness
2021-680	Recommendation: I recommend accepting this paper.	decision
2021-680	As it currently stands, this paper is borderlin on the acceptance threshold for me.	decision
2021-680	I like the novel use of the backward error analysis to gain insight into the behavior of SGD and I believe it would be of interest to *CONF* readers.	strength
2021-680	My main concerns are the papers' narrow focus on the mean behavior of SGD and the single experiment setting used to validate results.	weakness
2021-680	I would much more strongly support this paper if the theoretical analysis was stronger (e.g. analyzing the variance of individual SGD flows/regularizers to the mean SGD flow/regularizer) or if more experiments (in different settings) supported the results.	suggestion
2021-680	Questions: If we don't take the expectation over ξ(m) in Section 2.2, the theory suggests that there exist a (random) modified flow for each (random) ordering of minibatches C^0,…,C^m by equating equations (14) and (19).	weakness
2021-680	The main result Eq (20) would correspond to the expected value over the (random) modified flow.	weakness
2021-680	I believe this paper would be much stronger if there was some discussion of how the variance / deviations of these random flows from the mean flow (i.e the variance of ξ(m)) affects the implicit regularization and how this scales with batch size and properties of the loss.	suggestion
2021-680	Would the implicit regularization break down for some experiments?	suggestion
2021-680	Is the assumption that mϵ is small reasonable (so that we can ignore the higher order O(m3ϵ3) terms in the analysis)?	weakness
2021-680	Isn't m=N/B the number of updates per epochs very large in practice since N>>B?	weakness

2021-725	This paper studies the mean-field limit of the policy gradient method (with entropy regularized) and proves that any stationary point under this setting is a global minimizer.	abstract
2021-725	I am not able to verify the entire proof as it involves a lot of standard steps to bridging the finite parameter case and the mean-field limit.	weakness
2021-725	The result seems promising and well complements several theory results in RL in the past year, e.g. the optimality of policy gradient under NTK regime and the TD algorithm in the mean-field regime.	strength
2021-725	Although the paper does not provide the convergence guarantee of the mean-field density flow to a stationary point (please correct me if this is wrong), the characterization of the optimality is still a good contribution.	strength
2021-725	It well explains why a neural network policy is globally optimal given (1)it is stationary under the training via the first-order method (2)its parameterization has strong expressive power, e.g. it has infinite parameters or it is essentially a nonparametric model.	strength
2021-725	The convergence (section 4.2) to the many-particle limit.	strength
2021-725	i.e. mean-field limit, seems standard, as the authors claim it is very similar to the case of supervised learning.	strength
2021-725	I still would like to ask whether the authors found any key differences between the supervised learning case and RL objective, i.e., maximizing the total reward.	strength
2021-725	In particular, does the absence of a strongly convex loss function cause any difficulty in the proof?	weakness
2021-725	This paper studies the asymptotic convergence properties of (population-level) policy gradient methods with two-layer neural networks, softmax parametrization, and entropic regularization, in the mean-field regime.	abstract
2021-725	By modelling the hidden layer as a probability distribution over the parameter space, the training dynamics of policy gradient methods can be written as a partial differential equation.	abstract
2021-725	Under certain regularity conditions, the paper shows that if the training dynamics converge to a stationary point, this limiting point is a globally optimal policy.	abstract
2021-725	The paper also presents results for finite-time convergence of the training dynamics for neural networks to the mean-field limit.	abstract
2021-725	The optimization landscape and convergence properties of policy gradient methods have drawn attention in RL theory for a long time, and it is nice to see a work that studies this problem from the perspectives of mean-field limit of neural networks, albeit being completely asymptotic.	strength
2021-725	Overall I think this makes an interesting contribution, and I appreciate the sketch of proof ideas in the simpler bandit case.	strength
2021-725	Technically, it seems that the main results are built upon existing frameworks of Mei et al., (2018), Chizat and Bach et al., (2018), etc.	weakness
2021-725	But the author also pointed out an interesting technical novelty, which is the use of density arguments when the problem structure is in lack of the hidden convexity used in other works.	strength
2021-725	On the other hand, it appears to me that one major weakness of the result is that the theorem holds true only when the dynamics converges to a stationary point.	weakness
2021-725	Can the authors provide conditions under which this can happen?	weakness
2021-725	For example, would it be possible to establish some compactness under additional regularity conditions and use it to show the convergence of a subsequence?	suggestion
2021-725	If the convergence does fail to happen in certain regimes, how will the dynamics behave?	suggestion
2021-725	Will it convergence to a limiting cycle or diverge?	suggestion
2021-725	Are there some natural counter-examples?	suggestion
2021-725	It would be helpful if the authors could provide more discussions on this condition.	suggestion
2021-725	Additionally, it seems to me that the paper actually shows that (due to entropic regularization) the limiting point is the Boltzman policy induced by the optimal Q function (at temparature τ), instead of the optimal Q function iteslf.	weakness
2021-725	If that is the case, this needs to be stated clearly in the theorem.	weakness
2021-725	Overall, I vote for accepting.	decision
2021-725	This paper extends previous work in the parameter dynamics of simple neural networks to reinforcement learning framework in continuous state and action spaces with nonlinear function approximation and overcomes the challenge of lack of convexity.	abstract
2021-725	The main concern of mine would be that the theorems in the paper are not powerful enough to help us fully understand the experiments.	weakness
2021-725	Pros: This paper introduces the mean-field formulation into the reinforcement learning framework.	strength
2021-725	The technical proof seems highly challenging.	strength
2021-725	Under mild conditions, it demonstrates interesting convergence properties of the particle dynamics to the mean-field counterpart and further, the mean-field dynamics to the global optima.	strength
2021-725	This provides new insight into theoretical understanding of this problem.	strength
2021-725	Cons: There still exists gap between the theoretical results and numerical experiments to be filled.	weakness
2021-725	The experiments shown in Figure 1 are conducted with finite number of neutrons and constant step-size, but the theorems are stated under the adiabatic limit.	weakness
2021-725	Hence there is lack of quantitative results under the setting of finite number of samples and finite gradient step size.	weakness
2021-725	The paper only shows convergence, but fails to give more detailed properties like convergence rate, etc.	weakness
2021-725	This paper provides a mean-field characterization of entropy-regularized policy gradient dynamics for wide single hidden layer neural networks.	abstract
2021-725	The evolution of neural network parameters is described by a transport partial differential equation.	abstract
2021-725	And the convergence properties of the dynamics are established.	strength
2021-725	Overall, I vote for accepting.	decision
2021-725	The paper is well-written. The technical contents seem sound and a comprehensive literature review is provided.	strength
2021-725	And authors also conduct numerical experiments to validate the theory.	strength
2021-725	My two minor comments are as follows: The remarks in Section 4.1 are mainly explaining why authors need the assumptions to establish the theories.	weakness
2021-725	It would be nicer to provide examples when these assumptions hold or discuss the generality of the assumptions.	suggestion
2021-725	It would be nicer to spend more space to discuss the major differences of the theoretical analysis in this paper compared to earlier results on the mean-field limit in the supervised learning setting.	suggestion
2021-725	It would be helpful to discuss different aspects of the technical analysis more explicitly.	suggestion

